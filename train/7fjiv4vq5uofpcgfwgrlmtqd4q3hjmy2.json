{
    "id": "7fjiv4vq5uofpcgfwgrlmtqd4q3hjmy2",
    "title": "From Kernels to Causality",
    "info": {
        "author": [
            "Bernhard Sch\u00f6lkopf, Max Planck Institute for Biological Cybernetics, Max Planck Institute"
        ],
        "published": "Aug. 26, 2013",
        "recorded": "July 2013",
        "category": [
            "Top->Computer Science->Optimization Methods",
            "Top->Computer Science->Machine Learning->Kernel Methods->Support Vector Machines",
            "Top->Computer Science->Compressed Sensing",
            "Top->Computer Science->Machine Learning->Regularization"
        ]
    },
    "url": "http://videolectures.net/roks2013_schoelkopf_kernel/",
    "segmentation": [
        [
            "I was little bit puzzled what I should be speaking about.",
            "It took me awhile to figure it out.",
            "That's why in the leaflet interpolators, probably No title.",
            "I felt that I should be talking about kernels.",
            "On the other hand, nowadays spend most of my time thinking about causal inference.",
            "So I came up with some kind of compromise.",
            "Usually this compromise don't work, but let's see, maybe today is going to work and but I will start with kernels and maybe talk about kernels more than about the rest.",
            "So let's get started with this.",
            "So this is a this conference.",
            "The Title Rocks 2013 already suggests that it might be the beginning of some kind of series, and it's such a beautiful place here.",
            "So maybe the probability is high, so maybe we are witnessing historic event here, and it makes sense to also start with some other historical notes.",
            "And that's what I'm going to try and to give you a little bit of."
        ],
        [
            "Effective from my point of view about kernel methods, at least at the beginning.",
            "So most of you probably know this kernel swear first used in mathematics in the theory of integral equations by Hilbert.",
            "They were used in statistics probably first or maybe first in an extensive way by Agrace Barba and keyboard, or for her PhD advisor.",
            "They were actually also mentioned in the 1st edition of through their Hearts book, but what kind of dismissed as some mathematical peculiarity?",
            "That's not going to be very.",
            "Practical, so the quotation is here.",
            "They were used to approve the convergence of the potential function methods in Russia.",
            "Actually, in the lab where Vladimir Vapnik would join a little bit later or maybe around that time as a PhD student.",
            "So there were there was the lab of Eisaman and I think Bob Woman was in or where members of that lab and they had been studying theory of learning already for awhile.",
            "Vape Nixon.",
            "Terminix joined the lab of Llama, who I think at the time was not yet working learner, but then.",
            "Unlearning, but then maybe he had to start working with that, given his surname, and he asked the Institute director atop it, sneak off for permission, and started working on learning with happening in German Yankees.",
            "So here's a picture of a picnic.",
            "It then."
        ],
        [
            "Cook until the 90s before support vector machines the way we now know them were actually created and they were they went called support vector machine.",
            "Initially they start with some kind of identity crisis.",
            "At the beginning they were called optimal margin classifiers and then a little bit later after the inclusion of Slack variables they were first called soft margin classifiers.",
            "In the submitted version and I was trying to I have a.",
            "Copy of that, but I couldn't find it for this talk.",
            "And then for the final version of this paper that appeared in machine learning, they were called support vector networks.",
            "And this."
        ],
        [
            "Also, so this paper was also my first contact to SVM's, I joined Bell Labs as a peer St student at that time.",
            "And I have to simply admit that my role in this paper was actually just to run a neural network, so I sometimes I styled renewal networks and you can see it down here.",
            "I mentioned as the guy who ran the two layer neural network in this table here, and I think flooding there was quite pleased that my result was nowhere near to the SVM.",
            "But I have to admit, I also wasn't a big neural network expert at the time, so I was a PhD student.",
            "During this noon, but actually young Liqun also run this one.",
            "He also didn't do as well as the SVM at the time.",
            "So."
        ],
        [
            "My first paper, my my first paper that I wrote for myself about SVM's was this one extracting support data for a given task, and in this paper actually also in this paper is the name support vector machine."
        ],
        [
            "Only appeared and we were well.",
            "We made several points in that paper, but one of them was that.",
            "We were getting very similar results using polynomial kernels, RBF kernels and sigmoid kernels.",
            "These kernels were still fashionable at that time because the networks then looked like neural networks.",
            "And we kind of argued that this means that it's not so important what structure would set of functions you choose, but it's more important.",
            "What kind of capacity control?",
            "Or it's more important that you do the right capacity control in that set of functions.",
            "So maybe that's a slight contradiction to the recent developments that we're seeing nowadays.",
            "People are getting great results with things like deep neural networks using SVM's only at the last lay on.",
            "So in that sense, the structure is important, and.",
            "And I believe John might have touched upon some of these things in his talk earlier on.",
            "Anyway, so."
        ],
        [
            "Games then moved on kernel.",
            "PCA was developed.",
            "You probably know about that, so it's a nonlinear generalization of principal component analysis.",
            "Where, for instance, for this data set you get first 2 principal components.",
            "Kind of cluster these groups into three clusters, and then the higher order components look for structure within the clusters.",
            "And when we, once we had developed that we we notice of course, that it looks like this should be a good approach to clustering, and that I'm gonna veg who probably also many of you know, started his Masters thesis in Berlin and we gave him as a as a task to work out how to use kernel PCA for clustering and he tried all sorts of things and it worked quite well.",
            "But we never really got our head around it and never understood why it worked well and.",
            "This took a long time and only now it kind of makes sense to me because it's actually very similar to spectral clustering."
        ],
        [
            "And to explain this is this is also kind of commonplace.",
            "But then many people maybe don't know it, so I thought at this conference might be a good chance to mention this.",
            "So spectral clustering just to remind you you have a similarity matrix.",
            "Call it K so it's like our kernel matrix.",
            "Then there are several approaches, but let's focus mainly on this one.",
            "Normalized cuts.",
            "In this approach, we compute this matrix called the normalized Laplacian and then the prescription is.",
            "So this is computed based on the training data.",
            "Prescription is then to map the training points to the inputs.",
            "So remember the training points to the corresponding entries of the second smallest eigenvector of this matrix.",
            "It sounds like a funny prescription.",
            "So remember this matrix, the side length is the number of training points, so there is one entry for each.",
            "Input and therefore we can somehow map into this thing.",
            "And then we can.",
            "Once we have done this.",
            "So this is now our our new representation of the data.",
            "We can partition the data points based on these values, for instance by performing K means or something like that, and there's this slightly different version that I'm not going to discuss right now.",
            "Now, remember, kernel PCA?",
            "I didn't derive it, but probably you know it kinda.",
            "PCA looks quite different, so in kernel PCA we have the eigenvectors of the kernel matrix with corresponding eigenvalues.",
            "We get them by just diagonalizing the kernel matrix.",
            "Then the prescription is given a test point X.",
            "So this could not be any point, not just an element of the training set.",
            "Map this point to the reputation Hilbert space and then predict on tool to get the first feature project onto the largest eigenvector of K and then normalized by this quantity, which ensures that the eigenvector is normalized to length one in the reproducing kernel Hilbert space.",
            "So let's so it's just PCA.",
            "Then in the feature space.",
            "In here I have written it down.",
            "The prediction and other protection band looks like this.",
            "It's a kernel expansion which is also a consequence of the represent a theorem, and it looks rather different.",
            "However, remember this is for a general point X in up here.",
            "This is for the training points, and it turns out if we substitute a training point in here for kernel PCA, then this formula also simplifies."
        ],
        [
            "So this formula for a general point.",
            "Sorry for if we take a training point.",
            "Then actually.",
            "We can exploit the fact that this is an eigenvector and we simplify this quantity and we actually also get the coefficient vector.",
            "So the entries of the coefficient vector will also contain the representations the new representations off the training points.",
            "So it's actually very similar and we can even make it even more similar if we want by looking at so now it's similar on the on the structural level, but the kernel matrix still looks a bit different, but we can make the kernel matrix more similar by choosing a particular type of kernel matrix.",
            "So for instance, for a connected graph it's known that the normalized Laplacian has a single zero eigenvalue and the pseudo inverse of this normalized Laplacian is also known as the discrete greens function of the diffusion process.",
            "On the graph governed by L, so it will be like diffusivity and we can view this thing the pseudoinverse as a kernel matrix and namely at the kernel matrix that encodes the dot product that corresponds to the metric called the commute time.",
            "So commute time in a diffusion processes.",
            "How long does it take you to get from one point to the other one impaction that's symmetric, so for this special type of kernel, things are actually very much analogous.",
            "And this is the second thing I couldn't mention here is that the kernel matrix kernel PCA matrix has to be sent us.",
            "This implies it has a zero.",
            "I can value a single eigenvalue zero in the case that the kernel is strictly positive definite, and this exactly corresponds to the zero eigenvalue of the normalized Laplacian, which we ignore if we take the second smallest eigenvector.",
            "OK."
        ],
        [
            "And then of course, there's this tiny difference here.",
            "Trivial difference here.",
            "We take the second smallest.",
            "Here we take the largest, but all these things can be."
        ],
        [
            "Turn around by inverting the order of the eigenvectors.",
            "So."
        ],
        [
            "So far I talked about SVM, so talked about other kernel libraries that you can generate by using the reproducing kernel as a dot product.",
            "It was also soon noticed that the input domain doesn't have to be a vector space, and then finally the most important recent development maybe internal methods has been the theory of kernel mean embedding methods that I think that's still an active area of research and I want to spend 5 or 10 minutes on that.",
            "And then maybe the most the best known examples of this are kernel ICA and maximum mean discrepancy.",
            "So what is that about?"
        ],
        [
            "So let's imagine we have a set of points subset of our input domain.",
            "And define this object here the mean map as.",
            "Mapping all these points into the feature space.",
            "So this is 1 representation of our mapping into the feature space.",
            "I mean, I felt that this is a group of experts here.",
            "I don't have to define that, but just recall one way to write the mapping induced by reproducing kernel is to say that each point XI gets mapped to a function that we get by substituting XI into one argument of the kernel, and then the repetition commercial space is the linear completion of such functions.",
            "With the suitable dot product.",
            "So if we now map a set of points into the representative of space and then compute the mean in that space, we get this object here.",
            "And let's call this the main map.",
            "Now we can ask the question, when is this map injective?",
            "So in other words, suppose we have a certain second set of points Y, one through YM.",
            "Actually, it could also go up to N, could be a different size set, but let's not worry about that.",
            "From the same domain, and let's assume this second set of points is different from our initial one.",
            "Is it nevertheless possible that they mapped to the same mean in the reproducing kernel Hilbert space?",
            "So clearly if if we use a trivial arcade test, but we started with a vector space and we just map our points into the same space, so we're still in some finite dimensional vector space, then in general this map is not going to be injective, because you can imagine lots of different datasets that have the same mean.",
            "Not surprisingly, it turns out in the repetition kernel Hilbert space.",
            "The main contains a lot more information.",
            "And there's a large class of kernels for which this mapping is actually injective, so just mapping our data points in the professional space and then only remembering the mean is enough.",
            "And in principle we can reconstruct the whole data set from that.",
            "It is actually relatively easy to see in this is the case, at least for these mappings of point sets.",
            "It becomes a bit more complicated later if we have measures instead.",
            "But for point."
        ],
        [
            "That's so.",
            "Let's imagine we have two sets that have the same mean or actually slightly more generally.",
            "Let's imagine 2 minutes.",
            "Just we have coefficients here.",
            "1 / M like, but let's imagine we have more general coefficient values here.",
            "So let's imagine we have two kernel expansions.",
            "That are identical, and we also assume that none of the points are repeated on the left.",
            "None of the points on the right are repeated.",
            "If this identity holds true, then it's easy to prove.",
            "That actually these two point sets have to agree point by point.",
            "So this set of the axis in the set of the wise has to be identical.",
            "I'm not going to go through it, but I just wanted to show it's very short proof.",
            "So in this case things are injective, we don't lose."
        ],
        [
            "Information by mapping into the reproducing kernel Hilbert space and so a little some more things that I wanted to say about this.",
            "So this was our mapping again, since the kernel represents point evaluation of a function, so the DOT product between this kernel and the function gives us the function value at the point, so that's special about reproducing kernel Hilbert spaces.",
            "That's what gives them.",
            "Or that's crucial aspect functions are defined.",
            "For every possible point value that equivalence classes of functions where they are really functions.",
            "Due to the linearity of the DOT product, if we take this kernel mean and take a dot product with a function, we actually get the mean of the function evaluated on the sample.",
            "So this element here represents the operation of taking the mean of a function on the sample.",
            "Moreover, if we map two sets X&Y, then we can express.",
            "And the norm of the difference first of all, by a by a coaching Schwartz.",
            "Essentially as the soup over a dot products between the difference vector and functions from the unit ball.",
            "Which then we can rewrite in terms of this thing here as the soup of this quantity here.",
            "So that means 2 means in the future space are different only if we can find a function that distinguishes these two sets.",
            "In that sense, in terms of having different means on these two sets.",
            "And remember, if the kernel is strictly positive definite, this is equivalent to saying that the two sets are different, so we can always distinguish samples that are Dist."
        ],
        [
            "We can also look at what the function looks like that function that witness is a difference between two sets of points, and this is an example.",
            "If we take a Gaussian kernel.",
            "And we have data.",
            "1 sample.",
            "Let's say X is drawn from a Gaussian and why is drawn from a Laplacian with the same mean and 1st moment.",
            "Then this function that distinguishes the Gaussian in the Laplacian sample looks like this.",
            "It's a large in the middle, will apply and data is more likely.",
            "It's a negative here on the site where the Gaussian is more likely.",
            "And it is reasonably smooth because it's from the.",
            "It's a unit length function.",
            "So it's a unit link function in the reticle Hilbert space, which implies certain smoothness properties.",
            "OK, so we can generalize generalize the same."
        ],
        [
            "Two measures, and in this case we don't have sets of points, but we have Borel probability measures.",
            "We have to assume that these expectations here exists, which automatically is guaranteed.",
            "For instance, if available is bounded.",
            "We then define the image of such a Borel measure.",
            "Under the kernel mean map as the expectation of the feature space mapping.",
            "This exists under these conditions.",
            "In this element here.",
            "So the main embedding of a measure represents the operation of taking an expectation of this function with respect to sampling.",
            "The argument of the function from the Borel measure and just completely analogous to the last slide.",
            "We can also write the difference between two embeddings of measures, as this soup over the unit ball of these means of functions.",
            "On the two measures now we can ask the question again under which conditions is snapping you injective?",
            "So when do we not lose information by doing this?",
            "Because it's attempting thing to do we we have, we have a class of measures, we just represent them in representing kernel Hilbert space and then we can do linear algebra because we have now Hilbert space elements.",
            "So that's nice, and it's particularly nice if we don't lose information."
        ],
        [
            "This is true for an interesting class of kernels.",
            "And it's basically uses to some classical results of probability theory.",
            "Due to faulty in movie.",
            "Who showed that two Borel measures are identical if and only if?",
            "The soup if and only if even nonlinear.",
            "Let me put it this way if nonlinear transformations so clearly.",
            "If you look at the expectations of example from P -- X samples from Q, then this will not be sufficient to distinguish two measures.",
            "If you also allow for nonlinear transformations of your data by F before taking the expectations, and you require that no matter which nonlinear transformation you choose, these quantities are still different.",
            "It turns out if the function class is large enough, then this is equivalent to saying that the two measures are identical.",
            "So this is the same idea as the idea in kernel ICA and we have fun sis back here who has pioneered this direction and so this is a classical result and we can now for the kernel mean embedding the following way where we combine it with this formula that I showed in the last slide.",
            "So this quantity here can be expressed as this soup which looks almost the same as here.",
            "The only difference is here we have a unit ball in the representatives in kernel Hilbert space.",
            "Here have we have the function of all?",
            "Continuous bounded functions on some domain.",
            "Now we only have to argue that for certain kernels, our unit ball is dense in this set here, and that's the case for the classic kernels that has been called Universal by Stein Mart.",
            "And then we're done, and we can assert that if the kernel is universal, then the kernel mean mapping view is injective, which means that if the output of the mapping is the same, inputs also have to have been the same.",
            "So that's quite nice.",
            "And it's also reassuring that this connects to a number of things in."
        ],
        [
            "In the statistics, we get some interesting special cases here.",
            "If we use this kernel here in the exponential kernel, which is known as a universal kernel, we recover what's called the moment generating function of a random variable which is defined like this in our notation.",
            "If we put the imaginary unit in here, we recover what's called the characteristic function.",
            "OK, so if we have such an injective mapping, this provides us with a convenient metric on probability distributions and we can use that to do various things, for instance to check whether two distributions identical.",
            "So that's the."
        ],
        [
            "First application this so called 2 sample problem and I'm not going to go through details, but I think you get the idea.",
            "So we want to check this is the case now we have only empirical data, so we write down empirical estimators for this and in the end we end up with something that looks like this and we can show it's an unbiased estimator of this true difference squared error in the producing Hilbert space, and it's easy to compute and one can also compute it.",
            "Since kernel it's can be defined unstructured data.",
            "When can computed on things like graphs or strings etc.",
            "Also have been used also in feels like bioinformatics.",
            "And."
        ],
        [
            "The second application, so this is that I mentioned already.",
            "Fontsize back Colonel independence testing.",
            "So how do we recover kernel independence testing?",
            "Well, one way to recover it is to say that two random variables are independent if and only if the joint distribution and the product of the marginals are identical, which for universal kernel we can test by checking the difference between the embeddings of the joint distribution and the embeddings of the product of the marginals.",
            "And if we do that, it turns out and we get a nice estimator which looks like this, and it's also linked to something called the cross covariance.",
            "So we can this quantity here.",
            "Actually will be the same as the as the Hilbert Schmidt norm of the so called cross covariance operator between the two.",
            "Reproducing kernel Hilbert spaces.",
            "And yeah, so I think this was a little bit the last slide where a bit of review material, but maybe not everybody knew it and."
        ],
        [
            "Now I want to show something that maybe most of you haven't seen yet.",
            "I hope so at least, and this is a link to some problem of physics that I've been interested in awhile for awhile, and that's in found.",
            "Hope for imaging and super resolution.",
            "In this case we are considering kernel mean embeddings for stationary kernels, translation invariant kernels that take this form here, and I think probably most of you have heard of Fourier imaging.",
            "In their undergraduate degree, and so in a nutshell, or maybe even at high school.",
            "So if you have an image just as a distribution that tells us where, where do the photons come from.",
            "Now suppose you only have this blue line, so you have a Delta measure here.",
            "So your image in just a point.",
            "If you do this through some finite aperture of size D, then you know that you get this interference pattern.",
            "You get an intensity distribution, which can be computed to be this blue line here, which is the square of the Fourier transform.",
            "Of the aperture function and if I have an image that's a mixture of two point sources, I get a superposition of two such.",
            "So called sync functions sine X / X with some scaling factor and if these two points are very close then I won't be able to distinguish these two sync functions anymore.",
            "And people say that in this case we cannot resolve these point sources from each other anymore.",
            "That's the reason why now this this distance here scales inversely with the.",
            "Sorry, it scales with D. Therefore people try to build large telescopes with large apertures so that we can actually separate these.",
            "Images now in.",
            "If we have a general image here, then what we observe over here is the.",
            "Again, this square of the full year transform of the aperture.",
            "So by I did note an indicator function of this aperture convolved with the distribution that models are image, so that's what we're going to record back here at the sensor.",
            "And if we set K equal to the square root transform, so this is a this K up here, it's a stationary we can think of this as a stationary kernel.",
            "It will be actually a positive definite kernel biobox theorem.",
            "So bonus Theorem says that the transforms of non negative sign measures are positive definite trend kernels, roughly speaking.",
            "Then we recover this image over here exactly as the kernel mean embedding.",
            "Using this kernel function, so that's just the physical realization of a kernel mean embedding.",
            "And so that's quite nice.",
            "But unfortunately or maybe not surprisingly, this mapping file from you so the kernel mean embedded using this particular kernel is not invertible, because this particular kernel is not universally so in the sense that even reassuring, because I told you before, if these things get too close, we cannot resolve them anymore, so we cannot resolve very fine structure.",
            "If the aperture has finite size, so that's well known in physics.",
            "And it's every photo as the diffraction limit.",
            "Now it turns out and I want to show that during the next few slides, if we restrict the input domain to distributions that have compact support, these nothing is actually invertible and no matter how small this aperture is and this is some, it's a direct consequence of something that we notice in the context of studying universality or injectivity of kernel mean embeddings as a pure machine learning problem.",
            "But it turns out it applies in this domain and natural.",
            "Say it turns out that essentially what we're using in our proof has also been noticed by some people in this domain already independently.",
            "So there's a certain convergence of these two problems in the same kind of mathematics gets used on both sides.",
            "OK.",
            "So before I get into this just a short digression.",
            "So I quite like this this physical realization of the kernel here."
        ],
        [
            "We can think of it as the pointers, so the kernel in this case this describes the point respond response of a linear optical system.",
            "So this convolution is a linear process.",
            "And.",
            "So it's a it's a Green's function in more generally we can also in other domains analyze kernels as greens functions, and maybe I should mention this here because this conference is about regularization.",
            "This is probably well known for many of you.",
            "If we can write the arcade S norm as the norm of the result of applying some regularization operator to the function to be regularised.",
            "Then the kernel can be thought of as the greens function of P star P, where P is this regularization operator and for instance, people have analyzed the Gaussian kernel.",
            "This paper by Hulanicki Raj where they show that the Gaussian kernel corresponds to regularization operator, which computes an infinite series of derivatives of F. And more generally, it translation invariant kernels.",
            "We can weaken oversight P as a multiplication operator in Fourier space in the spectrum.",
            "That tells us how different parts of the frequencies in the functions are amplified or attenuated.",
            "Anyway, let's go back to the Fourier imaging."
        ],
        [
            "So if we assume that so I'm being a little bit sloppy here.",
            "If you want to know precisely, you have to look in the papers.",
            "We assume the densities exists.",
            "The kernel is shift invariant.",
            "Paul for your transform exists.",
            "Then we can derive this condition here.",
            "Which will imply that P is equal to Q, and we can then so the head denotes Fourier transform.",
            "So it depends on the Fourier transform of K and three transforms of the distributions.",
            "So it depends on whether K has is non zero in the part where we have frequencies that we want to distinguish etc.",
            "From this one for instance, see that if K had if the spectrum of K. Has full support.",
            "We find because then we can just divide by this.",
            "So so sorry maybe I was a bit too fast here so I should say that mu will be invertible exactly if this implication holds true, right?",
            "If whenever the images under of the kernel map are equal, the distribution should be equal.",
            "So I'm not just changing these a little bit, I want this implication to hold true.",
            "Clearly if K head has full support, I could divide by it and this becomes trivial or almost trivial and.",
            "But K hat does not have full support for our particular type of kernel because the aperture is finite, so it's just it's going to be the convolution of the aperture with itself."
        ],
        [
            "That's bad news.",
            "However, we can fix this by restricting the class of distributions in particularly the way we have to restrict it is we have to force them to have compact support.",
            "So now all our input our probability measures that appear as images on the left hand side have compact support.",
            "We can apply a famous theorem from complex analysis.",
            "So Paley, Wiener theorem and in this case due to this theorem it turns out that it's sufficient.",
            "To know the distributions characteristic function on a compact subset.",
            "So intuitively, I guess you can believe me that you will believe me that even though in this case, even though we cut out a certain part of the spectrum innocence, this paling return allows us to extend this and it's enough to have agreement on this spectrum.",
            "Therefore we can invert things.",
            "And for this to work, the Fourier transform key only needs to have a support with nonempty interior, and that's for interest.",
            "The case for our particular K as soon as the aperture has finite size.",
            "So whenever it's non zero, this will be the case.",
            "And this we already knew for awhile, but not from the not in the optical case.",
            "So it turns out applied to this problem with Hanover diffraction.",
            "The imaging process is not invertible in the general case, but it does become invertible if we restrict the class of distributions to compact support."
        ],
        [
            "And we recently also.",
            "Sort of tried to try this out or we tried it out on some very simple data.",
            "We cooked up a simple algorithmic method that exploits the.",
            "The bounded support the fact that the image has bounded support, so we actually have to of course generate images with bounded support and we also it turns out it also helps to exploit the non negativity of the image.",
            "So if this squared intensity image and so in this application here we have.",
            "We have a double star which is imaged through some optical system where the aperture is set relatively small such that so the true doubles.",
            "Well maybe let me start with this one so we have a double synthetic double slow just like an Eli D with a mask with two holes and if we image this double star with some optics with a relatively wide aperture.",
            "We get this image here down here.",
            "If we turn down the aperture, these things become blurry and in this case we have turned it down so much that the the separation between the two stars is 0.5 times the so called Rayleigh limits rate.",
            "The rate limit is typically used to characterize the optical resolution of such a system, but we can then run our deconvolution methods on this input.",
            "And recover this reconstruction using these constraints up here.",
            "You can see that the reconstruction is quite close to this image that we took over here.",
            "It's even a little bit sharper, which is some sense, not surprising, because of course.",
            "Also this image was taken with a finite aperture, so it's just as big as it got, but not large enough to make these really pointlike, so that was."
        ],
        [
            "Kind of reinsure reassuring, so I want to get to the end of the kernel stuff.",
            "But just mentioning so I'm going to move to causality, but we're still interested in kernel machines.",
            "We have spent some time.",
            "This developing an approach for SVM's that works with distributions rather than training points.",
            "This is also related to kernel mean embedding, and I think it is quite interesting and we have also recently started working on.",
            "And what statisticians know is the James Stein effect in the context of kernel machines.",
            "In particular, kernel mean estimation.",
            "OK."
        ],
        [
            "So machine learning I think so maybe this is now part of the motivation where where I partly moved to a different field has certain shortcomings and I think they all have to do with the difference between machine learning or statistical information in causal information.",
            "And there's different ways of saying it.",
            "One is machine learning is based on statistical information, when it's it's limited to ID data.",
            "And finally it's a black box approach and it doesn't really care about how the data are.",
            "Generated and."
        ],
        [
            "If you want example how this shows up, I have this image here.",
            "Someone took from the Amazon recommender system.",
            "So this is of a person who is buying this laptop rucksack and then Amazon recommends that the person should also buy a laptop to go with the rucksack.",
            "And it's it's kind of funny and everybody agrees that it's shouldn't do this.",
            "It feels wrong and.",
            "But maybe from a point of machine learning is not completely stupid, because maybe people looked at the data database.",
            "For Amazon, the shopping cards.",
            "What do people buy together?",
            "And probably if you look at the conditional probability of given that someone has bought rucksack, someone also bought a laptop.",
            "Maybe it's reasonably high, so maybe this should be a good recommendation from the point of view of machine learning, but still it feels wrong.",
            "And intuitively, we would probably all say that buying the laptop is causal for buying the rucksack, but not vice versa.",
            "And this is a real difference, I think intuitively we agree on this, and it's not.",
            "It's not the same as statistical dependence.",
            "It can't be because dependence is a symmetric concept.",
            "So dependence is not the same as causation, but there are links but."
        ],
        [
            "Independence and causation.",
            "And that's in a sense the starting point of field of causal inference from statistical data.",
            "Cancellation Bar was a philosopher, science and physicists.",
            "Who formulated this principle of common cause?",
            "Which says that if we see two observables that are statistically dependent, then there must be 1/3 observable that causally influences both of them.",
            "So if we see X&Y dependent, there must be a set that causes both of them.",
            "Of course that could coincide with X or Y, which case we get these special cases, but he says there's no cost, no statistical dependence without causality, and Moreover this third variable is such that screens X&Y from each other in the sense that.",
            "Conditioned on that, X&Y should become statistically independent, so let's.",
            "It's an assumption, or a principle that's links causality and statistics and."
        ],
        [
            "In a sense, this is at the core of what people do in causal inference, and it's not quite implies, but almost implies this framework of graphical models or causing graphical models.",
            "So graphical models as developed by pearlware, motivated by causality, actually, even though most people doing graphical models don't care about this nowadays, but in this graphical models one starting point and the one that I find most intuitive is you have a set of observables.",
            "You put them with a directed acyclic graph.",
            "Think of the observables as the vertices.",
            "And you say that the semantics of the graph is such that the parents, the graph theoretic parents, are direct causes.",
            "Each node computes a function of its parents and a noise variable.",
            "So this is a deterministic function, but the function can be different at each node.",
            "And we assume that all noise variables are jointly independent.",
            "Now if we do this on the tag will give us so that these noise variables are now the only element of randomness.",
            "They are random variables.",
            "The graph structure is a deck gives us a Canonical way of computing distributions of all nodes.",
            "We get an overall joint distribution on this graph.",
            "And it turns out this joint distribution has particular properties.",
            "In particular, has particular conditional independence structures, basically like the rating bar principle.",
            "What do I have a Prince implies?",
            "And the question now is, can we recover the graph from this joint distribution?",
            "Maybe from perfect knowledge of this distribution?",
            "And it turns out the answer is in general we can't, but under certain assumptions we can recover so-called Markov.",
            "Equivalence class.",
            "Of graphs, and these are basically all graphs that lead to the same conditional independence properties.",
            "So unfortunately we cannot in general assert that we can recover the one correct graph that we have used to generate the distribution and that will close would be the one that's the causal in some sense that causes description of this dependency structure.",
            "In particular, it doesn't work for the simplest case, if our graph has only two nodes.",
            "So if we only have cause and effect and we see there statistically dependent, we cannot infer what is cause and what is effect, and we have spent the last years working on this problem.",
            "This two variable problem and we've worked out two ideas how to do it in different cases and now briefly want to talk about those.",
            "Or maybe maybe just one of those.",
            "So I have until.",
            "Two OK.",
            "So."
        ],
        [
            "So the problem of inference of cause and effect is actually an old problem in philosophy.",
            "I was happy to find this in the this.",
            "No.",
            "Outlook of Nietzsche, who has written this book.",
            "Twilight of the idols.",
            "Apparently he wrote it within one week or two.",
            "Must have been some kind of writing frenzy, and in this book he has a chapter called The Four Great Errors in the first in this list of the four great errors is the error of mistaking costs for consequence.",
            "So he says there's no more dangerous error than mistaking the consequences for the cause.",
            "And.",
            "So let's see if we can do anything about this one."
        ],
        [
            "So I am.",
            "I have some harvest my mouse pointer.",
            "OK, so remember the two variable case so we have X&Y, remember that.",
            "Each of them has a noise variable in computer function of its parents and noise variable.",
            "Now it's one of the, in this case of two variables, one of them has to be the parent and has no other parents apart from the noise variable.",
            "We actually can identify these variable with its noise variable.",
            "So let's forget about this noise and just think of this one is a random quantity which is independent of this one here.",
            "So that's corresponds to the joint independence of the noises.",
            "Then we have some function that gives us the value of Y based on the value of X&N.",
            "So that's the general setting for two variables.",
            "And it is actually a very general setting, because imagine if this noise variable, for instance, is just a discrete random noise.",
            "It could for instance switch between a number of different mechanisms that corresponds to the number of different values that noise can take.",
            "So it's kind of surprising that one should have any hope of identifying such a system from data, because of course in reality the noise could be continuous.",
            "It could be switching between.",
            "An uncountable number of different mechanisms, unless we make some assumptions about the function, such as smoothness assumptions, etc.",
            "So this is normally not done in causality.",
            "That sort of our machine learning perspective.",
            "We want to do that and we do this by assuming that the noise is additive.",
            "So that's of course a strong restriction, and maybe you can motivate by saying, well, maybe it makes sense if the function is reasonably smooth or if the noise is reasonably concentrated, and then this is like a first order Taylor expansion.",
            "And the 1st result is that in this case."
        ],
        [
            "The system model becomes identifiable in the sense that if we assume that in the forward direction this model holds true.",
            "And then ask the question, is it also possible to write such a model with independent noise annoys independent of the input for the backward direction?",
            "Then the answer in the generic case is known and this is intuitively relatively easy to see.",
            "Look at this special case.",
            "He exposed the noise has bounded variance Sigma, so the noise is the same everywhere.",
            "It's independent of the input.",
            "Then if we now look at the backward direction then this variance will still be bounded.",
            "But if the function is nonlinear than the bound on the variance will now certainly depend on Y which is now the input.",
            "So intuitively it looks like it's not going to work and."
        ],
        [
            "When can also prove that one can prove that as if we assume such a model.",
            "So this is a. Factorizing model over noise, additive noise and inputs so they have to be independent.",
            "If we assume that such a model holds true in forward and backward direction, then we can derive a certain differential equation that connects the noise.",
            "Input density and the nonlinearity in such a way that there's only a 3 dimensional solution space, or in the generic case, it's very unlikely that there are matched to each other such that one can fit both forward and backward model, and there are some exceptions where we can do it, and these are exceptions to that makes sense in that unknown.",
            "So for instance, if everything is Gaussian in the function list Now, then we can find models in both directions."
        ],
        [
            "I'm going to skip this and so in practice what we can do is we compute the function regression.",
            "We compute the residuals and then we check whether these residuals and the input are statistically independent."
        ],
        [
            "Here's one example.",
            "It's a data set of different cities different at different altitudes, and this is the annual average average temperature.",
            "We believe that the altitude should have a causal effect on the temperature.",
            "The average temperature of a place, but not vice versa, at least not directly.",
            "And indeed."
        ],
        [
            "If we try and things in both directions in the forward direction, altitude to temperature, the residuals are quite independent.",
            "Whereas if we consider temperature the input, then their residuals quite structured an independent dependent."
        ],
        [
            "And actually, we can take this one step further and say, let's not compute regression using mean squared error.",
            "Let's actually do the regression such that it minimizes the independence of the residuals.",
            "M. So that gives us a slightly different method that we actually use.",
            "In this case, the repetition Hilbert space have distance between kernel mean embeddings of the joint distribution in the factorizing as a dependence measure.",
            "And this also works.",
            "And it's actually I think it's actually interesting regression method to say that we want the regressing to be such that the error terms don't contain information about the inputs.",
            "So in some sense we want to use up all the information in the inputs.",
            "There should be nothing left in the error terms.",
            "So that's all I wanted to say about this.",
            "Worth my first method.",
            "Now the second method."
        ],
        [
            "To solve 2 variable problem and it works with a different kind of assumption.",
            "In the second method, we assume that the input and the mechanism are independent, so by this I mean the following.",
            "So again, this is my little graph here.",
            "Identify the noise of the course with the course.",
            "Here's the effect and now I'm going to assume that the distribution of the course.",
            "And the conditional distribution of effect given costs.",
            "So that's like the mapping of the mechanism that produces the effect distribution from the course distribution and want this to be independent in some sense.",
            "And that's a little bit faster, and I'll show you one formalization."
        ],
        [
            "How to capture this independence?",
            "And it's actually for the deterministic case.",
            "So let's assume we have some input distribution and now we have a mechanism so deterministic case I mean the conditional is replaced just by a functional dependency.",
            "So I have some nonlinear mechanism that given the input distribution, produces this output distribution.",
            "If you look at this thing here, you notice that wherever this function was flat, the output distribution has or the output density as large mass.",
            "Let's clear dust by the way densities transform, which means that this nonlinear dependency leaves some trace on the output distribution.",
            "And we can write down some estimators that will have positive values for this pair and will be if they have zero and it will be 0 for this pair in the generic case and our assumption and the way we."
        ],
        [
            "Formalize this is we will, we will assert that in the forward direction independent and the independence of mechanism input the covariance of these two quantities viewed as random variables on this probability space.",
            "So let's say our probability space is zero to 1, so we do everything on the unit interval with respect to lebec measure.",
            "And then we consider the density.",
            "PX actually is a random variable.",
            "So it just takes a value.",
            "Everywhere on this interval.",
            "So it's a random variable, valid random variable, and likewise F prime, the logarithm of it is also a random variable.",
            "We can write down what we mean by covariance being zero, and if we postulate this, we can prove that in the backward direction the corresponding quantity for the inverse function and the output distribution will be at least zero.",
            "And actually it will be strictly positive as long as the function is not trivial.",
            "So for nonlinear function, this way probably actually strictly positive, so if it's if we postulate it's zero forward, it's actually non zero backwards.",
            "So it's an asymmetry that we can use for causal inference and."
        ],
        [
            "It's an asymmetry that can be interpreted in various ways, but I think I should not go into this right now."
        ],
        [
            "So we can think it's clear to you that we can also write down estimated that text for this asymmetry, and then we can run."
        ],
        [
            "These things on a number, of course."
        ],
        [
            "Effect pairs in evaluate different methods, and I don't want to discuss the details of this, only want to say that.",
            "And first of all, these methods have some.",
            "Typically compare some quantities, some some test statistics in forward, backward direction.",
            "We can set some cut off if these two values are very similar, we might not want to decide.",
            "So if we force the methods to always decide, then we over here, then the methods are some of them are already significantly better than chance.",
            "So this is our sort of chance area, but generally method should be able to do better if we don't force them to decide all the time.",
            "But the main message is that one can solve this kind of problem better than chance, which was surprising because at some point people thought these two variable case cannot be solved."
        ],
        [
            "So why is the two variable case important?",
            "So this is the last thing, and it turns out it has interesting implications for machine learning, and my motivation was always to connect causality to machine learning.",
            "So I was happy.",
            "When we found this link and the link is that, it turns out that in machine learning we sometimes learn causing problems and sometimes and he causes problems.",
            "An example of A cause of problem is this problem from bioinformatics where people use the M RNA sequence or DNA sequence to predict genes or maybe even proteins or certain properties of proteins.",
            "And in this case there are biological mechanisms that essentially implement this mapping in reality, but we don't understand these mechanisms.",
            "And we learn, we turn our machine learning assistant system to predict the outcome of this biological mechanism.",
            "So we are aligned with the true causal direction of nature.",
            "In this second example, down here, we're actually going in the other direction because these images were generated by someone who wanted to produce a digit three.",
            "Therefore, the images the effect in three, the class label is the cause.",
            "So we are actually learning something anti causal and."
        ],
        [
            "It turns out that if we make this assumption of mechanism of independence between input distribution and mechanism as I mentioned before, and that has certain implications and I want to highlight only the implication for semi supervised learning, if we learn in the causal direction then semi supervised learning means we get additional data of P of X of the input distribution.",
            "However, if our assumption which I have now written again informally, is true.",
            "Then the input distribution contains no information about the conditional, and learning always means estimating certain properties of the conditional.",
            "So for instance, in regression we want to estimate the conditional mean.",
            "So under our assumption.",
            "P of X contains no information about the conditional.",
            "Therefore, additional data of PFS shouldn't help us.",
            "Now in the other direction down here, remember before I was saying if we in the forward direction make this assumption of independence, then under certain conditions we can prove that in the backward direction this assumption is strictly violated.",
            "So in the backward direction the input distribution does contain information about the conditional and therefore applied to semi supervised learning.",
            "We can argue that in this case additional data, sorry down here, additional data does contain information about the conditional.",
            "And it should help."
        ],
        [
            "And actually all the known assumptions that people have come up with to motivate semi supervised learning are actually assumptions about the connection between P of X.",
            "So X is now the effect and the conditional of course given effect.",
            "So if maybe I don't go through detail through this in detail, but for instance the cluster assumption is precisely such an assumption about link between these two objects, so we."
        ],
        [
            "Try to we try this out in practice.",
            "Nice thing was we didn't have to run our own experiments, we just had to use other people's benchmarks and 1st label their datasets as causal in anti causal and then check if their results are."
        ],
        [
            "Assistant with what we?"
        ],
        [
            "And we did this."
        ],
        [
            "In several domains and I will only briefly sketch two of them.",
            "One is this large study using self training.",
            "One particular approach will be supervised learning this self for classification and here it turns out.",
            "So this is like the baselines, supervised learning.",
            "If we go up it means it helps.",
            "If we go down it gets worse and what you can see is as we predicted, for the cause of problems the red ones.",
            "Vice Lonely doesn't help for the anti causing problems.",
            "It sometimes helps but of course we don't guarantee that it helps.",
            "It will depend on the algorithm and the problem.",
            "So that's perfectly consistent and we also get a consistent pick."
        ],
        [
            "So for every regression this is using.",
            "Study of brefeldin all they were using Co regularization and it turns out that for the anti causal problems it does help by and large.",
            "So here the error.",
            "This is the error so lower is better and semi supervised is better everywhere.",
            "Where is for the datasets that we classify."
        ],
        [
            "It is causal.",
            "We didn't really get systematic improvements so.",
            "Then we have maybe."
        ],
        [
            "So we have recently started to try to understand from a physical point of view, how these structural equation models related to differential equation models, but I think I've already gone overtime so."
        ],
        [
            "I want to thank you for your attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I was little bit puzzled what I should be speaking about.",
                    "label": 0
                },
                {
                    "sent": "It took me awhile to figure it out.",
                    "label": 0
                },
                {
                    "sent": "That's why in the leaflet interpolators, probably No title.",
                    "label": 0
                },
                {
                    "sent": "I felt that I should be talking about kernels.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, nowadays spend most of my time thinking about causal inference.",
                    "label": 0
                },
                {
                    "sent": "So I came up with some kind of compromise.",
                    "label": 0
                },
                {
                    "sent": "Usually this compromise don't work, but let's see, maybe today is going to work and but I will start with kernels and maybe talk about kernels more than about the rest.",
                    "label": 0
                },
                {
                    "sent": "So let's get started with this.",
                    "label": 0
                },
                {
                    "sent": "So this is a this conference.",
                    "label": 0
                },
                {
                    "sent": "The Title Rocks 2013 already suggests that it might be the beginning of some kind of series, and it's such a beautiful place here.",
                    "label": 0
                },
                {
                    "sent": "So maybe the probability is high, so maybe we are witnessing historic event here, and it makes sense to also start with some other historical notes.",
                    "label": 0
                },
                {
                    "sent": "And that's what I'm going to try and to give you a little bit of.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Effective from my point of view about kernel methods, at least at the beginning.",
                    "label": 0
                },
                {
                    "sent": "So most of you probably know this kernel swear first used in mathematics in the theory of integral equations by Hilbert.",
                    "label": 1
                },
                {
                    "sent": "They were used in statistics probably first or maybe first in an extensive way by Agrace Barba and keyboard, or for her PhD advisor.",
                    "label": 0
                },
                {
                    "sent": "They were actually also mentioned in the 1st edition of through their Hearts book, but what kind of dismissed as some mathematical peculiarity?",
                    "label": 0
                },
                {
                    "sent": "That's not going to be very.",
                    "label": 0
                },
                {
                    "sent": "Practical, so the quotation is here.",
                    "label": 0
                },
                {
                    "sent": "They were used to approve the convergence of the potential function methods in Russia.",
                    "label": 1
                },
                {
                    "sent": "Actually, in the lab where Vladimir Vapnik would join a little bit later or maybe around that time as a PhD student.",
                    "label": 0
                },
                {
                    "sent": "So there were there was the lab of Eisaman and I think Bob Woman was in or where members of that lab and they had been studying theory of learning already for awhile.",
                    "label": 0
                },
                {
                    "sent": "Vape Nixon.",
                    "label": 0
                },
                {
                    "sent": "Terminix joined the lab of Llama, who I think at the time was not yet working learner, but then.",
                    "label": 0
                },
                {
                    "sent": "Unlearning, but then maybe he had to start working with that, given his surname, and he asked the Institute director atop it, sneak off for permission, and started working on learning with happening in German Yankees.",
                    "label": 0
                },
                {
                    "sent": "So here's a picture of a picnic.",
                    "label": 0
                },
                {
                    "sent": "It then.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Cook until the 90s before support vector machines the way we now know them were actually created and they were they went called support vector machine.",
                    "label": 0
                },
                {
                    "sent": "Initially they start with some kind of identity crisis.",
                    "label": 0
                },
                {
                    "sent": "At the beginning they were called optimal margin classifiers and then a little bit later after the inclusion of Slack variables they were first called soft margin classifiers.",
                    "label": 1
                },
                {
                    "sent": "In the submitted version and I was trying to I have a.",
                    "label": 0
                },
                {
                    "sent": "Copy of that, but I couldn't find it for this talk.",
                    "label": 1
                },
                {
                    "sent": "And then for the final version of this paper that appeared in machine learning, they were called support vector networks.",
                    "label": 0
                },
                {
                    "sent": "And this.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also, so this paper was also my first contact to SVM's, I joined Bell Labs as a peer St student at that time.",
                    "label": 0
                },
                {
                    "sent": "And I have to simply admit that my role in this paper was actually just to run a neural network, so I sometimes I styled renewal networks and you can see it down here.",
                    "label": 0
                },
                {
                    "sent": "I mentioned as the guy who ran the two layer neural network in this table here, and I think flooding there was quite pleased that my result was nowhere near to the SVM.",
                    "label": 0
                },
                {
                    "sent": "But I have to admit, I also wasn't a big neural network expert at the time, so I was a PhD student.",
                    "label": 0
                },
                {
                    "sent": "During this noon, but actually young Liqun also run this one.",
                    "label": 0
                },
                {
                    "sent": "He also didn't do as well as the SVM at the time.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My first paper, my my first paper that I wrote for myself about SVM's was this one extracting support data for a given task, and in this paper actually also in this paper is the name support vector machine.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Only appeared and we were well.",
                    "label": 0
                },
                {
                    "sent": "We made several points in that paper, but one of them was that.",
                    "label": 0
                },
                {
                    "sent": "We were getting very similar results using polynomial kernels, RBF kernels and sigmoid kernels.",
                    "label": 0
                },
                {
                    "sent": "These kernels were still fashionable at that time because the networks then looked like neural networks.",
                    "label": 0
                },
                {
                    "sent": "And we kind of argued that this means that it's not so important what structure would set of functions you choose, but it's more important.",
                    "label": 0
                },
                {
                    "sent": "What kind of capacity control?",
                    "label": 0
                },
                {
                    "sent": "Or it's more important that you do the right capacity control in that set of functions.",
                    "label": 0
                },
                {
                    "sent": "So maybe that's a slight contradiction to the recent developments that we're seeing nowadays.",
                    "label": 0
                },
                {
                    "sent": "People are getting great results with things like deep neural networks using SVM's only at the last lay on.",
                    "label": 0
                },
                {
                    "sent": "So in that sense, the structure is important, and.",
                    "label": 0
                },
                {
                    "sent": "And I believe John might have touched upon some of these things in his talk earlier on.",
                    "label": 0
                },
                {
                    "sent": "Anyway, so.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Games then moved on kernel.",
                    "label": 0
                },
                {
                    "sent": "PCA was developed.",
                    "label": 0
                },
                {
                    "sent": "You probably know about that, so it's a nonlinear generalization of principal component analysis.",
                    "label": 0
                },
                {
                    "sent": "Where, for instance, for this data set you get first 2 principal components.",
                    "label": 0
                },
                {
                    "sent": "Kind of cluster these groups into three clusters, and then the higher order components look for structure within the clusters.",
                    "label": 0
                },
                {
                    "sent": "And when we, once we had developed that we we notice of course, that it looks like this should be a good approach to clustering, and that I'm gonna veg who probably also many of you know, started his Masters thesis in Berlin and we gave him as a as a task to work out how to use kernel PCA for clustering and he tried all sorts of things and it worked quite well.",
                    "label": 0
                },
                {
                    "sent": "But we never really got our head around it and never understood why it worked well and.",
                    "label": 0
                },
                {
                    "sent": "This took a long time and only now it kind of makes sense to me because it's actually very similar to spectral clustering.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And to explain this is this is also kind of commonplace.",
                    "label": 0
                },
                {
                    "sent": "But then many people maybe don't know it, so I thought at this conference might be a good chance to mention this.",
                    "label": 0
                },
                {
                    "sent": "So spectral clustering just to remind you you have a similarity matrix.",
                    "label": 1
                },
                {
                    "sent": "Call it K so it's like our kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "Then there are several approaches, but let's focus mainly on this one.",
                    "label": 0
                },
                {
                    "sent": "Normalized cuts.",
                    "label": 1
                },
                {
                    "sent": "In this approach, we compute this matrix called the normalized Laplacian and then the prescription is.",
                    "label": 0
                },
                {
                    "sent": "So this is computed based on the training data.",
                    "label": 0
                },
                {
                    "sent": "Prescription is then to map the training points to the inputs.",
                    "label": 0
                },
                {
                    "sent": "So remember the training points to the corresponding entries of the second smallest eigenvector of this matrix.",
                    "label": 1
                },
                {
                    "sent": "It sounds like a funny prescription.",
                    "label": 0
                },
                {
                    "sent": "So remember this matrix, the side length is the number of training points, so there is one entry for each.",
                    "label": 0
                },
                {
                    "sent": "Input and therefore we can somehow map into this thing.",
                    "label": 0
                },
                {
                    "sent": "And then we can.",
                    "label": 0
                },
                {
                    "sent": "Once we have done this.",
                    "label": 0
                },
                {
                    "sent": "So this is now our our new representation of the data.",
                    "label": 0
                },
                {
                    "sent": "We can partition the data points based on these values, for instance by performing K means or something like that, and there's this slightly different version that I'm not going to discuss right now.",
                    "label": 0
                },
                {
                    "sent": "Now, remember, kernel PCA?",
                    "label": 0
                },
                {
                    "sent": "I didn't derive it, but probably you know it kinda.",
                    "label": 1
                },
                {
                    "sent": "PCA looks quite different, so in kernel PCA we have the eigenvectors of the kernel matrix with corresponding eigenvalues.",
                    "label": 1
                },
                {
                    "sent": "We get them by just diagonalizing the kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "Then the prescription is given a test point X.",
                    "label": 0
                },
                {
                    "sent": "So this could not be any point, not just an element of the training set.",
                    "label": 0
                },
                {
                    "sent": "Map this point to the reputation Hilbert space and then predict on tool to get the first feature project onto the largest eigenvector of K and then normalized by this quantity, which ensures that the eigenvector is normalized to length one in the reproducing kernel Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "So let's so it's just PCA.",
                    "label": 0
                },
                {
                    "sent": "Then in the feature space.",
                    "label": 0
                },
                {
                    "sent": "In here I have written it down.",
                    "label": 0
                },
                {
                    "sent": "The prediction and other protection band looks like this.",
                    "label": 0
                },
                {
                    "sent": "It's a kernel expansion which is also a consequence of the represent a theorem, and it looks rather different.",
                    "label": 0
                },
                {
                    "sent": "However, remember this is for a general point X in up here.",
                    "label": 0
                },
                {
                    "sent": "This is for the training points, and it turns out if we substitute a training point in here for kernel PCA, then this formula also simplifies.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this formula for a general point.",
                    "label": 0
                },
                {
                    "sent": "Sorry for if we take a training point.",
                    "label": 0
                },
                {
                    "sent": "Then actually.",
                    "label": 0
                },
                {
                    "sent": "We can exploit the fact that this is an eigenvector and we simplify this quantity and we actually also get the coefficient vector.",
                    "label": 0
                },
                {
                    "sent": "So the entries of the coefficient vector will also contain the representations the new representations off the training points.",
                    "label": 0
                },
                {
                    "sent": "So it's actually very similar and we can even make it even more similar if we want by looking at so now it's similar on the on the structural level, but the kernel matrix still looks a bit different, but we can make the kernel matrix more similar by choosing a particular type of kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "So for instance, for a connected graph it's known that the normalized Laplacian has a single zero eigenvalue and the pseudo inverse of this normalized Laplacian is also known as the discrete greens function of the diffusion process.",
                    "label": 1
                },
                {
                    "sent": "On the graph governed by L, so it will be like diffusivity and we can view this thing the pseudoinverse as a kernel matrix and namely at the kernel matrix that encodes the dot product that corresponds to the metric called the commute time.",
                    "label": 0
                },
                {
                    "sent": "So commute time in a diffusion processes.",
                    "label": 0
                },
                {
                    "sent": "How long does it take you to get from one point to the other one impaction that's symmetric, so for this special type of kernel, things are actually very much analogous.",
                    "label": 0
                },
                {
                    "sent": "And this is the second thing I couldn't mention here is that the kernel matrix kernel PCA matrix has to be sent us.",
                    "label": 0
                },
                {
                    "sent": "This implies it has a zero.",
                    "label": 0
                },
                {
                    "sent": "I can value a single eigenvalue zero in the case that the kernel is strictly positive definite, and this exactly corresponds to the zero eigenvalue of the normalized Laplacian, which we ignore if we take the second smallest eigenvector.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then of course, there's this tiny difference here.",
                    "label": 0
                },
                {
                    "sent": "Trivial difference here.",
                    "label": 0
                },
                {
                    "sent": "We take the second smallest.",
                    "label": 0
                },
                {
                    "sent": "Here we take the largest, but all these things can be.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Turn around by inverting the order of the eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So far I talked about SVM, so talked about other kernel libraries that you can generate by using the reproducing kernel as a dot product.",
                    "label": 0
                },
                {
                    "sent": "It was also soon noticed that the input domain doesn't have to be a vector space, and then finally the most important recent development maybe internal methods has been the theory of kernel mean embedding methods that I think that's still an active area of research and I want to spend 5 or 10 minutes on that.",
                    "label": 1
                },
                {
                    "sent": "And then maybe the most the best known examples of this are kernel ICA and maximum mean discrepancy.",
                    "label": 0
                },
                {
                    "sent": "So what is that about?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's imagine we have a set of points subset of our input domain.",
                    "label": 0
                },
                {
                    "sent": "And define this object here the mean map as.",
                    "label": 0
                },
                {
                    "sent": "Mapping all these points into the feature space.",
                    "label": 0
                },
                {
                    "sent": "So this is 1 representation of our mapping into the feature space.",
                    "label": 0
                },
                {
                    "sent": "I mean, I felt that this is a group of experts here.",
                    "label": 0
                },
                {
                    "sent": "I don't have to define that, but just recall one way to write the mapping induced by reproducing kernel is to say that each point XI gets mapped to a function that we get by substituting XI into one argument of the kernel, and then the repetition commercial space is the linear completion of such functions.",
                    "label": 0
                },
                {
                    "sent": "With the suitable dot product.",
                    "label": 0
                },
                {
                    "sent": "So if we now map a set of points into the representative of space and then compute the mean in that space, we get this object here.",
                    "label": 0
                },
                {
                    "sent": "And let's call this the main map.",
                    "label": 0
                },
                {
                    "sent": "Now we can ask the question, when is this map injective?",
                    "label": 0
                },
                {
                    "sent": "So in other words, suppose we have a certain second set of points Y, one through YM.",
                    "label": 0
                },
                {
                    "sent": "Actually, it could also go up to N, could be a different size set, but let's not worry about that.",
                    "label": 0
                },
                {
                    "sent": "From the same domain, and let's assume this second set of points is different from our initial one.",
                    "label": 0
                },
                {
                    "sent": "Is it nevertheless possible that they mapped to the same mean in the reproducing kernel Hilbert space?",
                    "label": 1
                },
                {
                    "sent": "So clearly if if we use a trivial arcade test, but we started with a vector space and we just map our points into the same space, so we're still in some finite dimensional vector space, then in general this map is not going to be injective, because you can imagine lots of different datasets that have the same mean.",
                    "label": 0
                },
                {
                    "sent": "Not surprisingly, it turns out in the repetition kernel Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "The main contains a lot more information.",
                    "label": 0
                },
                {
                    "sent": "And there's a large class of kernels for which this mapping is actually injective, so just mapping our data points in the professional space and then only remembering the mean is enough.",
                    "label": 0
                },
                {
                    "sent": "And in principle we can reconstruct the whole data set from that.",
                    "label": 0
                },
                {
                    "sent": "It is actually relatively easy to see in this is the case, at least for these mappings of point sets.",
                    "label": 0
                },
                {
                    "sent": "It becomes a bit more complicated later if we have measures instead.",
                    "label": 0
                },
                {
                    "sent": "But for point.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's so.",
                    "label": 0
                },
                {
                    "sent": "Let's imagine we have two sets that have the same mean or actually slightly more generally.",
                    "label": 0
                },
                {
                    "sent": "Let's imagine 2 minutes.",
                    "label": 0
                },
                {
                    "sent": "Just we have coefficients here.",
                    "label": 0
                },
                {
                    "sent": "1 / M like, but let's imagine we have more general coefficient values here.",
                    "label": 0
                },
                {
                    "sent": "So let's imagine we have two kernel expansions.",
                    "label": 0
                },
                {
                    "sent": "That are identical, and we also assume that none of the points are repeated on the left.",
                    "label": 0
                },
                {
                    "sent": "None of the points on the right are repeated.",
                    "label": 0
                },
                {
                    "sent": "If this identity holds true, then it's easy to prove.",
                    "label": 0
                },
                {
                    "sent": "That actually these two point sets have to agree point by point.",
                    "label": 0
                },
                {
                    "sent": "So this set of the axis in the set of the wise has to be identical.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go through it, but I just wanted to show it's very short proof.",
                    "label": 0
                },
                {
                    "sent": "So in this case things are injective, we don't lose.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Information by mapping into the reproducing kernel Hilbert space and so a little some more things that I wanted to say about this.",
                    "label": 0
                },
                {
                    "sent": "So this was our mapping again, since the kernel represents point evaluation of a function, so the DOT product between this kernel and the function gives us the function value at the point, so that's special about reproducing kernel Hilbert spaces.",
                    "label": 0
                },
                {
                    "sent": "That's what gives them.",
                    "label": 0
                },
                {
                    "sent": "Or that's crucial aspect functions are defined.",
                    "label": 0
                },
                {
                    "sent": "For every possible point value that equivalence classes of functions where they are really functions.",
                    "label": 0
                },
                {
                    "sent": "Due to the linearity of the DOT product, if we take this kernel mean and take a dot product with a function, we actually get the mean of the function evaluated on the sample.",
                    "label": 0
                },
                {
                    "sent": "So this element here represents the operation of taking the mean of a function on the sample.",
                    "label": 0
                },
                {
                    "sent": "Moreover, if we map two sets X&Y, then we can express.",
                    "label": 0
                },
                {
                    "sent": "And the norm of the difference first of all, by a by a coaching Schwartz.",
                    "label": 0
                },
                {
                    "sent": "Essentially as the soup over a dot products between the difference vector and functions from the unit ball.",
                    "label": 0
                },
                {
                    "sent": "Which then we can rewrite in terms of this thing here as the soup of this quantity here.",
                    "label": 0
                },
                {
                    "sent": "So that means 2 means in the future space are different only if we can find a function that distinguishes these two sets.",
                    "label": 1
                },
                {
                    "sent": "In that sense, in terms of having different means on these two sets.",
                    "label": 0
                },
                {
                    "sent": "And remember, if the kernel is strictly positive definite, this is equivalent to saying that the two sets are different, so we can always distinguish samples that are Dist.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can also look at what the function looks like that function that witness is a difference between two sets of points, and this is an example.",
                    "label": 0
                },
                {
                    "sent": "If we take a Gaussian kernel.",
                    "label": 0
                },
                {
                    "sent": "And we have data.",
                    "label": 0
                },
                {
                    "sent": "1 sample.",
                    "label": 0
                },
                {
                    "sent": "Let's say X is drawn from a Gaussian and why is drawn from a Laplacian with the same mean and 1st moment.",
                    "label": 0
                },
                {
                    "sent": "Then this function that distinguishes the Gaussian in the Laplacian sample looks like this.",
                    "label": 1
                },
                {
                    "sent": "It's a large in the middle, will apply and data is more likely.",
                    "label": 0
                },
                {
                    "sent": "It's a negative here on the site where the Gaussian is more likely.",
                    "label": 0
                },
                {
                    "sent": "And it is reasonably smooth because it's from the.",
                    "label": 0
                },
                {
                    "sent": "It's a unit length function.",
                    "label": 0
                },
                {
                    "sent": "So it's a unit link function in the reticle Hilbert space, which implies certain smoothness properties.",
                    "label": 0
                },
                {
                    "sent": "OK, so we can generalize generalize the same.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Two measures, and in this case we don't have sets of points, but we have Borel probability measures.",
                    "label": 1
                },
                {
                    "sent": "We have to assume that these expectations here exists, which automatically is guaranteed.",
                    "label": 0
                },
                {
                    "sent": "For instance, if available is bounded.",
                    "label": 0
                },
                {
                    "sent": "We then define the image of such a Borel measure.",
                    "label": 0
                },
                {
                    "sent": "Under the kernel mean map as the expectation of the feature space mapping.",
                    "label": 0
                },
                {
                    "sent": "This exists under these conditions.",
                    "label": 0
                },
                {
                    "sent": "In this element here.",
                    "label": 0
                },
                {
                    "sent": "So the main embedding of a measure represents the operation of taking an expectation of this function with respect to sampling.",
                    "label": 0
                },
                {
                    "sent": "The argument of the function from the Borel measure and just completely analogous to the last slide.",
                    "label": 0
                },
                {
                    "sent": "We can also write the difference between two embeddings of measures, as this soup over the unit ball of these means of functions.",
                    "label": 0
                },
                {
                    "sent": "On the two measures now we can ask the question again under which conditions is snapping you injective?",
                    "label": 1
                },
                {
                    "sent": "So when do we not lose information by doing this?",
                    "label": 0
                },
                {
                    "sent": "Because it's attempting thing to do we we have, we have a class of measures, we just represent them in representing kernel Hilbert space and then we can do linear algebra because we have now Hilbert space elements.",
                    "label": 0
                },
                {
                    "sent": "So that's nice, and it's particularly nice if we don't lose information.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is true for an interesting class of kernels.",
                    "label": 0
                },
                {
                    "sent": "And it's basically uses to some classical results of probability theory.",
                    "label": 0
                },
                {
                    "sent": "Due to faulty in movie.",
                    "label": 0
                },
                {
                    "sent": "Who showed that two Borel measures are identical if and only if?",
                    "label": 0
                },
                {
                    "sent": "The soup if and only if even nonlinear.",
                    "label": 0
                },
                {
                    "sent": "Let me put it this way if nonlinear transformations so clearly.",
                    "label": 0
                },
                {
                    "sent": "If you look at the expectations of example from P -- X samples from Q, then this will not be sufficient to distinguish two measures.",
                    "label": 0
                },
                {
                    "sent": "If you also allow for nonlinear transformations of your data by F before taking the expectations, and you require that no matter which nonlinear transformation you choose, these quantities are still different.",
                    "label": 0
                },
                {
                    "sent": "It turns out if the function class is large enough, then this is equivalent to saying that the two measures are identical.",
                    "label": 0
                },
                {
                    "sent": "So this is the same idea as the idea in kernel ICA and we have fun sis back here who has pioneered this direction and so this is a classical result and we can now for the kernel mean embedding the following way where we combine it with this formula that I showed in the last slide.",
                    "label": 0
                },
                {
                    "sent": "So this quantity here can be expressed as this soup which looks almost the same as here.",
                    "label": 0
                },
                {
                    "sent": "The only difference is here we have a unit ball in the representatives in kernel Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "Here have we have the function of all?",
                    "label": 0
                },
                {
                    "sent": "Continuous bounded functions on some domain.",
                    "label": 0
                },
                {
                    "sent": "Now we only have to argue that for certain kernels, our unit ball is dense in this set here, and that's the case for the classic kernels that has been called Universal by Stein Mart.",
                    "label": 0
                },
                {
                    "sent": "And then we're done, and we can assert that if the kernel is universal, then the kernel mean mapping view is injective, which means that if the output of the mapping is the same, inputs also have to have been the same.",
                    "label": 0
                },
                {
                    "sent": "So that's quite nice.",
                    "label": 0
                },
                {
                    "sent": "And it's also reassuring that this connects to a number of things in.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the statistics, we get some interesting special cases here.",
                    "label": 0
                },
                {
                    "sent": "If we use this kernel here in the exponential kernel, which is known as a universal kernel, we recover what's called the moment generating function of a random variable which is defined like this in our notation.",
                    "label": 1
                },
                {
                    "sent": "If we put the imaginary unit in here, we recover what's called the characteristic function.",
                    "label": 0
                },
                {
                    "sent": "OK, so if we have such an injective mapping, this provides us with a convenient metric on probability distributions and we can use that to do various things, for instance to check whether two distributions identical.",
                    "label": 1
                },
                {
                    "sent": "So that's the.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First application this so called 2 sample problem and I'm not going to go through details, but I think you get the idea.",
                    "label": 0
                },
                {
                    "sent": "So we want to check this is the case now we have only empirical data, so we write down empirical estimators for this and in the end we end up with something that looks like this and we can show it's an unbiased estimator of this true difference squared error in the producing Hilbert space, and it's easy to compute and one can also compute it.",
                    "label": 0
                },
                {
                    "sent": "Since kernel it's can be defined unstructured data.",
                    "label": 0
                },
                {
                    "sent": "When can computed on things like graphs or strings etc.",
                    "label": 0
                },
                {
                    "sent": "Also have been used also in feels like bioinformatics.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The second application, so this is that I mentioned already.",
                    "label": 0
                },
                {
                    "sent": "Fontsize back Colonel independence testing.",
                    "label": 0
                },
                {
                    "sent": "So how do we recover kernel independence testing?",
                    "label": 1
                },
                {
                    "sent": "Well, one way to recover it is to say that two random variables are independent if and only if the joint distribution and the product of the marginals are identical, which for universal kernel we can test by checking the difference between the embeddings of the joint distribution and the embeddings of the product of the marginals.",
                    "label": 0
                },
                {
                    "sent": "And if we do that, it turns out and we get a nice estimator which looks like this, and it's also linked to something called the cross covariance.",
                    "label": 0
                },
                {
                    "sent": "So we can this quantity here.",
                    "label": 0
                },
                {
                    "sent": "Actually will be the same as the as the Hilbert Schmidt norm of the so called cross covariance operator between the two.",
                    "label": 1
                },
                {
                    "sent": "Reproducing kernel Hilbert spaces.",
                    "label": 0
                },
                {
                    "sent": "And yeah, so I think this was a little bit the last slide where a bit of review material, but maybe not everybody knew it and.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now I want to show something that maybe most of you haven't seen yet.",
                    "label": 0
                },
                {
                    "sent": "I hope so at least, and this is a link to some problem of physics that I've been interested in awhile for awhile, and that's in found.",
                    "label": 0
                },
                {
                    "sent": "Hope for imaging and super resolution.",
                    "label": 0
                },
                {
                    "sent": "In this case we are considering kernel mean embeddings for stationary kernels, translation invariant kernels that take this form here, and I think probably most of you have heard of Fourier imaging.",
                    "label": 0
                },
                {
                    "sent": "In their undergraduate degree, and so in a nutshell, or maybe even at high school.",
                    "label": 0
                },
                {
                    "sent": "So if you have an image just as a distribution that tells us where, where do the photons come from.",
                    "label": 0
                },
                {
                    "sent": "Now suppose you only have this blue line, so you have a Delta measure here.",
                    "label": 0
                },
                {
                    "sent": "So your image in just a point.",
                    "label": 0
                },
                {
                    "sent": "If you do this through some finite aperture of size D, then you know that you get this interference pattern.",
                    "label": 0
                },
                {
                    "sent": "You get an intensity distribution, which can be computed to be this blue line here, which is the square of the Fourier transform.",
                    "label": 0
                },
                {
                    "sent": "Of the aperture function and if I have an image that's a mixture of two point sources, I get a superposition of two such.",
                    "label": 0
                },
                {
                    "sent": "So called sync functions sine X / X with some scaling factor and if these two points are very close then I won't be able to distinguish these two sync functions anymore.",
                    "label": 0
                },
                {
                    "sent": "And people say that in this case we cannot resolve these point sources from each other anymore.",
                    "label": 0
                },
                {
                    "sent": "That's the reason why now this this distance here scales inversely with the.",
                    "label": 0
                },
                {
                    "sent": "Sorry, it scales with D. Therefore people try to build large telescopes with large apertures so that we can actually separate these.",
                    "label": 0
                },
                {
                    "sent": "Images now in.",
                    "label": 0
                },
                {
                    "sent": "If we have a general image here, then what we observe over here is the.",
                    "label": 0
                },
                {
                    "sent": "Again, this square of the full year transform of the aperture.",
                    "label": 0
                },
                {
                    "sent": "So by I did note an indicator function of this aperture convolved with the distribution that models are image, so that's what we're going to record back here at the sensor.",
                    "label": 0
                },
                {
                    "sent": "And if we set K equal to the square root transform, so this is a this K up here, it's a stationary we can think of this as a stationary kernel.",
                    "label": 0
                },
                {
                    "sent": "It will be actually a positive definite kernel biobox theorem.",
                    "label": 0
                },
                {
                    "sent": "So bonus Theorem says that the transforms of non negative sign measures are positive definite trend kernels, roughly speaking.",
                    "label": 0
                },
                {
                    "sent": "Then we recover this image over here exactly as the kernel mean embedding.",
                    "label": 0
                },
                {
                    "sent": "Using this kernel function, so that's just the physical realization of a kernel mean embedding.",
                    "label": 0
                },
                {
                    "sent": "And so that's quite nice.",
                    "label": 0
                },
                {
                    "sent": "But unfortunately or maybe not surprisingly, this mapping file from you so the kernel mean embedded using this particular kernel is not invertible, because this particular kernel is not universally so in the sense that even reassuring, because I told you before, if these things get too close, we cannot resolve them anymore, so we cannot resolve very fine structure.",
                    "label": 0
                },
                {
                    "sent": "If the aperture has finite size, so that's well known in physics.",
                    "label": 0
                },
                {
                    "sent": "And it's every photo as the diffraction limit.",
                    "label": 0
                },
                {
                    "sent": "Now it turns out and I want to show that during the next few slides, if we restrict the input domain to distributions that have compact support, these nothing is actually invertible and no matter how small this aperture is and this is some, it's a direct consequence of something that we notice in the context of studying universality or injectivity of kernel mean embeddings as a pure machine learning problem.",
                    "label": 1
                },
                {
                    "sent": "But it turns out it applies in this domain and natural.",
                    "label": 0
                },
                {
                    "sent": "Say it turns out that essentially what we're using in our proof has also been noticed by some people in this domain already independently.",
                    "label": 0
                },
                {
                    "sent": "So there's a certain convergence of these two problems in the same kind of mathematics gets used on both sides.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So before I get into this just a short digression.",
                    "label": 0
                },
                {
                    "sent": "So I quite like this this physical realization of the kernel here.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can think of it as the pointers, so the kernel in this case this describes the point respond response of a linear optical system.",
                    "label": 0
                },
                {
                    "sent": "So this convolution is a linear process.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So it's a it's a Green's function in more generally we can also in other domains analyze kernels as greens functions, and maybe I should mention this here because this conference is about regularization.",
                    "label": 0
                },
                {
                    "sent": "This is probably well known for many of you.",
                    "label": 0
                },
                {
                    "sent": "If we can write the arcade S norm as the norm of the result of applying some regularization operator to the function to be regularised.",
                    "label": 0
                },
                {
                    "sent": "Then the kernel can be thought of as the greens function of P star P, where P is this regularization operator and for instance, people have analyzed the Gaussian kernel.",
                    "label": 1
                },
                {
                    "sent": "This paper by Hulanicki Raj where they show that the Gaussian kernel corresponds to regularization operator, which computes an infinite series of derivatives of F. And more generally, it translation invariant kernels.",
                    "label": 1
                },
                {
                    "sent": "We can weaken oversight P as a multiplication operator in Fourier space in the spectrum.",
                    "label": 0
                },
                {
                    "sent": "That tells us how different parts of the frequencies in the functions are amplified or attenuated.",
                    "label": 0
                },
                {
                    "sent": "Anyway, let's go back to the Fourier imaging.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if we assume that so I'm being a little bit sloppy here.",
                    "label": 0
                },
                {
                    "sent": "If you want to know precisely, you have to look in the papers.",
                    "label": 0
                },
                {
                    "sent": "We assume the densities exists.",
                    "label": 0
                },
                {
                    "sent": "The kernel is shift invariant.",
                    "label": 1
                },
                {
                    "sent": "Paul for your transform exists.",
                    "label": 0
                },
                {
                    "sent": "Then we can derive this condition here.",
                    "label": 0
                },
                {
                    "sent": "Which will imply that P is equal to Q, and we can then so the head denotes Fourier transform.",
                    "label": 1
                },
                {
                    "sent": "So it depends on the Fourier transform of K and three transforms of the distributions.",
                    "label": 0
                },
                {
                    "sent": "So it depends on whether K has is non zero in the part where we have frequencies that we want to distinguish etc.",
                    "label": 0
                },
                {
                    "sent": "From this one for instance, see that if K had if the spectrum of K. Has full support.",
                    "label": 1
                },
                {
                    "sent": "We find because then we can just divide by this.",
                    "label": 0
                },
                {
                    "sent": "So so sorry maybe I was a bit too fast here so I should say that mu will be invertible exactly if this implication holds true, right?",
                    "label": 0
                },
                {
                    "sent": "If whenever the images under of the kernel map are equal, the distribution should be equal.",
                    "label": 0
                },
                {
                    "sent": "So I'm not just changing these a little bit, I want this implication to hold true.",
                    "label": 1
                },
                {
                    "sent": "Clearly if K head has full support, I could divide by it and this becomes trivial or almost trivial and.",
                    "label": 0
                },
                {
                    "sent": "But K hat does not have full support for our particular type of kernel because the aperture is finite, so it's just it's going to be the convolution of the aperture with itself.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's bad news.",
                    "label": 0
                },
                {
                    "sent": "However, we can fix this by restricting the class of distributions in particularly the way we have to restrict it is we have to force them to have compact support.",
                    "label": 1
                },
                {
                    "sent": "So now all our input our probability measures that appear as images on the left hand side have compact support.",
                    "label": 0
                },
                {
                    "sent": "We can apply a famous theorem from complex analysis.",
                    "label": 0
                },
                {
                    "sent": "So Paley, Wiener theorem and in this case due to this theorem it turns out that it's sufficient.",
                    "label": 0
                },
                {
                    "sent": "To know the distributions characteristic function on a compact subset.",
                    "label": 1
                },
                {
                    "sent": "So intuitively, I guess you can believe me that you will believe me that even though in this case, even though we cut out a certain part of the spectrum innocence, this paling return allows us to extend this and it's enough to have agreement on this spectrum.",
                    "label": 1
                },
                {
                    "sent": "Therefore we can invert things.",
                    "label": 0
                },
                {
                    "sent": "And for this to work, the Fourier transform key only needs to have a support with nonempty interior, and that's for interest.",
                    "label": 0
                },
                {
                    "sent": "The case for our particular K as soon as the aperture has finite size.",
                    "label": 0
                },
                {
                    "sent": "So whenever it's non zero, this will be the case.",
                    "label": 0
                },
                {
                    "sent": "And this we already knew for awhile, but not from the not in the optical case.",
                    "label": 0
                },
                {
                    "sent": "So it turns out applied to this problem with Hanover diffraction.",
                    "label": 0
                },
                {
                    "sent": "The imaging process is not invertible in the general case, but it does become invertible if we restrict the class of distributions to compact support.",
                    "label": 1
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we recently also.",
                    "label": 0
                },
                {
                    "sent": "Sort of tried to try this out or we tried it out on some very simple data.",
                    "label": 0
                },
                {
                    "sent": "We cooked up a simple algorithmic method that exploits the.",
                    "label": 1
                },
                {
                    "sent": "The bounded support the fact that the image has bounded support, so we actually have to of course generate images with bounded support and we also it turns out it also helps to exploit the non negativity of the image.",
                    "label": 1
                },
                {
                    "sent": "So if this squared intensity image and so in this application here we have.",
                    "label": 0
                },
                {
                    "sent": "We have a double star which is imaged through some optical system where the aperture is set relatively small such that so the true doubles.",
                    "label": 0
                },
                {
                    "sent": "Well maybe let me start with this one so we have a double synthetic double slow just like an Eli D with a mask with two holes and if we image this double star with some optics with a relatively wide aperture.",
                    "label": 0
                },
                {
                    "sent": "We get this image here down here.",
                    "label": 0
                },
                {
                    "sent": "If we turn down the aperture, these things become blurry and in this case we have turned it down so much that the the separation between the two stars is 0.5 times the so called Rayleigh limits rate.",
                    "label": 0
                },
                {
                    "sent": "The rate limit is typically used to characterize the optical resolution of such a system, but we can then run our deconvolution methods on this input.",
                    "label": 0
                },
                {
                    "sent": "And recover this reconstruction using these constraints up here.",
                    "label": 0
                },
                {
                    "sent": "You can see that the reconstruction is quite close to this image that we took over here.",
                    "label": 0
                },
                {
                    "sent": "It's even a little bit sharper, which is some sense, not surprising, because of course.",
                    "label": 0
                },
                {
                    "sent": "Also this image was taken with a finite aperture, so it's just as big as it got, but not large enough to make these really pointlike, so that was.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Kind of reinsure reassuring, so I want to get to the end of the kernel stuff.",
                    "label": 0
                },
                {
                    "sent": "But just mentioning so I'm going to move to causality, but we're still interested in kernel machines.",
                    "label": 0
                },
                {
                    "sent": "We have spent some time.",
                    "label": 0
                },
                {
                    "sent": "This developing an approach for SVM's that works with distributions rather than training points.",
                    "label": 0
                },
                {
                    "sent": "This is also related to kernel mean embedding, and I think it is quite interesting and we have also recently started working on.",
                    "label": 0
                },
                {
                    "sent": "And what statisticians know is the James Stein effect in the context of kernel machines.",
                    "label": 1
                },
                {
                    "sent": "In particular, kernel mean estimation.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So machine learning I think so maybe this is now part of the motivation where where I partly moved to a different field has certain shortcomings and I think they all have to do with the difference between machine learning or statistical information in causal information.",
                    "label": 0
                },
                {
                    "sent": "And there's different ways of saying it.",
                    "label": 0
                },
                {
                    "sent": "One is machine learning is based on statistical information, when it's it's limited to ID data.",
                    "label": 1
                },
                {
                    "sent": "And finally it's a black box approach and it doesn't really care about how the data are.",
                    "label": 0
                },
                {
                    "sent": "Generated and.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you want example how this shows up, I have this image here.",
                    "label": 0
                },
                {
                    "sent": "Someone took from the Amazon recommender system.",
                    "label": 0
                },
                {
                    "sent": "So this is of a person who is buying this laptop rucksack and then Amazon recommends that the person should also buy a laptop to go with the rucksack.",
                    "label": 0
                },
                {
                    "sent": "And it's it's kind of funny and everybody agrees that it's shouldn't do this.",
                    "label": 0
                },
                {
                    "sent": "It feels wrong and.",
                    "label": 0
                },
                {
                    "sent": "But maybe from a point of machine learning is not completely stupid, because maybe people looked at the data database.",
                    "label": 0
                },
                {
                    "sent": "For Amazon, the shopping cards.",
                    "label": 0
                },
                {
                    "sent": "What do people buy together?",
                    "label": 0
                },
                {
                    "sent": "And probably if you look at the conditional probability of given that someone has bought rucksack, someone also bought a laptop.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's reasonably high, so maybe this should be a good recommendation from the point of view of machine learning, but still it feels wrong.",
                    "label": 0
                },
                {
                    "sent": "And intuitively, we would probably all say that buying the laptop is causal for buying the rucksack, but not vice versa.",
                    "label": 0
                },
                {
                    "sent": "And this is a real difference, I think intuitively we agree on this, and it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not the same as statistical dependence.",
                    "label": 0
                },
                {
                    "sent": "It can't be because dependence is a symmetric concept.",
                    "label": 0
                },
                {
                    "sent": "So dependence is not the same as causation, but there are links but.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Independence and causation.",
                    "label": 0
                },
                {
                    "sent": "And that's in a sense the starting point of field of causal inference from statistical data.",
                    "label": 0
                },
                {
                    "sent": "Cancellation Bar was a philosopher, science and physicists.",
                    "label": 0
                },
                {
                    "sent": "Who formulated this principle of common cause?",
                    "label": 1
                },
                {
                    "sent": "Which says that if we see two observables that are statistically dependent, then there must be 1/3 observable that causally influences both of them.",
                    "label": 1
                },
                {
                    "sent": "So if we see X&Y dependent, there must be a set that causes both of them.",
                    "label": 0
                },
                {
                    "sent": "Of course that could coincide with X or Y, which case we get these special cases, but he says there's no cost, no statistical dependence without causality, and Moreover this third variable is such that screens X&Y from each other in the sense that.",
                    "label": 0
                },
                {
                    "sent": "Conditioned on that, X&Y should become statistically independent, so let's.",
                    "label": 1
                },
                {
                    "sent": "It's an assumption, or a principle that's links causality and statistics and.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In a sense, this is at the core of what people do in causal inference, and it's not quite implies, but almost implies this framework of graphical models or causing graphical models.",
                    "label": 0
                },
                {
                    "sent": "So graphical models as developed by pearlware, motivated by causality, actually, even though most people doing graphical models don't care about this nowadays, but in this graphical models one starting point and the one that I find most intuitive is you have a set of observables.",
                    "label": 0
                },
                {
                    "sent": "You put them with a directed acyclic graph.",
                    "label": 1
                },
                {
                    "sent": "Think of the observables as the vertices.",
                    "label": 0
                },
                {
                    "sent": "And you say that the semantics of the graph is such that the parents, the graph theoretic parents, are direct causes.",
                    "label": 0
                },
                {
                    "sent": "Each node computes a function of its parents and a noise variable.",
                    "label": 0
                },
                {
                    "sent": "So this is a deterministic function, but the function can be different at each node.",
                    "label": 1
                },
                {
                    "sent": "And we assume that all noise variables are jointly independent.",
                    "label": 0
                },
                {
                    "sent": "Now if we do this on the tag will give us so that these noise variables are now the only element of randomness.",
                    "label": 0
                },
                {
                    "sent": "They are random variables.",
                    "label": 0
                },
                {
                    "sent": "The graph structure is a deck gives us a Canonical way of computing distributions of all nodes.",
                    "label": 0
                },
                {
                    "sent": "We get an overall joint distribution on this graph.",
                    "label": 0
                },
                {
                    "sent": "And it turns out this joint distribution has particular properties.",
                    "label": 1
                },
                {
                    "sent": "In particular, has particular conditional independence structures, basically like the rating bar principle.",
                    "label": 1
                },
                {
                    "sent": "What do I have a Prince implies?",
                    "label": 0
                },
                {
                    "sent": "And the question now is, can we recover the graph from this joint distribution?",
                    "label": 0
                },
                {
                    "sent": "Maybe from perfect knowledge of this distribution?",
                    "label": 0
                },
                {
                    "sent": "And it turns out the answer is in general we can't, but under certain assumptions we can recover so-called Markov.",
                    "label": 1
                },
                {
                    "sent": "Equivalence class.",
                    "label": 0
                },
                {
                    "sent": "Of graphs, and these are basically all graphs that lead to the same conditional independence properties.",
                    "label": 0
                },
                {
                    "sent": "So unfortunately we cannot in general assert that we can recover the one correct graph that we have used to generate the distribution and that will close would be the one that's the causal in some sense that causes description of this dependency structure.",
                    "label": 0
                },
                {
                    "sent": "In particular, it doesn't work for the simplest case, if our graph has only two nodes.",
                    "label": 0
                },
                {
                    "sent": "So if we only have cause and effect and we see there statistically dependent, we cannot infer what is cause and what is effect, and we have spent the last years working on this problem.",
                    "label": 0
                },
                {
                    "sent": "This two variable problem and we've worked out two ideas how to do it in different cases and now briefly want to talk about those.",
                    "label": 0
                },
                {
                    "sent": "Or maybe maybe just one of those.",
                    "label": 0
                },
                {
                    "sent": "So I have until.",
                    "label": 0
                },
                {
                    "sent": "Two OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the problem of inference of cause and effect is actually an old problem in philosophy.",
                    "label": 0
                },
                {
                    "sent": "I was happy to find this in the this.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Outlook of Nietzsche, who has written this book.",
                    "label": 0
                },
                {
                    "sent": "Twilight of the idols.",
                    "label": 0
                },
                {
                    "sent": "Apparently he wrote it within one week or two.",
                    "label": 0
                },
                {
                    "sent": "Must have been some kind of writing frenzy, and in this book he has a chapter called The Four Great Errors in the first in this list of the four great errors is the error of mistaking costs for consequence.",
                    "label": 0
                },
                {
                    "sent": "So he says there's no more dangerous error than mistaking the consequences for the cause.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So let's see if we can do anything about this one.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I am.",
                    "label": 0
                },
                {
                    "sent": "I have some harvest my mouse pointer.",
                    "label": 0
                },
                {
                    "sent": "OK, so remember the two variable case so we have X&Y, remember that.",
                    "label": 0
                },
                {
                    "sent": "Each of them has a noise variable in computer function of its parents and noise variable.",
                    "label": 0
                },
                {
                    "sent": "Now it's one of the, in this case of two variables, one of them has to be the parent and has no other parents apart from the noise variable.",
                    "label": 0
                },
                {
                    "sent": "We actually can identify these variable with its noise variable.",
                    "label": 0
                },
                {
                    "sent": "So let's forget about this noise and just think of this one is a random quantity which is independent of this one here.",
                    "label": 0
                },
                {
                    "sent": "So that's corresponds to the joint independence of the noises.",
                    "label": 0
                },
                {
                    "sent": "Then we have some function that gives us the value of Y based on the value of X&N.",
                    "label": 0
                },
                {
                    "sent": "So that's the general setting for two variables.",
                    "label": 0
                },
                {
                    "sent": "And it is actually a very general setting, because imagine if this noise variable, for instance, is just a discrete random noise.",
                    "label": 0
                },
                {
                    "sent": "It could for instance switch between a number of different mechanisms that corresponds to the number of different values that noise can take.",
                    "label": 1
                },
                {
                    "sent": "So it's kind of surprising that one should have any hope of identifying such a system from data, because of course in reality the noise could be continuous.",
                    "label": 0
                },
                {
                    "sent": "It could be switching between.",
                    "label": 0
                },
                {
                    "sent": "An uncountable number of different mechanisms, unless we make some assumptions about the function, such as smoothness assumptions, etc.",
                    "label": 0
                },
                {
                    "sent": "So this is normally not done in causality.",
                    "label": 0
                },
                {
                    "sent": "That sort of our machine learning perspective.",
                    "label": 0
                },
                {
                    "sent": "We want to do that and we do this by assuming that the noise is additive.",
                    "label": 0
                },
                {
                    "sent": "So that's of course a strong restriction, and maybe you can motivate by saying, well, maybe it makes sense if the function is reasonably smooth or if the noise is reasonably concentrated, and then this is like a first order Taylor expansion.",
                    "label": 0
                },
                {
                    "sent": "And the 1st result is that in this case.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The system model becomes identifiable in the sense that if we assume that in the forward direction this model holds true.",
                    "label": 0
                },
                {
                    "sent": "And then ask the question, is it also possible to write such a model with independent noise annoys independent of the input for the backward direction?",
                    "label": 0
                },
                {
                    "sent": "Then the answer in the generic case is known and this is intuitively relatively easy to see.",
                    "label": 0
                },
                {
                    "sent": "Look at this special case.",
                    "label": 0
                },
                {
                    "sent": "He exposed the noise has bounded variance Sigma, so the noise is the same everywhere.",
                    "label": 1
                },
                {
                    "sent": "It's independent of the input.",
                    "label": 1
                },
                {
                    "sent": "Then if we now look at the backward direction then this variance will still be bounded.",
                    "label": 0
                },
                {
                    "sent": "But if the function is nonlinear than the bound on the variance will now certainly depend on Y which is now the input.",
                    "label": 0
                },
                {
                    "sent": "So intuitively it looks like it's not going to work and.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When can also prove that one can prove that as if we assume such a model.",
                    "label": 0
                },
                {
                    "sent": "So this is a. Factorizing model over noise, additive noise and inputs so they have to be independent.",
                    "label": 0
                },
                {
                    "sent": "If we assume that such a model holds true in forward and backward direction, then we can derive a certain differential equation that connects the noise.",
                    "label": 0
                },
                {
                    "sent": "Input density and the nonlinearity in such a way that there's only a 3 dimensional solution space, or in the generic case, it's very unlikely that there are matched to each other such that one can fit both forward and backward model, and there are some exceptions where we can do it, and these are exceptions to that makes sense in that unknown.",
                    "label": 0
                },
                {
                    "sent": "So for instance, if everything is Gaussian in the function list Now, then we can find models in both directions.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to skip this and so in practice what we can do is we compute the function regression.",
                    "label": 0
                },
                {
                    "sent": "We compute the residuals and then we check whether these residuals and the input are statistically independent.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's one example.",
                    "label": 0
                },
                {
                    "sent": "It's a data set of different cities different at different altitudes, and this is the annual average average temperature.",
                    "label": 0
                },
                {
                    "sent": "We believe that the altitude should have a causal effect on the temperature.",
                    "label": 0
                },
                {
                    "sent": "The average temperature of a place, but not vice versa, at least not directly.",
                    "label": 1
                },
                {
                    "sent": "And indeed.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we try and things in both directions in the forward direction, altitude to temperature, the residuals are quite independent.",
                    "label": 0
                },
                {
                    "sent": "Whereas if we consider temperature the input, then their residuals quite structured an independent dependent.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And actually, we can take this one step further and say, let's not compute regression using mean squared error.",
                    "label": 0
                },
                {
                    "sent": "Let's actually do the regression such that it minimizes the independence of the residuals.",
                    "label": 1
                },
                {
                    "sent": "M. So that gives us a slightly different method that we actually use.",
                    "label": 0
                },
                {
                    "sent": "In this case, the repetition Hilbert space have distance between kernel mean embeddings of the joint distribution in the factorizing as a dependence measure.",
                    "label": 1
                },
                {
                    "sent": "And this also works.",
                    "label": 0
                },
                {
                    "sent": "And it's actually I think it's actually interesting regression method to say that we want the regressing to be such that the error terms don't contain information about the inputs.",
                    "label": 0
                },
                {
                    "sent": "So in some sense we want to use up all the information in the inputs.",
                    "label": 0
                },
                {
                    "sent": "There should be nothing left in the error terms.",
                    "label": 0
                },
                {
                    "sent": "So that's all I wanted to say about this.",
                    "label": 0
                },
                {
                    "sent": "Worth my first method.",
                    "label": 0
                },
                {
                    "sent": "Now the second method.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To solve 2 variable problem and it works with a different kind of assumption.",
                    "label": 0
                },
                {
                    "sent": "In the second method, we assume that the input and the mechanism are independent, so by this I mean the following.",
                    "label": 1
                },
                {
                    "sent": "So again, this is my little graph here.",
                    "label": 0
                },
                {
                    "sent": "Identify the noise of the course with the course.",
                    "label": 0
                },
                {
                    "sent": "Here's the effect and now I'm going to assume that the distribution of the course.",
                    "label": 0
                },
                {
                    "sent": "And the conditional distribution of effect given costs.",
                    "label": 0
                },
                {
                    "sent": "So that's like the mapping of the mechanism that produces the effect distribution from the course distribution and want this to be independent in some sense.",
                    "label": 0
                },
                {
                    "sent": "And that's a little bit faster, and I'll show you one formalization.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How to capture this independence?",
                    "label": 0
                },
                {
                    "sent": "And it's actually for the deterministic case.",
                    "label": 0
                },
                {
                    "sent": "So let's assume we have some input distribution and now we have a mechanism so deterministic case I mean the conditional is replaced just by a functional dependency.",
                    "label": 0
                },
                {
                    "sent": "So I have some nonlinear mechanism that given the input distribution, produces this output distribution.",
                    "label": 0
                },
                {
                    "sent": "If you look at this thing here, you notice that wherever this function was flat, the output distribution has or the output density as large mass.",
                    "label": 0
                },
                {
                    "sent": "Let's clear dust by the way densities transform, which means that this nonlinear dependency leaves some trace on the output distribution.",
                    "label": 0
                },
                {
                    "sent": "And we can write down some estimators that will have positive values for this pair and will be if they have zero and it will be 0 for this pair in the generic case and our assumption and the way we.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Formalize this is we will, we will assert that in the forward direction independent and the independence of mechanism input the covariance of these two quantities viewed as random variables on this probability space.",
                    "label": 1
                },
                {
                    "sent": "So let's say our probability space is zero to 1, so we do everything on the unit interval with respect to lebec measure.",
                    "label": 0
                },
                {
                    "sent": "And then we consider the density.",
                    "label": 1
                },
                {
                    "sent": "PX actually is a random variable.",
                    "label": 0
                },
                {
                    "sent": "So it just takes a value.",
                    "label": 0
                },
                {
                    "sent": "Everywhere on this interval.",
                    "label": 0
                },
                {
                    "sent": "So it's a random variable, valid random variable, and likewise F prime, the logarithm of it is also a random variable.",
                    "label": 0
                },
                {
                    "sent": "We can write down what we mean by covariance being zero, and if we postulate this, we can prove that in the backward direction the corresponding quantity for the inverse function and the output distribution will be at least zero.",
                    "label": 0
                },
                {
                    "sent": "And actually it will be strictly positive as long as the function is not trivial.",
                    "label": 0
                },
                {
                    "sent": "So for nonlinear function, this way probably actually strictly positive, so if it's if we postulate it's zero forward, it's actually non zero backwards.",
                    "label": 0
                },
                {
                    "sent": "So it's an asymmetry that we can use for causal inference and.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's an asymmetry that can be interpreted in various ways, but I think I should not go into this right now.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can think it's clear to you that we can also write down estimated that text for this asymmetry, and then we can run.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These things on a number, of course.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Effect pairs in evaluate different methods, and I don't want to discuss the details of this, only want to say that.",
                    "label": 0
                },
                {
                    "sent": "And first of all, these methods have some.",
                    "label": 0
                },
                {
                    "sent": "Typically compare some quantities, some some test statistics in forward, backward direction.",
                    "label": 0
                },
                {
                    "sent": "We can set some cut off if these two values are very similar, we might not want to decide.",
                    "label": 0
                },
                {
                    "sent": "So if we force the methods to always decide, then we over here, then the methods are some of them are already significantly better than chance.",
                    "label": 0
                },
                {
                    "sent": "So this is our sort of chance area, but generally method should be able to do better if we don't force them to decide all the time.",
                    "label": 0
                },
                {
                    "sent": "But the main message is that one can solve this kind of problem better than chance, which was surprising because at some point people thought these two variable case cannot be solved.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So why is the two variable case important?",
                    "label": 0
                },
                {
                    "sent": "So this is the last thing, and it turns out it has interesting implications for machine learning, and my motivation was always to connect causality to machine learning.",
                    "label": 0
                },
                {
                    "sent": "So I was happy.",
                    "label": 0
                },
                {
                    "sent": "When we found this link and the link is that, it turns out that in machine learning we sometimes learn causing problems and sometimes and he causes problems.",
                    "label": 0
                },
                {
                    "sent": "An example of A cause of problem is this problem from bioinformatics where people use the M RNA sequence or DNA sequence to predict genes or maybe even proteins or certain properties of proteins.",
                    "label": 0
                },
                {
                    "sent": "And in this case there are biological mechanisms that essentially implement this mapping in reality, but we don't understand these mechanisms.",
                    "label": 0
                },
                {
                    "sent": "And we learn, we turn our machine learning assistant system to predict the outcome of this biological mechanism.",
                    "label": 0
                },
                {
                    "sent": "So we are aligned with the true causal direction of nature.",
                    "label": 0
                },
                {
                    "sent": "In this second example, down here, we're actually going in the other direction because these images were generated by someone who wanted to produce a digit three.",
                    "label": 0
                },
                {
                    "sent": "Therefore, the images the effect in three, the class label is the cause.",
                    "label": 0
                },
                {
                    "sent": "So we are actually learning something anti causal and.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It turns out that if we make this assumption of mechanism of independence between input distribution and mechanism as I mentioned before, and that has certain implications and I want to highlight only the implication for semi supervised learning, if we learn in the causal direction then semi supervised learning means we get additional data of P of X of the input distribution.",
                    "label": 0
                },
                {
                    "sent": "However, if our assumption which I have now written again informally, is true.",
                    "label": 0
                },
                {
                    "sent": "Then the input distribution contains no information about the conditional, and learning always means estimating certain properties of the conditional.",
                    "label": 1
                },
                {
                    "sent": "So for instance, in regression we want to estimate the conditional mean.",
                    "label": 0
                },
                {
                    "sent": "So under our assumption.",
                    "label": 1
                },
                {
                    "sent": "P of X contains no information about the conditional.",
                    "label": 0
                },
                {
                    "sent": "Therefore, additional data of PFS shouldn't help us.",
                    "label": 0
                },
                {
                    "sent": "Now in the other direction down here, remember before I was saying if we in the forward direction make this assumption of independence, then under certain conditions we can prove that in the backward direction this assumption is strictly violated.",
                    "label": 0
                },
                {
                    "sent": "So in the backward direction the input distribution does contain information about the conditional and therefore applied to semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "We can argue that in this case additional data, sorry down here, additional data does contain information about the conditional.",
                    "label": 0
                },
                {
                    "sent": "And it should help.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And actually all the known assumptions that people have come up with to motivate semi supervised learning are actually assumptions about the connection between P of X.",
                    "label": 0
                },
                {
                    "sent": "So X is now the effect and the conditional of course given effect.",
                    "label": 0
                },
                {
                    "sent": "So if maybe I don't go through detail through this in detail, but for instance the cluster assumption is precisely such an assumption about link between these two objects, so we.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Try to we try this out in practice.",
                    "label": 0
                },
                {
                    "sent": "Nice thing was we didn't have to run our own experiments, we just had to use other people's benchmarks and 1st label their datasets as causal in anti causal and then check if their results are.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Assistant with what we?",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we did this.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In several domains and I will only briefly sketch two of them.",
                    "label": 0
                },
                {
                    "sent": "One is this large study using self training.",
                    "label": 0
                },
                {
                    "sent": "One particular approach will be supervised learning this self for classification and here it turns out.",
                    "label": 0
                },
                {
                    "sent": "So this is like the baselines, supervised learning.",
                    "label": 0
                },
                {
                    "sent": "If we go up it means it helps.",
                    "label": 0
                },
                {
                    "sent": "If we go down it gets worse and what you can see is as we predicted, for the cause of problems the red ones.",
                    "label": 0
                },
                {
                    "sent": "Vice Lonely doesn't help for the anti causing problems.",
                    "label": 0
                },
                {
                    "sent": "It sometimes helps but of course we don't guarantee that it helps.",
                    "label": 0
                },
                {
                    "sent": "It will depend on the algorithm and the problem.",
                    "label": 0
                },
                {
                    "sent": "So that's perfectly consistent and we also get a consistent pick.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for every regression this is using.",
                    "label": 0
                },
                {
                    "sent": "Study of brefeldin all they were using Co regularization and it turns out that for the anti causal problems it does help by and large.",
                    "label": 0
                },
                {
                    "sent": "So here the error.",
                    "label": 0
                },
                {
                    "sent": "This is the error so lower is better and semi supervised is better everywhere.",
                    "label": 0
                },
                {
                    "sent": "Where is for the datasets that we classify.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is causal.",
                    "label": 0
                },
                {
                    "sent": "We didn't really get systematic improvements so.",
                    "label": 0
                },
                {
                    "sent": "Then we have maybe.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have recently started to try to understand from a physical point of view, how these structural equation models related to differential equation models, but I think I've already gone overtime so.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I want to thank you for your attention.",
                    "label": 0
                }
            ]
        }
    }
}