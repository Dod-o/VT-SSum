{
    "id": "pcw66wqwhpvbry6khgvsuvrotxiv7gzl",
    "title": "Opening Address for the NIPS WS on the Generative and Discriminative Learning Interface",
    "info": {
        "organizer": [
            "Simon Lacoste-Julien, INRIA - SIERRA project-team",
            "Percy Liang, Computer Science Department, Stanford University"
        ],
        "published": "March 26, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops09_lacoste_julien_liang_opening/",
    "segmentation": [
        [
            "Welcome to the NIPS Workshop on the generative and Discriminative learning interface.",
            "I'm Simon this is Percy, and go will be your fellow organizers for the day.",
            "Here's the schedule for.",
            "Today"
        ],
        [
            "Notice at 10:00 AM the poster session.",
            "So for people who have posters, either put them up at 10:00 AM.",
            "Like we'll have tape or during the coffee break, which is from 9:00 to 9:30.",
            "There will be a panel at 5:50 with a bunch of with the invited speakers and Ben Taskar.",
            "An there were two changes in.",
            "If you have the printed version, one is that Dan Klein won't be on the panel.",
            "You couldn't make it an.",
            "There was also a poster presenter.",
            "Couldn't make it so if you look for somebody who's not there, well, they're not there.",
            "And then the other thing is that we would like to keep this workshop interactive, so feel free to ask questions during talk.",
            "Each contributed talk is 15 minutes and then 5 minutes for discussion, but we'll take in consideration if there are questions so that the speaker won't be that cut in the middle."
        ],
        [
            "Um, and that's about it for the schedule.",
            "Any question about it.",
            "No.",
            "So before going to the talks we would just want to give you a brief overview of motivation an give you a bit of basic terminology for what we mean by journey in discovering so that we're on the same page and some properties in hybrids so that we can have some food for thought for the rest of the day.",
            "But bear in mind that's not an exhaustive presentation about it, and it's not the only way to look at it is just to get us started.",
            "So."
        ],
        [
            "One of the main motivation would be that we want to solve real world predictions problem.",
            "And the question is how?",
            "Well, how should we go about it?",
            "So one way we could go for do prediction would be to define a prosthetic model on X&Y jointly and then learn by maximum likelihood or something.",
            "And that's what people call generative learning.",
            "Another way would be to just focus on the conditional of Y given X or just on the function of XY directly without maybe a plastic model and that would be more descriptive learning and so well.",
            "Which one should we go about?",
            "And so that's one dimension we could be interested in that will focus today.",
            "There's also a lot of other questions you could ask about that, like I could want to do science and just try to understand a phenomenon which is not just prediction and I could have a large amount of unlabeled data.",
            "And how do I harvest that?",
            "So all those questions are so relevant to this?",
            "But then."
        ],
        [
            "Why would we care about comparing generative learning or thinking about that?",
            "One thing is that by trying to leverage the advantage of both sides when you have a good understanding, we could enlarge our toolbox of methods.",
            "The other one is that often the appearance, very different communities and one of our aim with this workshop today was to bridge different communities together and actually already had very interesting discussion with my fellow Bajans at Cambridge, and so that's what we were trying to accomplish.",
            "So by contrasting one framework with the other often improve us, it will improve our just our basic understanding of learning in general.",
            "So just a bit of terminology to present the different framework in a bit more detail."
        ],
        [
            "We have an input which could be arbitrary.",
            "We want to predict a discrete output in the case of prediction.",
            "So usually the journey of this kind of distinction makes more sense in the prediction setting, so that's what we'll focus for now.",
            "Then you have a loss which is a measure of how costly each prediction is, given that the truth is why prime and I predicted why, for example, and then this decision theory framework for that would be that our goal is given some supervised training data pairs which are assumed to come from some fixed distribution IID.",
            "We want to learn a decision function for each X, which why should I predict which has low risk, which is basically just the generalization error which is the expectation over this true distribution.",
            "Of your loss of the predictions so you don't know this prediction that this distribution.",
            "So that's basically a framework in a goal."
        ],
        [
            "What are different ways to approach it so in general, learning like I said, basically the main idea is to model jointly X&Y.",
            "So the way you could do that would be to define the Patrick family of XY given some powder for example, and if you're a frequent this, you could fit it by rigorous maximum likelihood.",
            "If you're beige and you would marginalized out this parameter, but at the end what you get is still just a predictive distribution for both X&Y, so it's basically a density on X&Y and then given that you want you want to do prediction.",
            "So since you have information about the loss, what you could do is do inference to get the conditional P of Y given X and then find the prediction which will minimize the expected conditional loss under your current knowledge of the prediction.",
            "An example of a message like that would be, say, requires maximum likelihood.",
            "If you're doing structure prediction, then hmm, etc.",
            "OK, now in this kind of learning view I have split it in two sides so that it's clear though often they are forgotten.",
            "The first one is, I'll call it conditional learning, which is basically you just model the conditional Y given X still plastic model.",
            "You could still have a project family do maximum likelihood or something like that.",
            "Or if you're beige and you would have a family of Y given X, and depending on the parameter Theta, and you will modernize all this data like Gaussian process for example.",
            "And then given this conditional fit on training data, you would do the same thing, just predict by maybe including the loss information.",
            "Usually it's just maybe the 01 loss and then it just maximum.",
            "You just pick the why which maximize that.",
            "So then usually this is not really thinking construction, but it's still important to keep in mind because often you can have a symmetric cost.",
            "An example I said regarding maximum connected for structured prediction.",
            "Maybe the analog would be conditional random field.",
            "So there's another one, which is what I call less sensitive learning.",
            "There's a lot of different name for that, but the main we have gathered them together was two.",
            "Basically, given some data, Anna specific class that I care about, you will construct surrogate empirical loss.",
            "Which could be for support the hinge loss for support vector machine expansion allows for boosting or anything you can think of which is related to the loss and then given that you will choose the hypothesis H which is a function which will minimize that.",
            "So you directly think about age.",
            "There's no plastic model necessarily, so example of methods here is regonized empirical risk minimization.",
            "Should be the analog here for conditional likelihood and maximum likelihood.",
            "And then there is an active subscription is large margin methods like SVM, struct energy methods by Anna, an etc etc.",
            "And so that's kind of the landscape and in some sense it's a continuum.",
            "In my in my my opinion, and the more you go to the right, the more discriminative you are in a sense that you're more tuned towards the risk towards the last that you care about, and that's what you're trying to accomplish.",
            "Let's look at some properties just."
        ],
        [
            "Get a overview of things that we could think about.",
            "One aspect is modularity.",
            "Because driven conditional learning the way described them have a plastic model D or coherent flex dates are flexible language.",
            "It's modular in the sense it's easy to stack them together because all calibrated with a distribution, whereas discriminative because you don't have a holistic model there.",
            "It's harder to just plug things together in a meaningful fashion.",
            "Distribution robustness means that the mortal over.",
            "So for example, the young Lyon and I built disputed models out of modules that were initialized using labeled data.",
            "So we asked if you have labeled data for the Jimmy qualities, then you can plug these modules together and finding them.",
            "Very coherent, well if you fine tune the whole thing together then then it's fine.",
            "But for example like if I have learned a model here and I've learned a model there and I just want to stack them together without more learning, then you want them to be calibrated.",
            "Fine tuning used effectively belts.",
            "So it's not.",
            "It's not an absolute term.",
            "Trying to take a second model in the Cascade after trying the first one and so it does take into account the first one.",
            "I see it works fine.",
            "Yeah, OK. Fair enough food for thoughts, and that's the idea.",
            "So those are not absolute aspects, it's just that.",
            "And also sometimes there are myths and maybe Percy will talk about something, but so distribution robustness, the more you do the left, the more you are making distribution about the you're making assumptions about the distribution, and so you could think that if these assumptions are correct, you will have a problem changing loss.",
            "Here.",
            "The idea is that.",
            "For generating conditional, you make the prediction at the end after you have fit your model with the loss in terms.",
            "So if I just have my say my customer change their opinion and you want a different less well, you don't need to fit your model again.",
            "You just plug in there and make a different decision was for the discriminative approach, which was already tailored to the last.",
            "Then maybe you would have to retrain the whole system so it's not as flexible for that unlabeled data in the journey of case.",
            "It's kind of simple to include it because.",
            "Usually the distribution over X will influence your parameter was for condition on the screen.",
            "If it's not as obvious, though, it's doable.",
            "Um, computational efficiency.",
            "This is here I.",
            "This dimension is kind of a complex one, but I want to mention that in natural language processing, for example, Journal orphan very trivial to learn because it's fully supervised directed graphical model on discrete data.",
            "So it's basically just counting.",
            "On the other hand, in vision you have a very high dimensional continuous vector and maybe you're defining a Markov random field.",
            "You have a normalization constant to compute, and it's then it's really really intractable.",
            "If you go to the conditional setting.",
            "Hear what I said is that in the if you're in the just multiclass setting, there's no more normalization constant because it's just a thing to normalize over, so it's very trivial, though if you go to the structure prediction setting where you have an exponential number of possibilities.",
            "You could have problem with computing this normalization constant for PY given X.",
            "For example, if you have a conditional random field with loops on your labels, so that's when it becomes intractable and one advantage of this kind of method in this case is that you don't need the normalization for learning, and that's actually relevant, for example, in Word alignment and this kind of method, and I said no close form because that's just like aggression.",
            "For example, has no closed form, but yeah, so that's thing an.",
            "Yeah, I don't know this right side, but one thing is missing here, which I think is crucial, at least in natural language, which is that with discriminative models, experiences often being that you can use a much wider range of representations rich representations.",
            "And that for me is.",
            "The biggest reason to use instruments.",
            "What do you think, Percy?",
            "We had a discussion about that, yeah?",
            "Well there was because you said there was a Smith that.",
            "For discrete model, thinking that it's easier, you can have more flexible features.",
            "Add more flexible features of the entire entry, but the fact that you can, you cannot features at all with German.",
            "Every one of these.",
            "Yeah, OK, yeah approaches.",
            "You can have global features we have love.",
            "Difficult to get a job, so the idea is in general it's easier to because you don't have to have distribution over your over your your your variables.",
            "It's easier to have arbitrary features with long range dependency and just include them though.",
            "Yeah.",
            "It's not as easy.",
            "You can do it, but you could still have model is again you have a field function exactly.",
            "Yeah, so there's a tradeoff there.",
            "Distribution robustness You don't need to make assumptions about the of X. I guess you could include these features in very general.",
            "But can discuss it during the panel.",
            "But yeah, so.",
            "It's actually interesting because when we build this this, this, this, this we had a lot of argument about it an it's hard sometimes to put boxes around things, but it makes you think which is which is good.",
            "I think maybe."
        ],
        [
            "I've finished with this just one example of a theoretical result that have been cited a lot in machine learning, and it's a bit responsible for the folklore.",
            "If you have a lot of data using this kind of methods, if you have not much data, you suggested method, so that's this paper by Andrew Anglin Mike Jordan in 2002 where the compare Naive Bayes versus logistic regression for binary classification within our classifiers.",
            "Ann, just a cartoon of their result.",
            "I won't go into detail, but this is basically generalization error and this is the sample size.",
            "So the training set size an you have that with a lot of data.",
            "Logistic regression will beat my main base for robustness reason hopefully, but at the beginning actually naive Bayes was converging faster too.",
            "It's a synthetic error and hence you had this crossing behavior OK.",
            "This is Valerie.",
            "Yes, and I was going to say that.",
            "Yes, OK, so so the grain of salt here is that actually I asked Andrew Yang what happened when you rig arise.",
            "That and actually said that the curve goes like that so.",
            "Also, by the way, he didn't really use aggression and use some kind of mixture of empirical crystallization and logistic regression so, but there's still a flavor there, and some results which pointed into that an.",
            "And then it kind of bring motivation.",
            "Well, why not having like a lower envelope for that by considering some hybrid?",
            "And was this possible?",
            "And actually yes, and that's what Percy will talk about.",
            "Clips alright, alright, so one of the main motivations and what we wanted to get at this."
        ],
        [
            "Shop is kind of new algorithms that include the strengths of both discriminative and generative.",
            "So I'm basically going to talk about some paradigms of developing hybrid methods that people have considered.",
            "I've only restricted myself to 1 slide, so this is only a subset of the ones that people have done.",
            "So I kind of grouped them into two classes.",
            "The first is blending."
        ],
        [
            "So if you have a generative model P of X&Y and conditional model Y given X, then it might be natural to maximize and linear interpolation of these if they share the same parameters.",
            "So this is something that's done by and um."
        ],
        [
            "Of authors, if you don't want them to share the same parameters, you might give them different parameters, but tie the parameters together.",
            "And this has also been.",
            "Explored.",
            "Another intuition."
        ],
        [
            "Which leads to some theoretical analysis is suppose that X, usually X is a complex object, so assume there's some partition of it into two parts.",
            "Discriminative and generative are the where you condition or model."
        ],
        [
            "Or all of the X, but you can think of hybrids where you model some parts of the X and Kurdish while conditioning other parts.",
            "So that's a nice natural way to interpolate between the intuitions of modeling and conditioning."
        ],
        [
            "So these are kind of more model based where you have a discriminative model and the generative model and you maximize some criteria.",
            "So the."
        ],
        [
            "This.",
            "The staged techniques look like this, so one of the most common ones is you trying to generative model an.",
            "You use the output of a general model as features into a discriminative model, and this has been really successful in NLP envision as some of our invited speakers might."
        ],
        [
            "Talk about in a later talk.",
            "And Lastly, I just want to say that if you train a general model, you can use that to initialize the German model and this has been this kind of realization has been instrumental in kind of the development of deep belief Nets in the past four or five years, and so there's others.",
            "But this is only one slide."
        ],
        [
            "OK so I just wanted to mention that there is a continuum from generative discriminative and basically where we're going is from probabilistic generative, which includes naive Bayes, HMM refs etc.",
            "2 cases where we're conditioning on the data, logistic regression, CRF's to where we include the loss function which and throw away probabilities so.",
            "But I like to think about in terms of time to access one generative versus discriminative, where your.",
            "Essentially, in a very loose sense, modeling the data X in the input X or not versus probabilistic and non probabilistic, and these two dimensions kind of get conflated.",
            "So notice that.",
            "There's something.",
            "What is it?",
            "Hurting.",
            "Who?",
            "Yeah, that's uh, that is a weird one.",
            "Would have to think about OK, yeah, so there's something else though.",
            "Can anyone guess what it is?",
            "There might be several things, but this is just one.",
            "You all know it.",
            "You learned it in your first machine learning class.",
            "Alright, in the interest of time.",
            "Neighbors.",
            "Well."
        ],
        [
            "How would you think about that?",
            "Is non probabilistic OK, so no more slides.",
            "OK so that concludes our presentation and.",
            "What's a?",
            "And hopefully this gives you overview of kind of the things where looking at, and hopefully we can have a good discussion."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Welcome to the NIPS Workshop on the generative and Discriminative learning interface.",
                    "label": 1
                },
                {
                    "sent": "I'm Simon this is Percy, and go will be your fellow organizers for the day.",
                    "label": 0
                },
                {
                    "sent": "Here's the schedule for.",
                    "label": 0
                },
                {
                    "sent": "Today",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Notice at 10:00 AM the poster session.",
                    "label": 0
                },
                {
                    "sent": "So for people who have posters, either put them up at 10:00 AM.",
                    "label": 0
                },
                {
                    "sent": "Like we'll have tape or during the coffee break, which is from 9:00 to 9:30.",
                    "label": 0
                },
                {
                    "sent": "There will be a panel at 5:50 with a bunch of with the invited speakers and Ben Taskar.",
                    "label": 0
                },
                {
                    "sent": "An there were two changes in.",
                    "label": 0
                },
                {
                    "sent": "If you have the printed version, one is that Dan Klein won't be on the panel.",
                    "label": 0
                },
                {
                    "sent": "You couldn't make it an.",
                    "label": 0
                },
                {
                    "sent": "There was also a poster presenter.",
                    "label": 0
                },
                {
                    "sent": "Couldn't make it so if you look for somebody who's not there, well, they're not there.",
                    "label": 0
                },
                {
                    "sent": "And then the other thing is that we would like to keep this workshop interactive, so feel free to ask questions during talk.",
                    "label": 0
                },
                {
                    "sent": "Each contributed talk is 15 minutes and then 5 minutes for discussion, but we'll take in consideration if there are questions so that the speaker won't be that cut in the middle.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um, and that's about it for the schedule.",
                    "label": 0
                },
                {
                    "sent": "Any question about it.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "So before going to the talks we would just want to give you a brief overview of motivation an give you a bit of basic terminology for what we mean by journey in discovering so that we're on the same page and some properties in hybrids so that we can have some food for thought for the rest of the day.",
                    "label": 0
                },
                {
                    "sent": "But bear in mind that's not an exhaustive presentation about it, and it's not the only way to look at it is just to get us started.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One of the main motivation would be that we want to solve real world predictions problem.",
                    "label": 0
                },
                {
                    "sent": "And the question is how?",
                    "label": 0
                },
                {
                    "sent": "Well, how should we go about it?",
                    "label": 0
                },
                {
                    "sent": "So one way we could go for do prediction would be to define a prosthetic model on X&Y jointly and then learn by maximum likelihood or something.",
                    "label": 0
                },
                {
                    "sent": "And that's what people call generative learning.",
                    "label": 0
                },
                {
                    "sent": "Another way would be to just focus on the conditional of Y given X or just on the function of XY directly without maybe a plastic model and that would be more descriptive learning and so well.",
                    "label": 0
                },
                {
                    "sent": "Which one should we go about?",
                    "label": 0
                },
                {
                    "sent": "And so that's one dimension we could be interested in that will focus today.",
                    "label": 0
                },
                {
                    "sent": "There's also a lot of other questions you could ask about that, like I could want to do science and just try to understand a phenomenon which is not just prediction and I could have a large amount of unlabeled data.",
                    "label": 1
                },
                {
                    "sent": "And how do I harvest that?",
                    "label": 0
                },
                {
                    "sent": "So all those questions are so relevant to this?",
                    "label": 0
                },
                {
                    "sent": "But then.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Why would we care about comparing generative learning or thinking about that?",
                    "label": 1
                },
                {
                    "sent": "One thing is that by trying to leverage the advantage of both sides when you have a good understanding, we could enlarge our toolbox of methods.",
                    "label": 1
                },
                {
                    "sent": "The other one is that often the appearance, very different communities and one of our aim with this workshop today was to bridge different communities together and actually already had very interesting discussion with my fellow Bajans at Cambridge, and so that's what we were trying to accomplish.",
                    "label": 0
                },
                {
                    "sent": "So by contrasting one framework with the other often improve us, it will improve our just our basic understanding of learning in general.",
                    "label": 1
                },
                {
                    "sent": "So just a bit of terminology to present the different framework in a bit more detail.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have an input which could be arbitrary.",
                    "label": 0
                },
                {
                    "sent": "We want to predict a discrete output in the case of prediction.",
                    "label": 1
                },
                {
                    "sent": "So usually the journey of this kind of distinction makes more sense in the prediction setting, so that's what we'll focus for now.",
                    "label": 0
                },
                {
                    "sent": "Then you have a loss which is a measure of how costly each prediction is, given that the truth is why prime and I predicted why, for example, and then this decision theory framework for that would be that our goal is given some supervised training data pairs which are assumed to come from some fixed distribution IID.",
                    "label": 0
                },
                {
                    "sent": "We want to learn a decision function for each X, which why should I predict which has low risk, which is basically just the generalization error which is the expectation over this true distribution.",
                    "label": 1
                },
                {
                    "sent": "Of your loss of the predictions so you don't know this prediction that this distribution.",
                    "label": 0
                },
                {
                    "sent": "So that's basically a framework in a goal.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What are different ways to approach it so in general, learning like I said, basically the main idea is to model jointly X&Y.",
                    "label": 0
                },
                {
                    "sent": "So the way you could do that would be to define the Patrick family of XY given some powder for example, and if you're a frequent this, you could fit it by rigorous maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "If you're beige and you would marginalized out this parameter, but at the end what you get is still just a predictive distribution for both X&Y, so it's basically a density on X&Y and then given that you want you want to do prediction.",
                    "label": 0
                },
                {
                    "sent": "So since you have information about the loss, what you could do is do inference to get the conditional P of Y given X and then find the prediction which will minimize the expected conditional loss under your current knowledge of the prediction.",
                    "label": 0
                },
                {
                    "sent": "An example of a message like that would be, say, requires maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "If you're doing structure prediction, then hmm, etc.",
                    "label": 0
                },
                {
                    "sent": "OK, now in this kind of learning view I have split it in two sides so that it's clear though often they are forgotten.",
                    "label": 0
                },
                {
                    "sent": "The first one is, I'll call it conditional learning, which is basically you just model the conditional Y given X still plastic model.",
                    "label": 0
                },
                {
                    "sent": "You could still have a project family do maximum likelihood or something like that.",
                    "label": 0
                },
                {
                    "sent": "Or if you're beige and you would have a family of Y given X, and depending on the parameter Theta, and you will modernize all this data like Gaussian process for example.",
                    "label": 0
                },
                {
                    "sent": "And then given this conditional fit on training data, you would do the same thing, just predict by maybe including the loss information.",
                    "label": 0
                },
                {
                    "sent": "Usually it's just maybe the 01 loss and then it just maximum.",
                    "label": 0
                },
                {
                    "sent": "You just pick the why which maximize that.",
                    "label": 0
                },
                {
                    "sent": "So then usually this is not really thinking construction, but it's still important to keep in mind because often you can have a symmetric cost.",
                    "label": 0
                },
                {
                    "sent": "An example I said regarding maximum connected for structured prediction.",
                    "label": 0
                },
                {
                    "sent": "Maybe the analog would be conditional random field.",
                    "label": 0
                },
                {
                    "sent": "So there's another one, which is what I call less sensitive learning.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of different name for that, but the main we have gathered them together was two.",
                    "label": 0
                },
                {
                    "sent": "Basically, given some data, Anna specific class that I care about, you will construct surrogate empirical loss.",
                    "label": 1
                },
                {
                    "sent": "Which could be for support the hinge loss for support vector machine expansion allows for boosting or anything you can think of which is related to the loss and then given that you will choose the hypothesis H which is a function which will minimize that.",
                    "label": 0
                },
                {
                    "sent": "So you directly think about age.",
                    "label": 0
                },
                {
                    "sent": "There's no plastic model necessarily, so example of methods here is regonized empirical risk minimization.",
                    "label": 0
                },
                {
                    "sent": "Should be the analog here for conditional likelihood and maximum likelihood.",
                    "label": 1
                },
                {
                    "sent": "And then there is an active subscription is large margin methods like SVM, struct energy methods by Anna, an etc etc.",
                    "label": 0
                },
                {
                    "sent": "And so that's kind of the landscape and in some sense it's a continuum.",
                    "label": 0
                },
                {
                    "sent": "In my in my my opinion, and the more you go to the right, the more discriminative you are in a sense that you're more tuned towards the risk towards the last that you care about, and that's what you're trying to accomplish.",
                    "label": 1
                },
                {
                    "sent": "Let's look at some properties just.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Get a overview of things that we could think about.",
                    "label": 0
                },
                {
                    "sent": "One aspect is modularity.",
                    "label": 0
                },
                {
                    "sent": "Because driven conditional learning the way described them have a plastic model D or coherent flex dates are flexible language.",
                    "label": 0
                },
                {
                    "sent": "It's modular in the sense it's easy to stack them together because all calibrated with a distribution, whereas discriminative because you don't have a holistic model there.",
                    "label": 0
                },
                {
                    "sent": "It's harder to just plug things together in a meaningful fashion.",
                    "label": 0
                },
                {
                    "sent": "Distribution robustness means that the mortal over.",
                    "label": 1
                },
                {
                    "sent": "So for example, the young Lyon and I built disputed models out of modules that were initialized using labeled data.",
                    "label": 0
                },
                {
                    "sent": "So we asked if you have labeled data for the Jimmy qualities, then you can plug these modules together and finding them.",
                    "label": 0
                },
                {
                    "sent": "Very coherent, well if you fine tune the whole thing together then then it's fine.",
                    "label": 0
                },
                {
                    "sent": "But for example like if I have learned a model here and I've learned a model there and I just want to stack them together without more learning, then you want them to be calibrated.",
                    "label": 0
                },
                {
                    "sent": "Fine tuning used effectively belts.",
                    "label": 0
                },
                {
                    "sent": "So it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not an absolute term.",
                    "label": 0
                },
                {
                    "sent": "Trying to take a second model in the Cascade after trying the first one and so it does take into account the first one.",
                    "label": 0
                },
                {
                    "sent": "I see it works fine.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK. Fair enough food for thoughts, and that's the idea.",
                    "label": 0
                },
                {
                    "sent": "So those are not absolute aspects, it's just that.",
                    "label": 0
                },
                {
                    "sent": "And also sometimes there are myths and maybe Percy will talk about something, but so distribution robustness, the more you do the left, the more you are making distribution about the you're making assumptions about the distribution, and so you could think that if these assumptions are correct, you will have a problem changing loss.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "The idea is that.",
                    "label": 0
                },
                {
                    "sent": "For generating conditional, you make the prediction at the end after you have fit your model with the loss in terms.",
                    "label": 0
                },
                {
                    "sent": "So if I just have my say my customer change their opinion and you want a different less well, you don't need to fit your model again.",
                    "label": 0
                },
                {
                    "sent": "You just plug in there and make a different decision was for the discriminative approach, which was already tailored to the last.",
                    "label": 1
                },
                {
                    "sent": "Then maybe you would have to retrain the whole system so it's not as flexible for that unlabeled data in the journey of case.",
                    "label": 0
                },
                {
                    "sent": "It's kind of simple to include it because.",
                    "label": 0
                },
                {
                    "sent": "Usually the distribution over X will influence your parameter was for condition on the screen.",
                    "label": 0
                },
                {
                    "sent": "If it's not as obvious, though, it's doable.",
                    "label": 0
                },
                {
                    "sent": "Um, computational efficiency.",
                    "label": 0
                },
                {
                    "sent": "This is here I.",
                    "label": 0
                },
                {
                    "sent": "This dimension is kind of a complex one, but I want to mention that in natural language processing, for example, Journal orphan very trivial to learn because it's fully supervised directed graphical model on discrete data.",
                    "label": 0
                },
                {
                    "sent": "So it's basically just counting.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, in vision you have a very high dimensional continuous vector and maybe you're defining a Markov random field.",
                    "label": 0
                },
                {
                    "sent": "You have a normalization constant to compute, and it's then it's really really intractable.",
                    "label": 0
                },
                {
                    "sent": "If you go to the conditional setting.",
                    "label": 0
                },
                {
                    "sent": "Hear what I said is that in the if you're in the just multiclass setting, there's no more normalization constant because it's just a thing to normalize over, so it's very trivial, though if you go to the structure prediction setting where you have an exponential number of possibilities.",
                    "label": 0
                },
                {
                    "sent": "You could have problem with computing this normalization constant for PY given X.",
                    "label": 0
                },
                {
                    "sent": "For example, if you have a conditional random field with loops on your labels, so that's when it becomes intractable and one advantage of this kind of method in this case is that you don't need the normalization for learning, and that's actually relevant, for example, in Word alignment and this kind of method, and I said no close form because that's just like aggression.",
                    "label": 0
                },
                {
                    "sent": "For example, has no closed form, but yeah, so that's thing an.",
                    "label": 1
                },
                {
                    "sent": "Yeah, I don't know this right side, but one thing is missing here, which I think is crucial, at least in natural language, which is that with discriminative models, experiences often being that you can use a much wider range of representations rich representations.",
                    "label": 0
                },
                {
                    "sent": "And that for me is.",
                    "label": 0
                },
                {
                    "sent": "The biggest reason to use instruments.",
                    "label": 0
                },
                {
                    "sent": "What do you think, Percy?",
                    "label": 0
                },
                {
                    "sent": "We had a discussion about that, yeah?",
                    "label": 0
                },
                {
                    "sent": "Well there was because you said there was a Smith that.",
                    "label": 0
                },
                {
                    "sent": "For discrete model, thinking that it's easier, you can have more flexible features.",
                    "label": 0
                },
                {
                    "sent": "Add more flexible features of the entire entry, but the fact that you can, you cannot features at all with German.",
                    "label": 0
                },
                {
                    "sent": "Every one of these.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, yeah approaches.",
                    "label": 0
                },
                {
                    "sent": "You can have global features we have love.",
                    "label": 0
                },
                {
                    "sent": "Difficult to get a job, so the idea is in general it's easier to because you don't have to have distribution over your over your your your variables.",
                    "label": 0
                },
                {
                    "sent": "It's easier to have arbitrary features with long range dependency and just include them though.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "It's not as easy.",
                    "label": 0
                },
                {
                    "sent": "You can do it, but you could still have model is again you have a field function exactly.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so there's a tradeoff there.",
                    "label": 0
                },
                {
                    "sent": "Distribution robustness You don't need to make assumptions about the of X. I guess you could include these features in very general.",
                    "label": 0
                },
                {
                    "sent": "But can discuss it during the panel.",
                    "label": 0
                },
                {
                    "sent": "But yeah, so.",
                    "label": 0
                },
                {
                    "sent": "It's actually interesting because when we build this this, this, this, this we had a lot of argument about it an it's hard sometimes to put boxes around things, but it makes you think which is which is good.",
                    "label": 0
                },
                {
                    "sent": "I think maybe.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I've finished with this just one example of a theoretical result that have been cited a lot in machine learning, and it's a bit responsible for the folklore.",
                    "label": 0
                },
                {
                    "sent": "If you have a lot of data using this kind of methods, if you have not much data, you suggested method, so that's this paper by Andrew Anglin Mike Jordan in 2002 where the compare Naive Bayes versus logistic regression for binary classification within our classifiers.",
                    "label": 1
                },
                {
                    "sent": "Ann, just a cartoon of their result.",
                    "label": 0
                },
                {
                    "sent": "I won't go into detail, but this is basically generalization error and this is the sample size.",
                    "label": 0
                },
                {
                    "sent": "So the training set size an you have that with a lot of data.",
                    "label": 0
                },
                {
                    "sent": "Logistic regression will beat my main base for robustness reason hopefully, but at the beginning actually naive Bayes was converging faster too.",
                    "label": 0
                },
                {
                    "sent": "It's a synthetic error and hence you had this crossing behavior OK.",
                    "label": 0
                },
                {
                    "sent": "This is Valerie.",
                    "label": 0
                },
                {
                    "sent": "Yes, and I was going to say that.",
                    "label": 0
                },
                {
                    "sent": "Yes, OK, so so the grain of salt here is that actually I asked Andrew Yang what happened when you rig arise.",
                    "label": 0
                },
                {
                    "sent": "That and actually said that the curve goes like that so.",
                    "label": 0
                },
                {
                    "sent": "Also, by the way, he didn't really use aggression and use some kind of mixture of empirical crystallization and logistic regression so, but there's still a flavor there, and some results which pointed into that an.",
                    "label": 0
                },
                {
                    "sent": "And then it kind of bring motivation.",
                    "label": 0
                },
                {
                    "sent": "Well, why not having like a lower envelope for that by considering some hybrid?",
                    "label": 0
                },
                {
                    "sent": "And was this possible?",
                    "label": 0
                },
                {
                    "sent": "And actually yes, and that's what Percy will talk about.",
                    "label": 0
                },
                {
                    "sent": "Clips alright, alright, so one of the main motivations and what we wanted to get at this.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Shop is kind of new algorithms that include the strengths of both discriminative and generative.",
                    "label": 0
                },
                {
                    "sent": "So I'm basically going to talk about some paradigms of developing hybrid methods that people have considered.",
                    "label": 0
                },
                {
                    "sent": "I've only restricted myself to 1 slide, so this is only a subset of the ones that people have done.",
                    "label": 0
                },
                {
                    "sent": "So I kind of grouped them into two classes.",
                    "label": 0
                },
                {
                    "sent": "The first is blending.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you have a generative model P of X&Y and conditional model Y given X, then it might be natural to maximize and linear interpolation of these if they share the same parameters.",
                    "label": 0
                },
                {
                    "sent": "So this is something that's done by and um.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of authors, if you don't want them to share the same parameters, you might give them different parameters, but tie the parameters together.",
                    "label": 0
                },
                {
                    "sent": "And this has also been.",
                    "label": 0
                },
                {
                    "sent": "Explored.",
                    "label": 0
                },
                {
                    "sent": "Another intuition.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which leads to some theoretical analysis is suppose that X, usually X is a complex object, so assume there's some partition of it into two parts.",
                    "label": 0
                },
                {
                    "sent": "Discriminative and generative are the where you condition or model.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or all of the X, but you can think of hybrids where you model some parts of the X and Kurdish while conditioning other parts.",
                    "label": 0
                },
                {
                    "sent": "So that's a nice natural way to interpolate between the intuitions of modeling and conditioning.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these are kind of more model based where you have a discriminative model and the generative model and you maximize some criteria.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "The staged techniques look like this, so one of the most common ones is you trying to generative model an.",
                    "label": 0
                },
                {
                    "sent": "You use the output of a general model as features into a discriminative model, and this has been really successful in NLP envision as some of our invited speakers might.",
                    "label": 1
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Talk about in a later talk.",
                    "label": 0
                },
                {
                    "sent": "And Lastly, I just want to say that if you train a general model, you can use that to initialize the German model and this has been this kind of realization has been instrumental in kind of the development of deep belief Nets in the past four or five years, and so there's others.",
                    "label": 0
                },
                {
                    "sent": "But this is only one slide.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so I just wanted to mention that there is a continuum from generative discriminative and basically where we're going is from probabilistic generative, which includes naive Bayes, HMM refs etc.",
                    "label": 0
                },
                {
                    "sent": "2 cases where we're conditioning on the data, logistic regression, CRF's to where we include the loss function which and throw away probabilities so.",
                    "label": 0
                },
                {
                    "sent": "But I like to think about in terms of time to access one generative versus discriminative, where your.",
                    "label": 0
                },
                {
                    "sent": "Essentially, in a very loose sense, modeling the data X in the input X or not versus probabilistic and non probabilistic, and these two dimensions kind of get conflated.",
                    "label": 0
                },
                {
                    "sent": "So notice that.",
                    "label": 0
                },
                {
                    "sent": "There's something.",
                    "label": 0
                },
                {
                    "sent": "What is it?",
                    "label": 0
                },
                {
                    "sent": "Hurting.",
                    "label": 0
                },
                {
                    "sent": "Who?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's uh, that is a weird one.",
                    "label": 0
                },
                {
                    "sent": "Would have to think about OK, yeah, so there's something else though.",
                    "label": 0
                },
                {
                    "sent": "Can anyone guess what it is?",
                    "label": 0
                },
                {
                    "sent": "There might be several things, but this is just one.",
                    "label": 0
                },
                {
                    "sent": "You all know it.",
                    "label": 0
                },
                {
                    "sent": "You learned it in your first machine learning class.",
                    "label": 0
                },
                {
                    "sent": "Alright, in the interest of time.",
                    "label": 0
                },
                {
                    "sent": "Neighbors.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How would you think about that?",
                    "label": 0
                },
                {
                    "sent": "Is non probabilistic OK, so no more slides.",
                    "label": 0
                },
                {
                    "sent": "OK so that concludes our presentation and.",
                    "label": 0
                },
                {
                    "sent": "What's a?",
                    "label": 0
                },
                {
                    "sent": "And hopefully this gives you overview of kind of the things where looking at, and hopefully we can have a good discussion.",
                    "label": 0
                }
            ]
        }
    }
}