{
    "id": "6fjdum4gb3upyxsu2ncflu4iizebqyej",
    "title": "Norm-Based Capacity Control in Neural Networks",
    "info": {
        "author": [
            "Ryota Tomioka, Toyota Technological Institute at Chicago"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_tomioka_neural_networks/",
    "segmentation": [
        [
            "So this is joint work with Behnam Neyshabur in Nathans febrile.",
            "So this is Ben am who should who should have been here.",
            "But he couldn't make it here because of some of the issues I'm presenting place with him.",
            "So we're looking at neural networks."
        ],
        [
            "And for linear predictors we know very well that we can control the capacity by bounding some notion of norm independent of the number of characters we have in the model.",
            "So we ask this question, can we do the same thing for deep neural networks?",
            "Can we have a capacity control independent of the number of parameters?",
            "And this is important because these days before train very deep networks with lots of neurons and lots of parameters.",
            "It's hard to believe that the.",
            "Only measure of capacity control is a number of parameters and also people use norm based capacity controls and euro networks either as weight decay or it could be implicitly induced by the optimization procedure itself.",
            "So we study these kind of general feedforward neural networks specified by directed acyclic graph and weight sitting on top of each edge and."
        ],
        [
            "This is the norm.",
            "We consider the norm is defined as a group norm.",
            "So for each node we take the P norm of the the weights going into each node and take the two norm over all the nodes.",
            "So it's a group Norm and this measure of capacity for a particular instantiation of the input output relationship.",
            "So there are many ways to realize the same input output relationship.",
            "So when we talk about the capacity complexity of a function, we take the minimum.",
            "Of the capacity they find for the weight.",
            "And this is our main reason."
        ],
        [
            "Not so we can bound the Rademacher complexity of neural networks with bounded Norm Mu Mu PQ norm.",
            "We just defined bounded by mu and it sounded like this.",
            "So there are two parameters.",
            "The width age is a Max in degree of the verdicts.",
            "So it can be considered as a width of the network and the width dependency is nice.",
            "So when both P&Q are relatively small this thing becomes large.",
            "Each dependency disappears, so it means that we can have Internet in many neurons in each layer and still have bounded capacity.",
            "On the other hand, the dependency with respect to the depth D is exponential, can see it here and here that the capacity grows exponentially as you go deeper and deeper, and in fact both HD dependencies are tight.",
            "We showed this in their paper."
        ],
        [
            "So I would like to talk about two special cases, so the one is per unit regularization.",
            "This is when we take two to Infinity and take P general.",
            "And the interesting thing is that we can show that this is equivalent to something we call path regularization.",
            "Path realization is defined at some of all the directed paths going from input to output and take the product of the weights raised to the power of P along the path.",
            "The nice thing about this path based."
        ],
        [
            "Regularization is that in invariant to rescaling of the weights, you can multiply something to the first layer and scale down the second layer and this measure of complexity still stays the same and this allows us to develop a new efficient optimization algorithm.",
            "That's better, since Jesse gradient descent and adigrat.",
            "So this is our new paper.",
            "Unfortunately, path based regularization is a little bit too generous because Q is very large so the capacity becomes independent of the.",
            "The words only for P = 1 The other."
        ],
        [
            "Special case is overall regularization, which is when we take P = Q at.",
            "This includes weight decay, which is because to equals two or pico scoop.",
            "So one and the passkey is independent of the number of units in each layer.",
            "And the interesting thing is that the Pico sequels 2 case turns out to be actually cool into something known as convex neural networks, which has been studied recently by several people so.",
            "This."
        ],
        [
            "Is defined as the convex Hull of infinitely many hidden units, and the idea is to pick them by L1 regularization and it turns out that putting 2 normalization in both layers is equivalent to this thing.",
            "So in summary, we are able to get capacity control independent of the numbers of hidden units in each layer, and there are more results in our paper.",
            "In particular, there is something about whether at which condition the low norm class becomes convex class and this is given by this condition.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is joint work with Behnam Neyshabur in Nathans febrile.",
                    "label": 1
                },
                {
                    "sent": "So this is Ben am who should who should have been here.",
                    "label": 0
                },
                {
                    "sent": "But he couldn't make it here because of some of the issues I'm presenting place with him.",
                    "label": 0
                },
                {
                    "sent": "So we're looking at neural networks.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And for linear predictors we know very well that we can control the capacity by bounding some notion of norm independent of the number of characters we have in the model.",
                    "label": 0
                },
                {
                    "sent": "So we ask this question, can we do the same thing for deep neural networks?",
                    "label": 0
                },
                {
                    "sent": "Can we have a capacity control independent of the number of parameters?",
                    "label": 1
                },
                {
                    "sent": "And this is important because these days before train very deep networks with lots of neurons and lots of parameters.",
                    "label": 0
                },
                {
                    "sent": "It's hard to believe that the.",
                    "label": 0
                },
                {
                    "sent": "Only measure of capacity control is a number of parameters and also people use norm based capacity controls and euro networks either as weight decay or it could be implicitly induced by the optimization procedure itself.",
                    "label": 0
                },
                {
                    "sent": "So we study these kind of general feedforward neural networks specified by directed acyclic graph and weight sitting on top of each edge and.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is the norm.",
                    "label": 0
                },
                {
                    "sent": "We consider the norm is defined as a group norm.",
                    "label": 1
                },
                {
                    "sent": "So for each node we take the P norm of the the weights going into each node and take the two norm over all the nodes.",
                    "label": 0
                },
                {
                    "sent": "So it's a group Norm and this measure of capacity for a particular instantiation of the input output relationship.",
                    "label": 0
                },
                {
                    "sent": "So there are many ways to realize the same input output relationship.",
                    "label": 0
                },
                {
                    "sent": "So when we talk about the capacity complexity of a function, we take the minimum.",
                    "label": 1
                },
                {
                    "sent": "Of the capacity they find for the weight.",
                    "label": 0
                },
                {
                    "sent": "And this is our main reason.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Not so we can bound the Rademacher complexity of neural networks with bounded Norm Mu Mu PQ norm.",
                    "label": 1
                },
                {
                    "sent": "We just defined bounded by mu and it sounded like this.",
                    "label": 0
                },
                {
                    "sent": "So there are two parameters.",
                    "label": 0
                },
                {
                    "sent": "The width age is a Max in degree of the verdicts.",
                    "label": 0
                },
                {
                    "sent": "So it can be considered as a width of the network and the width dependency is nice.",
                    "label": 1
                },
                {
                    "sent": "So when both P&Q are relatively small this thing becomes large.",
                    "label": 0
                },
                {
                    "sent": "Each dependency disappears, so it means that we can have Internet in many neurons in each layer and still have bounded capacity.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, the dependency with respect to the depth D is exponential, can see it here and here that the capacity grows exponentially as you go deeper and deeper, and in fact both HD dependencies are tight.",
                    "label": 1
                },
                {
                    "sent": "We showed this in their paper.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I would like to talk about two special cases, so the one is per unit regularization.",
                    "label": 0
                },
                {
                    "sent": "This is when we take two to Infinity and take P general.",
                    "label": 0
                },
                {
                    "sent": "And the interesting thing is that we can show that this is equivalent to something we call path regularization.",
                    "label": 1
                },
                {
                    "sent": "Path realization is defined at some of all the directed paths going from input to output and take the product of the weights raised to the power of P along the path.",
                    "label": 0
                },
                {
                    "sent": "The nice thing about this path based.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Regularization is that in invariant to rescaling of the weights, you can multiply something to the first layer and scale down the second layer and this measure of complexity still stays the same and this allows us to develop a new efficient optimization algorithm.",
                    "label": 1
                },
                {
                    "sent": "That's better, since Jesse gradient descent and adigrat.",
                    "label": 0
                },
                {
                    "sent": "So this is our new paper.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, path based regularization is a little bit too generous because Q is very large so the capacity becomes independent of the.",
                    "label": 0
                },
                {
                    "sent": "The words only for P = 1 The other.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Special case is overall regularization, which is when we take P = Q at.",
                    "label": 1
                },
                {
                    "sent": "This includes weight decay, which is because to equals two or pico scoop.",
                    "label": 1
                },
                {
                    "sent": "So one and the passkey is independent of the number of units in each layer.",
                    "label": 0
                },
                {
                    "sent": "And the interesting thing is that the Pico sequels 2 case turns out to be actually cool into something known as convex neural networks, which has been studied recently by several people so.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is defined as the convex Hull of infinitely many hidden units, and the idea is to pick them by L1 regularization and it turns out that putting 2 normalization in both layers is equivalent to this thing.",
                    "label": 0
                },
                {
                    "sent": "So in summary, we are able to get capacity control independent of the numbers of hidden units in each layer, and there are more results in our paper.",
                    "label": 1
                },
                {
                    "sent": "In particular, there is something about whether at which condition the low norm class becomes convex class and this is given by this condition.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}