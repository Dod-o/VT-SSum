{
    "id": "g274u3q26mqpxz2sm7s74rzte5mqpft6",
    "title": "Answering Provenance-Aware Queries on RDF Data Cubes under Memory Budgets",
    "info": {
        "author": [
            "Simon Razniewski, Max Planck Institute for Informatics, Max Planck Institute"
        ],
        "published": "Nov. 22, 2018",
        "recorded": "October 2018",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2018_razniewski_answering_provenance_aware/",
    "segmentation": [
        [
            "OK, yeah, apologies from Louis for last minute issues, but I've been working with Louise in the past so I think I'm relatively knowledgeable on the topic so this work or Lewis with former collaborators from Aalborg University.",
            "The title of this paper is very long, answering provenance, aware queries on RDF data cubes under memory budgets.",
            "Therefore I'll go over it step by step so."
        ],
        [
            "Start very simple with datacubes."
        ],
        [
            "Well.",
            "Wow.",
            "Look nicer before.",
            "OK by a data cube basically is a multidimensional data structure.",
            "So in this example, we're looking at data about air pollution in different cities in different years.",
            "And for different pollutants.",
            "So we have your PM.",
            "10 PM.",
            "2.5 these are particles of different sizes.",
            "10 micrometres, 2.5 micrometres.",
            "So, if 3 dimensions here and dimensions can also have hierarchy.",
            "So here from City to country, there's a hierarchy going up.",
            "Data cubes can be used for OLAP."
        ],
        [
            "Various online analytical processing.",
            "So if we want to compute the average pollution of PM 10 per country, we can do that with two operations.",
            "We do a slice."
        ],
        [
            "We cut our cube into.",
            "Smaller slices take one slice out there and we two are."
        ],
        [
            "Pull up or slice.",
            "Um?",
            "Sorry, did this look really better?",
            "Start translator VGA adapter here.",
            "Um?",
            "The interest of time we try to explain this week.",
            "OK, I can probably change the equipment.",
            "Apologies for the format as we have here this slice and then we roll this up by country."
        ],
        [
            "And this is our query results of every exclusion per country per year."
        ],
        [
            "RTF data cubes."
        ],
        [
            "In model data groups in RDF, how do we do that?",
            "So each cell of the cube becomes an resource, so each cell become here.",
            "This cell in the upper left becomes observation, one observation.",
            "One is a QB observation, QB is an ontology.",
            "Vocabulary for normalizing data cubes and then we have a predicate for each dimension.",
            "So the year the value, the pollutant, the city, and why do we?"
        ],
        [
            "To use RDF data cubes, well we can link them to other resources.",
            "For example, we can say that the city is the DB pedia nodes and we can use information that.",
            "Lord is in France.",
            "In Front is part of EU.",
            "Anne."
        ],
        [
            "We can process our queries in sparkle thanks to the aggregates in Sparkle 1.1, so this would be our query average pollution per country."
        ],
        [
            "Year Provenance RDF data Cube so provenance inform."
        ],
        [
            "It is important to know where does data come from.",
            "So for example the measurement here might come from a certain."
        ],
        [
            "Source so we can add a provenance graph in addition to our data graph.",
            "Again, there's a vocabulary the Prof Ontology for that which could express that there's a Providence entity where this data comes from, which itself was computed by a join of two files."
        ],
        [
            "So this is called the provenance graph."
        ],
        [
            "And we can express the province information using quartz.",
            "That's one way.",
            "So for every triple, we had a fourth component saying better than people come from."
        ],
        [
            "Or another way to model provenance information is to introduce to use named graphs.",
            "So we group the triples by the problems entity where they're coming from."
        ],
        [
            "Now, how do we answer provenance about queries over RDF data cubes so."
        ],
        [
            "Here is a province about query, so average PM 10 pollution per country but using only data certified by the European energy Authoritie EA.",
            "So this query essentially consists of two subqueries, so one analytical query.",
            "The one we've seen before and one provenance query which expresses constraints on which data we want to use.",
            "And."
        ],
        [
            "Really, this could be expressed as this query, so we first compute a provenance graph of only 3%.",
            "Satisfy the provenance constraint, and then run our analytical query over that.",
            "However, it is not very efficient, so in most cases in examples will see later.",
            "This naive approach did not.",
            "Um finish in time so it timed out."
        ],
        [
            "Stepdaughter offers proposes to attack such queries in three phases, so they propose to first run the."
        ],
        [
            "Governance query so find out what other problems entities that satisfy the constraint in this."
        ],
        [
            "It might be P1P2P3 in the second phase."
        ],
        [
            "Yeah, to use an index to further filter down these entities.",
            "So the index here says which predicates actually occurring in which provenance graph.",
            "So maybe P1 contains value gluten city, country P2 contains only is a information P3 contains only our same as information?",
            "And what do we see here?",
            "Our query which predicates does it use?",
            "Well value, pollutant, city, country.",
            "Um, there are three constraints that 3 Providence entities that satisfy our first filtering by Providence Entity, but P2 and P3 don't contain any of the predicates were interested in based on this index.",
            "So what we can do is."
        ],
        [
            "Yeah, none of the predicates we want is in P2 and P3, so we can kick them out and continue processing our query only over P1.",
            "And."
        ],
        [
            "Third phase is to use the provenance of their cash to rewrite the query.",
            "So here we see example initialization state of the cash.",
            "So we see the signature of each cache fragment.",
            "F3 contains anything from Providence Entity tool.",
            "For example this triple fragment four contains.",
            "Any triples that are about the city predicate from Providence Entity one and so on.",
            "This example is very nicely chosen, so in this case what we need is only information from P1 for the predicates value pollutant city, country well, and we have exactly these fragments in the cache pollutant.",
            "P1 city.",
            "P1 country P1.",
            "Value P1.",
            "So what we can do here?",
            "We don't need to go to.",
            "Our hard disk at all.",
            "We can rewrite the query."
        ],
        [
            "By just going to these cache fragments and execute it efficiently?"
        ],
        [
            "I put something under the Rock, no.",
            "So there's a cash.",
            "So cash usually are not infinite in size.",
            "We have memory budgets, So what do we put in the cash?",
            "And that's the."
        ],
        [
            "Second main contribution of the paper.",
            "So the office proposed to build a provenance overcash by scanning the data set.",
            "Once building a tree of possible fragments that could go into cash.",
            "So at the root of the tree is to put the fragment that contains any triple in there.",
            "The next two nodes are splitting by Providence Entity, so maybe there are 603 plus from NTT one 400 triples from problems entity two and then in the next level we split further by the predicate and then last level by Easter.",
            "Property.",
            "And so which of these these are all the fragments that we could put in our cache?",
            "And which ones would be choose so the authors propose to use an IO P. To decide which fragments put in the."
        ],
        [
            "And this is the LP formulation, so put in as many fragments as you can.",
            "X Here is the decision variable whether to put a certain fragment in there or not.",
            "Under several constraints, so first."
        ],
        [
            "Train is obviously you can put with only as much as your budget allows, so the size of each fragment sum up cannot be greater than the budget."
        ],
        [
            "And then.",
            "Don't."
        ],
        [
            "Redundant information in the cache, so don't put.",
            "Quotes or triples twice there.",
            "What is redundant?",
            "Well, if I put this fragment that contains anything in the cash, then I don't need to put this fragment that contains data from province entity two anymore, since that's already subsumed in the fragment.",
            "Higher up.",
            "So for every path from 3 to leaf, choose only one fragment."
        ],
        [
            "And 3rd and 4th constraints are am give preference to.",
            "Predicates which are close to the observations and give preference to fragments that contain many different predicates."
        ],
        [
            "This based on the observation that all up queries are centered around observation, so almost every query asked something about the values and the predicates dimensions directly around the values fairs.",
            "This predicate country here is.",
            "One hop further away from the observation, so this gets less priority and caching."
        ],
        [
            "And the approach has been evaluated on two datasets, so the Star Schema benchmark.",
            "That's a synthetic data set with different sizes.",
            "This order para meters for and this is the number of triples resulting.",
            "Two settings, once balanced distribution of province entities once exponential distribution and the real data set, the QBO Air base Air pollution measurement from Denmark and United Kingdom.",
            "Anne."
        ],
        [
            "And.",
            "The methodology has been evaluated against the general TB native caching within without pre filtering based on the province of error three and against at least recently used strategy.",
            "And."
        ],
        [
            "Here we see runtimes for set of queries, so we've.",
            "Memory large enough to fit all data into the memory, so in memory.",
            "The approach clearly beats a generous native strategy.",
            "Look very evaluation and here on the right hand side we see what happens when the memories allows only to put 20% of the data in the cache.",
            "Also there park.",
            "In most cases outperforms PDP and here are different variants of combining parts of the pack approach.",
            "PDP or least recently used."
        ],
        [
            "So here we see examples of what happens if you increase the memory budget.",
            "On the QBO Air base we see there's little improvement after we allowed to put 20% of the data into the cache because it's a real data set.",
            "Actually, not all of the data is used in the queries, whereas on the synthetic data set.",
            "The data use is really random, so there we see continuous improvement.",
            "The more memory we give to the method."
        ],
        [
            "And we also see that the cache hit rate of the pack approach is better than the least recently used.",
            "And that approach also scales linearly with the number of triples."
        ],
        [
            "And to conclude, so it's possible to speed up and sparkle up queries without explicitly knowing the query load.",
            "So this were optimizations done purely based on strategy of reducing redundancy and taking observations closer.",
            "Um to the values.",
            "There's no need to load the entire cube into the memory, especially on real data.",
            "So if you have the critical parts, it's usually not being aware of the provenance helps to rewrite the query so as not to have to take the Union of all the provenance entities that satisfy the.",
            "Provenance constraints, but we take only those which are also useful for answering the query and future work.",
            "The office intend to look at dynamic or explicit query loads.",
            "That can help to even further speed up the method.",
            "Thank you and if there are questions, I hope I can answer them.",
            "Thanks, Simon."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, yeah, apologies from Louis for last minute issues, but I've been working with Louise in the past so I think I'm relatively knowledgeable on the topic so this work or Lewis with former collaborators from Aalborg University.",
                    "label": 0
                },
                {
                    "sent": "The title of this paper is very long, answering provenance, aware queries on RDF data cubes under memory budgets.",
                    "label": 1
                },
                {
                    "sent": "Therefore I'll go over it step by step so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Start very simple with datacubes.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "Wow.",
                    "label": 0
                },
                {
                    "sent": "Look nicer before.",
                    "label": 0
                },
                {
                    "sent": "OK by a data cube basically is a multidimensional data structure.",
                    "label": 0
                },
                {
                    "sent": "So in this example, we're looking at data about air pollution in different cities in different years.",
                    "label": 0
                },
                {
                    "sent": "And for different pollutants.",
                    "label": 0
                },
                {
                    "sent": "So we have your PM.",
                    "label": 0
                },
                {
                    "sent": "10 PM.",
                    "label": 0
                },
                {
                    "sent": "2.5 these are particles of different sizes.",
                    "label": 0
                },
                {
                    "sent": "10 micrometres, 2.5 micrometres.",
                    "label": 0
                },
                {
                    "sent": "So, if 3 dimensions here and dimensions can also have hierarchy.",
                    "label": 0
                },
                {
                    "sent": "So here from City to country, there's a hierarchy going up.",
                    "label": 0
                },
                {
                    "sent": "Data cubes can be used for OLAP.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Various online analytical processing.",
                    "label": 0
                },
                {
                    "sent": "So if we want to compute the average pollution of PM 10 per country, we can do that with two operations.",
                    "label": 0
                },
                {
                    "sent": "We do a slice.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We cut our cube into.",
                    "label": 0
                },
                {
                    "sent": "Smaller slices take one slice out there and we two are.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pull up or slice.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Sorry, did this look really better?",
                    "label": 0
                },
                {
                    "sent": "Start translator VGA adapter here.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The interest of time we try to explain this week.",
                    "label": 0
                },
                {
                    "sent": "OK, I can probably change the equipment.",
                    "label": 0
                },
                {
                    "sent": "Apologies for the format as we have here this slice and then we roll this up by country.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is our query results of every exclusion per country per year.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "RTF data cubes.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In model data groups in RDF, how do we do that?",
                    "label": 0
                },
                {
                    "sent": "So each cell of the cube becomes an resource, so each cell become here.",
                    "label": 0
                },
                {
                    "sent": "This cell in the upper left becomes observation, one observation.",
                    "label": 0
                },
                {
                    "sent": "One is a QB observation, QB is an ontology.",
                    "label": 0
                },
                {
                    "sent": "Vocabulary for normalizing data cubes and then we have a predicate for each dimension.",
                    "label": 0
                },
                {
                    "sent": "So the year the value, the pollutant, the city, and why do we?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To use RDF data cubes, well we can link them to other resources.",
                    "label": 1
                },
                {
                    "sent": "For example, we can say that the city is the DB pedia nodes and we can use information that.",
                    "label": 0
                },
                {
                    "sent": "Lord is in France.",
                    "label": 0
                },
                {
                    "sent": "In Front is part of EU.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can process our queries in sparkle thanks to the aggregates in Sparkle 1.1, so this would be our query average pollution per country.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Year Provenance RDF data Cube so provenance inform.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is important to know where does data come from.",
                    "label": 0
                },
                {
                    "sent": "So for example the measurement here might come from a certain.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Source so we can add a provenance graph in addition to our data graph.",
                    "label": 0
                },
                {
                    "sent": "Again, there's a vocabulary the Prof Ontology for that which could express that there's a Providence entity where this data comes from, which itself was computed by a join of two files.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is called the provenance graph.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we can express the province information using quartz.",
                    "label": 0
                },
                {
                    "sent": "That's one way.",
                    "label": 0
                },
                {
                    "sent": "So for every triple, we had a fourth component saying better than people come from.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or another way to model provenance information is to introduce to use named graphs.",
                    "label": 0
                },
                {
                    "sent": "So we group the triples by the problems entity where they're coming from.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, how do we answer provenance about queries over RDF data cubes so.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is a province about query, so average PM 10 pollution per country but using only data certified by the European energy Authoritie EA.",
                    "label": 1
                },
                {
                    "sent": "So this query essentially consists of two subqueries, so one analytical query.",
                    "label": 0
                },
                {
                    "sent": "The one we've seen before and one provenance query which expresses constraints on which data we want to use.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Really, this could be expressed as this query, so we first compute a provenance graph of only 3%.",
                    "label": 0
                },
                {
                    "sent": "Satisfy the provenance constraint, and then run our analytical query over that.",
                    "label": 0
                },
                {
                    "sent": "However, it is not very efficient, so in most cases in examples will see later.",
                    "label": 0
                },
                {
                    "sent": "This naive approach did not.",
                    "label": 0
                },
                {
                    "sent": "Um finish in time so it timed out.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stepdaughter offers proposes to attack such queries in three phases, so they propose to first run the.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Governance query so find out what other problems entities that satisfy the constraint in this.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It might be P1P2P3 in the second phase.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, to use an index to further filter down these entities.",
                    "label": 0
                },
                {
                    "sent": "So the index here says which predicates actually occurring in which provenance graph.",
                    "label": 0
                },
                {
                    "sent": "So maybe P1 contains value gluten city, country P2 contains only is a information P3 contains only our same as information?",
                    "label": 0
                },
                {
                    "sent": "And what do we see here?",
                    "label": 0
                },
                {
                    "sent": "Our query which predicates does it use?",
                    "label": 0
                },
                {
                    "sent": "Well value, pollutant, city, country.",
                    "label": 1
                },
                {
                    "sent": "Um, there are three constraints that 3 Providence entities that satisfy our first filtering by Providence Entity, but P2 and P3 don't contain any of the predicates were interested in based on this index.",
                    "label": 1
                },
                {
                    "sent": "So what we can do is.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, none of the predicates we want is in P2 and P3, so we can kick them out and continue processing our query only over P1.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Third phase is to use the provenance of their cash to rewrite the query.",
                    "label": 0
                },
                {
                    "sent": "So here we see example initialization state of the cash.",
                    "label": 0
                },
                {
                    "sent": "So we see the signature of each cache fragment.",
                    "label": 0
                },
                {
                    "sent": "F3 contains anything from Providence Entity tool.",
                    "label": 0
                },
                {
                    "sent": "For example this triple fragment four contains.",
                    "label": 0
                },
                {
                    "sent": "Any triples that are about the city predicate from Providence Entity one and so on.",
                    "label": 0
                },
                {
                    "sent": "This example is very nicely chosen, so in this case what we need is only information from P1 for the predicates value pollutant city, country well, and we have exactly these fragments in the cache pollutant.",
                    "label": 1
                },
                {
                    "sent": "P1 city.",
                    "label": 0
                },
                {
                    "sent": "P1 country P1.",
                    "label": 0
                },
                {
                    "sent": "Value P1.",
                    "label": 0
                },
                {
                    "sent": "So what we can do here?",
                    "label": 0
                },
                {
                    "sent": "We don't need to go to.",
                    "label": 0
                },
                {
                    "sent": "Our hard disk at all.",
                    "label": 0
                },
                {
                    "sent": "We can rewrite the query.",
                    "label": 1
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By just going to these cache fragments and execute it efficiently?",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I put something under the Rock, no.",
                    "label": 0
                },
                {
                    "sent": "So there's a cash.",
                    "label": 0
                },
                {
                    "sent": "So cash usually are not infinite in size.",
                    "label": 0
                },
                {
                    "sent": "We have memory budgets, So what do we put in the cash?",
                    "label": 1
                },
                {
                    "sent": "And that's the.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Second main contribution of the paper.",
                    "label": 0
                },
                {
                    "sent": "So the office proposed to build a provenance overcash by scanning the data set.",
                    "label": 0
                },
                {
                    "sent": "Once building a tree of possible fragments that could go into cash.",
                    "label": 0
                },
                {
                    "sent": "So at the root of the tree is to put the fragment that contains any triple in there.",
                    "label": 0
                },
                {
                    "sent": "The next two nodes are splitting by Providence Entity, so maybe there are 603 plus from NTT one 400 triples from problems entity two and then in the next level we split further by the predicate and then last level by Easter.",
                    "label": 0
                },
                {
                    "sent": "Property.",
                    "label": 0
                },
                {
                    "sent": "And so which of these these are all the fragments that we could put in our cache?",
                    "label": 0
                },
                {
                    "sent": "And which ones would be choose so the authors propose to use an IO P. To decide which fragments put in the.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is the LP formulation, so put in as many fragments as you can.",
                    "label": 1
                },
                {
                    "sent": "X Here is the decision variable whether to put a certain fragment in there or not.",
                    "label": 0
                },
                {
                    "sent": "Under several constraints, so first.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Train is obviously you can put with only as much as your budget allows, so the size of each fragment sum up cannot be greater than the budget.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "Don't.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Redundant information in the cache, so don't put.",
                    "label": 0
                },
                {
                    "sent": "Quotes or triples twice there.",
                    "label": 0
                },
                {
                    "sent": "What is redundant?",
                    "label": 0
                },
                {
                    "sent": "Well, if I put this fragment that contains anything in the cash, then I don't need to put this fragment that contains data from province entity two anymore, since that's already subsumed in the fragment.",
                    "label": 0
                },
                {
                    "sent": "Higher up.",
                    "label": 0
                },
                {
                    "sent": "So for every path from 3 to leaf, choose only one fragment.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And 3rd and 4th constraints are am give preference to.",
                    "label": 0
                },
                {
                    "sent": "Predicates which are close to the observations and give preference to fragments that contain many different predicates.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This based on the observation that all up queries are centered around observation, so almost every query asked something about the values and the predicates dimensions directly around the values fairs.",
                    "label": 0
                },
                {
                    "sent": "This predicate country here is.",
                    "label": 0
                },
                {
                    "sent": "One hop further away from the observation, so this gets less priority and caching.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the approach has been evaluated on two datasets, so the Star Schema benchmark.",
                    "label": 0
                },
                {
                    "sent": "That's a synthetic data set with different sizes.",
                    "label": 0
                },
                {
                    "sent": "This order para meters for and this is the number of triples resulting.",
                    "label": 0
                },
                {
                    "sent": "Two settings, once balanced distribution of province entities once exponential distribution and the real data set, the QBO Air base Air pollution measurement from Denmark and United Kingdom.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The methodology has been evaluated against the general TB native caching within without pre filtering based on the province of error three and against at least recently used strategy.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here we see runtimes for set of queries, so we've.",
                    "label": 0
                },
                {
                    "sent": "Memory large enough to fit all data into the memory, so in memory.",
                    "label": 0
                },
                {
                    "sent": "The approach clearly beats a generous native strategy.",
                    "label": 0
                },
                {
                    "sent": "Look very evaluation and here on the right hand side we see what happens when the memories allows only to put 20% of the data in the cache.",
                    "label": 0
                },
                {
                    "sent": "Also there park.",
                    "label": 0
                },
                {
                    "sent": "In most cases outperforms PDP and here are different variants of combining parts of the pack approach.",
                    "label": 0
                },
                {
                    "sent": "PDP or least recently used.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here we see examples of what happens if you increase the memory budget.",
                    "label": 0
                },
                {
                    "sent": "On the QBO Air base we see there's little improvement after we allowed to put 20% of the data into the cache because it's a real data set.",
                    "label": 0
                },
                {
                    "sent": "Actually, not all of the data is used in the queries, whereas on the synthetic data set.",
                    "label": 0
                },
                {
                    "sent": "The data use is really random, so there we see continuous improvement.",
                    "label": 0
                },
                {
                    "sent": "The more memory we give to the method.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we also see that the cache hit rate of the pack approach is better than the least recently used.",
                    "label": 0
                },
                {
                    "sent": "And that approach also scales linearly with the number of triples.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And to conclude, so it's possible to speed up and sparkle up queries without explicitly knowing the query load.",
                    "label": 1
                },
                {
                    "sent": "So this were optimizations done purely based on strategy of reducing redundancy and taking observations closer.",
                    "label": 0
                },
                {
                    "sent": "Um to the values.",
                    "label": 0
                },
                {
                    "sent": "There's no need to load the entire cube into the memory, especially on real data.",
                    "label": 1
                },
                {
                    "sent": "So if you have the critical parts, it's usually not being aware of the provenance helps to rewrite the query so as not to have to take the Union of all the provenance entities that satisfy the.",
                    "label": 1
                },
                {
                    "sent": "Provenance constraints, but we take only those which are also useful for answering the query and future work.",
                    "label": 0
                },
                {
                    "sent": "The office intend to look at dynamic or explicit query loads.",
                    "label": 0
                },
                {
                    "sent": "That can help to even further speed up the method.",
                    "label": 0
                },
                {
                    "sent": "Thank you and if there are questions, I hope I can answer them.",
                    "label": 0
                },
                {
                    "sent": "Thanks, Simon.",
                    "label": 0
                }
            ]
        }
    }
}