{
    "id": "tdyvw2fyzl7ac6hd3xgeurrqyyisupb6",
    "title": "Clustering with Prior Information",
    "info": {
        "author": [
            "Greg Ver Steeg, CalTech"
        ],
        "published": "Jan. 19, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Clustering"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops09_versteeg_cwp/",
    "segmentation": [
        [
            "So as I said, my name is Greg, I'm going to talk about some work I did with our men oliverian and Ram Galstian at the Information Science Institute.",
            "So we're going to talk about clustering and graphs using prior information.",
            "So I want to start with kind of the simplest possible."
        ],
        [
            "Beginnings we have two groups of N nodes and these groups are totally disconnected, but they have some internal density of links and so any pair of nodes within a link will just be on with probability Alpha over N. OK, so so that's fine.",
            "This is very easy.",
            "We can always split these groups up, but now we want to make things slightly more tricky and introduce some notion of these groups overlapping."
        ],
        [
            "So once you do this, one more parameter gamma which will characterize the density of links between the two different groups.",
            "So a gamma over N will be the probability that a pair of nodes from different groups are connected, so clearly.",
            "Intuitively, we can see that."
        ],
        [
            "If gamma equals Alpha, we don't have two groups at all, and if gamma equals zero, the groups are completely separated and everything is easy.",
            "So this seems like a very simple kind of definition for cluster structure, but actually even this has some problems lurking underneath the surface."
        ],
        [
            "So the first question is, you know, can we?",
            "Can we even detect these two clusters in the case that Alpha is greater than gamma is that?",
            "Is that really even possible?",
            "So I have a fancy animation?",
            "Get this going here.",
            "So so we first I should say we can imagine.",
            "Turning up gamma, putting our clusters closer together, and as we do so, we might at each step to try to partition our groups into two, and we're going to partition them into the red in the Green Group, and so at first it's simple reading.",
            "Green are easily separated, and now we've started to add a few, a few links and so far everything is still fine, but we're going to see how the accuracy changes as we turn up this noise parameter until gamma equals Alpha, and at this point we really expect there's no more cluster structure at all.",
            "OK, so things are looking OK for a little while, but actually we see that.",
            "What?",
            "We get kind of a steep decline in our accuracy around this point.",
            "So I should say that we're using just an approximate min cut algorithm for this little demonstration.",
            "OK, so by this point already it's not really clear that we're doing any better than just a random classification.",
            "Kind of things don't change much after this point, and so even though this is a long before the density of Inter cluster links has gotten big enough to wash out the cluster structure, still it doesn't seem like we can do any better than random at this point.",
            "So there's actually a theoretical result by Reichhardt and Lyons that shows that there actually is such a true transition in terms of where we can detect clusters, and at this threshold is actually sharp that there really is some point beyond which, even though gamma is still less than Alpha, we actually can do no better than than random in terms of partitioning this group.",
            "OK, so the motivation for work."
        ],
        [
            "To say what happens if we add some prior information to this situation, some prior information about the clusters.",
            "What happens to this detection threshold?",
            "How does it look?",
            "So there's two ways we can imagine adding.",
            "In some information.",
            "We could have some information about the links or over some information about the nodes themselves, so we're going to consider the latter scenario.",
            "Where we have some fraction of the nodes and we are we know what the correct partition is for those.",
            "OK, so."
        ],
        [
            "I'll give a few more details about this scenario and then I want to kind of introduce some of these statistical physics tools that we use to analyze this case.",
            "And then we'll I'll summarize the two kind of main results.",
            "One is about unweighted graphs where we show that this detection threshold really goes away even with the tiny tiny amount of prior information.",
            "And then if there's time we can talk a little bit about weighted graphs, where it seems that prior information sort of affects what gets defined as a cluster.",
            "OK, so the."
        ],
        [
            "Simple model.",
            "Just to reiterate, we have nodes in our two different graphs and intercluster links are on with probability Alpha over an gamma is for the between cluster links.",
            "So the reason we define Alpha and gamma in this way is so that the graph remains sparse.",
            "That is, as the graph gets very large, we want the connectivity to remain finite.",
            "So you can think of Alpha is the internal connectivity and gamma is the between cluster connectivity.",
            "OK, so."
        ],
        [
            "There's a bunch of different algorithms, rec for partitioning groups.",
            "Here's some of the common ones, like min cut where you just minimizing the weight of the cuts as you separate them into two groups, maximizing the modularity.",
            "But the method that we're going to talk about is the Ising spin model, because it allows us to bring some physics tools in and for the case of equitation partitions, we should note that these methods are really all equivalent.",
            "OK, so."
        ],
        [
            "To start the physics analogy, we're going to imagine that every node has a spin associated with it.",
            "Spend A plus or minus one.",
            "And then spin of plus one means that we're really in Group one and a spend of minus one means we're really in Group 2.",
            "So that means if we can find some configuration of spins, that's like partitioning the nodes into two groups.",
            "So an edge on our graph."
        ],
        [
            "Is going to correspond to some link between spins and the energy of that link depends on the value of the spins, so the spins want to line up energetically.",
            "That's more favorable, and it takes some work to flip one over so.",
            "So we said that's a higher energy configuration and.",
            "Yeah, so.",
            "So what?",
            "We're really trying to do is minimize the energy of this whole system.",
            "So for an entire graph that the energy really."
        ],
        [
            "Look something like this.",
            "We're just summing up over all the links and seeing what the energy is based on the configuration of different nodes.",
            "So we want to add one other constraint because the lowest energy condition would clearly be all of the spins in the same direction and no partition at all.",
            "So we had another constraint that we get to two groups.",
            "OK.",
            "So, so we're looking for the minimum energy state the ground state of the system, and we're hoping that that corresponds to some optimal partition.",
            "So this."
        ],
        [
            "This kind of problem is solved.",
            "With what's called the cavity method in statistical physics, but it's really a version of belief propagation or message passing, so you imagine that your child nodes are all have some spins and they kind of exert a field onto their parents and then that field determines the parent node spin and then the parent passes its state up the tree and this is how we kind of find this ground state.",
            "So this is an exact process for trees.",
            "Obviously the graphs we're considering aren't aren't trees, but as we get large graphs, the loops are very large and so it's kind of OK. OK, oh oh equations."
        ],
        [
            "So Aaron.",
            "Demanded I put equations on here, but Luckily I don't really have to talk about them.",
            "I guess this is sort of a field consistency equation and this is really what you solve.",
            "2 to figure out.",
            "How accurate your partitioning is?",
            "So let me go to this."
        ],
        [
            "So really what we're calculating is the magnetization of the two clusters, and that's just sort of the notion that in one of the clusters all the spins are lining up there, magnetized in One Direction, and insofar as that's happened, then we've succeeded.",
            "We have classified this cluster correctly is all one spin.",
            "OK, so.",
            "Little bit more of the physics analogy.",
            "Mechanization is really considered like an order parameter.",
            "There's some phase where it's completely disordered and there's no magnetisation, and that really corresponds to the regime where we can't detect detect anything.",
            "Any clusters at all, so OK. OK, so so."
        ],
        [
            "Here I have a graph of the accuracy that you could detect this partition.",
            "As a so we fixed the internal connectivity of the two graphs and now the X axis were just turning up the intergroup connectivity and we're getting closer and closer, and you see that really, we do get this.",
            "This sharp threshold and that it comes long before gamma equals Alpha.",
            "And after that just completely random and you can't partition it all.",
            "So you."
        ],
        [
            "Ask where where this point comes.",
            "Four different values of Alpha and gamma, so that's what's depicted here.",
            "I.",
            "So on this side of the line we have.",
            "The the space where we can detect cluster structure and on that side we can't at all as Alpha and gamma get big.",
            "That really means that the overall connectivity of the graph is getting very big as well.",
            "So we're not.",
            "We no longer have sparse graphs in that case and.",
            "Alpha gets closer and closer to gamma, the threshold gets closer, so as we get to non sparse graphs.",
            "OK.",
            "So."
        ],
        [
            "Now we want to add in some supervision some prior information.",
            "So we imagine that there's some random fraction of the nodes that we have the labels for, and so to put this into our Hamiltonian is very simple.",
            "We just put an energy penalty for incorrectly labeling those spins that we now those nodes that we know from physics perspective.",
            "This is still really simple Hamiltonian, so so we can still solve this.",
            "Using our previous method.",
            "OK, So what changes?"
        ],
        [
            "As as we.",
            "Start labeling some of these pins so.",
            "Kind of switched things around here, so now we're holding the intercluster connectivity fix and we're increasing the connectivity within the cluster.",
            "So, so this blue curve corresponds to what we talked about before, where we have no supervision and there's some sharp threshold.",
            "And that's all the same.",
            "But what's different?",
            "These two curves that are for some amount of supervision.",
            "So here we've labeled 5% of the nodes, and here 20%, and we see that there's a fundamental change in the behavior of this detection threshold.",
            "Now there's no more sharp threshold.",
            "Actually, even as the intercluster connectivity is almost equal to the intra cluster connectivity, we still have some better than random chance of partitioning our nodes correctly.",
            "So one other kind of point I guess I wanted to make."
        ],
        [
            "But this is in terms of the stability of an inference algorithm, right?",
            "So if you were doing a unsupervised inference on these clusters and it just happened that your Inter cluster density was near this threshold, clearly you'd have a fairly unstable algorithm, right?",
            "Because just shifting over a little bit to the right or the left will have a really big effects on what you can classify as a cluster, whereas if you have even a tiny amount of supervision.",
            "Your performance will degrade very gracefully, so it's quite a difference in how those algorithms would proceed.",
            "So we also did this experimentally for graphs of 10,000 nodes and using simulated annealing and the results are very similar.",
            "OK. Power lies.",
            "It doesn't matter what kind of graph you do."
        ],
        [
            "Is with.",
            "It all is very similar.",
            "So now in my last few minutes I wanted to talk to us."
        ],
        [
            "About weighted graphs.",
            "So if you consider this situation where you have one node that's connected with.",
            "Small weights, but many links to one group but one, but I have the link to another group.",
            "Clearly there's a lot of, there's a lot of ambiguity and how we should cluster this particular node, and you can come up with a lot of different ways that make sense that would classify it in different groups.",
            "If you threshold the weights, then the weights I'll go away and you put it in Group 2.",
            "If you neglect the weights, it has more on Group One, so you'd classify it that way from a min cut perspective.",
            "Cutting either of those is the same, so it could go either way.",
            "But now what happens when we when we add supervision to this?"
        ],
        [
            "Problem so.",
            "So now we're going to consider the case where are.",
            "Within clusters we have many but weaker links and between clusters we have heavier but fewer links.",
            "So intuitively, like from a min cut perspective.",
            "If if Alpha is twice gamma.",
            "Then that should be kind of."
        ],
        [
            "The point where clusters disappear right?",
            "And because then cutting through the middle or cutting inside should be about the same thing.",
            "Are for unsupervised detection that the threshold is still much bigger than what we should theoretically be able to get from min cut, which is just like before where Alpha equals gamma.",
            "OK.",
            "So when we add supervision to this picture, the things change a little bit.",
            "So first of all.",
            "Where the detection threshold is depends on row.",
            "Now for this particular case, the threshold actually get smaller or better for smaller amounts of supervision, but smaller but nonzero amounts of supervision, and for this case the threshold actually goes to 1.5 gamma, which is even lower than what we have expected from this min cut threshold.",
            "So.",
            "To give kind of a. Intuition for why that should be.",
            "I think what's really happening is if you have only tiny amounts of supervision then.",
            "You're more likely with numerous but weaker links to hit one of the labeled nodes and classify yourself correctly.",
            "So in this case, by adding that kind of supervision, tiny amounts you've sort of biased yourself towards.",
            "Tord's weaker.",
            "But more numerous links.",
            "OK, so just to summarize, so we showed that you know for Spa."
        ],
        [
            "Graphs we really have some detection threshold beyond which you can really detect any partitions, but even a little bit of a prior information can eliminate that threshold or move it to its smallest possible value.",
            "And for weighted graphs.",
            "The threshold depends on the prior information that you have and it even kind of affects your notion.",
            "What a cluster should be OK, thanks.",
            "Question.",
            "Sure.",
            "Yeah.",
            "A lot depends on the fact that you had a second order phase transition in your analysis.",
            "For example, the smearing out and so on.",
            "Is that a generic behavior for these types of problems?",
            "Because it looks like that you are essentially solving Updike relation problem with an external global constraint despite partitioning constraint is global and then you have this disorder from the productivity of your graph.",
            "Which is basically positive, right?",
            "So you're wondering if.",
            "If we didn't have this, this bipartition constraint, would there still be?",
            "This picture is with respect to different definitions of clusterings.",
            "Is is, is that now a very specific behavior for this type of calculation, clustering problem or or do you expect that this is Fortunately?",
            "Right, so we've done it for kind of different degree distributions with similar results.",
            "And when you write down these equations.",
            "To some extent, there they are independent of the form of the internal.",
            "Clusters the structure of the clusters.",
            "It sort of depends mostly on if you look at a particular node and you want to know which fraction of its links are inside and which fraction of its links go outside.",
            "That seems to be what makes a difference and not so much the structure of the cluster itself.",
            "Change your AI JS.",
            "I assume this AI JS is the adjacent right, right, right, OK?",
            "So if you if you replace it with the matrix which has ones and minus ones in there.",
            "Oh, that should make any difference at all.",
            "You mean just?",
            "Completely different.",
            "Ask the question, how do you?",
            "How do you publishing bipartition awaited graph with competing in the local interactions and not only are competing globally?",
            "So I'm not sure I understand, so you'd like to have like some anti ferromagnetic interactions as well.",
            "So sort of like anti links that are, well that's good question.",
            "I don't really know what would happen in that case.",
            "Correlation clustering?",
            "Showing.",
            "Yeah, so I don't know actually what the.",
            "I'll just have my head with the effect would be.",
            "Question so.",
            "Just make sure I understand that the main picture that you showed, I guess the other one.",
            "Yeah, so that's that's not information theoretic statement about if you were to choose this value of gamma Alpha and solve for the minimum cost by partition would be the expected error that you would get.",
            "So so question is that.",
            "Necessarily a sufficient statistics here?",
            "In other words, could it be that there is some other?",
            "Is the value of the cut by partition?",
            "Have sufficient is that all the information you have or could it be that if you actually went and did like a full beige and everything there would be a way to get a different cut out of it so?",
            "Maybe the question is, is there another algorithm that wouldn't have this threshold for another another same process but another optimization criteria may be hardly more complicated, or is this kind of information theoretically?",
            "I think it would be difficult to make a statement that you.",
            "It's not possible that you could do better than some other kind of.",
            "Partitioning algorithm.",
            "Won't have this this threshold.",
            "Yes, I don't know.",
            "On the other hand, you know how, how hard will it be?",
            "I mean, I mean, changing your algorithm is kind of changing what you mean by cluster anyway, so.",
            "Right, right?",
            "If you're just saying what the accuracy should be, calculations are correct.",
            "This is the maximum entropy.",
            "Prove it mathematically rigorous.",
            "Use tools by talagaaa and then you go through all these measure theory and show the fixed.",
            "Yeah, I.",
            "Could it be the case that this clustering here has the fewest edges across it?",
            "There's another partition with more edges, but for some reason is Africa is more probable?",
            "Or or yeah.",
            "Really, really, the maximum likely if you're really trying to minimize the number of edges, then I think you're right.",
            "Theoretically, this shows you can't do it.",
            "If that's really what you mean, then theoretically you can't do better than this, But if you have some other notion of partitioning where you don't really care, maybe this partition has more cuts, more edges, but you have some other like notion of what makes it good.",
            "Maybe you could do better, but I think I think most of the algorithms we normally think of really do.",
            "Try to maximize the intercluster links.",
            "The inside cluster links.",
            "So from that perspective all of those approaches are really going to suffer this same fate.",
            "Yeah.",
            "Yep.",
            "Sorry, could you say that again, it's.",
            "Increase so.",
            "Do you need?",
            "Can you reduce the function of labeled speeds?",
            "Sorry, so you want to reduce the number of how many spins you've labeled fraction the fraction.",
            "The number of points goes up.",
            "Yeah, you won't achieve the same accuracy.",
            "Can your fraction unlabeled points go down or does it stay constant?",
            "That's a question I see I see.",
            "So.",
            "I mean, in some sense, if you want to.",
            "I mean, if you want to approximate, you know this curve with you know 5 where you've labeled 5% of your notes and you go from a 10,000 node graph to 20,000.",
            "Either you have to, you know.",
            "Take label more nodes or you'll end up on a different curve, which will be you know a little bit closer down here and sort of as your fraction goes down, you'll get closer.",
            "You'll still have a smooth curve, but you'll get closer and closer to this.",
            "Threshold line yeah.",
            "Thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as I said, my name is Greg, I'm going to talk about some work I did with our men oliverian and Ram Galstian at the Information Science Institute.",
                    "label": 0
                },
                {
                    "sent": "So we're going to talk about clustering and graphs using prior information.",
                    "label": 1
                },
                {
                    "sent": "So I want to start with kind of the simplest possible.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Beginnings we have two groups of N nodes and these groups are totally disconnected, but they have some internal density of links and so any pair of nodes within a link will just be on with probability Alpha over N. OK, so so that's fine.",
                    "label": 1
                },
                {
                    "sent": "This is very easy.",
                    "label": 0
                },
                {
                    "sent": "We can always split these groups up, but now we want to make things slightly more tricky and introduce some notion of these groups overlapping.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So once you do this, one more parameter gamma which will characterize the density of links between the two different groups.",
                    "label": 0
                },
                {
                    "sent": "So a gamma over N will be the probability that a pair of nodes from different groups are connected, so clearly.",
                    "label": 0
                },
                {
                    "sent": "Intuitively, we can see that.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If gamma equals Alpha, we don't have two groups at all, and if gamma equals zero, the groups are completely separated and everything is easy.",
                    "label": 0
                },
                {
                    "sent": "So this seems like a very simple kind of definition for cluster structure, but actually even this has some problems lurking underneath the surface.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the first question is, you know, can we?",
                    "label": 0
                },
                {
                    "sent": "Can we even detect these two clusters in the case that Alpha is greater than gamma is that?",
                    "label": 1
                },
                {
                    "sent": "Is that really even possible?",
                    "label": 0
                },
                {
                    "sent": "So I have a fancy animation?",
                    "label": 0
                },
                {
                    "sent": "Get this going here.",
                    "label": 0
                },
                {
                    "sent": "So so we first I should say we can imagine.",
                    "label": 0
                },
                {
                    "sent": "Turning up gamma, putting our clusters closer together, and as we do so, we might at each step to try to partition our groups into two, and we're going to partition them into the red in the Green Group, and so at first it's simple reading.",
                    "label": 0
                },
                {
                    "sent": "Green are easily separated, and now we've started to add a few, a few links and so far everything is still fine, but we're going to see how the accuracy changes as we turn up this noise parameter until gamma equals Alpha, and at this point we really expect there's no more cluster structure at all.",
                    "label": 0
                },
                {
                    "sent": "OK, so things are looking OK for a little while, but actually we see that.",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "We get kind of a steep decline in our accuracy around this point.",
                    "label": 0
                },
                {
                    "sent": "So I should say that we're using just an approximate min cut algorithm for this little demonstration.",
                    "label": 0
                },
                {
                    "sent": "OK, so by this point already it's not really clear that we're doing any better than just a random classification.",
                    "label": 0
                },
                {
                    "sent": "Kind of things don't change much after this point, and so even though this is a long before the density of Inter cluster links has gotten big enough to wash out the cluster structure, still it doesn't seem like we can do any better than random at this point.",
                    "label": 0
                },
                {
                    "sent": "So there's actually a theoretical result by Reichhardt and Lyons that shows that there actually is such a true transition in terms of where we can detect clusters, and at this threshold is actually sharp that there really is some point beyond which, even though gamma is still less than Alpha, we actually can do no better than than random in terms of partitioning this group.",
                    "label": 0
                },
                {
                    "sent": "OK, so the motivation for work.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To say what happens if we add some prior information to this situation, some prior information about the clusters.",
                    "label": 0
                },
                {
                    "sent": "What happens to this detection threshold?",
                    "label": 0
                },
                {
                    "sent": "How does it look?",
                    "label": 0
                },
                {
                    "sent": "So there's two ways we can imagine adding.",
                    "label": 0
                },
                {
                    "sent": "In some information.",
                    "label": 0
                },
                {
                    "sent": "We could have some information about the links or over some information about the nodes themselves, so we're going to consider the latter scenario.",
                    "label": 0
                },
                {
                    "sent": "Where we have some fraction of the nodes and we are we know what the correct partition is for those.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll give a few more details about this scenario and then I want to kind of introduce some of these statistical physics tools that we use to analyze this case.",
                    "label": 0
                },
                {
                    "sent": "And then we'll I'll summarize the two kind of main results.",
                    "label": 0
                },
                {
                    "sent": "One is about unweighted graphs where we show that this detection threshold really goes away even with the tiny tiny amount of prior information.",
                    "label": 0
                },
                {
                    "sent": "And then if there's time we can talk a little bit about weighted graphs, where it seems that prior information sort of affects what gets defined as a cluster.",
                    "label": 0
                },
                {
                    "sent": "OK, so the.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Simple model.",
                    "label": 0
                },
                {
                    "sent": "Just to reiterate, we have nodes in our two different graphs and intercluster links are on with probability Alpha over an gamma is for the between cluster links.",
                    "label": 1
                },
                {
                    "sent": "So the reason we define Alpha and gamma in this way is so that the graph remains sparse.",
                    "label": 0
                },
                {
                    "sent": "That is, as the graph gets very large, we want the connectivity to remain finite.",
                    "label": 0
                },
                {
                    "sent": "So you can think of Alpha is the internal connectivity and gamma is the between cluster connectivity.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's a bunch of different algorithms, rec for partitioning groups.",
                    "label": 0
                },
                {
                    "sent": "Here's some of the common ones, like min cut where you just minimizing the weight of the cuts as you separate them into two groups, maximizing the modularity.",
                    "label": 0
                },
                {
                    "sent": "But the method that we're going to talk about is the Ising spin model, because it allows us to bring some physics tools in and for the case of equitation partitions, we should note that these methods are really all equivalent.",
                    "label": 1
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To start the physics analogy, we're going to imagine that every node has a spin associated with it.",
                    "label": 0
                },
                {
                    "sent": "Spend A plus or minus one.",
                    "label": 0
                },
                {
                    "sent": "And then spin of plus one means that we're really in Group one and a spend of minus one means we're really in Group 2.",
                    "label": 1
                },
                {
                    "sent": "So that means if we can find some configuration of spins, that's like partitioning the nodes into two groups.",
                    "label": 0
                },
                {
                    "sent": "So an edge on our graph.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is going to correspond to some link between spins and the energy of that link depends on the value of the spins, so the spins want to line up energetically.",
                    "label": 0
                },
                {
                    "sent": "That's more favorable, and it takes some work to flip one over so.",
                    "label": 0
                },
                {
                    "sent": "So we said that's a higher energy configuration and.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                },
                {
                    "sent": "We're really trying to do is minimize the energy of this whole system.",
                    "label": 0
                },
                {
                    "sent": "So for an entire graph that the energy really.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Look something like this.",
                    "label": 0
                },
                {
                    "sent": "We're just summing up over all the links and seeing what the energy is based on the configuration of different nodes.",
                    "label": 0
                },
                {
                    "sent": "So we want to add one other constraint because the lowest energy condition would clearly be all of the spins in the same direction and no partition at all.",
                    "label": 1
                },
                {
                    "sent": "So we had another constraint that we get to two groups.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So, so we're looking for the minimum energy state the ground state of the system, and we're hoping that that corresponds to some optimal partition.",
                    "label": 1
                },
                {
                    "sent": "So this.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This kind of problem is solved.",
                    "label": 0
                },
                {
                    "sent": "With what's called the cavity method in statistical physics, but it's really a version of belief propagation or message passing, so you imagine that your child nodes are all have some spins and they kind of exert a field onto their parents and then that field determines the parent node spin and then the parent passes its state up the tree and this is how we kind of find this ground state.",
                    "label": 1
                },
                {
                    "sent": "So this is an exact process for trees.",
                    "label": 0
                },
                {
                    "sent": "Obviously the graphs we're considering aren't aren't trees, but as we get large graphs, the loops are very large and so it's kind of OK. OK, oh oh equations.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So Aaron.",
                    "label": 0
                },
                {
                    "sent": "Demanded I put equations on here, but Luckily I don't really have to talk about them.",
                    "label": 0
                },
                {
                    "sent": "I guess this is sort of a field consistency equation and this is really what you solve.",
                    "label": 0
                },
                {
                    "sent": "2 to figure out.",
                    "label": 0
                },
                {
                    "sent": "How accurate your partitioning is?",
                    "label": 0
                },
                {
                    "sent": "So let me go to this.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So really what we're calculating is the magnetization of the two clusters, and that's just sort of the notion that in one of the clusters all the spins are lining up there, magnetized in One Direction, and insofar as that's happened, then we've succeeded.",
                    "label": 0
                },
                {
                    "sent": "We have classified this cluster correctly is all one spin.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Little bit more of the physics analogy.",
                    "label": 0
                },
                {
                    "sent": "Mechanization is really considered like an order parameter.",
                    "label": 1
                },
                {
                    "sent": "There's some phase where it's completely disordered and there's no magnetisation, and that really corresponds to the regime where we can't detect detect anything.",
                    "label": 0
                },
                {
                    "sent": "Any clusters at all, so OK. OK, so so.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here I have a graph of the accuracy that you could detect this partition.",
                    "label": 0
                },
                {
                    "sent": "As a so we fixed the internal connectivity of the two graphs and now the X axis were just turning up the intergroup connectivity and we're getting closer and closer, and you see that really, we do get this.",
                    "label": 0
                },
                {
                    "sent": "This sharp threshold and that it comes long before gamma equals Alpha.",
                    "label": 0
                },
                {
                    "sent": "And after that just completely random and you can't partition it all.",
                    "label": 0
                },
                {
                    "sent": "So you.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ask where where this point comes.",
                    "label": 0
                },
                {
                    "sent": "Four different values of Alpha and gamma, so that's what's depicted here.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "So on this side of the line we have.",
                    "label": 0
                },
                {
                    "sent": "The the space where we can detect cluster structure and on that side we can't at all as Alpha and gamma get big.",
                    "label": 0
                },
                {
                    "sent": "That really means that the overall connectivity of the graph is getting very big as well.",
                    "label": 0
                },
                {
                    "sent": "So we're not.",
                    "label": 0
                },
                {
                    "sent": "We no longer have sparse graphs in that case and.",
                    "label": 0
                },
                {
                    "sent": "Alpha gets closer and closer to gamma, the threshold gets closer, so as we get to non sparse graphs.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we want to add in some supervision some prior information.",
                    "label": 0
                },
                {
                    "sent": "So we imagine that there's some random fraction of the nodes that we have the labels for, and so to put this into our Hamiltonian is very simple.",
                    "label": 1
                },
                {
                    "sent": "We just put an energy penalty for incorrectly labeling those spins that we now those nodes that we know from physics perspective.",
                    "label": 0
                },
                {
                    "sent": "This is still really simple Hamiltonian, so so we can still solve this.",
                    "label": 0
                },
                {
                    "sent": "Using our previous method.",
                    "label": 0
                },
                {
                    "sent": "OK, So what changes?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As as we.",
                    "label": 0
                },
                {
                    "sent": "Start labeling some of these pins so.",
                    "label": 0
                },
                {
                    "sent": "Kind of switched things around here, so now we're holding the intercluster connectivity fix and we're increasing the connectivity within the cluster.",
                    "label": 0
                },
                {
                    "sent": "So, so this blue curve corresponds to what we talked about before, where we have no supervision and there's some sharp threshold.",
                    "label": 0
                },
                {
                    "sent": "And that's all the same.",
                    "label": 0
                },
                {
                    "sent": "But what's different?",
                    "label": 0
                },
                {
                    "sent": "These two curves that are for some amount of supervision.",
                    "label": 0
                },
                {
                    "sent": "So here we've labeled 5% of the nodes, and here 20%, and we see that there's a fundamental change in the behavior of this detection threshold.",
                    "label": 0
                },
                {
                    "sent": "Now there's no more sharp threshold.",
                    "label": 0
                },
                {
                    "sent": "Actually, even as the intercluster connectivity is almost equal to the intra cluster connectivity, we still have some better than random chance of partitioning our nodes correctly.",
                    "label": 1
                },
                {
                    "sent": "So one other kind of point I guess I wanted to make.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But this is in terms of the stability of an inference algorithm, right?",
                    "label": 0
                },
                {
                    "sent": "So if you were doing a unsupervised inference on these clusters and it just happened that your Inter cluster density was near this threshold, clearly you'd have a fairly unstable algorithm, right?",
                    "label": 0
                },
                {
                    "sent": "Because just shifting over a little bit to the right or the left will have a really big effects on what you can classify as a cluster, whereas if you have even a tiny amount of supervision.",
                    "label": 0
                },
                {
                    "sent": "Your performance will degrade very gracefully, so it's quite a difference in how those algorithms would proceed.",
                    "label": 0
                },
                {
                    "sent": "So we also did this experimentally for graphs of 10,000 nodes and using simulated annealing and the results are very similar.",
                    "label": 1
                },
                {
                    "sent": "OK. Power lies.",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter what kind of graph you do.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is with.",
                    "label": 0
                },
                {
                    "sent": "It all is very similar.",
                    "label": 0
                },
                {
                    "sent": "So now in my last few minutes I wanted to talk to us.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About weighted graphs.",
                    "label": 0
                },
                {
                    "sent": "So if you consider this situation where you have one node that's connected with.",
                    "label": 0
                },
                {
                    "sent": "Small weights, but many links to one group but one, but I have the link to another group.",
                    "label": 0
                },
                {
                    "sent": "Clearly there's a lot of, there's a lot of ambiguity and how we should cluster this particular node, and you can come up with a lot of different ways that make sense that would classify it in different groups.",
                    "label": 0
                },
                {
                    "sent": "If you threshold the weights, then the weights I'll go away and you put it in Group 2.",
                    "label": 0
                },
                {
                    "sent": "If you neglect the weights, it has more on Group One, so you'd classify it that way from a min cut perspective.",
                    "label": 0
                },
                {
                    "sent": "Cutting either of those is the same, so it could go either way.",
                    "label": 0
                },
                {
                    "sent": "But now what happens when we when we add supervision to this?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem so.",
                    "label": 0
                },
                {
                    "sent": "So now we're going to consider the case where are.",
                    "label": 0
                },
                {
                    "sent": "Within clusters we have many but weaker links and between clusters we have heavier but fewer links.",
                    "label": 0
                },
                {
                    "sent": "So intuitively, like from a min cut perspective.",
                    "label": 0
                },
                {
                    "sent": "If if Alpha is twice gamma.",
                    "label": 0
                },
                {
                    "sent": "Then that should be kind of.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The point where clusters disappear right?",
                    "label": 0
                },
                {
                    "sent": "And because then cutting through the middle or cutting inside should be about the same thing.",
                    "label": 0
                },
                {
                    "sent": "Are for unsupervised detection that the threshold is still much bigger than what we should theoretically be able to get from min cut, which is just like before where Alpha equals gamma.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So when we add supervision to this picture, the things change a little bit.",
                    "label": 0
                },
                {
                    "sent": "So first of all.",
                    "label": 0
                },
                {
                    "sent": "Where the detection threshold is depends on row.",
                    "label": 0
                },
                {
                    "sent": "Now for this particular case, the threshold actually get smaller or better for smaller amounts of supervision, but smaller but nonzero amounts of supervision, and for this case the threshold actually goes to 1.5 gamma, which is even lower than what we have expected from this min cut threshold.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "To give kind of a. Intuition for why that should be.",
                    "label": 0
                },
                {
                    "sent": "I think what's really happening is if you have only tiny amounts of supervision then.",
                    "label": 0
                },
                {
                    "sent": "You're more likely with numerous but weaker links to hit one of the labeled nodes and classify yourself correctly.",
                    "label": 0
                },
                {
                    "sent": "So in this case, by adding that kind of supervision, tiny amounts you've sort of biased yourself towards.",
                    "label": 0
                },
                {
                    "sent": "Tord's weaker.",
                    "label": 0
                },
                {
                    "sent": "But more numerous links.",
                    "label": 0
                },
                {
                    "sent": "OK, so just to summarize, so we showed that you know for Spa.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Graphs we really have some detection threshold beyond which you can really detect any partitions, but even a little bit of a prior information can eliminate that threshold or move it to its smallest possible value.",
                    "label": 1
                },
                {
                    "sent": "And for weighted graphs.",
                    "label": 0
                },
                {
                    "sent": "The threshold depends on the prior information that you have and it even kind of affects your notion.",
                    "label": 1
                },
                {
                    "sent": "What a cluster should be OK, thanks.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Sure.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "A lot depends on the fact that you had a second order phase transition in your analysis.",
                    "label": 0
                },
                {
                    "sent": "For example, the smearing out and so on.",
                    "label": 0
                },
                {
                    "sent": "Is that a generic behavior for these types of problems?",
                    "label": 0
                },
                {
                    "sent": "Because it looks like that you are essentially solving Updike relation problem with an external global constraint despite partitioning constraint is global and then you have this disorder from the productivity of your graph.",
                    "label": 0
                },
                {
                    "sent": "Which is basically positive, right?",
                    "label": 0
                },
                {
                    "sent": "So you're wondering if.",
                    "label": 0
                },
                {
                    "sent": "If we didn't have this, this bipartition constraint, would there still be?",
                    "label": 0
                },
                {
                    "sent": "This picture is with respect to different definitions of clusterings.",
                    "label": 0
                },
                {
                    "sent": "Is is, is that now a very specific behavior for this type of calculation, clustering problem or or do you expect that this is Fortunately?",
                    "label": 0
                },
                {
                    "sent": "Right, so we've done it for kind of different degree distributions with similar results.",
                    "label": 0
                },
                {
                    "sent": "And when you write down these equations.",
                    "label": 0
                },
                {
                    "sent": "To some extent, there they are independent of the form of the internal.",
                    "label": 0
                },
                {
                    "sent": "Clusters the structure of the clusters.",
                    "label": 0
                },
                {
                    "sent": "It sort of depends mostly on if you look at a particular node and you want to know which fraction of its links are inside and which fraction of its links go outside.",
                    "label": 0
                },
                {
                    "sent": "That seems to be what makes a difference and not so much the structure of the cluster itself.",
                    "label": 0
                },
                {
                    "sent": "Change your AI JS.",
                    "label": 0
                },
                {
                    "sent": "I assume this AI JS is the adjacent right, right, right, OK?",
                    "label": 0
                },
                {
                    "sent": "So if you if you replace it with the matrix which has ones and minus ones in there.",
                    "label": 0
                },
                {
                    "sent": "Oh, that should make any difference at all.",
                    "label": 0
                },
                {
                    "sent": "You mean just?",
                    "label": 0
                },
                {
                    "sent": "Completely different.",
                    "label": 0
                },
                {
                    "sent": "Ask the question, how do you?",
                    "label": 0
                },
                {
                    "sent": "How do you publishing bipartition awaited graph with competing in the local interactions and not only are competing globally?",
                    "label": 0
                },
                {
                    "sent": "So I'm not sure I understand, so you'd like to have like some anti ferromagnetic interactions as well.",
                    "label": 0
                },
                {
                    "sent": "So sort of like anti links that are, well that's good question.",
                    "label": 0
                },
                {
                    "sent": "I don't really know what would happen in that case.",
                    "label": 0
                },
                {
                    "sent": "Correlation clustering?",
                    "label": 0
                },
                {
                    "sent": "Showing.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I don't know actually what the.",
                    "label": 0
                },
                {
                    "sent": "I'll just have my head with the effect would be.",
                    "label": 0
                },
                {
                    "sent": "Question so.",
                    "label": 0
                },
                {
                    "sent": "Just make sure I understand that the main picture that you showed, I guess the other one.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so that's that's not information theoretic statement about if you were to choose this value of gamma Alpha and solve for the minimum cost by partition would be the expected error that you would get.",
                    "label": 0
                },
                {
                    "sent": "So so question is that.",
                    "label": 0
                },
                {
                    "sent": "Necessarily a sufficient statistics here?",
                    "label": 0
                },
                {
                    "sent": "In other words, could it be that there is some other?",
                    "label": 0
                },
                {
                    "sent": "Is the value of the cut by partition?",
                    "label": 0
                },
                {
                    "sent": "Have sufficient is that all the information you have or could it be that if you actually went and did like a full beige and everything there would be a way to get a different cut out of it so?",
                    "label": 0
                },
                {
                    "sent": "Maybe the question is, is there another algorithm that wouldn't have this threshold for another another same process but another optimization criteria may be hardly more complicated, or is this kind of information theoretically?",
                    "label": 0
                },
                {
                    "sent": "I think it would be difficult to make a statement that you.",
                    "label": 0
                },
                {
                    "sent": "It's not possible that you could do better than some other kind of.",
                    "label": 0
                },
                {
                    "sent": "Partitioning algorithm.",
                    "label": 0
                },
                {
                    "sent": "Won't have this this threshold.",
                    "label": 0
                },
                {
                    "sent": "Yes, I don't know.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, you know how, how hard will it be?",
                    "label": 0
                },
                {
                    "sent": "I mean, I mean, changing your algorithm is kind of changing what you mean by cluster anyway, so.",
                    "label": 0
                },
                {
                    "sent": "Right, right?",
                    "label": 0
                },
                {
                    "sent": "If you're just saying what the accuracy should be, calculations are correct.",
                    "label": 0
                },
                {
                    "sent": "This is the maximum entropy.",
                    "label": 0
                },
                {
                    "sent": "Prove it mathematically rigorous.",
                    "label": 0
                },
                {
                    "sent": "Use tools by talagaaa and then you go through all these measure theory and show the fixed.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I.",
                    "label": 0
                },
                {
                    "sent": "Could it be the case that this clustering here has the fewest edges across it?",
                    "label": 0
                },
                {
                    "sent": "There's another partition with more edges, but for some reason is Africa is more probable?",
                    "label": 0
                },
                {
                    "sent": "Or or yeah.",
                    "label": 0
                },
                {
                    "sent": "Really, really, the maximum likely if you're really trying to minimize the number of edges, then I think you're right.",
                    "label": 0
                },
                {
                    "sent": "Theoretically, this shows you can't do it.",
                    "label": 0
                },
                {
                    "sent": "If that's really what you mean, then theoretically you can't do better than this, But if you have some other notion of partitioning where you don't really care, maybe this partition has more cuts, more edges, but you have some other like notion of what makes it good.",
                    "label": 0
                },
                {
                    "sent": "Maybe you could do better, but I think I think most of the algorithms we normally think of really do.",
                    "label": 0
                },
                {
                    "sent": "Try to maximize the intercluster links.",
                    "label": 0
                },
                {
                    "sent": "The inside cluster links.",
                    "label": 0
                },
                {
                    "sent": "So from that perspective all of those approaches are really going to suffer this same fate.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Sorry, could you say that again, it's.",
                    "label": 0
                },
                {
                    "sent": "Increase so.",
                    "label": 0
                },
                {
                    "sent": "Do you need?",
                    "label": 0
                },
                {
                    "sent": "Can you reduce the function of labeled speeds?",
                    "label": 0
                },
                {
                    "sent": "Sorry, so you want to reduce the number of how many spins you've labeled fraction the fraction.",
                    "label": 0
                },
                {
                    "sent": "The number of points goes up.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you won't achieve the same accuracy.",
                    "label": 0
                },
                {
                    "sent": "Can your fraction unlabeled points go down or does it stay constant?",
                    "label": 0
                },
                {
                    "sent": "That's a question I see I see.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I mean, in some sense, if you want to.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you want to approximate, you know this curve with you know 5 where you've labeled 5% of your notes and you go from a 10,000 node graph to 20,000.",
                    "label": 0
                },
                {
                    "sent": "Either you have to, you know.",
                    "label": 0
                },
                {
                    "sent": "Take label more nodes or you'll end up on a different curve, which will be you know a little bit closer down here and sort of as your fraction goes down, you'll get closer.",
                    "label": 0
                },
                {
                    "sent": "You'll still have a smooth curve, but you'll get closer and closer to this.",
                    "label": 0
                },
                {
                    "sent": "Threshold line yeah.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                }
            ]
        }
    }
}