{
    "id": "l5rqimabhwv4y5wwh7qrtcryafezpbpp",
    "title": "Scaling Parallel Rule-based Reasoning",
    "info": {
        "author": [
            "Martin Peters, Fachhochschule Dortmund"
        ],
        "published": "July 30, 2014",
        "recorded": "May 2014",
        "category": [
            "Top->Computer Science->Semantic Web",
            "Top->Computer Science->Big Data"
        ]
    },
    "url": "http://videolectures.net/eswc2014_peters_rule_based/",
    "segmentation": [
        [
            "So my name is Martin Peters.",
            "I'm from University of Applied Science and Arts in Dortmund, Germany and my talks about scaling parallel rule based reasoning which is a joint work of Christopher subpoena, Albert and me so them."
        ],
        [
            "Motivation behind our work said reasoning is still one key feature when using ontologies and it basically means to create new knowledge by inferring facts that are implicitly given by by the existing data and the ontology language that you find which triple shall be deferred can often be described in Rule based way.",
            "Root root sets that are used are for example zero DF ruleset, which is basically a subset of RDS RDS itself or PD star, which is also known as old host.",
            "Of course, also application specific rules are possible.",
            "Even there popped up quite a few approaches for parallel reasoning to speed up in scales.",
            "Reasoning process reasoning can still be a very time consuming task, which mainly depends on the data set that is used.",
            "So the size of the data set as well as on the use Ontology language.",
            "In a."
        ],
        [
            "Our previous work called Rule Based Reasoning or massively parallel hardware.",
            "We introduced forward chaining RTI based implementation of reason as advance on massively parallel hardware.",
            "So it basically run uses a parallel architecture of graphic card so of a GPU and unlike many other reasons it does not depend on a specific kind of semantic but instead you can define some rules in a root file and input the rule file together with your RDF data to the reason and the reason is going to use the GPU.",
            "To process the data.",
            "As we showed in the paper or approach already showed, a pretty good performance for rulesets like our DFS and Old Horst.",
            "Nevertheless there."
        ],
        [
            "Also, some limitations of our approach.",
            "One of the limitation was that the size of the process of the data was limited by the amount of available memory of the use GPU.",
            "That was big cause we had to load all of the semantic data available to the main memory of the GPU to be able to process this Treaty algorithm.",
            "Another limitation exists by the use of only a single GPU, so we were not able to scale our approach in terms of adding multiple GPU's.",
            "And we figured out that for what some rule sets, the rule firing was one of the most time consuming tasks.",
            "So the question."
        ],
        [
            "How's how to overcome these limitations?",
            "And that's the topic I'm going to talk about today to do so, I'm first going to introduce workload partitioning, which basically means that I'm going to show how the workload can be partitioned into smaller chunks that then can be processed independently and be cause they can be processed independently.",
            "They can also display distributed to multiple devices, so we can.",
            "We can use multiple GPU's for the reasoning process, and we're also able to size.",
            "The chunks with respect to the target device.",
            "So if you've got a GPU with much memory, you can choose a bigger chunk size.",
            "I'm also going to show how to paralyze the rule firing, which basically means that we are inferring the new facts also on the GPU, and based on this we also introduced a concept to reduce the invalid triples.",
            "Invalid triples are, for example duplicates that get infer during the reasoning process or triples holding literal as a subject.",
            "By the way, during the talk I'm often going to use the.",
            "I'm going to talk about the device and the host, and if I'm going to talk about the device, I'm usually referring to the GPU and the host on the other side is the outer logic, which runs is for example, a simple Java application on your CPU.",
            "So before I can start to introduce a new concept, I first have to to introduce the basic."
        ],
        [
            "Pretty algorithm, and to do so I've got a very simple rule based here and I'm going to show a small example how to use Rita algorithm.",
            "With this rules.",
            "The first step is to create a routine network and to create a region network.",
            "We first have to create Alpha nodes and each Alpha node exactly corresponds to one rule term, and as you can see we've got rule number one with only a single term and rule #2 with two rule terms and two creates Alpha nodes.",
            "We create the first Alpha node by using the first root him of rule #1, which is equal to the rule term.",
            "The first rule terminal #2, so we're only info 1A node.",
            "Now we've got one rule term left and it's going going to be our second Alpha node and now we created all of our Alpha nodes.",
            "Furthermore, we have to create better nodes and beta note always has exactly two parent nodes and thus combines two other nodes.",
            "And in this case we have to connect A1A two to be able to map rule #2 to a single node and this will be our beta one node.",
            "So now we created our routine network and we can start to iterate our input data through the network and to iterate it through the network.",
            "I've got three input triples and we first start with the Alpha matching and the Alpha matching.",
            "Basically means that we have to match all of our input triples against all of the Alpha nodes of our reaching network, and for A1.",
            "That is quite easy because you can see that.",
            "Rule term which A1 corresponds to a snow condition.",
            "So basically every triple is going to match these condition.",
            "So we're creating a working memory and in this working memory we're storing references to all of our triples.",
            "For A2 there is a condition and the condition says that the predicate meet needs to be in our DFS domain, and that's the case for triple #3.",
            "So we are creating a working memory, holding a reference to triple #3.",
            "So that was the Alpha matching.",
            "Now we can continue with the beta matching and better matching means that we have to match all of the items that are contained in working memory of one of the parent node against all of the items that are contained in.",
            "The other parent node.",
            "And the working memory of the other parent node and this will result in this working memory.",
            "So we've got basically two matches where each match is a combination of one of the items of the parent nodes.",
            "Now we also finished the beta matching and we can start to fire the rules and to fire the rules.",
            "We're going to start with rule #1.",
            "And we're using the working memory of A1 to fire this rule, and we basically simply use this instructions to get the new triples, and we get 3 new triples where one of them is a duplicate.",
            "So we're going to ignore it.",
            "For rule #2 we have to use the working memory of beta one.",
            "And we also get 2 new triples.",
            "So basically, after rule firing we've got 4 new triples in our triple base.",
            "With this for New Triples would now continue doing the Alpha matching again, doing better matching again, and fires rules until no more triples can be derived.",
            "So this is the whole RTI algorithm process."
        ],
        [
            "So the question now is, how can we parallelize this targeting massively parallel hardware?",
            "And if I'm talking about massively parallel hardware, we're not talking about 10 or 20 of parallel threads, but in the best case about millions of parallel threads that can be executed."
        ],
        [
            "For Alpha matching we create one triple for every input, one thread for every input triple and this thread then iterates through all of the Alpha nodes to check if the triple corresponds to the condition of the Alpha node, and if so it adds an entry to the working memory of the Alpha node.",
            "For better matching, this is quite similar.",
            "Instead we're using as working memories of the parent nodes and so for example, for this case we would create a thread for each of these items in the working memory of A1, and those threads would iterate through all of the items in the working memory of A2 and check if the combination of both match the condition of beta one, and if so, add an entry to the working memory of beta one.",
            "So the question now is how we can distribute this workload to multiple devices and size the workload so that it fits into the working into the main memory of a GPU.",
            "Anne."
        ],
        [
            "For the Alpha matching, this is quite easy because we can simply divide the workload into smaller chunks, so each chunk we can, we can compute or process the input triples independently of each other, and that allows us to simply divide the workload and distribute it to multiple devices."
        ],
        [
            "For the beta matching, this is a little bit more complex because during the beta matching we have to use the working memories and the working memory is only hold a reference to the triples and that means that we have to resolve these references and to be able to do that on the GPU.",
            "We also have to load all of our triples to the main memory of the GPU and that is basically what limits the size of processor data processable data on the amount of triples that fit into the main memory of the GPU.",
            "So to overcome this limitation where introducing a triple match, which is the data structure not holding only is reference but also the triple itself.",
            "And using these triple matches.",
            "Um?"
        ],
        [
            "We can transform the working memories to a list of triple matches and then use the triple matches on the GPU for beta matching, for example.",
            "This also allows us to divide the working memories into smaller chunks, and these chunks then can be processed independently on the one side, then can be they can be processed on a GPU independently, but on the other side we can also on the host side, use multithreading to prepare these chunks.",
            "So the transformation from the working memories to a list of triple matches can be done in parallel to.",
            "And because we are using triple matches journey to be no more references to be resolved and well, we can process an arbitrary number of triples."
        ],
        [
            "Um, another problem we figured out was rule firing, and as I said before we are.",
            "We have paralyzed rule firing two and to do so.",
            "We are also using the working memories that need to be transformed to true matches again, and we can then transfer these triple matches together with some instructions.",
            "Housing New triples are derived to the GPU and infers and you fix on the GPU, so if we've got the working memories for beta one for example with two entries we could handle them independently and transform the chunks to triple matches, then loaded to the GPU.",
            "The GPU would create our new triples.",
            "And they would have been two transformed, transferred back to the host and there they could be submitted to the internal Triple Store and the triple stores then would be responsible to reject invalid triples like duplicates, for example.",
            "We also introduced a simple concept based on the transformation to triple matches to reduce the amount of inverted triples that are inferred, but due to time issues I have to refer to the paper for more details on this point.",
            "Um, to test out."
        ],
        [
            "Implement or new concepts.",
            "We use implementation from our previous work, which is the Java based implementation using open CL.",
            "And here you can see a part of our architecture and on the left side you can see different chunks that were divided and each of these chunks now can be processed by a single thread on our in our Java application.",
            "So these chunks are working memories and they need to be transferred to triple matches.",
            "And after it's Red did the transformation.",
            "It can be submitted to a Q and on the other side we've got working threads.",
            "And there's one thread for every GPU.",
            "And.",
            "Well, because we've got one thread for every GPU, and we're using this Q.",
            "We can simply scale our approach to use multiple devices so we can easily add more GPUs at this site and just scale the hardware.",
            "Um?"
        ],
        [
            "For our tests, we use two different data sets.",
            "One is University benchmark data set where we can generate it up to 8000 universities which are about 1 billion triples, and we use the DB PEDIA data data set with scale to different sizes.",
            "We also used Ubuntu Workstation with six core processor, 64 gigabytes of memory and 2MG gaming graphic cards holding 3 gigabytes of memory each.",
            "1st."
        ],
        [
            "All we wanted to see how the parallel implementation of rule firing, including the reduction of inverted triples, performs and here you can see the detailed reasoning time for applying the RDF's rule set.",
            "On the left side or on the left bar you can see the serial implementation of rule firing.",
            "In the middle.",
            "The parallel implementation and on the right side we also applied the reduction concept, and as you can see the problem implementation is much faster than the serial one and using reduction concept we could also improve the performance of about 10 more person.",
            "And this applies to both data sets.",
            "So the University data set as well as the PEDIA data set both improved.",
            "The reasoning was much faster with the parallel concepts for both data sets."
        ],
        [
            "Furthermore, we wanted to see the impact of the parallelization of our concepts.",
            "So on the first side, the parallelization using the chunks and preparing the chunks in parallel on the host side and on the other side using multiple GPUs.",
            "To do so, we executed our reasoner first using a non threaded version, which means that each chunk was prepared and processed one after the other and then submitted to a single GPU.",
            "The second bar shows a threaded version.",
            "We where we prepared the workloads in parallel, but submitted also everything to a single GPU.",
            "And finally, we also used two GPU's.",
            "And for ADFS, you can see it's mainly benefits from adding the first level of parallelization, which is this Reddit one that is becausw the workload on the GPU is not that high for RDF's.",
            "On the other side, you can see the PD star rules set, which is a much more complex rulesets becausw.",
            "They need to be computed much more joints, and these joints introduce a heavy workload on the GPU, and when we add a second GPU here for this rule set.",
            "We can gain really a high performance improvement.",
            "Um?",
            "So."
        ],
        [
            "Finally, we wanted to see how our system performs with really large data sets and to do so we had to use a different hardware because we ran out of memory.",
            "So we used a cloud server with two NVIDIA gaming graphic cards and 192 gigabytes of memory and we used the University data sets was up to 1.1 billion triples and applied ODF as well as our DFS rulesets.",
            "And first of all you can see that our approach scales nearly in a linear way, which is pretty good.",
            "And we were able to reach through part of 2.7 million triples a second, which is quite fast, I think.",
            "For comparison, web Pi, for example, reported through part of 2.1 billion triples a second on 64 computing nodes.",
            "While we only use a single computing node equipped with two gaming graphic cards and we reached an even higher thruput.",
            "Nevertheless, of course, we have to admit that website is still able to handle much larger data sets and we are able to.",
            "For RDF S, We reached a through part of 1.4 million triples a second."
        ],
        [
            "OK, coming to the end of my presentation we showed how to parallelize RTI algorithm for semantic reasoning in ways that the preparation of workload can be performed in parallel using simple multithreading and how to distribute the workload to multiple GPU's so we can not only use the massively parallel hardware on the single GPU but of multiple GPU's.",
            "And as we did so, we were able to perform the reasoning on a data set with more than one billion triples in about 430 seconds for the rodeo rules set and in 1200 seconds for the DFS ruleset.",
            "Nevertheless, we reached a new limitation, which now is the main memory of the of the computing node itself, and that is also why our future work will include the investigation of concepts that can be integrated to reduce the main memory usage of Sereti algorithm.",
            "And it would also be interesting to see not only to distribute the workload to multiple GPUs, but also to multiple computing nodes that can be equipped with multiple GPU's.",
            "Um, yes."
        ],
        [
            "Thank you for attention.",
            "OK, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So my name is Martin Peters.",
                    "label": 0
                },
                {
                    "sent": "I'm from University of Applied Science and Arts in Dortmund, Germany and my talks about scaling parallel rule based reasoning which is a joint work of Christopher subpoena, Albert and me so them.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Motivation behind our work said reasoning is still one key feature when using ontologies and it basically means to create new knowledge by inferring facts that are implicitly given by by the existing data and the ontology language that you find which triple shall be deferred can often be described in Rule based way.",
                    "label": 1
                },
                {
                    "sent": "Root root sets that are used are for example zero DF ruleset, which is basically a subset of RDS RDS itself or PD star, which is also known as old host.",
                    "label": 0
                },
                {
                    "sent": "Of course, also application specific rules are possible.",
                    "label": 0
                },
                {
                    "sent": "Even there popped up quite a few approaches for parallel reasoning to speed up in scales.",
                    "label": 1
                },
                {
                    "sent": "Reasoning process reasoning can still be a very time consuming task, which mainly depends on the data set that is used.",
                    "label": 0
                },
                {
                    "sent": "So the size of the data set as well as on the use Ontology language.",
                    "label": 0
                },
                {
                    "sent": "In a.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our previous work called Rule Based Reasoning or massively parallel hardware.",
                    "label": 0
                },
                {
                    "sent": "We introduced forward chaining RTI based implementation of reason as advance on massively parallel hardware.",
                    "label": 1
                },
                {
                    "sent": "So it basically run uses a parallel architecture of graphic card so of a GPU and unlike many other reasons it does not depend on a specific kind of semantic but instead you can define some rules in a root file and input the rule file together with your RDF data to the reason and the reason is going to use the GPU.",
                    "label": 0
                },
                {
                    "sent": "To process the data.",
                    "label": 1
                },
                {
                    "sent": "As we showed in the paper or approach already showed, a pretty good performance for rulesets like our DFS and Old Horst.",
                    "label": 0
                },
                {
                    "sent": "Nevertheless there.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Also, some limitations of our approach.",
                    "label": 0
                },
                {
                    "sent": "One of the limitation was that the size of the process of the data was limited by the amount of available memory of the use GPU.",
                    "label": 1
                },
                {
                    "sent": "That was big cause we had to load all of the semantic data available to the main memory of the GPU to be able to process this Treaty algorithm.",
                    "label": 0
                },
                {
                    "sent": "Another limitation exists by the use of only a single GPU, so we were not able to scale our approach in terms of adding multiple GPU's.",
                    "label": 1
                },
                {
                    "sent": "And we figured out that for what some rule sets, the rule firing was one of the most time consuming tasks.",
                    "label": 0
                },
                {
                    "sent": "So the question.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How's how to overcome these limitations?",
                    "label": 1
                },
                {
                    "sent": "And that's the topic I'm going to talk about today to do so, I'm first going to introduce workload partitioning, which basically means that I'm going to show how the workload can be partitioned into smaller chunks that then can be processed independently and be cause they can be processed independently.",
                    "label": 1
                },
                {
                    "sent": "They can also display distributed to multiple devices, so we can.",
                    "label": 1
                },
                {
                    "sent": "We can use multiple GPU's for the reasoning process, and we're also able to size.",
                    "label": 0
                },
                {
                    "sent": "The chunks with respect to the target device.",
                    "label": 0
                },
                {
                    "sent": "So if you've got a GPU with much memory, you can choose a bigger chunk size.",
                    "label": 0
                },
                {
                    "sent": "I'm also going to show how to paralyze the rule firing, which basically means that we are inferring the new facts also on the GPU, and based on this we also introduced a concept to reduce the invalid triples.",
                    "label": 0
                },
                {
                    "sent": "Invalid triples are, for example duplicates that get infer during the reasoning process or triples holding literal as a subject.",
                    "label": 0
                },
                {
                    "sent": "By the way, during the talk I'm often going to use the.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about the device and the host, and if I'm going to talk about the device, I'm usually referring to the GPU and the host on the other side is the outer logic, which runs is for example, a simple Java application on your CPU.",
                    "label": 0
                },
                {
                    "sent": "So before I can start to introduce a new concept, I first have to to introduce the basic.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pretty algorithm, and to do so I've got a very simple rule based here and I'm going to show a small example how to use Rita algorithm.",
                    "label": 0
                },
                {
                    "sent": "With this rules.",
                    "label": 0
                },
                {
                    "sent": "The first step is to create a routine network and to create a region network.",
                    "label": 0
                },
                {
                    "sent": "We first have to create Alpha nodes and each Alpha node exactly corresponds to one rule term, and as you can see we've got rule number one with only a single term and rule #2 with two rule terms and two creates Alpha nodes.",
                    "label": 0
                },
                {
                    "sent": "We create the first Alpha node by using the first root him of rule #1, which is equal to the rule term.",
                    "label": 0
                },
                {
                    "sent": "The first rule terminal #2, so we're only info 1A node.",
                    "label": 0
                },
                {
                    "sent": "Now we've got one rule term left and it's going going to be our second Alpha node and now we created all of our Alpha nodes.",
                    "label": 0
                },
                {
                    "sent": "Furthermore, we have to create better nodes and beta note always has exactly two parent nodes and thus combines two other nodes.",
                    "label": 0
                },
                {
                    "sent": "And in this case we have to connect A1A two to be able to map rule #2 to a single node and this will be our beta one node.",
                    "label": 0
                },
                {
                    "sent": "So now we created our routine network and we can start to iterate our input data through the network and to iterate it through the network.",
                    "label": 0
                },
                {
                    "sent": "I've got three input triples and we first start with the Alpha matching and the Alpha matching.",
                    "label": 0
                },
                {
                    "sent": "Basically means that we have to match all of our input triples against all of the Alpha nodes of our reaching network, and for A1.",
                    "label": 0
                },
                {
                    "sent": "That is quite easy because you can see that.",
                    "label": 0
                },
                {
                    "sent": "Rule term which A1 corresponds to a snow condition.",
                    "label": 0
                },
                {
                    "sent": "So basically every triple is going to match these condition.",
                    "label": 0
                },
                {
                    "sent": "So we're creating a working memory and in this working memory we're storing references to all of our triples.",
                    "label": 0
                },
                {
                    "sent": "For A2 there is a condition and the condition says that the predicate meet needs to be in our DFS domain, and that's the case for triple #3.",
                    "label": 0
                },
                {
                    "sent": "So we are creating a working memory, holding a reference to triple #3.",
                    "label": 0
                },
                {
                    "sent": "So that was the Alpha matching.",
                    "label": 0
                },
                {
                    "sent": "Now we can continue with the beta matching and better matching means that we have to match all of the items that are contained in working memory of one of the parent node against all of the items that are contained in.",
                    "label": 0
                },
                {
                    "sent": "The other parent node.",
                    "label": 0
                },
                {
                    "sent": "And the working memory of the other parent node and this will result in this working memory.",
                    "label": 0
                },
                {
                    "sent": "So we've got basically two matches where each match is a combination of one of the items of the parent nodes.",
                    "label": 0
                },
                {
                    "sent": "Now we also finished the beta matching and we can start to fire the rules and to fire the rules.",
                    "label": 0
                },
                {
                    "sent": "We're going to start with rule #1.",
                    "label": 0
                },
                {
                    "sent": "And we're using the working memory of A1 to fire this rule, and we basically simply use this instructions to get the new triples, and we get 3 new triples where one of them is a duplicate.",
                    "label": 0
                },
                {
                    "sent": "So we're going to ignore it.",
                    "label": 0
                },
                {
                    "sent": "For rule #2 we have to use the working memory of beta one.",
                    "label": 0
                },
                {
                    "sent": "And we also get 2 new triples.",
                    "label": 0
                },
                {
                    "sent": "So basically, after rule firing we've got 4 new triples in our triple base.",
                    "label": 0
                },
                {
                    "sent": "With this for New Triples would now continue doing the Alpha matching again, doing better matching again, and fires rules until no more triples can be derived.",
                    "label": 0
                },
                {
                    "sent": "So this is the whole RTI algorithm process.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the question now is, how can we parallelize this targeting massively parallel hardware?",
                    "label": 0
                },
                {
                    "sent": "And if I'm talking about massively parallel hardware, we're not talking about 10 or 20 of parallel threads, but in the best case about millions of parallel threads that can be executed.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For Alpha matching we create one triple for every input, one thread for every input triple and this thread then iterates through all of the Alpha nodes to check if the triple corresponds to the condition of the Alpha node, and if so it adds an entry to the working memory of the Alpha node.",
                    "label": 1
                },
                {
                    "sent": "For better matching, this is quite similar.",
                    "label": 0
                },
                {
                    "sent": "Instead we're using as working memories of the parent nodes and so for example, for this case we would create a thread for each of these items in the working memory of A1, and those threads would iterate through all of the items in the working memory of A2 and check if the combination of both match the condition of beta one, and if so, add an entry to the working memory of beta one.",
                    "label": 0
                },
                {
                    "sent": "So the question now is how we can distribute this workload to multiple devices and size the workload so that it fits into the working into the main memory of a GPU.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the Alpha matching, this is quite easy because we can simply divide the workload into smaller chunks, so each chunk we can, we can compute or process the input triples independently of each other, and that allows us to simply divide the workload and distribute it to multiple devices.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For the beta matching, this is a little bit more complex because during the beta matching we have to use the working memories and the working memory is only hold a reference to the triples and that means that we have to resolve these references and to be able to do that on the GPU.",
                    "label": 0
                },
                {
                    "sent": "We also have to load all of our triples to the main memory of the GPU and that is basically what limits the size of processor data processable data on the amount of triples that fit into the main memory of the GPU.",
                    "label": 1
                },
                {
                    "sent": "So to overcome this limitation where introducing a triple match, which is the data structure not holding only is reference but also the triple itself.",
                    "label": 0
                },
                {
                    "sent": "And using these triple matches.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can transform the working memories to a list of triple matches and then use the triple matches on the GPU for beta matching, for example.",
                    "label": 0
                },
                {
                    "sent": "This also allows us to divide the working memories into smaller chunks, and these chunks then can be processed independently on the one side, then can be they can be processed on a GPU independently, but on the other side we can also on the host side, use multithreading to prepare these chunks.",
                    "label": 1
                },
                {
                    "sent": "So the transformation from the working memories to a list of triple matches can be done in parallel to.",
                    "label": 1
                },
                {
                    "sent": "And because we are using triple matches journey to be no more references to be resolved and well, we can process an arbitrary number of triples.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um, another problem we figured out was rule firing, and as I said before we are.",
                    "label": 0
                },
                {
                    "sent": "We have paralyzed rule firing two and to do so.",
                    "label": 0
                },
                {
                    "sent": "We are also using the working memories that need to be transformed to true matches again, and we can then transfer these triple matches together with some instructions.",
                    "label": 0
                },
                {
                    "sent": "Housing New triples are derived to the GPU and infers and you fix on the GPU, so if we've got the working memories for beta one for example with two entries we could handle them independently and transform the chunks to triple matches, then loaded to the GPU.",
                    "label": 0
                },
                {
                    "sent": "The GPU would create our new triples.",
                    "label": 0
                },
                {
                    "sent": "And they would have been two transformed, transferred back to the host and there they could be submitted to the internal Triple Store and the triple stores then would be responsible to reject invalid triples like duplicates, for example.",
                    "label": 0
                },
                {
                    "sent": "We also introduced a simple concept based on the transformation to triple matches to reduce the amount of inverted triples that are inferred, but due to time issues I have to refer to the paper for more details on this point.",
                    "label": 0
                },
                {
                    "sent": "Um, to test out.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Implement or new concepts.",
                    "label": 0
                },
                {
                    "sent": "We use implementation from our previous work, which is the Java based implementation using open CL.",
                    "label": 0
                },
                {
                    "sent": "And here you can see a part of our architecture and on the left side you can see different chunks that were divided and each of these chunks now can be processed by a single thread on our in our Java application.",
                    "label": 0
                },
                {
                    "sent": "So these chunks are working memories and they need to be transferred to triple matches.",
                    "label": 1
                },
                {
                    "sent": "And after it's Red did the transformation.",
                    "label": 0
                },
                {
                    "sent": "It can be submitted to a Q and on the other side we've got working threads.",
                    "label": 1
                },
                {
                    "sent": "And there's one thread for every GPU.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Well, because we've got one thread for every GPU, and we're using this Q.",
                    "label": 0
                },
                {
                    "sent": "We can simply scale our approach to use multiple devices so we can easily add more GPUs at this site and just scale the hardware.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For our tests, we use two different data sets.",
                    "label": 0
                },
                {
                    "sent": "One is University benchmark data set where we can generate it up to 8000 universities which are about 1 billion triples, and we use the DB PEDIA data data set with scale to different sizes.",
                    "label": 0
                },
                {
                    "sent": "We also used Ubuntu Workstation with six core processor, 64 gigabytes of memory and 2MG gaming graphic cards holding 3 gigabytes of memory each.",
                    "label": 1
                },
                {
                    "sent": "1st.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All we wanted to see how the parallel implementation of rule firing, including the reduction of inverted triples, performs and here you can see the detailed reasoning time for applying the RDF's rule set.",
                    "label": 0
                },
                {
                    "sent": "On the left side or on the left bar you can see the serial implementation of rule firing.",
                    "label": 0
                },
                {
                    "sent": "In the middle.",
                    "label": 0
                },
                {
                    "sent": "The parallel implementation and on the right side we also applied the reduction concept, and as you can see the problem implementation is much faster than the serial one and using reduction concept we could also improve the performance of about 10 more person.",
                    "label": 0
                },
                {
                    "sent": "And this applies to both data sets.",
                    "label": 0
                },
                {
                    "sent": "So the University data set as well as the PEDIA data set both improved.",
                    "label": 0
                },
                {
                    "sent": "The reasoning was much faster with the parallel concepts for both data sets.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Furthermore, we wanted to see the impact of the parallelization of our concepts.",
                    "label": 0
                },
                {
                    "sent": "So on the first side, the parallelization using the chunks and preparing the chunks in parallel on the host side and on the other side using multiple GPUs.",
                    "label": 1
                },
                {
                    "sent": "To do so, we executed our reasoner first using a non threaded version, which means that each chunk was prepared and processed one after the other and then submitted to a single GPU.",
                    "label": 0
                },
                {
                    "sent": "The second bar shows a threaded version.",
                    "label": 0
                },
                {
                    "sent": "We where we prepared the workloads in parallel, but submitted also everything to a single GPU.",
                    "label": 0
                },
                {
                    "sent": "And finally, we also used two GPU's.",
                    "label": 0
                },
                {
                    "sent": "And for ADFS, you can see it's mainly benefits from adding the first level of parallelization, which is this Reddit one that is becausw the workload on the GPU is not that high for RDF's.",
                    "label": 0
                },
                {
                    "sent": "On the other side, you can see the PD star rules set, which is a much more complex rulesets becausw.",
                    "label": 0
                },
                {
                    "sent": "They need to be computed much more joints, and these joints introduce a heavy workload on the GPU, and when we add a second GPU here for this rule set.",
                    "label": 0
                },
                {
                    "sent": "We can gain really a high performance improvement.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Finally, we wanted to see how our system performs with really large data sets and to do so we had to use a different hardware because we ran out of memory.",
                    "label": 0
                },
                {
                    "sent": "So we used a cloud server with two NVIDIA gaming graphic cards and 192 gigabytes of memory and we used the University data sets was up to 1.1 billion triples and applied ODF as well as our DFS rulesets.",
                    "label": 0
                },
                {
                    "sent": "And first of all you can see that our approach scales nearly in a linear way, which is pretty good.",
                    "label": 0
                },
                {
                    "sent": "And we were able to reach through part of 2.7 million triples a second, which is quite fast, I think.",
                    "label": 0
                },
                {
                    "sent": "For comparison, web Pi, for example, reported through part of 2.1 billion triples a second on 64 computing nodes.",
                    "label": 1
                },
                {
                    "sent": "While we only use a single computing node equipped with two gaming graphic cards and we reached an even higher thruput.",
                    "label": 0
                },
                {
                    "sent": "Nevertheless, of course, we have to admit that website is still able to handle much larger data sets and we are able to.",
                    "label": 0
                },
                {
                    "sent": "For RDF S, We reached a through part of 1.4 million triples a second.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, coming to the end of my presentation we showed how to parallelize RTI algorithm for semantic reasoning in ways that the preparation of workload can be performed in parallel using simple multithreading and how to distribute the workload to multiple GPU's so we can not only use the massively parallel hardware on the single GPU but of multiple GPU's.",
                    "label": 0
                },
                {
                    "sent": "And as we did so, we were able to perform the reasoning on a data set with more than one billion triples in about 430 seconds for the rodeo rules set and in 1200 seconds for the DFS ruleset.",
                    "label": 1
                },
                {
                    "sent": "Nevertheless, we reached a new limitation, which now is the main memory of the of the computing node itself, and that is also why our future work will include the investigation of concepts that can be integrated to reduce the main memory usage of Sereti algorithm.",
                    "label": 0
                },
                {
                    "sent": "And it would also be interesting to see not only to distribute the workload to multiple GPUs, but also to multiple computing nodes that can be equipped with multiple GPU's.",
                    "label": 0
                },
                {
                    "sent": "Um, yes.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you for attention.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                }
            ]
        }
    }
}