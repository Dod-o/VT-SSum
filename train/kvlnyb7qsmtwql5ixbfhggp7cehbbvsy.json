{
    "id": "kvlnyb7qsmtwql5ixbfhggp7cehbbvsy",
    "title": "conTEXT -- Lightweight Text Analytics using Linked Data",
    "info": {
        "author": [
            "Ali Khalili, Agile Knowledge Engineering and Semantic Web (AKSW), University of Leipzig"
        ],
        "published": "July 30, 2014",
        "recorded": "May 2014",
        "category": [
            "Top->Computer Science->Semantic Web",
            "Top->Computer Science->Big Data"
        ]
    },
    "url": "http://videolectures.net/eswc2014_khalili_context/",
    "segmentation": [
        [
            "So I'm not present context for likely takes analytics using linked data."
        ],
        [
            "So as a quick agent of my Clock, so I will first explain the main motivation behind creating context.",
            "Then I will tell you how context works and then talk about the evaluation we did.",
            "And finally I conclude.",
            "And if we get time, that's so I will present you at live demo of the tool."
        ],
        [
            "So they will have democratized publishing, know everybody can easily publish information on a website with plug or on social networking platforms, but there seems to be an imbalance underwear so hundreds of millions of users are continuously sharing stories about their life on these social networking platforms like Facebook, Google Plus Twitter.",
            "But their results which can be drawn from analyzing their content.",
            "Rarely shared back to the users, on the other hand, the social networking platforms exploit the results of analyzing user content for targeted placement of advertisements for promotions, for doing user studies an in."
        ],
        [
            "In general, to make money out of this data.",
            "So we think that."
        ],
        [
            "At least people should also be able to find out what patterns.",
            "Can be discovered and what conclusions can be drawn from?",
            "The data, the information they share on this platform.",
            "Looking at this."
        ],
        [
            "Existing tools and approaches for text analytics we try to roughly categorize them into different categories, so we use the four dimensions here, so we have the.",
            "The targeted user time.",
            "So as you see here it goes from expert programmer to non programmer.",
            "We have the flexibility of user interface which shows if the approach presents you eyes for visualization, exploration and even refinement of the results.",
            "We have also another dimension which deals with the degree of structure which these tools are dealing with from unstructured to structured data and also the size of circles here.",
            "Is the generosity of the Arctic architecture so in terms of the scalability and generosity of the this, the platform, how easy it is to add new features to the system.",
            "So we have here this depics annouces develop."
        ],
        [
            "Environments like as you see your gate or patching my IBM content analytics platform which.",
            "Provide comprehensive supports for developing customized takes analytics workflow so they provide a higher level of flexibility at the user interface, but they need to users to be an expert program.",
            "We have"
        ],
        [
            "Takes analysis tools which provide a higher level of abstraction, but at the cost of generosity.",
            "As you see here.",
            "So we have."
        ],
        [
            "Business intelligence tools, which are usually dealing with semi structured data for facilitating business decision-making, but they still require user to have some knowledge of programming, at least competences in creating queries or designing reports."
        ],
        [
            "NLP API's like Alchemy and Open Kelly which.",
            "Enable natural language processing services like named entity recognition or relation extraction.",
            "They don't provide a higher level of flexibility.",
            "Pelsin in types of in terms of the user interface and also the architecture.",
            "And still need user to know some to have some knowledge of programming."
        ],
        [
            "We have spreadsheets which are easy to use so they don't require much programming knowledge.",
            "But they are usually limited to tabular data, so examples as you see our Excel data Wrangler or Google Google Docs we have made."
        ],
        [
            "Data analysis tools like Vestian Kewpies which are used for visualization and exploration of linked data.",
            "They provide the high level of flexibility in the user interface, but based on the existing surveys they are mostly usable by tech users.",
            "So for non programmers."
        ],
        [
            "They have social media analysis tools.",
            "Like Topsy, Fluminis and Tweetdeck.",
            "Which basically focused on integrating content or aggregating content across large repositories.",
            "For example, they get the whole Twitter and perform perform trend analysis or popularity analysis.",
            "So what?"
        ],
        [
            "We see here as a gap is."
        ],
        [
            "The lack of tools dealing with unstructured content, catering non expert users and providing extensible analytics.",
            "Interfaces that was."
        ],
        [
            "The main motivation for developing context, which is the platform for lightweight takes analytics.",
            "It's available online at context that a csw.org context tries to lower the barrier for text analytics.",
            "So the approach is that no installation and configuration is required.",
            "You can access content from a variety of sources.",
            "It shows instantly the results of analysis in a variety of visualizations.",
            "It allows refinement of automatic annotations and also takes feedback into account.",
            "It provides a generic architecture where different modules for content acquisition, natural language processing and visualization can be plugged together."
        ],
        [
            "So in my next slides I will talk about the architecture, how context works."
        ],
        [
            "So the process of takes analytics in context starts by collecting data.",
            "So the main issue is here to deal with the interactivity of different input types.",
            "So."
        ],
        [
            "Data contains in different containers.",
            "We have social networking platforms.",
            "Block lugging platform microblogging platforms and so on.",
            "So we need to deal with this."
        ],
        [
            "Hit or density of data and also we have different access methods to get to collect this data so we can use rest APIs.",
            "We can use the sparkle and points RSA to more RDF feeds or we have to write or own web crawlers to get data from these data sources.",
            "So in context we have a unified and standard data model."
        ],
        [
            "This supports the both relational and RDF based model of your argument based model.",
            "We are using Nip which I will explain to you in the next slides."
        ],
        [
            "So after we collected data comes the data analysis phase.",
            "So for this we actually we need some tools to extract patterns out of our data and we need a reference to ground these extracted patterns with this reference.",
            "So we use."
        ],
        [
            "Call language processing here.",
            "They have."
        ],
        [
            "DB Pedia as a reference.",
            "Ontology and be used.",
            "NLP services like the Pedia Spotlight and folks.",
            "To extract named entities and then we ground them to DVD's or reference, not television.",
            "So DVD's spotlight, as you probably know is an LP service which imitates the mentions of DPD in the text and.",
            "It's actually or in other words, the Wikipedia article, and we have folks which stands for Federated Knowledge Extraction framework which is created by my colleague excelling God against every research group.",
            "In comparison with Spotlight, it provides a higher level of precision and recall because it uses assemble learning to merge the result.",
            "The different NLP algorithms which leads to higher precision and recall.",
            "And any other NLP services which supports need can be added easily to context.",
            "So."
        ],
        [
            "Niff stands for NLP interchange format.",
            "It's also created by my colleague Sebastian Heilman, as part of the NLP two RDF project.",
            "It's an arguable old based format which aims to provide interoperability between different NLP resources, tools and services all stirred up writing customized RDF wrappers for each NLP tool using.",
            "If you can easily merge the different NLP services, tools and resources so it tries to standardize.",
            "Access parameters, annotations, validation and lock messages."
        ],
        [
            "So here I have the simple example of leave, so you have for example this sentence and we get this part of the sentence.",
            "We can apply different NLP tools to this train.",
            "So we have tokenizers.",
            "We can have stemmers the part of speech taggers.",
            "And the name entity recognition tools and in if you have unique ur eyes for different parts of the text and using the NIF ontology, then you have RDF statements for each two and then you can easily merge all these tools together."
        ],
        [
            "The next phase in context, is data enrichment, so we have two mechanisms to do data enrichment in context."
        ],
        [
            "One is get referencing the DV pedia, ur eyes of the recognized entities.",
            "So if we have for example the location, we get the longitude latitude for this location or if it's a person we can get the various state and some other information from this person."
        ],
        [
            "We are also using the matching of the Entity Co occurrences with some predefined natural language patterns which are provided by war.",
            "For example, if we have a book and author in a sentence, it can give you the relation to authorship relation here so."
        ],
        [
            "Going back to this, the chemistry analogy so you can see it like a catalyst, so it provides interaction between different data entities here so and thereby you will have more insights on your data."
        ],
        [
            "We have data mixing or they tell myself in context thanks to the need which I already explained to you, you can easily integrate the results of different NLP services and you can create composite corpora.",
            "For example, you can get the user's Twitter account is Facebook's blog is LinkedIn and any other source of information about the user and then you will have an integrated view on all of these data.",
            "So it also helps to create a user model so you know the user preferences when you integrate all these things and you get more context about the user."
        ],
        [
            "We have data visualization and exploration modules, so having this semantically enriched data, we can create different views on data for different purposes.",
            "For example, if you have this simple structure of water for different purposes, you will need to create different visualizations.",
            "So in context we do the same.",
            "We use exhibit and D3 JS to create different visualizations.",
            "So I just."
        ],
        [
            "Show you some examples here so we have, for example faceted browsing, in which you can define different facets to browse your data.",
            "You can easily select some facets as a constraint for your data and to drill down the results to find the most relevant information you need."
        ],
        [
            "We have places map and people timeline so you will see the dimensions of locations on a map or a temporal relation between people on a timeline.",
            "We have"
        ],
        [
            "Tech Cloud, which shows the frequency of entities we."
        ],
        [
            "I have this chordal graph view which shows the relation between entities and the possible relation type if it's available."
        ],
        [
            "We have metrics Cooccurrence view which shows the Co occurrence of entities in different clusters.",
            "As you see here."
        ],
        [
            "We have the trend view which shows the frequency of entities over the time, so it's used for doing trend analysis we."
        ],
        [
            "Sentiment views or we apply sentiment analysis on different articles and then you will see the overall sentiment as well as their sentiments changes over the time."
        ],
        [
            "We have image view which is similar to the tag cloud but using an image collage and there can be much more views so it's easy to add your own view on on this semantically and structured content."
        ],
        [
            "Then we have the annotation refinement phase.",
            "So with this lightweight stakes analytics.",
            "Without that, we can provide direct incentives to the users to adopt the semantic annotation.",
            "So because when they refine the annotations, they will get more precise analytics, so it's like an incentive for the users we.",
            "Applied the integrated RDF face.",
            "VCV Mediterr resume stands for what you see is not what you get but also what you mean so its uses RDF a format to embed the annotations and then you can easily integrate it back to the.",
            "To the system.",
            "So this can be used for NLP calibration.",
            "For example, this image that you see is an Indian operator which is calibrating incubator.",
            "So this can be used to.",
            "To break the temperature of this the the incubator from the feedback it gets so we have the similar approach we in collaboration with DB Pedia Spotlight and folks.",
            "We created special rest APIs to send feedback to this NLP services for calibration."
        ],
        [
            "So and we created this UI so you can search for the DB pedia entities.",
            "You can add new entities.",
            "Or you can delete or refine the currently annotated entities."
        ],
        [
            "So this was the general architecture or talked about.",
            "So we collected data we processed, enriched and mixed data based underneath.",
            "We then have different Explorer exploring and visualizing components and then we have the feedback component."
        ],
        [
            "There are more features in context, so I just quickly named him then so we have interactive unprogressive and note."
        ],
        [
            "Station we have real time semantic analysis which is interesting so we can deal with big data.",
            "For example the life Twitter streams we can get these data, analyze it instantly and then show their results.",
            "Here in this view."
        ],
        [
            "And another feature is search engine optimization.",
            "Thanks to the Jason LD which is which was currently added to schema.org.",
            "Now we can get the results of this manually or automatically generated annotations and then integrate it back to the user corpora and we can.",
            "Change also the DB pedia or reference ontology here for provide a mapping from our ontology to DB pedia and or we can also drill down results unique using a subset of the pedia."
        ],
        [
            "So for evaluating the context, we did the usefulness study as well as a usability study.",
            "So I'm not going much into the details.",
            "You can refer to the paper for much detailed description of this study, so we follow the test driven usefulness.",
            "Study with 25 students.",
            "There were ten questions, like for example, we ask them what are the five most mentioned countries by Bill Gates tweets.",
            "And the similar questions.",
            "So we had a set of corpora and ask these 10 questions.",
            "We designed this evaluation platform.",
            "There we could measure the time which was spent by users to answer these questions and also the correctness of answers."
        ],
        [
            "So here you see the lift diagram shows the time spent by users.",
            "The right one shows the correctness of answer.",
            "So we used Jaccard similarity score.",
            "Because we are we have sets of answers and we need to match them against or gold standard.",
            "So we use Jacquard here and."
        ],
        [
            "In general.",
            "Users needed an average 136% more time without context to find the answers for these questions, and for most of the tasks, context enabled users to do the task three times faster than before.",
            "And for some questions with without using context, it was impossible to find the answer.",
            "So context enables user to find answer to the task which was out of reach before.",
            "And."
        ],
        [
            "OK, and then we we did another study usability study with system usability scale which was also 10 questions with answers ranging from strongly aggregate.",
            "We strongly disagree.",
            "So and for this we got a score of 82 which shows a higher level of usability for our system."
        ],
        [
            "OK, to conclude my talk, I talked about the lightweight takes analytics using linked data.",
            "Our goal was first to democratize NLP usage so that ordinary users can also apply sophisticated NLP tasks to their data.",
            "We also we tried to alleviate the semantic Web's chicken and egg problem because for example, one problem we are currently seeing is that search engines need need structured data to provide better service capabilities and on the other hand this better search capabilities is a motivation for users to publish semantically structured content.",
            "With context, we try to provide instant results for users so that they can adopt to semantic technology and they can.",
            "Publish more structured content on the web and other goal was harnessing the power of feedback loops.",
            "So with taking feedback into account, we aimed to enhance the current NLP tools."
        ],
        [
            "For future work, we plan to improve the performance and scalability of different views to expose APIs for third parties.",
            "To use context to enable batch refinement of annotations and to add more input sources and many more.",
            "Thanks."
        ],
        [
            "OK, thank you for your attention.",
            "So if you have any questions I will be more than happy to answer them."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm not present context for likely takes analytics using linked data.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as a quick agent of my Clock, so I will first explain the main motivation behind creating context.",
                    "label": 0
                },
                {
                    "sent": "Then I will tell you how context works and then talk about the evaluation we did.",
                    "label": 0
                },
                {
                    "sent": "And finally I conclude.",
                    "label": 0
                },
                {
                    "sent": "And if we get time, that's so I will present you at live demo of the tool.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So they will have democratized publishing, know everybody can easily publish information on a website with plug or on social networking platforms, but there seems to be an imbalance underwear so hundreds of millions of users are continuously sharing stories about their life on these social networking platforms like Facebook, Google Plus Twitter.",
                    "label": 0
                },
                {
                    "sent": "But their results which can be drawn from analyzing their content.",
                    "label": 0
                },
                {
                    "sent": "Rarely shared back to the users, on the other hand, the social networking platforms exploit the results of analyzing user content for targeted placement of advertisements for promotions, for doing user studies an in.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In general, to make money out of this data.",
                    "label": 0
                },
                {
                    "sent": "So we think that.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At least people should also be able to find out what patterns.",
                    "label": 1
                },
                {
                    "sent": "Can be discovered and what conclusions can be drawn from?",
                    "label": 1
                },
                {
                    "sent": "The data, the information they share on this platform.",
                    "label": 0
                },
                {
                    "sent": "Looking at this.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Existing tools and approaches for text analytics we try to roughly categorize them into different categories, so we use the four dimensions here, so we have the.",
                    "label": 1
                },
                {
                    "sent": "The targeted user time.",
                    "label": 0
                },
                {
                    "sent": "So as you see here it goes from expert programmer to non programmer.",
                    "label": 0
                },
                {
                    "sent": "We have the flexibility of user interface which shows if the approach presents you eyes for visualization, exploration and even refinement of the results.",
                    "label": 0
                },
                {
                    "sent": "We have also another dimension which deals with the degree of structure which these tools are dealing with from unstructured to structured data and also the size of circles here.",
                    "label": 0
                },
                {
                    "sent": "Is the generosity of the Arctic architecture so in terms of the scalability and generosity of the this, the platform, how easy it is to add new features to the system.",
                    "label": 0
                },
                {
                    "sent": "So we have here this depics annouces develop.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Environments like as you see your gate or patching my IBM content analytics platform which.",
                    "label": 0
                },
                {
                    "sent": "Provide comprehensive supports for developing customized takes analytics workflow so they provide a higher level of flexibility at the user interface, but they need to users to be an expert program.",
                    "label": 0
                },
                {
                    "sent": "We have",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Takes analysis tools which provide a higher level of abstraction, but at the cost of generosity.",
                    "label": 0
                },
                {
                    "sent": "As you see here.",
                    "label": 0
                },
                {
                    "sent": "So we have.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Business intelligence tools, which are usually dealing with semi structured data for facilitating business decision-making, but they still require user to have some knowledge of programming, at least competences in creating queries or designing reports.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "NLP API's like Alchemy and Open Kelly which.",
                    "label": 0
                },
                {
                    "sent": "Enable natural language processing services like named entity recognition or relation extraction.",
                    "label": 0
                },
                {
                    "sent": "They don't provide a higher level of flexibility.",
                    "label": 0
                },
                {
                    "sent": "Pelsin in types of in terms of the user interface and also the architecture.",
                    "label": 0
                },
                {
                    "sent": "And still need user to know some to have some knowledge of programming.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have spreadsheets which are easy to use so they don't require much programming knowledge.",
                    "label": 0
                },
                {
                    "sent": "But they are usually limited to tabular data, so examples as you see our Excel data Wrangler or Google Google Docs we have made.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Data analysis tools like Vestian Kewpies which are used for visualization and exploration of linked data.",
                    "label": 0
                },
                {
                    "sent": "They provide the high level of flexibility in the user interface, but based on the existing surveys they are mostly usable by tech users.",
                    "label": 0
                },
                {
                    "sent": "So for non programmers.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They have social media analysis tools.",
                    "label": 0
                },
                {
                    "sent": "Like Topsy, Fluminis and Tweetdeck.",
                    "label": 0
                },
                {
                    "sent": "Which basically focused on integrating content or aggregating content across large repositories.",
                    "label": 0
                },
                {
                    "sent": "For example, they get the whole Twitter and perform perform trend analysis or popularity analysis.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We see here as a gap is.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The lack of tools dealing with unstructured content, catering non expert users and providing extensible analytics.",
                    "label": 0
                },
                {
                    "sent": "Interfaces that was.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The main motivation for developing context, which is the platform for lightweight takes analytics.",
                    "label": 0
                },
                {
                    "sent": "It's available online at context that a csw.org context tries to lower the barrier for text analytics.",
                    "label": 0
                },
                {
                    "sent": "So the approach is that no installation and configuration is required.",
                    "label": 0
                },
                {
                    "sent": "You can access content from a variety of sources.",
                    "label": 1
                },
                {
                    "sent": "It shows instantly the results of analysis in a variety of visualizations.",
                    "label": 1
                },
                {
                    "sent": "It allows refinement of automatic annotations and also takes feedback into account.",
                    "label": 0
                },
                {
                    "sent": "It provides a generic architecture where different modules for content acquisition, natural language processing and visualization can be plugged together.",
                    "label": 1
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in my next slides I will talk about the architecture, how context works.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the process of takes analytics in context starts by collecting data.",
                    "label": 0
                },
                {
                    "sent": "So the main issue is here to deal with the interactivity of different input types.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Data contains in different containers.",
                    "label": 0
                },
                {
                    "sent": "We have social networking platforms.",
                    "label": 0
                },
                {
                    "sent": "Block lugging platform microblogging platforms and so on.",
                    "label": 0
                },
                {
                    "sent": "So we need to deal with this.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hit or density of data and also we have different access methods to get to collect this data so we can use rest APIs.",
                    "label": 0
                },
                {
                    "sent": "We can use the sparkle and points RSA to more RDF feeds or we have to write or own web crawlers to get data from these data sources.",
                    "label": 0
                },
                {
                    "sent": "So in context we have a unified and standard data model.",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This supports the both relational and RDF based model of your argument based model.",
                    "label": 0
                },
                {
                    "sent": "We are using Nip which I will explain to you in the next slides.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So after we collected data comes the data analysis phase.",
                    "label": 0
                },
                {
                    "sent": "So for this we actually we need some tools to extract patterns out of our data and we need a reference to ground these extracted patterns with this reference.",
                    "label": 0
                },
                {
                    "sent": "So we use.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Call language processing here.",
                    "label": 0
                },
                {
                    "sent": "They have.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "DB Pedia as a reference.",
                    "label": 0
                },
                {
                    "sent": "Ontology and be used.",
                    "label": 0
                },
                {
                    "sent": "NLP services like the Pedia Spotlight and folks.",
                    "label": 0
                },
                {
                    "sent": "To extract named entities and then we ground them to DVD's or reference, not television.",
                    "label": 0
                },
                {
                    "sent": "So DVD's spotlight, as you probably know is an LP service which imitates the mentions of DPD in the text and.",
                    "label": 0
                },
                {
                    "sent": "It's actually or in other words, the Wikipedia article, and we have folks which stands for Federated Knowledge Extraction framework which is created by my colleague excelling God against every research group.",
                    "label": 0
                },
                {
                    "sent": "In comparison with Spotlight, it provides a higher level of precision and recall because it uses assemble learning to merge the result.",
                    "label": 0
                },
                {
                    "sent": "The different NLP algorithms which leads to higher precision and recall.",
                    "label": 0
                },
                {
                    "sent": "And any other NLP services which supports need can be added easily to context.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Niff stands for NLP interchange format.",
                    "label": 1
                },
                {
                    "sent": "It's also created by my colleague Sebastian Heilman, as part of the NLP two RDF project.",
                    "label": 0
                },
                {
                    "sent": "It's an arguable old based format which aims to provide interoperability between different NLP resources, tools and services all stirred up writing customized RDF wrappers for each NLP tool using.",
                    "label": 0
                },
                {
                    "sent": "If you can easily merge the different NLP services, tools and resources so it tries to standardize.",
                    "label": 1
                },
                {
                    "sent": "Access parameters, annotations, validation and lock messages.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here I have the simple example of leave, so you have for example this sentence and we get this part of the sentence.",
                    "label": 0
                },
                {
                    "sent": "We can apply different NLP tools to this train.",
                    "label": 0
                },
                {
                    "sent": "So we have tokenizers.",
                    "label": 0
                },
                {
                    "sent": "We can have stemmers the part of speech taggers.",
                    "label": 0
                },
                {
                    "sent": "And the name entity recognition tools and in if you have unique ur eyes for different parts of the text and using the NIF ontology, then you have RDF statements for each two and then you can easily merge all these tools together.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The next phase in context, is data enrichment, so we have two mechanisms to do data enrichment in context.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One is get referencing the DV pedia, ur eyes of the recognized entities.",
                    "label": 0
                },
                {
                    "sent": "So if we have for example the location, we get the longitude latitude for this location or if it's a person we can get the various state and some other information from this person.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We are also using the matching of the Entity Co occurrences with some predefined natural language patterns which are provided by war.",
                    "label": 0
                },
                {
                    "sent": "For example, if we have a book and author in a sentence, it can give you the relation to authorship relation here so.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going back to this, the chemistry analogy so you can see it like a catalyst, so it provides interaction between different data entities here so and thereby you will have more insights on your data.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have data mixing or they tell myself in context thanks to the need which I already explained to you, you can easily integrate the results of different NLP services and you can create composite corpora.",
                    "label": 0
                },
                {
                    "sent": "For example, you can get the user's Twitter account is Facebook's blog is LinkedIn and any other source of information about the user and then you will have an integrated view on all of these data.",
                    "label": 0
                },
                {
                    "sent": "So it also helps to create a user model so you know the user preferences when you integrate all these things and you get more context about the user.",
                    "label": 1
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have data visualization and exploration modules, so having this semantically enriched data, we can create different views on data for different purposes.",
                    "label": 1
                },
                {
                    "sent": "For example, if you have this simple structure of water for different purposes, you will need to create different visualizations.",
                    "label": 0
                },
                {
                    "sent": "So in context we do the same.",
                    "label": 0
                },
                {
                    "sent": "We use exhibit and D3 JS to create different visualizations.",
                    "label": 0
                },
                {
                    "sent": "So I just.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Show you some examples here so we have, for example faceted browsing, in which you can define different facets to browse your data.",
                    "label": 0
                },
                {
                    "sent": "You can easily select some facets as a constraint for your data and to drill down the results to find the most relevant information you need.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have places map and people timeline so you will see the dimensions of locations on a map or a temporal relation between people on a timeline.",
                    "label": 0
                },
                {
                    "sent": "We have",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tech Cloud, which shows the frequency of entities we.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I have this chordal graph view which shows the relation between entities and the possible relation type if it's available.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have metrics Cooccurrence view which shows the Co occurrence of entities in different clusters.",
                    "label": 0
                },
                {
                    "sent": "As you see here.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have the trend view which shows the frequency of entities over the time, so it's used for doing trend analysis we.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sentiment views or we apply sentiment analysis on different articles and then you will see the overall sentiment as well as their sentiments changes over the time.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have image view which is similar to the tag cloud but using an image collage and there can be much more views so it's easy to add your own view on on this semantically and structured content.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then we have the annotation refinement phase.",
                    "label": 1
                },
                {
                    "sent": "So with this lightweight stakes analytics.",
                    "label": 0
                },
                {
                    "sent": "Without that, we can provide direct incentives to the users to adopt the semantic annotation.",
                    "label": 0
                },
                {
                    "sent": "So because when they refine the annotations, they will get more precise analytics, so it's like an incentive for the users we.",
                    "label": 1
                },
                {
                    "sent": "Applied the integrated RDF face.",
                    "label": 0
                },
                {
                    "sent": "VCV Mediterr resume stands for what you see is not what you get but also what you mean so its uses RDF a format to embed the annotations and then you can easily integrate it back to the.",
                    "label": 1
                },
                {
                    "sent": "To the system.",
                    "label": 0
                },
                {
                    "sent": "So this can be used for NLP calibration.",
                    "label": 0
                },
                {
                    "sent": "For example, this image that you see is an Indian operator which is calibrating incubator.",
                    "label": 0
                },
                {
                    "sent": "So this can be used to.",
                    "label": 0
                },
                {
                    "sent": "To break the temperature of this the the incubator from the feedback it gets so we have the similar approach we in collaboration with DB Pedia Spotlight and folks.",
                    "label": 0
                },
                {
                    "sent": "We created special rest APIs to send feedback to this NLP services for calibration.",
                    "label": 1
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So and we created this UI so you can search for the DB pedia entities.",
                    "label": 0
                },
                {
                    "sent": "You can add new entities.",
                    "label": 0
                },
                {
                    "sent": "Or you can delete or refine the currently annotated entities.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this was the general architecture or talked about.",
                    "label": 0
                },
                {
                    "sent": "So we collected data we processed, enriched and mixed data based underneath.",
                    "label": 0
                },
                {
                    "sent": "We then have different Explorer exploring and visualizing components and then we have the feedback component.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are more features in context, so I just quickly named him then so we have interactive unprogressive and note.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Station we have real time semantic analysis which is interesting so we can deal with big data.",
                    "label": 1
                },
                {
                    "sent": "For example the life Twitter streams we can get these data, analyze it instantly and then show their results.",
                    "label": 0
                },
                {
                    "sent": "Here in this view.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And another feature is search engine optimization.",
                    "label": 1
                },
                {
                    "sent": "Thanks to the Jason LD which is which was currently added to schema.org.",
                    "label": 0
                },
                {
                    "sent": "Now we can get the results of this manually or automatically generated annotations and then integrate it back to the user corpora and we can.",
                    "label": 0
                },
                {
                    "sent": "Change also the DB pedia or reference ontology here for provide a mapping from our ontology to DB pedia and or we can also drill down results unique using a subset of the pedia.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for evaluating the context, we did the usefulness study as well as a usability study.",
                    "label": 0
                },
                {
                    "sent": "So I'm not going much into the details.",
                    "label": 0
                },
                {
                    "sent": "You can refer to the paper for much detailed description of this study, so we follow the test driven usefulness.",
                    "label": 0
                },
                {
                    "sent": "Study with 25 students.",
                    "label": 0
                },
                {
                    "sent": "There were ten questions, like for example, we ask them what are the five most mentioned countries by Bill Gates tweets.",
                    "label": 1
                },
                {
                    "sent": "And the similar questions.",
                    "label": 0
                },
                {
                    "sent": "So we had a set of corpora and ask these 10 questions.",
                    "label": 0
                },
                {
                    "sent": "We designed this evaluation platform.",
                    "label": 0
                },
                {
                    "sent": "There we could measure the time which was spent by users to answer these questions and also the correctness of answers.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here you see the lift diagram shows the time spent by users.",
                    "label": 0
                },
                {
                    "sent": "The right one shows the correctness of answer.",
                    "label": 0
                },
                {
                    "sent": "So we used Jaccard similarity score.",
                    "label": 1
                },
                {
                    "sent": "Because we are we have sets of answers and we need to match them against or gold standard.",
                    "label": 0
                },
                {
                    "sent": "So we use Jacquard here and.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In general.",
                    "label": 0
                },
                {
                    "sent": "Users needed an average 136% more time without context to find the answers for these questions, and for most of the tasks, context enabled users to do the task three times faster than before.",
                    "label": 1
                },
                {
                    "sent": "And for some questions with without using context, it was impossible to find the answer.",
                    "label": 0
                },
                {
                    "sent": "So context enables user to find answer to the task which was out of reach before.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and then we we did another study usability study with system usability scale which was also 10 questions with answers ranging from strongly aggregate.",
                    "label": 1
                },
                {
                    "sent": "We strongly disagree.",
                    "label": 0
                },
                {
                    "sent": "So and for this we got a score of 82 which shows a higher level of usability for our system.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, to conclude my talk, I talked about the lightweight takes analytics using linked data.",
                    "label": 1
                },
                {
                    "sent": "Our goal was first to democratize NLP usage so that ordinary users can also apply sophisticated NLP tasks to their data.",
                    "label": 0
                },
                {
                    "sent": "We also we tried to alleviate the semantic Web's chicken and egg problem because for example, one problem we are currently seeing is that search engines need need structured data to provide better service capabilities and on the other hand this better search capabilities is a motivation for users to publish semantically structured content.",
                    "label": 0
                },
                {
                    "sent": "With context, we try to provide instant results for users so that they can adopt to semantic technology and they can.",
                    "label": 0
                },
                {
                    "sent": "Publish more structured content on the web and other goal was harnessing the power of feedback loops.",
                    "label": 1
                },
                {
                    "sent": "So with taking feedback into account, we aimed to enhance the current NLP tools.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For future work, we plan to improve the performance and scalability of different views to expose APIs for third parties.",
                    "label": 1
                },
                {
                    "sent": "To use context to enable batch refinement of annotations and to add more input sources and many more.",
                    "label": 1
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, thank you for your attention.",
                    "label": 0
                },
                {
                    "sent": "So if you have any questions I will be more than happy to answer them.",
                    "label": 0
                }
            ]
        }
    }
}