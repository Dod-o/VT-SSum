{
    "id": "bdsvadi6xsmktrrunyrrxuhqbtmlss6n",
    "title": "Deep Reinforcement Learning",
    "info": {
        "author": [
            "David Silver, Department of Computer Science, University College London"
        ],
        "published": "July 28, 2015",
        "recorded": "June 2015",
        "category": [
            "Top->Medicine->Neuroscience",
            "Top->Technology->Engineering->Electrical Engineering->Control Engineering",
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Artificial Intelligence",
            "Top->Social Sciences->Psychology",
            "Top->Social Sciences->Economics"
        ]
    },
    "url": "http://videolectures.net/rldm2015_silver_reinforcement_learning/",
    "segmentation": [
        [
            "So it's really a pleasure to be here, so I'm afraid you're absolutely zero that I'm not even going to mention the word Bayes after this.",
            "Once we get into the talk.",
            "So sorry, but it is going to be about a combination of deep learning with reinforcement learning."
        ],
        [
            "And so the plan for today is really to start with a basic tutorial, an outline of some of the main ideas which have come into this revolution in deep learning which has happened over the last few years and really transformed a lot of the applications of machine learning and leading to a lot of success across many different challenging areas.",
            "And then the rest of the talk is about how to really combine that with what we know, which is reinforcement learning and what I'm going to do is start with a brief introduction and outline of the particular parts of PRL that we're going to consider.",
            "So sorry for those.",
            "I'm going to just.",
            "Try and hopefully Michael Lipman's talk is already prepared to you, but I'm just going to go over a couple of little bits to make sure everyone's on the same page there and then I'm going to talk really about three different paradigms for how we can combine deep learning with reinforcement learning, and the question really is a very simple one.",
            "It's like, you know, we have this very powerful function approximator.",
            "This powerful tool kit of deep learning, and where should we put that tool capture?",
            "We use it to represent value functions.",
            "We use it to represent policies to easier to represent our transition models, and we're going to look at each of those a little bit in turn.",
            "The last one will be very, very brief, so just warning.",
            "About that, so that's the sort of plan for today, so it's sort of half tutorial and also half a little bit of kind of some some recent results, so hope you know.",
            "Bear with me if actually present some of the work we've been doing ourselves recently.",
            "With that I'm going to start off with the first section, so just getting into."
        ],
        [
            "A real background to to deep learning, so I wanted to start by really talking about you know why are we here?",
            "What's the big goal?",
            "And for me and for us at DeepMind the goal is to really try and use reinforcement learning to solve some of the bigger visions of AI, and I think you know the reason that many of us are so excited about reinforcement learning is that it is the paradigm for artificial intelligence.",
            "I mean whenever we have agents that actually take actions in the world and want to influence their environment, ultimately that's our goal.",
            "We want to influence the environment, want our agents to actually do things to take action to have an effect in the world.",
            "And to do something to achieve and to maximize its reward and its impact on the world and its environment.",
            "And the RL is the way to do that.",
            "So arels the paradigm that describes how to learn to make optimal decisions in any environment.",
            "But more than that, what we care about is coming out with a single algorithm, a single agent, and we don't want to have to rewrite this agent every time we come to some new environment.",
            "We want to have a single algorithm which we can just sort of drop in there.",
            "You know, like the baby, you put the baby down in some front of some new task, and eventually it kind of played around with it, figures out how to deal with the toy and and, and it's by the end of the day.",
            "Maybe it's figured out how to deal with this knew piece of its environment.",
            "It's never encountered before and so really what we want is a single agent that can solve any of a wide range of challenging tasks.",
            "And that's really the essence of what we mean by an intelligent agent, and so the piece I'm really going to focus on today is just one piece of that, you know.",
            "Obviously rather ambitious goal, and the piece I'm going to focus on is how to bring in powerful representations into the story.",
            "In other words, if we want to have an agent that can deal with all kinds of different problems, it better have a very powerful and flexible toolkit for representing its environment and understanding what's going on in that environment, and so really, what we're going to use is deep learning as our toolkit for representations for dealing.",
            "For constructing features for representing state.",
            "For turning this environment into something that the agent can characterize for itself."
        ],
        [
            "OK, so let's start with deep learning."
        ],
        [
            "So I'm not going to put up the sort of standard picture of a neural network.",
            "I really want to avoid that.",
            "I really wanted you to think of deep learning in a different way, so the way I'd like you to think of deep learning is just about compositions of functions.",
            "So let's back off a minute and just talk about what it means to be deep.",
            "This really, it's just, of course a relabelling of ideas which have been around for decades now, but a deep representation really just means a composition of many functions, and so these arrows here.",
            "They basically indicate each of these arrows represents a function that's parameterized by some parameter vector.",
            "So here we've got a function going from the input X.",
            "This hidden state, hy parameterized by some parameter vector W one, and then a second function that goes from this hidden state using parameter vector, W2 and so forth.",
            "And each of these.",
            "Each of these functions gets composed together to give an increasingly sophisticated representation of what's going on in that input, until eventually at the end of it we come to some output.",
            "And that's that's our representation.",
            "If you like, that's how function.",
            "So a deep representation is nothing more than the function that can be broken down into a series of composition of individual functions, each of which can be richly parameterized, and one of the reasons that deep learning is effective is because this can be done very efficiently and it suits the toolkit of machinery that we have to build very large parameter vectors in a typically.",
            "For example, if we're dealing with images in Google, this might be the parameter vectors might contain 10s of millions of components.",
            "And there might be, say, 20 or 30 layers to this representation.",
            "And the fundamental idea what makes deep learning special is that we can optimize these representations very effectively, and the way that we optimize them is really the most familiar and simplest idea in machine learning, which is by following its gradient, and so really the key idea is that we can we need to be able to compute the gradient of any one of these rich and powerful representations, and the way we do that is essentially by something known as backpropagation, which in some sense is nothing more than the chain rule.",
            "So all I've done in this second figure here is to basically reverse the flow.",
            "Of this composition of functions and at the top of the.",
            "Of the screen here and see what we can do is we can work backwards and we can now say OK. Well if we started our output over here and we want to know well how do we compute the gradient of this output?",
            "In other words, if we want to know the gradient of the output with respect to our imperatore with respect to any of our parameter vectors, this backward flow shows us precisely how to compute any one of those gradients simply by applying the chain rule.",
            "So we can think of these as just the multiplication of all of the partial derivatives.",
            "So for example, if we want to compute the gradient with respect to.",
            "These parameters here we would just get the gradient with respect to this hidden state here and then the gradient of the hidden state with respect to parameter vectors.",
            "Multiply that by that and now we know how this particular parameter vector influences the output of the whole function.",
            "And so the power of backpropagation is that it gives us a very simple and effective and efficient way to compute gradients for anyone of a very large toolkit of possible rich function approximators.",
            "OK."
        ],
        [
            "So what about neural networks?",
            "So in some sense, the deep neural network is.",
            "Analogous or synonymous with a deep representation, but people tend to use a particular set of functions in the tool kit that they use, and so the most typical feedforward neural network that you might see would be composed of two things it would alternate between linear transformations where the hidden state one step is basically a linear combination for the hidden step hidden state we had at the time at the Step 4, you basically take your hidden state, you perform a linear transformation by multiplying by some weight matrix.",
            "And it gives you a new hidden state.",
            "And of course you could sequence together many of these different matrices.",
            "You could be one matrix multiplied by another matrix multiplied by another matrix, but at the end of the day you would just end up with one giant linear transformation and you wouldn't have gained any power in your representational capacity.",
            "So the other idea, which is typically used in almost every neural network, is what's called an activation function, which is simply a nonlinear function that takes the hidden state and pass it through some.",
            "For example, thresholding function or sigmoid squashing function and output some value that basically add some nonlinearity to this function.",
            "So what typically happens in your network is it's a composition of functions that alternates between these very simple linear transformations, and these also equally simple nonlinear activation functions, and that combination is particularly simple to work with.",
            "Has resulted in a large family of successful function approximators."
        ],
        [
            "So one more idea that comes from the deep learning world and.",
            "Is the idea of white sharing, so this is really.",
            "I remember reading a book a few years ago where it described weight sharing is the only idea that works in your own networks, so I don't know if I would take it that far, but it's certainly very mainstay of deep learning that could be applied effectively in many different contexts.",
            "And two of the most popular architectures for deep learning actually make use of weight sharing, and they use weight sharing to deal with structured data.",
            "So the first formal structured data is where you have.",
            "Time series data.",
            "We have sequential data coming in and in that case you can use a recurrent neural network to deal with the sequential structure of the data and so the idea is that again we've got exactly the same idea where we've got a compositional function.",
            "But now what's happening is that every step and you input is coming in, so the times that one we've got this X1 coming into the network and some output Y1 coming out of the network and then this gets passed through and the hidden layer kind of combines together some new state and then some new information comes in.",
            "This X2 comes in at the next step.",
            "And an output Y two is made at the next step and this goes on and on through many many time steps.",
            "And what differentiates it from the feed forward networks that we saw in the previous slide is essentially this idea that we use weight sharing that the same transformation, the same linear transformation is applied at every single step.",
            "So the same weight matrix W. And here and here.",
            "And this represents the kind of our temporal invariants that basically we're going to use the same operation at every single step of time to combine together the latest information that's coming in the input and the previous state.",
            "We're going to combine that together to construct a new state at the next time step, and also use the same weight matrix to perform whatever kind of output comes out of our network.",
            "So this is 1 mechanism and I really want to, you know again, for people here.",
            "I think it has perhaps the closest match with reinforcement learning.",
            "I'm not going to talk about it too much today, but you can think of this as the Canonical way where deep learning can deal with partial, observable partial observe ability.",
            "So if you have some domain where observations are coming in step by step and you want to construct some state representation that can remember the things that you've seen so far are currently on network is kind of the basic building block for dealing with such things.",
            "Commodi planning perspective.",
            "OK, the 2nd way in which weight sharing can be used is shown at the bottom of this slide here, and that's where typically used when we want to deal with structured data such as images.",
            "And So what we can see here is that we've got, say, an image on the left hand side.",
            "This image X and the idea of a convolutional network is it basically has the ZIL patches, where each Patch kind of represents some feature that's being constructed from some region of the image.",
            "So here we have a little Patch that's being used.",
            "One feature in the output, this hidden representation here, so this hidden node here is a function of all of the inputs within this region only.",
            "And similarly this hidden value here is a function of everything in the corresponding region.",
            "Here in the input and the idea of a convolutional network is that the transformations from these input spaces to the hidden space A shared in other ways, there's this translation invariants that we gain from this where wherever you are in the image.",
            "For example, if you might learn to identify a face, and if that face here.",
            "Then this note here can identify that base and there might be a feature saying hey, there's a face here and that gets stored here.",
            "But equally interface occurs here that face to be recognized by using the same shared weights and that face recognizer will be stored at this part of the hidden representation.",
            "What isn't indicated on this bigger is that the hidden representation could have many many planes to it.",
            "In the convolutional network, so this is just showing 1 hidden plane, but there will be a whole vector of inputs.",
            "Indeed there could be a whole vector inputs X and a whole vector of hidden states, H1 and so forth.",
            "All of these have this kind of third dimension to them.",
            "And then again, this can be applied compositionally, so that's just one layer of the network.",
            "But then we can compose these again and again step by step, so we can now take that representation that we just built up, for example here, here and now we can combine together those higher level features looking at a region here and building some even higher level representation which retains its kind of spatial relevance, and so the idea of the convolutional network is that every stage you retain some kind of spatial representation of the data.",
            "Whereas in a standard feedforward neural network you kind of lose that information.",
            "Everything is just a vector of hidden state with no particular locations associated with it.",
            "So you start off with an image.",
            "You process that image to perhaps give you a lower resolution representation of that image, and you keep passing this through this pipeline to get more and more sophisticated representations of what's going on in that image, but still represented as some kind of image in image space if you like.",
            "OK, so that's just a brief tutorial to some of the architectures that typically get used in deep learning.",
            "So now what do we do with them?"
        ],
        [
            "Well, ultimately we want to do some kind of machine learning using our representation, and so one way to do to apply machine learning to our deep neural networks is to simply come up with the loss function that basically says you know is this particular output good or bad.",
            "So each time we pass our input into the into the network, we get some output.",
            "Why out of it?",
            "And now we're going to find some loss function this LY.",
            "That basically tells us whether we think that output is good.",
            "Does it match the target we want?",
            "Does it give us a lot of reward?",
            "Is it representing exactly the target that we want and so the two Canonical examples of loss functions which we're going to consider?",
            "The first we're going to use throughout the talk, which is the mean squared error.",
            "So the mean squared error in some sense is the simplest loss function.",
            "It just represents, you know, let's say there's actually some true target.",
            "Why star?",
            "Well, imagine for now that there's some Oracle that tells us a teacher that tells us what their real target, why star should be as in supervised learning, and then as the talk progresses, we'll see how to relax that constraint.",
            "But if we did have this, why star?",
            "Then we could simply look at the mean squared error between the true target by star and what the network actually outputs Y and that would give us our loss function.",
            "Or if this was a probabilistic network, we might choose to use something like a log likelihood where we might ask the network to make some prediction of, for example a category or class or something.",
            "And now we want to make it more likely to predict the class that the expert actually suggests.",
            "So once we have a loss function, what we do with it?",
            "Well, we can simply append it to forward computation.",
            "So really, the way to understand this is that we just have two passes when we work with neural networks, we have a forward pass and backward pass in the forward pass, we basically computer neural network.",
            "We start off with our input, we pass it through all of the hidden States and at the end of it we come up with some output which we passed through our loss function.",
            "And then in the backward pass what we do is we compute the gradient and so we start off by computing the gradient of this loss function in red there and then we do a backward pass where we sweep back and we accumulate all of our gradients as we go back through the data as we go back through this compositional chain of functions and every step we can workout how much those particular parameters associated with that particular function influenced the loss function that we got.",
            "So if we multiply these together, for example, if you want to know, know how much do these parameters here influence the loss function that we actually got?",
            "All we need to do is to multiply together all of these partial derivatives and will actually end up with the computation just by the chain rule of how the loss function depends on Wii.",
            "Why do you like my PHN PHN by DH N -- 1 and so forth all the way back until we actually compute exactly the influence of those parameters on our loss.",
            "So this is the basic building blocks of house to machine learning with the deep neural network."
        ],
        [
            "OK, so once we have the loss function, what do we do?",
            "Well, we're going to build on perhaps the most common and successful idea in machine learning, which is gradient descent.",
            "So what we're going to do is to minimize the expected loss.",
            "So we've got our loss function.",
            "Now we're going to assume that there's some distribution over our inputs X, and So what we care about is across all of these inputs, we're going to pass them all into our neural network.",
            "We're going to look at the outputs and what we want to do is to minimize the loss across that whole data set.",
            "So that's just the expectation over this loss function and what the way we're going to minimize that is by following the gradient.",
            "So we're basically going to find the gradient of this expected loss with respect to our parameters, and that can itself.",
            "We can push the expectation.",
            "Outside here.",
            "Patient here who wants to take the derivative of this, we can push the expectation then and what we see is that we end up with the expectation for our derivatives.",
            "So what we really care about are these individual samples.",
            "So every sample is basically one of."
        ],
        [
            "These backward passes right in every sample we can do our forward pass and backward pass.",
            "And then there's a result of that backward pass."
        ],
        [
            "We're going to end up with some direction in which to adjust our parameters.",
            "And that direction is given by the derivative of our loss function with respect to those parameters, and so we're just going to every single sample, find the gradient, move a little bit in the direction which makes us get a better loss and adjust our parameters step by step by step and in expectation when we apply these updates over the whole data set, we find that we actually compute the true gradient of the expected loss.",
            "And So what we do is, we see that there's some kind of error surface, so these surface is kind of tell you.",
            "Well, you know how much error is there going to be?",
            "How much loss is there going to be?",
            "This is like a contour map of the loss over parameter space, and now the idea is we're just going to adjust our parameters a little bit to take us closer and closer to some local minimum in this space.",
            "So it's probably worth saying a couple of words about local Optima as well, which is, you know, a lot of people from outside deep learning.",
            "They worry an awful lot about the fact that we've defined some very complicated loss surface, and we're using gradient descent to find some kind of local minimum, and the question is, well, don't you just get stuck with some poor local minimum?",
            "And there's been a lot of empirical evidence and recently also some theoretical evidence, but actually this doesn't happen at all.",
            "And what happens is that our.",
            "Intuitions about high dimensional spaces are actually misleading, and it turns out that when you're in a very high dimensional surface, you're trying to find and use gradient descent to find some kind of optimum in that high dimensional space that by the time you actually find some optimum which is which is a stationary point in all of your many different dimensions, that with very high probability that local optimum is almost as good as a global optimum.",
            "And so this is true for many statistical loss functions that you can come up with and seems to be true empirically for most interesting problems as well.",
            "In other words, as the size of the parameter vector which you're optimizing gets larger and larger and larger, you need to worry less and less about local Optima and gradient descent just becomes essentially a very reasonable computational thing to do to help you find a good parameter that actually solves the problem you care about.",
            "So the motto in some sense is trust.",
            "Your gradients follow your gradients and and.",
            "And of course there are many, many clever algorithms.",
            "Then stochastic gradient descent, but at the same time stochastic gradient descent applied to a richly parameterized function is often enough to get really remarkably good results.",
            "OK.",
            "Yes, good.",
            "High like a million.",
            "Yeah, in this setting 20 is low.",
            "Yes, I like a million.",
            "Thank you.",
            "Yeah.",
            "No, no you will.",
            "So you will find a local optimum through through this approach, but that local optimum is highly probable to be close to the global optimum in its value.",
            "That's what the theory says.",
            "So, so there are many, many local Optima and this put people off neural networks.",
            "But my almost a decade the fact that there are exponentially many local minima.",
            "But it turns out that most of those exponentially many local minima really good minima.",
            "That's the story.",
            "You also have to be very careful about how you initialize your parameters for this to be true as well.",
            "Empirically, so yeah."
        ],
        [
            "OK, so deep supervised learning then, so I really don't want to spend too too long on the supervised learning story, but really what I want to summarize is just to say that deep neural networks have achieved remarkable success is state of the art for images, for audio, for language, natural language processing.",
            "And the ingredients, which are it used to, actually solve these problems?",
            "Are a really straightforward, so use a deep network as your function approximator.",
            "You define a loss function, and you optimize the parameters end to end by stochastic gradient descent.",
            "So in a sense it's almost like an excuse to not think too hard, you just kind of set your big model up and you attach gradient descent and you press go.",
            "And often this actually outperforms models which apparently seemed to be clever because they're breaking down the problem according to the knowledge we know about it.",
            "Message seems to be trust your gradients and the representation will find the right way to solve the problem given enough computation.",
            "So the big question then is."
        ],
        [
            "Can we follow the same recipe for reinforcement learning and so the rest of this talk is about how to do this?"
        ],
        [
            "OK.",
            "So.",
            "I'm going to start with just cherry picking a little bit of RL background, so I really I'm sort of aware that you've already some of you have sat through Michael Lipman's tutorial and seen all of this already.",
            "Some of you already know this, but some of you were in the other one and maybe haven't, so I just want to give that a couple of slides to make sure we're all on the same page with the type of reinforcement learning that we're going to use."
        ],
        [
            "So apologies if you are falling asleep.",
            "You know there's probably some comfort comfortable pillows around.",
            "You just make use of them for awhile and you know I wake you up in a minute, but I think it's really good just to make sure we're all talking the same language, so I'm going to start talking about policies and value functions.",
            "So I am with a little bit of sleight of hand going to talk about policy.",
            "I'm going to just refer to as it's terministic policy.",
            "Sometimes.",
            "It may also be stochastic, but for now I'm just going to represent it by this policy Pi so.",
            "You're in some state for some definition of state S. We've got some policy, which is a way to pick actions, so this is the way by which our agent selects its actions, and then we've got a value function which basically tells us how much reward our agent can expect to get.",
            "If it follows this particular policy Pi that follows this behavior strategy to pick all of its actions in all of the states that encounters, and So what we see is that we have this value function.",
            "I'm just going to focus on the Q values, the action values in this talk.",
            "I was trying to actually not even mention vehicle, but I've just got this one slide later.",
            "I couldn't seem to avoid it, so the action value function tells you basically how good is action A in state S. How much award will I get if I start in state S, take action A and then follow my policy pie from there onwards?",
            "So this is the quantity which we're going to optimize for, and we're going to use to evaluate how well we're doing in any situation."
        ],
        [
            "And then this gives us basically three approaches to reinforcement learning.",
            "So the 1st two are based exactly on the previous slide, where you know idea number one is to say OK. Well, we had this idea of a policy and the question is what's the best policy.",
            "So we're going to look for this policy Pi star, the optimal policy which achieves the maximum future award.",
            "And so this is really kind of direct policy set.",
            "Searching directly through the things we care about the policy parameters themselves.",
            "So it's defined policy parameters that actually solve the problem in the best way.",
            "The second approach is what we call value based reinforcement learning, where we work with the value function instead.",
            "And so here the goal is going to be to estimate the optimal value function.",
            "In other words, directly workout the maximum value that we can achieve using any policy.",
            "So that's this.",
            "Q, star Q Star basically tells us under all policies what's the maximum amount of reward we could get from any state by picking actions in the right way.",
            "So that gives us two ways to solve the reinforcement learning problem.",
            "But of course there's always a third way."
        ],
        [
            "This case, the 3rd way is model based reinforcement learning and in that case for model based reinforcement learning, the main idea is to build when I call a transition model and environment model, something which basically describes how the environment transitions from from state to state.",
            "I mean this can be from the agent's perspective as well, you know.",
            "Ideally it's something that basically says how the agent can explain the data it sees.",
            "How can explain the observations it sees within the world and which tells it how to go from one step to the next step and once it understands the world it's in, it can use this understanding of the world to plan to look ahead.",
            "Using its model and rolling forward this model step by Step 2, for example, build a tree and then start to figure out you know by doing a look ahead on that tree and figure out what's the best sequence of actions that will maximize the amount of reward against.",
            "So that's model based reinforcement learning.",
            "So."
        ],
        [
            "Really, the main idea of this talk is to say, well, if we're going to apply deep learning reinforcement learning, first idea should be to look at these three Canonical ways to deal with reinforcement learning and just plug deep learning indirectly, as the function approximator.",
            "So the idea is that we can use our deep network to represent the value function.",
            "We can use our deep network to represent the policy, or we can use our deep network to represent the transition model.",
            "And in each case we're going to try and follow the formula that was successful for supervised learning.",
            "This recipe, which was straightforward recipe of combining these models with stochastic gradient descent.",
            "We're just going to optimize either the value function of the policy or the model end to end by defining some appropriate loss function or some appropriate gradient and following that gradient using SGD and hope that that finds us a good solution.",
            "So in some sense, you know we can always be cleverer than this, But this is in some sense is the starting point to basically go back to basics with reinforcement learning and say how can we plug in DRL and effectively build on on the standard approaches to reinforcement learning."
        ],
        [
            "So I'm going to start by talking about value functions.",
            "And.",
            "For each of these, I apologize for just building on this idea of calling everything deep, but I am going to call everything deep so we can have deep value functions.",
            "Then deep policies and then deep models.",
            "So deep value functions.",
            "We're basically going to now plug in a deep representation and deep neural network as function approximator.",
            "To estimate Q2, estimate this value function and we're going to see how far we can get by optimizing for this thing directly I'm going to start off by introducing methods which have been known for perhaps.",
            "Two or three decades now and then I'm going to talk about some recent developments that have helped to make them much more robust."
        ],
        [
            "So.",
            "Michael Lippman, of course talked about this, but again, just to reiterate the Bellman equation, basically, we start off with our value function Q and we can unroll the value function by basically saying that the value Now the value in this state is equal to the immediate reward plus the value from the state, which I end up in.",
            "That's what this tells us, so the value function is basically the expected reward from time step T + 1 plus time step T + 2 plus time plus step T + 3.",
            "But if we just unroll this, we see that everything from this point onwards is exactly the value function for the next time step, and so we can always represent the value function in terms of these two parts.",
            "The immediate reward plus the reward from that time step onwards, the value from that time step onwards."
        ],
        [
            "We can do the same thing with the Bellman optimality equation and this tells us how to compute Q star.",
            "So the Bellman expectation equation just tells us what's the value function for some fixed pie where at the Bellman optimality equation tells us how to compute the optimal value function Q star.",
            "And so again we can unroll it in terms of the reward Now plus now the maximum action value from the successor state, taking the successor action at that point.",
            "And so we can unroll this by taking a Max over the action which we take at the successor state.",
            "And this gives us the well known or one form of the well known Bellman optimality equation.",
            "And So what we're going to do then, is sold for these two Bellman equations, and by solving for these Bellman equations, we're going to try and find either the value function for our current policy."
        ],
        [
            "Which is known as policy iteration.",
            "I'm all the value function for the optimal policy, which is known as value iteration.",
            "So in the first case, what happens is we take our Bellman expectation equation and we basically turn this into an iterative update.",
            "So the goal is for any given policy we want to work out how good is that policy.",
            "We want to work out how much award will I get under that fixed policy.",
            "So what we can do is we can use this Bellman expectation equation to figure out the value of our Q values and so we can basically keep updating our Q values by taking our Bellman equation and just plugging it in to give us a new iteration.",
            "So we have some idea of what if we had some previous estimates of QRQI.",
            "What we do is we estimate this target now by saying in this state.",
            "Here what I think the target should be is the immediate reward that I got plus the discounted value of the next day.",
            "So imagine you're playing a game.",
            "Imagine playing game of chess or something.",
            "You think you're in a winning position and then you play a move and you suddenly realize Oh no, I made a terrible blunder.",
            "I'm actually losing the game so that basically tells us there's a discrepancy between these things and what we want to do is always just look one step ahead and update our value that we had before towards the value after you made that move.",
            "After you took that, actually have some more accurate estimate of what the value is.",
            "Your Q value after that step is made and we use that as the target to update our Q value at the previous step.",
            "So in policy iteration we just use this Bellman expectation equation to give us and you estimate.",
            "So we start off with Qi.",
            "We plug that into the right hand side.",
            "We turn the handle and we estimate our new Q values.",
            "This Qi plus one from all of this right hand side.",
            "So it's just an iterative approach.",
            "We take a publication, we iterate it again and again and again, and that gives us a new Q value.",
            "Once we have that key value, you can improve the policy to find a better policy just by acting greedy with respect to it.",
            "I appreciate I'm doing this part a little quickly.",
            "I just didn't want to duplicate Michael stuff too much.",
            "So the second approach is is value iteration.",
            "So if value iteration we take the Bellman optimality equation.",
            "This second equation here we iterate over the Bellman optimality equation.",
            "So again what we do is we take the right hand side of the Bell Notes multi equation.",
            "We treat that as a target.",
            "We take out old estimate of Q.",
            "We plug that into the right hand side and we use it as a target to estimate what a new iteration is gonna be.",
            "So we basically start off with this target here, plug that in an estimate.",
            "He values our next iteration policy iteration and value iteration.",
            "Um?",
            "OK."
        ],
        [
            "So now let's get concrete and start talking about some algorithms.",
            "So the first algorithm I want to talk about is nonlinear Sosa.",
            "So this is a form of policy iteration, and the reason I'm presenting in this way is just so we can get some idea of whether you want to connect it together with deep learning and so actually you know some choices for the loss function that we should actually use to learn our Q values into end.",
            "So what we're going to do is we're going to start off by representing our value function.",
            "These Q values by a deep neural network by a deep representation, and so this Q network is going to be represented now.",
            "We've got this network that takes an input, the state and the action.",
            "And it has some parameters, W's or parameterized.",
            "This whole deep function.",
            "And we're going to use this network to estimate the true Q values."
        ],
        [
            "And So what we can do then is to basically come up with an objective function where we basically choose our loss function.",
            "The mean squared error.",
            "So we're going to basically have this loss function, which is the squared error between our target which came from the right hand side of the Bellman equation.",
            "Missing reward plus value at the next step.",
            "That's going to be our target.",
            "We're going to look at the difference between that target and our current estimate of the Q value.",
            "It's a little bit like taking the difference between the right hand side and the left hand side of the Bellman equation and treating that as R-squared error.",
            "So it kind of 1 tower RQ values to always represent the thing which we see at the next step.",
            "So we can take this mean squared error.",
            "And we can."
        ],
        [
            "Take the gradient of this.",
            "So as to arrive at an algorithm which we're going to call nonlinear Sosa.",
            "And So what we do is we differentiate this squared error, treating this target as a fixed target, just as in the Bellman equation.",
            "We treat the right hand side as a fixed target, and we update the left hand side towards the right hand side.",
            "And that gives us this saucer gradient here.",
            "Well, all we've done is applied the chain rule to this squared error, so we end up with this error term multiplied by the derivative of this error term and the target we're treating is fixed.",
            "So the only term which comes out of that is the derivative of our Q function with respect to the weights.",
            "So the way to understand this is that we're basically saying that we've got this.",
            "This loss function.",
            "We've got the gradient of the loss function is now we've got.",
            "Tierra town here.",
            "This error term telling us how much discrepancy there is between what we thought the value was going to be and what the value ended up being multiplied by this gradient term.",
            "That tells us how to adjust the parameters so as to make this this estimate more or less more correct.",
            "And then what we're going to do is optimize this objective end to end by stochastic gradient descent just by following this gradient.",
            "So this is a concrete way to take the deep learning idea.",
            "Apply it to reinforcement learning so the end to end.",
            "We can try and optimize for some objective function.",
            "In this case the squared error between our target Q values and our estimated Q values.",
            "So we can do the same thing with with value iteration."
        ],
        [
            "As well, and that leads to the Q learning algorithm.",
            "So all we've done here is."
        ],
        [
            "Exactly the same procedure."
        ],
        [
            "But now we apply we were using the Bellman optimality equation instead, so we end up with this Max over the actions that we take out the next step in our target.",
            "So I target is now the reward plus the Max over all the things we could do at the next step on the Q values at that step.",
            "Those become our targets, but again we follow the same procedure we represent their Q network.",
            "We use this Q network to represent our value function.",
            "We computer mean squared error.",
            "We follow the gradient of that mean squared error and we optimize the objective end to end by stochastic gradient descent.",
            "So we're going to call this nonlinear Q learning this algorithm.",
            "OK, let's get some examples.",
            "OK, Anna.",
            "Yeah.",
            "So, so the question from Anna there was, you know this isn't really the true gradient of this error because we're this term in blue were basically treating us fixed and we're ignoring the gradient term that comes from that.",
            "There are corrections to this which people in this room have have worked on.",
            "For now I'm just presenting the naive view where we can kind of view this as gradient descent by substitution, where we take a target we can substitute in this target and replace for a fixed target and follow the gradient with respect to that just to get some intuition into how we can actually do that.",
            "And later will actually do something which is slightly more principled.",
            "OK."
        ],
        [
            "So I'm going to take a well known example.",
            "Many of you will have heard of this, but I think it's helpful to place it in the context of the flow of what's come before, and that's the TD Gammon program.",
            "So this was Jerry Tesauro's famous backgammon program that had a small neural network with just one hidden layer with around either 40 to 80 hidden nodes and what it did was it basically took the input of the backgammon board.",
            "And this is the backgammon board, over here.",
            "Ground and Blacks trying to move all its pieces around this way and Meanwhile rates trying to move more around this way.",
            "And there's some rules associated to that, but basically roll some dice and then move your pieces and what Jerry Tesauro did.",
            "We just took the raw representation of that board and he kind of flattened it out into this representation that was used as the input in your network.",
            "So this representation basically says things like you know there will be one input saying are there 2 Blackstone's here?",
            "So are there?",
            "2 white stones here.",
            "And so there's this binary input representation.",
            "There was a flattening out of the board and that input representation was passed through a neural network and the output of the neural network was an estimate of the value function for that position.",
            "So this is where the V comes in.",
            "So sorry bout that.",
            "But it's an estimator saying how good is this position?",
            "Am I going to win from this position?",
            "And if the position is good, if you know if you think you're actually in a position where you're going to win this be will be high and the job of the neural network is to come up with the right representation itself.",
            "Without telling it the right features, it has to learn a representation.",
            "It has to build its own features.",
            "It has to build its own.",
            "This so the composition of functions is learning for itself.",
            "How to compose these very raw features into more structured, interesting features that are able to actually capture the value function very accurately.",
            "So."
        ],
        [
            "When this was actually used, it was initialized with random weights and it was trained by games of self play, so played itself many thousands of games and it used this nonlinear Sarsa algorithm and I'm using again a little bit of sleight of hand here.",
            "Just basically make it look like the algorithm I already presented, which is to say that we can think of the function approximator Q that I presented in my saucer algorithm as just being the after state value function.",
            "So we basically"
        ],
        [
            "Here we had a neural network that said how good is a position whereas the Q values tell you how good is the position.",
            "If I play a particular action."
        ],
        [
            "And so all we do is basically transform the V into a queue and then roughly the same algorithm applies there.",
            "And what was interesting about TV Gammon was that even though it was used, this training procedure was done quite naively, so there was no exploration.",
            "Just use the dice roll in the game to make sure that it reached a nice variety of different positions.",
            "The algorithm actually converged in practice very robustly, but it turned out that wasn't true for other games, and indeed not for other environments outside of games as well.",
            "But nevertheless it did extreme."
        ],
        [
            "Well, anti gammon.",
            "Just by training from self play, was able to defeat the human world champion very convincingly many years ago.",
            "And yet, what strange is that people sort of fled away from this approach just a few years later, and I think the reason is that people discovered that although it worked very effectively in backgammon, which has this very smooth value function due to the stochasticity of the dice, these nonlinear function approximators, when they're combined with reinforcement learning, turned out to be very unstable.",
            "And so when people try to do this, their algorithms just blew up and it led to this reputation of deep learning or neural networks at that time being very difficult to work with and causing all kinds of instabilities and oscillations and divergents and craziness."
        ],
        [
            "OK, one detail I should mention."
        ],
        [
            "I presented this as if these features were completely by scratch.",
            "Starting from the raw inputs, but this original victory."
        ],
        [
            "Which was presented here, actually used some handcrafted features, but it turns out Jerry presented some results few years ago when."
        ],
        [
            "Went back with some kind of more advanced hardware, reran the exactly the same experiment, but just let it train for longer and he started off just with the raw input features that I presented, and it turned out that as you added more capacity to the neural network going from 10 to 20 to 40 to 80 hidden units, still keeping just one hidden layer, it did better and better and better.",
            "And when it was trained for 10,000,000 self play training games, in actually outperformed significantly the original TD Gammon, and so I think it's fair to say that this was done completely from scratch.",
            "And represents the first significant success of RL building its own features completely end to end from scratch just by following gradients."
        ],
        [
            "OK, so now it's time to deal with some of those pesky stability issues, like how can we actually make things work better.",
            "So what are the problems?",
            "First of all.",
            "So if we use naive Q, learning or Sosa and combine it with a neural net, we find that we get oscillation or divergent for perhaps three reasons.",
            "I'm just going to highlight these three reasons.",
            "There may be some other small issues, but these seem to be the most significant ones and this is really from quite an empirical point of you.",
            "I'm not going to present theory saying that any of these new ideas actually guaranteed to converge, I'm just going to present empirical results showing that we can address these issues and come up with a robust end to end learning algorithm where we can apply IRL with an arbitrary deep network.",
            "So.",
            "So the first problem is that the data that we see in our L is sequential.",
            "You know we have an agent living in an environment and you know if you've got some robots and it takes a step and then it takes another step.",
            "The second step it takes is very correlated to the first step and you would expect that the value functions for these things are going to be very correlated as well, and so we have this extremely non IID setting so.",
            "This really is the first problem.",
            "The first issue, why if you just try to naively apply supervised learning methods, they sometimes just fall over.",
            "The second problem is that the policy itself can change dramatically, so if you just imagine one situation where where the Q value, let's say you can either go left or right and it turns out that going left seems to be a little bit better than going right?",
            "Well then, most of the time your agent will go left.",
            "This part of the world maybe the world is this stage, so now I'm going to visit this part of the world vastly more than this.",
            "Other parts of the world.",
            "And so now I have a distribution of data which is all about the left hand side of.",
            "But maybe some random event happens and all of a sudden something switches and now I think that going to the right is slightly better.",
            "So something just small changes in my Q values.",
            "Well now I'm going to go over to the right hand side of this world to explore the right hand side of this world a lot.",
            "I'm gonna get distribution of data that comes almost entirely from the right hand side of the world and.",
            "My tight data distribution will have dramatically swung from one extreme to the other.",
            "And this can lead to real problems.",
            "For supervised, you know, for building on this sort of simple supervised learning ideas aren't designed to deal with these disks.",
            "Swings from in the distribution of data.",
            "So we're going to try and address that problem as well, and the third issue is 1, which is.",
            "I think specific to these neural network representations, which is?",
            "It turns out they're very sensitive to the reward scale.",
            "In other words, if you plug in into one of these neural networks.",
            "If you take one of these backward sweeps."
        ],
        [
            "And you plug in some very large value here because of the nonlinearities that we have in the backward propagation, everything explodes.",
            "So if you just naively try to back propagate gradients where your loss function is not well controlled for the neural networks aren't well behaved, and so this is one of the issues that has to be dealt with in a principled way.",
            "Otherwise things just don't work too well.",
            "That's not going empirical point.",
            "So."
        ],
        [
            "What we introduced with this idea called Deep Q Network switches.",
            "One stable solution to the deep value based reinforcement learning paradigm and so the idea was to specifically address these three issues which are known to cause problems to destabilize neural networks, and so the first idea is to use experience replay.",
            "So the idea is very similar to break the correlations in the data that we basically go back over the data.",
            "So we generate the data.",
            "So maybe I go to the left and then I go to the right.",
            "But when I actually come to learn from that data, I randomly sample from all of the data I've seen so far.",
            "And that breaks all of the correlations in the data and essentially brings it back to an IID setting where all of the samples that you use.",
            "Because you're randomly drawing them from all of the experience that you've seen so far.",
            "Everything is completely decorrelated now.",
            "And it also has the advantage that we can learn from past policies.",
            "So again, if we go back to this situation where I first of all visit the left and then visit the right if I learn completely online, I will only ever be learning from the particular distribution that I'm in at that moment like now I'm in this world where I'm over at the right, I completely forgot about all the learning which I did while I was over there.",
            "Whereas if we use experience replay, we mixed together the experience which was generated by the policy that took me to the left and the policy which took me to the right and it makes it much more robust against these kind of oscillations that you get in the in the policies and therefore the distribution of data.",
            "Yeah question.",
            "Yeah, OK, so great question.",
            "So Johnny was asking.",
            "If one of the losses of this approach, the experience replay it gains you this ability to stabilize by decorrelation.",
            "But at the loss of keeping the temporal sequence, which lets you do methods like eligibility traces.",
            "And and I think that's absolutely true that there is a lost to this approach, and I think there are alternatives, and I think it's certainly something we're keen to explore.",
            "But yes, I mean the methods which we are presenting our naive methods that just completely randomized over the experience and only look at individual experience samples.",
            "I think the simplest approach that you could take to bridge the gap would be to do what long dealing called lessons where you basically replay short.",
            "Sequences of experience and that might give you just long enough eligibility traces to be able to get the best of both worlds.",
            "It's probably worth saying that when you when you do learn from past policies, you have to use an off policy learning algorithm because you need to learn from policies that we were using before, which are different than what you're using now.",
            "So if you're in that setting, your eligibility traces can never get that long anyway, because typically your new policy will do something different at one of the steps.",
            "Eventually anyway, and so I think having these lessons may be a way to just bridge the gaps that you don't have to go too far to get most of the benefits of traces, but I think that's that's an open question and absolutely, we've we've kind of sacrificed that for now, and we will definitely want to get that back at some point.",
            "So the second idea that we use is to freeze the target Q network.",
            "So really what we try to do is to get a bit more back to the spirit of really value iteration by freezing the right hand side of the Bellman equation and just treating that as a target for awhile and learning about that right hand side.",
            "Without changing it, so it basically freeze the target network, learn our new iteration and this helps avoid some of the oscillations and again break some of these correlations between the Q network in the target.",
            "This is an idea also known as fitted Q iteration.",
            "And finally we either clip the rewards, which is the naive approach, or more recently we normalize the network adaptively to give us some robustness to the different magnitude of rewards that you might see."
        ],
        [
            "OK, so just to spell that out a little bit.",
            "OK, So what we do is we build a data set from the agent's own experience.",
            "We take an action, we store the transition that we observe from that action in our replay memory, we sample around a mini batch of transitions from our replay memory and then just as before we optimize the mean squared error between the queue network and the targets.",
            "So just as before, we look at these targets here.",
            "We are squared error between that, but we're using experience replay now.",
            "So what we've done is we've changed the expectation instead of being over the real sample the the online samples that were actually encountering.",
            "We actually draw these samples from our own experience replay memory.",
            "So this is an expectation with respect to our own sampling process."
        ],
        [
            "And then the second thing we do is to fix these target networks and all we do there is.",
            "We basically keep track of some old parameters these W minus.",
            "So we freeze these old parameters and we update our mean squared error.",
            "Now we look at the mean squared error between these frozen targets and the frozen parameters.",
            "So it frees up parameters.",
            "We treat that as a target for some period of time.",
            "We hold these parameters frozen.",
            "RQ targets now become fixed, so it's just like we're doing supervised learning towards these targets with a fixed target network.",
            "And now we look at the mean squared error between these fixed targets and our current estimate of the Q values.",
            "Just like in fitted Q iteration.",
            "And then what we do is we really periodically update our fixed parameters.",
            "We can also do this online in a smooth way, and so we have an online way to do something like fitted Q iteration.",
            "So this addresses these instabilities.",
            "OK, so I want to get into an example."
        ],
        [
            "So I'm going to talk about the Atari domain now, so these hormones are really fun domain introduced by oh Hey, yeah, sure."
        ],
        [
            "So regarding the question of correlation in the samples, then you say that it makes the whole system unstable.",
            "So can you give provide a bit more intuition what?",
            "What is the actual fragile Pirates in the whole algorithm?",
            "Because is it stochastic gradient dissent that will break if use correlated samples?",
            "If that's the case, I mean it is not at least some towels.",
            "Got results are kind of robust with having dependent.",
            "Series where price is going to the stochastic gradient descent and also we use like something like momentum which artificially add some kind of dependence between the direction of the updates.",
            "How is that good in that case?",
            "But using correlations in samples or bad?",
            "OK, so the question was to give some more insight into how why correlations problematic.",
            "So I think I want to say that yeah, even in the even in the simple setting, if you were just to do supervised learning with the stochastic gradient descent correlation, basically.",
            "In some sense, it pushes down the effective weight rate at which you can learn, so you can always remove correlation by having a very very low learning rate, so that effectively by the time you've moved on you kind of break the correlation by just not changing your parameters very much between steps.",
            "But what happens is you have to push down if you have extreme correlations, you have to push down your learning rate an extreme amount, and in practice to actually see anything actually happen in your.",
            "In your experiment, you're forced to increase the learning rate to a level which actually causes things to become unstable and break.",
            "That's the intuition I have.",
            "I think that's the first issue, which is correlation.",
            "Just in the data.",
            "Then you have correlation between the targets and the Q values, which is more about the bootstrapping process, which is a separate source of instability, and so there we basically reduce things to something a little bit more like supervised learning.",
            "So at any given moment, you're just doing supervised learning towards a fixed target, and therefore you are following a well defined gradient iteration by iteration you are following a true gradient."
        ],
        [
            "OK, so I'm going to talk about the Atari domain.",
            "This is a fun domain where basically what we're going to do is sit some agent down in front of this Atari emulator, so it's just as if you have some Atari emulator available and what our agent is able to do is it just gets to watch the video screen it gets to watch this video playing of what's going on in the game.",
            "It sees the score that's its reward signal when it gets to take controls which are basically the 18 different actions which are the nine different joystick directions times pressing fire or not.",
            "And so those are the controls it gets to take, and the idea is that it doesn't get told anything about the game itself, and any game might be plugged into this emulator.",
            "Here has a bunch of different cartridges which you can play.",
            "And what we do is we plug in one of maybe 50 different games into this emulator and we ask the agent just like a baby sitting down in front of this thing.",
            "We ask it to learn from scratch how to perform well in this domain end to end without giving any additional clues.",
            "And."
        ],
        [
            "And So what we do is we apply this.",
            "We applied this DQN algorithm so it's end to end learning of the Q values directly from the raw pixel inputs.",
            "So we just."
        ],
        [
            "Divide.",
            "Frames for the last frame.",
            "The frame before that, and a stack of four different frames as the input, which sort of characterizes what's just been going on recently in that game.",
            "This avoids different artifacts from like bullets flashing in and out, and so forth to give some limited history to deal with partial absorbability."
        ],
        [
            "And then the output is just this joystick action and the reward is just the change in score for that step.",
            "And what we do is we choose a convolutional neural network.",
            "Is the neural network representation is our deep representation that we plug in as our Q network is our function approximator for the Q values?",
            "We plug that in.",
            "We have this convolutional network that basically learns things like, you know, is there an alien at this particular location in Space Invaders and it can learn that across the whole set up and figure out this internal features and.",
            "Construct its own hidden layers and its own hidden features characterizing what's going on in the whole screen and at the end of it it outputs AQ value for all the different actions.",
            "So that means in a single forward pass, it can estimate the Q values for all the different joystick actions and therefore picking an action only takes 1 pass rather than having to do a separate pass for each action.",
            "So what we do is we use the same network architecture, the same convolutional network, actually as an additional layer in the results, I'm going to present in the hidden units and we fix the network architecture.",
            "We fix all of the hyperparameters, like the learning rates and so forth.",
            "There's nothing that actually is tuned to the individual game, and we try this across all 50 Atari games."
        ],
        [
            "And these were the results we got relative to a human.",
            "So this line here roughly indicates what we where you can't statistically tell the difference between our human expert tester.",
            "And the agent and everything to this side is outperforming our human tester and everything to this side is doing worse than our human tester, including in some games, pretty disastrously worse.",
            "These are in the more challenging games over on the right, like Montezuma's Revenge.",
            "And what we have here is basically the performance measured by actually playing real games using the trained.",
            "So at the end of training we just evaluate the test performance on a bunch of separate games.",
            "Once it's completed training."
        ],
        [
            "So it's fun to look at a demo of this.",
            "So maybe I can just talk through some of these environments.",
            "So in euro is like a racing game where it has to kind of figure out how to get past all these different cars and the environment switches from ice to grass and ask to learn a different strategy where it actually this the ice is much skinnier, has to learn a different driving strategy.",
            "This game is called River rate, so it's a side scrolling game, whereas to learn to shoot the fuel avoids the things that cause it to die and find its way through.",
            "Navigating through to their maximum amount of score.",
            "So what you start to see is just how different the representations are.",
            "Some of them are sort of pseudo 3D like this one.",
            "This battle zone like this pseudo three primitive pseudo 3D representation as to kind of learn to find its perspective, shoot these things, freeway possibly the stupidest game ever invented.",
            "To get the chicken to cross the road and without getting squashed by a.",
            "So this one's demon attack.",
            "So you get fast waves of attacking aliens coming down.",
            "You guys are probably Mike bowling, talking bout this.",
            "He's great at presenting this Atari platform.",
            "This is Pong which is the classic game where we're actually controlling the greenback here and trying to find the angle that just kind of pings the ball past the opponent.",
            "So we're just playing in single Agent mode here against a fixed computer.",
            "Opponent finds this particular angle that that's able to win every time.",
            "This ones beamrider so another kind of shoot'em up space Invader game.",
            "But with this kind of slightly pseudo 3D perspective to it, it has to avoid all the bullets coming down and maximize its core.",
            "We got seaquest.",
            "We have to learn to.",
            "Basically it's oxygen starts going down and down and down.",
            "It has to learn to go right up to the top and refuel its oxygen before carrying on a shooting things again.",
            "This is Cuba or it has to kind of bounce around and turn the whole game to one particular color.",
            "Go up on this magic platform up to the top again.",
            "And as to avoid this snake thing called coily.",
            "So very very different representations, and so in some sense it should be surprising that this convolutional network is able to learn the right features across all of these different games to be able to find exactly the features that we need.",
            "And yet even if it's side scrolling or pseudo 3D, it's actually this simple idea of building up these features region by region is sufficient to actually understand and characterize the value function sufficiently to play the game.",
            "Well, this is the classic space invaders, where it shoots the mother ship there.",
            "Takes like this human like strategy of taking out the Space Invaders column by column.",
            "Which gives it actually more time for them to move from side to side.",
            "It's quite a nice strategy to choose.",
            "And hides behind these these blocks there.",
            "That's the shelter itself.",
            "Yeah.",
            "Yeah, good clarification.",
            "So the question was, is it 1 representation shared across all games or separately learned representation for each game?",
            "It's a separately land representation for each game so the baby gets reborn for each game separately has its own life where it's exposed to just that game only and it learns the representation just for that game.",
            "The other problem is of course interesting to David.",
            "Yeah, in many of the games, it seems like the computer player is very very jumpy.",
            "Did you have any like a human doesn't play that way?",
            "Did you have any cost on the actions or on smoothness of action?",
            "Yeah, we chose not to put any additional costs on the actions so you could of course encourage it to play more smoothly, but there's nothing in the score itself that says you should or have too.",
            "So we just went with the simplest version.",
            "OK. Do you train a separate method for each action?",
            "Or do you combine state and action information and use to train a separate network rejection?",
            "No, it's one is 1 network that estimates the Q values for all actions, so all the features are shared like the the network is common amongst all the action, so so the features are re used to estimate the value of different actions.",
            "See if the action just comes in the final layer.",
            "OK, can you show some examples of games where the system is not so good and the human is better?",
            "I don't have videos.",
            "I can tell you about it.",
            "Yeah, I can tell you about them.",
            "For some reason we weren't so keen on making those videos again.",
            "So so the hardest Atari game is something called Montezuma's Revenge, and it's basically like 100 different rooms connected together in like a giant maze with loads of keys and doors and you have to get the red key to get through the red door and then go backwards and forwards through this maze.",
            "I mean we don't even get through, you know we don't get the first key for the first door.",
            "I mean, it's just you know the the search space is so vast that getting off the ground with random exploration is just too challenging.",
            "People have done well in the planning setting in that in that domain.",
            "I'll take one more and then I'm going to move on to some other parts and we can have more questions at the end of this.",
            "OK, was your goal to build a model that was better than human being initially, or did you want to build a model that is equally as good as human?",
            "Because if your goal is to build something that acts like human beings, then you should probably consider forgetfulness or some failure in actions.",
            "Or yeah, so we didn't impose like biological constraints like noise or.",
            "Reaction time or anything like that.",
            "We just again kept with the simplest version.",
            "I think it would be interesting to try that.",
            "Honestly, I don't think it would really change their results qualitatively, because we're already doing kind of some naive things like duplicating the action four times in a row.",
            "So although the inputs coming at 60 Hertz, we only select actions at 15 Hertz and then duplicate.",
            "And maybe yeah, I think it'll be interesting.",
            "OK, if it's OK, I'll take more questions on this at the end, so I want to."
        ],
        [
            "Say a few more things.",
            "So first of all, the question is, you know, how much does this really help compared to just doing, say, naive Q learning?",
            "So this is basically teasing apart the components of the algorithm piece by piece, and so if we just start off with Q learning, we see that across a sample of five games which weren't specially chosen to make appoint.",
            "These were just the five games we originally tried.",
            "We see Q learning actually does pretty badly and games like breakout it gets like 3 points when we combine it with the target Q.",
            "This idea of freezing the target network and just doing kind of any given time following the gradient towards that target network, we do significantly better across all games when we add replay in.",
            "That makes a very large difference across all games and combining them together does better again and so in some sense it's the difference between and are not working at all and working, so it's.",
            "Although the ideas are simple, I think that sort of perhaps at least if you take this approach, we can think of them as necessary ingredients, or at least you need to deal with the issues that Q learning has in some way to achieve the kind of performance that we really want to get towards the basic algorithms just aren't stable enough."
        ],
        [
            "Anne.",
            "I just wanted to make a brief comment that you know in the original nature paper we always just clip the rewards to minus 1 + 1, which is a complete hack and many are L people are probably, you know.",
            "Feeling very uncomfortable when they saw that and so you should in some sense.",
            "I mean it works very effectively, but but it actually changes the problem they're trying to solve because it means that we can't tell the difference between the mother ship in Space Invaders and just blowing up in a single alien.",
            "So what we?"
        ],
        [
            "I've done more recently is actually to normalize the network output.",
            "There are many ways to do this in the deep learning literature we we chose one.",
            "And."
        ],
        [
            "This works quite nicely, so if I show for example, what happens in Pacman, which was?",
            "Again, we were doing very poorly on before.",
            "We can see that.",
            "Now we've got the ability to tell the difference between eating a ghost.",
            "Once you've got power pal, which gets you a lot of points and just eating the normal pills and it lends this nice strategy where it really just homes in on the big points so it gets the power pill, gobbles up all this ghost, and then it learned a strategy that are humans didn't figure out which is.",
            "It gets a second power pill here now.",
            "It basically goes and hangs out by the Dem.",
            "And it waits for them all to come out and it just gobbles them up as they come out.",
            "And it gets this huge score.",
            "Which is quite nice to think about.",
            "So it's um.",
            "So I think the moral there is, you know, always solve the problem you care about rather than a simplification.",
            "If you can.",
            "And so I think we've taken, you know, maybe one more step towards that."
        ],
        [
            "Um?",
            "OK, so in the remaining time I wanted to talk about policies and models a little bit so.",
            "So let me at least give some.",
            "Basic intuition into approach to do deep learning with policy searching in real."
        ],
        [
            "So I'm going to focus in particular on the continuous action case, so so you know, this is motivated by the fact that the methods we've seen so far they don't directly apply to domains with continuous action spaces.",
            "Why?",
            "Because when we've got a discrete action space we can easily Max over those actions like in Q learning, whereas when you have continuous action space, performing that Max is tricky.",
            "It's an optimization problem in its own right, and you simply can't afford to do that optimization affectively every single step.",
            "And So what we're going to do instead is to parameterise the policy directly using a deep network.",
            "This pie.",
            "And now we're going to have a different parameter vector you so W as the parameters for our Q network.",
            "Now we have parameter vector U which is going to parameterise our actions directly.",
            "The way in which we pick actions, and.",
            "So what we're going to do is basically define an objective function like the total reward that we get across the whole the whole domain, and we want to basically ask how can we optimize this objective function end to end by stochastic gradient descent.",
            "This is our goal and to end STD.",
            "In other words, adjusting the policy parameters in the way that gets more reward.",
            "We want to do this in continuous domains, building on what has been effective and what we've seen working with deep learning in the in the discrete domains."
        ],
        [
            "So the approach that we take is based on the deterministic policy gradient.",
            "So this is just a simple observation that that the actual true gradient of the policy with respect to this objective function can basically be described as the direction that most improves Q.",
            "That we knew our Q values for any given policy.",
            "All we need to do is to adjust our parameters in the direction that gets us more cute.",
            "He was telling us how good our policy is, how much award we're getting.",
            "So if we average that over all of the states that we actually visit, it turns out that the gradient really is just the gradient of these Q values with respect to our policy parameters.",
            "So what we're going to do is follow that gradient.",
            "We can adjust our policy parameters to get more Q.",
            "And so that requires us to know Q."
        ],
        [
            "And So what we're going to do is use something called the deterministic actor critic Owen.",
            "Actor critic is basically a algorithm has two parts to it.",
            "It has a policy, the actor which has its own parameters.",
            "So this is actor here, something which we started.",
            "This stage, we've got a policy parameters and maybe it goes through a bunch of different hidden States and ultimately it outputs the action we're choosing.",
            "Then we've got a critic, which is basically the value function, and the critic is basically there to evaluate our policy to tell the actor how well it's doing, so the critic.",
            "Stop taking this state in an action and it has some parameters that actually goes through again its own hidden States and maybe this is very deep network and at the end of its output Q value and that Q value its job is to estimate the value function for our current policy for the policy being used by the actor.",
            "And So what we're going to do is basically use the critic as a loss function for the actor.",
            "That's how we're going to do this deeper actor critic.",
            "So what we're going to do is sequence these things together.",
            "So we're going to start off with our state.",
            "To our policy function using our policy parameters to go through some hidden States and eventually pick an action.",
            "But now we've got our states in our action going to plug those into our Q network, go through some more layers and eventually output our Q values at the end of it.",
            "And once we can do that, well, we know from the first part of the talk that we can just compute the gradient of this whole thing.",
            "Now we know how to compute the gradient of.",
            "In other words, we know how to adjust these policy parameters so as to get a small queue at the end of it just by reversing the flow.",
            "And applying the chain rule and propagating these gradients all the way back through this network.",
            "So we use the critic as a loss function for the actor.",
            "Sequence them together flow the gradients all the way back through."
        ],
        [
            "OK so yeah."
        ],
        [
            "I'm.",
            "So it turns out you can do this with discrete actions, but it just collapses to the standard stochastic policy gradient, so it turns out to be identical.",
            "You can basically push the stochastic your policy into the environment and so basically you kind of pick the parameters of your softmax or whatever.",
            "Treat that as a terministic policy, and then the randomness.",
            "You can imagine all happens in your environment, you end up with exactly the same, which is reassuring.",
            "You basically end up with exactly the standard stochastic policy gradient.",
            "So the reason that we choose this approach is precisely because in a continuous domain there is this gradient signal that fault that we can follow and can get a more efficient gradient, and we want to make use of that and build on it and not throw away good information when we have it."
        ],
        [
            "So what we're going to do is we're going to basically use this actor critic.",
            "Then we're going to estimate the value of the current policy by using Q learning.",
            "So let's build on what we've seen in the first half of the talk, and the value based approach.",
            "So now we need to get our critic to get the right value.",
            "The critic needs to figure out how good is this policy we're following, so we're just going to use Q learning there.",
            "So we're going to basically follow the loss.",
            "Minimize this mean squared error loss to adjust our Q values towards the target in blue there.",
            "And at the same time we're going to update our policy parameters in the direction that improves queue by basically."
        ],
        [
            "Going all the way back to identify the parameters that best that help us to maximize our Q values, so that's just."
        ],
        [
            "Chain rule.",
            "To find the direction that gives us more Q.",
            "So the cube ID you the way that our value function depends on you is just the Q by Dada by DU is the chain rule."
        ],
        [
            "OK.",
            "So, so this seems very nice, but again, it doesn't work, it's unstable and So what we have to do."
        ],
        [
            "Is apply the same toolkit that we've already seen for DQN?",
            "Can we be applied to the actor critic and it also stabilizes the actor critic and gives much nicer results?"
        ],
        [
            "So the idea is basically to apply the same three steps to stabilize the actor critic.",
            "So we're going to use experience replay for both the actor and the critic.",
            "We're going to freeze the target network to avoid oscillations.",
            "In that case that this in this case it means we freeze the network for both the actor and the critic.",
            "So we're going to keep old parameter values for both our policy and for our value function.",
            "And we're going to update our target for our Q.",
            "Learning is going to be the old frozen target, so this is really like a fixed target.",
            "So we are still doing supervised learning towards these targets at any given time.",
            "And then what we've done is all we've done from the previous slides, with game replaced our instead of learning this thing online.",
            "Learning by sampling from our own experience.",
            "So we take a step.",
            "We add that into our database of experience in every step.",
            "What we do is we sample of a mini batch of experience from our own experience replay database and we.",
            "Follow the gradient with respect to that mini batch, so this gives defines the gradients that we follow during learning."
        ],
        [
            "OK, So what we did was we applied this to continuous control now so we took the majokko simulator for physics.",
            "We made a bunch of different domains.",
            "We passed it through exactly the same confident that we used in the first half of the talk.",
            "Starting from a stack of frames again passing it through a confident and then giving RQ values, but also starting from the same stack of frames with a different set of parameters and giving us our policy.",
            "So two separate networks, and this is some recent work by Tim Lillicrap and another deep mind."
        ],
        [
            "So we have a look at how this works now.",
            "OK, so starting off with some when it says low dimensional features this is basically problems where we actually provide the correct state representation to the agent.",
            "So the first part of this demo is all using the correct state representation.",
            "But what we can see is it's very robust and again, we're using exactly the same representation, exactly the same hyperparameters across quite a range of different tasks, and it's able to effectively learn to solve these tasks.",
            "From scratch without any additional guidance about how that problem work, so we don't tell anything about the dynamics of the problem or give any clues.",
            "This one ask to balance this quadruped, which likes to fall over a lot.",
            "This ones moving a block to a particular target.",
            "This is a it's called the Cheetah Half Cheetah task.",
            "This is another kind of gripping and moving task.",
            "It is also able to move the thing around a little bit.",
            "This is a Walker.",
            "Then it's quite a fun gate there.",
            "And now we're actually learning or from raw pixels, so this is what it actually looks like to the agent.",
            "Kind of sees this very kind of crude representation, so it's got like a camera fixed and watching the problem from above.",
            "So imagine there's just this camera fixed on it, and it just sees the pixels.",
            "It's not told anything about the state representation, and it has to build its own representation directly from scratch, and learn how to balance here.",
            "And this is a 3 dimensional kind of Reacher task is to move this thing around.",
            "This one's a bit hard to see, and then this one is a.",
            "Is a racing game.",
            "I have continuous actions.",
            "This is the same one with.",
            "This is what it actually sees when it learns it directly from from pixels.",
            "It's kind of.",
            "Even though at this very crude representation it learns something, there we go.",
            "OK."
        ],
        [
            "OK, so we have a few minutes left to talk about models.",
            "So summary so far.",
            "So we had our recipe.",
            "We wanted to do end to end deep learning where we define a gradient.",
            "We follow that gradient end to end and we come up with some way to kind of represent a loss function for value functions.",
            "We found.",
            "Now using this deterministic policy gradient subgradient that we can follow to adjust our policy parameters directly.",
            "And now what about models?"
        ],
        [
            "So model based reinforcement learning, we the idea is to learn something like a transition model of the environment by transition model.",
            "I just used the word transition there because if you talk about models to anyone in deep learning they get really confused.",
            "So so transition model.",
            "Here we mean the way that the environment has works like probability of getting some reward and some next state given the state you're in the action you Turkey for some definition of state which could be the agent's own state representation.",
            "And what we're going to do is, once we learned this model, we know the reason that model based RL is appealing is that once you've learned such a transition model, you can in principle plan with that transition model, which it feels like should give us a big win.",
            "It feels like we should be able to learn very efficiently because we learn this compact model that represents how the structure of the problem actually works, and then we can do this very powerful look ahead.",
            "We can plan into the future.",
            "We can do that research.",
            "We can do all kinds of things so as to find the optimal actions.",
            "So if you imagine you're playing this game seaquest, you might do this by if you had a model.",
            "I told you what would happen if I if I took the move joystick select and then what would happen if I move the joystick to the right.",
            "I can compare that to what would happen if I move to the right and left and I can change through this hypothetical consequences and workout what the best string of actions might be and therefore the best actions take at the start state.",
            "So that's the appeal of model based RL."
        ],
        [
            "And So what I want to tell you is there's sort of some good news and some bad news.",
            "The good news is that, at least so far we and others have been able to do quite a good job.",
            "I think of actually learning what appears to be a nice looking transition model using a deep network.",
            "And there are various ways you can do this, and I don't want to kind of get into the details of this, except to say that you can define objective function that measures the goodness of the model and the one I'm going to show you actually used the number of bits required to reconstruct the next state.",
            "So like an information theoretic.",
            "Loss function, but it's exactly the same end to end principle where you define this loss function you optimize for that loss function by stochastic gradient descent and you just pump the data through as much as you can.",
            "Anne."
        ],
        [
            "You end up with something which looks quite nice in the game of Atari, so if we do this.",
            "So this is that River rate gain.",
            "The sidescrolling game that we were looking at earlier.",
            "And here we asked it to fantasize a complete trajectory.",
            "So it's not being reconstructed every step.",
            "This is like seeding at once and telling it to just run forwards for the rest of the demo.",
            "So this is 1 long hypothetical trajectory generated by the model, and you can see it's doing a pretty reasonable job of kind of hypothesizing all the different structures.",
            "We just did it in black and white for the outlines of the objects, which is why it looks a little bit different.",
            "But it's got some idea of what's going on in the game.",
            "It kind of comes up with interesting objects.",
            "And yet when you actually use this to plan now, you plug this into your favorite planner and you try to plan with it.",
            "It's horrible, does much, much worse than the model free methods that we saw earlier, and the question is why?",
            "So?",
            "I just wanted to kind of leave you with this.",
            "You know, that's the last thing I really want to talk about is this big question?",
            "Why?",
            "Why is it so hard to?",
            "Get the model based approach working effectively."
        ],
        [
            "And so I think really, one of the fundamental issues which we're facing is the problem compounding errors so.",
            "In something like Atari, we have this kind of messy state space.",
            "You've got these pixels high dimensional pixels going to another step of high dimensional pixels, and so inevitably you're going to have some error at the one step level.",
            "But the problem is more that an entire trajectory lost 10s of thousands of steps, and so over the trajectory those errors compound until by the end of the trajectory you might have something somewhat meaningless, which actually you know you think you're going to imagine this whole trajectory through that that imaginary kind of River rate scenario.",
            "And at the end of it, perhaps you imagine some Mother ship appears and you get to shoot it and get 1000 points, whereas in reality that just might not happen at all, and so the air is just compound and compound.",
            "This also happens the hidden level of the neural network, so you kind of get these quite abstract compounded errors where you can kind of fantasize things which are quite inaccurate at a high level, even if they look correct at a low level.",
            "And so by the end of a long trajectory, the rewards can just be totally wrong, and for us at least, and others we've spoken to on this model based IRL hasn't done so well in these types of domains.",
            "And so."
        ],
        [
            "So.",
            "There's an alternative, and I think it's an interesting alternative, which is to try and plan implicitly.",
            "And So what I want to kind of get you thinking about is this idea that actually, you know, we can think of a neural network as a computational process where we started off with this sequence of functions and every one of those functions is performing computational step.",
            "And so in some sense, those computational steps can learn to do the steps of planning for themselves.",
            "If you trust the gradients of your neural network, the great their neural network can actually learn to plan for itself, like if you allow it to do 20 layers of computation, those 20 layers of computation can essentially look ahead 20 steps in some kind of hypothetical plan.",
            "So the question is, you know our transition model is actually required at all?",
            "Or is the network kind of implicitly learning some internal representation of model that doesn't correspond to our own semantics and our own expectations of what should go into a model?",
            "So I wanted to leave you with one example, which kind of surprised me the most, because if you'd asked me to come up with one domain where model based planning.",
            "So if you ask me, you know, come up with one domain where you absolutely have to plan where model free approach is just never going to compete with something which explicitly plans into the future.",
            "I would have chosen the game of go, so this is something I used to work on.",
            "It's extremely tactical.",
            "You have these very complicated tactical situations with very sharp values where if you just play one stone in the wrong place.",
            "You end up completely transforming the value of the position into something from winning to losing.",
            "I."
        ],
        [
            "And yet it turns out that.",
            "Yes, OK, so that historically the background is that Monte Carlo search methods, which is 1 particular way to look ahead planning, have been extremely effective in the game of go.",
            "They were the first kind of breakthrough that led to the first strong programs, and so there was this first wrong program logo, for example, and Mogo was able to look ahead millions of positions into the future at rollout.",
            "All these positions going into the future, and it would use those rollouts to estimate the right thing to do.",
            "It was doing planning, but by rolling out and hypothesize in the future using a perfect transition model.",
            "And so the question is, how well can we do with just a neural network which just does these steps of computation?",
            "Surely it can't compete with an explicit search that looks ahead millions of positions, but it turns out."
        ],
        [
            "But it does so when we built a 12 layer convolutional neural network and we just trained it by supervised learning to predict expert move.",
            "So this wasn't really RL yet, so we just asked it to then look at one position so it just looks at the position that there's this convolutional network that passes that position through 12 convolutional layers, and then at the end outputs of probability that the expert would have played each one of those moves.",
            "So it's trying to predict which move the expert would have played and so outputs probabilities.",
            "The expert would have played in each position.",
            "And then we ask it to play the game.",
            "So we use those probabilities to then play the game and it turns out that it does just as well as Mogo actually got a 50% win rate against Mogo.",
            "This breakthrough Monte Carlo search program.",
            "And so I just kind of want to leave you with those two thoughts, which first of all, you know if you build a big representation and trust your gradients, you can build surprisingly effective representations that can learn for themselves.",
            "Things which you might otherwise have thought would need specific handcrafted features or complex knowledge of the domain, but also that that process can actually learn implicitly to do the things which we thought explicit planning required for.",
            "So maybe you don't even need to do that research.",
            "For example, maybe you just build a big neural network and it figures it all out for itself.",
            "So."
        ],
        [
            "That's basically yet, so just want to conclude, so we've got this general purpose framework that's RL.",
            "That's why we all work on it.",
            "And really, what we've talked about in this tutorial is how RL problems can be solved by end to end deep learning.",
            "Getting to the point where a single agent can actually be applied to many different challenging tasks so you don't have to redefine your new features, it just you plug it in end to end.",
            "It figures out the right representation and uses that representation to solve the problem you care about.",
            "And so you know, maybe we're taking one step closer to that equation at the bottom, which I think where we would like to get to.",
            "OK, thank you."
        ],
        [
            "Alright, so how much have we just moved?",
            "The problem just moved the problem of.",
            "Learning into the decision about what to do at the different layers of the deep network.",
            "Because, OK, we'll just do gradient descent, but now the decision is what kind of functions do we apply there?",
            "And I imagine there's an infinite space of functions you could use.",
            "So there are a very large.",
            "There is the space of possible functions you can use is very large, but.",
            "Some standard architectures.",
            "Already do quite well without optimizing specifically to the domain you care about, and I think that's the lesson again that we can take from Atari.",
            "As you might think there well, you need to optimize the architecture again for each individual game, because you know, 3D game is very different from a side scrolling game, but it turns out that actually if you trust the gradients then it tends to just find the way to use the architecture you give it to the best of its ability, and if the architecture has sufficient flexibility then it will find a way to solve the problem.",
            "And we know that neural networks are universal function approximators, which basically just means that if you provide enough layers and enough parameters, they can represent any function.",
            "And so I mean a lot of people ask, you know what happened, you know they people think that we spent ages kind of optimizing for this particular."
        ],
        [
            "Architecture that we used and the truth is it was actually the first one we tried and it worked and then we went from that one to the to the nature one we added in one more layer and it worked better.",
            "And so you add in more parameters it does better but I think I I think there's this.",
            "Perhaps misconception that you have to.",
            "Be very, very careful about particular architecture use, and I think there are some basic tool kits that are useful like the weight sharing and the recurrent neural networks in the convolutional networks, but beyond that I think they're surprisingly robust to architectural choices, so it seems there is a black box.",
            "I don't want to go too far in that you know this is just one type of problem you know I couldn't make a general claim about all problems, but it feels like we're.",
            "Yes, I kind of want to say optimistically, yes.",
            "So."
        ],
        [
            "You said that we should trust the gradients, right?",
            "This is part of the message and I'm wondering to what extent you've been plagued by banishing gradients at all, which when you have a very deep in that might affect you, and in particular, if you're you know if the model is that an N layered network, does you know N steps of look ahead in computation?",
            "If I want that tend to be very big, then the gradients might still end up not being reliable.",
            "So it's a great question so.",
            "At this level, I don't think vanishing gradients are too much of a problem, so you know I've talked about the the architecture we used for Atari, the architecture we used for the continuous problems they had.",
            "You know, maybe say six layers, that isn't sufficient to cause vanishing gradient issues.",
            "The go example used 12 layers, again not sufficient to banishing gradient problems when you go to maybe 100 layers like in recurrent neural network.",
            "This is typically where it occurs.",
            "Then you do have vanishing gradient problems, but there's.",
            "Well known extensions which actually addressed the vanishing gradient so.",
            "Recurrent neural network glitch to people tend to fall back on LST EMS to avoid vanishing gradients, but there are many other techniques that Jeff Hinton's been working on her another method recently that he there are various ways you can kind of try to condition.",
            "You can initialize your weights as well.",
            "It turns out that that has a big effect on the venture gradients.",
            "If you basically make sure that all of your eigen values around one when you start off your weights, they tend to stay one for quite awhile and you don't end up with much for so much problem with benching gradients.",
            "But for all the work here.",
            "I think the answer is it wasn't too much of an issue.",
            "OK. OK Albert, I have a simple question in the plot that you showed about all the Atari games and your performance versus human players.",
            "Have you looked into?"
        ],
        [
            "Those cases that you're not actually doing well and then have some metric with respect, like how well humans actually is.",
            "It actually a hard game for humans as well or not.",
            "So, so let me preface this by saying, you know, we just had one human tester, so this is not like you know there are humans who can do way better at these games.",
            "So superhuman would mean doing much, much better in all of these games.",
            "Some of the games we do badly on, so I think the way I like to characterize it is that we're doing quite a good job of learning the representation using the deep learning end to end, and so if you take a game and if you imagine that you could handcrafts and features which were just right for that domain, and then you would expect a naive reinforcement learning algorithm on top of those features to work well, then we do really well on it.",
            "That's kind of the games over here.",
            "The games we do badly at the ones where, even if you came up with just the right features for that game.",
            "You would still expect the our algorithm to just work badly, for example Montezuma's Revenge.",
            "There's an expiration needle in the haystack problem.",
            "There's no way you'd expect your naive saucer algorithm even if you gave it the right features, you wouldn't expect it to be able to kind of figure out how to get through 100 different rooms to get to the treasure room, and then figure out exactly how to solve the problem.",
            "So I think what's nice is that maybe we've taken a step towards addressing the representation learning problem, but that helps us get now to the real reinforcement learning issues and be more creative and experimental with actually trying to find out.",
            "You know what is the PRL problems that that we need to face to deal with hierarchy or expert expiration or memory, or state construction or possibility?",
            "You know all these issues are the reasons why we do poorly here, but I think where it's just about representation learning.",
            "We do quite a good job now.",
            "Whoever's got the microphone, 'cause I I can't see after you train these networks, is there anything you can interpret out of them?",
            "Once you have the network actually trained in successful?",
            "So.",
            "It's a really interesting question, so I think you know one thing we'd like to do is to do as neuro scientists do, and to kind of, you know, look inside the network and do the equivalent of the fMRI and try to understand what it's what it's thinking so far.",
            "I mean, the one thing we can say is we have some plots.",
            "Which I don't think I have here, at least not without searching, which show like a.",
            "You can look at you can visualize.",
            "What the features are, which the network has learned, and you can basically see that it learns this very interesting clustering where all of the features in the network are clustered around a particular combination of what the image looks like combined with how much it's worth.",
            "So it doesn't just learn things about how it looks, and it doesn't just learn things about how much reward you're going to get from that point, it learns something which kind of combines them two together and you end up in situations, for example in Space Invaders where features clustered together in all the situations where the mother ship is on the screen and you're shooting a bullet towards it.",
            "Or you the situation is clustered together where in all the situations where there are only three aliens left on the screen and they are represented very close together in the networks own representation, so it lends some representation that kind of abstracts over the details of which particular three aliens it might be in your local region of the screen.",
            "But it knows that they are similar because they all have a similar expected score like the value function is the same amongst them, but at the same time it does include some spatial information because that's what the government is built on.",
            "So if they look totally different, they would be.",
            "It would kind of duplicate one feature kind of cluster to represent those three aliens in one part of the space, and another feature cluster in a in another part of the space.",
            "Whoever has the microphone again said so.",
            "This is going back to diners question about eligibility traces.",
            "When you're using experience replay, really don't need eligibility traces because you just need to replay the experience in the backward order.",
            "You get the effect of eligibility traces, which is probably what you're seeing because.",
            "With the experience replay of performance is much better.",
            "It might not be just a factor of three correlating it, but also getting difficult.",
            "Political traces.",
            "OK, so when long chilin defined experience replay, he kind of combined together 2 ideas, one of which was storing all of the experience in some database and then replaying samples of that experience.",
            "But he also had a second idea, which is what you're referring to, which is that you experience the play forward.",
            "The trajectory in real life forwards.",
            "Of course, you know this is an agent living in a world.",
            "But then when you learn, you go backwards through that experience in reverse order, and this can have some efficiency gains.",
            "We only use the first of those two ideas, not the second, so we intentionally randomize the sampling.",
            "We don't have any backward sweeping through the data, and the reason we randomize is to break the correlations.",
            "You don't have to backward sweep through the data, I mean, as long as you're doing it in random order, you would see some effect of the backward sweep.",
            "Today.",
            "That's all I see.",
            "OK."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it's really a pleasure to be here, so I'm afraid you're absolutely zero that I'm not even going to mention the word Bayes after this.",
                    "label": 0
                },
                {
                    "sent": "Once we get into the talk.",
                    "label": 0
                },
                {
                    "sent": "So sorry, but it is going to be about a combination of deep learning with reinforcement learning.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so the plan for today is really to start with a basic tutorial, an outline of some of the main ideas which have come into this revolution in deep learning which has happened over the last few years and really transformed a lot of the applications of machine learning and leading to a lot of success across many different challenging areas.",
                    "label": 0
                },
                {
                    "sent": "And then the rest of the talk is about how to really combine that with what we know, which is reinforcement learning and what I'm going to do is start with a brief introduction and outline of the particular parts of PRL that we're going to consider.",
                    "label": 0
                },
                {
                    "sent": "So sorry for those.",
                    "label": 0
                },
                {
                    "sent": "I'm going to just.",
                    "label": 0
                },
                {
                    "sent": "Try and hopefully Michael Lipman's talk is already prepared to you, but I'm just going to go over a couple of little bits to make sure everyone's on the same page there and then I'm going to talk really about three different paradigms for how we can combine deep learning with reinforcement learning, and the question really is a very simple one.",
                    "label": 0
                },
                {
                    "sent": "It's like, you know, we have this very powerful function approximator.",
                    "label": 0
                },
                {
                    "sent": "This powerful tool kit of deep learning, and where should we put that tool capture?",
                    "label": 0
                },
                {
                    "sent": "We use it to represent value functions.",
                    "label": 0
                },
                {
                    "sent": "We use it to represent policies to easier to represent our transition models, and we're going to look at each of those a little bit in turn.",
                    "label": 0
                },
                {
                    "sent": "The last one will be very, very brief, so just warning.",
                    "label": 0
                },
                {
                    "sent": "About that, so that's the sort of plan for today, so it's sort of half tutorial and also half a little bit of kind of some some recent results, so hope you know.",
                    "label": 0
                },
                {
                    "sent": "Bear with me if actually present some of the work we've been doing ourselves recently.",
                    "label": 0
                },
                {
                    "sent": "With that I'm going to start off with the first section, so just getting into.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A real background to to deep learning, so I wanted to start by really talking about you know why are we here?",
                    "label": 0
                },
                {
                    "sent": "What's the big goal?",
                    "label": 0
                },
                {
                    "sent": "And for me and for us at DeepMind the goal is to really try and use reinforcement learning to solve some of the bigger visions of AI, and I think you know the reason that many of us are so excited about reinforcement learning is that it is the paradigm for artificial intelligence.",
                    "label": 0
                },
                {
                    "sent": "I mean whenever we have agents that actually take actions in the world and want to influence their environment, ultimately that's our goal.",
                    "label": 0
                },
                {
                    "sent": "We want to influence the environment, want our agents to actually do things to take action to have an effect in the world.",
                    "label": 0
                },
                {
                    "sent": "And to do something to achieve and to maximize its reward and its impact on the world and its environment.",
                    "label": 0
                },
                {
                    "sent": "And the RL is the way to do that.",
                    "label": 1
                },
                {
                    "sent": "So arels the paradigm that describes how to learn to make optimal decisions in any environment.",
                    "label": 0
                },
                {
                    "sent": "But more than that, what we care about is coming out with a single algorithm, a single agent, and we don't want to have to rewrite this agent every time we come to some new environment.",
                    "label": 0
                },
                {
                    "sent": "We want to have a single algorithm which we can just sort of drop in there.",
                    "label": 0
                },
                {
                    "sent": "You know, like the baby, you put the baby down in some front of some new task, and eventually it kind of played around with it, figures out how to deal with the toy and and, and it's by the end of the day.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's figured out how to deal with this knew piece of its environment.",
                    "label": 0
                },
                {
                    "sent": "It's never encountered before and so really what we want is a single agent that can solve any of a wide range of challenging tasks.",
                    "label": 1
                },
                {
                    "sent": "And that's really the essence of what we mean by an intelligent agent, and so the piece I'm really going to focus on today is just one piece of that, you know.",
                    "label": 0
                },
                {
                    "sent": "Obviously rather ambitious goal, and the piece I'm going to focus on is how to bring in powerful representations into the story.",
                    "label": 0
                },
                {
                    "sent": "In other words, if we want to have an agent that can deal with all kinds of different problems, it better have a very powerful and flexible toolkit for representing its environment and understanding what's going on in that environment, and so really, what we're going to use is deep learning as our toolkit for representations for dealing.",
                    "label": 0
                },
                {
                    "sent": "For constructing features for representing state.",
                    "label": 0
                },
                {
                    "sent": "For turning this environment into something that the agent can characterize for itself.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's start with deep learning.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm not going to put up the sort of standard picture of a neural network.",
                    "label": 0
                },
                {
                    "sent": "I really want to avoid that.",
                    "label": 0
                },
                {
                    "sent": "I really wanted you to think of deep learning in a different way, so the way I'd like you to think of deep learning is just about compositions of functions.",
                    "label": 0
                },
                {
                    "sent": "So let's back off a minute and just talk about what it means to be deep.",
                    "label": 0
                },
                {
                    "sent": "This really, it's just, of course a relabelling of ideas which have been around for decades now, but a deep representation really just means a composition of many functions, and so these arrows here.",
                    "label": 1
                },
                {
                    "sent": "They basically indicate each of these arrows represents a function that's parameterized by some parameter vector.",
                    "label": 0
                },
                {
                    "sent": "So here we've got a function going from the input X.",
                    "label": 0
                },
                {
                    "sent": "This hidden state, hy parameterized by some parameter vector W one, and then a second function that goes from this hidden state using parameter vector, W2 and so forth.",
                    "label": 0
                },
                {
                    "sent": "And each of these.",
                    "label": 0
                },
                {
                    "sent": "Each of these functions gets composed together to give an increasingly sophisticated representation of what's going on in that input, until eventually at the end of it we come to some output.",
                    "label": 0
                },
                {
                    "sent": "And that's that's our representation.",
                    "label": 0
                },
                {
                    "sent": "If you like, that's how function.",
                    "label": 0
                },
                {
                    "sent": "So a deep representation is nothing more than the function that can be broken down into a series of composition of individual functions, each of which can be richly parameterized, and one of the reasons that deep learning is effective is because this can be done very efficiently and it suits the toolkit of machinery that we have to build very large parameter vectors in a typically.",
                    "label": 0
                },
                {
                    "sent": "For example, if we're dealing with images in Google, this might be the parameter vectors might contain 10s of millions of components.",
                    "label": 0
                },
                {
                    "sent": "And there might be, say, 20 or 30 layers to this representation.",
                    "label": 0
                },
                {
                    "sent": "And the fundamental idea what makes deep learning special is that we can optimize these representations very effectively, and the way that we optimize them is really the most familiar and simplest idea in machine learning, which is by following its gradient, and so really the key idea is that we can we need to be able to compute the gradient of any one of these rich and powerful representations, and the way we do that is essentially by something known as backpropagation, which in some sense is nothing more than the chain rule.",
                    "label": 0
                },
                {
                    "sent": "So all I've done in this second figure here is to basically reverse the flow.",
                    "label": 0
                },
                {
                    "sent": "Of this composition of functions and at the top of the.",
                    "label": 0
                },
                {
                    "sent": "Of the screen here and see what we can do is we can work backwards and we can now say OK. Well if we started our output over here and we want to know well how do we compute the gradient of this output?",
                    "label": 0
                },
                {
                    "sent": "In other words, if we want to know the gradient of the output with respect to our imperatore with respect to any of our parameter vectors, this backward flow shows us precisely how to compute any one of those gradients simply by applying the chain rule.",
                    "label": 0
                },
                {
                    "sent": "So we can think of these as just the multiplication of all of the partial derivatives.",
                    "label": 0
                },
                {
                    "sent": "So for example, if we want to compute the gradient with respect to.",
                    "label": 0
                },
                {
                    "sent": "These parameters here we would just get the gradient with respect to this hidden state here and then the gradient of the hidden state with respect to parameter vectors.",
                    "label": 0
                },
                {
                    "sent": "Multiply that by that and now we know how this particular parameter vector influences the output of the whole function.",
                    "label": 0
                },
                {
                    "sent": "And so the power of backpropagation is that it gives us a very simple and effective and efficient way to compute gradients for anyone of a very large toolkit of possible rich function approximators.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what about neural networks?",
                    "label": 0
                },
                {
                    "sent": "So in some sense, the deep neural network is.",
                    "label": 1
                },
                {
                    "sent": "Analogous or synonymous with a deep representation, but people tend to use a particular set of functions in the tool kit that they use, and so the most typical feedforward neural network that you might see would be composed of two things it would alternate between linear transformations where the hidden state one step is basically a linear combination for the hidden step hidden state we had at the time at the Step 4, you basically take your hidden state, you perform a linear transformation by multiplying by some weight matrix.",
                    "label": 0
                },
                {
                    "sent": "And it gives you a new hidden state.",
                    "label": 0
                },
                {
                    "sent": "And of course you could sequence together many of these different matrices.",
                    "label": 0
                },
                {
                    "sent": "You could be one matrix multiplied by another matrix multiplied by another matrix, but at the end of the day you would just end up with one giant linear transformation and you wouldn't have gained any power in your representational capacity.",
                    "label": 0
                },
                {
                    "sent": "So the other idea, which is typically used in almost every neural network, is what's called an activation function, which is simply a nonlinear function that takes the hidden state and pass it through some.",
                    "label": 0
                },
                {
                    "sent": "For example, thresholding function or sigmoid squashing function and output some value that basically add some nonlinearity to this function.",
                    "label": 0
                },
                {
                    "sent": "So what typically happens in your network is it's a composition of functions that alternates between these very simple linear transformations, and these also equally simple nonlinear activation functions, and that combination is particularly simple to work with.",
                    "label": 0
                },
                {
                    "sent": "Has resulted in a large family of successful function approximators.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one more idea that comes from the deep learning world and.",
                    "label": 0
                },
                {
                    "sent": "Is the idea of white sharing, so this is really.",
                    "label": 0
                },
                {
                    "sent": "I remember reading a book a few years ago where it described weight sharing is the only idea that works in your own networks, so I don't know if I would take it that far, but it's certainly very mainstay of deep learning that could be applied effectively in many different contexts.",
                    "label": 0
                },
                {
                    "sent": "And two of the most popular architectures for deep learning actually make use of weight sharing, and they use weight sharing to deal with structured data.",
                    "label": 1
                },
                {
                    "sent": "So the first formal structured data is where you have.",
                    "label": 0
                },
                {
                    "sent": "Time series data.",
                    "label": 0
                },
                {
                    "sent": "We have sequential data coming in and in that case you can use a recurrent neural network to deal with the sequential structure of the data and so the idea is that again we've got exactly the same idea where we've got a compositional function.",
                    "label": 1
                },
                {
                    "sent": "But now what's happening is that every step and you input is coming in, so the times that one we've got this X1 coming into the network and some output Y1 coming out of the network and then this gets passed through and the hidden layer kind of combines together some new state and then some new information comes in.",
                    "label": 0
                },
                {
                    "sent": "This X2 comes in at the next step.",
                    "label": 0
                },
                {
                    "sent": "And an output Y two is made at the next step and this goes on and on through many many time steps.",
                    "label": 0
                },
                {
                    "sent": "And what differentiates it from the feed forward networks that we saw in the previous slide is essentially this idea that we use weight sharing that the same transformation, the same linear transformation is applied at every single step.",
                    "label": 0
                },
                {
                    "sent": "So the same weight matrix W. And here and here.",
                    "label": 0
                },
                {
                    "sent": "And this represents the kind of our temporal invariants that basically we're going to use the same operation at every single step of time to combine together the latest information that's coming in the input and the previous state.",
                    "label": 0
                },
                {
                    "sent": "We're going to combine that together to construct a new state at the next time step, and also use the same weight matrix to perform whatever kind of output comes out of our network.",
                    "label": 0
                },
                {
                    "sent": "So this is 1 mechanism and I really want to, you know again, for people here.",
                    "label": 0
                },
                {
                    "sent": "I think it has perhaps the closest match with reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to talk about it too much today, but you can think of this as the Canonical way where deep learning can deal with partial, observable partial observe ability.",
                    "label": 0
                },
                {
                    "sent": "So if you have some domain where observations are coming in step by step and you want to construct some state representation that can remember the things that you've seen so far are currently on network is kind of the basic building block for dealing with such things.",
                    "label": 0
                },
                {
                    "sent": "Commodi planning perspective.",
                    "label": 0
                },
                {
                    "sent": "OK, the 2nd way in which weight sharing can be used is shown at the bottom of this slide here, and that's where typically used when we want to deal with structured data such as images.",
                    "label": 0
                },
                {
                    "sent": "And So what we can see here is that we've got, say, an image on the left hand side.",
                    "label": 0
                },
                {
                    "sent": "This image X and the idea of a convolutional network is it basically has the ZIL patches, where each Patch kind of represents some feature that's being constructed from some region of the image.",
                    "label": 0
                },
                {
                    "sent": "So here we have a little Patch that's being used.",
                    "label": 0
                },
                {
                    "sent": "One feature in the output, this hidden representation here, so this hidden node here is a function of all of the inputs within this region only.",
                    "label": 0
                },
                {
                    "sent": "And similarly this hidden value here is a function of everything in the corresponding region.",
                    "label": 0
                },
                {
                    "sent": "Here in the input and the idea of a convolutional network is that the transformations from these input spaces to the hidden space A shared in other ways, there's this translation invariants that we gain from this where wherever you are in the image.",
                    "label": 0
                },
                {
                    "sent": "For example, if you might learn to identify a face, and if that face here.",
                    "label": 0
                },
                {
                    "sent": "Then this note here can identify that base and there might be a feature saying hey, there's a face here and that gets stored here.",
                    "label": 0
                },
                {
                    "sent": "But equally interface occurs here that face to be recognized by using the same shared weights and that face recognizer will be stored at this part of the hidden representation.",
                    "label": 0
                },
                {
                    "sent": "What isn't indicated on this bigger is that the hidden representation could have many many planes to it.",
                    "label": 0
                },
                {
                    "sent": "In the convolutional network, so this is just showing 1 hidden plane, but there will be a whole vector of inputs.",
                    "label": 0
                },
                {
                    "sent": "Indeed there could be a whole vector inputs X and a whole vector of hidden states, H1 and so forth.",
                    "label": 0
                },
                {
                    "sent": "All of these have this kind of third dimension to them.",
                    "label": 0
                },
                {
                    "sent": "And then again, this can be applied compositionally, so that's just one layer of the network.",
                    "label": 0
                },
                {
                    "sent": "But then we can compose these again and again step by step, so we can now take that representation that we just built up, for example here, here and now we can combine together those higher level features looking at a region here and building some even higher level representation which retains its kind of spatial relevance, and so the idea of the convolutional network is that every stage you retain some kind of spatial representation of the data.",
                    "label": 1
                },
                {
                    "sent": "Whereas in a standard feedforward neural network you kind of lose that information.",
                    "label": 0
                },
                {
                    "sent": "Everything is just a vector of hidden state with no particular locations associated with it.",
                    "label": 0
                },
                {
                    "sent": "So you start off with an image.",
                    "label": 0
                },
                {
                    "sent": "You process that image to perhaps give you a lower resolution representation of that image, and you keep passing this through this pipeline to get more and more sophisticated representations of what's going on in that image, but still represented as some kind of image in image space if you like.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's just a brief tutorial to some of the architectures that typically get used in deep learning.",
                    "label": 0
                },
                {
                    "sent": "So now what do we do with them?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, ultimately we want to do some kind of machine learning using our representation, and so one way to do to apply machine learning to our deep neural networks is to simply come up with the loss function that basically says you know is this particular output good or bad.",
                    "label": 0
                },
                {
                    "sent": "So each time we pass our input into the into the network, we get some output.",
                    "label": 0
                },
                {
                    "sent": "Why out of it?",
                    "label": 0
                },
                {
                    "sent": "And now we're going to find some loss function this LY.",
                    "label": 0
                },
                {
                    "sent": "That basically tells us whether we think that output is good.",
                    "label": 0
                },
                {
                    "sent": "Does it match the target we want?",
                    "label": 0
                },
                {
                    "sent": "Does it give us a lot of reward?",
                    "label": 0
                },
                {
                    "sent": "Is it representing exactly the target that we want and so the two Canonical examples of loss functions which we're going to consider?",
                    "label": 1
                },
                {
                    "sent": "The first we're going to use throughout the talk, which is the mean squared error.",
                    "label": 0
                },
                {
                    "sent": "So the mean squared error in some sense is the simplest loss function.",
                    "label": 0
                },
                {
                    "sent": "It just represents, you know, let's say there's actually some true target.",
                    "label": 0
                },
                {
                    "sent": "Why star?",
                    "label": 0
                },
                {
                    "sent": "Well, imagine for now that there's some Oracle that tells us a teacher that tells us what their real target, why star should be as in supervised learning, and then as the talk progresses, we'll see how to relax that constraint.",
                    "label": 0
                },
                {
                    "sent": "But if we did have this, why star?",
                    "label": 0
                },
                {
                    "sent": "Then we could simply look at the mean squared error between the true target by star and what the network actually outputs Y and that would give us our loss function.",
                    "label": 0
                },
                {
                    "sent": "Or if this was a probabilistic network, we might choose to use something like a log likelihood where we might ask the network to make some prediction of, for example a category or class or something.",
                    "label": 0
                },
                {
                    "sent": "And now we want to make it more likely to predict the class that the expert actually suggests.",
                    "label": 0
                },
                {
                    "sent": "So once we have a loss function, what we do with it?",
                    "label": 1
                },
                {
                    "sent": "Well, we can simply append it to forward computation.",
                    "label": 1
                },
                {
                    "sent": "So really, the way to understand this is that we just have two passes when we work with neural networks, we have a forward pass and backward pass in the forward pass, we basically computer neural network.",
                    "label": 0
                },
                {
                    "sent": "We start off with our input, we pass it through all of the hidden States and at the end of it we come up with some output which we passed through our loss function.",
                    "label": 0
                },
                {
                    "sent": "And then in the backward pass what we do is we compute the gradient and so we start off by computing the gradient of this loss function in red there and then we do a backward pass where we sweep back and we accumulate all of our gradients as we go back through the data as we go back through this compositional chain of functions and every step we can workout how much those particular parameters associated with that particular function influenced the loss function that we got.",
                    "label": 1
                },
                {
                    "sent": "So if we multiply these together, for example, if you want to know, know how much do these parameters here influence the loss function that we actually got?",
                    "label": 0
                },
                {
                    "sent": "All we need to do is to multiply together all of these partial derivatives and will actually end up with the computation just by the chain rule of how the loss function depends on Wii.",
                    "label": 0
                },
                {
                    "sent": "Why do you like my PHN PHN by DH N -- 1 and so forth all the way back until we actually compute exactly the influence of those parameters on our loss.",
                    "label": 0
                },
                {
                    "sent": "So this is the basic building blocks of house to machine learning with the deep neural network.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so once we have the loss function, what do we do?",
                    "label": 0
                },
                {
                    "sent": "Well, we're going to build on perhaps the most common and successful idea in machine learning, which is gradient descent.",
                    "label": 1
                },
                {
                    "sent": "So what we're going to do is to minimize the expected loss.",
                    "label": 0
                },
                {
                    "sent": "So we've got our loss function.",
                    "label": 0
                },
                {
                    "sent": "Now we're going to assume that there's some distribution over our inputs X, and So what we care about is across all of these inputs, we're going to pass them all into our neural network.",
                    "label": 0
                },
                {
                    "sent": "We're going to look at the outputs and what we want to do is to minimize the loss across that whole data set.",
                    "label": 0
                },
                {
                    "sent": "So that's just the expectation over this loss function and what the way we're going to minimize that is by following the gradient.",
                    "label": 0
                },
                {
                    "sent": "So we're basically going to find the gradient of this expected loss with respect to our parameters, and that can itself.",
                    "label": 1
                },
                {
                    "sent": "We can push the expectation.",
                    "label": 0
                },
                {
                    "sent": "Outside here.",
                    "label": 0
                },
                {
                    "sent": "Patient here who wants to take the derivative of this, we can push the expectation then and what we see is that we end up with the expectation for our derivatives.",
                    "label": 0
                },
                {
                    "sent": "So what we really care about are these individual samples.",
                    "label": 0
                },
                {
                    "sent": "So every sample is basically one of.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These backward passes right in every sample we can do our forward pass and backward pass.",
                    "label": 0
                },
                {
                    "sent": "And then there's a result of that backward pass.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We're going to end up with some direction in which to adjust our parameters.",
                    "label": 0
                },
                {
                    "sent": "And that direction is given by the derivative of our loss function with respect to those parameters, and so we're just going to every single sample, find the gradient, move a little bit in the direction which makes us get a better loss and adjust our parameters step by step by step and in expectation when we apply these updates over the whole data set, we find that we actually compute the true gradient of the expected loss.",
                    "label": 1
                },
                {
                    "sent": "And So what we do is, we see that there's some kind of error surface, so these surface is kind of tell you.",
                    "label": 0
                },
                {
                    "sent": "Well, you know how much error is there going to be?",
                    "label": 0
                },
                {
                    "sent": "How much loss is there going to be?",
                    "label": 0
                },
                {
                    "sent": "This is like a contour map of the loss over parameter space, and now the idea is we're just going to adjust our parameters a little bit to take us closer and closer to some local minimum in this space.",
                    "label": 0
                },
                {
                    "sent": "So it's probably worth saying a couple of words about local Optima as well, which is, you know, a lot of people from outside deep learning.",
                    "label": 0
                },
                {
                    "sent": "They worry an awful lot about the fact that we've defined some very complicated loss surface, and we're using gradient descent to find some kind of local minimum, and the question is, well, don't you just get stuck with some poor local minimum?",
                    "label": 0
                },
                {
                    "sent": "And there's been a lot of empirical evidence and recently also some theoretical evidence, but actually this doesn't happen at all.",
                    "label": 0
                },
                {
                    "sent": "And what happens is that our.",
                    "label": 0
                },
                {
                    "sent": "Intuitions about high dimensional spaces are actually misleading, and it turns out that when you're in a very high dimensional surface, you're trying to find and use gradient descent to find some kind of optimum in that high dimensional space that by the time you actually find some optimum which is which is a stationary point in all of your many different dimensions, that with very high probability that local optimum is almost as good as a global optimum.",
                    "label": 0
                },
                {
                    "sent": "And so this is true for many statistical loss functions that you can come up with and seems to be true empirically for most interesting problems as well.",
                    "label": 0
                },
                {
                    "sent": "In other words, as the size of the parameter vector which you're optimizing gets larger and larger and larger, you need to worry less and less about local Optima and gradient descent just becomes essentially a very reasonable computational thing to do to help you find a good parameter that actually solves the problem you care about.",
                    "label": 0
                },
                {
                    "sent": "So the motto in some sense is trust.",
                    "label": 0
                },
                {
                    "sent": "Your gradients follow your gradients and and.",
                    "label": 0
                },
                {
                    "sent": "And of course there are many, many clever algorithms.",
                    "label": 0
                },
                {
                    "sent": "Then stochastic gradient descent, but at the same time stochastic gradient descent applied to a richly parameterized function is often enough to get really remarkably good results.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Yes, good.",
                    "label": 0
                },
                {
                    "sent": "High like a million.",
                    "label": 0
                },
                {
                    "sent": "Yeah, in this setting 20 is low.",
                    "label": 0
                },
                {
                    "sent": "Yes, I like a million.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "No, no you will.",
                    "label": 0
                },
                {
                    "sent": "So you will find a local optimum through through this approach, but that local optimum is highly probable to be close to the global optimum in its value.",
                    "label": 0
                },
                {
                    "sent": "That's what the theory says.",
                    "label": 0
                },
                {
                    "sent": "So, so there are many, many local Optima and this put people off neural networks.",
                    "label": 0
                },
                {
                    "sent": "But my almost a decade the fact that there are exponentially many local minima.",
                    "label": 0
                },
                {
                    "sent": "But it turns out that most of those exponentially many local minima really good minima.",
                    "label": 0
                },
                {
                    "sent": "That's the story.",
                    "label": 0
                },
                {
                    "sent": "You also have to be very careful about how you initialize your parameters for this to be true as well.",
                    "label": 0
                },
                {
                    "sent": "Empirically, so yeah.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so deep supervised learning then, so I really don't want to spend too too long on the supervised learning story, but really what I want to summarize is just to say that deep neural networks have achieved remarkable success is state of the art for images, for audio, for language, natural language processing.",
                    "label": 1
                },
                {
                    "sent": "And the ingredients, which are it used to, actually solve these problems?",
                    "label": 1
                },
                {
                    "sent": "Are a really straightforward, so use a deep network as your function approximator.",
                    "label": 0
                },
                {
                    "sent": "You define a loss function, and you optimize the parameters end to end by stochastic gradient descent.",
                    "label": 0
                },
                {
                    "sent": "So in a sense it's almost like an excuse to not think too hard, you just kind of set your big model up and you attach gradient descent and you press go.",
                    "label": 0
                },
                {
                    "sent": "And often this actually outperforms models which apparently seemed to be clever because they're breaking down the problem according to the knowledge we know about it.",
                    "label": 0
                },
                {
                    "sent": "Message seems to be trust your gradients and the representation will find the right way to solve the problem given enough computation.",
                    "label": 0
                },
                {
                    "sent": "So the big question then is.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can we follow the same recipe for reinforcement learning and so the rest of this talk is about how to do this?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I'm going to start with just cherry picking a little bit of RL background, so I really I'm sort of aware that you've already some of you have sat through Michael Lipman's tutorial and seen all of this already.",
                    "label": 0
                },
                {
                    "sent": "Some of you already know this, but some of you were in the other one and maybe haven't, so I just want to give that a couple of slides to make sure we're all on the same page with the type of reinforcement learning that we're going to use.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So apologies if you are falling asleep.",
                    "label": 0
                },
                {
                    "sent": "You know there's probably some comfort comfortable pillows around.",
                    "label": 0
                },
                {
                    "sent": "You just make use of them for awhile and you know I wake you up in a minute, but I think it's really good just to make sure we're all talking the same language, so I'm going to start talking about policies and value functions.",
                    "label": 0
                },
                {
                    "sent": "So I am with a little bit of sleight of hand going to talk about policy.",
                    "label": 0
                },
                {
                    "sent": "I'm going to just refer to as it's terministic policy.",
                    "label": 0
                },
                {
                    "sent": "Sometimes.",
                    "label": 0
                },
                {
                    "sent": "It may also be stochastic, but for now I'm just going to represent it by this policy Pi so.",
                    "label": 0
                },
                {
                    "sent": "You're in some state for some definition of state S. We've got some policy, which is a way to pick actions, so this is the way by which our agent selects its actions, and then we've got a value function which basically tells us how much reward our agent can expect to get.",
                    "label": 0
                },
                {
                    "sent": "If it follows this particular policy Pi that follows this behavior strategy to pick all of its actions in all of the states that encounters, and So what we see is that we have this value function.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to focus on the Q values, the action values in this talk.",
                    "label": 0
                },
                {
                    "sent": "I was trying to actually not even mention vehicle, but I've just got this one slide later.",
                    "label": 0
                },
                {
                    "sent": "I couldn't seem to avoid it, so the action value function tells you basically how good is action A in state S. How much award will I get if I start in state S, take action A and then follow my policy pie from there onwards?",
                    "label": 1
                },
                {
                    "sent": "So this is the quantity which we're going to optimize for, and we're going to use to evaluate how well we're doing in any situation.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then this gives us basically three approaches to reinforcement learning.",
                    "label": 1
                },
                {
                    "sent": "So the 1st two are based exactly on the previous slide, where you know idea number one is to say OK. Well, we had this idea of a policy and the question is what's the best policy.",
                    "label": 0
                },
                {
                    "sent": "So we're going to look for this policy Pi star, the optimal policy which achieves the maximum future award.",
                    "label": 1
                },
                {
                    "sent": "And so this is really kind of direct policy set.",
                    "label": 0
                },
                {
                    "sent": "Searching directly through the things we care about the policy parameters themselves.",
                    "label": 0
                },
                {
                    "sent": "So it's defined policy parameters that actually solve the problem in the best way.",
                    "label": 0
                },
                {
                    "sent": "The second approach is what we call value based reinforcement learning, where we work with the value function instead.",
                    "label": 0
                },
                {
                    "sent": "And so here the goal is going to be to estimate the optimal value function.",
                    "label": 1
                },
                {
                    "sent": "In other words, directly workout the maximum value that we can achieve using any policy.",
                    "label": 0
                },
                {
                    "sent": "So that's this.",
                    "label": 0
                },
                {
                    "sent": "Q, star Q Star basically tells us under all policies what's the maximum amount of reward we could get from any state by picking actions in the right way.",
                    "label": 0
                },
                {
                    "sent": "So that gives us two ways to solve the reinforcement learning problem.",
                    "label": 0
                },
                {
                    "sent": "But of course there's always a third way.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This case, the 3rd way is model based reinforcement learning and in that case for model based reinforcement learning, the main idea is to build when I call a transition model and environment model, something which basically describes how the environment transitions from from state to state.",
                    "label": 1
                },
                {
                    "sent": "I mean this can be from the agent's perspective as well, you know.",
                    "label": 0
                },
                {
                    "sent": "Ideally it's something that basically says how the agent can explain the data it sees.",
                    "label": 0
                },
                {
                    "sent": "How can explain the observations it sees within the world and which tells it how to go from one step to the next step and once it understands the world it's in, it can use this understanding of the world to plan to look ahead.",
                    "label": 0
                },
                {
                    "sent": "Using its model and rolling forward this model step by Step 2, for example, build a tree and then start to figure out you know by doing a look ahead on that tree and figure out what's the best sequence of actions that will maximize the amount of reward against.",
                    "label": 0
                },
                {
                    "sent": "So that's model based reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Really, the main idea of this talk is to say, well, if we're going to apply deep learning reinforcement learning, first idea should be to look at these three Canonical ways to deal with reinforcement learning and just plug deep learning indirectly, as the function approximator.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that we can use our deep network to represent the value function.",
                    "label": 1
                },
                {
                    "sent": "We can use our deep network to represent the policy, or we can use our deep network to represent the transition model.",
                    "label": 0
                },
                {
                    "sent": "And in each case we're going to try and follow the formula that was successful for supervised learning.",
                    "label": 1
                },
                {
                    "sent": "This recipe, which was straightforward recipe of combining these models with stochastic gradient descent.",
                    "label": 0
                },
                {
                    "sent": "We're just going to optimize either the value function of the policy or the model end to end by defining some appropriate loss function or some appropriate gradient and following that gradient using SGD and hope that that finds us a good solution.",
                    "label": 0
                },
                {
                    "sent": "So in some sense, you know we can always be cleverer than this, But this is in some sense is the starting point to basically go back to basics with reinforcement learning and say how can we plug in DRL and effectively build on on the standard approaches to reinforcement learning.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to start by talking about value functions.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "For each of these, I apologize for just building on this idea of calling everything deep, but I am going to call everything deep so we can have deep value functions.",
                    "label": 0
                },
                {
                    "sent": "Then deep policies and then deep models.",
                    "label": 0
                },
                {
                    "sent": "So deep value functions.",
                    "label": 0
                },
                {
                    "sent": "We're basically going to now plug in a deep representation and deep neural network as function approximator.",
                    "label": 0
                },
                {
                    "sent": "To estimate Q2, estimate this value function and we're going to see how far we can get by optimizing for this thing directly I'm going to start off by introducing methods which have been known for perhaps.",
                    "label": 0
                },
                {
                    "sent": "Two or three decades now and then I'm going to talk about some recent developments that have helped to make them much more robust.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Michael Lippman, of course talked about this, but again, just to reiterate the Bellman equation, basically, we start off with our value function Q and we can unroll the value function by basically saying that the value Now the value in this state is equal to the immediate reward plus the value from the state, which I end up in.",
                    "label": 0
                },
                {
                    "sent": "That's what this tells us, so the value function is basically the expected reward from time step T + 1 plus time step T + 2 plus time plus step T + 3.",
                    "label": 0
                },
                {
                    "sent": "But if we just unroll this, we see that everything from this point onwards is exactly the value function for the next time step, and so we can always represent the value function in terms of these two parts.",
                    "label": 0
                },
                {
                    "sent": "The immediate reward plus the reward from that time step onwards, the value from that time step onwards.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can do the same thing with the Bellman optimality equation and this tells us how to compute Q star.",
                    "label": 0
                },
                {
                    "sent": "So the Bellman expectation equation just tells us what's the value function for some fixed pie where at the Bellman optimality equation tells us how to compute the optimal value function Q star.",
                    "label": 1
                },
                {
                    "sent": "And so again we can unroll it in terms of the reward Now plus now the maximum action value from the successor state, taking the successor action at that point.",
                    "label": 0
                },
                {
                    "sent": "And so we can unroll this by taking a Max over the action which we take at the successor state.",
                    "label": 0
                },
                {
                    "sent": "And this gives us the well known or one form of the well known Bellman optimality equation.",
                    "label": 0
                },
                {
                    "sent": "And So what we're going to do then, is sold for these two Bellman equations, and by solving for these Bellman equations, we're going to try and find either the value function for our current policy.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which is known as policy iteration.",
                    "label": 0
                },
                {
                    "sent": "I'm all the value function for the optimal policy, which is known as value iteration.",
                    "label": 0
                },
                {
                    "sent": "So in the first case, what happens is we take our Bellman expectation equation and we basically turn this into an iterative update.",
                    "label": 0
                },
                {
                    "sent": "So the goal is for any given policy we want to work out how good is that policy.",
                    "label": 0
                },
                {
                    "sent": "We want to work out how much award will I get under that fixed policy.",
                    "label": 0
                },
                {
                    "sent": "So what we can do is we can use this Bellman expectation equation to figure out the value of our Q values and so we can basically keep updating our Q values by taking our Bellman equation and just plugging it in to give us a new iteration.",
                    "label": 0
                },
                {
                    "sent": "So we have some idea of what if we had some previous estimates of QRQI.",
                    "label": 0
                },
                {
                    "sent": "What we do is we estimate this target now by saying in this state.",
                    "label": 0
                },
                {
                    "sent": "Here what I think the target should be is the immediate reward that I got plus the discounted value of the next day.",
                    "label": 0
                },
                {
                    "sent": "So imagine you're playing a game.",
                    "label": 0
                },
                {
                    "sent": "Imagine playing game of chess or something.",
                    "label": 0
                },
                {
                    "sent": "You think you're in a winning position and then you play a move and you suddenly realize Oh no, I made a terrible blunder.",
                    "label": 0
                },
                {
                    "sent": "I'm actually losing the game so that basically tells us there's a discrepancy between these things and what we want to do is always just look one step ahead and update our value that we had before towards the value after you made that move.",
                    "label": 0
                },
                {
                    "sent": "After you took that, actually have some more accurate estimate of what the value is.",
                    "label": 0
                },
                {
                    "sent": "Your Q value after that step is made and we use that as the target to update our Q value at the previous step.",
                    "label": 0
                },
                {
                    "sent": "So in policy iteration we just use this Bellman expectation equation to give us and you estimate.",
                    "label": 1
                },
                {
                    "sent": "So we start off with Qi.",
                    "label": 0
                },
                {
                    "sent": "We plug that into the right hand side.",
                    "label": 0
                },
                {
                    "sent": "We turn the handle and we estimate our new Q values.",
                    "label": 0
                },
                {
                    "sent": "This Qi plus one from all of this right hand side.",
                    "label": 0
                },
                {
                    "sent": "So it's just an iterative approach.",
                    "label": 0
                },
                {
                    "sent": "We take a publication, we iterate it again and again and again, and that gives us a new Q value.",
                    "label": 0
                },
                {
                    "sent": "Once we have that key value, you can improve the policy to find a better policy just by acting greedy with respect to it.",
                    "label": 0
                },
                {
                    "sent": "I appreciate I'm doing this part a little quickly.",
                    "label": 0
                },
                {
                    "sent": "I just didn't want to duplicate Michael stuff too much.",
                    "label": 0
                },
                {
                    "sent": "So the second approach is is value iteration.",
                    "label": 1
                },
                {
                    "sent": "So if value iteration we take the Bellman optimality equation.",
                    "label": 1
                },
                {
                    "sent": "This second equation here we iterate over the Bellman optimality equation.",
                    "label": 0
                },
                {
                    "sent": "So again what we do is we take the right hand side of the Bell Notes multi equation.",
                    "label": 0
                },
                {
                    "sent": "We treat that as a target.",
                    "label": 0
                },
                {
                    "sent": "We take out old estimate of Q.",
                    "label": 0
                },
                {
                    "sent": "We plug that into the right hand side and we use it as a target to estimate what a new iteration is gonna be.",
                    "label": 0
                },
                {
                    "sent": "So we basically start off with this target here, plug that in an estimate.",
                    "label": 0
                },
                {
                    "sent": "He values our next iteration policy iteration and value iteration.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now let's get concrete and start talking about some algorithms.",
                    "label": 0
                },
                {
                    "sent": "So the first algorithm I want to talk about is nonlinear Sosa.",
                    "label": 0
                },
                {
                    "sent": "So this is a form of policy iteration, and the reason I'm presenting in this way is just so we can get some idea of whether you want to connect it together with deep learning and so actually you know some choices for the loss function that we should actually use to learn our Q values into end.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do is we're going to start off by representing our value function.",
                    "label": 1
                },
                {
                    "sent": "These Q values by a deep neural network by a deep representation, and so this Q network is going to be represented now.",
                    "label": 0
                },
                {
                    "sent": "We've got this network that takes an input, the state and the action.",
                    "label": 0
                },
                {
                    "sent": "And it has some parameters, W's or parameterized.",
                    "label": 0
                },
                {
                    "sent": "This whole deep function.",
                    "label": 0
                },
                {
                    "sent": "And we're going to use this network to estimate the true Q values.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And So what we can do then is to basically come up with an objective function where we basically choose our loss function.",
                    "label": 0
                },
                {
                    "sent": "The mean squared error.",
                    "label": 0
                },
                {
                    "sent": "So we're going to basically have this loss function, which is the squared error between our target which came from the right hand side of the Bellman equation.",
                    "label": 0
                },
                {
                    "sent": "Missing reward plus value at the next step.",
                    "label": 0
                },
                {
                    "sent": "That's going to be our target.",
                    "label": 0
                },
                {
                    "sent": "We're going to look at the difference between that target and our current estimate of the Q value.",
                    "label": 0
                },
                {
                    "sent": "It's a little bit like taking the difference between the right hand side and the left hand side of the Bellman equation and treating that as R-squared error.",
                    "label": 0
                },
                {
                    "sent": "So it kind of 1 tower RQ values to always represent the thing which we see at the next step.",
                    "label": 0
                },
                {
                    "sent": "So we can take this mean squared error.",
                    "label": 0
                },
                {
                    "sent": "And we can.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Take the gradient of this.",
                    "label": 0
                },
                {
                    "sent": "So as to arrive at an algorithm which we're going to call nonlinear Sosa.",
                    "label": 0
                },
                {
                    "sent": "And So what we do is we differentiate this squared error, treating this target as a fixed target, just as in the Bellman equation.",
                    "label": 0
                },
                {
                    "sent": "We treat the right hand side as a fixed target, and we update the left hand side towards the right hand side.",
                    "label": 0
                },
                {
                    "sent": "And that gives us this saucer gradient here.",
                    "label": 0
                },
                {
                    "sent": "Well, all we've done is applied the chain rule to this squared error, so we end up with this error term multiplied by the derivative of this error term and the target we're treating is fixed.",
                    "label": 0
                },
                {
                    "sent": "So the only term which comes out of that is the derivative of our Q function with respect to the weights.",
                    "label": 0
                },
                {
                    "sent": "So the way to understand this is that we're basically saying that we've got this.",
                    "label": 0
                },
                {
                    "sent": "This loss function.",
                    "label": 0
                },
                {
                    "sent": "We've got the gradient of the loss function is now we've got.",
                    "label": 0
                },
                {
                    "sent": "Tierra town here.",
                    "label": 0
                },
                {
                    "sent": "This error term telling us how much discrepancy there is between what we thought the value was going to be and what the value ended up being multiplied by this gradient term.",
                    "label": 0
                },
                {
                    "sent": "That tells us how to adjust the parameters so as to make this this estimate more or less more correct.",
                    "label": 0
                },
                {
                    "sent": "And then what we're going to do is optimize this objective end to end by stochastic gradient descent just by following this gradient.",
                    "label": 0
                },
                {
                    "sent": "So this is a concrete way to take the deep learning idea.",
                    "label": 0
                },
                {
                    "sent": "Apply it to reinforcement learning so the end to end.",
                    "label": 0
                },
                {
                    "sent": "We can try and optimize for some objective function.",
                    "label": 0
                },
                {
                    "sent": "In this case the squared error between our target Q values and our estimated Q values.",
                    "label": 0
                },
                {
                    "sent": "So we can do the same thing with with value iteration.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As well, and that leads to the Q learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "So all we've done here is.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Exactly the same procedure.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But now we apply we were using the Bellman optimality equation instead, so we end up with this Max over the actions that we take out the next step in our target.",
                    "label": 0
                },
                {
                    "sent": "So I target is now the reward plus the Max over all the things we could do at the next step on the Q values at that step.",
                    "label": 0
                },
                {
                    "sent": "Those become our targets, but again we follow the same procedure we represent their Q network.",
                    "label": 0
                },
                {
                    "sent": "We use this Q network to represent our value function.",
                    "label": 0
                },
                {
                    "sent": "We computer mean squared error.",
                    "label": 0
                },
                {
                    "sent": "We follow the gradient of that mean squared error and we optimize the objective end to end by stochastic gradient descent.",
                    "label": 0
                },
                {
                    "sent": "So we're going to call this nonlinear Q learning this algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, let's get some examples.",
                    "label": 0
                },
                {
                    "sent": "OK, Anna.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So, so the question from Anna there was, you know this isn't really the true gradient of this error because we're this term in blue were basically treating us fixed and we're ignoring the gradient term that comes from that.",
                    "label": 0
                },
                {
                    "sent": "There are corrections to this which people in this room have have worked on.",
                    "label": 0
                },
                {
                    "sent": "For now I'm just presenting the naive view where we can kind of view this as gradient descent by substitution, where we take a target we can substitute in this target and replace for a fixed target and follow the gradient with respect to that just to get some intuition into how we can actually do that.",
                    "label": 0
                },
                {
                    "sent": "And later will actually do something which is slightly more principled.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to take a well known example.",
                    "label": 0
                },
                {
                    "sent": "Many of you will have heard of this, but I think it's helpful to place it in the context of the flow of what's come before, and that's the TD Gammon program.",
                    "label": 0
                },
                {
                    "sent": "So this was Jerry Tesauro's famous backgammon program that had a small neural network with just one hidden layer with around either 40 to 80 hidden nodes and what it did was it basically took the input of the backgammon board.",
                    "label": 0
                },
                {
                    "sent": "And this is the backgammon board, over here.",
                    "label": 0
                },
                {
                    "sent": "Ground and Blacks trying to move all its pieces around this way and Meanwhile rates trying to move more around this way.",
                    "label": 0
                },
                {
                    "sent": "And there's some rules associated to that, but basically roll some dice and then move your pieces and what Jerry Tesauro did.",
                    "label": 0
                },
                {
                    "sent": "We just took the raw representation of that board and he kind of flattened it out into this representation that was used as the input in your network.",
                    "label": 0
                },
                {
                    "sent": "So this representation basically says things like you know there will be one input saying are there 2 Blackstone's here?",
                    "label": 0
                },
                {
                    "sent": "So are there?",
                    "label": 0
                },
                {
                    "sent": "2 white stones here.",
                    "label": 0
                },
                {
                    "sent": "And so there's this binary input representation.",
                    "label": 0
                },
                {
                    "sent": "There was a flattening out of the board and that input representation was passed through a neural network and the output of the neural network was an estimate of the value function for that position.",
                    "label": 0
                },
                {
                    "sent": "So this is where the V comes in.",
                    "label": 0
                },
                {
                    "sent": "So sorry bout that.",
                    "label": 0
                },
                {
                    "sent": "But it's an estimator saying how good is this position?",
                    "label": 0
                },
                {
                    "sent": "Am I going to win from this position?",
                    "label": 0
                },
                {
                    "sent": "And if the position is good, if you know if you think you're actually in a position where you're going to win this be will be high and the job of the neural network is to come up with the right representation itself.",
                    "label": 0
                },
                {
                    "sent": "Without telling it the right features, it has to learn a representation.",
                    "label": 0
                },
                {
                    "sent": "It has to build its own features.",
                    "label": 0
                },
                {
                    "sent": "It has to build its own.",
                    "label": 0
                },
                {
                    "sent": "This so the composition of functions is learning for itself.",
                    "label": 0
                },
                {
                    "sent": "How to compose these very raw features into more structured, interesting features that are able to actually capture the value function very accurately.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "When this was actually used, it was initialized with random weights and it was trained by games of self play, so played itself many thousands of games and it used this nonlinear Sarsa algorithm and I'm using again a little bit of sleight of hand here.",
                    "label": 1
                },
                {
                    "sent": "Just basically make it look like the algorithm I already presented, which is to say that we can think of the function approximator Q that I presented in my saucer algorithm as just being the after state value function.",
                    "label": 0
                },
                {
                    "sent": "So we basically",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here we had a neural network that said how good is a position whereas the Q values tell you how good is the position.",
                    "label": 0
                },
                {
                    "sent": "If I play a particular action.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so all we do is basically transform the V into a queue and then roughly the same algorithm applies there.",
                    "label": 0
                },
                {
                    "sent": "And what was interesting about TV Gammon was that even though it was used, this training procedure was done quite naively, so there was no exploration.",
                    "label": 0
                },
                {
                    "sent": "Just use the dice roll in the game to make sure that it reached a nice variety of different positions.",
                    "label": 0
                },
                {
                    "sent": "The algorithm actually converged in practice very robustly, but it turned out that wasn't true for other games, and indeed not for other environments outside of games as well.",
                    "label": 1
                },
                {
                    "sent": "But nevertheless it did extreme.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, anti gammon.",
                    "label": 0
                },
                {
                    "sent": "Just by training from self play, was able to defeat the human world champion very convincingly many years ago.",
                    "label": 0
                },
                {
                    "sent": "And yet, what strange is that people sort of fled away from this approach just a few years later, and I think the reason is that people discovered that although it worked very effectively in backgammon, which has this very smooth value function due to the stochasticity of the dice, these nonlinear function approximators, when they're combined with reinforcement learning, turned out to be very unstable.",
                    "label": 0
                },
                {
                    "sent": "And so when people try to do this, their algorithms just blew up and it led to this reputation of deep learning or neural networks at that time being very difficult to work with and causing all kinds of instabilities and oscillations and divergents and craziness.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, one detail I should mention.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I presented this as if these features were completely by scratch.",
                    "label": 0
                },
                {
                    "sent": "Starting from the raw inputs, but this original victory.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which was presented here, actually used some handcrafted features, but it turns out Jerry presented some results few years ago when.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Went back with some kind of more advanced hardware, reran the exactly the same experiment, but just let it train for longer and he started off just with the raw input features that I presented, and it turned out that as you added more capacity to the neural network going from 10 to 20 to 40 to 80 hidden units, still keeping just one hidden layer, it did better and better and better.",
                    "label": 0
                },
                {
                    "sent": "And when it was trained for 10,000,000 self play training games, in actually outperformed significantly the original TD Gammon, and so I think it's fair to say that this was done completely from scratch.",
                    "label": 0
                },
                {
                    "sent": "And represents the first significant success of RL building its own features completely end to end from scratch just by following gradients.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now it's time to deal with some of those pesky stability issues, like how can we actually make things work better.",
                    "label": 0
                },
                {
                    "sent": "So what are the problems?",
                    "label": 0
                },
                {
                    "sent": "First of all.",
                    "label": 0
                },
                {
                    "sent": "So if we use naive Q, learning or Sosa and combine it with a neural net, we find that we get oscillation or divergent for perhaps three reasons.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to highlight these three reasons.",
                    "label": 0
                },
                {
                    "sent": "There may be some other small issues, but these seem to be the most significant ones and this is really from quite an empirical point of you.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to present theory saying that any of these new ideas actually guaranteed to converge, I'm just going to present empirical results showing that we can address these issues and come up with a robust end to end learning algorithm where we can apply IRL with an arbitrary deep network.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So the first problem is that the data that we see in our L is sequential.",
                    "label": 0
                },
                {
                    "sent": "You know we have an agent living in an environment and you know if you've got some robots and it takes a step and then it takes another step.",
                    "label": 0
                },
                {
                    "sent": "The second step it takes is very correlated to the first step and you would expect that the value functions for these things are going to be very correlated as well, and so we have this extremely non IID setting so.",
                    "label": 0
                },
                {
                    "sent": "This really is the first problem.",
                    "label": 0
                },
                {
                    "sent": "The first issue, why if you just try to naively apply supervised learning methods, they sometimes just fall over.",
                    "label": 0
                },
                {
                    "sent": "The second problem is that the policy itself can change dramatically, so if you just imagine one situation where where the Q value, let's say you can either go left or right and it turns out that going left seems to be a little bit better than going right?",
                    "label": 0
                },
                {
                    "sent": "Well then, most of the time your agent will go left.",
                    "label": 0
                },
                {
                    "sent": "This part of the world maybe the world is this stage, so now I'm going to visit this part of the world vastly more than this.",
                    "label": 0
                },
                {
                    "sent": "Other parts of the world.",
                    "label": 0
                },
                {
                    "sent": "And so now I have a distribution of data which is all about the left hand side of.",
                    "label": 0
                },
                {
                    "sent": "But maybe some random event happens and all of a sudden something switches and now I think that going to the right is slightly better.",
                    "label": 0
                },
                {
                    "sent": "So something just small changes in my Q values.",
                    "label": 0
                },
                {
                    "sent": "Well now I'm going to go over to the right hand side of this world to explore the right hand side of this world a lot.",
                    "label": 0
                },
                {
                    "sent": "I'm gonna get distribution of data that comes almost entirely from the right hand side of the world and.",
                    "label": 0
                },
                {
                    "sent": "My tight data distribution will have dramatically swung from one extreme to the other.",
                    "label": 1
                },
                {
                    "sent": "And this can lead to real problems.",
                    "label": 0
                },
                {
                    "sent": "For supervised, you know, for building on this sort of simple supervised learning ideas aren't designed to deal with these disks.",
                    "label": 1
                },
                {
                    "sent": "Swings from in the distribution of data.",
                    "label": 0
                },
                {
                    "sent": "So we're going to try and address that problem as well, and the third issue is 1, which is.",
                    "label": 0
                },
                {
                    "sent": "I think specific to these neural network representations, which is?",
                    "label": 0
                },
                {
                    "sent": "It turns out they're very sensitive to the reward scale.",
                    "label": 0
                },
                {
                    "sent": "In other words, if you plug in into one of these neural networks.",
                    "label": 0
                },
                {
                    "sent": "If you take one of these backward sweeps.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you plug in some very large value here because of the nonlinearities that we have in the backward propagation, everything explodes.",
                    "label": 0
                },
                {
                    "sent": "So if you just naively try to back propagate gradients where your loss function is not well controlled for the neural networks aren't well behaved, and so this is one of the issues that has to be dealt with in a principled way.",
                    "label": 0
                },
                {
                    "sent": "Otherwise things just don't work too well.",
                    "label": 0
                },
                {
                    "sent": "That's not going empirical point.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What we introduced with this idea called Deep Q Network switches.",
                    "label": 0
                },
                {
                    "sent": "One stable solution to the deep value based reinforcement learning paradigm and so the idea was to specifically address these three issues which are known to cause problems to destabilize neural networks, and so the first idea is to use experience replay.",
                    "label": 1
                },
                {
                    "sent": "So the idea is very similar to break the correlations in the data that we basically go back over the data.",
                    "label": 0
                },
                {
                    "sent": "So we generate the data.",
                    "label": 0
                },
                {
                    "sent": "So maybe I go to the left and then I go to the right.",
                    "label": 0
                },
                {
                    "sent": "But when I actually come to learn from that data, I randomly sample from all of the data I've seen so far.",
                    "label": 1
                },
                {
                    "sent": "And that breaks all of the correlations in the data and essentially brings it back to an IID setting where all of the samples that you use.",
                    "label": 0
                },
                {
                    "sent": "Because you're randomly drawing them from all of the experience that you've seen so far.",
                    "label": 1
                },
                {
                    "sent": "Everything is completely decorrelated now.",
                    "label": 0
                },
                {
                    "sent": "And it also has the advantage that we can learn from past policies.",
                    "label": 0
                },
                {
                    "sent": "So again, if we go back to this situation where I first of all visit the left and then visit the right if I learn completely online, I will only ever be learning from the particular distribution that I'm in at that moment like now I'm in this world where I'm over at the right, I completely forgot about all the learning which I did while I was over there.",
                    "label": 0
                },
                {
                    "sent": "Whereas if we use experience replay, we mixed together the experience which was generated by the policy that took me to the left and the policy which took me to the right and it makes it much more robust against these kind of oscillations that you get in the in the policies and therefore the distribution of data.",
                    "label": 0
                },
                {
                    "sent": "Yeah question.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, so great question.",
                    "label": 0
                },
                {
                    "sent": "So Johnny was asking.",
                    "label": 0
                },
                {
                    "sent": "If one of the losses of this approach, the experience replay it gains you this ability to stabilize by decorrelation.",
                    "label": 0
                },
                {
                    "sent": "But at the loss of keeping the temporal sequence, which lets you do methods like eligibility traces.",
                    "label": 0
                },
                {
                    "sent": "And and I think that's absolutely true that there is a lost to this approach, and I think there are alternatives, and I think it's certainly something we're keen to explore.",
                    "label": 0
                },
                {
                    "sent": "But yes, I mean the methods which we are presenting our naive methods that just completely randomized over the experience and only look at individual experience samples.",
                    "label": 0
                },
                {
                    "sent": "I think the simplest approach that you could take to bridge the gap would be to do what long dealing called lessons where you basically replay short.",
                    "label": 0
                },
                {
                    "sent": "Sequences of experience and that might give you just long enough eligibility traces to be able to get the best of both worlds.",
                    "label": 0
                },
                {
                    "sent": "It's probably worth saying that when you when you do learn from past policies, you have to use an off policy learning algorithm because you need to learn from policies that we were using before, which are different than what you're using now.",
                    "label": 0
                },
                {
                    "sent": "So if you're in that setting, your eligibility traces can never get that long anyway, because typically your new policy will do something different at one of the steps.",
                    "label": 0
                },
                {
                    "sent": "Eventually anyway, and so I think having these lessons may be a way to just bridge the gaps that you don't have to go too far to get most of the benefits of traces, but I think that's that's an open question and absolutely, we've we've kind of sacrificed that for now, and we will definitely want to get that back at some point.",
                    "label": 0
                },
                {
                    "sent": "So the second idea that we use is to freeze the target Q network.",
                    "label": 0
                },
                {
                    "sent": "So really what we try to do is to get a bit more back to the spirit of really value iteration by freezing the right hand side of the Bellman equation and just treating that as a target for awhile and learning about that right hand side.",
                    "label": 0
                },
                {
                    "sent": "Without changing it, so it basically freeze the target network, learn our new iteration and this helps avoid some of the oscillations and again break some of these correlations between the Q network in the target.",
                    "label": 0
                },
                {
                    "sent": "This is an idea also known as fitted Q iteration.",
                    "label": 0
                },
                {
                    "sent": "And finally we either clip the rewards, which is the naive approach, or more recently we normalize the network adaptively to give us some robustness to the different magnitude of rewards that you might see.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so just to spell that out a little bit.",
                    "label": 0
                },
                {
                    "sent": "OK, So what we do is we build a data set from the agent's own experience.",
                    "label": 1
                },
                {
                    "sent": "We take an action, we store the transition that we observe from that action in our replay memory, we sample around a mini batch of transitions from our replay memory and then just as before we optimize the mean squared error between the queue network and the targets.",
                    "label": 0
                },
                {
                    "sent": "So just as before, we look at these targets here.",
                    "label": 0
                },
                {
                    "sent": "We are squared error between that, but we're using experience replay now.",
                    "label": 0
                },
                {
                    "sent": "So what we've done is we've changed the expectation instead of being over the real sample the the online samples that were actually encountering.",
                    "label": 0
                },
                {
                    "sent": "We actually draw these samples from our own experience replay memory.",
                    "label": 1
                },
                {
                    "sent": "So this is an expectation with respect to our own sampling process.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then the second thing we do is to fix these target networks and all we do there is.",
                    "label": 0
                },
                {
                    "sent": "We basically keep track of some old parameters these W minus.",
                    "label": 0
                },
                {
                    "sent": "So we freeze these old parameters and we update our mean squared error.",
                    "label": 0
                },
                {
                    "sent": "Now we look at the mean squared error between these frozen targets and the frozen parameters.",
                    "label": 0
                },
                {
                    "sent": "So it frees up parameters.",
                    "label": 0
                },
                {
                    "sent": "We treat that as a target for some period of time.",
                    "label": 0
                },
                {
                    "sent": "We hold these parameters frozen.",
                    "label": 0
                },
                {
                    "sent": "RQ targets now become fixed, so it's just like we're doing supervised learning towards these targets with a fixed target network.",
                    "label": 1
                },
                {
                    "sent": "And now we look at the mean squared error between these fixed targets and our current estimate of the Q values.",
                    "label": 0
                },
                {
                    "sent": "Just like in fitted Q iteration.",
                    "label": 0
                },
                {
                    "sent": "And then what we do is we really periodically update our fixed parameters.",
                    "label": 1
                },
                {
                    "sent": "We can also do this online in a smooth way, and so we have an online way to do something like fitted Q iteration.",
                    "label": 0
                },
                {
                    "sent": "So this addresses these instabilities.",
                    "label": 0
                },
                {
                    "sent": "OK, so I want to get into an example.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to talk about the Atari domain now, so these hormones are really fun domain introduced by oh Hey, yeah, sure.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So regarding the question of correlation in the samples, then you say that it makes the whole system unstable.",
                    "label": 0
                },
                {
                    "sent": "So can you give provide a bit more intuition what?",
                    "label": 0
                },
                {
                    "sent": "What is the actual fragile Pirates in the whole algorithm?",
                    "label": 0
                },
                {
                    "sent": "Because is it stochastic gradient dissent that will break if use correlated samples?",
                    "label": 0
                },
                {
                    "sent": "If that's the case, I mean it is not at least some towels.",
                    "label": 0
                },
                {
                    "sent": "Got results are kind of robust with having dependent.",
                    "label": 0
                },
                {
                    "sent": "Series where price is going to the stochastic gradient descent and also we use like something like momentum which artificially add some kind of dependence between the direction of the updates.",
                    "label": 0
                },
                {
                    "sent": "How is that good in that case?",
                    "label": 0
                },
                {
                    "sent": "But using correlations in samples or bad?",
                    "label": 0
                },
                {
                    "sent": "OK, so the question was to give some more insight into how why correlations problematic.",
                    "label": 0
                },
                {
                    "sent": "So I think I want to say that yeah, even in the even in the simple setting, if you were just to do supervised learning with the stochastic gradient descent correlation, basically.",
                    "label": 0
                },
                {
                    "sent": "In some sense, it pushes down the effective weight rate at which you can learn, so you can always remove correlation by having a very very low learning rate, so that effectively by the time you've moved on you kind of break the correlation by just not changing your parameters very much between steps.",
                    "label": 0
                },
                {
                    "sent": "But what happens is you have to push down if you have extreme correlations, you have to push down your learning rate an extreme amount, and in practice to actually see anything actually happen in your.",
                    "label": 0
                },
                {
                    "sent": "In your experiment, you're forced to increase the learning rate to a level which actually causes things to become unstable and break.",
                    "label": 0
                },
                {
                    "sent": "That's the intuition I have.",
                    "label": 0
                },
                {
                    "sent": "I think that's the first issue, which is correlation.",
                    "label": 0
                },
                {
                    "sent": "Just in the data.",
                    "label": 0
                },
                {
                    "sent": "Then you have correlation between the targets and the Q values, which is more about the bootstrapping process, which is a separate source of instability, and so there we basically reduce things to something a little bit more like supervised learning.",
                    "label": 0
                },
                {
                    "sent": "So at any given moment, you're just doing supervised learning towards a fixed target, and therefore you are following a well defined gradient iteration by iteration you are following a true gradient.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I'm going to talk about the Atari domain.",
                    "label": 0
                },
                {
                    "sent": "This is a fun domain where basically what we're going to do is sit some agent down in front of this Atari emulator, so it's just as if you have some Atari emulator available and what our agent is able to do is it just gets to watch the video screen it gets to watch this video playing of what's going on in the game.",
                    "label": 0
                },
                {
                    "sent": "It sees the score that's its reward signal when it gets to take controls which are basically the 18 different actions which are the nine different joystick directions times pressing fire or not.",
                    "label": 0
                },
                {
                    "sent": "And so those are the controls it gets to take, and the idea is that it doesn't get told anything about the game itself, and any game might be plugged into this emulator.",
                    "label": 0
                },
                {
                    "sent": "Here has a bunch of different cartridges which you can play.",
                    "label": 0
                },
                {
                    "sent": "And what we do is we plug in one of maybe 50 different games into this emulator and we ask the agent just like a baby sitting down in front of this thing.",
                    "label": 0
                },
                {
                    "sent": "We ask it to learn from scratch how to perform well in this domain end to end without giving any additional clues.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And So what we do is we apply this.",
                    "label": 0
                },
                {
                    "sent": "We applied this DQN algorithm so it's end to end learning of the Q values directly from the raw pixel inputs.",
                    "label": 0
                },
                {
                    "sent": "So we just.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Divide.",
                    "label": 0
                },
                {
                    "sent": "Frames for the last frame.",
                    "label": 0
                },
                {
                    "sent": "The frame before that, and a stack of four different frames as the input, which sort of characterizes what's just been going on recently in that game.",
                    "label": 0
                },
                {
                    "sent": "This avoids different artifacts from like bullets flashing in and out, and so forth to give some limited history to deal with partial absorbability.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then the output is just this joystick action and the reward is just the change in score for that step.",
                    "label": 1
                },
                {
                    "sent": "And what we do is we choose a convolutional neural network.",
                    "label": 0
                },
                {
                    "sent": "Is the neural network representation is our deep representation that we plug in as our Q network is our function approximator for the Q values?",
                    "label": 0
                },
                {
                    "sent": "We plug that in.",
                    "label": 0
                },
                {
                    "sent": "We have this convolutional network that basically learns things like, you know, is there an alien at this particular location in Space Invaders and it can learn that across the whole set up and figure out this internal features and.",
                    "label": 0
                },
                {
                    "sent": "Construct its own hidden layers and its own hidden features characterizing what's going on in the whole screen and at the end of it it outputs AQ value for all the different actions.",
                    "label": 0
                },
                {
                    "sent": "So that means in a single forward pass, it can estimate the Q values for all the different joystick actions and therefore picking an action only takes 1 pass rather than having to do a separate pass for each action.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we use the same network architecture, the same convolutional network, actually as an additional layer in the results, I'm going to present in the hidden units and we fix the network architecture.",
                    "label": 0
                },
                {
                    "sent": "We fix all of the hyperparameters, like the learning rates and so forth.",
                    "label": 0
                },
                {
                    "sent": "There's nothing that actually is tuned to the individual game, and we try this across all 50 Atari games.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And these were the results we got relative to a human.",
                    "label": 0
                },
                {
                    "sent": "So this line here roughly indicates what we where you can't statistically tell the difference between our human expert tester.",
                    "label": 0
                },
                {
                    "sent": "And the agent and everything to this side is outperforming our human tester and everything to this side is doing worse than our human tester, including in some games, pretty disastrously worse.",
                    "label": 0
                },
                {
                    "sent": "These are in the more challenging games over on the right, like Montezuma's Revenge.",
                    "label": 0
                },
                {
                    "sent": "And what we have here is basically the performance measured by actually playing real games using the trained.",
                    "label": 0
                },
                {
                    "sent": "So at the end of training we just evaluate the test performance on a bunch of separate games.",
                    "label": 0
                },
                {
                    "sent": "Once it's completed training.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's fun to look at a demo of this.",
                    "label": 0
                },
                {
                    "sent": "So maybe I can just talk through some of these environments.",
                    "label": 0
                },
                {
                    "sent": "So in euro is like a racing game where it has to kind of figure out how to get past all these different cars and the environment switches from ice to grass and ask to learn a different strategy where it actually this the ice is much skinnier, has to learn a different driving strategy.",
                    "label": 0
                },
                {
                    "sent": "This game is called River rate, so it's a side scrolling game, whereas to learn to shoot the fuel avoids the things that cause it to die and find its way through.",
                    "label": 0
                },
                {
                    "sent": "Navigating through to their maximum amount of score.",
                    "label": 0
                },
                {
                    "sent": "So what you start to see is just how different the representations are.",
                    "label": 0
                },
                {
                    "sent": "Some of them are sort of pseudo 3D like this one.",
                    "label": 0
                },
                {
                    "sent": "This battle zone like this pseudo three primitive pseudo 3D representation as to kind of learn to find its perspective, shoot these things, freeway possibly the stupidest game ever invented.",
                    "label": 0
                },
                {
                    "sent": "To get the chicken to cross the road and without getting squashed by a.",
                    "label": 0
                },
                {
                    "sent": "So this one's demon attack.",
                    "label": 0
                },
                {
                    "sent": "So you get fast waves of attacking aliens coming down.",
                    "label": 0
                },
                {
                    "sent": "You guys are probably Mike bowling, talking bout this.",
                    "label": 0
                },
                {
                    "sent": "He's great at presenting this Atari platform.",
                    "label": 0
                },
                {
                    "sent": "This is Pong which is the classic game where we're actually controlling the greenback here and trying to find the angle that just kind of pings the ball past the opponent.",
                    "label": 0
                },
                {
                    "sent": "So we're just playing in single Agent mode here against a fixed computer.",
                    "label": 0
                },
                {
                    "sent": "Opponent finds this particular angle that that's able to win every time.",
                    "label": 0
                },
                {
                    "sent": "This ones beamrider so another kind of shoot'em up space Invader game.",
                    "label": 0
                },
                {
                    "sent": "But with this kind of slightly pseudo 3D perspective to it, it has to avoid all the bullets coming down and maximize its core.",
                    "label": 0
                },
                {
                    "sent": "We got seaquest.",
                    "label": 0
                },
                {
                    "sent": "We have to learn to.",
                    "label": 0
                },
                {
                    "sent": "Basically it's oxygen starts going down and down and down.",
                    "label": 0
                },
                {
                    "sent": "It has to learn to go right up to the top and refuel its oxygen before carrying on a shooting things again.",
                    "label": 0
                },
                {
                    "sent": "This is Cuba or it has to kind of bounce around and turn the whole game to one particular color.",
                    "label": 0
                },
                {
                    "sent": "Go up on this magic platform up to the top again.",
                    "label": 0
                },
                {
                    "sent": "And as to avoid this snake thing called coily.",
                    "label": 0
                },
                {
                    "sent": "So very very different representations, and so in some sense it should be surprising that this convolutional network is able to learn the right features across all of these different games to be able to find exactly the features that we need.",
                    "label": 0
                },
                {
                    "sent": "And yet even if it's side scrolling or pseudo 3D, it's actually this simple idea of building up these features region by region is sufficient to actually understand and characterize the value function sufficiently to play the game.",
                    "label": 0
                },
                {
                    "sent": "Well, this is the classic space invaders, where it shoots the mother ship there.",
                    "label": 0
                },
                {
                    "sent": "Takes like this human like strategy of taking out the Space Invaders column by column.",
                    "label": 0
                },
                {
                    "sent": "Which gives it actually more time for them to move from side to side.",
                    "label": 0
                },
                {
                    "sent": "It's quite a nice strategy to choose.",
                    "label": 0
                },
                {
                    "sent": "And hides behind these these blocks there.",
                    "label": 0
                },
                {
                    "sent": "That's the shelter itself.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, good clarification.",
                    "label": 0
                },
                {
                    "sent": "So the question was, is it 1 representation shared across all games or separately learned representation for each game?",
                    "label": 0
                },
                {
                    "sent": "It's a separately land representation for each game so the baby gets reborn for each game separately has its own life where it's exposed to just that game only and it learns the representation just for that game.",
                    "label": 0
                },
                {
                    "sent": "The other problem is of course interesting to David.",
                    "label": 0
                },
                {
                    "sent": "Yeah, in many of the games, it seems like the computer player is very very jumpy.",
                    "label": 0
                },
                {
                    "sent": "Did you have any like a human doesn't play that way?",
                    "label": 0
                },
                {
                    "sent": "Did you have any cost on the actions or on smoothness of action?",
                    "label": 0
                },
                {
                    "sent": "Yeah, we chose not to put any additional costs on the actions so you could of course encourage it to play more smoothly, but there's nothing in the score itself that says you should or have too.",
                    "label": 0
                },
                {
                    "sent": "So we just went with the simplest version.",
                    "label": 0
                },
                {
                    "sent": "OK. Do you train a separate method for each action?",
                    "label": 0
                },
                {
                    "sent": "Or do you combine state and action information and use to train a separate network rejection?",
                    "label": 0
                },
                {
                    "sent": "No, it's one is 1 network that estimates the Q values for all actions, so all the features are shared like the the network is common amongst all the action, so so the features are re used to estimate the value of different actions.",
                    "label": 0
                },
                {
                    "sent": "See if the action just comes in the final layer.",
                    "label": 0
                },
                {
                    "sent": "OK, can you show some examples of games where the system is not so good and the human is better?",
                    "label": 0
                },
                {
                    "sent": "I don't have videos.",
                    "label": 0
                },
                {
                    "sent": "I can tell you about it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I can tell you about them.",
                    "label": 0
                },
                {
                    "sent": "For some reason we weren't so keen on making those videos again.",
                    "label": 0
                },
                {
                    "sent": "So so the hardest Atari game is something called Montezuma's Revenge, and it's basically like 100 different rooms connected together in like a giant maze with loads of keys and doors and you have to get the red key to get through the red door and then go backwards and forwards through this maze.",
                    "label": 0
                },
                {
                    "sent": "I mean we don't even get through, you know we don't get the first key for the first door.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's just you know the the search space is so vast that getting off the ground with random exploration is just too challenging.",
                    "label": 0
                },
                {
                    "sent": "People have done well in the planning setting in that in that domain.",
                    "label": 0
                },
                {
                    "sent": "I'll take one more and then I'm going to move on to some other parts and we can have more questions at the end of this.",
                    "label": 0
                },
                {
                    "sent": "OK, was your goal to build a model that was better than human being initially, or did you want to build a model that is equally as good as human?",
                    "label": 0
                },
                {
                    "sent": "Because if your goal is to build something that acts like human beings, then you should probably consider forgetfulness or some failure in actions.",
                    "label": 0
                },
                {
                    "sent": "Or yeah, so we didn't impose like biological constraints like noise or.",
                    "label": 0
                },
                {
                    "sent": "Reaction time or anything like that.",
                    "label": 0
                },
                {
                    "sent": "We just again kept with the simplest version.",
                    "label": 0
                },
                {
                    "sent": "I think it would be interesting to try that.",
                    "label": 0
                },
                {
                    "sent": "Honestly, I don't think it would really change their results qualitatively, because we're already doing kind of some naive things like duplicating the action four times in a row.",
                    "label": 0
                },
                {
                    "sent": "So although the inputs coming at 60 Hertz, we only select actions at 15 Hertz and then duplicate.",
                    "label": 0
                },
                {
                    "sent": "And maybe yeah, I think it'll be interesting.",
                    "label": 0
                },
                {
                    "sent": "OK, if it's OK, I'll take more questions on this at the end, so I want to.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Say a few more things.",
                    "label": 0
                },
                {
                    "sent": "So first of all, the question is, you know, how much does this really help compared to just doing, say, naive Q learning?",
                    "label": 1
                },
                {
                    "sent": "So this is basically teasing apart the components of the algorithm piece by piece, and so if we just start off with Q learning, we see that across a sample of five games which weren't specially chosen to make appoint.",
                    "label": 0
                },
                {
                    "sent": "These were just the five games we originally tried.",
                    "label": 1
                },
                {
                    "sent": "We see Q learning actually does pretty badly and games like breakout it gets like 3 points when we combine it with the target Q.",
                    "label": 0
                },
                {
                    "sent": "This idea of freezing the target network and just doing kind of any given time following the gradient towards that target network, we do significantly better across all games when we add replay in.",
                    "label": 0
                },
                {
                    "sent": "That makes a very large difference across all games and combining them together does better again and so in some sense it's the difference between and are not working at all and working, so it's.",
                    "label": 0
                },
                {
                    "sent": "Although the ideas are simple, I think that sort of perhaps at least if you take this approach, we can think of them as necessary ingredients, or at least you need to deal with the issues that Q learning has in some way to achieve the kind of performance that we really want to get towards the basic algorithms just aren't stable enough.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "I just wanted to make a brief comment that you know in the original nature paper we always just clip the rewards to minus 1 + 1, which is a complete hack and many are L people are probably, you know.",
                    "label": 1
                },
                {
                    "sent": "Feeling very uncomfortable when they saw that and so you should in some sense.",
                    "label": 0
                },
                {
                    "sent": "I mean it works very effectively, but but it actually changes the problem they're trying to solve because it means that we can't tell the difference between the mother ship in Space Invaders and just blowing up in a single alien.",
                    "label": 0
                },
                {
                    "sent": "So what we?",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I've done more recently is actually to normalize the network output.",
                    "label": 0
                },
                {
                    "sent": "There are many ways to do this in the deep learning literature we we chose one.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This works quite nicely, so if I show for example, what happens in Pacman, which was?",
                    "label": 0
                },
                {
                    "sent": "Again, we were doing very poorly on before.",
                    "label": 0
                },
                {
                    "sent": "We can see that.",
                    "label": 0
                },
                {
                    "sent": "Now we've got the ability to tell the difference between eating a ghost.",
                    "label": 0
                },
                {
                    "sent": "Once you've got power pal, which gets you a lot of points and just eating the normal pills and it lends this nice strategy where it really just homes in on the big points so it gets the power pill, gobbles up all this ghost, and then it learned a strategy that are humans didn't figure out which is.",
                    "label": 0
                },
                {
                    "sent": "It gets a second power pill here now.",
                    "label": 0
                },
                {
                    "sent": "It basically goes and hangs out by the Dem.",
                    "label": 0
                },
                {
                    "sent": "And it waits for them all to come out and it just gobbles them up as they come out.",
                    "label": 0
                },
                {
                    "sent": "And it gets this huge score.",
                    "label": 0
                },
                {
                    "sent": "Which is quite nice to think about.",
                    "label": 0
                },
                {
                    "sent": "So it's um.",
                    "label": 0
                },
                {
                    "sent": "So I think the moral there is, you know, always solve the problem you care about rather than a simplification.",
                    "label": 0
                },
                {
                    "sent": "If you can.",
                    "label": 0
                },
                {
                    "sent": "And so I think we've taken, you know, maybe one more step towards that.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, so in the remaining time I wanted to talk about policies and models a little bit so.",
                    "label": 0
                },
                {
                    "sent": "So let me at least give some.",
                    "label": 0
                },
                {
                    "sent": "Basic intuition into approach to do deep learning with policy searching in real.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm going to focus in particular on the continuous action case, so so you know, this is motivated by the fact that the methods we've seen so far they don't directly apply to domains with continuous action spaces.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Because when we've got a discrete action space we can easily Max over those actions like in Q learning, whereas when you have continuous action space, performing that Max is tricky.",
                    "label": 0
                },
                {
                    "sent": "It's an optimization problem in its own right, and you simply can't afford to do that optimization affectively every single step.",
                    "label": 0
                },
                {
                    "sent": "And So what we're going to do instead is to parameterise the policy directly using a deep network.",
                    "label": 1
                },
                {
                    "sent": "This pie.",
                    "label": 0
                },
                {
                    "sent": "And now we're going to have a different parameter vector you so W as the parameters for our Q network.",
                    "label": 0
                },
                {
                    "sent": "Now we have parameter vector U which is going to parameterise our actions directly.",
                    "label": 0
                },
                {
                    "sent": "The way in which we pick actions, and.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do is basically define an objective function like the total reward that we get across the whole the whole domain, and we want to basically ask how can we optimize this objective function end to end by stochastic gradient descent.",
                    "label": 0
                },
                {
                    "sent": "This is our goal and to end STD.",
                    "label": 0
                },
                {
                    "sent": "In other words, adjusting the policy parameters in the way that gets more reward.",
                    "label": 1
                },
                {
                    "sent": "We want to do this in continuous domains, building on what has been effective and what we've seen working with deep learning in the in the discrete domains.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the approach that we take is based on the deterministic policy gradient.",
                    "label": 0
                },
                {
                    "sent": "So this is just a simple observation that that the actual true gradient of the policy with respect to this objective function can basically be described as the direction that most improves Q.",
                    "label": 1
                },
                {
                    "sent": "That we knew our Q values for any given policy.",
                    "label": 0
                },
                {
                    "sent": "All we need to do is to adjust our parameters in the direction that gets us more cute.",
                    "label": 0
                },
                {
                    "sent": "He was telling us how good our policy is, how much award we're getting.",
                    "label": 0
                },
                {
                    "sent": "So if we average that over all of the states that we actually visit, it turns out that the gradient really is just the gradient of these Q values with respect to our policy parameters.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do is follow that gradient.",
                    "label": 0
                },
                {
                    "sent": "We can adjust our policy parameters to get more Q.",
                    "label": 0
                },
                {
                    "sent": "And so that requires us to know Q.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And So what we're going to do is use something called the deterministic actor critic Owen.",
                    "label": 0
                },
                {
                    "sent": "Actor critic is basically a algorithm has two parts to it.",
                    "label": 0
                },
                {
                    "sent": "It has a policy, the actor which has its own parameters.",
                    "label": 1
                },
                {
                    "sent": "So this is actor here, something which we started.",
                    "label": 0
                },
                {
                    "sent": "This stage, we've got a policy parameters and maybe it goes through a bunch of different hidden States and ultimately it outputs the action we're choosing.",
                    "label": 0
                },
                {
                    "sent": "Then we've got a critic, which is basically the value function, and the critic is basically there to evaluate our policy to tell the actor how well it's doing, so the critic.",
                    "label": 1
                },
                {
                    "sent": "Stop taking this state in an action and it has some parameters that actually goes through again its own hidden States and maybe this is very deep network and at the end of its output Q value and that Q value its job is to estimate the value function for our current policy for the policy being used by the actor.",
                    "label": 0
                },
                {
                    "sent": "And So what we're going to do is basically use the critic as a loss function for the actor.",
                    "label": 0
                },
                {
                    "sent": "That's how we're going to do this deeper actor critic.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do is sequence these things together.",
                    "label": 0
                },
                {
                    "sent": "So we're going to start off with our state.",
                    "label": 0
                },
                {
                    "sent": "To our policy function using our policy parameters to go through some hidden States and eventually pick an action.",
                    "label": 0
                },
                {
                    "sent": "But now we've got our states in our action going to plug those into our Q network, go through some more layers and eventually output our Q values at the end of it.",
                    "label": 0
                },
                {
                    "sent": "And once we can do that, well, we know from the first part of the talk that we can just compute the gradient of this whole thing.",
                    "label": 0
                },
                {
                    "sent": "Now we know how to compute the gradient of.",
                    "label": 0
                },
                {
                    "sent": "In other words, we know how to adjust these policy parameters so as to get a small queue at the end of it just by reversing the flow.",
                    "label": 0
                },
                {
                    "sent": "And applying the chain rule and propagating these gradients all the way back through this network.",
                    "label": 1
                },
                {
                    "sent": "So we use the critic as a loss function for the actor.",
                    "label": 0
                },
                {
                    "sent": "Sequence them together flow the gradients all the way back through.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so yeah.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "So it turns out you can do this with discrete actions, but it just collapses to the standard stochastic policy gradient, so it turns out to be identical.",
                    "label": 0
                },
                {
                    "sent": "You can basically push the stochastic your policy into the environment and so basically you kind of pick the parameters of your softmax or whatever.",
                    "label": 0
                },
                {
                    "sent": "Treat that as a terministic policy, and then the randomness.",
                    "label": 0
                },
                {
                    "sent": "You can imagine all happens in your environment, you end up with exactly the same, which is reassuring.",
                    "label": 0
                },
                {
                    "sent": "You basically end up with exactly the standard stochastic policy gradient.",
                    "label": 0
                },
                {
                    "sent": "So the reason that we choose this approach is precisely because in a continuous domain there is this gradient signal that fault that we can follow and can get a more efficient gradient, and we want to make use of that and build on it and not throw away good information when we have it.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we're going to do is we're going to basically use this actor critic.",
                    "label": 0
                },
                {
                    "sent": "Then we're going to estimate the value of the current policy by using Q learning.",
                    "label": 1
                },
                {
                    "sent": "So let's build on what we've seen in the first half of the talk, and the value based approach.",
                    "label": 0
                },
                {
                    "sent": "So now we need to get our critic to get the right value.",
                    "label": 0
                },
                {
                    "sent": "The critic needs to figure out how good is this policy we're following, so we're just going to use Q learning there.",
                    "label": 0
                },
                {
                    "sent": "So we're going to basically follow the loss.",
                    "label": 0
                },
                {
                    "sent": "Minimize this mean squared error loss to adjust our Q values towards the target in blue there.",
                    "label": 1
                },
                {
                    "sent": "And at the same time we're going to update our policy parameters in the direction that improves queue by basically.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going all the way back to identify the parameters that best that help us to maximize our Q values, so that's just.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Chain rule.",
                    "label": 0
                },
                {
                    "sent": "To find the direction that gives us more Q.",
                    "label": 1
                },
                {
                    "sent": "So the cube ID you the way that our value function depends on you is just the Q by Dada by DU is the chain rule.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So, so this seems very nice, but again, it doesn't work, it's unstable and So what we have to do.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is apply the same toolkit that we've already seen for DQN?",
                    "label": 0
                },
                {
                    "sent": "Can we be applied to the actor critic and it also stabilizes the actor critic and gives much nicer results?",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the idea is basically to apply the same three steps to stabilize the actor critic.",
                    "label": 0
                },
                {
                    "sent": "So we're going to use experience replay for both the actor and the critic.",
                    "label": 1
                },
                {
                    "sent": "We're going to freeze the target network to avoid oscillations.",
                    "label": 0
                },
                {
                    "sent": "In that case that this in this case it means we freeze the network for both the actor and the critic.",
                    "label": 0
                },
                {
                    "sent": "So we're going to keep old parameter values for both our policy and for our value function.",
                    "label": 0
                },
                {
                    "sent": "And we're going to update our target for our Q.",
                    "label": 0
                },
                {
                    "sent": "Learning is going to be the old frozen target, so this is really like a fixed target.",
                    "label": 0
                },
                {
                    "sent": "So we are still doing supervised learning towards these targets at any given time.",
                    "label": 0
                },
                {
                    "sent": "And then what we've done is all we've done from the previous slides, with game replaced our instead of learning this thing online.",
                    "label": 0
                },
                {
                    "sent": "Learning by sampling from our own experience.",
                    "label": 0
                },
                {
                    "sent": "So we take a step.",
                    "label": 0
                },
                {
                    "sent": "We add that into our database of experience in every step.",
                    "label": 0
                },
                {
                    "sent": "What we do is we sample of a mini batch of experience from our own experience replay database and we.",
                    "label": 0
                },
                {
                    "sent": "Follow the gradient with respect to that mini batch, so this gives defines the gradients that we follow during learning.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what we did was we applied this to continuous control now so we took the majokko simulator for physics.",
                    "label": 1
                },
                {
                    "sent": "We made a bunch of different domains.",
                    "label": 0
                },
                {
                    "sent": "We passed it through exactly the same confident that we used in the first half of the talk.",
                    "label": 1
                },
                {
                    "sent": "Starting from a stack of frames again passing it through a confident and then giving RQ values, but also starting from the same stack of frames with a different set of parameters and giving us our policy.",
                    "label": 0
                },
                {
                    "sent": "So two separate networks, and this is some recent work by Tim Lillicrap and another deep mind.",
                    "label": 1
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have a look at how this works now.",
                    "label": 0
                },
                {
                    "sent": "OK, so starting off with some when it says low dimensional features this is basically problems where we actually provide the correct state representation to the agent.",
                    "label": 0
                },
                {
                    "sent": "So the first part of this demo is all using the correct state representation.",
                    "label": 0
                },
                {
                    "sent": "But what we can see is it's very robust and again, we're using exactly the same representation, exactly the same hyperparameters across quite a range of different tasks, and it's able to effectively learn to solve these tasks.",
                    "label": 0
                },
                {
                    "sent": "From scratch without any additional guidance about how that problem work, so we don't tell anything about the dynamics of the problem or give any clues.",
                    "label": 0
                },
                {
                    "sent": "This one ask to balance this quadruped, which likes to fall over a lot.",
                    "label": 0
                },
                {
                    "sent": "This ones moving a block to a particular target.",
                    "label": 0
                },
                {
                    "sent": "This is a it's called the Cheetah Half Cheetah task.",
                    "label": 0
                },
                {
                    "sent": "This is another kind of gripping and moving task.",
                    "label": 0
                },
                {
                    "sent": "It is also able to move the thing around a little bit.",
                    "label": 0
                },
                {
                    "sent": "This is a Walker.",
                    "label": 0
                },
                {
                    "sent": "Then it's quite a fun gate there.",
                    "label": 0
                },
                {
                    "sent": "And now we're actually learning or from raw pixels, so this is what it actually looks like to the agent.",
                    "label": 0
                },
                {
                    "sent": "Kind of sees this very kind of crude representation, so it's got like a camera fixed and watching the problem from above.",
                    "label": 0
                },
                {
                    "sent": "So imagine there's just this camera fixed on it, and it just sees the pixels.",
                    "label": 0
                },
                {
                    "sent": "It's not told anything about the state representation, and it has to build its own representation directly from scratch, and learn how to balance here.",
                    "label": 0
                },
                {
                    "sent": "And this is a 3 dimensional kind of Reacher task is to move this thing around.",
                    "label": 0
                },
                {
                    "sent": "This one's a bit hard to see, and then this one is a.",
                    "label": 0
                },
                {
                    "sent": "Is a racing game.",
                    "label": 0
                },
                {
                    "sent": "I have continuous actions.",
                    "label": 0
                },
                {
                    "sent": "This is the same one with.",
                    "label": 0
                },
                {
                    "sent": "This is what it actually sees when it learns it directly from from pixels.",
                    "label": 0
                },
                {
                    "sent": "It's kind of.",
                    "label": 0
                },
                {
                    "sent": "Even though at this very crude representation it learns something, there we go.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we have a few minutes left to talk about models.",
                    "label": 0
                },
                {
                    "sent": "So summary so far.",
                    "label": 0
                },
                {
                    "sent": "So we had our recipe.",
                    "label": 0
                },
                {
                    "sent": "We wanted to do end to end deep learning where we define a gradient.",
                    "label": 0
                },
                {
                    "sent": "We follow that gradient end to end and we come up with some way to kind of represent a loss function for value functions.",
                    "label": 0
                },
                {
                    "sent": "We found.",
                    "label": 0
                },
                {
                    "sent": "Now using this deterministic policy gradient subgradient that we can follow to adjust our policy parameters directly.",
                    "label": 0
                },
                {
                    "sent": "And now what about models?",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So model based reinforcement learning, we the idea is to learn something like a transition model of the environment by transition model.",
                    "label": 1
                },
                {
                    "sent": "I just used the word transition there because if you talk about models to anyone in deep learning they get really confused.",
                    "label": 0
                },
                {
                    "sent": "So so transition model.",
                    "label": 0
                },
                {
                    "sent": "Here we mean the way that the environment has works like probability of getting some reward and some next state given the state you're in the action you Turkey for some definition of state which could be the agent's own state representation.",
                    "label": 0
                },
                {
                    "sent": "And what we're going to do is, once we learned this model, we know the reason that model based RL is appealing is that once you've learned such a transition model, you can in principle plan with that transition model, which it feels like should give us a big win.",
                    "label": 0
                },
                {
                    "sent": "It feels like we should be able to learn very efficiently because we learn this compact model that represents how the structure of the problem actually works, and then we can do this very powerful look ahead.",
                    "label": 0
                },
                {
                    "sent": "We can plan into the future.",
                    "label": 0
                },
                {
                    "sent": "We can do that research.",
                    "label": 1
                },
                {
                    "sent": "We can do all kinds of things so as to find the optimal actions.",
                    "label": 0
                },
                {
                    "sent": "So if you imagine you're playing this game seaquest, you might do this by if you had a model.",
                    "label": 0
                },
                {
                    "sent": "I told you what would happen if I if I took the move joystick select and then what would happen if I move the joystick to the right.",
                    "label": 0
                },
                {
                    "sent": "I can compare that to what would happen if I move to the right and left and I can change through this hypothetical consequences and workout what the best string of actions might be and therefore the best actions take at the start state.",
                    "label": 0
                },
                {
                    "sent": "So that's the appeal of model based RL.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And So what I want to tell you is there's sort of some good news and some bad news.",
                    "label": 0
                },
                {
                    "sent": "The good news is that, at least so far we and others have been able to do quite a good job.",
                    "label": 0
                },
                {
                    "sent": "I think of actually learning what appears to be a nice looking transition model using a deep network.",
                    "label": 1
                },
                {
                    "sent": "And there are various ways you can do this, and I don't want to kind of get into the details of this, except to say that you can define objective function that measures the goodness of the model and the one I'm going to show you actually used the number of bits required to reconstruct the next state.",
                    "label": 1
                },
                {
                    "sent": "So like an information theoretic.",
                    "label": 0
                },
                {
                    "sent": "Loss function, but it's exactly the same end to end principle where you define this loss function you optimize for that loss function by stochastic gradient descent and you just pump the data through as much as you can.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You end up with something which looks quite nice in the game of Atari, so if we do this.",
                    "label": 0
                },
                {
                    "sent": "So this is that River rate gain.",
                    "label": 0
                },
                {
                    "sent": "The sidescrolling game that we were looking at earlier.",
                    "label": 0
                },
                {
                    "sent": "And here we asked it to fantasize a complete trajectory.",
                    "label": 0
                },
                {
                    "sent": "So it's not being reconstructed every step.",
                    "label": 0
                },
                {
                    "sent": "This is like seeding at once and telling it to just run forwards for the rest of the demo.",
                    "label": 0
                },
                {
                    "sent": "So this is 1 long hypothetical trajectory generated by the model, and you can see it's doing a pretty reasonable job of kind of hypothesizing all the different structures.",
                    "label": 0
                },
                {
                    "sent": "We just did it in black and white for the outlines of the objects, which is why it looks a little bit different.",
                    "label": 0
                },
                {
                    "sent": "But it's got some idea of what's going on in the game.",
                    "label": 0
                },
                {
                    "sent": "It kind of comes up with interesting objects.",
                    "label": 0
                },
                {
                    "sent": "And yet when you actually use this to plan now, you plug this into your favorite planner and you try to plan with it.",
                    "label": 0
                },
                {
                    "sent": "It's horrible, does much, much worse than the model free methods that we saw earlier, and the question is why?",
                    "label": 0
                },
                {
                    "sent": "So?",
                    "label": 0
                },
                {
                    "sent": "I just wanted to kind of leave you with this.",
                    "label": 0
                },
                {
                    "sent": "You know, that's the last thing I really want to talk about is this big question?",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Why is it so hard to?",
                    "label": 0
                },
                {
                    "sent": "Get the model based approach working effectively.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so I think really, one of the fundamental issues which we're facing is the problem compounding errors so.",
                    "label": 0
                },
                {
                    "sent": "In something like Atari, we have this kind of messy state space.",
                    "label": 0
                },
                {
                    "sent": "You've got these pixels high dimensional pixels going to another step of high dimensional pixels, and so inevitably you're going to have some error at the one step level.",
                    "label": 0
                },
                {
                    "sent": "But the problem is more that an entire trajectory lost 10s of thousands of steps, and so over the trajectory those errors compound until by the end of the trajectory you might have something somewhat meaningless, which actually you know you think you're going to imagine this whole trajectory through that that imaginary kind of River rate scenario.",
                    "label": 0
                },
                {
                    "sent": "And at the end of it, perhaps you imagine some Mother ship appears and you get to shoot it and get 1000 points, whereas in reality that just might not happen at all, and so the air is just compound and compound.",
                    "label": 0
                },
                {
                    "sent": "This also happens the hidden level of the neural network, so you kind of get these quite abstract compounded errors where you can kind of fantasize things which are quite inaccurate at a high level, even if they look correct at a low level.",
                    "label": 0
                },
                {
                    "sent": "And so by the end of a long trajectory, the rewards can just be totally wrong, and for us at least, and others we've spoken to on this model based IRL hasn't done so well in these types of domains.",
                    "label": 1
                },
                {
                    "sent": "And so.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "There's an alternative, and I think it's an interesting alternative, which is to try and plan implicitly.",
                    "label": 1
                },
                {
                    "sent": "And So what I want to kind of get you thinking about is this idea that actually, you know, we can think of a neural network as a computational process where we started off with this sequence of functions and every one of those functions is performing computational step.",
                    "label": 0
                },
                {
                    "sent": "And so in some sense, those computational steps can learn to do the steps of planning for themselves.",
                    "label": 0
                },
                {
                    "sent": "If you trust the gradients of your neural network, the great their neural network can actually learn to plan for itself, like if you allow it to do 20 layers of computation, those 20 layers of computation can essentially look ahead 20 steps in some kind of hypothetical plan.",
                    "label": 0
                },
                {
                    "sent": "So the question is, you know our transition model is actually required at all?",
                    "label": 1
                },
                {
                    "sent": "Or is the network kind of implicitly learning some internal representation of model that doesn't correspond to our own semantics and our own expectations of what should go into a model?",
                    "label": 0
                },
                {
                    "sent": "So I wanted to leave you with one example, which kind of surprised me the most, because if you'd asked me to come up with one domain where model based planning.",
                    "label": 0
                },
                {
                    "sent": "So if you ask me, you know, come up with one domain where you absolutely have to plan where model free approach is just never going to compete with something which explicitly plans into the future.",
                    "label": 0
                },
                {
                    "sent": "I would have chosen the game of go, so this is something I used to work on.",
                    "label": 1
                },
                {
                    "sent": "It's extremely tactical.",
                    "label": 0
                },
                {
                    "sent": "You have these very complicated tactical situations with very sharp values where if you just play one stone in the wrong place.",
                    "label": 0
                },
                {
                    "sent": "You end up completely transforming the value of the position into something from winning to losing.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And yet it turns out that.",
                    "label": 0
                },
                {
                    "sent": "Yes, OK, so that historically the background is that Monte Carlo search methods, which is 1 particular way to look ahead planning, have been extremely effective in the game of go.",
                    "label": 0
                },
                {
                    "sent": "They were the first kind of breakthrough that led to the first strong programs, and so there was this first wrong program logo, for example, and Mogo was able to look ahead millions of positions into the future at rollout.",
                    "label": 1
                },
                {
                    "sent": "All these positions going into the future, and it would use those rollouts to estimate the right thing to do.",
                    "label": 0
                },
                {
                    "sent": "It was doing planning, but by rolling out and hypothesize in the future using a perfect transition model.",
                    "label": 0
                },
                {
                    "sent": "And so the question is, how well can we do with just a neural network which just does these steps of computation?",
                    "label": 0
                },
                {
                    "sent": "Surely it can't compete with an explicit search that looks ahead millions of positions, but it turns out.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But it does so when we built a 12 layer convolutional neural network and we just trained it by supervised learning to predict expert move.",
                    "label": 0
                },
                {
                    "sent": "So this wasn't really RL yet, so we just asked it to then look at one position so it just looks at the position that there's this convolutional network that passes that position through 12 convolutional layers, and then at the end outputs of probability that the expert would have played each one of those moves.",
                    "label": 0
                },
                {
                    "sent": "So it's trying to predict which move the expert would have played and so outputs probabilities.",
                    "label": 0
                },
                {
                    "sent": "The expert would have played in each position.",
                    "label": 0
                },
                {
                    "sent": "And then we ask it to play the game.",
                    "label": 0
                },
                {
                    "sent": "So we use those probabilities to then play the game and it turns out that it does just as well as Mogo actually got a 50% win rate against Mogo.",
                    "label": 0
                },
                {
                    "sent": "This breakthrough Monte Carlo search program.",
                    "label": 0
                },
                {
                    "sent": "And so I just kind of want to leave you with those two thoughts, which first of all, you know if you build a big representation and trust your gradients, you can build surprisingly effective representations that can learn for themselves.",
                    "label": 0
                },
                {
                    "sent": "Things which you might otherwise have thought would need specific handcrafted features or complex knowledge of the domain, but also that that process can actually learn implicitly to do the things which we thought explicit planning required for.",
                    "label": 0
                },
                {
                    "sent": "So maybe you don't even need to do that research.",
                    "label": 0
                },
                {
                    "sent": "For example, maybe you just build a big neural network and it figures it all out for itself.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's basically yet, so just want to conclude, so we've got this general purpose framework that's RL.",
                    "label": 0
                },
                {
                    "sent": "That's why we all work on it.",
                    "label": 0
                },
                {
                    "sent": "And really, what we've talked about in this tutorial is how RL problems can be solved by end to end deep learning.",
                    "label": 1
                },
                {
                    "sent": "Getting to the point where a single agent can actually be applied to many different challenging tasks so you don't have to redefine your new features, it just you plug it in end to end.",
                    "label": 0
                },
                {
                    "sent": "It figures out the right representation and uses that representation to solve the problem you care about.",
                    "label": 0
                },
                {
                    "sent": "And so you know, maybe we're taking one step closer to that equation at the bottom, which I think where we would like to get to.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so how much have we just moved?",
                    "label": 0
                },
                {
                    "sent": "The problem just moved the problem of.",
                    "label": 0
                },
                {
                    "sent": "Learning into the decision about what to do at the different layers of the deep network.",
                    "label": 0
                },
                {
                    "sent": "Because, OK, we'll just do gradient descent, but now the decision is what kind of functions do we apply there?",
                    "label": 0
                },
                {
                    "sent": "And I imagine there's an infinite space of functions you could use.",
                    "label": 0
                },
                {
                    "sent": "So there are a very large.",
                    "label": 0
                },
                {
                    "sent": "There is the space of possible functions you can use is very large, but.",
                    "label": 0
                },
                {
                    "sent": "Some standard architectures.",
                    "label": 0
                },
                {
                    "sent": "Already do quite well without optimizing specifically to the domain you care about, and I think that's the lesson again that we can take from Atari.",
                    "label": 0
                },
                {
                    "sent": "As you might think there well, you need to optimize the architecture again for each individual game, because you know, 3D game is very different from a side scrolling game, but it turns out that actually if you trust the gradients then it tends to just find the way to use the architecture you give it to the best of its ability, and if the architecture has sufficient flexibility then it will find a way to solve the problem.",
                    "label": 0
                },
                {
                    "sent": "And we know that neural networks are universal function approximators, which basically just means that if you provide enough layers and enough parameters, they can represent any function.",
                    "label": 0
                },
                {
                    "sent": "And so I mean a lot of people ask, you know what happened, you know they people think that we spent ages kind of optimizing for this particular.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Architecture that we used and the truth is it was actually the first one we tried and it worked and then we went from that one to the to the nature one we added in one more layer and it worked better.",
                    "label": 0
                },
                {
                    "sent": "And so you add in more parameters it does better but I think I I think there's this.",
                    "label": 0
                },
                {
                    "sent": "Perhaps misconception that you have to.",
                    "label": 0
                },
                {
                    "sent": "Be very, very careful about particular architecture use, and I think there are some basic tool kits that are useful like the weight sharing and the recurrent neural networks in the convolutional networks, but beyond that I think they're surprisingly robust to architectural choices, so it seems there is a black box.",
                    "label": 0
                },
                {
                    "sent": "I don't want to go too far in that you know this is just one type of problem you know I couldn't make a general claim about all problems, but it feels like we're.",
                    "label": 0
                },
                {
                    "sent": "Yes, I kind of want to say optimistically, yes.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You said that we should trust the gradients, right?",
                    "label": 0
                },
                {
                    "sent": "This is part of the message and I'm wondering to what extent you've been plagued by banishing gradients at all, which when you have a very deep in that might affect you, and in particular, if you're you know if the model is that an N layered network, does you know N steps of look ahead in computation?",
                    "label": 0
                },
                {
                    "sent": "If I want that tend to be very big, then the gradients might still end up not being reliable.",
                    "label": 0
                },
                {
                    "sent": "So it's a great question so.",
                    "label": 0
                },
                {
                    "sent": "At this level, I don't think vanishing gradients are too much of a problem, so you know I've talked about the the architecture we used for Atari, the architecture we used for the continuous problems they had.",
                    "label": 0
                },
                {
                    "sent": "You know, maybe say six layers, that isn't sufficient to cause vanishing gradient issues.",
                    "label": 0
                },
                {
                    "sent": "The go example used 12 layers, again not sufficient to banishing gradient problems when you go to maybe 100 layers like in recurrent neural network.",
                    "label": 0
                },
                {
                    "sent": "This is typically where it occurs.",
                    "label": 0
                },
                {
                    "sent": "Then you do have vanishing gradient problems, but there's.",
                    "label": 0
                },
                {
                    "sent": "Well known extensions which actually addressed the vanishing gradient so.",
                    "label": 0
                },
                {
                    "sent": "Recurrent neural network glitch to people tend to fall back on LST EMS to avoid vanishing gradients, but there are many other techniques that Jeff Hinton's been working on her another method recently that he there are various ways you can kind of try to condition.",
                    "label": 0
                },
                {
                    "sent": "You can initialize your weights as well.",
                    "label": 0
                },
                {
                    "sent": "It turns out that that has a big effect on the venture gradients.",
                    "label": 0
                },
                {
                    "sent": "If you basically make sure that all of your eigen values around one when you start off your weights, they tend to stay one for quite awhile and you don't end up with much for so much problem with benching gradients.",
                    "label": 0
                },
                {
                    "sent": "But for all the work here.",
                    "label": 0
                },
                {
                    "sent": "I think the answer is it wasn't too much of an issue.",
                    "label": 0
                },
                {
                    "sent": "OK. OK Albert, I have a simple question in the plot that you showed about all the Atari games and your performance versus human players.",
                    "label": 0
                },
                {
                    "sent": "Have you looked into?",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Those cases that you're not actually doing well and then have some metric with respect, like how well humans actually is.",
                    "label": 0
                },
                {
                    "sent": "It actually a hard game for humans as well or not.",
                    "label": 0
                },
                {
                    "sent": "So, so let me preface this by saying, you know, we just had one human tester, so this is not like you know there are humans who can do way better at these games.",
                    "label": 0
                },
                {
                    "sent": "So superhuman would mean doing much, much better in all of these games.",
                    "label": 0
                },
                {
                    "sent": "Some of the games we do badly on, so I think the way I like to characterize it is that we're doing quite a good job of learning the representation using the deep learning end to end, and so if you take a game and if you imagine that you could handcrafts and features which were just right for that domain, and then you would expect a naive reinforcement learning algorithm on top of those features to work well, then we do really well on it.",
                    "label": 0
                },
                {
                    "sent": "That's kind of the games over here.",
                    "label": 0
                },
                {
                    "sent": "The games we do badly at the ones where, even if you came up with just the right features for that game.",
                    "label": 0
                },
                {
                    "sent": "You would still expect the our algorithm to just work badly, for example Montezuma's Revenge.",
                    "label": 0
                },
                {
                    "sent": "There's an expiration needle in the haystack problem.",
                    "label": 0
                },
                {
                    "sent": "There's no way you'd expect your naive saucer algorithm even if you gave it the right features, you wouldn't expect it to be able to kind of figure out how to get through 100 different rooms to get to the treasure room, and then figure out exactly how to solve the problem.",
                    "label": 0
                },
                {
                    "sent": "So I think what's nice is that maybe we've taken a step towards addressing the representation learning problem, but that helps us get now to the real reinforcement learning issues and be more creative and experimental with actually trying to find out.",
                    "label": 0
                },
                {
                    "sent": "You know what is the PRL problems that that we need to face to deal with hierarchy or expert expiration or memory, or state construction or possibility?",
                    "label": 0
                },
                {
                    "sent": "You know all these issues are the reasons why we do poorly here, but I think where it's just about representation learning.",
                    "label": 0
                },
                {
                    "sent": "We do quite a good job now.",
                    "label": 0
                },
                {
                    "sent": "Whoever's got the microphone, 'cause I I can't see after you train these networks, is there anything you can interpret out of them?",
                    "label": 0
                },
                {
                    "sent": "Once you have the network actually trained in successful?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "It's a really interesting question, so I think you know one thing we'd like to do is to do as neuro scientists do, and to kind of, you know, look inside the network and do the equivalent of the fMRI and try to understand what it's what it's thinking so far.",
                    "label": 0
                },
                {
                    "sent": "I mean, the one thing we can say is we have some plots.",
                    "label": 0
                },
                {
                    "sent": "Which I don't think I have here, at least not without searching, which show like a.",
                    "label": 0
                },
                {
                    "sent": "You can look at you can visualize.",
                    "label": 0
                },
                {
                    "sent": "What the features are, which the network has learned, and you can basically see that it learns this very interesting clustering where all of the features in the network are clustered around a particular combination of what the image looks like combined with how much it's worth.",
                    "label": 0
                },
                {
                    "sent": "So it doesn't just learn things about how it looks, and it doesn't just learn things about how much reward you're going to get from that point, it learns something which kind of combines them two together and you end up in situations, for example in Space Invaders where features clustered together in all the situations where the mother ship is on the screen and you're shooting a bullet towards it.",
                    "label": 0
                },
                {
                    "sent": "Or you the situation is clustered together where in all the situations where there are only three aliens left on the screen and they are represented very close together in the networks own representation, so it lends some representation that kind of abstracts over the details of which particular three aliens it might be in your local region of the screen.",
                    "label": 0
                },
                {
                    "sent": "But it knows that they are similar because they all have a similar expected score like the value function is the same amongst them, but at the same time it does include some spatial information because that's what the government is built on.",
                    "label": 0
                },
                {
                    "sent": "So if they look totally different, they would be.",
                    "label": 0
                },
                {
                    "sent": "It would kind of duplicate one feature kind of cluster to represent those three aliens in one part of the space, and another feature cluster in a in another part of the space.",
                    "label": 0
                },
                {
                    "sent": "Whoever has the microphone again said so.",
                    "label": 0
                },
                {
                    "sent": "This is going back to diners question about eligibility traces.",
                    "label": 0
                },
                {
                    "sent": "When you're using experience replay, really don't need eligibility traces because you just need to replay the experience in the backward order.",
                    "label": 0
                },
                {
                    "sent": "You get the effect of eligibility traces, which is probably what you're seeing because.",
                    "label": 0
                },
                {
                    "sent": "With the experience replay of performance is much better.",
                    "label": 0
                },
                {
                    "sent": "It might not be just a factor of three correlating it, but also getting difficult.",
                    "label": 0
                },
                {
                    "sent": "Political traces.",
                    "label": 0
                },
                {
                    "sent": "OK, so when long chilin defined experience replay, he kind of combined together 2 ideas, one of which was storing all of the experience in some database and then replaying samples of that experience.",
                    "label": 0
                },
                {
                    "sent": "But he also had a second idea, which is what you're referring to, which is that you experience the play forward.",
                    "label": 0
                },
                {
                    "sent": "The trajectory in real life forwards.",
                    "label": 0
                },
                {
                    "sent": "Of course, you know this is an agent living in a world.",
                    "label": 0
                },
                {
                    "sent": "But then when you learn, you go backwards through that experience in reverse order, and this can have some efficiency gains.",
                    "label": 0
                },
                {
                    "sent": "We only use the first of those two ideas, not the second, so we intentionally randomize the sampling.",
                    "label": 0
                },
                {
                    "sent": "We don't have any backward sweeping through the data, and the reason we randomize is to break the correlations.",
                    "label": 0
                },
                {
                    "sent": "You don't have to backward sweep through the data, I mean, as long as you're doing it in random order, you would see some effect of the backward sweep.",
                    "label": 0
                },
                {
                    "sent": "Today.",
                    "label": 0
                },
                {
                    "sent": "That's all I see.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        }
    }
}