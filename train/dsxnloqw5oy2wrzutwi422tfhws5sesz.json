{
    "id": "dsxnloqw5oy2wrzutwi422tfhws5sesz",
    "title": "Pay-as-you-go Approximate Join Top-k Processing for the Web of Data",
    "info": {
        "author": [
            "Andreas Wagner, Institute of Applied Informatics and Formal Description Methods (AIFB), Karlsruhe Institute of Technology (KIT)"
        ],
        "published": "July 30, 2014",
        "recorded": "May 2014",
        "category": [
            "Top->Computer Science->Semantic Web",
            "Top->Computer Science->Big Data"
        ]
    },
    "url": "http://videolectures.net/eswc2014_wagner_web_of_data/",
    "segmentation": [
        [
            "OK, um today I'm going to talk about pace.",
            "You go approximate, cupcake repossessing.",
            "I'm Andreas this is joint work with Billy and entering."
        ],
        [
            "So firstly let me start with a brief brief introduction."
        ],
        [
            "So when you consider queries of our web data query, these queries frequently read to lead to very large result sets."
        ],
        [
            "So what you actually want to, if you want to have ranking so you want to have you want to assist users in finding finding the right the right result items via rank list."
        ],
        [
            "So consider the following examples.",
            "User is searching for movies with highest ratings featuring actually actress Audrey Hepburn.",
            "Playing somewhere close to Rome."
        ],
        [
            "Um, so when you consider."
        ],
        [
            "This this rank query."
        ],
        [
            "In order to evaluate efficiently previous works on Top K query processing have been have been proposed or have been have been done.",
            "And essentially what they they tried to do is they try to compute these top K results or the top ranked results without full result computation, right?",
            "So while admitting the remaining resides?"
        ],
        [
            "Um?",
            "However, it is also clear that for many applications, most notably most notably web search result accuracy and completeness, is actually not important or not so important.",
            "Actually, the key thing is to have to have quick result computation time."
        ],
        [
            "So the system should be able actually to tradeoff result accuracy result completeness without with computation with computation time.",
            "Alright, so in other words, you want to have approximated ranked results for many applications."
        ],
        [
            "So regarding the state of the art of these approaches, so previous works on RDF processing on Top K RDF processing mainly focused on an exact and complete top K results."
        ],
        [
            "On the other hand, in the database community, lots of works have actually been proposed.",
            "However, in our opinion, these are not well suited for web data and also for web queries.",
            "This is mostly becausw.",
            "These works assume the ranking scores be completely known it offline time, so they assume that every ranking score is known at indexing time and they complete, and they compute heavyweight statistics over these.",
            "These ranking scores.",
            "However, when you have web data, but data frequently changes, so you actually have very expensive maintenance of these these statistics.",
            "On the other hand, when you have web queries, you also want to rank rank your resides on runtime.",
            "So consider a keyword queries, consider spatial queries, consider temporal queries.",
            "All these queries require actually runtime ranking and so with regard to this web queries and these this web data, these approaches are actually not not well suited."
        ],
        [
            "Our work actually goes into a completely different."
        ],
        [
            "Different approach it is very lightweight from its architecture and we completely compute the score statistics basically on during query processing.",
            "So on runtime.",
            "And we do this in a pay as you go iterative interview affection."
        ],
        [
            "We actually conducted experiments on two sparkle benchmarks and could show that we could learn sufficiently good statistics in in such a manner and could achieve time savings via this approximation of up to 60% while still maintaining while still maintaining high quality, high quality results."
        ],
        [
            "OK, so let me briefly show you how this is how this is done."
        ],
        [
            "Consider the example query from before, so find movies with Audrey Hepburn highest ratings and playing close close room."
        ],
        [
            "You can actually decompose it."
        ],
        [
            "Interbeat BGP query with two triple sorry three triple pattern.",
            "First one asking for the ratings of the movie, second one asking for the.",
            "Stating that there should be Audrey Hepburn in this movie and the third one asking for location."
        ],
        [
            "On the other hand, you want to have a ranking function that basically sums up rating values of these.",
            "These findings sums up the editors sense of little of that binding to Audrey Hepburn, and Lastly also the distance of the location to Rome.",
            "So these are the."
        ],
        [
            "These are actually the Geo locations of Rome.",
            "Right?"
        ],
        [
            "So when you consider such a query, what you would normally do is actually.",
            "You obviously would construct."
        ],
        [
            "Very tree for that.",
            "So in this setting you have actually.",
            "You actually have three inputs here, so sorted access here, here, and here.",
            "Each of them for one chipper pattern, and you have two joints which combine these inputs file.",
            "Unfortunately, for time reasons I won't go into detail about Top K query processing."
        ],
        [
            "However, this work is mainly important to look at these things here.",
            "That is what we called partial binding.",
            "So essentially this has one binding for the first triple pattern.",
            "And triple fed on three.",
            "And sorry, two and three are unevaluated, OK.",
            "So essentially it's just from here and no joints have it been performed right?"
        ],
        [
            "So what you really want to do is you want to estimate."
        ],
        [
            "How likely this guy here will actually contribute to the final top K results?",
            "So how likely will that actually lead to something that is useful for me?",
            "Alright, if this is unlikely."
        ],
        [
            "Then I can just throw it away right?",
            "And save time by not processing it further.",
            "This is really the key intuition."
        ],
        [
            "So in order to estimate this probability, here we have what we call."
        ],
        [
            "Adopt A test which is performed by something that is called a probabilistic component.",
            "It's not really important, but what is it?"
        ],
        [
            "Courtney, this has two probabilities.",
            "The first is the binding probability.",
            "So what the binding probability essentially does?",
            "Is it estimates to what likelihood or probability this partial binding will lead to a complete binary or contribute to any complete binary?"
        ],
        [
            "The second thing is the score probability, so assume this will lead to a complete binding.",
            "So what you know what score will it likely have, right?",
            "So this is the second second component, second part."
        ],
        [
            "OK, so for the."
        ],
        [
            "In probability, we actually reuse existing work on selectivity estimation for just basic basic graph patterns and spa."
        ],
        [
            "Occurs.",
            "And we have the following very simple indicator function, so you you be given be right."
        ],
        [
            "So what that means is basically QU stands for the unevaluated, unevaluated query patterns from B.",
            "So these are partially binding.",
            "Given that particular partial volume, OK?",
            "And then we."
        ],
        [
            "Use this, this is actually a set of chipper patterns and we just use the selectivity estimation function.",
            "For example, this works here to estimate is that probability is that selectivity greater than zero.",
            "If yes, then indicator function returns 1 otherwise."
        ],
        [
            "Zero, OK?",
            "So fairly fairly simple.",
            "So."
        ],
        [
            "Let me just give you an exam."
        ],
        [
            "Before that, so consider all partial binding from before, where just ripple pattern one has been has been bound to a particular triple."
        ],
        [
            "Then you basically have QU as 2 two triple patterns, right?",
            "So chipper pattern two which asks starring Audrey Hepburn and triple pattern three, which asks the location of that particular that this particular movie has as a location."
        ],
        [
            "Then this then this and this year will basically resolve to the following.",
            "We will basically just replace the variables here.",
            "And here with the binding's dictated by P11 in this case, M3, right?",
            "So this is what this given here means.",
            "So I really substitute variables with their concrete with their concrete bindings.",
            "And as you can see this is just normal set of triple patterns which can be evaluated by any kind of selectivity estimation function.",
            "OK."
        ],
        [
            "Um, so this war probability, on the other hand."
        ],
        [
            "So for the score probability, what you have is basically you want to obtain this guy here.",
            "So a random variable XSQUB.",
            "So what that means is basically you want to estimate the scores of the unevaluated triple patterns QB.",
            "So from the basically and capture these distributions these score distributions via this.",
            "This random variable.",
            "Here.",
            "So this is what you what you want to have."
        ],
        [
            "Again, let me give you a brief example for that.",
            "Again, the."
        ],
        [
            "Partial binding be 1 one that you have seen seen before."
        ],
        [
            "Again, the two super patterns here so and."
        ],
        [
            "Then we basically just have that particular partial binding.",
            "We have one random variable which captures the scores of these particular table patterns, so bindings to those two guys basically right.",
            "OK."
        ],
        [
            "And given that.",
            "Given that random arrival, we can now actually estimate the probability of B contributing to a complete binding that has a score larger than XX is just a just a desired score, basically in its OK Setting X would be basically the least lowest top keybindings for this.",
            "And this is just the probability of this random variable XS being larger larger than or equal to Delta XP, right?",
            "And this Delta XP is just X or the science score minus minus a certain score minus the current score that B because he is a partial binding already has right?",
            "So this die or it has a certain binding in this example 8.5.",
            "So this is this year and so we just need the Delta basically here."
        ],
        [
            "Of course, the question remains how to actually estimate a probability distribution for this particular random variable.",
            "So this is really the key key task actually.",
            "Um?"
        ],
        [
            "For this we use basically learning at runtime, right?",
            "So what we do is."
        ],
        [
            "We assume that each.",
            "That each binding from particular input follows the same distribution.",
            "So as you have seen before, these random variables, right?",
            "They're actually dependent on B.",
            "However, we simplify that right to just be independent on the input eyes, so each triple it comes from.",
            "So each partial binding it comes from the same input follows the same distribution.",
            "It's a simplifying assumption."
        ],
        [
            "Furthermore, since we don't know the true distributions of the of the of the triples and also after partial bindings.",
            "And we have to assume some kind of some kind of distribution, and we actually use the Gaussian distribution.",
            "Eh?"
        ],
        [
            "And we use conjugate priors to actually train discussion distributions, right?",
            "So basically we have an unknown mean of this distribution from from this guy here.",
            "And we have an unknown Orions, and both parameters are basically trained during query processing.",
            "So what does it mean so?"
        ],
        [
            "Conjugate priors can be formulated as follows, so here you have a couple of parameters.",
            "The most important one is actually this year."
        ],
        [
            "Set a parameter and this basically just captures this unknown mean in this unknown brians, OK?"
        ],
        [
            "On the other hand, yes."
        ],
        [
            "These X vectors mine and this you can simply perceive as a set of evidence set of training data that you observe during query processing in our in our setting.",
            "These are just scores of complete or maybe partial bindings.",
            "And also."
        ],
        [
            "Very important is to."
        ],
        [
            "Alpha, right, so Alpha is currently or commonly known actually as high as hyperparameters.",
            "So these hyperparameters basically just stayed some kind of prior beliefs about data, right?",
            "So they encode some kind of prior knowledge that I have about data in our case is unknown mean and the unknown varience OK?"
        ],
        [
            "I know what this base."
        ],
        [
            "It gives you as a prior distribution, so you encode with.",
            "This Alpha actually shows itself actually parameterized.",
            "This prior distribution here, so these are basically knowledge it offline time."
        ],
        [
            "It online."
        ],
        [
            "You observe these samples and you can say OK. How likely are these samples given these TDR parameters, right?"
        ],
        [
            "And then this guy."
        ],
        [
            "If you actually, then you can basically compute what is called a posterior distribution, so the likelihood of the probability of these terror parameters values of these terror parameters in our case mean and brians.",
            "Given the training samples.",
            "So the scores that I that I observed and the Alpha OK."
        ],
        [
            "So learning at runtime.",
            "So the key thing."
        ],
        [
            "It's really to train these alphas here, right?",
            "And this is very lightweight because the number of parameters of hyperparameters are very small.",
            "So in our case it's just four parameters, which makes it very lightweight.",
            "Actually, just store and also two."
        ],
        [
            "To maintain."
        ],
        [
            "So essentially we compute sufficient statistics that offline time from the solid accesses.",
            "To obtain these, these are fast OK."
        ],
        [
            "So so it accesses basically something for each trip, a pattern you have basically is sorted list of bindings to these triple patterns, sorted according to scores, then you."
        ],
        [
            "And basically use this force the source sample in this point 8.5 eight point 1 seven point 7 to obtain a sample mean in a simple rhymes and based on these what is commonly called sufficient statistics.",
            "You then compute the other phone, right?",
            "Jim."
        ],
        [
            "Very time then you can compute the posterior distribution, which gives you the data.",
            "End of course."
        ],
        [
            "You can update the Alpha parameter based on the observed scores that is based on these slides here based on the X, so we can iteratively learn these Alpha parameters so that we can compute new and better estimates for these status."
        ],
        [
            "OK, so briefly my evaluation results are."
        ],
        [
            "The evaluation."
        ],
        [
            "Setting is as follows."
        ],
        [
            "Data increase data from 2 spotted benchmarks.",
            "SP2 and DPS Spartan."
        ],
        [
            "Benchmark we generated datasets of 10,000,000 triples each."
        ],
        [
            "We had around 133 queries over both datasets.",
            "We had three system."
        ],
        [
            "Times."
        ],
        [
            "1st is on joint sort system which basically just has an exact computer exact results, sorts them afterwards and gives them back to the user."
        ],
        [
            "Then we have an exact incomplete.",
            "Took a dropper."
        ],
        [
            "Later and of course we have our our approximately approach.",
            "The ranking function."
        ],
        [
            "This is."
        ],
        [
            "Random scores for triple pen and bindings."
        ],
        [
            "And combines them so combine some as an SM application as a summation, OK?"
        ],
        [
            "Now I wait for time reasons.",
            "Just go into the first benchmark results.",
            "So for the effect.",
            "Sorry for the efficient efficiency.",
            "You can see the three systems seaso in green is our system, rat is the exact OK approach influenced join sort approach.",
            "The threshold Theta is basically an error threshold, so that is basically the probability for the for the top K chest to decide whether or not to throw away a partial binding.",
            "So as you can see, we're actually doing quite well, so we're actually saving inputs here, even for very low, very low thresh."
        ],
        [
            "Notes and overall our system consumes 25% less inputs than the exact OK approach and 30% less than the joins with the sort approach, right?",
            "So mainly the main reason for this is of course the binding.",
            "Of course the pruning of the partial bindings and mainly so the main trick here is to save, join, attend.",
            "So when you have a tree and you prune early in this tree, a partial binding, then of course this leads to less join attempts, thereby leading to less inputs being being materialized in the higher ordered tree.",
            "So in higher order joins right?",
            "So this is the main reason for this here, right?",
            "And as you can see here we have a slight increase in inputs preserved in this, mainly the cause.",
            "Here we actually pruned too much, so here we actually pruned also partial bindings that would have contributed to except to.",
            "To talk a result, however, since we prove them, we actually had to materialize even more bindings.",
            "So that was basically the reason here."
        ],
        [
            "Of course this inputs safety.",
            "It translates in time savings."
        ],
        [
            "Regarding the effectiveness.",
            "So here we have just our approximate top K approach with regard to three different distributions for triple scores, so a uniform, sorry uniform and normal in an exponential distribution.",
            "So.",
            "So here we have again our threshold and the most important thing to see is actually so for a small tower so for us."
        ],
        [
            "Our threshold we actually produce in very high quality results, so in between 8 point.",
            "So sorry open eight and oh .95, which is quite high quality.",
            "And the second important thing to note is that these curves.",
            "Still they have different school distributions, but they still behave similarly.",
            "Therefore, we could learn good distributions for all of these guys, which is considering our simplifying assumptions, right partners?"
        ],
        [
            "So considering the time input saved and the precision at these so that costs basically in terms of precision, you can give a quite nice threshold, quite nice tradeoff."
        ],
        [
            "OK so I have to have."
        ],
        [
            "Include."
        ],
        [
            "So I showed you basically an approximate OK join approach, which we think is well suited for web data and we're queries."
        ],
        [
            "We extended a well known framework for this."
        ],
        [
            "And our very sorry our relation resides could achieve cognize 65% while still maintaining high precision."
        ],
        [
            "Thank you for your time and attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, um today I'm going to talk about pace.",
                    "label": 0
                },
                {
                    "sent": "You go approximate, cupcake repossessing.",
                    "label": 0
                },
                {
                    "sent": "I'm Andreas this is joint work with Billy and entering.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So firstly let me start with a brief brief introduction.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So when you consider queries of our web data query, these queries frequently read to lead to very large result sets.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what you actually want to, if you want to have ranking so you want to have you want to assist users in finding finding the right the right result items via rank list.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So consider the following examples.",
                    "label": 0
                },
                {
                    "sent": "User is searching for movies with highest ratings featuring actually actress Audrey Hepburn.",
                    "label": 0
                },
                {
                    "sent": "Playing somewhere close to Rome.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um, so when you consider.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This this rank query.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In order to evaluate efficiently previous works on Top K query processing have been have been proposed or have been have been done.",
                    "label": 0
                },
                {
                    "sent": "And essentially what they they tried to do is they try to compute these top K results or the top ranked results without full result computation, right?",
                    "label": 0
                },
                {
                    "sent": "So while admitting the remaining resides?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "However, it is also clear that for many applications, most notably most notably web search result accuracy and completeness, is actually not important or not so important.",
                    "label": 0
                },
                {
                    "sent": "Actually, the key thing is to have to have quick result computation time.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the system should be able actually to tradeoff result accuracy result completeness without with computation with computation time.",
                    "label": 0
                },
                {
                    "sent": "Alright, so in other words, you want to have approximated ranked results for many applications.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So regarding the state of the art of these approaches, so previous works on RDF processing on Top K RDF processing mainly focused on an exact and complete top K results.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the other hand, in the database community, lots of works have actually been proposed.",
                    "label": 0
                },
                {
                    "sent": "However, in our opinion, these are not well suited for web data and also for web queries.",
                    "label": 0
                },
                {
                    "sent": "This is mostly becausw.",
                    "label": 0
                },
                {
                    "sent": "These works assume the ranking scores be completely known it offline time, so they assume that every ranking score is known at indexing time and they complete, and they compute heavyweight statistics over these.",
                    "label": 0
                },
                {
                    "sent": "These ranking scores.",
                    "label": 0
                },
                {
                    "sent": "However, when you have web data, but data frequently changes, so you actually have very expensive maintenance of these these statistics.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, when you have web queries, you also want to rank rank your resides on runtime.",
                    "label": 0
                },
                {
                    "sent": "So consider a keyword queries, consider spatial queries, consider temporal queries.",
                    "label": 0
                },
                {
                    "sent": "All these queries require actually runtime ranking and so with regard to this web queries and these this web data, these approaches are actually not not well suited.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our work actually goes into a completely different.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Different approach it is very lightweight from its architecture and we completely compute the score statistics basically on during query processing.",
                    "label": 0
                },
                {
                    "sent": "So on runtime.",
                    "label": 0
                },
                {
                    "sent": "And we do this in a pay as you go iterative interview affection.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We actually conducted experiments on two sparkle benchmarks and could show that we could learn sufficiently good statistics in in such a manner and could achieve time savings via this approximation of up to 60% while still maintaining while still maintaining high quality, high quality results.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let me briefly show you how this is how this is done.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Consider the example query from before, so find movies with Audrey Hepburn highest ratings and playing close close room.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can actually decompose it.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Interbeat BGP query with two triple sorry three triple pattern.",
                    "label": 0
                },
                {
                    "sent": "First one asking for the ratings of the movie, second one asking for the.",
                    "label": 0
                },
                {
                    "sent": "Stating that there should be Audrey Hepburn in this movie and the third one asking for location.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On the other hand, you want to have a ranking function that basically sums up rating values of these.",
                    "label": 0
                },
                {
                    "sent": "These findings sums up the editors sense of little of that binding to Audrey Hepburn, and Lastly also the distance of the location to Rome.",
                    "label": 1
                },
                {
                    "sent": "So these are the.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are actually the Geo locations of Rome.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So when you consider such a query, what you would normally do is actually.",
                    "label": 0
                },
                {
                    "sent": "You obviously would construct.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very tree for that.",
                    "label": 0
                },
                {
                    "sent": "So in this setting you have actually.",
                    "label": 0
                },
                {
                    "sent": "You actually have three inputs here, so sorted access here, here, and here.",
                    "label": 0
                },
                {
                    "sent": "Each of them for one chipper pattern, and you have two joints which combine these inputs file.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, for time reasons I won't go into detail about Top K query processing.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, this work is mainly important to look at these things here.",
                    "label": 0
                },
                {
                    "sent": "That is what we called partial binding.",
                    "label": 0
                },
                {
                    "sent": "So essentially this has one binding for the first triple pattern.",
                    "label": 0
                },
                {
                    "sent": "And triple fed on three.",
                    "label": 0
                },
                {
                    "sent": "And sorry, two and three are unevaluated, OK.",
                    "label": 0
                },
                {
                    "sent": "So essentially it's just from here and no joints have it been performed right?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what you really want to do is you want to estimate.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How likely this guy here will actually contribute to the final top K results?",
                    "label": 0
                },
                {
                    "sent": "So how likely will that actually lead to something that is useful for me?",
                    "label": 0
                },
                {
                    "sent": "Alright, if this is unlikely.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then I can just throw it away right?",
                    "label": 0
                },
                {
                    "sent": "And save time by not processing it further.",
                    "label": 0
                },
                {
                    "sent": "This is really the key intuition.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in order to estimate this probability, here we have what we call.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Adopt A test which is performed by something that is called a probabilistic component.",
                    "label": 0
                },
                {
                    "sent": "It's not really important, but what is it?",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Courtney, this has two probabilities.",
                    "label": 0
                },
                {
                    "sent": "The first is the binding probability.",
                    "label": 0
                },
                {
                    "sent": "So what the binding probability essentially does?",
                    "label": 0
                },
                {
                    "sent": "Is it estimates to what likelihood or probability this partial binding will lead to a complete binary or contribute to any complete binary?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second thing is the score probability, so assume this will lead to a complete binding.",
                    "label": 0
                },
                {
                    "sent": "So what you know what score will it likely have, right?",
                    "label": 0
                },
                {
                    "sent": "So this is the second second component, second part.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so for the.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In probability, we actually reuse existing work on selectivity estimation for just basic basic graph patterns and spa.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Occurs.",
                    "label": 0
                },
                {
                    "sent": "And we have the following very simple indicator function, so you you be given be right.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what that means is basically QU stands for the unevaluated, unevaluated query patterns from B.",
                    "label": 0
                },
                {
                    "sent": "So these are partially binding.",
                    "label": 0
                },
                {
                    "sent": "Given that particular partial volume, OK?",
                    "label": 0
                },
                {
                    "sent": "And then we.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Use this, this is actually a set of chipper patterns and we just use the selectivity estimation function.",
                    "label": 0
                },
                {
                    "sent": "For example, this works here to estimate is that probability is that selectivity greater than zero.",
                    "label": 0
                },
                {
                    "sent": "If yes, then indicator function returns 1 otherwise.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Zero, OK?",
                    "label": 0
                },
                {
                    "sent": "So fairly fairly simple.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me just give you an exam.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Before that, so consider all partial binding from before, where just ripple pattern one has been has been bound to a particular triple.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then you basically have QU as 2 two triple patterns, right?",
                    "label": 0
                },
                {
                    "sent": "So chipper pattern two which asks starring Audrey Hepburn and triple pattern three, which asks the location of that particular that this particular movie has as a location.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then this then this and this year will basically resolve to the following.",
                    "label": 0
                },
                {
                    "sent": "We will basically just replace the variables here.",
                    "label": 0
                },
                {
                    "sent": "And here with the binding's dictated by P11 in this case, M3, right?",
                    "label": 0
                },
                {
                    "sent": "So this is what this given here means.",
                    "label": 0
                },
                {
                    "sent": "So I really substitute variables with their concrete with their concrete bindings.",
                    "label": 0
                },
                {
                    "sent": "And as you can see this is just normal set of triple patterns which can be evaluated by any kind of selectivity estimation function.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um, so this war probability, on the other hand.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for the score probability, what you have is basically you want to obtain this guy here.",
                    "label": 0
                },
                {
                    "sent": "So a random variable XSQUB.",
                    "label": 0
                },
                {
                    "sent": "So what that means is basically you want to estimate the scores of the unevaluated triple patterns QB.",
                    "label": 0
                },
                {
                    "sent": "So from the basically and capture these distributions these score distributions via this.",
                    "label": 0
                },
                {
                    "sent": "This random variable.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "So this is what you what you want to have.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, let me give you a brief example for that.",
                    "label": 0
                },
                {
                    "sent": "Again, the.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Partial binding be 1 one that you have seen seen before.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, the two super patterns here so and.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then we basically just have that particular partial binding.",
                    "label": 1
                },
                {
                    "sent": "We have one random variable which captures the scores of these particular table patterns, so bindings to those two guys basically right.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And given that.",
                    "label": 0
                },
                {
                    "sent": "Given that random arrival, we can now actually estimate the probability of B contributing to a complete binding that has a score larger than XX is just a just a desired score, basically in its OK Setting X would be basically the least lowest top keybindings for this.",
                    "label": 0
                },
                {
                    "sent": "And this is just the probability of this random variable XS being larger larger than or equal to Delta XP, right?",
                    "label": 1
                },
                {
                    "sent": "And this Delta XP is just X or the science score minus minus a certain score minus the current score that B because he is a partial binding already has right?",
                    "label": 0
                },
                {
                    "sent": "So this die or it has a certain binding in this example 8.5.",
                    "label": 0
                },
                {
                    "sent": "So this is this year and so we just need the Delta basically here.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of course, the question remains how to actually estimate a probability distribution for this particular random variable.",
                    "label": 0
                },
                {
                    "sent": "So this is really the key key task actually.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For this we use basically learning at runtime, right?",
                    "label": 0
                },
                {
                    "sent": "So what we do is.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We assume that each.",
                    "label": 0
                },
                {
                    "sent": "That each binding from particular input follows the same distribution.",
                    "label": 0
                },
                {
                    "sent": "So as you have seen before, these random variables, right?",
                    "label": 0
                },
                {
                    "sent": "They're actually dependent on B.",
                    "label": 0
                },
                {
                    "sent": "However, we simplify that right to just be independent on the input eyes, so each triple it comes from.",
                    "label": 0
                },
                {
                    "sent": "So each partial binding it comes from the same input follows the same distribution.",
                    "label": 0
                },
                {
                    "sent": "It's a simplifying assumption.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Furthermore, since we don't know the true distributions of the of the of the triples and also after partial bindings.",
                    "label": 0
                },
                {
                    "sent": "And we have to assume some kind of some kind of distribution, and we actually use the Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "Eh?",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we use conjugate priors to actually train discussion distributions, right?",
                    "label": 0
                },
                {
                    "sent": "So basically we have an unknown mean of this distribution from from this guy here.",
                    "label": 0
                },
                {
                    "sent": "And we have an unknown Orions, and both parameters are basically trained during query processing.",
                    "label": 0
                },
                {
                    "sent": "So what does it mean so?",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Conjugate priors can be formulated as follows, so here you have a couple of parameters.",
                    "label": 0
                },
                {
                    "sent": "The most important one is actually this year.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Set a parameter and this basically just captures this unknown mean in this unknown brians, OK?",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the other hand, yes.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These X vectors mine and this you can simply perceive as a set of evidence set of training data that you observe during query processing in our in our setting.",
                    "label": 0
                },
                {
                    "sent": "These are just scores of complete or maybe partial bindings.",
                    "label": 0
                },
                {
                    "sent": "And also.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very important is to.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alpha, right, so Alpha is currently or commonly known actually as high as hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "So these hyperparameters basically just stayed some kind of prior beliefs about data, right?",
                    "label": 0
                },
                {
                    "sent": "So they encode some kind of prior knowledge that I have about data in our case is unknown mean and the unknown varience OK?",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I know what this base.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It gives you as a prior distribution, so you encode with.",
                    "label": 0
                },
                {
                    "sent": "This Alpha actually shows itself actually parameterized.",
                    "label": 0
                },
                {
                    "sent": "This prior distribution here, so these are basically knowledge it offline time.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It online.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You observe these samples and you can say OK. How likely are these samples given these TDR parameters, right?",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then this guy.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you actually, then you can basically compute what is called a posterior distribution, so the likelihood of the probability of these terror parameters values of these terror parameters in our case mean and brians.",
                    "label": 0
                },
                {
                    "sent": "Given the training samples.",
                    "label": 0
                },
                {
                    "sent": "So the scores that I that I observed and the Alpha OK.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So learning at runtime.",
                    "label": 0
                },
                {
                    "sent": "So the key thing.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's really to train these alphas here, right?",
                    "label": 0
                },
                {
                    "sent": "And this is very lightweight because the number of parameters of hyperparameters are very small.",
                    "label": 0
                },
                {
                    "sent": "So in our case it's just four parameters, which makes it very lightweight.",
                    "label": 0
                },
                {
                    "sent": "Actually, just store and also two.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To maintain.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So essentially we compute sufficient statistics that offline time from the solid accesses.",
                    "label": 0
                },
                {
                    "sent": "To obtain these, these are fast OK.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So so it accesses basically something for each trip, a pattern you have basically is sorted list of bindings to these triple patterns, sorted according to scores, then you.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And basically use this force the source sample in this point 8.5 eight point 1 seven point 7 to obtain a sample mean in a simple rhymes and based on these what is commonly called sufficient statistics.",
                    "label": 1
                },
                {
                    "sent": "You then compute the other phone, right?",
                    "label": 0
                },
                {
                    "sent": "Jim.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very time then you can compute the posterior distribution, which gives you the data.",
                    "label": 0
                },
                {
                    "sent": "End of course.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can update the Alpha parameter based on the observed scores that is based on these slides here based on the X, so we can iteratively learn these Alpha parameters so that we can compute new and better estimates for these status.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so briefly my evaluation results are.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The evaluation.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Setting is as follows.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Data increase data from 2 spotted benchmarks.",
                    "label": 0
                },
                {
                    "sent": "SP2 and DPS Spartan.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Benchmark we generated datasets of 10,000,000 triples each.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We had around 133 queries over both datasets.",
                    "label": 0
                },
                {
                    "sent": "We had three system.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Times.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "1st is on joint sort system which basically just has an exact computer exact results, sorts them afterwards and gives them back to the user.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we have an exact incomplete.",
                    "label": 0
                },
                {
                    "sent": "Took a dropper.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Later and of course we have our our approximately approach.",
                    "label": 0
                },
                {
                    "sent": "The ranking function.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Random scores for triple pen and bindings.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And combines them so combine some as an SM application as a summation, OK?",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now I wait for time reasons.",
                    "label": 0
                },
                {
                    "sent": "Just go into the first benchmark results.",
                    "label": 0
                },
                {
                    "sent": "So for the effect.",
                    "label": 0
                },
                {
                    "sent": "Sorry for the efficient efficiency.",
                    "label": 0
                },
                {
                    "sent": "You can see the three systems seaso in green is our system, rat is the exact OK approach influenced join sort approach.",
                    "label": 0
                },
                {
                    "sent": "The threshold Theta is basically an error threshold, so that is basically the probability for the for the top K chest to decide whether or not to throw away a partial binding.",
                    "label": 0
                },
                {
                    "sent": "So as you can see, we're actually doing quite well, so we're actually saving inputs here, even for very low, very low thresh.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Notes and overall our system consumes 25% less inputs than the exact OK approach and 30% less than the joins with the sort approach, right?",
                    "label": 0
                },
                {
                    "sent": "So mainly the main reason for this is of course the binding.",
                    "label": 0
                },
                {
                    "sent": "Of course the pruning of the partial bindings and mainly so the main trick here is to save, join, attend.",
                    "label": 0
                },
                {
                    "sent": "So when you have a tree and you prune early in this tree, a partial binding, then of course this leads to less join attempts, thereby leading to less inputs being being materialized in the higher ordered tree.",
                    "label": 0
                },
                {
                    "sent": "So in higher order joins right?",
                    "label": 0
                },
                {
                    "sent": "So this is the main reason for this here, right?",
                    "label": 0
                },
                {
                    "sent": "And as you can see here we have a slight increase in inputs preserved in this, mainly the cause.",
                    "label": 0
                },
                {
                    "sent": "Here we actually pruned too much, so here we actually pruned also partial bindings that would have contributed to except to.",
                    "label": 0
                },
                {
                    "sent": "To talk a result, however, since we prove them, we actually had to materialize even more bindings.",
                    "label": 0
                },
                {
                    "sent": "So that was basically the reason here.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of course this inputs safety.",
                    "label": 0
                },
                {
                    "sent": "It translates in time savings.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Regarding the effectiveness.",
                    "label": 0
                },
                {
                    "sent": "So here we have just our approximate top K approach with regard to three different distributions for triple scores, so a uniform, sorry uniform and normal in an exponential distribution.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So here we have again our threshold and the most important thing to see is actually so for a small tower so for us.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our threshold we actually produce in very high quality results, so in between 8 point.",
                    "label": 0
                },
                {
                    "sent": "So sorry open eight and oh .95, which is quite high quality.",
                    "label": 0
                },
                {
                    "sent": "And the second important thing to note is that these curves.",
                    "label": 0
                },
                {
                    "sent": "Still they have different school distributions, but they still behave similarly.",
                    "label": 0
                },
                {
                    "sent": "Therefore, we could learn good distributions for all of these guys, which is considering our simplifying assumptions, right partners?",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So considering the time input saved and the precision at these so that costs basically in terms of precision, you can give a quite nice threshold, quite nice tradeoff.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so I have to have.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Include.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I showed you basically an approximate OK join approach, which we think is well suited for web data and we're queries.",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We extended a well known framework for this.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And our very sorry our relation resides could achieve cognize 65% while still maintaining high precision.",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you for your time and attention.",
                    "label": 0
                }
            ]
        }
    }
}