{
    "id": "36pifisxvomnx4bitwujb6ingrycjiij",
    "title": "Partitioning Well-Clustered Graphs: Spectral Clustering Works!",
    "info": {
        "author": [
            "Luca Zanetti, Department of Computer Science, University of Bristol"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_zanetti_spectral_clustering/",
    "segmentation": [
        [
            "OK, so this is joint work with Richard Pangle and how soon."
        ],
        [
            "And we will work with the normalized Laplacian matrix of our graph G with eigenvalues Lambda one to London, and our goal is to partition the graph in K clusters that are better connected on the inside then towards the outside.",
            "Popular approach to solve this problem is spectral clustering.",
            "Let me remind you I outworks.",
            "First we embed our graph are in a low dimensional Euclidean space.",
            "This embedding is computing using the bottom K eigenvectors of the normalized Laplacian matrix.",
            "Then we apply our favorite convince algorithms on this embedded point, and we return a partition of the graph according to the output of Kimmins and spectral clustering.",
            "Works really well, at least in practice.",
            "But"
        ],
        [
            "Can we analyze this framework also theoretically?",
            "And they won't.",
            "Worker is enough tempt to answer this question."
        ],
        [
            "To give some provable guarantees on the partition output by spectral clustering, we first needed to define a measure of quality of the partition.",
            "From this reason, let me introduce the notion of conductance.",
            "Given the subset of vertices S, its conductance is just the number of edges going from asset to the rest of the graph over the volume of this subset.",
            "We can then define the expansion constant, the keyway expansion constant of the graph as the minimum overall keyway partition of the vertex set of the maximum conductance of a set in the partition, and we will call a partition achieving the keyway expansion constant in optimal partition.",
            "Let me remark that often, instead of the Kiwis pension constant denormalize the notion of normalized cat is used instead.",
            "But actually these donations are tightly related and our work older.",
            "Our result, although also for the normalized cut, we then need to assume a cluster ability condition on the graph.",
            "We need to assume that the graph is way cluster and with this I mean that if we partition it in.",
            "Key plus one subset, then at least one of these subsets will have a conductance that is much bigger than the Kiwi expansion of the graph.",
            "This assumption can be formally stated as a lower bound on a certain parameter upsilon defined as the key plus one eigenvalue over the keyway.",
            "Expansion of the rough, but actually also a sharp drop in the two consecutive eigenvalues will do."
        ],
        [
            "And exploiting this cluster ability assumption we can show essentially 2 interesting facts about the spectral embedding.",
            "First, we can show that there are K approximated centers such that every cluster in the spectral embedding is concentrated around one of these key points.",
            "Second, we can show that.",
            "The average distance between points belonging to different clusters is fairly large."
        ],
        [
            "Combining this treasure, we can show that spectral clustering apply to this family of graphs, actually worse in the sense that it returns a partition that is close to the optimal one.",
            "An.",
            "With this I mean that for every cluster in the optimal partition there is a cluster output by spectral clustering that misclassify only a small fraction of.",
            "Vertices and how small this fraction can be.",
            "This depends essentially on our parameter epsilon.",
            "Moreover, notice that if K is fairly larger than computing all the bottom K again vectors is expensive.",
            "For this reason, we also developed faster algorithm that runs in nearly linear time and exploits it.",
            "Colonel embedding with approximate nearest neighbor data structure.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is joint work with Richard Pangle and how soon.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we will work with the normalized Laplacian matrix of our graph G with eigenvalues Lambda one to London, and our goal is to partition the graph in K clusters that are better connected on the inside then towards the outside.",
                    "label": 1
                },
                {
                    "sent": "Popular approach to solve this problem is spectral clustering.",
                    "label": 0
                },
                {
                    "sent": "Let me remind you I outworks.",
                    "label": 0
                },
                {
                    "sent": "First we embed our graph are in a low dimensional Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "This embedding is computing using the bottom K eigenvectors of the normalized Laplacian matrix.",
                    "label": 0
                },
                {
                    "sent": "Then we apply our favorite convince algorithms on this embedded point, and we return a partition of the graph according to the output of Kimmins and spectral clustering.",
                    "label": 0
                },
                {
                    "sent": "Works really well, at least in practice.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Can we analyze this framework also theoretically?",
                    "label": 1
                },
                {
                    "sent": "And they won't.",
                    "label": 0
                },
                {
                    "sent": "Worker is enough tempt to answer this question.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To give some provable guarantees on the partition output by spectral clustering, we first needed to define a measure of quality of the partition.",
                    "label": 0
                },
                {
                    "sent": "From this reason, let me introduce the notion of conductance.",
                    "label": 0
                },
                {
                    "sent": "Given the subset of vertices S, its conductance is just the number of edges going from asset to the rest of the graph over the volume of this subset.",
                    "label": 0
                },
                {
                    "sent": "We can then define the expansion constant, the keyway expansion constant of the graph as the minimum overall keyway partition of the vertex set of the maximum conductance of a set in the partition, and we will call a partition achieving the keyway expansion constant in optimal partition.",
                    "label": 1
                },
                {
                    "sent": "Let me remark that often, instead of the Kiwis pension constant denormalize the notion of normalized cat is used instead.",
                    "label": 0
                },
                {
                    "sent": "But actually these donations are tightly related and our work older.",
                    "label": 0
                },
                {
                    "sent": "Our result, although also for the normalized cut, we then need to assume a cluster ability condition on the graph.",
                    "label": 0
                },
                {
                    "sent": "We need to assume that the graph is way cluster and with this I mean that if we partition it in.",
                    "label": 1
                },
                {
                    "sent": "Key plus one subset, then at least one of these subsets will have a conductance that is much bigger than the Kiwi expansion of the graph.",
                    "label": 1
                },
                {
                    "sent": "This assumption can be formally stated as a lower bound on a certain parameter upsilon defined as the key plus one eigenvalue over the keyway.",
                    "label": 0
                },
                {
                    "sent": "Expansion of the rough, but actually also a sharp drop in the two consecutive eigenvalues will do.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And exploiting this cluster ability assumption we can show essentially 2 interesting facts about the spectral embedding.",
                    "label": 0
                },
                {
                    "sent": "First, we can show that there are K approximated centers such that every cluster in the spectral embedding is concentrated around one of these key points.",
                    "label": 0
                },
                {
                    "sent": "Second, we can show that.",
                    "label": 0
                },
                {
                    "sent": "The average distance between points belonging to different clusters is fairly large.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Combining this treasure, we can show that spectral clustering apply to this family of graphs, actually worse in the sense that it returns a partition that is close to the optimal one.",
                    "label": 1
                },
                {
                    "sent": "An.",
                    "label": 0
                },
                {
                    "sent": "With this I mean that for every cluster in the optimal partition there is a cluster output by spectral clustering that misclassify only a small fraction of.",
                    "label": 1
                },
                {
                    "sent": "Vertices and how small this fraction can be.",
                    "label": 0
                },
                {
                    "sent": "This depends essentially on our parameter epsilon.",
                    "label": 0
                },
                {
                    "sent": "Moreover, notice that if K is fairly larger than computing all the bottom K again vectors is expensive.",
                    "label": 0
                },
                {
                    "sent": "For this reason, we also developed faster algorithm that runs in nearly linear time and exploits it.",
                    "label": 0
                },
                {
                    "sent": "Colonel embedding with approximate nearest neighbor data structure.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}