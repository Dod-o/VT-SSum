{
    "id": "twisg7zyppqg6mcdkcinia37gmwbutbm",
    "title": "Impact analysis of data placement strategies on query efforts in distributed RDF stores",
    "info": {
        "author": [
            "Daniel Janke, Institute for Web Science and Technologies (WeST), University of Koblenz-Landau"
        ],
        "published": "Nov. 22, 2018",
        "recorded": "October 2018",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2018_janke_impact_distributed_rdf/",
    "segmentation": [
        [
            "So my setting is that."
        ],
        [
            "We are headed breeding with distributed RDF stores and distributed after the graph is distributed among several compute nodes.",
            "And to illustrate it here, I have distributed setting of two compute nodes and I've distributed a graph that basically describes 2 research institutes, namely gave US interests and the employees and some information about their employees like their names and who they know.",
            "And this graphic I if we presented them as a graph.",
            "But actually what the compute node stores are set of triples.",
            "So."
        ],
        [
            "Now people usually ask why do you use distributed at after some not centralized IDF doors and I will usually answer this query by this question by giving 2 examples.",
            "First one is Wikipedia, Wikipedia, wiki data.",
            "They use a data set of 5 billion triples and they decided to store it in a distributed place cards because they answer the query faster and they treat for higher availability with it.",
            "Another example was in 2010 and BBC decided.",
            "To use the distributed crafty be cause they expected one up to 1,000,000 spark queries per day and with this distributed settings they were also able to execute the queries faster and achieve high availability."
        ],
        [
            "But this distributed setting also creates several changes.",
            "These there are many challenges, like for instance how do you distribute the data?",
            "How do you distribute?",
            "How do you create the distributed data?",
            "Or how to evaluate these distributed RDF stores?",
            "And in my talk I will focus."
        ],
        [
            "The data placement strategy and the evaluation."
        ],
        [
            "Before I continue with my talk, I first introduced some terminology when distributing the data will basically assign triples to different compute nodes.",
            "In this assignment I call a graph cover in.",
            "If you're familiar with distributed databases, there it's called to sharding.",
            "And the set of triples that are assigned to one specific compute node.",
            "I call a graph chunk and in databases these are called charts."
        ],
        [
            "When I searched the literature, I could figure out that there are basically 3 common types of data pattern strategies that are used.",
            "The first one is a hitch cover in a hash cover.",
            "You basically computes a hash on trip on the subject of a triple compute module with the number of compute nodes you have and that will start in the compute node to be trouble, assign the specific triple.",
            "And but this hash cover has a disadvantage that more or less randomly distributes the data among different compute nodes, and in order to increase the locality of the data on the specific compute nodes, improvement was suggested, which is called hierarchal cover.",
            "And instead of computing the hash of the complete hash and the computer UI, they only computed on some common prefixes so that data with the with triples with the same.",
            "You should be a play located on the same compute node.",
            "A different type of approach that I found come from the graph partitioning area and there the minimal edge cut cover is usually used in the minimum edge cut cover.",
            "You assign vertices to different compute nodes and this assignment is done in a way that's a number of cut edges between the different politicians.",
            "They said used as bad at the same time they tried to keep the number of vertices assigned to the different compute nodes balanced."
        ],
        [
            "These different cap cover strategies have influenced the query performance and 1st we thought of how does this influence in the query performance look like in the first type of influence that we could identify, we called horizontal containment to illustrate what this means, I've given here at the bottom the craft the initial data distribution that are there in the first slide and I will now want to process this query.",
            "This query basically asks which our research institutes exist.",
            "And what are the names of the employees and in the table in the middle gives an overview about the results of this type of this query.",
            "And horizontal containment means now that all triples that are required to create one of these results should be located on the same compute node.",
            "I indicated it here with the different colors.",
            "Second type of how the data presents artistry could influence the query performance."
        ],
        [
            "We call vertical parallelization in this idea is that if given a perfect horizontal containment, then you could speed up the query.",
            "If you distribute the processing of the different query results in parallel over the different compute nodes.",
            "Now the question came up for us, how is the influence of the different graphical strategies evaluate it in the literature?"
        ],
        [
            "And when I survey the literature, I found out that there are one frequent type of.",
            "Valuations is that you evaluate completely different at F stores as a whole.",
            "And to this is problematic, and if you can, you could imagine if you want to figure out which will allows cars to drive faster, and then they take two different cars with two different wheels and then they'll check which of these cars is running faster.",
            "But you can get an idea how fast the car is, but it's difficult to say whether the measured speed of the cars relies on the actual wheels or not, just because some other factors like a different engine.",
            "A different type of evaluation that I found was."
        ],
        [
            "For instance, if they used batch processing frameworks like a Patriot group, they just typed out several different graph cover strategies and then evaluated the query performance.",
            "Basically, for if you say you want if they wanted to use a picture, dupes is defined strategy, but in Hadoop when you want to transfer data between different computers or you want to combine data from different compute nodes, you will need to have some network traffic.",
            "And in these frameworks these network traffic usually relies.",
            "Also, on several disk iOS that you have and this this arkhaios make it questionable whether the observed performance is also applicable for distributed, after which the compute nodes can directly communicate with each other.",
            "Two over."
        ],
        [
            "With limitations, we propose a novella evaluation strategy, and our idea is that everything is fixed except the graph cover strategies that we will verify, and in order to make evaluations that are reliable to real distributed RDF store, we have developed distributed RDF store that in which you can exchange the cap cover strategy arbitrarily.",
            "And Additionally, if you feel some data set and queries and to introduce new evaluation measures to heaven benchmark that can be easily used."
        ],
        [
            "As data set in our experiments, we used to 1 billion triples, a subset of a billion tuple change data set from 2014, and the queries be used with generated with plot and the generation process considered different query characteristics.",
            "For instance, we used two or a triple patterns and these triple patterns were joint either in a path shaped verification or in a Starship fashion, and the number of triples with which these topic pattern matched were 1,000,000 or 10,000,000 triples."
        ],
        [
            "We've distributed after that we do veloped we called call and it is open source so everyone can use it and it basically has a master slave architecture.",
            "When you load the data sets, the masterkey to cover and sends the individual chunks to the different compute nodes that stored in their local triple indices, and when a query comes, the Masters responsible for coordinating the query execution and each of these slaves execute the part of the queries which is assigned to them on their local triple indices and during preprocessing.",
            "They are also able to communicate the intermediate results directly with each other."
        ],
        [
            "In order to get insights in the query processing, we first measure the overall query performance and for this we have users query execution time in order to measure the horizontal containment, we decided to measure the number of packets that are transferred between these compute nodes and one packet basically is a.",
            "Is a package of 100 intermediate results.",
            "In order to measure the vertical parallelization, it become more complex, because here we decided to combine the packet transfer with the workload imbalance and in order to measure the workload impairments, we count how many drunk comparisons are performed on the different compute nodes, and then we perform the Gini coefficient on it.",
            "And whenever we have high workload imbalance, the vertical parallelization is low.",
            "But when we have a low workload imbalance, it depends on the picture transfer.",
            "Whether we have a higher vertical parallelization or low to medium vertical parallelization."
        ],
        [
            "In our experimental setup, we compare the hash cover, their Uncle hash, covering the minimum edge cut cover, and as computational environment.",
            "We run the master on a compute node with four cores and 64 gigabyte of main memory, and in our experiment setting we used 1020 or 40 slaves, and each of these slaves source equipped with.",
            "1.2 gigabyte main memories that were connected with one capital."
        ],
        [
            "Net.",
            "Maybe a valuated now we first had to look at the overall query performed for 10 slaves, and we think the query times were very different.",
            "We represent over here the queer time relative to the hash based cover, and we could observe, for instance, that the minimal edge cut cover in almost all queries had a larger overall query or executed queries slower than the hash based covered.",
            "But we could not see that one specific hash based cover was faster than the others.",
            "In order to identify why the minimal edge cut cover was so slow, we first had to look."
        ],
        [
            "Went to the horizontal containment and there we could observe that the minimum edge cut cover could reduce the number of transferred packages by up to 38% by the difference between the both hash covers was neglectable.",
            "So the the bad performance with minimal extra cover was not relying do on the horizontal containment, so we had to look at."
        ],
        [
            "The workload imbalance there.",
            "We could see that the workload of the minimal edge card cover was much more imbalanced than the one of the hash base covers, and again here the difference between the both hash based covers was neglectable."
        ],
        [
            "If we now combine the workload central containment with the workload in parents, we can see that the minimal edge cut cover as low vertical parallelization where the vertical parallelization of bowl hash and the icon which cover medium.",
            "And this difference be also reflects the overall performance in symmetric cover was slower than both hash based covers.",
            "Then we made experiments."
        ],
        [
            "Scaling up the number of slaves and for scaling up the number of slaves, we kept the number the size of the data set constant, and we then scaled it up to 20 slaves and 40 slaves which reduced the actual.",
            "The actual number of triples that are stored on the different compute nodes an for all graph covers tighter trees.",
            "We could observe the total computational effort for the query was pet among more compute nodes, but these distribution of the query workload or more unbalanced than in the case.",
            "There of the 10 slave situation.",
            "Also we have what we could observe for a higher number of transferred packets between.",
            "The water is probably relying to due to the smaller size of the datasets that are stored on the different compute nodes.",
            "I and all we could observe that the vertical parallelization decreased if we scaled up the number of slave.",
            "And to give an impression how it looked like, I've preventia over here the change of the performance of the minimal edge cover when we scaled up the number of slaves will be caught up surf, then when we scaled up from 10 to 20 slave we could have the query performance improved, But when we scaled from 20 to 40 slave the overhead of the network communication became so large that the performance started to drop."
        ],
        [
            "In order to conclude, we have developed an evaluation methodology for different cover strategies and what I've recognized during this conference.",
            "When I talk with other people that it is actually already used by other researchers and they found it quite easy to extend call by new graph cover strategies in the valuated, valuate them with them and doing our valuation we had expected and surprisingly sales expected results were that's a minimal edge cut cover, has a better horizontal containment in the hash based cover.",
            "And that when we scaled up, the number of flayed, the horizontal containment became Earth.",
            "The piling for us both that the hash based cover had a better overall performance in the minimal edge cover and that the proposed extension that was realized in the helical hash cover.",
            "This almost have no effect on the query performance.",
            "Another surprising insight for us was that the workload impairments increased when we also increased the number of slaves."
        ],
        [
            "I want to thank you for your attention.",
            "Are there any questions?",
            "Thank you for the representation.",
            "I have the question regarding the evaluation.",
            "You use splodge version aerated Federated query yes and you generate Starship Curry and pass ship queries, yes.",
            "But in some limitation of splits you generate only pass Curry.",
            "So you turns a code or.",
            "Pardon, I didn't dump limitation of time.",
            "Remember example imitation of splodge?",
            "You can generate only pass ship queries.",
            "I only I extended prohibit that we were also able to create Starship queries.",
            "Add code is already a bit of the extension.",
            "Pardon can can I use yours extension of your cut?",
            "It's about I will put it online.",
            "OK OK thank you.",
            "Yeah, thanks for your talk.",
            "My question is more about did you measure also the loading and the partitioning preprocessing time on your evaluation steps?",
            "And these also is quite OK when you have a benchmarking, but when you want to have a daily use of your framework for specific single query that may play a role on the execution time I guess and the second one is how do you cope with IO and network overhead?",
            "And also is this fault tolerant because when we deal with large data?",
            "Basically, when one of the node fails, you may lose the whole data.",
            "Yes, yes, first I want to cover your second question.",
            "We didn't have a look at failure of nodes.",
            "And we also did not.",
            "We investigated some replication approach that replicate some data, but due to the high number of duplicate query intermediate results that were produced, the performance worth worth, then the original version of the data.",
            "But the strategy for handling her failure note would be to investigate in the future.",
            "And your first question, could you repeat it?",
            "Please?",
            "Yeah, I I. I the question was for loading time and the preprocessing.",
            "Like you have to apply this partition strategy to the different nodes on the 1st fly on the first shot and then basically you run your queries over the engine right?",
            "Yeah this is pretty fine when you have the benchmarking, but let's say if some people want the user framework for just normal query data and they have a let's say larger data set, let's say.",
            "36 billions of triple or whatever.",
            "And if this loading time, the preprocessing happens every time 1 user wants to query data.",
            "Did you also thought to keep this like first spread the data over the network and basically provide an API which can?",
            "You can also query single query not on the benchmark stage.",
            "So we measure the loading time of the different datasets.",
            "So how how long does it take to partition the graph and loading the different compute nodes they are contained in the Journal, but since it was not the focus, we did not try to optimize it to have a very short execution times.",
            "And querying only one specific.",
            "Slave, we didn't focus on it, so you just after query to the master node and then he would distribute the query but only one slave would apply to this single query.",
            "Thank you.",
            "I'm really interested in your work because I'm also working the same thing.",
            "Thank you, thank you.",
            "So in your evaluation, hash based uh covers performs the best.",
            "I mean the better from the others, and what I see the reason is it's generated less workload or what is the specific reason for them after this generated further evaluations and we figured out that actually the workload of the individual query depends whether it's more important reference weather data partitioning, whether the whole central containment.",
            "Or the workload imbalances, more as a stronger impact on the query performance.",
            "If you have in our setting we had created with a large with a higher query workload and for them the parallelization among different compute nodes or the workload imbalance is a much more important factor than the network traffic.",
            "But if you have a just a short query that would only flight, I'm sure only metrics with very few triples, then the network traffic becomes their kids did universe ticket Y hash based covers?",
            "Generate less workload.",
            "What was the specific reason that it generates less workload is compared to other approach?",
            "It does not produce less workload.",
            "OK, if the workload was the same independent of the data placement strategies that we used, but the performance gain was basically.",
            "Cost, but the overall the total workload was more paralyzed among all compute nodes in the minimal effort setting we had a few compute nodes which were had much more work to do than most other compute nodes in the hash based almost all compute nodes had the same amount of work to do.",
            "And this is due to the random distribution that has introduced."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So my setting is that.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We are headed breeding with distributed RDF stores and distributed after the graph is distributed among several compute nodes.",
                    "label": 1
                },
                {
                    "sent": "And to illustrate it here, I have distributed setting of two compute nodes and I've distributed a graph that basically describes 2 research institutes, namely gave US interests and the employees and some information about their employees like their names and who they know.",
                    "label": 0
                },
                {
                    "sent": "And this graphic I if we presented them as a graph.",
                    "label": 0
                },
                {
                    "sent": "But actually what the compute node stores are set of triples.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now people usually ask why do you use distributed at after some not centralized IDF doors and I will usually answer this query by this question by giving 2 examples.",
                    "label": 0
                },
                {
                    "sent": "First one is Wikipedia, Wikipedia, wiki data.",
                    "label": 0
                },
                {
                    "sent": "They use a data set of 5 billion triples and they decided to store it in a distributed place cards because they answer the query faster and they treat for higher availability with it.",
                    "label": 1
                },
                {
                    "sent": "Another example was in 2010 and BBC decided.",
                    "label": 1
                },
                {
                    "sent": "To use the distributed crafty be cause they expected one up to 1,000,000 spark queries per day and with this distributed settings they were also able to execute the queries faster and achieve high availability.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But this distributed setting also creates several changes.",
                    "label": 0
                },
                {
                    "sent": "These there are many challenges, like for instance how do you distribute the data?",
                    "label": 1
                },
                {
                    "sent": "How do you distribute?",
                    "label": 1
                },
                {
                    "sent": "How do you create the distributed data?",
                    "label": 0
                },
                {
                    "sent": "Or how to evaluate these distributed RDF stores?",
                    "label": 1
                },
                {
                    "sent": "And in my talk I will focus.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The data placement strategy and the evaluation.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Before I continue with my talk, I first introduced some terminology when distributing the data will basically assign triples to different compute nodes.",
                    "label": 0
                },
                {
                    "sent": "In this assignment I call a graph cover in.",
                    "label": 1
                },
                {
                    "sent": "If you're familiar with distributed databases, there it's called to sharding.",
                    "label": 1
                },
                {
                    "sent": "And the set of triples that are assigned to one specific compute node.",
                    "label": 1
                },
                {
                    "sent": "I call a graph chunk and in databases these are called charts.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "When I searched the literature, I could figure out that there are basically 3 common types of data pattern strategies that are used.",
                    "label": 1
                },
                {
                    "sent": "The first one is a hitch cover in a hash cover.",
                    "label": 1
                },
                {
                    "sent": "You basically computes a hash on trip on the subject of a triple compute module with the number of compute nodes you have and that will start in the compute node to be trouble, assign the specific triple.",
                    "label": 0
                },
                {
                    "sent": "And but this hash cover has a disadvantage that more or less randomly distributes the data among different compute nodes, and in order to increase the locality of the data on the specific compute nodes, improvement was suggested, which is called hierarchal cover.",
                    "label": 0
                },
                {
                    "sent": "And instead of computing the hash of the complete hash and the computer UI, they only computed on some common prefixes so that data with the with triples with the same.",
                    "label": 0
                },
                {
                    "sent": "You should be a play located on the same compute node.",
                    "label": 0
                },
                {
                    "sent": "A different type of approach that I found come from the graph partitioning area and there the minimal edge cut cover is usually used in the minimum edge cut cover.",
                    "label": 0
                },
                {
                    "sent": "You assign vertices to different compute nodes and this assignment is done in a way that's a number of cut edges between the different politicians.",
                    "label": 1
                },
                {
                    "sent": "They said used as bad at the same time they tried to keep the number of vertices assigned to the different compute nodes balanced.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These different cap cover strategies have influenced the query performance and 1st we thought of how does this influence in the query performance look like in the first type of influence that we could identify, we called horizontal containment to illustrate what this means, I've given here at the bottom the craft the initial data distribution that are there in the first slide and I will now want to process this query.",
                    "label": 0
                },
                {
                    "sent": "This query basically asks which our research institutes exist.",
                    "label": 0
                },
                {
                    "sent": "And what are the names of the employees and in the table in the middle gives an overview about the results of this type of this query.",
                    "label": 0
                },
                {
                    "sent": "And horizontal containment means now that all triples that are required to create one of these results should be located on the same compute node.",
                    "label": 0
                },
                {
                    "sent": "I indicated it here with the different colors.",
                    "label": 0
                },
                {
                    "sent": "Second type of how the data presents artistry could influence the query performance.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We call vertical parallelization in this idea is that if given a perfect horizontal containment, then you could speed up the query.",
                    "label": 0
                },
                {
                    "sent": "If you distribute the processing of the different query results in parallel over the different compute nodes.",
                    "label": 0
                },
                {
                    "sent": "Now the question came up for us, how is the influence of the different graphical strategies evaluate it in the literature?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And when I survey the literature, I found out that there are one frequent type of.",
                    "label": 0
                },
                {
                    "sent": "Valuations is that you evaluate completely different at F stores as a whole.",
                    "label": 0
                },
                {
                    "sent": "And to this is problematic, and if you can, you could imagine if you want to figure out which will allows cars to drive faster, and then they take two different cars with two different wheels and then they'll check which of these cars is running faster.",
                    "label": 0
                },
                {
                    "sent": "But you can get an idea how fast the car is, but it's difficult to say whether the measured speed of the cars relies on the actual wheels or not, just because some other factors like a different engine.",
                    "label": 0
                },
                {
                    "sent": "A different type of evaluation that I found was.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For instance, if they used batch processing frameworks like a Patriot group, they just typed out several different graph cover strategies and then evaluated the query performance.",
                    "label": 1
                },
                {
                    "sent": "Basically, for if you say you want if they wanted to use a picture, dupes is defined strategy, but in Hadoop when you want to transfer data between different computers or you want to combine data from different compute nodes, you will need to have some network traffic.",
                    "label": 0
                },
                {
                    "sent": "And in these frameworks these network traffic usually relies.",
                    "label": 0
                },
                {
                    "sent": "Also, on several disk iOS that you have and this this arkhaios make it questionable whether the observed performance is also applicable for distributed, after which the compute nodes can directly communicate with each other.",
                    "label": 0
                },
                {
                    "sent": "Two over.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With limitations, we propose a novella evaluation strategy, and our idea is that everything is fixed except the graph cover strategies that we will verify, and in order to make evaluations that are reliable to real distributed RDF store, we have developed distributed RDF store that in which you can exchange the cap cover strategy arbitrarily.",
                    "label": 0
                },
                {
                    "sent": "And Additionally, if you feel some data set and queries and to introduce new evaluation measures to heaven benchmark that can be easily used.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As data set in our experiments, we used to 1 billion triples, a subset of a billion tuple change data set from 2014, and the queries be used with generated with plot and the generation process considered different query characteristics.",
                    "label": 0
                },
                {
                    "sent": "For instance, we used two or a triple patterns and these triple patterns were joint either in a path shaped verification or in a Starship fashion, and the number of triples with which these topic pattern matched were 1,000,000 or 10,000,000 triples.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We've distributed after that we do veloped we called call and it is open source so everyone can use it and it basically has a master slave architecture.",
                    "label": 0
                },
                {
                    "sent": "When you load the data sets, the masterkey to cover and sends the individual chunks to the different compute nodes that stored in their local triple indices, and when a query comes, the Masters responsible for coordinating the query execution and each of these slaves execute the part of the queries which is assigned to them on their local triple indices and during preprocessing.",
                    "label": 1
                },
                {
                    "sent": "They are also able to communicate the intermediate results directly with each other.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In order to get insights in the query processing, we first measure the overall query performance and for this we have users query execution time in order to measure the horizontal containment, we decided to measure the number of packets that are transferred between these compute nodes and one packet basically is a.",
                    "label": 1
                },
                {
                    "sent": "Is a package of 100 intermediate results.",
                    "label": 1
                },
                {
                    "sent": "In order to measure the vertical parallelization, it become more complex, because here we decided to combine the packet transfer with the workload imbalance and in order to measure the workload impairments, we count how many drunk comparisons are performed on the different compute nodes, and then we perform the Gini coefficient on it.",
                    "label": 0
                },
                {
                    "sent": "And whenever we have high workload imbalance, the vertical parallelization is low.",
                    "label": 0
                },
                {
                    "sent": "But when we have a low workload imbalance, it depends on the picture transfer.",
                    "label": 0
                },
                {
                    "sent": "Whether we have a higher vertical parallelization or low to medium vertical parallelization.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In our experimental setup, we compare the hash cover, their Uncle hash, covering the minimum edge cut cover, and as computational environment.",
                    "label": 1
                },
                {
                    "sent": "We run the master on a compute node with four cores and 64 gigabyte of main memory, and in our experiment setting we used 1020 or 40 slaves, and each of these slaves source equipped with.",
                    "label": 0
                },
                {
                    "sent": "1.2 gigabyte main memories that were connected with one capital.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Net.",
                    "label": 0
                },
                {
                    "sent": "Maybe a valuated now we first had to look at the overall query performed for 10 slaves, and we think the query times were very different.",
                    "label": 1
                },
                {
                    "sent": "We represent over here the queer time relative to the hash based cover, and we could observe, for instance, that the minimal edge cut cover in almost all queries had a larger overall query or executed queries slower than the hash based covered.",
                    "label": 0
                },
                {
                    "sent": "But we could not see that one specific hash based cover was faster than the others.",
                    "label": 0
                },
                {
                    "sent": "In order to identify why the minimal edge cut cover was so slow, we first had to look.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Went to the horizontal containment and there we could observe that the minimum edge cut cover could reduce the number of transferred packages by up to 38% by the difference between the both hash covers was neglectable.",
                    "label": 0
                },
                {
                    "sent": "So the the bad performance with minimal extra cover was not relying do on the horizontal containment, so we had to look at.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The workload imbalance there.",
                    "label": 0
                },
                {
                    "sent": "We could see that the workload of the minimal edge card cover was much more imbalanced than the one of the hash base covers, and again here the difference between the both hash based covers was neglectable.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If we now combine the workload central containment with the workload in parents, we can see that the minimal edge cut cover as low vertical parallelization where the vertical parallelization of bowl hash and the icon which cover medium.",
                    "label": 0
                },
                {
                    "sent": "And this difference be also reflects the overall performance in symmetric cover was slower than both hash based covers.",
                    "label": 1
                },
                {
                    "sent": "Then we made experiments.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Scaling up the number of slaves and for scaling up the number of slaves, we kept the number the size of the data set constant, and we then scaled it up to 20 slaves and 40 slaves which reduced the actual.",
                    "label": 1
                },
                {
                    "sent": "The actual number of triples that are stored on the different compute nodes an for all graph covers tighter trees.",
                    "label": 0
                },
                {
                    "sent": "We could observe the total computational effort for the query was pet among more compute nodes, but these distribution of the query workload or more unbalanced than in the case.",
                    "label": 1
                },
                {
                    "sent": "There of the 10 slave situation.",
                    "label": 0
                },
                {
                    "sent": "Also we have what we could observe for a higher number of transferred packets between.",
                    "label": 0
                },
                {
                    "sent": "The water is probably relying to due to the smaller size of the datasets that are stored on the different compute nodes.",
                    "label": 0
                },
                {
                    "sent": "I and all we could observe that the vertical parallelization decreased if we scaled up the number of slave.",
                    "label": 0
                },
                {
                    "sent": "And to give an impression how it looked like, I've preventia over here the change of the performance of the minimal edge cover when we scaled up the number of slaves will be caught up surf, then when we scaled up from 10 to 20 slave we could have the query performance improved, But when we scaled from 20 to 40 slave the overhead of the network communication became so large that the performance started to drop.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In order to conclude, we have developed an evaluation methodology for different cover strategies and what I've recognized during this conference.",
                    "label": 1
                },
                {
                    "sent": "When I talk with other people that it is actually already used by other researchers and they found it quite easy to extend call by new graph cover strategies in the valuated, valuate them with them and doing our valuation we had expected and surprisingly sales expected results were that's a minimal edge cut cover, has a better horizontal containment in the hash based cover.",
                    "label": 1
                },
                {
                    "sent": "And that when we scaled up, the number of flayed, the horizontal containment became Earth.",
                    "label": 1
                },
                {
                    "sent": "The piling for us both that the hash based cover had a better overall performance in the minimal edge cover and that the proposed extension that was realized in the helical hash cover.",
                    "label": 0
                },
                {
                    "sent": "This almost have no effect on the query performance.",
                    "label": 0
                },
                {
                    "sent": "Another surprising insight for us was that the workload impairments increased when we also increased the number of slaves.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I want to thank you for your attention.",
                    "label": 1
                },
                {
                    "sent": "Are there any questions?",
                    "label": 0
                },
                {
                    "sent": "Thank you for the representation.",
                    "label": 0
                },
                {
                    "sent": "I have the question regarding the evaluation.",
                    "label": 0
                },
                {
                    "sent": "You use splodge version aerated Federated query yes and you generate Starship Curry and pass ship queries, yes.",
                    "label": 0
                },
                {
                    "sent": "But in some limitation of splits you generate only pass Curry.",
                    "label": 0
                },
                {
                    "sent": "So you turns a code or.",
                    "label": 0
                },
                {
                    "sent": "Pardon, I didn't dump limitation of time.",
                    "label": 0
                },
                {
                    "sent": "Remember example imitation of splodge?",
                    "label": 0
                },
                {
                    "sent": "You can generate only pass ship queries.",
                    "label": 0
                },
                {
                    "sent": "I only I extended prohibit that we were also able to create Starship queries.",
                    "label": 0
                },
                {
                    "sent": "Add code is already a bit of the extension.",
                    "label": 0
                },
                {
                    "sent": "Pardon can can I use yours extension of your cut?",
                    "label": 0
                },
                {
                    "sent": "It's about I will put it online.",
                    "label": 0
                },
                {
                    "sent": "OK OK thank you.",
                    "label": 0
                },
                {
                    "sent": "Yeah, thanks for your talk.",
                    "label": 0
                },
                {
                    "sent": "My question is more about did you measure also the loading and the partitioning preprocessing time on your evaluation steps?",
                    "label": 0
                },
                {
                    "sent": "And these also is quite OK when you have a benchmarking, but when you want to have a daily use of your framework for specific single query that may play a role on the execution time I guess and the second one is how do you cope with IO and network overhead?",
                    "label": 0
                },
                {
                    "sent": "And also is this fault tolerant because when we deal with large data?",
                    "label": 0
                },
                {
                    "sent": "Basically, when one of the node fails, you may lose the whole data.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes, first I want to cover your second question.",
                    "label": 1
                },
                {
                    "sent": "We didn't have a look at failure of nodes.",
                    "label": 0
                },
                {
                    "sent": "And we also did not.",
                    "label": 0
                },
                {
                    "sent": "We investigated some replication approach that replicate some data, but due to the high number of duplicate query intermediate results that were produced, the performance worth worth, then the original version of the data.",
                    "label": 0
                },
                {
                    "sent": "But the strategy for handling her failure note would be to investigate in the future.",
                    "label": 0
                },
                {
                    "sent": "And your first question, could you repeat it?",
                    "label": 0
                },
                {
                    "sent": "Please?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I I. I the question was for loading time and the preprocessing.",
                    "label": 0
                },
                {
                    "sent": "Like you have to apply this partition strategy to the different nodes on the 1st fly on the first shot and then basically you run your queries over the engine right?",
                    "label": 0
                },
                {
                    "sent": "Yeah this is pretty fine when you have the benchmarking, but let's say if some people want the user framework for just normal query data and they have a let's say larger data set, let's say.",
                    "label": 0
                },
                {
                    "sent": "36 billions of triple or whatever.",
                    "label": 0
                },
                {
                    "sent": "And if this loading time, the preprocessing happens every time 1 user wants to query data.",
                    "label": 0
                },
                {
                    "sent": "Did you also thought to keep this like first spread the data over the network and basically provide an API which can?",
                    "label": 0
                },
                {
                    "sent": "You can also query single query not on the benchmark stage.",
                    "label": 0
                },
                {
                    "sent": "So we measure the loading time of the different datasets.",
                    "label": 0
                },
                {
                    "sent": "So how how long does it take to partition the graph and loading the different compute nodes they are contained in the Journal, but since it was not the focus, we did not try to optimize it to have a very short execution times.",
                    "label": 0
                },
                {
                    "sent": "And querying only one specific.",
                    "label": 0
                },
                {
                    "sent": "Slave, we didn't focus on it, so you just after query to the master node and then he would distribute the query but only one slave would apply to this single query.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "I'm really interested in your work because I'm also working the same thing.",
                    "label": 0
                },
                {
                    "sent": "Thank you, thank you.",
                    "label": 0
                },
                {
                    "sent": "So in your evaluation, hash based uh covers performs the best.",
                    "label": 0
                },
                {
                    "sent": "I mean the better from the others, and what I see the reason is it's generated less workload or what is the specific reason for them after this generated further evaluations and we figured out that actually the workload of the individual query depends whether it's more important reference weather data partitioning, whether the whole central containment.",
                    "label": 0
                },
                {
                    "sent": "Or the workload imbalances, more as a stronger impact on the query performance.",
                    "label": 0
                },
                {
                    "sent": "If you have in our setting we had created with a large with a higher query workload and for them the parallelization among different compute nodes or the workload imbalance is a much more important factor than the network traffic.",
                    "label": 0
                },
                {
                    "sent": "But if you have a just a short query that would only flight, I'm sure only metrics with very few triples, then the network traffic becomes their kids did universe ticket Y hash based covers?",
                    "label": 0
                },
                {
                    "sent": "Generate less workload.",
                    "label": 0
                },
                {
                    "sent": "What was the specific reason that it generates less workload is compared to other approach?",
                    "label": 0
                },
                {
                    "sent": "It does not produce less workload.",
                    "label": 1
                },
                {
                    "sent": "OK, if the workload was the same independent of the data placement strategies that we used, but the performance gain was basically.",
                    "label": 0
                },
                {
                    "sent": "Cost, but the overall the total workload was more paralyzed among all compute nodes in the minimal effort setting we had a few compute nodes which were had much more work to do than most other compute nodes in the hash based almost all compute nodes had the same amount of work to do.",
                    "label": 0
                },
                {
                    "sent": "And this is due to the random distribution that has introduced.",
                    "label": 0
                }
            ]
        }
    }
}