{
    "id": "jealbzrb7gm4ey4nd267ihq3oj6zly5c",
    "title": "Sparse Filtering",
    "info": {
        "author": [
            "Jiquan Ngiam, Stanford University"
        ],
        "published": "Sept. 6, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Preprocessing",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/nips2011_ngiam_sparse/",
    "segmentation": [
        [
            "So a common pipeline and machine learning that we use for classification is one that looks like this.",
            "You first you're labeled training examples, extra some features for them and put them into a classifier.",
            "So it turns out that in this pipeline, what is most important is usually the features that you extract.",
            "So in our work we interested in using unlabeled examples to learn the right features to extract."
        ],
        [
            "So in particular, this area, the area of deep learning and unsupervised feature learning, recently had given us many tools that we can use like sparse.",
            "Are BMS and sparse autoencoders.",
            "However, this these methods often have many hyperparameters to tune and often really hard to get working in practice.",
            "So in this work we tried to address this issues by introducing a new method that we did that sparse filtering and in particular posturing is an algorithm that is really easy to implement, runs really quickly, so it's faster train and it's very few hyperparameters to tune.",
            "On top of that, the objective function is easy to evaluate in its minimal data processing required an.",
            "I really hope that the center garden that's easy enough that you guys can go back home and try today at your application.",
            "So."
        ],
        [
            "Does this algorithm, so hopefully next one minute I tried to explain to you what it does so the digital algorithm is like this.",
            "So concretely, imagine if you have M unlabeled training examples and features that you want to learn.",
            "So for example, the identity matrix on the left.",
            "So for every example and every feature, you computer certain value.",
            "So then these pass filtering objective function is the one that says first.",
            "Let's normalize across the rules.",
            "So treat each row as a vector and map it to the unit bowl.",
            "This has the effect of giving every feature about the same variance, so saying that every feature is about equally important.",
            "So second, what you want to do now is to then normalize across the columns.",
            "So take every column and rapidly netball and this is the effect of giving features or introducing competition between features and finally the cost function is really just a sum of the normalized entries that you computed.",
            "And the way you optimize for this kind of cost functions that you plug it into a off the shelf optimizer such as LB FTS."
        ],
        [
            "So it turns out we're surprising is that an algorithm cost function as simple as this works really well in practice, so we tried this method on audio data, image data, and share some visualization software we learn from the data.",
            "So on the left side you can see first layer features that from the STL data set.",
            "So there's a color features that we learn and on the right side what you see is the 2nd day of features.",
            "That means we figure them two times the exact same thing to learn a different way of features and what you see is that this actually works in even in a second layer to pull together different plus layer features.",
            "Very nicely.",
            "So if you want more insights and details about algorithm, really hope you can come closer and hope to see you there.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So a common pipeline and machine learning that we use for classification is one that looks like this.",
                    "label": 0
                },
                {
                    "sent": "You first you're labeled training examples, extra some features for them and put them into a classifier.",
                    "label": 1
                },
                {
                    "sent": "So it turns out that in this pipeline, what is most important is usually the features that you extract.",
                    "label": 1
                },
                {
                    "sent": "So in our work we interested in using unlabeled examples to learn the right features to extract.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in particular, this area, the area of deep learning and unsupervised feature learning, recently had given us many tools that we can use like sparse.",
                    "label": 0
                },
                {
                    "sent": "Are BMS and sparse autoencoders.",
                    "label": 0
                },
                {
                    "sent": "However, this these methods often have many hyperparameters to tune and often really hard to get working in practice.",
                    "label": 0
                },
                {
                    "sent": "So in this work we tried to address this issues by introducing a new method that we did that sparse filtering and in particular posturing is an algorithm that is really easy to implement, runs really quickly, so it's faster train and it's very few hyperparameters to tune.",
                    "label": 0
                },
                {
                    "sent": "On top of that, the objective function is easy to evaluate in its minimal data processing required an.",
                    "label": 1
                },
                {
                    "sent": "I really hope that the center garden that's easy enough that you guys can go back home and try today at your application.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Does this algorithm, so hopefully next one minute I tried to explain to you what it does so the digital algorithm is like this.",
                    "label": 0
                },
                {
                    "sent": "So concretely, imagine if you have M unlabeled training examples and features that you want to learn.",
                    "label": 0
                },
                {
                    "sent": "So for example, the identity matrix on the left.",
                    "label": 0
                },
                {
                    "sent": "So for every example and every feature, you computer certain value.",
                    "label": 0
                },
                {
                    "sent": "So then these pass filtering objective function is the one that says first.",
                    "label": 1
                },
                {
                    "sent": "Let's normalize across the rules.",
                    "label": 0
                },
                {
                    "sent": "So treat each row as a vector and map it to the unit bowl.",
                    "label": 0
                },
                {
                    "sent": "This has the effect of giving every feature about the same variance, so saying that every feature is about equally important.",
                    "label": 0
                },
                {
                    "sent": "So second, what you want to do now is to then normalize across the columns.",
                    "label": 0
                },
                {
                    "sent": "So take every column and rapidly netball and this is the effect of giving features or introducing competition between features and finally the cost function is really just a sum of the normalized entries that you computed.",
                    "label": 1
                },
                {
                    "sent": "And the way you optimize for this kind of cost functions that you plug it into a off the shelf optimizer such as LB FTS.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it turns out we're surprising is that an algorithm cost function as simple as this works really well in practice, so we tried this method on audio data, image data, and share some visualization software we learn from the data.",
                    "label": 0
                },
                {
                    "sent": "So on the left side you can see first layer features that from the STL data set.",
                    "label": 0
                },
                {
                    "sent": "So there's a color features that we learn and on the right side what you see is the 2nd day of features.",
                    "label": 0
                },
                {
                    "sent": "That means we figure them two times the exact same thing to learn a different way of features and what you see is that this actually works in even in a second layer to pull together different plus layer features.",
                    "label": 0
                },
                {
                    "sent": "Very nicely.",
                    "label": 0
                },
                {
                    "sent": "So if you want more insights and details about algorithm, really hope you can come closer and hope to see you there.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}