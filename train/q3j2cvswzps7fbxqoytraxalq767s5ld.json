{
    "id": "q3j2cvswzps7fbxqoytraxalq767s5ld",
    "title": "Bayesian Inference",
    "info": {
        "author": [
            "Peter Green, Department of Mathematics, University of Bristol"
        ],
        "published": "Oct. 12, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/mlss2011_green_bayesian/",
    "segmentation": [
        [
            "I'm going to be talking about it."
        ],
        [
            "Once an input."
        ],
        [
            "Look at the Bayesian inference."
        ],
        [
            "An inference that is different from data analysis.",
            "Inferences about learning about.",
            "Not just the data you've got, but the data you will have.",
            "Or you might have.",
            "So it inevitably involves modeling, and it involves assumption.",
            "So inference is the process of discovering from the data something about typically about mechanisms that either did or might have caused the data or generated the data.",
            "Or if that's too much, at least mechanisms that might explain the data.",
            "The goals of doing this are quite can be quite varied.",
            "There's not one simple single thing we're trying to do.",
            "We may simply be trying to predict future data in many.",
            "Commercial applications, that's the main thing you want to do I guess.",
            "But in scientific applications it's usually a little more.",
            "We typically want to try and learn something about.",
            "Truth, that's a bit of a overblown word, but something about the scientific truth about scientific laws, something about how society works, whatever it is so.",
            "If you're an applied mathematician, you naturally think of that as an inverse problem instead of going from assumptions towards data in inference, we go in the opposite direction.",
            "We're going from data back towards assumptions and trying to to learn something, and the idea in Bayesian inference is simply that we use probability to do all of that.",
            "OK, we absolutely rigorously stick to the laws of probability, and that's pretty much the only thing we use to conduct all of our inferential processes.",
            "Now I'm going to spend some minutes in these opening slides talking about the implications of this, but the key strength essentially is that all sources of uncertainty are simultaneously and coherently accounted for.",
            "I mean, in the in the old days of small data problems, that wasn't terribly important, 'cause they probably want only was one source of uncertainty, but nowadays we're dealing with complex systems with many different sources of variation, and it's crucial to be able to properly integrate all of those uncertainties and all of that randomness, and the laws of probability make that easy, impossible, and it's about the best game in town for doing that in my opinion.",
            "Is model based and in I think in the machine learning language we would call all these generative models.",
            "These are models that are capable of generating the data.",
            "I'm a further strength of the Bayesian approach is that models are not, of course models have to be assumed and and so on, and our analysis are conditional on the on the truth of those models.",
            "But also we can use Bayesian methods themselves to criticize those models.",
            "So it's like it's internally self criticising which is which is quite important."
        ],
        [
            "OK, so over the three talks today and the 2:00 tomorrow, I'm going to cover various different things.",
            "And I'll tell you a little bit later about the actual program to today's lecture is quite introduction."
        ],
        [
            "Three, but will get on to some more difficult things later.",
            "Everything connects pretty well.",
            "Everything does connect.",
            "The three other themes that at this summer school that particularly connect with my talks are these three.",
            "Here you've been hearing about.",
            "Monte Carlo methods from from Arnaud and he'll continue today.",
            "The Bayesian Nonparametrics theme on Monday and Tuesday is also very relevant and at the end of next week the graphical models theme from Martin Rob Wainwright is also very important.",
            "Now, if they weren't here, I will probably be talking about all of those things in less detail, so I'm going to deliberately try and downplay those particular issues and relying on them.",
            "I'm sure they did it a lot better than I would, but relying on there to cover those."
        ],
        [
            "Those areas.",
            "Now I'm not as you know.",
            "I'm not a machine learner of any description.",
            "So I was a little bit nervous coming here.",
            "Ann and I spent a little time browsing on the web to find out what other people have done.",
            "So and in doing so, I decided to steal two slides from two people.",
            "I admire a lot.",
            "I think they're very highly esteemed both in the statistics and the machine learning community.",
            "'cause I thought they might have an interesting perspective on perhaps the connections between the subjects if they are indeed different, discuss.",
            "And also on the role of Bayesian methods.",
            "In them, so this first one is Michael Jordan.",
            "And you can see what he says here.",
            "He was trying to explain what machine learning was to a statistician.",
            "You so you may disagree.",
            "Of course he says.",
            "But what he says is this.",
            "That's a loose Federation of themes in statistical inference.",
            "Is a focus on prediction and exploratory data analysis.",
            "A focus on computational methodologies an and particularly empirical evaluation.",
            "And sometimes she quaintest and sometimes Bayesian, so that was his his take very briefly on machine learning and statistics."
        ],
        [
            "But my my second here.",
            "Here is Chris Bishop.",
            "And he's promoting ideas of what he calls third generation machine intelligence.",
            "And again the Bayesian framework graphical models on a write up.",
            "There's that as part of his.",
            "He's the key things underpinning what he's regarding is the future of machine intelligence.",
            "So you know I'm having found these slides.",
            "Could course taking the very selectively from those available, but having found these slides.",
            "I felt OK.",
            "I can come and talk to you and we'll."
        ],
        [
            "Will we have some sort of common ground?",
            "So this is my take are on the connection between the two areas.",
            "I think the difference between statistics and machine learning is much more difference of community rather than a difference of the kind of questions we address.",
            "Yeah, the origins of these two areas are in different scientific communities and when they have different traditions and they behave different ways.",
            "For example, you know in statistics we focus much more on.",
            "On referee journals, for example, in machine learning using to have various conferences which are very high status.",
            "So there are various differences in the way the way we work.",
            "I think machine learning has typically been more ambitious than statistics in terms of the reach of areas you try to get into the scale of problems you try to address.",
            "I'm.",
            "And there's certainly in the in the datasets your machine learners typically look at.",
            "There's a lot of structure within observations, and we're interested in exploring that structure and using it to create powerful methodology.",
            "But the typically.",
            "My view of machine learning it's tends to be not much structure between observations.",
            "We're talking typically about a stream of things that are pretty much similar to each other.",
            "There's a focus on prediction, and often things are evaluated with cross validation.",
            "Another essentially ways of evaluating predictive strength in statistics.",
            "I think there's much more emphasis on model building, much more thinking about what was the process that made the data.",
            "Trying to uncover that trying or something.",
            "Yeah, trying on assumptions.",
            "We tend to do things that are reliant on the models.",
            "But we have more of a name of trying, as it were, to understand what what science or society, or whatever he's doing, and less of a name on less of a concentration on, you know.",
            "Through portal producting how?",
            "Running on on huge real time situations.",
            "And we tend to it.",
            "We tend to use models to evaluate, and that's a clear difference.",
            "So I think this is a fascinating distinction, and this we have a lot to learn from each other.",
            "You know, I'm really thrilled that the two areas seem to be converging and.",
            "I. I hope these tutorials will help that a little bit.",
            "So.",
            "Haven't got that out of the way.",
            "I mean, I'm a statistician and my illustrations will be of statistics models and they may not be the same character that you might hear about from somebody else giving lectures of this this kind, but I think that that's a reasonable put, partly because it's what I know about, and partly because I think some of these statistical models are.",
            "I'll show you and talk about actually expose a few more of the different issues.",
            "The fundamental issues in Bayesian inference, then that you could get from the machine learning perspective."
        ],
        [
            "OK, just a little bit on the contrast between Bayesian and frequentist statistics.",
            "Is there actually a lot of different paradigms though?",
            "They're just the two we tend to talk about quite a lot and tend to perhaps contrast.",
            "There are many other things in many other ways of doing inference, but these are the two most most prominent.",
            "Just to give one example of something that's different from both of them, there's a whole theory of inference based solely on likelihoods.",
            "Likelihood inference is not the same as frequentist inference, in fact.",
            "There's a principle called the likelihood principle, which frequentist statisticians do not obey.",
            "But you can do things based solely on likelihood, and it's in some sense closer to the Bayesian view than the frequentist one.",
            "So within statistics we've you know we've had a lot of fun, but also wasted a lot of energy over the years.",
            "Arguing about about paradigms and philosophies and so on.",
            "It's been somewhat distracting sometimes.",
            "Sometimes we've learned from it.",
            "Sometimes it's being destructive.",
            "I think nowadays we're all settling down and being a bit more sensible about this and many, many statisticians will be not prepared to say they're in one camp.",
            "On the other, they would be very happy to to use ideas from both paradigms in different situations."
        ],
        [
            "But if you wanted to contrast them, I suppose this would be a way you could do it.",
            "In Bayesian statistics, we typically don't dream up methods.",
            "I meant to say a little while ago.",
            "Please interrupt at any point, but I wasn't.",
            "I wasn't quite thinking of that.",
            "Yeah, if I say anything is rubbish, please please please interrupt.",
            "Yeah I mean Bayesian stoned.",
            "Invent methods.",
            "Bayesian Bill, Bailey's build models and the models tool.",
            "Tell us what the method should be whereas frequentist are happy to consider methods they dream up from anywhere.",
            "Bayesian inference is a place always conditional on the native we've got.",
            "They don't transcend thing about anything else, whereas in frequentist the frequentist world the emphasis is on giving good answers in repeated use and so on.",
            "So there are some important distinctions here, and the two can be looked at in parallel.",
            "But you know, it's not as if there's a competition."
        ],
        [
            "In the two in which one clearly wins, except perhaps as we come into the sort of the present day.",
            "It's just a few other issues.",
            "I want to emphasize.",
            "There was a time when I think it was difficult to be Bayesian if you had.",
            "Clients, or you know, if you had to end users because there's a lot of needing to defend what you've done, I think those days are rather gone, but perhaps not completely.",
            "One of the strengths of Bayesian inference is that we can go directly and report the inferences we want.",
            "We don't have to.",
            "Twist ourselves into logical, not thinking about null hypothesis and you know trying to express everything.",
            "What's often, at least for beginning students, are very counter intuitive way.",
            "Very key idea in Bayesian inference is borrowing strength.",
            "This idea that one thing is always informative about another.",
            "Everything connects.",
            "If you measure something you're learning not only about the things that directly influence it, but many other things as well.",
            "And we'll see that will see that thing.",
            "We will see that coming out in some of the examples.",
            "I think Bayesian inference really pays off in complex and high dimensional problems in small scale low dimensional problems.",
            "To be honest we can have different philosophies about how we're going to do it, but we end up with pretty much the same answers.",
            "But as problems get bigger and more complex than there are clear differences and Bayesians able to do to do more.",
            "But this one bitter, even even committed Bayesian, will always defend one aspect of the frequentist idea, and that is that we happy typically for do use frequentist methods to evaluate performance.",
            "So this sort of the frequentist consistency of frequentist performance of of statistical Bayesian statistical process processes is a perfectly legitimate thing to think about."
        ],
        [
            "OK, well that's that's the end of my sort of opening.",
            "Motivation trying to get over some of the key ideas and this is the contents list for the remainder of this this tutorial.",
            "Now.",
            "Only arrived last night, so I haven't had a chance to meet any of you yet.",
            "I I first thing I asked when I got to the bar was what are the students like so I don't know anything about you individually, but I know a few things about you in general.",
            "I know you're very bright.",
            "I know that you are very selected to get here.",
            "You beat the competition.",
            "So congratulations, but I also know you're pretty diverse in terms of where you've come from and what you've done before.",
            "So.",
            "That I say that by way of explaining that some of the things I'm going to talk about, particularly today will seem really very basic.",
            "For some of you.",
            "But trust me, I think it's I think they're worth doing, and I suspect there's some of you here who will appreciate evenly the elementary things I talk about.",
            "So.",
            "Let's take a little bit about probability.",
            "I'm not going to say anything more about probability in this one slide, so if you don't know about probability, you really are going to have to go and look it up so."
        ],
        [
            "Or else.",
            "So probability is nothing but common sense reduced to calculus.",
            "Somebody said Laplaca, I think.",
            "Just to reduce your recap, it measures uncertainty on a 01 scale zero and one mean what you think they mean.",
            "And there's only really one property that probably has, and that is if you look at two events, and if they can't both happen, then the probability of one or other is the sum of the two probabilities.",
            "And it's extraordinary.",
            "But that's all you need.",
            "Pretty much all that.",
            "And the same thing generalized to accountable collection of events pretty much generates the whole theory of probability.",
            "So it's remarkably light in terms of axioms.",
            "And of course, when we're dealing with data, typically the refocus probability not on the general theory of events, but we talk particularly about random variables.",
            "So almost all of the probability we use will be talking bout distributions of random variables, discrete or continuous.",
            "Scalar or vector or living in all sorts of different spaces, random variables and their and their distributions."
        ],
        [
            "And there's only one thing we particularly need from probability theory, and that of course is Bayes theorem.",
            "You may think that maybe that's where the word where the name Bayesian analysis comes from.",
            "So here's a really small little problem.",
            "I like puzzles and this this is a version of a puzzle that appeared in in the Guardian newspaper recently.",
            "So these are little little game setters set in a sort of fictitious context, so you really you're allowed to draw 2 balls.",
            "You can't.",
            "You can't see inside the urns.",
            "You know the constitutions of the two, but you don't know which one is which.",
            "And you really need a Red Bull.",
            "OK, and you you choose an urn at random and you pick out a bowl and it's blue.",
            "OK, and you only get one chance left.",
            "So the question we're asking is.",
            "What's your chance of escape or should chance of getting a Red Bull?",
            "And would it be a good idea to Switch earns after the first?",
            "Draw.",
            "OK, this is a homework question so you can start thinking about the answer to that question."
        ],
        [
            "Now, this is obviously a really silly problem.",
            "OK, but the reason I've included it is because it includes many of the main ideas of inference.",
            "We have some data because we've drawn a ball.",
            "We want to make some inference.",
            "One of the things we're interested in is now we've drawn a ball when we know it's blue, which earned.",
            "Do we think we've drawn it from?",
            "OK, that would be an example of an inferential question.",
            "I guess we understand that probably now it's the right hand turn.",
            "Yep.",
            "Prediction what's the next ball going to be?",
            "How likely I are you to get a Red Bull?",
            "And then decision which is the better of the two strategies.",
            "Sticking with the same earn that you chose the first time or or switching.",
            "And the point is, of course, that all of this is a probability question, not a statistic question, but you can see that by solving the probability question, we can do all those inferential things."
        ],
        [
            "And I don't wanna spend very long in the calculations.",
            "You can look at the slides later and it's it's a bit tedious, but we just basically work through the conditional and the joint probabilities.",
            "And it turns out that the the three things we were thinking about other choice of the urn, was it the left turn right?",
            "And you choose what's the first ball drawn and what's the second ball drawn?",
            "And the inference question asked us is asking what is the privacy?",
            "We chose the left hand and given that we've now got the data that we up all these blue and we can work that out using the laws of conditional probability.",
            "And it turns out that the answer is eight 8 / 20 three 1/3."
        ],
        [
            "Roughly, and that's because in this particular case, the variable we're talking about the choice of the earn, only has two different values.",
            "We can do.",
            "We can write down Bayes theorem in particularly neat way.",
            "In that case in terms of odds and the.",
            "The posterior odds that we've got the left hand turn rather than the right given the evidence that the ball is blue, is the prior odds multiplied by the ratio of the likelihoods.",
            "And that's that's a key key thing to remember.",
            "So that solves the inference question.",
            "Is about 2:00 to 1:00.",
            "That we've drawn the left ball.",
            "Now.",
            "I'm not saying frequentist would do this, but there there would be a temptation.",
            "If you follow the rules of frequented statistic to say oh, now we know it's the right hand turn and will proceed as if that's true.",
            "But of course, it's not true.",
            "It's possibly true, and.",
            "One of the key things in uncertain inferences to propagate uncertainty through to other parts of your problem, and we want to make a prediction, and we should propagate through that uncertainty through.",
            "And the laws of probability probability tell us exactly how."
        ],
        [
            "To do that?",
            "So the prediction question is what's the chance of getting?",
            "A red ball, the second time, given that the first ball was blue.",
            "OK, and then again that's a question of working through the probabilities.",
            "And.",
            "I say you can look at the slides later, but it turns out the probability of blue followed by red is eleven 40th's and the probability of blue followed by blue is twelve 40th.",
            "So the ratio is 11 to 12.",
            "So it's like the odds are slightly against you.",
            "It's slightly more likely than not that you're going to visit the firing squad because you didn't get a Red Bull a second time.",
            "OK."
        ],
        [
            "What about the prediction of the decision question?",
            "Well, this is a very easy big decision.",
            "You presumably don't want to go to the firing squad so you will want to try and make that probability of a red ball a bit higher.",
            "What do you think?",
            "Better to stick or to switch?",
            "You seem quite sure we have a show of hands who thinks it's better to switch.",
            "Who think is you forgot about once and once only?",
            "Who thinks better to stick?",
            "Interesting and who's just too confused and OK. Well that's that you work that out if you want.",
            "Please for tomorrow I think you'll be surprised at the answer.",
            "Was a clue.",
            "If you get the answer, and if you're surprised by it, then there's a follow up supplementary question.",
            "Namely, is this the I made up the Constitution?",
            "Then I just wrote some random numbers down.",
            "Did I did I choose it at random?",
            "did I choose?",
            "did I try a lot of different cases before I got that particular answer?",
            "Anyway, there's a few different things there.",
            "It's a bit surprising."
        ],
        [
            "But the main point main point for now is that probability answers the questions.",
            "Now you find this stuff all rather silly and you just want to see the algebra.",
            "Well, this is the algebra of been using.",
            "But for the inference question, what we've been looking at is the conditional distribution of theater.",
            "Given why the parameter given why?",
            "And for the prediction question, what we've been looking at is what's the distribution of the future data White plus given the data, why we worked it out using the laws of probability?",
            "And.",
            "The way we did it was to do that.",
            "But we could also have done that, and it's their algebraically algebraically identical so alternative way to think about how you solve the prediction question is to use the posterior you evaluated 1st and use that as it were to mix over the particular distributions given Theta and you get exactly the same."
        ],
        [
            "So.",
            "The idea of Bayesian statistics is.",
            "Is that magnified is to is we're going to use probability theory consistently to evaluate uncertainty in all sorts of situations where we're interested in dealing with data.",
            "And we're allowed to do this because the Bayesian view is that everything is a random variable.",
            "What every possible variable in your system observed, run, observed, observable, and observable things that you think are parameters.",
            "Things that are states of nature think you know everything hidden.",
            "Visible missing, whatever it is.",
            "There are random variables.",
            "Now, that's not to say that.",
            "Bayesians think they're all the same philosophically.",
            "They don't.",
            "They understand those are different things.",
            "The future is different from the past and blah blah blah.",
            "But they're just treated uniformly for mathematical purposes, so we treat them as random variables.",
            "And it's just quickly, it's worth reminding ourselves that there's different sorts of randomness, and indeed.",
            "The distinctions can be can get quite blurred, so we sometimes distinguish between epistemological randomness, annalia tree randomness.",
            "And the first of those is concerned with lack of knowledge.",
            "Things you don't know.",
            "And the second is to do with things that are inherently random."
        ],
        [
            "So I am indebted to my friend David Spiegelhalter.",
            "I'm going to make a mess of this.",
            "A problem is that euro coins don't have.",
            "Don't have their heads on them, but.",
            "One of the several things wrong with the euro.",
            "OK, I I've jumped ahead to the second step, so.",
            "I'm going to hold up a coin you imagine I hadn't thrown it.",
            "I fold it up.",
            "And I asked the audience what's the chance that this will come up heads.",
            "OK, what's the answer?",
            "Right?",
            "We're not worried about silly things like calling being biased.",
            "And we're not worried about yours not aiming heads.",
            "I then toss the coin, flip it on the back of my hand and I don't reveal it.",
            "I don't look at it or anything.",
            "I need to go to flip that I now say what's the probability that is heads?",
            "OK, so yeah, you're not calling nobody.",
            "Nobody said anything different from half yet, but you're a little bit slower to say so.",
            "Yeah.",
            "Now I look at it OK and I know now.",
            "What's the, what's the chance?",
            "Yeah, OK, I know it.",
            "My chances are now 01.",
            "What are your chances?",
            "OK, so some things change.",
            "Yeah, uncertainties are moved around.",
            "You're still getting 5050, I think.",
            "It may take you longer to think to say so, but you don't.",
            "You still don't have any evidence for one or the other.",
            "But something all the strangers have."
        ],
        [
            "Because it's no longer random.",
            "And what's happened is we've moved from its tail.",
            "By the way, we've moved from something that was purely a tree.",
            "It was determined by something in the future that was genuinely chance to something that.",
            "Is only epistemological, it's just to do with something you don't know.",
            "And you and I think differently about it.",
            "You thought at certain stage in that experiment you had one view and I had another.",
            "So it's in the eyes of the beholder, and so on.",
            "So the uncertainty in there can be of different kinds, but we can use the laws of probability to talk about both kinds and.",
            "The different kinds can be perceived differently by different by different observers.",
            "So Bayesian understand all that, but they're still willing to use probability for both purposes, and they use it uniformly and true."
        ],
        [
            "Everything is random variable.",
            "So the idea is to work throughout with a joint probability distribution you.",
            "You you put together in one huge model, the joint distribution of everything you don't know everything you will know.",
            "Whatever their status, where does that come from?",
            "Well, it's not defined.",
            "It's not.",
            "There's not a right answer is something that's assessed you.",
            "You come to a scientific judgment about it, your since the result has to obey the laws of probability.",
            "You will be guided by the laws of probability in doing that.",
            "And then at some point we'll have some data.",
            "You know?",
            "We looked at the coin or whatever it was.",
            "And we model that process of observation by conditioning this joint distribution on the things we've observed.",
            "And we then use that condition distribution.",
            "For everything else, we want to know for our inference for our.",
            "Predictions for our decisions and so on.",
            "The probability distribution of the hidden variables given given the visible ones.",
            "So it's completely clean.",
            "Or it's a straight forward?",
            "It's philosophically."
        ],
        [
            "But yeah, there's not.",
            "There's no particularly difficult philosophical ideas at that level.",
            "Now I've got quite a long way.",
            "Without mentioning likelihoods and priors, don't think.",
            "And in a sense, likelihoods and priors are a little bit.",
            "Yeah, and they're not really the the front rank of the theory that the main thing we need is.",
            "This is this joint distribution of observables and unobservables.",
            "But typically, of course, that joint distribution comes about by combining a likelihood and prior.",
            "So the standard situation.",
            "But this orphan Illa problem is you gotta parameter vector theater.",
            "You got some some data why you will have some numbers.",
            "And though the two feature and why there may be big vectors, but there there just the two objects we care about and we're going to make inference about theater given why?",
            "And as I've already said, we're going to.",
            "We're going to use the conditional distribution of theater given why to make to make that inference?",
            "And the key thing, of course, is that it's the joint distribution of theater and Y divided by the marginal distribution of Y.",
            "And that's so far as theater concerned is proportion in theater.",
            "So the.",
            "Conditional distribution we want to use for inference is just proportional to the joint distribution that you decided on.",
            "And and yeah, we said we could just start with the joint distribution, but in fact we almost always construct that joint distribution by combining the likelihood in the prior.",
            "So so it is, it is the common the normal thing to do.",
            "So it's constructed from those two things, the that the marginal distribution for the parameter is the prior.",
            "The conditional distribution for the data given the parameter is the likelihood if we're modeling in this direction, that's a generative model and then multiplied to give the joint distribution.",
            "The.",
            "Which is worth, I think I'll just repeat that point that it's the joint distribution of theater and why that's really the key idea.",
            "And in fact, if you started from that point, you don't even need Bayes theorem to do Bayesian statistics, which is slightly scary thought.",
            "Now I'm going to talk quite a bit about prize later.",
            "Where do they come from, but?",
            "Your mind what I've said about the two sorts of uncertainty.",
            "Typically.",
            "Those two factors, the likelihood in the prior, can be quite different in character.",
            "The prior is often entirely subjective.",
            "Now.",
            "Subjective is a bit of a dirty word.",
            "I don't like it because it has that pejorative.",
            "Connotation this is something somehow you've just made up, but it would be much better if we said scientific judgment.",
            "I think it's much better way to think about subject subjectivity.",
            "But the prior is very often based on scientific judgment.",
            "That's not hard, and yet it isn't quantified in a particularly hard way.",
            "Whereas the likelihood may well be something that's open to empirical evaluation.",
            "Yeah, yeah, in a lab you could perhaps create data and actually incest assess distributions directly and numerically."
        ],
        [
            "Now there are many, many positive things that come out of using probability theory consistently, and this is just a very simple example of that, and it's something we use all the time.",
            "What happens when data is acquired sequentially?",
            "OK, so every day you get in you.",
            "Every day you get a new set of weather and you know we we we we want to update beliefs in the light of what we now know and so.",
            "Imagine there's some sort of state of nature theater that is our focus of our inference, and that the different data we acquire are conditionally independent, given that.",
            "In fact then, in that case the the joint distribution is just the product of the.",
            "The prior theater, multiplied by the light hoods for all the individual data, and that's.",
            "Yeah, that's basically lower probability, so we can infer from that that the conditional distribution we want is proportional to that product.",
            "I notice that we can break off the last factor and we see that in fact what we're seeing is that the posterior after end data is is the product or proportional to the product of the posterior after N -- 1 data and the likelihood for the for the for the last observation.",
            "So if you like the prior for the last data point, is the posterior from the step before, so it's very nice sequential.",
            "Sequential coherence and consistency of the distribution as you learn more stuff."
        ],
        [
            "OK so I put I put prior likelihood on the table.",
            "For many people that's where you start.",
            "But I say I was starting the slightly different place.",
            "So is there anything else apart from prior likelihood?",
            "And very importantly, there is.",
            "Yes, yes there is, and it's not always center stage.",
            "And this concerns issues about utility and loss.",
            "So.",
            "Um?",
            "Often we don't need to think very hard about that.",
            "If you're content to display or visualize a posterior distribution, and that's it, if that's the focus of your inference, then you don't need to think about anything else.",
            "But if you wanted to make take decisions or test hypothesis, if you want to do a serious job of estimation, even you have to think about something else.",
            "And that's leads us into areas of decision theory.",
            "So decision there is a big subject bit dry.",
            "I haven't say actually, but we just need a few key ideas from that."
        ],
        [
            "There's a theory of utility which says what do we?",
            "How do we evaluate uncertainty?",
            "It typically tries to evaluate the cost to us as individuals of being uncertain in monetary terms.",
            "So we talk about utility owner pseudo money scale.",
            "Let's try and just formalize this process in a minute.",
            "We'll see.",
            "We'll see some practical consequences in a second.",
            "What's the sequence?",
            "We observed data why we're going to make a decision on that date based on that data.",
            "So if it's based on that data, then our decision is a function of the data.",
            "So D the decision is Delta of Y.",
            "And then we pay a price and the price we pay is a function of the decision we took.",
            "And whatever the true state of nature was.",
            "So that's where loss function is is a function of the decision you took and the true state of nature.",
            "So it allows us to ask the question, how bad is it to decide?",
            "Delta why?",
            "When theater is true.",
            "And the idea we can use then is to try and minimize our losses.",
            "And that gives us a means of deciding things."
        ],
        [
            "Well, let's make this concrete.",
            "The simplest possible thing you could do is decide between two hypothesis about a parameter.",
            "It is supposed to just two.",
            "I've written it as Omega Zero and Omega one, so he's theater in Omega 0.",
            "Or is the screen Omega one?",
            "We're going to use data to decide which of those things is true.",
            "Now we can say we got to take a decision D0 or D1.",
            "These areas, I think theaters in Omega 0.",
            "OK, so it's just a true or false situation.",
            "Nothing could be simpler if you get the right answer.",
            "Fine, OK, everyone's happy, so there's no loss.",
            "If you get making mistake, then there's a loss and the losses might be different in the two cases.",
            "So if you decide DI like 01 if you decide diyan you're wrong, then the price you pay is AI.",
            "AI is a positive number.",
            "OK, so that's the set up and we now observe why.",
            "What do you do?"
        ],
        [
            "What's the what's the?",
            "What was the decision you should take?",
            "Well, if you knew, theater of course is no problem, but you don't know theater only know why.",
            "And a natural natural thing to do.",
            "And indeed it's correct according to sort of metatheory.",
            "Axiomatic theory of utility is to minimize your expected loss.",
            "So nervous you minimize.",
            "L of the theater is your loss, and we integrate that with respect to the posterior distribution.",
            "But you don't know theater, but you do have the posterior distribution.",
            "That's the expected loss, and you do the best you can.",
            "And you can work that out very quickly.",
            "The expected loss if you choose DI is AI times the probability that you're wrong.",
            "That's the posterior probability that euro.",
            "And so this amounts to essentially yes.",
            "Is it possible to give some idea of why that is optimal?",
            "It is.",
            "But you don't, yeah.",
            "Las Yep.",
            "Everything.",
            "Somehow decision theory says you can just choose this path.",
            "Why can you just use the expectation?",
            "Well, I think the simplest way to think about is just as a long run thing you know so.",
            "This is not the only decision you're going to take in your life.",
            "You're going to take lots.",
            "Let's suppose you know when you're a teenager, you learn how to take decisions.",
            "Seems unlikely, but and.",
            "Stick to that process that this is the law.",
            "This is the way of this is the choice you would take.",
            "That would minimize your total loss over your lifetime.",
            "Because of laws of large numbers.",
            "That's the simplest explanation.",
            "Well then you have to.",
            "Then you have to appeal to ideas of exchangeability and noticed you say this one is typical of the rest.",
            "And yeah, but you're quite right.",
            "I mean, if you.",
            "If you knew you're only ever gonna say one decision in your life.",
            "Then it's up to you.",
            "And in particular, you is perfectly rational if you only take one decision in your life to say, well, I can afford to lose a pound, but I can't afford to lose 1000 pounds.",
            "That's a perfectly rational thing to do.",
            "Anyway, the effect of that is that that that minimized expected risk issue tells you to threshold the posterior probability.",
            "It's a very natural answer because if these two potential losses a zero and a one are different in size, so a mistake in One Direction is more important than a mistake or more costly than a mistake in the other.",
            "Then thresholding the probability using something depending on the AA allows you to take a sensible decision.",
            "OK, so that of course that optimal decision is a funk."
        ],
        [
            "And of the data when we come to estimation, the decision is a number or vector.",
            "Is the the value you're guessing for theater.",
            "Let's call it Theta hat.",
            "Ann and common choices is quadratic loss.",
            "Let's say we take the squared difference between what you guessed and what it really was.",
            "And when you when you do the maths, when you do that integration, it turns out the the expected.",
            "The expected losses then simply the if you like the bias, the difference between theater hat and the posterior expectation and plus the variance.",
            "Of the posterior distribution.",
            "That's straightforward.",
            "In the square.",
            "So given that this posterior is something that's fixed today with your decision, the best we can possibly do is choose Theta hat to equal the posterior expectation.",
            "So that's the justification for using posterior expectation as a summary of a posterior distribution.",
            "It holds more generally it holds for in vector case as well.",
            "But it's more important to remember is the only justification for using the posterior mean.",
            "Posterior mean has really no other.",
            "Has no other philosophical meaning except as the thing that minimizes the quadratic loss, and perhaps perhaps, as with the yesterday with the testing situation, perhaps that quadratic symmetric.",
            "Loss function doesn't capture the cost to you of the mistakes you might make."
        ],
        [
            "A very common thing position to find yourself in is that overestimating and underestimating are not equally costly.",
            "OK, so imagine.",
            "If the world's financial mathematicians had behaved.",
            "Without that sort of cartoon, yeah, that sort of thought in mind.",
            "Perhaps losses in One Direction or differently different than losses in another.",
            "So let's suppose overestimation is expensive then.",
            "A loss function like this might be appropriate.",
            "Where tour here is a number between zero and one, and in the case I'm talking about tour will be less than 0.",
            "Less than half.",
            "This is the one thing I got wrong in the slides that were on the web by the way, they corrected in that version, but I got the I got these two things the wrong way around, so if you.",
            "Yeah, I'm assuming will download a corrected version later.",
            "But if you do that, you play that game.",
            "It turns out the posterior expected loss is a particular quantile, sensible quantile.",
            "The 100 tour percentile of the posterior distribution.",
            "So that has the correct effect.",
            "You know if you.",
            "If if overestimation was that, say, three times more expensive than underestimation, then you would choose the lower quartile of your distribution.",
            "So this decision theory idea allows you to properly think about it.",
            "Forces you to probably think about the costs of different kinds of error and and to bring that into your procedures.",
            "And this choice of the quadratic loss is not absolutely inevitable."
        ],
        [
            "I don't really want to say much about frequentist theory except except because of the nice little punch line at the bottom here.",
            "Frequentis also think about about decision theory and this really comes back to your question.",
            "You know they don't think about expectations of loss functions automatically because they wouldn't have any reason to take expectations with respect to the posterior distribution.",
            "What they might think about is taking expectation of the loss.",
            "Under the likelihood.",
            "OK, for each theater, if theater were Noble.",
            "And what in regarding regard why is uncertain, then we could look at the look at the expectation over Y.",
            "And this idea that you could excuse me, you could afford to lose a pound or not a not 1000 pounds that speaks to the idea that what we're trying to do is minimize the maximum loss we could make.",
            "And that's so called Minimax decision rule.",
            "And that's one of the.",
            "One of the driving fundamental principles of much of frequentist theory.",
            "Now there's this idea that you would be stupid to use a decision rule that is.",
            "Beaten by another decision rule, whatever the value of theater.",
            "And then, rather remarkably, with, with a little bit of a caveat about.",
            "Mathematical regularity every admissible rule is actually a Bayes rule.",
            "So in optimal decision theory, this is one point where frequentists are forced to be Bayesian."
        ],
        [
            "OK, I want to say it about priors.",
            "And.",
            "If you go back to the early days of Bayesian statistics.",
            "Then this issue is terribly important.",
            "You know, before we had before we had computers that didn't really cost anything.",
            "It was really important to be able to in order to use, but the Bayesian method is really important about to do the integrals.",
            "And that really ties your hands in in what the forms of the functions could possibly be, and a particular choice for certain.",
            "Many standard likelihood functions a particular choice of prior gives you huge algebraic and computational advantages, so this is the idea of conjugacy.",
            "So prior is conjugate.",
            "If basically it gets updated in a neat way so that the form of the posterior, the normal distribution, or exponential or whatever the form of the distribution doesn't change just the parameters of that distribution get updated.",
            "So for one very simple example, let's take a take the case of the professional distribution for the data.",
            "OK, this is a personal distribution.",
            "Now we could take various possible priors for the parameter theater.",
            "It's a non negative real number, so let's let's try various things.",
            "But if we chose the gamma prior gamma distribution which I've drawn there then and then go through the Bayes.",
            "Go through Bayes rules, we find the posterior distribution is again of exactly the same form.",
            "And what's happened is that the Priors got updated to the posterior simply by tweaking the parameters we started off with Gamma, Alpha, Beta and now we have gamma Alpha plus Y beta plus one."
        ],
        [
            "Now think about now what would happen if you took a second observation.",
            "Again, I told you about sequential acquisition of data while the prior inside the posterior after the first observation is that for the next, so you can see what happens is that we just get the new observations, gets added on, and the beta parameter gets added further on again.",
            "And that's a very nice idea, because it tells us something about how to interpret prior distributions in this case, because you can see that effectively.",
            "What would have eventually is Alpha plus the sum of all the data and beta plus the number of observations.",
            "So that's just like starting from zero in both 0 total observations, 0 number of observations and then.",
            "Where we start is to imagine that there are a total.",
            "There are beater observations and their total is Alpha.",
            "So priors can then be thought of as as like prior data.",
            "It's like data you had before you started observing anything, and that's quite a useful trick if you're trying to elicit a prior from someone who's reluctant to give one.",
            "That's quite a good way of pull out from him or her what the.",
            "Will be kind of uncertainty.",
            "They think about theater radius.",
            "Now.",
            "This used to be a really important principle because you couldn't.",
            "You couldn't do Bayesian statistics except in this sort of simple situation.",
            "It's become like much less important now.",
            "There's still some temptation sometimes to use.",
            "Conditionally conjugate priors.",
            "So in a big big model.",
            "You know at least, conditional on some other parameters.",
            "Maybe the prior going to use is is is conjugate and that sort of thing can make computation little bit little bit neater.",
            "But I would.",
            "I would say you know, really this is history and it's a mistake to let that kind of consideration.",
            "Persuade you to use a prior that doesn't reflect what you really think."
        ],
        [
            "Now we can't say much more about prize without distinguishing two different subspecies of Bayesians, and these are the subjective ones and the objective ones.",
            "Now.",
            "The subjective ones.",
            "A kind of ideally idealist living on a Hill, and the objective ones are the ones who have to get on with reality is in some sense subjective ones.",
            "Take the view that.",
            "Providing we work hard to think about all our probabilities and write things down consistently.",
            "OK, and we really draw best.",
            "Then use Bayes theorem properly.",
            "Then all the probabilities we come up with properly represent our personal degrees of belief in the uncertainties we still have.",
            "OK, very clean and nobody else can really say anything about that.",
            "If you really believe.",
            "The sun won't rise tomorrow, or that.",
            "Yeah, the euro will go up tomorrow or whatever it is.",
            "That's your beliefs are your beliefs, and that's fine.",
            "The objective view is that achieving that in complex problems is just too hard.",
            "OK, you can be.",
            "You could be fully coherent, properly understand your own uncertainties when there's a small number of variables.",
            "Or perhaps you can't when there's a lot.",
            "So the objective Bayesian takes the view.",
            "You'll be inevitably forced into simplifying things.",
            "You're going to have to approximate your beliefs by something to something you can work with.",
            "In particular, you often have to assume lots of things are independent when you really don't know they are.",
            "And their view kind of leads into saying that conditional probabilities don't represent ordinary judgments in the actual sense, but simply quantifying the extent to which one.",
            "Event logically determines another is a sort of necessity about it rather than.",
            "Rather than the sort of best guess."
        ],
        [
            "So that emphasis now switches into choosing priors to have minimal impact on posterior inference rather than properly quantifying prior beliefs.",
            "So it's a little coming from another different place.",
            "I think for interested I might skip over this, but there's been some effort to try and objectify the choice of priors in objective Bayesian inference.",
            "And.",
            "And the particular you might come across Jeffreys priors and reference prior another maximum other entropy based priors.",
            "So these are all attempts to try and write down priors that don't typically quantify anything anybody actually thinks, but nevertheless have this property of being kind of like vanilla.",
            "They don't.",
            "They don't sort of.",
            "Influence, influence, influence too much and then particularly popular to do things like this when you're trying to compare models for example.",
            "But it's still an ongoing quest.",
            "I mean, this is not sorry.",
            "Here's.",
            "So.",
            "There is also GPL spray or is working.",
            "Do I have any intuition?",
            "Oh well, the particular question the Jeffreys prior is trying to to deal with is the question mentioned here.",
            "If you if you and I think the same thing about something let's but do you think naturally in terms of the standard deviation of a distribution?",
            "I think naturally about the variance.",
            "There ought to be a way that you could write down your relative ignorance about Sigma, and I can write down my ignorance about Sigma squared and it wouldn't matter.",
            "You know that the answer you get in the end would would would be the same.",
            "Yeah, mine would be the square of yours.",
            "That's a very.",
            "That's quite a strong condition.",
            "Actually doesn't sound much, but it's quite strong condition of what the prior can be, and that's where Jeffreys prior comes from, so that's why it uses the influence the information matrix.",
            "I'm.",
            "The problem with all of these things is the problem down at the bottom is that these are all priors that depend on the likelihood.",
            "In some way.",
            "And.",
            "If we if we wanted to think of prior information.",
            "As being what we knew.",
            "About theater, before we had any data.",
            "OK, that's what prior means.",
            "Surely it would also ought to be what we think about theater before we know what kind of data we're going to get.",
            "Yeah, this is a real thing.",
            "I think this about it tomorrow, today.",
            "Tomorrow I'm going to find out if I'm going to have a normally distributed.",
            "Observation, whose mean is theater or price on distributed observations, meaning theater.",
            "I'm only going to find that out tomorrow, but what do I think today about theater?",
            "So these views of setting priors don't allow you to have a prior in that situation.",
            "That's quite a. Troubling thought.",
            "So this is this is really an active area, particularly we don't know how to do objective priors properly in large dimensional problems, and it's an active research area and there isn't a clear answer at the moment."
        ],
        [
            "Yes.",
            "So we need maximum entropy prize.",
            "You need to know the form of the line here, or you can yes.",
            "I guess my understanding that yeah, now these prices are often improper.",
            "And.",
            "Improper and improper distribution in probability is one that's you know, it's positive and negative, but is integral is infinite or not not not one.",
            "OK. Like a normal distribution with different variance.",
            "Um, are you allowed to use those?",
            "Well, yes, you are.",
            "But be careful.",
            "That's what this slide says.",
            "Improper priors are only safe to use if they have this nice property that there.",
            "They can be viewed as the limits of proper priors.",
            "And the posterior you get is is correctly the limit of the corresponding series of posteriors, so.",
            "This is this is a bit of advice that's not taken by anybody, but nonetheless I'll give it, you know, that's the that's the test you have to apply every time you think about using an improper prior, does it?",
            "Does it behave properly in this sense, 'cause it doesn't always.",
            "That's not always true."
        ],
        [
            "OK, I'm going to use the rest of this first session to talk about some.",
            "Principles of modeling.",
            "How do you model?",
            "Well, it's a little difficult to do this in a kind of generic way, right?",
            "I don't have a particular thing in front of me.",
            "A nuclear power station or a financial system or something so.",
            "But I wanted to keep it general, So what I'm going to do is talk about some themes that you see quite a lot when people think about modeling.",
            "So these are quite generic ideas in.",
            "In the people use when building models.",
            "Weather there.",
            "Statisticians or machine learners, I think."
        ],
        [
            "And the first of these is to do is called hierarchical modeling and just introducing this very informally.",
            "If you have a bunch of related things.",
            "Um, other rules about how you should think about.",
            "Modeling them jointly."
        ],
        [
            "Something concrete.",
            "Um?",
            "So this is a little sort of medical stats type example.",
            "So you got 12 hospitals carrying out certain kind of operation on this.",
            "Actually, this is based on data on infant cardiac surgery, so these are little babies very ill with heart defects they need.",
            "They need some surgery to live and it's almost collected statistics on how successful that surgery is in 12 different hospitals.",
            "OK, and we have two hospitals and their different respective.",
            "How many operations they've done and what was the mortality rate.",
            "So for example hospital A.",
            "Everybody, every baby lived OK, perfect, but they didn't do many operations.",
            "And so.",
            "So let's think about some questions.",
            "That might arise there.",
            "Which are the best and worst hospitals?",
            "Are the difference is significant?",
            "What rate would you expect in the 13th hospital?",
            "You mentioned there was actually another one.",
            "Do you have any information now about?",
            "Infant cardiac surgery mortality in the 13th hospital.",
            "Or perhaps what about the 12th, just the 12th hospital in a different year?",
            "To what extent is this data tell us about something that doesn't directly tell us about?",
            "Now before I showed you this.",
            "Picture you would have had no idea.",
            "Weather mortality ratio in this situation is typically 1% point, one percent, 20%.",
            "80% yes, right?",
            "The reason why you should know I haven't even explained my husband bought operation.",
            "It was but OK, you wouldn't know, but I guess you do know something now.",
            "If you were really forced to give a rate for the 13th hospital, I guess most people would say it's about 7%.",
            "Yeah.",
            "Would anyone say I still know absolutely nothing?"
        ],
        [
            "OK, let's think about how to think about that.",
            "Statistically so.",
            "We need a model so the natural model is that the data are normally distributed OK. Each hospital is.",
            "Conducted NI operations, the chance of Chance of Death was Theater I each time, so it's it's like tossing a biased coin.",
            "So there's various possibilities.",
            "The we might say that.",
            "There is just one theater for all hospitals.",
            "The mortality rate for.",
            "This collection of hospitals is theater.",
            "So these are binomial distributions with the same anion, different and different enemies in the same theater.",
            "The second possibility is that they're all different.",
            "Now, in the first case, we can't distinguish hospitals 'cause they work the same same body.",
            "To the best we can possibly do with that data is clump all the wires together.",
            "Clamp all the ends together and just say this is 1 binomial experiment.",
            "In the second case we can't do any combination.",
            "These are different hospitals.",
            "They will have different theaters and that 13th hospital will have a different theater as well.",
            "So we still know nothing about the heart rate, mortality, heart surgery, mortality in that hospital.",
            "And surely the truth is somewhere in between.",
            "The hospitals aren't the same.",
            "OK, but we do know something from one about the other.",
            "And this idea of hierarchical modeling allows us to fill in that gap.",
            "So I think."
        ],
        [
            "I won't go through these in maximum detail, but just to spell it all out in the slides and you can refer to them later.",
            "I first look at the.",
            "In this, in a non Bayesian way.",
            "OK, so we get some estimates 10739 and we get.",
            "Individual hospital estimates vary from zero up to 14.4%.",
            "If we wanted to, we could test the hypothesis that they're all the same, and they very definitely aren't.",
            "There's a very, very tiny probability that you'd get.",
            "Um, you know?",
            "You get variations as extreme as that.",
            "If the theaters were all the same.",
            "And that's pretty much where you have to stop.",
            "I think is a frequentist."
        ],
        [
            "You could try and do both of those.",
            "Things are Bayesian way.",
            "You could say, well, I have a bit of information about theater before I started, perhaps I know what the.",
            "Mortality rate was last year.",
            "And I'm going to use that as a prior distribution from my analysis of these data and you have the same pair of choices.",
            "You can say, well, there's a single theater.",
            "I know a little bit about.",
            "And I'll use that for the current year's data.",
            "Or you could say there's 12 different theaters.",
            "I know a little bit about that I could use that.",
            "And it's pretty much yes.",
            "Oh absolutely, I know.",
            "Yeah, that's absolutely Fairpoint now.",
            "I agree with that.",
            "I mean your prior distribution, yours or mine, is is the result of thinking about your uncertainties and and you know and and quantifying the best you can.",
            "I completely agree.",
            "And for example, you might probably you're not 'cause you're not a, you know.",
            "Surgeon, another my but you know you might know something about how it should have changed from last year.",
            "OK, so your best guess might be a little bit late.",
            "Last year only.",
            "It's a bit better.",
            "Whatever it is.",
            "I'm not saying you have to use last year's data by any means.",
            "It's an example of what you might use in building a prior.",
            "But so, but the mathematical point is either of these choices still leads you to the same position that you'd not combining the data.",
            "You can either combine the completely or not at all, but you can't do anything in between and the effects are simply that you slightly regularize the estimates.",
            "So.",
            "For example.",
            "Hospital A we really annoyed about this.",
            "'cause it had this perfect rate before you had this perfect.",
            "Didn't kill anybody position before, but now these bayesians are telling us we have a 4% mortality rate.",
            "Because bringing in the prior information is reflecting the flat well, they were a bit lucky they didn't do very many operations.",
            "They were a bit lucky too.",
            "Um?",
            "A bit lucky to get away with it.",
            "Yeah, let me come back to this point.",
            "I'm not for a minute suggesting this is a serious analysis of these data.",
            "I'm trying to show you the.",
            "The implications of different styles of modeling.",
            "And of course, if you are really in the position of monitoring public health, you would take the process of price choice very seriously."
        ],
        [
            "So.",
            "Um, which is best when either of those is best?",
            "Really, they both have problems.",
            "'cause you've only got the data from hospital.",
            "I used in inferring theater I."
        ],
        [
            "And.",
            "Here's a here's a slightly different way to think about it.",
            "Um?",
            "Let's imagine that.",
            "These twelve hospitals are not the only hospitals in the world.",
            "These 12 hospitals are themselves a sample from a collection of hospitals.",
            "And that across that collection of hospitals there is variation in theater I.",
            "So that's one way.",
            "That's one way to give a concrete as it were interpretation of what a prior distribution might mean.",
            "It's really, it's not a.",
            "It's not just a degree of belief.",
            "Now it's a model about variation across the different hospitals.",
            "Let's suppose we take this beta distribution."
        ],
        [
            "Will we be in the same position of how do we choose values for Alpha and beta?",
            "Well, there's a so called empirical Bayes approach.",
            "I'll say a little bit more about this tomorrow where you get your Alpha and beta from analyzing this year's data.",
            "So you do something like this.",
            "I'm not advocating this, so the details are not important, but you take something like this year's data.",
            "You did summarize it, you'd find means and variations, means, variances and you use that.",
            "That observed empirical variation in the mortality rates as a guide to the population variation in hospitals.",
            "Sounds like a sword."
        ],
        [
            "The sensible thing to do.",
            "The difficulty with it mathematically and principle is that you're using the data twice.",
            "You'll be essentially using the data to decide your prior, and you use that that that prior to analyze the data and so each bit of data comes into the analysis twice, and the effect of that is a bit like saying we got twice as much data as we really have.",
            "Of course, you have more data, you have less variation, so the effect is that you're going to estimate how precise you really are."
        ],
        [
            "So the the the hierarchal approach avoids all these horrible pitfalls.",
            "If it avoids you having to take an extreme view about whether hospitals are separate or all the same and it avoids you recognizing the differences in the without counting data twice.",
            "And what it comes to is saying, well, there's another level of variation here.",
            "There are really 3 levels of variation.",
            "Stop this parameters and data.",
            "There are three levels.",
            "This is the actual hazard of this kind of operation.",
            "How dangerous is this operation?",
            "We don't know, so that's uncertain.",
            "OK, so that's a random variable.",
            "Then this variability in the actual rate that a particular hospital.",
            "Has practice.",
            "So that some particular hospitals might be more or less risky because they have.",
            "And then the surgeons have better practice, or worse, practice whatever it is.",
            "And then this chance factors the individual patients outcome.",
            "You know, even if you know that theater exactly .07, that doesn't predict the actual outcome for any single single baby.",
            "So we have three real levels of variation, and that the idea of hierarchical modeling is to model those three."
        ],
        [
            "Random variables and put them all together.",
            "So the approach in this case might be to put priors on Alpha and beta.",
            "So Alpha and Beta are the are the parameters that describe the population variation.",
            "But as the population of hospitals that that variation in in the individual mortality rates and we put priors on Alpha and beta themselves but three left."
        ],
        [
            "In the model.",
            "And the effect of that is that you get some what we call it.",
            "Borrowing a strength, it provides a principled way of sharing information between the different hospitals.",
            "Cause we don't know what Alpha and beta is, but the first 11 hospitals.",
            "Lead to US learning something about Alpha and Beta, and that information propagates down to things we say about the 12th hospital.",
            "So this is a common theme in statistical modeling that we think about.",
            "Variation properly at different levels and certain we consider adding an extra layer and the effect of that is to have a system have a method of mathematical."
        ],
        [
            "Method for sharing information across.",
            "Across across units.",
            "Now, I don't say very much about graphical models, 'cause 'cause Martin Wainwright's going to next week.",
            "But these these different choices can be symbolized by these pictures.",
            "These are called directed acyclic graphs.",
            "In these pictures, the circles are variables that we know.",
            "The random variables that.",
            "We don't know, and also eventually things like the data that we will at some point now.",
            "And that the blocks the squares represent replication plates as they called.",
            "And they say these are the different least the different situations, and so the hierarchical model is the one with this 3 levels.",
            "Of nodes with circles in them."
        ],
        [
            "In some cases you can work things out.",
            "The effect of this out algebraically, so I put in."
        ],
        [
            "Just a quick couple of slides there about how it works in the normal case, but if you want to see that."
        ],
        [
            "Let's look at that later.",
            "So that's the first principle that we commonly see in in statistical modeling.",
            "This idea of hierarchical modeling.",
            "It also illustrates a second principle, which is which is.",
            "Can be very important and that's the idea of exchangeability.",
            "So, um.",
            "Conditional independence and exchangeability.",
            "There are the two most important things in.",
            "In in Bayesian statistics, in my view.",
            "So Exchangeability represents the is a way of encoding the idea that we find no systematic reason to distinguish between a collection of random variables.",
            "So.",
            "Suppose you came to this kind of.",
            "Data for the first time.",
            "You're told we're gonna look at 12 hospitals.",
            "OK, you don't know anything about them.",
            "You perhaps you know something about.",
            "Infant hearts mortality and the surgery.",
            "If you don't think about the hospitals so you have no reason to think anything different about theater one or Theater 3.",
            "OK, your views about the theaters are completely exchangeable.",
            "The the label and the theater is irrelevant to what you think.",
            "That's not the same as saying you think they're independent because I think.",
            "I think I persuaded you that.",
            "Yeah, those first 11 hospitals do tell you something about the 12th.",
            "So they know it can't be independent.",
            "What you think about these hospitals, but it's exchangeable.",
            "So this can be an important ingredient when trying to build priors of big collections of things.",
            "What aspects of the things you're looking at are exchangeable?",
            "Do you have no no opinions about to distinguish?",
            "Well, we can look at this particularly clearly in the case of parameters that are just binary.",
            "So let's for the moment, think of the theater eyes as being zero or one.",
            "So the definition of exchangeability, or to be precise, infinite exchangeability is that the joint distribution of these variables is independent of the order in which the variables are written, and that same thing is true have a longer string of variables."
        ],
        [
            "Yes.",
            "Now if we.",
            "If the variables were the result of Fair coin tossing independent fair coin tossing, then they would definitely be exchangeable because the order of heads and tails is completely.",
            "Yeah, it's clear and there's no.",
            "There's no preference of 1 order over any other.",
            "They will regret that.",
            "Now suppose to be slightly generalized that we don't need to do ordinary coin tossing.",
            "We do coin tossing with the choice of one of two coins.",
            "So I have two coins in my pocket, and they both biased.",
            "I pick out one of these two.",
            "I don't know which one it is.",
            "And then I toss that one repeatedly.",
            "OK, so whether I take.",
            "They want the coin that favors heads or the coin that favors tails doesn't matter.",
            "Given that choice of coin.",
            "There's still no preference for any particular order of heads and tails.",
            "So it must be true, therefore, that a sequence of.",
            "Tosses of a randomly chosen coin is always exchangeable.",
            "And that's what I say on this.",
            "In the in the screen there that the Bernoulli probabilities multiplied together, then mixed over a distribution of the probability of ahead.",
            "That must be a distribution that's exchangeable.",
            "Indeed, you can see there that is exchangeable because it's clear that that value depends only on the.",
            "Set of values that are 40 to 70 to 9.",
            "In fact, it depends actually only on there some, not the order in which they appear.",
            "OK, so mixing together.",
            "Bernoulli distributions gives you exchangeable sequences.",
            "Now the most amazing thing is that that's an if and only a.",
            "That's that's called definitive theorem.",
            "It says the only way you can get infinitely exchangeable 01 random variables.",
            "Is by Bernoulli trials with a randomly chosen probability of heads.",
            "Yes.",
            "Sorry.",
            "Patient is different from IAD.",
            "Yes, I mean the bottom line what I've stated is mathematically correct.",
            "For 01.",
            "It's true in a more subtle sense for any.",
            "Something from any distribution whatsoever.",
            "OK, the thing that's different in the general case in the brewing case, there's only one parameter.",
            "Obviously the probability of ahead, and so this is a 1 dimensional integral, but in a."
        ],
        [
            "As soon as you get away from 2 variables, then there's the minimum number of parameters.",
            "Is more than one, and so it's a.",
            "It's a, it's a multivariate integral.",
            "But something that formally similar to this is always true, so exchangeability is essentially about mixing of independent things.",
            "That's a very powerful idea, and.",
            "And in particular, it gives you another reason to think hierarchically.",
            "OK, because even if you.",
            "Even if you found my argument about population variation in hospital mortality rate, even you found that.",
            "Not completely convincing.",
            "If you just believe that the hospitals mortality rates are exchangeable, you'll be forced to do pretty much the same thing.",
            "Because because the only way to get exchangeable mortality rates for hospitals is to use to mix over these independent cases."
        ],
        [
            "So high school models do a lot more different things than that.",
            "There are theme throughout complex modeling.",
            "I've listed some of the situations there and I'm sure in your experience with statistical models you can you can.",
            "You can remember lots of situations where you've.",
            "Where you've used hierarchal models?",
            "Even if you haven't used that particular phrase, variation at multiple levels where essentially breaking up modeling into a number of different smaller tasks and thinking about the significance of the variation at each different level and the effect of that, is this sharing of information across the model."
        ],
        [
            "So there's quite a lot of different reasons for thinking this way about modeling.",
            "Um?",
            "Most of which I've mentioned, I think now.",
            "Oh yes, I didn't mention the second one here, the.",
            "Some people find it difficult to settle on what you know.",
            "Why should I choose that particular beta distribution?",
            "And what we find is that the further away from the data, the choice of specific parameter values for prior distributions is the further away you get, the less.",
            "Less sensitive, the consequences are."
        ],
        [
            "OK I don't have alot of time left.",
            "I'll just quickly talk about the main points of this final section.",
            "So this is another sort generic theme.",
            "You find a lot of statistical modeling and I'm going to talk about two very specific classes of models, hidden Markov models, state space models, and I guess you've all heard of.",
            "At least one, possibly both of those.",
            "And they're really the same thing.",
            "That's the interesting thing, so.",
            "I'm.",
            "But I'm talking about them not only for their own sake, but also becausw.",
            "There are sort of paradigm for the other things that we do.",
            "So the key idea is you gotta hidden sequence and indivisible sequence X is and wise.",
            "Let's say it's for the moment just talking bout time order situation so.",
            "012 and so on.",
            "And the structure in the system is that the X is the things that are not observed for Markov chain.",
            "OK, so it's a dependant process where you can predict the future.",
            "As well, by knowing the present as by knowing any of the past as well.",
            "Go to market trainers.",
            "And then, given given the series of X is, then each?",
            "Why is just annoying, you know, determines distribution is determined only by its.",
            "Its particular X&Y is like a noisy version of X.",
            "And that that that structure gives you very.",
            "Important benefits and let's look at."
        ],
        [
            "Look at some of that.",
            "So here's a picture of the dependence.",
            "Schematically this this graph has a mathematical meaning about about about dependence.",
            "Again, Martin is going to talk a lot about that, but the the arrows here for the moment just reflect the way we build the model.",
            "So the model for the X is is that it's a Markov chain and we build that sequentially along there and then the wise are noisy versions at each stage."
        ],
        [
            "So hidden Markov models.",
            "That's the phrase we typically use when the X is have a finite set of values finite state space.",
            "Maybe known set of values, maybe not.",
            "Anne.",
            "And.",
            "There's lots of important examples of that.",
            "They used all over the place, particularly these days.",
            "Very important in in biological sequencing of various kinds and historically very important.",
            "Also, insight in speech wreck."
        ],
        [
            "Now state space models have the same structure, but there the X variable is continuous and the very commonly studied cases where everything is normally distributed and then you often get this kind of structure so that the.",
            "Is a sort of autoregressive process driving these days, so that's driving the axis, so each.",
            "HX is a multiple of the previous X plus some noise.",
            "OK, so you get some correlation in the sequence when a is non zero and then the Y is a multiple of the current X plus some further noise.",
            "So that's a generic 1 dimensional normal linear state space model and of course you can do exactly the same thing with vectors."
        ],
        [
            "And that's a class of models that also has enormous implications all over the place, particularly metrics."
        ],
        [
            "And in signal processing.",
            "And the linear structure allows you to do all sorts of things, and in particular allows us to access these kind of tasks.",
            "Filtering, smoothing and prediction there very.",
            "Key problems filtering is the process of observing the sequence up to.",
            "Now the T is now.",
            "And saying, OK, where are we now?",
            "OK, I've thought why one?",
            "I've got YTY T -- 1 otherwise back from now, not the future and I want to make the best inference I can about the current X.",
            "That's the filtering problem.",
            "The smoothing problem.",
            "Is.",
            "I've observed the weather up today today and I want to say the best possible thing I can about last Saturday.",
            "Or if you like to play around.",
            "I can predict I can.",
            "I can observe things a little bit in the future before making inferences about today.",
            "And prediction is the opposite.",
            "Prediction is trying to.",
            "The thing is more useful in weather saying what's going to happen next week.",
            "And so all of those have the property that you talk about.",
            "Data up to now.",
            "OK, not not.",
            "Not, not, not not indefinitely into the future as well.",
            "And very often we we don't models of this kind, even when there's no requirement to to do to do online inference.",
            "Yeah, I agree it is almost coffee time.",
            "Often you will adopt this sort of formulation of a model even when you do actually have in the future."
        ],
        [
            "As well, so there isn't time to go through the details, but the in the continuous state space case.",
            "This speaks to the idea of Kalman filtering, where we have explicit expressions for all these filtering and prediction.",
            "Smoothing algorithms is simple algebraic stuff."
        ],
        [
            "And.",
            "Yeah, it's probably the most often used algorithm in whole of electrical engineering.",
            "When you get away from the Gaussian linear case, then nothing is explicit anymore and we come to the idea of needing to do filtering when these.",
            "This beautiful explicit algebraic theory isn't available, and in particular the idea of particle filtering tremendously valuable here, and I expect that our nose either be talking about that, or will we talk about it today.",
            "OK, so that's where particle filtering fits into this scheme of things."
        ],
        [
            "In the discrete case, there's another set of interesting algorithms that allow you to do things very explicitly, just like the filtering case, they rely very much on the.",
            "Linear sequence here.",
            "Please look at the slides late."
        ],
        [
            "'cause I want to try and stop on time, but the point is you."
        ],
        [
            "We we completely avoid the apparent enormous combinatorial explosion of summation that you would get trying to do exact Bayesian inference.",
            "In in the systems, we completely avoid all that by cleverly writing.",
            "Sums of products of things interleaving partial sums and partial product so that you get a much smaller order of."
        ],
        [
            "Imputation so that you can get exact Bayesian inference in a very small.",
            "In a very small amount of computing."
        ],
        [
            "The more general lessons, though, getting away from this.",
            "This particular details the more general lessons are that this idea of modeling.",
            "Independent when you got dependent data.",
            "Modeling the dependence one step removed from the actual data.",
            "That's a very powerful idea, and we see that time and time again in statistical modeling.",
            "So treating modeling the dependence in some hidden level called X and the dependence in wise inherited from that by regarding the wise as noisy versions of the X is and so one of the advantages of that way of thinking about modeling is that you can model the data.",
            "According to.",
            "Observed empirical properties of data about their distribution and so on, and you still got some flexibility in the way you formulate the hidden process that provides the dependence."
        ],
        [
            "In particular, going back a step, the forwards backwards type algorithms for PR programs and so on.",
            "They apply equally to other other graph structures other than linear systems, and particularly can do inference on treason, junction trees and so on with very much the same idea.",
            "So there's quite a powerful collection of.",
            "Partly abstract lessons that you get from thinking about state space models and hidden Markov models are powerful way to think about modeling."
        ],
        [
            "OK, thank you for your attention.",
            "We're just about on time.",
            "Enjoy your coffee and I'll be.",
            "Here again twice tomorrow, but the first one is.",
            "Moving on to more, more substantial things, and particularly talking about Bayesian computation and about models, sensitivity and model criticism.",
            "And then the third one thereafter.",
            "Coffee tomorrow I'm going to.",
            "Put together a collection of lessons learned in talk about them in the context of.",
            "Various research problems I've had I've dealt with in the last few years and I've gotta try and pick out the way the different things I've been talking about influence the way we did the modeling and the inference in those situations.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to be talking about it.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Once an input.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look at the Bayesian inference.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An inference that is different from data analysis.",
                    "label": 0
                },
                {
                    "sent": "Inferences about learning about.",
                    "label": 0
                },
                {
                    "sent": "Not just the data you've got, but the data you will have.",
                    "label": 0
                },
                {
                    "sent": "Or you might have.",
                    "label": 0
                },
                {
                    "sent": "So it inevitably involves modeling, and it involves assumption.",
                    "label": 0
                },
                {
                    "sent": "So inference is the process of discovering from the data something about typically about mechanisms that either did or might have caused the data or generated the data.",
                    "label": 1
                },
                {
                    "sent": "Or if that's too much, at least mechanisms that might explain the data.",
                    "label": 0
                },
                {
                    "sent": "The goals of doing this are quite can be quite varied.",
                    "label": 0
                },
                {
                    "sent": "There's not one simple single thing we're trying to do.",
                    "label": 0
                },
                {
                    "sent": "We may simply be trying to predict future data in many.",
                    "label": 0
                },
                {
                    "sent": "Commercial applications, that's the main thing you want to do I guess.",
                    "label": 0
                },
                {
                    "sent": "But in scientific applications it's usually a little more.",
                    "label": 0
                },
                {
                    "sent": "We typically want to try and learn something about.",
                    "label": 0
                },
                {
                    "sent": "Truth, that's a bit of a overblown word, but something about the scientific truth about scientific laws, something about how society works, whatever it is so.",
                    "label": 0
                },
                {
                    "sent": "If you're an applied mathematician, you naturally think of that as an inverse problem instead of going from assumptions towards data in inference, we go in the opposite direction.",
                    "label": 0
                },
                {
                    "sent": "We're going from data back towards assumptions and trying to to learn something, and the idea in Bayesian inference is simply that we use probability to do all of that.",
                    "label": 0
                },
                {
                    "sent": "OK, we absolutely rigorously stick to the laws of probability, and that's pretty much the only thing we use to conduct all of our inferential processes.",
                    "label": 1
                },
                {
                    "sent": "Now I'm going to spend some minutes in these opening slides talking about the implications of this, but the key strength essentially is that all sources of uncertainty are simultaneously and coherently accounted for.",
                    "label": 0
                },
                {
                    "sent": "I mean, in the in the old days of small data problems, that wasn't terribly important, 'cause they probably want only was one source of uncertainty, but nowadays we're dealing with complex systems with many different sources of variation, and it's crucial to be able to properly integrate all of those uncertainties and all of that randomness, and the laws of probability make that easy, impossible, and it's about the best game in town for doing that in my opinion.",
                    "label": 0
                },
                {
                    "sent": "Is model based and in I think in the machine learning language we would call all these generative models.",
                    "label": 0
                },
                {
                    "sent": "These are models that are capable of generating the data.",
                    "label": 1
                },
                {
                    "sent": "I'm a further strength of the Bayesian approach is that models are not, of course models have to be assumed and and so on, and our analysis are conditional on the on the truth of those models.",
                    "label": 0
                },
                {
                    "sent": "But also we can use Bayesian methods themselves to criticize those models.",
                    "label": 0
                },
                {
                    "sent": "So it's like it's internally self criticising which is which is quite important.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so over the three talks today and the 2:00 tomorrow, I'm going to cover various different things.",
                    "label": 0
                },
                {
                    "sent": "And I'll tell you a little bit later about the actual program to today's lecture is quite introduction.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Three, but will get on to some more difficult things later.",
                    "label": 0
                },
                {
                    "sent": "Everything connects pretty well.",
                    "label": 0
                },
                {
                    "sent": "Everything does connect.",
                    "label": 0
                },
                {
                    "sent": "The three other themes that at this summer school that particularly connect with my talks are these three.",
                    "label": 0
                },
                {
                    "sent": "Here you've been hearing about.",
                    "label": 0
                },
                {
                    "sent": "Monte Carlo methods from from Arnaud and he'll continue today.",
                    "label": 1
                },
                {
                    "sent": "The Bayesian Nonparametrics theme on Monday and Tuesday is also very relevant and at the end of next week the graphical models theme from Martin Rob Wainwright is also very important.",
                    "label": 0
                },
                {
                    "sent": "Now, if they weren't here, I will probably be talking about all of those things in less detail, so I'm going to deliberately try and downplay those particular issues and relying on them.",
                    "label": 0
                },
                {
                    "sent": "I'm sure they did it a lot better than I would, but relying on there to cover those.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Those areas.",
                    "label": 0
                },
                {
                    "sent": "Now I'm not as you know.",
                    "label": 0
                },
                {
                    "sent": "I'm not a machine learner of any description.",
                    "label": 0
                },
                {
                    "sent": "So I was a little bit nervous coming here.",
                    "label": 0
                },
                {
                    "sent": "Ann and I spent a little time browsing on the web to find out what other people have done.",
                    "label": 0
                },
                {
                    "sent": "So and in doing so, I decided to steal two slides from two people.",
                    "label": 0
                },
                {
                    "sent": "I admire a lot.",
                    "label": 0
                },
                {
                    "sent": "I think they're very highly esteemed both in the statistics and the machine learning community.",
                    "label": 0
                },
                {
                    "sent": "'cause I thought they might have an interesting perspective on perhaps the connections between the subjects if they are indeed different, discuss.",
                    "label": 0
                },
                {
                    "sent": "And also on the role of Bayesian methods.",
                    "label": 0
                },
                {
                    "sent": "In them, so this first one is Michael Jordan.",
                    "label": 0
                },
                {
                    "sent": "And you can see what he says here.",
                    "label": 0
                },
                {
                    "sent": "He was trying to explain what machine learning was to a statistician.",
                    "label": 1
                },
                {
                    "sent": "You so you may disagree.",
                    "label": 0
                },
                {
                    "sent": "Of course he says.",
                    "label": 0
                },
                {
                    "sent": "But what he says is this.",
                    "label": 0
                },
                {
                    "sent": "That's a loose Federation of themes in statistical inference.",
                    "label": 1
                },
                {
                    "sent": "Is a focus on prediction and exploratory data analysis.",
                    "label": 1
                },
                {
                    "sent": "A focus on computational methodologies an and particularly empirical evaluation.",
                    "label": 0
                },
                {
                    "sent": "And sometimes she quaintest and sometimes Bayesian, so that was his his take very briefly on machine learning and statistics.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But my my second here.",
                    "label": 0
                },
                {
                    "sent": "Here is Chris Bishop.",
                    "label": 0
                },
                {
                    "sent": "And he's promoting ideas of what he calls third generation machine intelligence.",
                    "label": 1
                },
                {
                    "sent": "And again the Bayesian framework graphical models on a write up.",
                    "label": 0
                },
                {
                    "sent": "There's that as part of his.",
                    "label": 0
                },
                {
                    "sent": "He's the key things underpinning what he's regarding is the future of machine intelligence.",
                    "label": 0
                },
                {
                    "sent": "So you know I'm having found these slides.",
                    "label": 0
                },
                {
                    "sent": "Could course taking the very selectively from those available, but having found these slides.",
                    "label": 0
                },
                {
                    "sent": "I felt OK.",
                    "label": 0
                },
                {
                    "sent": "I can come and talk to you and we'll.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Will we have some sort of common ground?",
                    "label": 0
                },
                {
                    "sent": "So this is my take are on the connection between the two areas.",
                    "label": 0
                },
                {
                    "sent": "I think the difference between statistics and machine learning is much more difference of community rather than a difference of the kind of questions we address.",
                    "label": 1
                },
                {
                    "sent": "Yeah, the origins of these two areas are in different scientific communities and when they have different traditions and they behave different ways.",
                    "label": 1
                },
                {
                    "sent": "For example, you know in statistics we focus much more on.",
                    "label": 0
                },
                {
                    "sent": "On referee journals, for example, in machine learning using to have various conferences which are very high status.",
                    "label": 0
                },
                {
                    "sent": "So there are various differences in the way the way we work.",
                    "label": 0
                },
                {
                    "sent": "I think machine learning has typically been more ambitious than statistics in terms of the reach of areas you try to get into the scale of problems you try to address.",
                    "label": 1
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "And there's certainly in the in the datasets your machine learners typically look at.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of structure within observations, and we're interested in exploring that structure and using it to create powerful methodology.",
                    "label": 0
                },
                {
                    "sent": "But the typically.",
                    "label": 1
                },
                {
                    "sent": "My view of machine learning it's tends to be not much structure between observations.",
                    "label": 1
                },
                {
                    "sent": "We're talking typically about a stream of things that are pretty much similar to each other.",
                    "label": 0
                },
                {
                    "sent": "There's a focus on prediction, and often things are evaluated with cross validation.",
                    "label": 0
                },
                {
                    "sent": "Another essentially ways of evaluating predictive strength in statistics.",
                    "label": 0
                },
                {
                    "sent": "I think there's much more emphasis on model building, much more thinking about what was the process that made the data.",
                    "label": 0
                },
                {
                    "sent": "Trying to uncover that trying or something.",
                    "label": 0
                },
                {
                    "sent": "Yeah, trying on assumptions.",
                    "label": 0
                },
                {
                    "sent": "We tend to do things that are reliant on the models.",
                    "label": 0
                },
                {
                    "sent": "But we have more of a name of trying, as it were, to understand what what science or society, or whatever he's doing, and less of a name on less of a concentration on, you know.",
                    "label": 0
                },
                {
                    "sent": "Through portal producting how?",
                    "label": 0
                },
                {
                    "sent": "Running on on huge real time situations.",
                    "label": 0
                },
                {
                    "sent": "And we tend to it.",
                    "label": 0
                },
                {
                    "sent": "We tend to use models to evaluate, and that's a clear difference.",
                    "label": 0
                },
                {
                    "sent": "So I think this is a fascinating distinction, and this we have a lot to learn from each other.",
                    "label": 1
                },
                {
                    "sent": "You know, I'm really thrilled that the two areas seem to be converging and.",
                    "label": 0
                },
                {
                    "sent": "I. I hope these tutorials will help that a little bit.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Haven't got that out of the way.",
                    "label": 0
                },
                {
                    "sent": "I mean, I'm a statistician and my illustrations will be of statistics models and they may not be the same character that you might hear about from somebody else giving lectures of this this kind, but I think that that's a reasonable put, partly because it's what I know about, and partly because I think some of these statistical models are.",
                    "label": 0
                },
                {
                    "sent": "I'll show you and talk about actually expose a few more of the different issues.",
                    "label": 0
                },
                {
                    "sent": "The fundamental issues in Bayesian inference, then that you could get from the machine learning perspective.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, just a little bit on the contrast between Bayesian and frequentist statistics.",
                    "label": 1
                },
                {
                    "sent": "Is there actually a lot of different paradigms though?",
                    "label": 0
                },
                {
                    "sent": "They're just the two we tend to talk about quite a lot and tend to perhaps contrast.",
                    "label": 0
                },
                {
                    "sent": "There are many other things in many other ways of doing inference, but these are the two most most prominent.",
                    "label": 0
                },
                {
                    "sent": "Just to give one example of something that's different from both of them, there's a whole theory of inference based solely on likelihoods.",
                    "label": 0
                },
                {
                    "sent": "Likelihood inference is not the same as frequentist inference, in fact.",
                    "label": 0
                },
                {
                    "sent": "There's a principle called the likelihood principle, which frequentist statisticians do not obey.",
                    "label": 0
                },
                {
                    "sent": "But you can do things based solely on likelihood, and it's in some sense closer to the Bayesian view than the frequentist one.",
                    "label": 0
                },
                {
                    "sent": "So within statistics we've you know we've had a lot of fun, but also wasted a lot of energy over the years.",
                    "label": 0
                },
                {
                    "sent": "Arguing about about paradigms and philosophies and so on.",
                    "label": 0
                },
                {
                    "sent": "It's been somewhat distracting sometimes.",
                    "label": 0
                },
                {
                    "sent": "Sometimes we've learned from it.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it's being destructive.",
                    "label": 0
                },
                {
                    "sent": "I think nowadays we're all settling down and being a bit more sensible about this and many, many statisticians will be not prepared to say they're in one camp.",
                    "label": 0
                },
                {
                    "sent": "On the other, they would be very happy to to use ideas from both paradigms in different situations.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But if you wanted to contrast them, I suppose this would be a way you could do it.",
                    "label": 0
                },
                {
                    "sent": "In Bayesian statistics, we typically don't dream up methods.",
                    "label": 0
                },
                {
                    "sent": "I meant to say a little while ago.",
                    "label": 0
                },
                {
                    "sent": "Please interrupt at any point, but I wasn't.",
                    "label": 0
                },
                {
                    "sent": "I wasn't quite thinking of that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, if I say anything is rubbish, please please please interrupt.",
                    "label": 0
                },
                {
                    "sent": "Yeah I mean Bayesian stoned.",
                    "label": 0
                },
                {
                    "sent": "Invent methods.",
                    "label": 0
                },
                {
                    "sent": "Bayesian Bill, Bailey's build models and the models tool.",
                    "label": 0
                },
                {
                    "sent": "Tell us what the method should be whereas frequentist are happy to consider methods they dream up from anywhere.",
                    "label": 0
                },
                {
                    "sent": "Bayesian inference is a place always conditional on the native we've got.",
                    "label": 1
                },
                {
                    "sent": "They don't transcend thing about anything else, whereas in frequentist the frequentist world the emphasis is on giving good answers in repeated use and so on.",
                    "label": 1
                },
                {
                    "sent": "So there are some important distinctions here, and the two can be looked at in parallel.",
                    "label": 0
                },
                {
                    "sent": "But you know, it's not as if there's a competition.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the two in which one clearly wins, except perhaps as we come into the sort of the present day.",
                    "label": 0
                },
                {
                    "sent": "It's just a few other issues.",
                    "label": 0
                },
                {
                    "sent": "I want to emphasize.",
                    "label": 0
                },
                {
                    "sent": "There was a time when I think it was difficult to be Bayesian if you had.",
                    "label": 0
                },
                {
                    "sent": "Clients, or you know, if you had to end users because there's a lot of needing to defend what you've done, I think those days are rather gone, but perhaps not completely.",
                    "label": 0
                },
                {
                    "sent": "One of the strengths of Bayesian inference is that we can go directly and report the inferences we want.",
                    "label": 0
                },
                {
                    "sent": "We don't have to.",
                    "label": 0
                },
                {
                    "sent": "Twist ourselves into logical, not thinking about null hypothesis and you know trying to express everything.",
                    "label": 0
                },
                {
                    "sent": "What's often, at least for beginning students, are very counter intuitive way.",
                    "label": 0
                },
                {
                    "sent": "Very key idea in Bayesian inference is borrowing strength.",
                    "label": 0
                },
                {
                    "sent": "This idea that one thing is always informative about another.",
                    "label": 1
                },
                {
                    "sent": "Everything connects.",
                    "label": 0
                },
                {
                    "sent": "If you measure something you're learning not only about the things that directly influence it, but many other things as well.",
                    "label": 0
                },
                {
                    "sent": "And we'll see that will see that thing.",
                    "label": 0
                },
                {
                    "sent": "We will see that coming out in some of the examples.",
                    "label": 1
                },
                {
                    "sent": "I think Bayesian inference really pays off in complex and high dimensional problems in small scale low dimensional problems.",
                    "label": 0
                },
                {
                    "sent": "To be honest we can have different philosophies about how we're going to do it, but we end up with pretty much the same answers.",
                    "label": 0
                },
                {
                    "sent": "But as problems get bigger and more complex than there are clear differences and Bayesians able to do to do more.",
                    "label": 0
                },
                {
                    "sent": "But this one bitter, even even committed Bayesian, will always defend one aspect of the frequentist idea, and that is that we happy typically for do use frequentist methods to evaluate performance.",
                    "label": 0
                },
                {
                    "sent": "So this sort of the frequentist consistency of frequentist performance of of statistical Bayesian statistical process processes is a perfectly legitimate thing to think about.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, well that's that's the end of my sort of opening.",
                    "label": 0
                },
                {
                    "sent": "Motivation trying to get over some of the key ideas and this is the contents list for the remainder of this this tutorial.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Only arrived last night, so I haven't had a chance to meet any of you yet.",
                    "label": 0
                },
                {
                    "sent": "I I first thing I asked when I got to the bar was what are the students like so I don't know anything about you individually, but I know a few things about you in general.",
                    "label": 0
                },
                {
                    "sent": "I know you're very bright.",
                    "label": 0
                },
                {
                    "sent": "I know that you are very selected to get here.",
                    "label": 0
                },
                {
                    "sent": "You beat the competition.",
                    "label": 0
                },
                {
                    "sent": "So congratulations, but I also know you're pretty diverse in terms of where you've come from and what you've done before.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "That I say that by way of explaining that some of the things I'm going to talk about, particularly today will seem really very basic.",
                    "label": 0
                },
                {
                    "sent": "For some of you.",
                    "label": 0
                },
                {
                    "sent": "But trust me, I think it's I think they're worth doing, and I suspect there's some of you here who will appreciate evenly the elementary things I talk about.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let's take a little bit about probability.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to say anything more about probability in this one slide, so if you don't know about probability, you really are going to have to go and look it up so.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or else.",
                    "label": 0
                },
                {
                    "sent": "So probability is nothing but common sense reduced to calculus.",
                    "label": 1
                },
                {
                    "sent": "Somebody said Laplaca, I think.",
                    "label": 1
                },
                {
                    "sent": "Just to reduce your recap, it measures uncertainty on a 01 scale zero and one mean what you think they mean.",
                    "label": 0
                },
                {
                    "sent": "And there's only really one property that probably has, and that is if you look at two events, and if they can't both happen, then the probability of one or other is the sum of the two probabilities.",
                    "label": 0
                },
                {
                    "sent": "And it's extraordinary.",
                    "label": 1
                },
                {
                    "sent": "But that's all you need.",
                    "label": 0
                },
                {
                    "sent": "Pretty much all that.",
                    "label": 0
                },
                {
                    "sent": "And the same thing generalized to accountable collection of events pretty much generates the whole theory of probability.",
                    "label": 0
                },
                {
                    "sent": "So it's remarkably light in terms of axioms.",
                    "label": 0
                },
                {
                    "sent": "And of course, when we're dealing with data, typically the refocus probability not on the general theory of events, but we talk particularly about random variables.",
                    "label": 0
                },
                {
                    "sent": "So almost all of the probability we use will be talking bout distributions of random variables, discrete or continuous.",
                    "label": 1
                },
                {
                    "sent": "Scalar or vector or living in all sorts of different spaces, random variables and their and their distributions.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there's only one thing we particularly need from probability theory, and that of course is Bayes theorem.",
                    "label": 0
                },
                {
                    "sent": "You may think that maybe that's where the word where the name Bayesian analysis comes from.",
                    "label": 0
                },
                {
                    "sent": "So here's a really small little problem.",
                    "label": 0
                },
                {
                    "sent": "I like puzzles and this this is a version of a puzzle that appeared in in the Guardian newspaper recently.",
                    "label": 0
                },
                {
                    "sent": "So these are little little game setters set in a sort of fictitious context, so you really you're allowed to draw 2 balls.",
                    "label": 0
                },
                {
                    "sent": "You can't.",
                    "label": 0
                },
                {
                    "sent": "You can't see inside the urns.",
                    "label": 0
                },
                {
                    "sent": "You know the constitutions of the two, but you don't know which one is which.",
                    "label": 0
                },
                {
                    "sent": "And you really need a Red Bull.",
                    "label": 0
                },
                {
                    "sent": "OK, and you you choose an urn at random and you pick out a bowl and it's blue.",
                    "label": 1
                },
                {
                    "sent": "OK, and you only get one chance left.",
                    "label": 0
                },
                {
                    "sent": "So the question we're asking is.",
                    "label": 1
                },
                {
                    "sent": "What's your chance of escape or should chance of getting a Red Bull?",
                    "label": 0
                },
                {
                    "sent": "And would it be a good idea to Switch earns after the first?",
                    "label": 0
                },
                {
                    "sent": "Draw.",
                    "label": 0
                },
                {
                    "sent": "OK, this is a homework question so you can start thinking about the answer to that question.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, this is obviously a really silly problem.",
                    "label": 1
                },
                {
                    "sent": "OK, but the reason I've included it is because it includes many of the main ideas of inference.",
                    "label": 1
                },
                {
                    "sent": "We have some data because we've drawn a ball.",
                    "label": 0
                },
                {
                    "sent": "We want to make some inference.",
                    "label": 0
                },
                {
                    "sent": "One of the things we're interested in is now we've drawn a ball when we know it's blue, which earned.",
                    "label": 0
                },
                {
                    "sent": "Do we think we've drawn it from?",
                    "label": 0
                },
                {
                    "sent": "OK, that would be an example of an inferential question.",
                    "label": 0
                },
                {
                    "sent": "I guess we understand that probably now it's the right hand turn.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Prediction what's the next ball going to be?",
                    "label": 1
                },
                {
                    "sent": "How likely I are you to get a Red Bull?",
                    "label": 0
                },
                {
                    "sent": "And then decision which is the better of the two strategies.",
                    "label": 1
                },
                {
                    "sent": "Sticking with the same earn that you chose the first time or or switching.",
                    "label": 0
                },
                {
                    "sent": "And the point is, of course, that all of this is a probability question, not a statistic question, but you can see that by solving the probability question, we can do all those inferential things.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I don't wanna spend very long in the calculations.",
                    "label": 0
                },
                {
                    "sent": "You can look at the slides later and it's it's a bit tedious, but we just basically work through the conditional and the joint probabilities.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that the the three things we were thinking about other choice of the urn, was it the left turn right?",
                    "label": 0
                },
                {
                    "sent": "And you choose what's the first ball drawn and what's the second ball drawn?",
                    "label": 1
                },
                {
                    "sent": "And the inference question asked us is asking what is the privacy?",
                    "label": 1
                },
                {
                    "sent": "We chose the left hand and given that we've now got the data that we up all these blue and we can work that out using the laws of conditional probability.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that the answer is eight 8 / 20 three 1/3.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Roughly, and that's because in this particular case, the variable we're talking about the choice of the earn, only has two different values.",
                    "label": 0
                },
                {
                    "sent": "We can do.",
                    "label": 0
                },
                {
                    "sent": "We can write down Bayes theorem in particularly neat way.",
                    "label": 0
                },
                {
                    "sent": "In that case in terms of odds and the.",
                    "label": 0
                },
                {
                    "sent": "The posterior odds that we've got the left hand turn rather than the right given the evidence that the ball is blue, is the prior odds multiplied by the ratio of the likelihoods.",
                    "label": 0
                },
                {
                    "sent": "And that's that's a key key thing to remember.",
                    "label": 0
                },
                {
                    "sent": "So that solves the inference question.",
                    "label": 1
                },
                {
                    "sent": "Is about 2:00 to 1:00.",
                    "label": 0
                },
                {
                    "sent": "That we've drawn the left ball.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "I'm not saying frequentist would do this, but there there would be a temptation.",
                    "label": 1
                },
                {
                    "sent": "If you follow the rules of frequented statistic to say oh, now we know it's the right hand turn and will proceed as if that's true.",
                    "label": 0
                },
                {
                    "sent": "But of course, it's not true.",
                    "label": 0
                },
                {
                    "sent": "It's possibly true, and.",
                    "label": 1
                },
                {
                    "sent": "One of the key things in uncertain inferences to propagate uncertainty through to other parts of your problem, and we want to make a prediction, and we should propagate through that uncertainty through.",
                    "label": 0
                },
                {
                    "sent": "And the laws of probability probability tell us exactly how.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To do that?",
                    "label": 0
                },
                {
                    "sent": "So the prediction question is what's the chance of getting?",
                    "label": 1
                },
                {
                    "sent": "A red ball, the second time, given that the first ball was blue.",
                    "label": 0
                },
                {
                    "sent": "OK, and then again that's a question of working through the probabilities.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "I say you can look at the slides later, but it turns out the probability of blue followed by red is eleven 40th's and the probability of blue followed by blue is twelve 40th.",
                    "label": 0
                },
                {
                    "sent": "So the ratio is 11 to 12.",
                    "label": 1
                },
                {
                    "sent": "So it's like the odds are slightly against you.",
                    "label": 0
                },
                {
                    "sent": "It's slightly more likely than not that you're going to visit the firing squad because you didn't get a Red Bull a second time.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What about the prediction of the decision question?",
                    "label": 1
                },
                {
                    "sent": "Well, this is a very easy big decision.",
                    "label": 0
                },
                {
                    "sent": "You presumably don't want to go to the firing squad so you will want to try and make that probability of a red ball a bit higher.",
                    "label": 0
                },
                {
                    "sent": "What do you think?",
                    "label": 1
                },
                {
                    "sent": "Better to stick or to switch?",
                    "label": 0
                },
                {
                    "sent": "You seem quite sure we have a show of hands who thinks it's better to switch.",
                    "label": 0
                },
                {
                    "sent": "Who think is you forgot about once and once only?",
                    "label": 0
                },
                {
                    "sent": "Who thinks better to stick?",
                    "label": 0
                },
                {
                    "sent": "Interesting and who's just too confused and OK. Well that's that you work that out if you want.",
                    "label": 0
                },
                {
                    "sent": "Please for tomorrow I think you'll be surprised at the answer.",
                    "label": 0
                },
                {
                    "sent": "Was a clue.",
                    "label": 0
                },
                {
                    "sent": "If you get the answer, and if you're surprised by it, then there's a follow up supplementary question.",
                    "label": 0
                },
                {
                    "sent": "Namely, is this the I made up the Constitution?",
                    "label": 0
                },
                {
                    "sent": "Then I just wrote some random numbers down.",
                    "label": 0
                },
                {
                    "sent": "Did I did I choose it at random?",
                    "label": 0
                },
                {
                    "sent": "did I choose?",
                    "label": 0
                },
                {
                    "sent": "did I try a lot of different cases before I got that particular answer?",
                    "label": 0
                },
                {
                    "sent": "Anyway, there's a few different things there.",
                    "label": 0
                },
                {
                    "sent": "It's a bit surprising.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But the main point main point for now is that probability answers the questions.",
                    "label": 0
                },
                {
                    "sent": "Now you find this stuff all rather silly and you just want to see the algebra.",
                    "label": 0
                },
                {
                    "sent": "Well, this is the algebra of been using.",
                    "label": 0
                },
                {
                    "sent": "But for the inference question, what we've been looking at is the conditional distribution of theater.",
                    "label": 0
                },
                {
                    "sent": "Given why the parameter given why?",
                    "label": 0
                },
                {
                    "sent": "And for the prediction question, what we've been looking at is what's the distribution of the future data White plus given the data, why we worked it out using the laws of probability?",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The way we did it was to do that.",
                    "label": 0
                },
                {
                    "sent": "But we could also have done that, and it's their algebraically algebraically identical so alternative way to think about how you solve the prediction question is to use the posterior you evaluated 1st and use that as it were to mix over the particular distributions given Theta and you get exactly the same.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The idea of Bayesian statistics is.",
                    "label": 0
                },
                {
                    "sent": "Is that magnified is to is we're going to use probability theory consistently to evaluate uncertainty in all sorts of situations where we're interested in dealing with data.",
                    "label": 0
                },
                {
                    "sent": "And we're allowed to do this because the Bayesian view is that everything is a random variable.",
                    "label": 0
                },
                {
                    "sent": "What every possible variable in your system observed, run, observed, observable, and observable things that you think are parameters.",
                    "label": 0
                },
                {
                    "sent": "Things that are states of nature think you know everything hidden.",
                    "label": 0
                },
                {
                    "sent": "Visible missing, whatever it is.",
                    "label": 0
                },
                {
                    "sent": "There are random variables.",
                    "label": 0
                },
                {
                    "sent": "Now, that's not to say that.",
                    "label": 0
                },
                {
                    "sent": "Bayesians think they're all the same philosophically.",
                    "label": 0
                },
                {
                    "sent": "They don't.",
                    "label": 0
                },
                {
                    "sent": "They understand those are different things.",
                    "label": 0
                },
                {
                    "sent": "The future is different from the past and blah blah blah.",
                    "label": 0
                },
                {
                    "sent": "But they're just treated uniformly for mathematical purposes, so we treat them as random variables.",
                    "label": 1
                },
                {
                    "sent": "And it's just quickly, it's worth reminding ourselves that there's different sorts of randomness, and indeed.",
                    "label": 1
                },
                {
                    "sent": "The distinctions can be can get quite blurred, so we sometimes distinguish between epistemological randomness, annalia tree randomness.",
                    "label": 0
                },
                {
                    "sent": "And the first of those is concerned with lack of knowledge.",
                    "label": 1
                },
                {
                    "sent": "Things you don't know.",
                    "label": 0
                },
                {
                    "sent": "And the second is to do with things that are inherently random.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I am indebted to my friend David Spiegelhalter.",
                    "label": 1
                },
                {
                    "sent": "I'm going to make a mess of this.",
                    "label": 0
                },
                {
                    "sent": "A problem is that euro coins don't have.",
                    "label": 0
                },
                {
                    "sent": "Don't have their heads on them, but.",
                    "label": 0
                },
                {
                    "sent": "One of the several things wrong with the euro.",
                    "label": 0
                },
                {
                    "sent": "OK, I I've jumped ahead to the second step, so.",
                    "label": 0
                },
                {
                    "sent": "I'm going to hold up a coin you imagine I hadn't thrown it.",
                    "label": 0
                },
                {
                    "sent": "I fold it up.",
                    "label": 0
                },
                {
                    "sent": "And I asked the audience what's the chance that this will come up heads.",
                    "label": 1
                },
                {
                    "sent": "OK, what's the answer?",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "We're not worried about silly things like calling being biased.",
                    "label": 0
                },
                {
                    "sent": "And we're not worried about yours not aiming heads.",
                    "label": 0
                },
                {
                    "sent": "I then toss the coin, flip it on the back of my hand and I don't reveal it.",
                    "label": 1
                },
                {
                    "sent": "I don't look at it or anything.",
                    "label": 0
                },
                {
                    "sent": "I need to go to flip that I now say what's the probability that is heads?",
                    "label": 0
                },
                {
                    "sent": "OK, so yeah, you're not calling nobody.",
                    "label": 0
                },
                {
                    "sent": "Nobody said anything different from half yet, but you're a little bit slower to say so.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Now I look at it OK and I know now.",
                    "label": 0
                },
                {
                    "sent": "What's the, what's the chance?",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, I know it.",
                    "label": 0
                },
                {
                    "sent": "My chances are now 01.",
                    "label": 0
                },
                {
                    "sent": "What are your chances?",
                    "label": 0
                },
                {
                    "sent": "OK, so some things change.",
                    "label": 0
                },
                {
                    "sent": "Yeah, uncertainties are moved around.",
                    "label": 0
                },
                {
                    "sent": "You're still getting 5050, I think.",
                    "label": 0
                },
                {
                    "sent": "It may take you longer to think to say so, but you don't.",
                    "label": 0
                },
                {
                    "sent": "You still don't have any evidence for one or the other.",
                    "label": 0
                },
                {
                    "sent": "But something all the strangers have.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Because it's no longer random.",
                    "label": 0
                },
                {
                    "sent": "And what's happened is we've moved from its tail.",
                    "label": 0
                },
                {
                    "sent": "By the way, we've moved from something that was purely a tree.",
                    "label": 0
                },
                {
                    "sent": "It was determined by something in the future that was genuinely chance to something that.",
                    "label": 0
                },
                {
                    "sent": "Is only epistemological, it's just to do with something you don't know.",
                    "label": 0
                },
                {
                    "sent": "And you and I think differently about it.",
                    "label": 0
                },
                {
                    "sent": "You thought at certain stage in that experiment you had one view and I had another.",
                    "label": 0
                },
                {
                    "sent": "So it's in the eyes of the beholder, and so on.",
                    "label": 1
                },
                {
                    "sent": "So the uncertainty in there can be of different kinds, but we can use the laws of probability to talk about both kinds and.",
                    "label": 1
                },
                {
                    "sent": "The different kinds can be perceived differently by different by different observers.",
                    "label": 0
                },
                {
                    "sent": "So Bayesian understand all that, but they're still willing to use probability for both purposes, and they use it uniformly and true.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Everything is random variable.",
                    "label": 0
                },
                {
                    "sent": "So the idea is to work throughout with a joint probability distribution you.",
                    "label": 1
                },
                {
                    "sent": "You you put together in one huge model, the joint distribution of everything you don't know everything you will know.",
                    "label": 1
                },
                {
                    "sent": "Whatever their status, where does that come from?",
                    "label": 0
                },
                {
                    "sent": "Well, it's not defined.",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "There's not a right answer is something that's assessed you.",
                    "label": 0
                },
                {
                    "sent": "You come to a scientific judgment about it, your since the result has to obey the laws of probability.",
                    "label": 0
                },
                {
                    "sent": "You will be guided by the laws of probability in doing that.",
                    "label": 1
                },
                {
                    "sent": "And then at some point we'll have some data.",
                    "label": 0
                },
                {
                    "sent": "You know?",
                    "label": 0
                },
                {
                    "sent": "We looked at the coin or whatever it was.",
                    "label": 0
                },
                {
                    "sent": "And we model that process of observation by conditioning this joint distribution on the things we've observed.",
                    "label": 1
                },
                {
                    "sent": "And we then use that condition distribution.",
                    "label": 0
                },
                {
                    "sent": "For everything else, we want to know for our inference for our.",
                    "label": 0
                },
                {
                    "sent": "Predictions for our decisions and so on.",
                    "label": 0
                },
                {
                    "sent": "The probability distribution of the hidden variables given given the visible ones.",
                    "label": 0
                },
                {
                    "sent": "So it's completely clean.",
                    "label": 0
                },
                {
                    "sent": "Or it's a straight forward?",
                    "label": 0
                },
                {
                    "sent": "It's philosophically.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But yeah, there's not.",
                    "label": 0
                },
                {
                    "sent": "There's no particularly difficult philosophical ideas at that level.",
                    "label": 0
                },
                {
                    "sent": "Now I've got quite a long way.",
                    "label": 0
                },
                {
                    "sent": "Without mentioning likelihoods and priors, don't think.",
                    "label": 0
                },
                {
                    "sent": "And in a sense, likelihoods and priors are a little bit.",
                    "label": 1
                },
                {
                    "sent": "Yeah, and they're not really the the front rank of the theory that the main thing we need is.",
                    "label": 0
                },
                {
                    "sent": "This is this joint distribution of observables and unobservables.",
                    "label": 0
                },
                {
                    "sent": "But typically, of course, that joint distribution comes about by combining a likelihood and prior.",
                    "label": 1
                },
                {
                    "sent": "So the standard situation.",
                    "label": 0
                },
                {
                    "sent": "But this orphan Illa problem is you gotta parameter vector theater.",
                    "label": 0
                },
                {
                    "sent": "You got some some data why you will have some numbers.",
                    "label": 1
                },
                {
                    "sent": "And though the two feature and why there may be big vectors, but there there just the two objects we care about and we're going to make inference about theater given why?",
                    "label": 0
                },
                {
                    "sent": "And as I've already said, we're going to.",
                    "label": 0
                },
                {
                    "sent": "We're going to use the conditional distribution of theater given why to make to make that inference?",
                    "label": 0
                },
                {
                    "sent": "And the key thing, of course, is that it's the joint distribution of theater and Y divided by the marginal distribution of Y.",
                    "label": 0
                },
                {
                    "sent": "And that's so far as theater concerned is proportion in theater.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "Conditional distribution we want to use for inference is just proportional to the joint distribution that you decided on.",
                    "label": 0
                },
                {
                    "sent": "And and yeah, we said we could just start with the joint distribution, but in fact we almost always construct that joint distribution by combining the likelihood in the prior.",
                    "label": 0
                },
                {
                    "sent": "So so it is, it is the common the normal thing to do.",
                    "label": 0
                },
                {
                    "sent": "So it's constructed from those two things, the that the marginal distribution for the parameter is the prior.",
                    "label": 1
                },
                {
                    "sent": "The conditional distribution for the data given the parameter is the likelihood if we're modeling in this direction, that's a generative model and then multiplied to give the joint distribution.",
                    "label": 1
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Which is worth, I think I'll just repeat that point that it's the joint distribution of theater and why that's really the key idea.",
                    "label": 0
                },
                {
                    "sent": "And in fact, if you started from that point, you don't even need Bayes theorem to do Bayesian statistics, which is slightly scary thought.",
                    "label": 0
                },
                {
                    "sent": "Now I'm going to talk quite a bit about prize later.",
                    "label": 0
                },
                {
                    "sent": "Where do they come from, but?",
                    "label": 0
                },
                {
                    "sent": "Your mind what I've said about the two sorts of uncertainty.",
                    "label": 0
                },
                {
                    "sent": "Typically.",
                    "label": 0
                },
                {
                    "sent": "Those two factors, the likelihood in the prior, can be quite different in character.",
                    "label": 1
                },
                {
                    "sent": "The prior is often entirely subjective.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 1
                },
                {
                    "sent": "Subjective is a bit of a dirty word.",
                    "label": 0
                },
                {
                    "sent": "I don't like it because it has that pejorative.",
                    "label": 0
                },
                {
                    "sent": "Connotation this is something somehow you've just made up, but it would be much better if we said scientific judgment.",
                    "label": 0
                },
                {
                    "sent": "I think it's much better way to think about subject subjectivity.",
                    "label": 1
                },
                {
                    "sent": "But the prior is very often based on scientific judgment.",
                    "label": 0
                },
                {
                    "sent": "That's not hard, and yet it isn't quantified in a particularly hard way.",
                    "label": 0
                },
                {
                    "sent": "Whereas the likelihood may well be something that's open to empirical evaluation.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, in a lab you could perhaps create data and actually incest assess distributions directly and numerically.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now there are many, many positive things that come out of using probability theory consistently, and this is just a very simple example of that, and it's something we use all the time.",
                    "label": 0
                },
                {
                    "sent": "What happens when data is acquired sequentially?",
                    "label": 0
                },
                {
                    "sent": "OK, so every day you get in you.",
                    "label": 0
                },
                {
                    "sent": "Every day you get a new set of weather and you know we we we we want to update beliefs in the light of what we now know and so.",
                    "label": 1
                },
                {
                    "sent": "Imagine there's some sort of state of nature theater that is our focus of our inference, and that the different data we acquire are conditionally independent, given that.",
                    "label": 1
                },
                {
                    "sent": "In fact then, in that case the the joint distribution is just the product of the.",
                    "label": 0
                },
                {
                    "sent": "The prior theater, multiplied by the light hoods for all the individual data, and that's.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's basically lower probability, so we can infer from that that the conditional distribution we want is proportional to that product.",
                    "label": 0
                },
                {
                    "sent": "I notice that we can break off the last factor and we see that in fact what we're seeing is that the posterior after end data is is the product or proportional to the product of the posterior after N -- 1 data and the likelihood for the for the for the last observation.",
                    "label": 1
                },
                {
                    "sent": "So if you like the prior for the last data point, is the posterior from the step before, so it's very nice sequential.",
                    "label": 1
                },
                {
                    "sent": "Sequential coherence and consistency of the distribution as you learn more stuff.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so I put I put prior likelihood on the table.",
                    "label": 0
                },
                {
                    "sent": "For many people that's where you start.",
                    "label": 0
                },
                {
                    "sent": "But I say I was starting the slightly different place.",
                    "label": 0
                },
                {
                    "sent": "So is there anything else apart from prior likelihood?",
                    "label": 0
                },
                {
                    "sent": "And very importantly, there is.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes there is, and it's not always center stage.",
                    "label": 0
                },
                {
                    "sent": "And this concerns issues about utility and loss.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Often we don't need to think very hard about that.",
                    "label": 0
                },
                {
                    "sent": "If you're content to display or visualize a posterior distribution, and that's it, if that's the focus of your inference, then you don't need to think about anything else.",
                    "label": 1
                },
                {
                    "sent": "But if you wanted to make take decisions or test hypothesis, if you want to do a serious job of estimation, even you have to think about something else.",
                    "label": 1
                },
                {
                    "sent": "And that's leads us into areas of decision theory.",
                    "label": 0
                },
                {
                    "sent": "So decision there is a big subject bit dry.",
                    "label": 0
                },
                {
                    "sent": "I haven't say actually, but we just need a few key ideas from that.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's a theory of utility which says what do we?",
                    "label": 0
                },
                {
                    "sent": "How do we evaluate uncertainty?",
                    "label": 0
                },
                {
                    "sent": "It typically tries to evaluate the cost to us as individuals of being uncertain in monetary terms.",
                    "label": 0
                },
                {
                    "sent": "So we talk about utility owner pseudo money scale.",
                    "label": 0
                },
                {
                    "sent": "Let's try and just formalize this process in a minute.",
                    "label": 0
                },
                {
                    "sent": "We'll see.",
                    "label": 0
                },
                {
                    "sent": "We'll see some practical consequences in a second.",
                    "label": 0
                },
                {
                    "sent": "What's the sequence?",
                    "label": 0
                },
                {
                    "sent": "We observed data why we're going to make a decision on that date based on that data.",
                    "label": 1
                },
                {
                    "sent": "So if it's based on that data, then our decision is a function of the data.",
                    "label": 0
                },
                {
                    "sent": "So D the decision is Delta of Y.",
                    "label": 0
                },
                {
                    "sent": "And then we pay a price and the price we pay is a function of the decision we took.",
                    "label": 1
                },
                {
                    "sent": "And whatever the true state of nature was.",
                    "label": 0
                },
                {
                    "sent": "So that's where loss function is is a function of the decision you took and the true state of nature.",
                    "label": 0
                },
                {
                    "sent": "So it allows us to ask the question, how bad is it to decide?",
                    "label": 1
                },
                {
                    "sent": "Delta why?",
                    "label": 0
                },
                {
                    "sent": "When theater is true.",
                    "label": 0
                },
                {
                    "sent": "And the idea we can use then is to try and minimize our losses.",
                    "label": 0
                },
                {
                    "sent": "And that gives us a means of deciding things.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, let's make this concrete.",
                    "label": 0
                },
                {
                    "sent": "The simplest possible thing you could do is decide between two hypothesis about a parameter.",
                    "label": 0
                },
                {
                    "sent": "It is supposed to just two.",
                    "label": 0
                },
                {
                    "sent": "I've written it as Omega Zero and Omega one, so he's theater in Omega 0.",
                    "label": 0
                },
                {
                    "sent": "Or is the screen Omega one?",
                    "label": 0
                },
                {
                    "sent": "We're going to use data to decide which of those things is true.",
                    "label": 1
                },
                {
                    "sent": "Now we can say we got to take a decision D0 or D1.",
                    "label": 0
                },
                {
                    "sent": "These areas, I think theaters in Omega 0.",
                    "label": 1
                },
                {
                    "sent": "OK, so it's just a true or false situation.",
                    "label": 1
                },
                {
                    "sent": "Nothing could be simpler if you get the right answer.",
                    "label": 0
                },
                {
                    "sent": "Fine, OK, everyone's happy, so there's no loss.",
                    "label": 1
                },
                {
                    "sent": "If you get making mistake, then there's a loss and the losses might be different in the two cases.",
                    "label": 0
                },
                {
                    "sent": "So if you decide DI like 01 if you decide diyan you're wrong, then the price you pay is AI.",
                    "label": 0
                },
                {
                    "sent": "AI is a positive number.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the set up and we now observe why.",
                    "label": 0
                },
                {
                    "sent": "What do you do?",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What's the what's the?",
                    "label": 0
                },
                {
                    "sent": "What was the decision you should take?",
                    "label": 0
                },
                {
                    "sent": "Well, if you knew, theater of course is no problem, but you don't know theater only know why.",
                    "label": 0
                },
                {
                    "sent": "And a natural natural thing to do.",
                    "label": 0
                },
                {
                    "sent": "And indeed it's correct according to sort of metatheory.",
                    "label": 0
                },
                {
                    "sent": "Axiomatic theory of utility is to minimize your expected loss.",
                    "label": 0
                },
                {
                    "sent": "So nervous you minimize.",
                    "label": 0
                },
                {
                    "sent": "L of the theater is your loss, and we integrate that with respect to the posterior distribution.",
                    "label": 1
                },
                {
                    "sent": "But you don't know theater, but you do have the posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "That's the expected loss, and you do the best you can.",
                    "label": 0
                },
                {
                    "sent": "And you can work that out very quickly.",
                    "label": 0
                },
                {
                    "sent": "The expected loss if you choose DI is AI times the probability that you're wrong.",
                    "label": 1
                },
                {
                    "sent": "That's the posterior probability that euro.",
                    "label": 0
                },
                {
                    "sent": "And so this amounts to essentially yes.",
                    "label": 0
                },
                {
                    "sent": "Is it possible to give some idea of why that is optimal?",
                    "label": 0
                },
                {
                    "sent": "It is.",
                    "label": 0
                },
                {
                    "sent": "But you don't, yeah.",
                    "label": 0
                },
                {
                    "sent": "Las Yep.",
                    "label": 0
                },
                {
                    "sent": "Everything.",
                    "label": 0
                },
                {
                    "sent": "Somehow decision theory says you can just choose this path.",
                    "label": 0
                },
                {
                    "sent": "Why can you just use the expectation?",
                    "label": 0
                },
                {
                    "sent": "Well, I think the simplest way to think about is just as a long run thing you know so.",
                    "label": 0
                },
                {
                    "sent": "This is not the only decision you're going to take in your life.",
                    "label": 0
                },
                {
                    "sent": "You're going to take lots.",
                    "label": 0
                },
                {
                    "sent": "Let's suppose you know when you're a teenager, you learn how to take decisions.",
                    "label": 0
                },
                {
                    "sent": "Seems unlikely, but and.",
                    "label": 0
                },
                {
                    "sent": "Stick to that process that this is the law.",
                    "label": 0
                },
                {
                    "sent": "This is the way of this is the choice you would take.",
                    "label": 0
                },
                {
                    "sent": "That would minimize your total loss over your lifetime.",
                    "label": 0
                },
                {
                    "sent": "Because of laws of large numbers.",
                    "label": 0
                },
                {
                    "sent": "That's the simplest explanation.",
                    "label": 0
                },
                {
                    "sent": "Well then you have to.",
                    "label": 0
                },
                {
                    "sent": "Then you have to appeal to ideas of exchangeability and noticed you say this one is typical of the rest.",
                    "label": 0
                },
                {
                    "sent": "And yeah, but you're quite right.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you.",
                    "label": 0
                },
                {
                    "sent": "If you knew you're only ever gonna say one decision in your life.",
                    "label": 0
                },
                {
                    "sent": "Then it's up to you.",
                    "label": 0
                },
                {
                    "sent": "And in particular, you is perfectly rational if you only take one decision in your life to say, well, I can afford to lose a pound, but I can't afford to lose 1000 pounds.",
                    "label": 0
                },
                {
                    "sent": "That's a perfectly rational thing to do.",
                    "label": 0
                },
                {
                    "sent": "Anyway, the effect of that is that that that minimized expected risk issue tells you to threshold the posterior probability.",
                    "label": 0
                },
                {
                    "sent": "It's a very natural answer because if these two potential losses a zero and a one are different in size, so a mistake in One Direction is more important than a mistake or more costly than a mistake in the other.",
                    "label": 0
                },
                {
                    "sent": "Then thresholding the probability using something depending on the AA allows you to take a sensible decision.",
                    "label": 0
                },
                {
                    "sent": "OK, so that of course that optimal decision is a funk.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And of the data when we come to estimation, the decision is a number or vector.",
                    "label": 0
                },
                {
                    "sent": "Is the the value you're guessing for theater.",
                    "label": 1
                },
                {
                    "sent": "Let's call it Theta hat.",
                    "label": 0
                },
                {
                    "sent": "Ann and common choices is quadratic loss.",
                    "label": 1
                },
                {
                    "sent": "Let's say we take the squared difference between what you guessed and what it really was.",
                    "label": 0
                },
                {
                    "sent": "And when you when you do the maths, when you do that integration, it turns out the the expected.",
                    "label": 0
                },
                {
                    "sent": "The expected losses then simply the if you like the bias, the difference between theater hat and the posterior expectation and plus the variance.",
                    "label": 1
                },
                {
                    "sent": "Of the posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "That's straightforward.",
                    "label": 0
                },
                {
                    "sent": "In the square.",
                    "label": 0
                },
                {
                    "sent": "So given that this posterior is something that's fixed today with your decision, the best we can possibly do is choose Theta hat to equal the posterior expectation.",
                    "label": 1
                },
                {
                    "sent": "So that's the justification for using posterior expectation as a summary of a posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "It holds more generally it holds for in vector case as well.",
                    "label": 0
                },
                {
                    "sent": "But it's more important to remember is the only justification for using the posterior mean.",
                    "label": 0
                },
                {
                    "sent": "Posterior mean has really no other.",
                    "label": 0
                },
                {
                    "sent": "Has no other philosophical meaning except as the thing that minimizes the quadratic loss, and perhaps perhaps, as with the yesterday with the testing situation, perhaps that quadratic symmetric.",
                    "label": 0
                },
                {
                    "sent": "Loss function doesn't capture the cost to you of the mistakes you might make.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A very common thing position to find yourself in is that overestimating and underestimating are not equally costly.",
                    "label": 0
                },
                {
                    "sent": "OK, so imagine.",
                    "label": 0
                },
                {
                    "sent": "If the world's financial mathematicians had behaved.",
                    "label": 0
                },
                {
                    "sent": "Without that sort of cartoon, yeah, that sort of thought in mind.",
                    "label": 0
                },
                {
                    "sent": "Perhaps losses in One Direction or differently different than losses in another.",
                    "label": 0
                },
                {
                    "sent": "So let's suppose overestimation is expensive then.",
                    "label": 1
                },
                {
                    "sent": "A loss function like this might be appropriate.",
                    "label": 0
                },
                {
                    "sent": "Where tour here is a number between zero and one, and in the case I'm talking about tour will be less than 0.",
                    "label": 0
                },
                {
                    "sent": "Less than half.",
                    "label": 0
                },
                {
                    "sent": "This is the one thing I got wrong in the slides that were on the web by the way, they corrected in that version, but I got the I got these two things the wrong way around, so if you.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'm assuming will download a corrected version later.",
                    "label": 0
                },
                {
                    "sent": "But if you do that, you play that game.",
                    "label": 0
                },
                {
                    "sent": "It turns out the posterior expected loss is a particular quantile, sensible quantile.",
                    "label": 1
                },
                {
                    "sent": "The 100 tour percentile of the posterior distribution.",
                    "label": 1
                },
                {
                    "sent": "So that has the correct effect.",
                    "label": 1
                },
                {
                    "sent": "You know if you.",
                    "label": 0
                },
                {
                    "sent": "If if overestimation was that, say, three times more expensive than underestimation, then you would choose the lower quartile of your distribution.",
                    "label": 1
                },
                {
                    "sent": "So this decision theory idea allows you to properly think about it.",
                    "label": 0
                },
                {
                    "sent": "Forces you to probably think about the costs of different kinds of error and and to bring that into your procedures.",
                    "label": 0
                },
                {
                    "sent": "And this choice of the quadratic loss is not absolutely inevitable.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I don't really want to say much about frequentist theory except except because of the nice little punch line at the bottom here.",
                    "label": 0
                },
                {
                    "sent": "Frequentis also think about about decision theory and this really comes back to your question.",
                    "label": 0
                },
                {
                    "sent": "You know they don't think about expectations of loss functions automatically because they wouldn't have any reason to take expectations with respect to the posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "What they might think about is taking expectation of the loss.",
                    "label": 1
                },
                {
                    "sent": "Under the likelihood.",
                    "label": 0
                },
                {
                    "sent": "OK, for each theater, if theater were Noble.",
                    "label": 0
                },
                {
                    "sent": "And what in regarding regard why is uncertain, then we could look at the look at the expectation over Y.",
                    "label": 0
                },
                {
                    "sent": "And this idea that you could excuse me, you could afford to lose a pound or not a not 1000 pounds that speaks to the idea that what we're trying to do is minimize the maximum loss we could make.",
                    "label": 1
                },
                {
                    "sent": "And that's so called Minimax decision rule.",
                    "label": 1
                },
                {
                    "sent": "And that's one of the.",
                    "label": 0
                },
                {
                    "sent": "One of the driving fundamental principles of much of frequentist theory.",
                    "label": 0
                },
                {
                    "sent": "Now there's this idea that you would be stupid to use a decision rule that is.",
                    "label": 1
                },
                {
                    "sent": "Beaten by another decision rule, whatever the value of theater.",
                    "label": 0
                },
                {
                    "sent": "And then, rather remarkably, with, with a little bit of a caveat about.",
                    "label": 0
                },
                {
                    "sent": "Mathematical regularity every admissible rule is actually a Bayes rule.",
                    "label": 1
                },
                {
                    "sent": "So in optimal decision theory, this is one point where frequentists are forced to be Bayesian.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, I want to say it about priors.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "If you go back to the early days of Bayesian statistics.",
                    "label": 0
                },
                {
                    "sent": "Then this issue is terribly important.",
                    "label": 0
                },
                {
                    "sent": "You know, before we had before we had computers that didn't really cost anything.",
                    "label": 0
                },
                {
                    "sent": "It was really important to be able to in order to use, but the Bayesian method is really important about to do the integrals.",
                    "label": 0
                },
                {
                    "sent": "And that really ties your hands in in what the forms of the functions could possibly be, and a particular choice for certain.",
                    "label": 0
                },
                {
                    "sent": "Many standard likelihood functions a particular choice of prior gives you huge algebraic and computational advantages, so this is the idea of conjugacy.",
                    "label": 1
                },
                {
                    "sent": "So prior is conjugate.",
                    "label": 0
                },
                {
                    "sent": "If basically it gets updated in a neat way so that the form of the posterior, the normal distribution, or exponential or whatever the form of the distribution doesn't change just the parameters of that distribution get updated.",
                    "label": 0
                },
                {
                    "sent": "So for one very simple example, let's take a take the case of the professional distribution for the data.",
                    "label": 0
                },
                {
                    "sent": "OK, this is a personal distribution.",
                    "label": 0
                },
                {
                    "sent": "Now we could take various possible priors for the parameter theater.",
                    "label": 0
                },
                {
                    "sent": "It's a non negative real number, so let's let's try various things.",
                    "label": 1
                },
                {
                    "sent": "But if we chose the gamma prior gamma distribution which I've drawn there then and then go through the Bayes.",
                    "label": 0
                },
                {
                    "sent": "Go through Bayes rules, we find the posterior distribution is again of exactly the same form.",
                    "label": 0
                },
                {
                    "sent": "And what's happened is that the Priors got updated to the posterior simply by tweaking the parameters we started off with Gamma, Alpha, Beta and now we have gamma Alpha plus Y beta plus one.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now think about now what would happen if you took a second observation.",
                    "label": 1
                },
                {
                    "sent": "Again, I told you about sequential acquisition of data while the prior inside the posterior after the first observation is that for the next, so you can see what happens is that we just get the new observations, gets added on, and the beta parameter gets added further on again.",
                    "label": 0
                },
                {
                    "sent": "And that's a very nice idea, because it tells us something about how to interpret prior distributions in this case, because you can see that effectively.",
                    "label": 0
                },
                {
                    "sent": "What would have eventually is Alpha plus the sum of all the data and beta plus the number of observations.",
                    "label": 0
                },
                {
                    "sent": "So that's just like starting from zero in both 0 total observations, 0 number of observations and then.",
                    "label": 0
                },
                {
                    "sent": "Where we start is to imagine that there are a total.",
                    "label": 1
                },
                {
                    "sent": "There are beater observations and their total is Alpha.",
                    "label": 0
                },
                {
                    "sent": "So priors can then be thought of as as like prior data.",
                    "label": 1
                },
                {
                    "sent": "It's like data you had before you started observing anything, and that's quite a useful trick if you're trying to elicit a prior from someone who's reluctant to give one.",
                    "label": 0
                },
                {
                    "sent": "That's quite a good way of pull out from him or her what the.",
                    "label": 0
                },
                {
                    "sent": "Will be kind of uncertainty.",
                    "label": 1
                },
                {
                    "sent": "They think about theater radius.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 1
                },
                {
                    "sent": "This used to be a really important principle because you couldn't.",
                    "label": 1
                },
                {
                    "sent": "You couldn't do Bayesian statistics except in this sort of simple situation.",
                    "label": 0
                },
                {
                    "sent": "It's become like much less important now.",
                    "label": 0
                },
                {
                    "sent": "There's still some temptation sometimes to use.",
                    "label": 0
                },
                {
                    "sent": "Conditionally conjugate priors.",
                    "label": 0
                },
                {
                    "sent": "So in a big big model.",
                    "label": 0
                },
                {
                    "sent": "You know at least, conditional on some other parameters.",
                    "label": 1
                },
                {
                    "sent": "Maybe the prior going to use is is is conjugate and that sort of thing can make computation little bit little bit neater.",
                    "label": 0
                },
                {
                    "sent": "But I would.",
                    "label": 0
                },
                {
                    "sent": "I would say you know, really this is history and it's a mistake to let that kind of consideration.",
                    "label": 1
                },
                {
                    "sent": "Persuade you to use a prior that doesn't reflect what you really think.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we can't say much more about prize without distinguishing two different subspecies of Bayesians, and these are the subjective ones and the objective ones.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "The subjective ones.",
                    "label": 0
                },
                {
                    "sent": "A kind of ideally idealist living on a Hill, and the objective ones are the ones who have to get on with reality is in some sense subjective ones.",
                    "label": 0
                },
                {
                    "sent": "Take the view that.",
                    "label": 0
                },
                {
                    "sent": "Providing we work hard to think about all our probabilities and write things down consistently.",
                    "label": 0
                },
                {
                    "sent": "OK, and we really draw best.",
                    "label": 0
                },
                {
                    "sent": "Then use Bayes theorem properly.",
                    "label": 1
                },
                {
                    "sent": "Then all the probabilities we come up with properly represent our personal degrees of belief in the uncertainties we still have.",
                    "label": 1
                },
                {
                    "sent": "OK, very clean and nobody else can really say anything about that.",
                    "label": 0
                },
                {
                    "sent": "If you really believe.",
                    "label": 0
                },
                {
                    "sent": "The sun won't rise tomorrow, or that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the euro will go up tomorrow or whatever it is.",
                    "label": 0
                },
                {
                    "sent": "That's your beliefs are your beliefs, and that's fine.",
                    "label": 0
                },
                {
                    "sent": "The objective view is that achieving that in complex problems is just too hard.",
                    "label": 1
                },
                {
                    "sent": "OK, you can be.",
                    "label": 0
                },
                {
                    "sent": "You could be fully coherent, properly understand your own uncertainties when there's a small number of variables.",
                    "label": 1
                },
                {
                    "sent": "Or perhaps you can't when there's a lot.",
                    "label": 1
                },
                {
                    "sent": "So the objective Bayesian takes the view.",
                    "label": 0
                },
                {
                    "sent": "You'll be inevitably forced into simplifying things.",
                    "label": 1
                },
                {
                    "sent": "You're going to have to approximate your beliefs by something to something you can work with.",
                    "label": 1
                },
                {
                    "sent": "In particular, you often have to assume lots of things are independent when you really don't know they are.",
                    "label": 0
                },
                {
                    "sent": "And their view kind of leads into saying that conditional probabilities don't represent ordinary judgments in the actual sense, but simply quantifying the extent to which one.",
                    "label": 0
                },
                {
                    "sent": "Event logically determines another is a sort of necessity about it rather than.",
                    "label": 0
                },
                {
                    "sent": "Rather than the sort of best guess.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that emphasis now switches into choosing priors to have minimal impact on posterior inference rather than properly quantifying prior beliefs.",
                    "label": 0
                },
                {
                    "sent": "So it's a little coming from another different place.",
                    "label": 0
                },
                {
                    "sent": "I think for interested I might skip over this, but there's been some effort to try and objectify the choice of priors in objective Bayesian inference.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And the particular you might come across Jeffreys priors and reference prior another maximum other entropy based priors.",
                    "label": 1
                },
                {
                    "sent": "So these are all attempts to try and write down priors that don't typically quantify anything anybody actually thinks, but nevertheless have this property of being kind of like vanilla.",
                    "label": 1
                },
                {
                    "sent": "They don't.",
                    "label": 0
                },
                {
                    "sent": "They don't sort of.",
                    "label": 0
                },
                {
                    "sent": "Influence, influence, influence too much and then particularly popular to do things like this when you're trying to compare models for example.",
                    "label": 0
                },
                {
                    "sent": "But it's still an ongoing quest.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is not sorry.",
                    "label": 0
                },
                {
                    "sent": "Here's.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "There is also GPL spray or is working.",
                    "label": 1
                },
                {
                    "sent": "Do I have any intuition?",
                    "label": 0
                },
                {
                    "sent": "Oh well, the particular question the Jeffreys prior is trying to to deal with is the question mentioned here.",
                    "label": 0
                },
                {
                    "sent": "If you if you and I think the same thing about something let's but do you think naturally in terms of the standard deviation of a distribution?",
                    "label": 0
                },
                {
                    "sent": "I think naturally about the variance.",
                    "label": 0
                },
                {
                    "sent": "There ought to be a way that you could write down your relative ignorance about Sigma, and I can write down my ignorance about Sigma squared and it wouldn't matter.",
                    "label": 0
                },
                {
                    "sent": "You know that the answer you get in the end would would would be the same.",
                    "label": 0
                },
                {
                    "sent": "Yeah, mine would be the square of yours.",
                    "label": 0
                },
                {
                    "sent": "That's a very.",
                    "label": 0
                },
                {
                    "sent": "That's quite a strong condition.",
                    "label": 1
                },
                {
                    "sent": "Actually doesn't sound much, but it's quite strong condition of what the prior can be, and that's where Jeffreys prior comes from, so that's why it uses the influence the information matrix.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "The problem with all of these things is the problem down at the bottom is that these are all priors that depend on the likelihood.",
                    "label": 0
                },
                {
                    "sent": "In some way.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "If we if we wanted to think of prior information.",
                    "label": 0
                },
                {
                    "sent": "As being what we knew.",
                    "label": 0
                },
                {
                    "sent": "About theater, before we had any data.",
                    "label": 0
                },
                {
                    "sent": "OK, that's what prior means.",
                    "label": 0
                },
                {
                    "sent": "Surely it would also ought to be what we think about theater before we know what kind of data we're going to get.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is a real thing.",
                    "label": 0
                },
                {
                    "sent": "I think this about it tomorrow, today.",
                    "label": 0
                },
                {
                    "sent": "Tomorrow I'm going to find out if I'm going to have a normally distributed.",
                    "label": 0
                },
                {
                    "sent": "Observation, whose mean is theater or price on distributed observations, meaning theater.",
                    "label": 0
                },
                {
                    "sent": "I'm only going to find that out tomorrow, but what do I think today about theater?",
                    "label": 0
                },
                {
                    "sent": "So these views of setting priors don't allow you to have a prior in that situation.",
                    "label": 0
                },
                {
                    "sent": "That's quite a. Troubling thought.",
                    "label": 0
                },
                {
                    "sent": "So this is this is really an active area, particularly we don't know how to do objective priors properly in large dimensional problems, and it's an active research area and there isn't a clear answer at the moment.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So we need maximum entropy prize.",
                    "label": 0
                },
                {
                    "sent": "You need to know the form of the line here, or you can yes.",
                    "label": 0
                },
                {
                    "sent": "I guess my understanding that yeah, now these prices are often improper.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Improper and improper distribution in probability is one that's you know, it's positive and negative, but is integral is infinite or not not not one.",
                    "label": 0
                },
                {
                    "sent": "OK. Like a normal distribution with different variance.",
                    "label": 0
                },
                {
                    "sent": "Um, are you allowed to use those?",
                    "label": 0
                },
                {
                    "sent": "Well, yes, you are.",
                    "label": 0
                },
                {
                    "sent": "But be careful.",
                    "label": 0
                },
                {
                    "sent": "That's what this slide says.",
                    "label": 0
                },
                {
                    "sent": "Improper priors are only safe to use if they have this nice property that there.",
                    "label": 0
                },
                {
                    "sent": "They can be viewed as the limits of proper priors.",
                    "label": 0
                },
                {
                    "sent": "And the posterior you get is is correctly the limit of the corresponding series of posteriors, so.",
                    "label": 0
                },
                {
                    "sent": "This is this is a bit of advice that's not taken by anybody, but nonetheless I'll give it, you know, that's the that's the test you have to apply every time you think about using an improper prior, does it?",
                    "label": 0
                },
                {
                    "sent": "Does it behave properly in this sense, 'cause it doesn't always.",
                    "label": 0
                },
                {
                    "sent": "That's not always true.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, I'm going to use the rest of this first session to talk about some.",
                    "label": 0
                },
                {
                    "sent": "Principles of modeling.",
                    "label": 0
                },
                {
                    "sent": "How do you model?",
                    "label": 0
                },
                {
                    "sent": "Well, it's a little difficult to do this in a kind of generic way, right?",
                    "label": 0
                },
                {
                    "sent": "I don't have a particular thing in front of me.",
                    "label": 0
                },
                {
                    "sent": "A nuclear power station or a financial system or something so.",
                    "label": 0
                },
                {
                    "sent": "But I wanted to keep it general, So what I'm going to do is talk about some themes that you see quite a lot when people think about modeling.",
                    "label": 0
                },
                {
                    "sent": "So these are quite generic ideas in.",
                    "label": 0
                },
                {
                    "sent": "In the people use when building models.",
                    "label": 0
                },
                {
                    "sent": "Weather there.",
                    "label": 0
                },
                {
                    "sent": "Statisticians or machine learners, I think.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the first of these is to do is called hierarchical modeling and just introducing this very informally.",
                    "label": 0
                },
                {
                    "sent": "If you have a bunch of related things.",
                    "label": 0
                },
                {
                    "sent": "Um, other rules about how you should think about.",
                    "label": 0
                },
                {
                    "sent": "Modeling them jointly.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Something concrete.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So this is a little sort of medical stats type example.",
                    "label": 0
                },
                {
                    "sent": "So you got 12 hospitals carrying out certain kind of operation on this.",
                    "label": 1
                },
                {
                    "sent": "Actually, this is based on data on infant cardiac surgery, so these are little babies very ill with heart defects they need.",
                    "label": 0
                },
                {
                    "sent": "They need some surgery to live and it's almost collected statistics on how successful that surgery is in 12 different hospitals.",
                    "label": 0
                },
                {
                    "sent": "OK, and we have two hospitals and their different respective.",
                    "label": 0
                },
                {
                    "sent": "How many operations they've done and what was the mortality rate.",
                    "label": 0
                },
                {
                    "sent": "So for example hospital A.",
                    "label": 0
                },
                {
                    "sent": "Everybody, every baby lived OK, perfect, but they didn't do many operations.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "So let's think about some questions.",
                    "label": 0
                },
                {
                    "sent": "That might arise there.",
                    "label": 1
                },
                {
                    "sent": "Which are the best and worst hospitals?",
                    "label": 1
                },
                {
                    "sent": "Are the difference is significant?",
                    "label": 1
                },
                {
                    "sent": "What rate would you expect in the 13th hospital?",
                    "label": 0
                },
                {
                    "sent": "You mentioned there was actually another one.",
                    "label": 0
                },
                {
                    "sent": "Do you have any information now about?",
                    "label": 0
                },
                {
                    "sent": "Infant cardiac surgery mortality in the 13th hospital.",
                    "label": 1
                },
                {
                    "sent": "Or perhaps what about the 12th, just the 12th hospital in a different year?",
                    "label": 0
                },
                {
                    "sent": "To what extent is this data tell us about something that doesn't directly tell us about?",
                    "label": 0
                },
                {
                    "sent": "Now before I showed you this.",
                    "label": 0
                },
                {
                    "sent": "Picture you would have had no idea.",
                    "label": 0
                },
                {
                    "sent": "Weather mortality ratio in this situation is typically 1% point, one percent, 20%.",
                    "label": 0
                },
                {
                    "sent": "80% yes, right?",
                    "label": 0
                },
                {
                    "sent": "The reason why you should know I haven't even explained my husband bought operation.",
                    "label": 0
                },
                {
                    "sent": "It was but OK, you wouldn't know, but I guess you do know something now.",
                    "label": 0
                },
                {
                    "sent": "If you were really forced to give a rate for the 13th hospital, I guess most people would say it's about 7%.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Would anyone say I still know absolutely nothing?",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, let's think about how to think about that.",
                    "label": 0
                },
                {
                    "sent": "Statistically so.",
                    "label": 0
                },
                {
                    "sent": "We need a model so the natural model is that the data are normally distributed OK. Each hospital is.",
                    "label": 1
                },
                {
                    "sent": "Conducted NI operations, the chance of Chance of Death was Theater I each time, so it's it's like tossing a biased coin.",
                    "label": 0
                },
                {
                    "sent": "So there's various possibilities.",
                    "label": 1
                },
                {
                    "sent": "The we might say that.",
                    "label": 0
                },
                {
                    "sent": "There is just one theater for all hospitals.",
                    "label": 0
                },
                {
                    "sent": "The mortality rate for.",
                    "label": 0
                },
                {
                    "sent": "This collection of hospitals is theater.",
                    "label": 0
                },
                {
                    "sent": "So these are binomial distributions with the same anion, different and different enemies in the same theater.",
                    "label": 0
                },
                {
                    "sent": "The second possibility is that they're all different.",
                    "label": 0
                },
                {
                    "sent": "Now, in the first case, we can't distinguish hospitals 'cause they work the same same body.",
                    "label": 0
                },
                {
                    "sent": "To the best we can possibly do with that data is clump all the wires together.",
                    "label": 1
                },
                {
                    "sent": "Clamp all the ends together and just say this is 1 binomial experiment.",
                    "label": 0
                },
                {
                    "sent": "In the second case we can't do any combination.",
                    "label": 0
                },
                {
                    "sent": "These are different hospitals.",
                    "label": 0
                },
                {
                    "sent": "They will have different theaters and that 13th hospital will have a different theater as well.",
                    "label": 0
                },
                {
                    "sent": "So we still know nothing about the heart rate, mortality, heart surgery, mortality in that hospital.",
                    "label": 0
                },
                {
                    "sent": "And surely the truth is somewhere in between.",
                    "label": 0
                },
                {
                    "sent": "The hospitals aren't the same.",
                    "label": 0
                },
                {
                    "sent": "OK, but we do know something from one about the other.",
                    "label": 0
                },
                {
                    "sent": "And this idea of hierarchical modeling allows us to fill in that gap.",
                    "label": 0
                },
                {
                    "sent": "So I think.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I won't go through these in maximum detail, but just to spell it all out in the slides and you can refer to them later.",
                    "label": 0
                },
                {
                    "sent": "I first look at the.",
                    "label": 0
                },
                {
                    "sent": "In this, in a non Bayesian way.",
                    "label": 0
                },
                {
                    "sent": "OK, so we get some estimates 10739 and we get.",
                    "label": 0
                },
                {
                    "sent": "Individual hospital estimates vary from zero up to 14.4%.",
                    "label": 0
                },
                {
                    "sent": "If we wanted to, we could test the hypothesis that they're all the same, and they very definitely aren't.",
                    "label": 0
                },
                {
                    "sent": "There's a very, very tiny probability that you'd get.",
                    "label": 0
                },
                {
                    "sent": "Um, you know?",
                    "label": 0
                },
                {
                    "sent": "You get variations as extreme as that.",
                    "label": 0
                },
                {
                    "sent": "If the theaters were all the same.",
                    "label": 0
                },
                {
                    "sent": "And that's pretty much where you have to stop.",
                    "label": 0
                },
                {
                    "sent": "I think is a frequentist.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You could try and do both of those.",
                    "label": 0
                },
                {
                    "sent": "Things are Bayesian way.",
                    "label": 0
                },
                {
                    "sent": "You could say, well, I have a bit of information about theater before I started, perhaps I know what the.",
                    "label": 0
                },
                {
                    "sent": "Mortality rate was last year.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to use that as a prior distribution from my analysis of these data and you have the same pair of choices.",
                    "label": 0
                },
                {
                    "sent": "You can say, well, there's a single theater.",
                    "label": 0
                },
                {
                    "sent": "I know a little bit about.",
                    "label": 0
                },
                {
                    "sent": "And I'll use that for the current year's data.",
                    "label": 0
                },
                {
                    "sent": "Or you could say there's 12 different theaters.",
                    "label": 0
                },
                {
                    "sent": "I know a little bit about that I could use that.",
                    "label": 0
                },
                {
                    "sent": "And it's pretty much yes.",
                    "label": 0
                },
                {
                    "sent": "Oh absolutely, I know.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's absolutely Fairpoint now.",
                    "label": 0
                },
                {
                    "sent": "I agree with that.",
                    "label": 0
                },
                {
                    "sent": "I mean your prior distribution, yours or mine, is is the result of thinking about your uncertainties and and you know and and quantifying the best you can.",
                    "label": 0
                },
                {
                    "sent": "I completely agree.",
                    "label": 0
                },
                {
                    "sent": "And for example, you might probably you're not 'cause you're not a, you know.",
                    "label": 0
                },
                {
                    "sent": "Surgeon, another my but you know you might know something about how it should have changed from last year.",
                    "label": 0
                },
                {
                    "sent": "OK, so your best guess might be a little bit late.",
                    "label": 0
                },
                {
                    "sent": "Last year only.",
                    "label": 0
                },
                {
                    "sent": "It's a bit better.",
                    "label": 0
                },
                {
                    "sent": "Whatever it is.",
                    "label": 0
                },
                {
                    "sent": "I'm not saying you have to use last year's data by any means.",
                    "label": 0
                },
                {
                    "sent": "It's an example of what you might use in building a prior.",
                    "label": 0
                },
                {
                    "sent": "But so, but the mathematical point is either of these choices still leads you to the same position that you'd not combining the data.",
                    "label": 0
                },
                {
                    "sent": "You can either combine the completely or not at all, but you can't do anything in between and the effects are simply that you slightly regularize the estimates.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "For example.",
                    "label": 0
                },
                {
                    "sent": "Hospital A we really annoyed about this.",
                    "label": 0
                },
                {
                    "sent": "'cause it had this perfect rate before you had this perfect.",
                    "label": 0
                },
                {
                    "sent": "Didn't kill anybody position before, but now these bayesians are telling us we have a 4% mortality rate.",
                    "label": 0
                },
                {
                    "sent": "Because bringing in the prior information is reflecting the flat well, they were a bit lucky they didn't do very many operations.",
                    "label": 0
                },
                {
                    "sent": "They were a bit lucky too.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "A bit lucky to get away with it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, let me come back to this point.",
                    "label": 0
                },
                {
                    "sent": "I'm not for a minute suggesting this is a serious analysis of these data.",
                    "label": 0
                },
                {
                    "sent": "I'm trying to show you the.",
                    "label": 0
                },
                {
                    "sent": "The implications of different styles of modeling.",
                    "label": 0
                },
                {
                    "sent": "And of course, if you are really in the position of monitoring public health, you would take the process of price choice very seriously.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Um, which is best when either of those is best?",
                    "label": 1
                },
                {
                    "sent": "Really, they both have problems.",
                    "label": 0
                },
                {
                    "sent": "'cause you've only got the data from hospital.",
                    "label": 1
                },
                {
                    "sent": "I used in inferring theater I.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Here's a here's a slightly different way to think about it.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Let's imagine that.",
                    "label": 0
                },
                {
                    "sent": "These twelve hospitals are not the only hospitals in the world.",
                    "label": 1
                },
                {
                    "sent": "These 12 hospitals are themselves a sample from a collection of hospitals.",
                    "label": 1
                },
                {
                    "sent": "And that across that collection of hospitals there is variation in theater I.",
                    "label": 0
                },
                {
                    "sent": "So that's one way.",
                    "label": 0
                },
                {
                    "sent": "That's one way to give a concrete as it were interpretation of what a prior distribution might mean.",
                    "label": 0
                },
                {
                    "sent": "It's really, it's not a.",
                    "label": 0
                },
                {
                    "sent": "It's not just a degree of belief.",
                    "label": 0
                },
                {
                    "sent": "Now it's a model about variation across the different hospitals.",
                    "label": 0
                },
                {
                    "sent": "Let's suppose we take this beta distribution.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Will we be in the same position of how do we choose values for Alpha and beta?",
                    "label": 0
                },
                {
                    "sent": "Well, there's a so called empirical Bayes approach.",
                    "label": 1
                },
                {
                    "sent": "I'll say a little bit more about this tomorrow where you get your Alpha and beta from analyzing this year's data.",
                    "label": 0
                },
                {
                    "sent": "So you do something like this.",
                    "label": 0
                },
                {
                    "sent": "I'm not advocating this, so the details are not important, but you take something like this year's data.",
                    "label": 0
                },
                {
                    "sent": "You did summarize it, you'd find means and variations, means, variances and you use that.",
                    "label": 0
                },
                {
                    "sent": "That observed empirical variation in the mortality rates as a guide to the population variation in hospitals.",
                    "label": 0
                },
                {
                    "sent": "Sounds like a sword.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The sensible thing to do.",
                    "label": 0
                },
                {
                    "sent": "The difficulty with it mathematically and principle is that you're using the data twice.",
                    "label": 1
                },
                {
                    "sent": "You'll be essentially using the data to decide your prior, and you use that that that prior to analyze the data and so each bit of data comes into the analysis twice, and the effect of that is a bit like saying we got twice as much data as we really have.",
                    "label": 0
                },
                {
                    "sent": "Of course, you have more data, you have less variation, so the effect is that you're going to estimate how precise you really are.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the the the hierarchal approach avoids all these horrible pitfalls.",
                    "label": 0
                },
                {
                    "sent": "If it avoids you having to take an extreme view about whether hospitals are separate or all the same and it avoids you recognizing the differences in the without counting data twice.",
                    "label": 1
                },
                {
                    "sent": "And what it comes to is saying, well, there's another level of variation here.",
                    "label": 0
                },
                {
                    "sent": "There are really 3 levels of variation.",
                    "label": 1
                },
                {
                    "sent": "Stop this parameters and data.",
                    "label": 1
                },
                {
                    "sent": "There are three levels.",
                    "label": 0
                },
                {
                    "sent": "This is the actual hazard of this kind of operation.",
                    "label": 1
                },
                {
                    "sent": "How dangerous is this operation?",
                    "label": 0
                },
                {
                    "sent": "We don't know, so that's uncertain.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's a random variable.",
                    "label": 0
                },
                {
                    "sent": "Then this variability in the actual rate that a particular hospital.",
                    "label": 0
                },
                {
                    "sent": "Has practice.",
                    "label": 1
                },
                {
                    "sent": "So that some particular hospitals might be more or less risky because they have.",
                    "label": 0
                },
                {
                    "sent": "And then the surgeons have better practice, or worse, practice whatever it is.",
                    "label": 0
                },
                {
                    "sent": "And then this chance factors the individual patients outcome.",
                    "label": 0
                },
                {
                    "sent": "You know, even if you know that theater exactly .07, that doesn't predict the actual outcome for any single single baby.",
                    "label": 0
                },
                {
                    "sent": "So we have three real levels of variation, and that the idea of hierarchical modeling is to model those three.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Random variables and put them all together.",
                    "label": 0
                },
                {
                    "sent": "So the approach in this case might be to put priors on Alpha and beta.",
                    "label": 0
                },
                {
                    "sent": "So Alpha and Beta are the are the parameters that describe the population variation.",
                    "label": 0
                },
                {
                    "sent": "But as the population of hospitals that that variation in in the individual mortality rates and we put priors on Alpha and beta themselves but three left.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the model.",
                    "label": 0
                },
                {
                    "sent": "And the effect of that is that you get some what we call it.",
                    "label": 0
                },
                {
                    "sent": "Borrowing a strength, it provides a principled way of sharing information between the different hospitals.",
                    "label": 0
                },
                {
                    "sent": "Cause we don't know what Alpha and beta is, but the first 11 hospitals.",
                    "label": 0
                },
                {
                    "sent": "Lead to US learning something about Alpha and Beta, and that information propagates down to things we say about the 12th hospital.",
                    "label": 0
                },
                {
                    "sent": "So this is a common theme in statistical modeling that we think about.",
                    "label": 0
                },
                {
                    "sent": "Variation properly at different levels and certain we consider adding an extra layer and the effect of that is to have a system have a method of mathematical.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Method for sharing information across.",
                    "label": 0
                },
                {
                    "sent": "Across across units.",
                    "label": 0
                },
                {
                    "sent": "Now, I don't say very much about graphical models, 'cause 'cause Martin Wainwright's going to next week.",
                    "label": 0
                },
                {
                    "sent": "But these these different choices can be symbolized by these pictures.",
                    "label": 0
                },
                {
                    "sent": "These are called directed acyclic graphs.",
                    "label": 1
                },
                {
                    "sent": "In these pictures, the circles are variables that we know.",
                    "label": 0
                },
                {
                    "sent": "The random variables that.",
                    "label": 0
                },
                {
                    "sent": "We don't know, and also eventually things like the data that we will at some point now.",
                    "label": 0
                },
                {
                    "sent": "And that the blocks the squares represent replication plates as they called.",
                    "label": 0
                },
                {
                    "sent": "And they say these are the different least the different situations, and so the hierarchical model is the one with this 3 levels.",
                    "label": 0
                },
                {
                    "sent": "Of nodes with circles in them.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In some cases you can work things out.",
                    "label": 0
                },
                {
                    "sent": "The effect of this out algebraically, so I put in.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just a quick couple of slides there about how it works in the normal case, but if you want to see that.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's look at that later.",
                    "label": 0
                },
                {
                    "sent": "So that's the first principle that we commonly see in in statistical modeling.",
                    "label": 0
                },
                {
                    "sent": "This idea of hierarchical modeling.",
                    "label": 0
                },
                {
                    "sent": "It also illustrates a second principle, which is which is.",
                    "label": 0
                },
                {
                    "sent": "Can be very important and that's the idea of exchangeability.",
                    "label": 0
                },
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "Conditional independence and exchangeability.",
                    "label": 0
                },
                {
                    "sent": "There are the two most important things in.",
                    "label": 0
                },
                {
                    "sent": "In in Bayesian statistics, in my view.",
                    "label": 0
                },
                {
                    "sent": "So Exchangeability represents the is a way of encoding the idea that we find no systematic reason to distinguish between a collection of random variables.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Suppose you came to this kind of.",
                    "label": 0
                },
                {
                    "sent": "Data for the first time.",
                    "label": 0
                },
                {
                    "sent": "You're told we're gonna look at 12 hospitals.",
                    "label": 0
                },
                {
                    "sent": "OK, you don't know anything about them.",
                    "label": 0
                },
                {
                    "sent": "You perhaps you know something about.",
                    "label": 0
                },
                {
                    "sent": "Infant hearts mortality and the surgery.",
                    "label": 0
                },
                {
                    "sent": "If you don't think about the hospitals so you have no reason to think anything different about theater one or Theater 3.",
                    "label": 0
                },
                {
                    "sent": "OK, your views about the theaters are completely exchangeable.",
                    "label": 0
                },
                {
                    "sent": "The the label and the theater is irrelevant to what you think.",
                    "label": 0
                },
                {
                    "sent": "That's not the same as saying you think they're independent because I think.",
                    "label": 0
                },
                {
                    "sent": "I think I persuaded you that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, those first 11 hospitals do tell you something about the 12th.",
                    "label": 0
                },
                {
                    "sent": "So they know it can't be independent.",
                    "label": 0
                },
                {
                    "sent": "What you think about these hospitals, but it's exchangeable.",
                    "label": 0
                },
                {
                    "sent": "So this can be an important ingredient when trying to build priors of big collections of things.",
                    "label": 0
                },
                {
                    "sent": "What aspects of the things you're looking at are exchangeable?",
                    "label": 0
                },
                {
                    "sent": "Do you have no no opinions about to distinguish?",
                    "label": 0
                },
                {
                    "sent": "Well, we can look at this particularly clearly in the case of parameters that are just binary.",
                    "label": 0
                },
                {
                    "sent": "So let's for the moment, think of the theater eyes as being zero or one.",
                    "label": 1
                },
                {
                    "sent": "So the definition of exchangeability, or to be precise, infinite exchangeability is that the joint distribution of these variables is independent of the order in which the variables are written, and that same thing is true have a longer string of variables.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Now if we.",
                    "label": 0
                },
                {
                    "sent": "If the variables were the result of Fair coin tossing independent fair coin tossing, then they would definitely be exchangeable because the order of heads and tails is completely.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's clear and there's no.",
                    "label": 0
                },
                {
                    "sent": "There's no preference of 1 order over any other.",
                    "label": 0
                },
                {
                    "sent": "They will regret that.",
                    "label": 0
                },
                {
                    "sent": "Now suppose to be slightly generalized that we don't need to do ordinary coin tossing.",
                    "label": 0
                },
                {
                    "sent": "We do coin tossing with the choice of one of two coins.",
                    "label": 0
                },
                {
                    "sent": "So I have two coins in my pocket, and they both biased.",
                    "label": 0
                },
                {
                    "sent": "I pick out one of these two.",
                    "label": 0
                },
                {
                    "sent": "I don't know which one it is.",
                    "label": 0
                },
                {
                    "sent": "And then I toss that one repeatedly.",
                    "label": 0
                },
                {
                    "sent": "OK, so whether I take.",
                    "label": 0
                },
                {
                    "sent": "They want the coin that favors heads or the coin that favors tails doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "Given that choice of coin.",
                    "label": 0
                },
                {
                    "sent": "There's still no preference for any particular order of heads and tails.",
                    "label": 0
                },
                {
                    "sent": "So it must be true, therefore, that a sequence of.",
                    "label": 0
                },
                {
                    "sent": "Tosses of a randomly chosen coin is always exchangeable.",
                    "label": 0
                },
                {
                    "sent": "And that's what I say on this.",
                    "label": 0
                },
                {
                    "sent": "In the in the screen there that the Bernoulli probabilities multiplied together, then mixed over a distribution of the probability of ahead.",
                    "label": 0
                },
                {
                    "sent": "That must be a distribution that's exchangeable.",
                    "label": 0
                },
                {
                    "sent": "Indeed, you can see there that is exchangeable because it's clear that that value depends only on the.",
                    "label": 0
                },
                {
                    "sent": "Set of values that are 40 to 70 to 9.",
                    "label": 0
                },
                {
                    "sent": "In fact, it depends actually only on there some, not the order in which they appear.",
                    "label": 0
                },
                {
                    "sent": "OK, so mixing together.",
                    "label": 0
                },
                {
                    "sent": "Bernoulli distributions gives you exchangeable sequences.",
                    "label": 0
                },
                {
                    "sent": "Now the most amazing thing is that that's an if and only a.",
                    "label": 0
                },
                {
                    "sent": "That's that's called definitive theorem.",
                    "label": 0
                },
                {
                    "sent": "It says the only way you can get infinitely exchangeable 01 random variables.",
                    "label": 1
                },
                {
                    "sent": "Is by Bernoulli trials with a randomly chosen probability of heads.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Patient is different from IAD.",
                    "label": 0
                },
                {
                    "sent": "Yes, I mean the bottom line what I've stated is mathematically correct.",
                    "label": 0
                },
                {
                    "sent": "For 01.",
                    "label": 0
                },
                {
                    "sent": "It's true in a more subtle sense for any.",
                    "label": 0
                },
                {
                    "sent": "Something from any distribution whatsoever.",
                    "label": 0
                },
                {
                    "sent": "OK, the thing that's different in the general case in the brewing case, there's only one parameter.",
                    "label": 0
                },
                {
                    "sent": "Obviously the probability of ahead, and so this is a 1 dimensional integral, but in a.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As soon as you get away from 2 variables, then there's the minimum number of parameters.",
                    "label": 1
                },
                {
                    "sent": "Is more than one, and so it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a, it's a multivariate integral.",
                    "label": 0
                },
                {
                    "sent": "But something that formally similar to this is always true, so exchangeability is essentially about mixing of independent things.",
                    "label": 0
                },
                {
                    "sent": "That's a very powerful idea, and.",
                    "label": 0
                },
                {
                    "sent": "And in particular, it gives you another reason to think hierarchically.",
                    "label": 1
                },
                {
                    "sent": "OK, because even if you.",
                    "label": 0
                },
                {
                    "sent": "Even if you found my argument about population variation in hospital mortality rate, even you found that.",
                    "label": 0
                },
                {
                    "sent": "Not completely convincing.",
                    "label": 0
                },
                {
                    "sent": "If you just believe that the hospitals mortality rates are exchangeable, you'll be forced to do pretty much the same thing.",
                    "label": 1
                },
                {
                    "sent": "Because because the only way to get exchangeable mortality rates for hospitals is to use to mix over these independent cases.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So high school models do a lot more different things than that.",
                    "label": 0
                },
                {
                    "sent": "There are theme throughout complex modeling.",
                    "label": 0
                },
                {
                    "sent": "I've listed some of the situations there and I'm sure in your experience with statistical models you can you can.",
                    "label": 0
                },
                {
                    "sent": "You can remember lots of situations where you've.",
                    "label": 0
                },
                {
                    "sent": "Where you've used hierarchal models?",
                    "label": 0
                },
                {
                    "sent": "Even if you haven't used that particular phrase, variation at multiple levels where essentially breaking up modeling into a number of different smaller tasks and thinking about the significance of the variation at each different level and the effect of that, is this sharing of information across the model.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there's quite a lot of different reasons for thinking this way about modeling.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Most of which I've mentioned, I think now.",
                    "label": 0
                },
                {
                    "sent": "Oh yes, I didn't mention the second one here, the.",
                    "label": 0
                },
                {
                    "sent": "Some people find it difficult to settle on what you know.",
                    "label": 0
                },
                {
                    "sent": "Why should I choose that particular beta distribution?",
                    "label": 0
                },
                {
                    "sent": "And what we find is that the further away from the data, the choice of specific parameter values for prior distributions is the further away you get, the less.",
                    "label": 0
                },
                {
                    "sent": "Less sensitive, the consequences are.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK I don't have alot of time left.",
                    "label": 0
                },
                {
                    "sent": "I'll just quickly talk about the main points of this final section.",
                    "label": 0
                },
                {
                    "sent": "So this is another sort generic theme.",
                    "label": 1
                },
                {
                    "sent": "You find a lot of statistical modeling and I'm going to talk about two very specific classes of models, hidden Markov models, state space models, and I guess you've all heard of.",
                    "label": 1
                },
                {
                    "sent": "At least one, possibly both of those.",
                    "label": 0
                },
                {
                    "sent": "And they're really the same thing.",
                    "label": 0
                },
                {
                    "sent": "That's the interesting thing, so.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 1
                },
                {
                    "sent": "But I'm talking about them not only for their own sake, but also becausw.",
                    "label": 0
                },
                {
                    "sent": "There are sort of paradigm for the other things that we do.",
                    "label": 1
                },
                {
                    "sent": "So the key idea is you gotta hidden sequence and indivisible sequence X is and wise.",
                    "label": 0
                },
                {
                    "sent": "Let's say it's for the moment just talking bout time order situation so.",
                    "label": 0
                },
                {
                    "sent": "012 and so on.",
                    "label": 0
                },
                {
                    "sent": "And the structure in the system is that the X is the things that are not observed for Markov chain.",
                    "label": 1
                },
                {
                    "sent": "OK, so it's a dependant process where you can predict the future.",
                    "label": 0
                },
                {
                    "sent": "As well, by knowing the present as by knowing any of the past as well.",
                    "label": 0
                },
                {
                    "sent": "Go to market trainers.",
                    "label": 0
                },
                {
                    "sent": "And then, given given the series of X is, then each?",
                    "label": 1
                },
                {
                    "sent": "Why is just annoying, you know, determines distribution is determined only by its.",
                    "label": 0
                },
                {
                    "sent": "Its particular X&Y is like a noisy version of X.",
                    "label": 0
                },
                {
                    "sent": "And that that that structure gives you very.",
                    "label": 0
                },
                {
                    "sent": "Important benefits and let's look at.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look at some of that.",
                    "label": 0
                },
                {
                    "sent": "So here's a picture of the dependence.",
                    "label": 0
                },
                {
                    "sent": "Schematically this this graph has a mathematical meaning about about about dependence.",
                    "label": 0
                },
                {
                    "sent": "Again, Martin is going to talk a lot about that, but the the arrows here for the moment just reflect the way we build the model.",
                    "label": 0
                },
                {
                    "sent": "So the model for the X is is that it's a Markov chain and we build that sequentially along there and then the wise are noisy versions at each stage.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So hidden Markov models.",
                    "label": 0
                },
                {
                    "sent": "That's the phrase we typically use when the X is have a finite set of values finite state space.",
                    "label": 1
                },
                {
                    "sent": "Maybe known set of values, maybe not.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "There's lots of important examples of that.",
                    "label": 0
                },
                {
                    "sent": "They used all over the place, particularly these days.",
                    "label": 0
                },
                {
                    "sent": "Very important in in biological sequencing of various kinds and historically very important.",
                    "label": 0
                },
                {
                    "sent": "Also, insight in speech wreck.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now state space models have the same structure, but there the X variable is continuous and the very commonly studied cases where everything is normally distributed and then you often get this kind of structure so that the.",
                    "label": 1
                },
                {
                    "sent": "Is a sort of autoregressive process driving these days, so that's driving the axis, so each.",
                    "label": 0
                },
                {
                    "sent": "HX is a multiple of the previous X plus some noise.",
                    "label": 0
                },
                {
                    "sent": "OK, so you get some correlation in the sequence when a is non zero and then the Y is a multiple of the current X plus some further noise.",
                    "label": 0
                },
                {
                    "sent": "So that's a generic 1 dimensional normal linear state space model and of course you can do exactly the same thing with vectors.",
                    "label": 1
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's a class of models that also has enormous implications all over the place, particularly metrics.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in signal processing.",
                    "label": 0
                },
                {
                    "sent": "And the linear structure allows you to do all sorts of things, and in particular allows us to access these kind of tasks.",
                    "label": 0
                },
                {
                    "sent": "Filtering, smoothing and prediction there very.",
                    "label": 1
                },
                {
                    "sent": "Key problems filtering is the process of observing the sequence up to.",
                    "label": 0
                },
                {
                    "sent": "Now the T is now.",
                    "label": 0
                },
                {
                    "sent": "And saying, OK, where are we now?",
                    "label": 0
                },
                {
                    "sent": "OK, I've thought why one?",
                    "label": 0
                },
                {
                    "sent": "I've got YTY T -- 1 otherwise back from now, not the future and I want to make the best inference I can about the current X.",
                    "label": 0
                },
                {
                    "sent": "That's the filtering problem.",
                    "label": 0
                },
                {
                    "sent": "The smoothing problem.",
                    "label": 0
                },
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "I've observed the weather up today today and I want to say the best possible thing I can about last Saturday.",
                    "label": 0
                },
                {
                    "sent": "Or if you like to play around.",
                    "label": 0
                },
                {
                    "sent": "I can predict I can.",
                    "label": 0
                },
                {
                    "sent": "I can observe things a little bit in the future before making inferences about today.",
                    "label": 0
                },
                {
                    "sent": "And prediction is the opposite.",
                    "label": 0
                },
                {
                    "sent": "Prediction is trying to.",
                    "label": 0
                },
                {
                    "sent": "The thing is more useful in weather saying what's going to happen next week.",
                    "label": 0
                },
                {
                    "sent": "And so all of those have the property that you talk about.",
                    "label": 0
                },
                {
                    "sent": "Data up to now.",
                    "label": 0
                },
                {
                    "sent": "OK, not not.",
                    "label": 0
                },
                {
                    "sent": "Not, not, not not indefinitely into the future as well.",
                    "label": 0
                },
                {
                    "sent": "And very often we we don't models of this kind, even when there's no requirement to to do to do online inference.",
                    "label": 1
                },
                {
                    "sent": "Yeah, I agree it is almost coffee time.",
                    "label": 0
                },
                {
                    "sent": "Often you will adopt this sort of formulation of a model even when you do actually have in the future.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As well, so there isn't time to go through the details, but the in the continuous state space case.",
                    "label": 0
                },
                {
                    "sent": "This speaks to the idea of Kalman filtering, where we have explicit expressions for all these filtering and prediction.",
                    "label": 0
                },
                {
                    "sent": "Smoothing algorithms is simple algebraic stuff.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's probably the most often used algorithm in whole of electrical engineering.",
                    "label": 1
                },
                {
                    "sent": "When you get away from the Gaussian linear case, then nothing is explicit anymore and we come to the idea of needing to do filtering when these.",
                    "label": 0
                },
                {
                    "sent": "This beautiful explicit algebraic theory isn't available, and in particular the idea of particle filtering tremendously valuable here, and I expect that our nose either be talking about that, or will we talk about it today.",
                    "label": 1
                },
                {
                    "sent": "OK, so that's where particle filtering fits into this scheme of things.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the discrete case, there's another set of interesting algorithms that allow you to do things very explicitly, just like the filtering case, they rely very much on the.",
                    "label": 0
                },
                {
                    "sent": "Linear sequence here.",
                    "label": 0
                },
                {
                    "sent": "Please look at the slides late.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "'cause I want to try and stop on time, but the point is you.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We we completely avoid the apparent enormous combinatorial explosion of summation that you would get trying to do exact Bayesian inference.",
                    "label": 0
                },
                {
                    "sent": "In in the systems, we completely avoid all that by cleverly writing.",
                    "label": 0
                },
                {
                    "sent": "Sums of products of things interleaving partial sums and partial product so that you get a much smaller order of.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Imputation so that you can get exact Bayesian inference in a very small.",
                    "label": 0
                },
                {
                    "sent": "In a very small amount of computing.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The more general lessons, though, getting away from this.",
                    "label": 0
                },
                {
                    "sent": "This particular details the more general lessons are that this idea of modeling.",
                    "label": 0
                },
                {
                    "sent": "Independent when you got dependent data.",
                    "label": 0
                },
                {
                    "sent": "Modeling the dependence one step removed from the actual data.",
                    "label": 0
                },
                {
                    "sent": "That's a very powerful idea, and we see that time and time again in statistical modeling.",
                    "label": 0
                },
                {
                    "sent": "So treating modeling the dependence in some hidden level called X and the dependence in wise inherited from that by regarding the wise as noisy versions of the X is and so one of the advantages of that way of thinking about modeling is that you can model the data.",
                    "label": 0
                },
                {
                    "sent": "According to.",
                    "label": 0
                },
                {
                    "sent": "Observed empirical properties of data about their distribution and so on, and you still got some flexibility in the way you formulate the hidden process that provides the dependence.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In particular, going back a step, the forwards backwards type algorithms for PR programs and so on.",
                    "label": 1
                },
                {
                    "sent": "They apply equally to other other graph structures other than linear systems, and particularly can do inference on treason, junction trees and so on with very much the same idea.",
                    "label": 1
                },
                {
                    "sent": "So there's quite a powerful collection of.",
                    "label": 0
                },
                {
                    "sent": "Partly abstract lessons that you get from thinking about state space models and hidden Markov models are powerful way to think about modeling.",
                    "label": 1
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, thank you for your attention.",
                    "label": 0
                },
                {
                    "sent": "We're just about on time.",
                    "label": 0
                },
                {
                    "sent": "Enjoy your coffee and I'll be.",
                    "label": 0
                },
                {
                    "sent": "Here again twice tomorrow, but the first one is.",
                    "label": 0
                },
                {
                    "sent": "Moving on to more, more substantial things, and particularly talking about Bayesian computation and about models, sensitivity and model criticism.",
                    "label": 0
                },
                {
                    "sent": "And then the third one thereafter.",
                    "label": 0
                },
                {
                    "sent": "Coffee tomorrow I'm going to.",
                    "label": 0
                },
                {
                    "sent": "Put together a collection of lessons learned in talk about them in the context of.",
                    "label": 0
                },
                {
                    "sent": "Various research problems I've had I've dealt with in the last few years and I've gotta try and pick out the way the different things I've been talking about influence the way we did the modeling and the inference in those situations.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}