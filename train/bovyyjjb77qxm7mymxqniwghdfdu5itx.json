{
    "id": "bovyyjjb77qxm7mymxqniwghdfdu5itx",
    "title": "Dynamic Bayesian Networks for Multimodal Interaction",
    "info": {
        "author": [
            "Tony Jebara, Department of Computer Science, Columbia University"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "June 2005",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/mlmi04uk_jebara_dbnmi/",
    "segmentation": [
        [
            "So I'm happy to chair this next session.",
            "My name is Ryan Stifel Hagen and it's going to be a very interesting session right after lunch and in particular we will have a very interesting invited talk given by Tony Jebara.",
            "You can find his very impressive biography and the material handed out at the reception, but I just want to say one or two words.",
            "He graduated from MIT Media Lab in 2002.",
            "He is now with Columbia University.",
            "He received an NSF career Award, and most important, she has just written a very interesting book.",
            "Which you can assess why his website about on machine learning and so he's talked today will be machine learning applied to multimodal interfaces.",
            "So with that I'll have to go to you and.",
            "Great thanks, I'm very excited to be here.",
            "Still on New York time because I flew in this morning so I'll try to maintain a steady talking pace, but I'm actually also really excited because a lot of the ideas here are novel for us.",
            "We're looking at new datasets and I think it's great that we have a community here where people are really doing ambitious things with multiple modalities, specially with the very last data set were working on, which is uncharted territory for us, which is looking at surgical drills and surgical procedures would.",
            "Laparoscopic robots will talk about that in a second, but we really aren't into a new area here where we don't know anything about the signal properties, the noise, the dynamics, and things like that.",
            "It's a very complicated robot.",
            "Cost multiple millions of dollars and so on, but it's actually becoming an industry standard.",
            "So now we're looking forward to sharing this data and sharing some of the code.",
            "The main approach is going to be this dynamic Bayesian networks approach, which is popular in the machine learning community."
        ],
        [
            "And.",
            "I'll be quickly talking about.",
            "What motivated us to go to DBN's and?",
            "You can model it depends from a multi model Anna multi person perspective and then we'll just review a little bit about the underlying machine which is really just Bayesian networks and then the standard algorithms behind Bayesian networks and how they can be easily applied to variations of the classical Bayesian networks, which are things like hidden Markov model models and common filters and how we can extend things like the expectation maximization algorithm and matching Michael estimation tenyu graphs that are more relevant to our particular multimodal multi purpose.",
            "Datasets and then I'll show a few examples and there's just scattered examples of different topologies in these networks.",
            "The first one is just hidden Arma model and will talk about learning that and doing maximum likelihood and how you train it and then use it to learn interactive gestures.",
            "Another example is input output hidden Markov models, which also fit the DBN framework, and they can be used to do audiovisual interaction learning on a wearable platform.",
            "And then we'll move up to what's kind of the.",
            "Current state of yard in the graphical models community, and that's intractable.",
            "Dynamic Bayes Nets and there are talks coming up on variational estimation.",
            "Structure me feel and so on.",
            "What we talk about generalizing am generalizing the learning algorithms in the inference algorithms to handle graphs that are really intractable.",
            "Where the distributions blow up on us if we try to do exact inference and then we'll show our particular model which appeared about a year ago at UA I 2004, which is this dynamical systems tree, which is a.",
            "Tree on top of many different dynamical systems.",
            "So imagine connecting many common filters and Hmm's in a tree structure and then we'll look at that for multi person visual interaction.",
            "So football plays by tracking football players.",
            "This is American football, so there's a.",
            "There's a handful of players being tracked on the field and then we'll talk about how we were trying to apply this now to surgical drills and then getting other people involved in ongoing directions and how.",
            "There are some synergies between this type of data.",
            "And the data."
        ],
        [
            "That you're working with.",
            "OK, so let's just quickly motivate while we wanted to go beyond the simplest dynamical systems and those dynamical systems are hidden Markov models in common filters and actually has a graph that exact same graphical structure.",
            "The difference is the common filter has a continuous hidden state and the hidden Markov model has a discrete hidden state, but these are really meant to model one single process in time.",
            "One single dynamic process with a single kind of Markov assumption.",
            "But it turns out when we start working with multiple modalities, there's multiple processes, and that's true for combining multiple videos, multiple audios and also heterogeneous modalities, audio and video, partly because there are sometimes occurring in different time scales.",
            "So audio and video and haptics have different scales of time.",
            "Also, different amplitude scales also different noise characteristics, so they're not all Gaussian noise types of processes, or they're not all common filters, or they're all.",
            "Multinomial distributions there's all sorts of unusual noise properties, so we can't just slam them all together into one monolithic model, and that's true.",
            "Also, we start working with multi person data as well for tracking even a single person may have multiple limbs.",
            "Each limb has different dynamics.",
            "You can move your hand more quickly and you can move your upper arm.",
            "Also, two person and group dynamics also have different speeds of dynamics, and you can also imagine their only weakly coupled, so people don't.",
            "Walk in lockstep when they're walking down the street together, but they're still somewhat synchronized.",
            "Someone stops.",
            "The other person will stop as well.",
            "The other person will continue with.",
            "If they're talking with having a conversation.",
            "So there's we couplings and we want to capture these types of different couplings.",
            "We don't want to just put everything on to one data set, so if we're given two time series.",
            "So maybe this is one person walking, and here's another person walking or tracking down limbs.",
            "We don't want to just put this on to one monolithic time series and say recognized this as these two people.",
            "Do.",
            "These two people are walking together because this person might change the phase of their walk.",
            "Let's say in this person might be walking right stop and then catch up.",
            "That doesn't mean they're not walking together, but if you put them to one time series, you can only do one kind of dynamic time warping.",
            "Let's say within Markov models, and they really need to align perfectly as templates.",
            "Otherwise you don't recognize that.",
            "Really is the same gesture or the same behavior?",
            "So we're going to be looking at different ways to zipper multiple interacting processes with different graphical models and different Bayesian networks.",
            "So let's just do a quick review and for men."
        ],
        [
            "If you folks, this might be redundant stuff, but let's just talk a little bit about this elegant framework and how it really is very general, and we don't necessarily.",
            "Sometimes have to reinvent parts of it because it all fits very nicely together, so Bayesian that's also called graphical models really are just combination of statistics and graph theory and just basically simplifying what might be a huge distribution.",
            "OK, so when we're dealing with many people many variables, many time steps, the distribution over all these variables is this gigantic distribution over thousands of variables in their cross product.",
            "So what graphical models and base that's do is they let us simplify this gigantic distribution.",
            "Into a product of smaller simple distributions, these smaller conditionals.",
            "So if I have 1000 variables, this thing, if they're all binary, could be 2 to the 1000 large, which means I can't store it on a computer, but instead I know some kind of independence of conditional independence structure between the variables.",
            "For example, this variable generated these two and then this triggered this one and this one, and then there's a Markov chain of triggering over here.",
            "Then I can just describe the whole distribution as a product of a bunch of simple conditional distributions of each node given its parent variables.",
            "So instead of having this 2 to the power of 6.",
            "Hypercube of values.",
            "I've got a small 2 by 1 table two by two 2 by 2, two by two by two in it and a two by two by two cube and then I can write this distribution down with many fewer numbers than two to the power of 6.",
            "So that's what really is happening with this graphical modeling.",
            "And also it lets us intuitively interpret what's going on that there's this kind of.",
            "Main trigger variable that triggered this causal flow of or this causal chain reaction or Markov chain over here it's hard.",
            "It's hard to really say anything about causality with these networks, and if people aren't certain pure causality arguments, we should look at causing that.",
            "Works in Judea Pearl is a great book about that really.",
            "We're just talking about conditional dependent.",
            "So a node basically is written in terms of conditional introduction given its parents in the joint.",
            "Overall variables is just this product.",
            "And we can imagine these exercise being discreet and in which case these tables are just contingency tables or multinomial distributions or even continuous.",
            "So X3 and X5 could be a 2D Gaussian.",
            "There's a 2D Gaussian relationship if their continuous variables and you could use any any member of the exponential family, not just these two.",
            "And everything is still fine.",
            "And typically what people do is they split the Bayes net into a series of hidden variables, an observed variables and I use this color for hidden variables in the future slides and.",
            "This color for observed variables an there are three basic operations we typically do with Bayes Nets.",
            "You've got this probability of all the nodes, so this is the set of all visible instead of all hidden variables and it splits this way as a product given some parameters that tell you what to put in for these tables, the three basic operations are well in addition to computing the probability for any specific setting, we'd like to compute marginals over the hidden variables given the observed ones, so I observe.",
            "Certain variables and I want to query some unknown ones and give me a distribution of that conditional or that marginal.",
            "Another thing I'd like to do is compute the likelihood of the data so for all the hidden variables, I sum them out and I have observed observed these XV visible variables.",
            "This is the probability of X given the model.",
            "So how likely was this observation given my model?",
            "And then another thing I'd like to do is maximize likelihood.",
            "The model by adjusting this data or modifying the tables here so that I fit the data better so I can evaluate how well I'm doing with that data.",
            "I can also push the model towards the data and the two key algorithms for steps wanted.",
            "The two key algorithms are the junction tree algorithm in the EM algorithm and junction.",
            "She will help you with this inference problem and compute the likelihood and M maximize the likelihood whenever you have hidden variables.",
            "OK. And so."
        ],
        [
            "The junction tree algorithm and I'll show I'm just showing a quick overview because a lot of people.",
            "Have more details on this in different papers, but this is the basic workhorse of the junction of Bayesian Nets and we start off with the Bayesian data.",
            "Typically we go from that to a junction tree and we do is we moralize the graph by marrying parents.",
            "So apparent of this note is over here, and here's another parent.",
            "They're both at the root of these arrows.",
            "In coming to this common child, we marry them by drawing extra link and then we drop the arrowheads and now we have this moral graph which says that there's dependencies between all the connected variables.",
            "And then from that moral graph we triangulate it, which prevents cycles of of four or more by drawing an extra link here.",
            "Then we find all the maximal connected cliques.",
            "So here's a fully connected clique of nodes X one X2X3.",
            "Here's another one X3X5X2X2 X 5X6 and X2X4, and then we build this thing called the junction tree, which is a set of mega nodes containing all the nodes in one single clicks or X1X2X3 is right here, and it touches X 2X3X5.",
            "Over here, which touches X2 X 5X6 over here and so here are the three year four cliques and we sort of out here and between the clicks you write separator potentials that connect them and contain the variables that have in common.",
            "So X one X2X3 shares X 2X3 with this clique.",
            "So then we have this junction tree and now we can run the junction tree algorithm.",
            "And the junction tree algorithm is basically going."
        ],
        [
            "Take this complicated distribution and send messages around to make sure everybody is consistent, and we're mean by consistent is I want the various tables in this junction tree and each potential function, so each one of these cliques.",
            "We saw earlier has a function over these variables and over these variables over these variables.",
            "So I have a function over X2 X 3M function over X1X2X3.",
            "I want all these functions to agree on the variables they share.",
            "OK, if I ask this function, what does it think is a likely value for X2 should actually be true or false?",
            "Well, this one likes that extrudes true, but this one likes to say X2 is false.",
            "They're not in agreement, so the junction tree algorithm is going to just all the tables.",
            "By sending messages from each clique to its neighboring cliques, saying this is what I think the marginal is over this shared variable.",
            "So maybe will send a message to BC saying if I sum out over my 2 dimensional table to get entries Overby.",
            "Here's what I think the entry should be.",
            "An NBC gets that message and then updates its table so that if it were to come out over C you would get the same table for being.",
            "So basically you get a two by two table here 21 dimensional to buy one table and two by two table and summing over this table should give you the same value.",
            "Something over this table over C. So that's what we mean by message passing.",
            "And so the separator gets updated.",
            "Then you update the clique with this message.",
            "Then you do the same for the reverse message.",
            "Then both potentials are in agreement.",
            "We have a consistent distribution.",
            "An there's something very valuable about that, because once the junction tree algorithm, this consistency and we have marginal distributions that are correct and on a tree this junction tree algorithm is guaranteed, and you typically proceed in two steps, you initialize all your clique potentials and your separate potentials.",
            "Then you pull messages up towards the root, everybody updates.",
            "Let's say upstream until you get to this root.",
            "Everybody updates their tables and then the root updates and send messages back downstream to all the individual leads and at the end the junction trees done.",
            "You sent these sum and product messages.",
            "It's also called the sum product algorithm because of this.",
            "And at the very end, when you read off your clique tables in your potential tables, you're separated."
        ],
        [
            "They all become potentials.",
            "They all become marginals or conditionals over the hidden variables given the data.",
            "So that's what we wanted for inference.",
            "So each one of these.",
            "Clicks now becomes a probability of the variables.",
            "It's describing, given all the observed variables that you've actually observed before we started running the junction tree algorithm.",
            "So we have all these marginals telling you what configurations are likely for the hidden variable.",
            "Here this in variable, then this pair of hidden variables as a 2 dimensional table.",
            "And also the normalizer for each potential function is just the likelihood of the data, the probability of the data if I were to sum over the hidden variables.",
            "So for basically running one.",
            "Very efficient algorithm on a tree just to collect and distribute.",
            "We have.",
            "All these interesting marginals and conditionals over hidden variables, given the data and the likelihood of the data given the model.",
            "OK, so that steps one or two that we like to do with.",
            "Without Bayes Nets."
        ],
        [
            "And then another step is maximizing the likelihood, so I can evaluate that likelihood.",
            "But I also want to increase it by wiggling some parameters, adjusting entries in my tables or adjusting some Gaussian means and covariances.",
            "So we'd like to maximize the likelihood by summing over the hidden variables.",
            "Basically, the probability that the XV given the Theta when you come out over XH, it disappears, so that's what maximum likelihood is trying to do.",
            "Typically, if this isn't easy.",
            "No hidden problem maximization, we could just take derivatives and set to zero over Theta and get back the most likely setting.",
            "But instead if there's there are some issues over hidden variables.",
            "These derivatives aren't analytic, we can't do them in close form.",
            "So instead we apply this expectation maximization algorithm EM, which basically maximizes a lower bound on the log likelihood.",
            "So instead of maximizing this, we maximize the log likelihood.",
            "That's OK, it's the exact same problem.",
            "But we apply Jensen's inequality to rewrite this as the expected complete likelihood plus some KL divergent between this proposal distribution and the posterior over the hidden variables given some parameters.",
            "So this turns out to be a lot easier to work with than this.",
            "This is a function over some proposal distribution Q over the hidden variable.",
            "That I want to maximize andsome Theta, so this is a function over 2 variables Q and Theta which is always less than the original log likelihood of data.",
            "So instead of maximizing in one fell swoop over Theta, the log likelihood I literally maximize this bound, which has two parameters.",
            "This proposal, distribution of what my guess is, the distribution of the hidden variable should be.",
            "That's the queue, and then the parameters for this lower bound.",
            "So this looks like this access parallel optimization we maximize over Q that pushes this lower bound up the maximizing with it, and we keep pushing up or slowly going to increase this thing by iteratively increasing this thing.",
            "And if we start off here, we first maximize over Q.",
            "This gives us the best Q, then the best day to the best Q bestatin.",
            "So we iteratively maximize and this is easy to maximize over Theta and easy to maximize over Q.",
            "Except I can get stuck in local minima.",
            "So over here if I started with this state I guess.",
            "Then I would get this best Q than this best data this best Q.",
            "So each step is going to give you the best Q and the best data we need to start at a good place to get to the good global optimum.",
            "And what's going on is in the eastep for maximizing this functional over.",
            "Distributions of hidden variables and parameters.",
            "And it turns out the most like the highest value of this function is when we set the Q distribution equal to the.",
            "Posterior distribution over the hidden variables given the observed variables and current guest of your model parameters.",
            "OK, so this takes the lower bound.",
            "And maximizing such that it hits the log likelihood.",
            "So this.",
            "This is the right function.",
            "Is this L the capital L function it touches makes this lower red bound kiss the log likelihood, and then we maximize over Theta to get to the peak of red lower bound downstairs and I give you the best model parameter.",
            "So you iterate these two steps and then you train your models on data.",
            "So that's all there is to Bayesian Nets an at the end of the day those are the three key steps that you need to do, no matter how complicated the network gets.",
            "Fundamentally, maximizing this and maximizing this.",
            "And running junction tree algorithm because we need XH given XP and that was the marginals and the conditionals that we got out of Junction tree algorithm.",
            "So that's kind of a full clause world that we need in terms of Bayes net tools.",
            "Obviously there's extra extensions of things, So what are dynamic Bayes Nets will?"
        ],
        [
            "Just to keep things simple, dynamic Bayes Nets are just Bayes Nets that are unrolled in time.",
            "So instead of just a network of static variables, you've got this timestamp for every single variable, and this is time T = 0 = 1 Tables, 2 tables, three an really.",
            "What's going on is something is happening in time, but I'm just representing it as eight random variables in.",
            "Kind of probability distribution and this can continue indefinitely if it's a really long time series.",
            "And so the two classical examples of dynamic Bayes Nets are the hidden Markov model over here and.",
            "This is in yellow, which means this is a hidden state that evolves with Markov dynamics, so the current state depends on my past date.",
            "Once I know my current state, I generate an observation that's related to my current state.",
            "So imagine this is somebody's mood, we can't directly observe their happy, sad, or neutral, but we can look at how much they're smiling down here.",
            "And so we typically evolve from happy to neutral and then from neutral, sad and jump back and forth between two extremes.",
            "There is some kind of Markov assumption here telling you how you evolve between S1 and S2.",
            "Typically that's described as state transition matrix.",
            "Given us two, I'll tell you how much the person smiling and then S3 persons frowning etc.",
            "So these are observations and there's a hidden state and the state evolves as this matrix of values probability of the current state.",
            "Depends on the past state.",
            "We have this prior over initial prior distribution over the first set of States and then the emission model is given.",
            "My states.",
            "I'm going to sample from another table which gives you my observations or a Gaussian which gives me continuous observations.",
            "That's all hidden Markov model is, so the hidden states might be phonemes in a speech recognizer and then the samples are the raster vector of coefficients or the capsule coefficient, let's say from a single Gaussian centered around that.",
            "Pony and then similarly we have the same topology here, but now it's a linear dynamical system or common filter.",
            "And here we have X 0 being continuous vector instead of a discrete state vector.",
            "And now this thing also evolves as a Gaussian.",
            "So given my past I sample around my pass X1 with the Gaussian model something like this.",
            "Which is a linear function, so this is good for things like tracking or.",
            "Flying planes and things like that.",
            "It's not good for discrete types of hidden state problems, and then given my hidden state I sample also an admission.",
            "So those are the two classical things, but in the in the DBN framework they look pretty much the same and it turns out that junction tree steps and the junction tree structure look pretty much the same as well.",
            "So once we've done one, we've triangulated moralise.",
            "We figured out how to do.",
            "Message passing is pretty much the same for the other.",
            "We just need to do some integration status summation for the Gaussian, yeah?",
            "Sure.",
            "Are you limited to the assumption that the state transition model is Gaussian?",
            "You're a junction tree album.",
            "You don't have to stick to Gaussians against do other exponential families, but things do sometimes get nasty if you move beyond Gaussian or multi million.",
            "Mixture.",
            "If it is a mixture, then things might blow up with a junction tree algorithm, so complexity might grow.",
            "If you're doing a mixture of Gaussians, each time you propagate a message that mixture of Gaussians might go from 10 Gaussians to 100 to 1000, with each step of the propagation.",
            "So that's an open problem right now is how to mitigate that exponential growth as you do junction Tree message passing when you have non Gaussian distributions.",
            "Assumption for your addition model.",
            "Yeah, you could move away from the Gaussian emission model, and there's things like the kernelized Kalman filter, which is a nonlinear Gaussian in some other space abende Gaussian fuel.",
            "So people have started doing things like that as well.",
            "It also helps avoid putting nonlinearities in here, which can cause problems with junction tree.",
            "So these are all kind of active areas.",
            "Definitely in modifying these models and so adding these mixtures or nonlinearities extended common filters, and so on.",
            "An people in the control literature are doing some of this people in the.",
            "Kind of Markov random fields, literature and so on.",
            "So there's there's in physics as well.",
            "Variations of these non Gaussian models.",
            "Um?",
            "It's hard to keep track of everybody, but.",
            "We're going to.",
            "We're also going to go in there and modify the graphical models a little bit.",
            "So one thing we want to do is to look at two person interaction and see if there's two people interacting.",
            "What can we learn about two people person?",
            "Why?",
            "Let's say in person X.",
            "So here's person wires, person X.",
            "They're interacting and we can try."
        ],
        [
            "Back this person and synthesize some.",
            "Caricature them to this person and then they're just going through this virtual interaction over the Internet.",
            "Let's say we'd like to watch two people interact, so we get a model of the behavior of Y given the behavior of X, and if we learn enough about this distribution, then I can mimic this person.",
            "With basically a distribution and predict what's the most likely why?",
            "Let's say for this human or sorry, so This is why what was the most likely wise response to what X just did, and we can learn these simple kind of stimulus response types of behaviors.",
            "So what's one way to take these two people?",
            "And applying Markov models to them?",
            "Well, we could just take 1 hidden Markov model for each person and then model this person within HMM and model this person within HMM.",
            "But then there's no coupling.",
            "So when I try to predict this person's behavior from this one well this person is just their own hidden Markov model.",
            "Let's say we're tracking their mouth and we're seeing their moods and they have happy neutral.",
            "Sad well I just model them by themselves.",
            "I can't synthesize a virtual person.",
            "So that's no coupling.",
            "Another way is just to take both their time series data and learn one giant in Markov model for both, but that's too rigid.",
            "Again, that's the problem of just slamming everything into one time series.",
            "Then if something happens at slightly out of lockstep, one person smiles one second too late or too early.",
            "Then it looks like a completely new pattern an you don't recognize it.",
            "That was the same behavior.",
            "So what we'd like to do is do something a little more flexible than just put things together or just keeping keeping them apart.",
            "Some kind of slower coupling."
        ],
        [
            "And So what we're going to do is we're going to try this hidden Arma model first, an arm.",
            "A stands for autoregressive moving average, and we're going to take the past of this person X.",
            "In the past of this person Y and predict the next.",
            "Behavior over here for this person and then instant future.",
            "So we're going to watch two people talking.",
            "And as this person moves our lips, this person stays quiet.",
            "When this person stops talking.",
            "Typically that's when this person can take the floor, so we're just tracking the openness of the mouth here.",
            "This person talks and then stops talking.",
            "That means chances are this person should start.",
            "So we've learned this two person interaction with the way we're going to do that.",
            "We're not just going to slam everything together, even though in a sense this is a little slam together.",
            "We're going to fix that later on, but we're going to learn to predict Y.",
            "From the past X&Y here.",
            "The past of this person in the past of this person to predict what this person should do next.",
            "And it turns out that this can be thought of as an ARMA model.",
            "We're also going to make it a hidden Arma model.",
            "And we're going to focus on predicting what person Y will do from both their past and the other person's past an it's a hidden ARMA model because in our model is really a linear model at the moving average auto regressive moving average.",
            "And instead we're going to have many auto regressive moving averages which capture different regimes of behavior, and we're going to switch between them with this S. Discrete variable.",
            "So this allows us to make a nonlinear Arma model.",
            "Basically because instead of having a single ARMA model, we're switching between many different linear ones and that pieces together is.",
            "And a linear landscape into this nonlinear type of landscape.",
            "And so here is.",
            "And we're also going to do PCA to really allow long-term moving averages of windows of 100, let's say, as opposed to really short term moving averages, which is maybe more typical for things like stock market prediction.",
            "So here's an example of just a second order moving average.",
            "Here's the person we're trying to predict, evolving in time, and we're going to protect them over here based on what they just did and what they did 2 steps ago and what the other person X1 just did, and let the other."
        ],
        [
            "Person X1X0 did 2 steps ago and I'm going to pick which Arma model to use with S2 so as to is the choice of which ARMA model.",
            "And it doesn't matter that you just use one normal model, you're just going to pick which one out of my five ARMA models.",
            "Once I picked it, I just know how to map these four values into a prediction through a linear mapping.",
            "So now we just see that this classical.",
            "Time series forecasting method in statistics.",
            "An in other application areas can be written as a graphical model and folks at Microsoft Research are actually the ones that we're proposing this it was at UA 2004 that you can describe a variety of Arma Arima models as graphical models and be very Bayesian and very Bayesian network about your treatment of them.",
            "OK, so that's the model and the features we've."
        ],
        [
            "And for the model or just the head and hand coordinates for each person, two people go out and interact and play these gestural games.",
            "And the setup looks something like this.",
            "Let me just go into it.",
            "So."
        ],
        [
            "We've got two people sitting in front of computer screens, just waving at each other and playing little games.",
            "We get this time series of a few minutes of this type of behavior, probably about 15 minutes.",
            "And.",
            "Each person is tracked by forming again a distribution over RGB by three Gaussians describing the color of skin that gives you these types of images from the video, which are all only the skin colors, and these are clustered with a mixture of spatial Gaussians, three spatial Gaussians in 2D will cluster the coordinates of the head in hands.",
            "And here's an example of that.",
            "Looks like as you track the person.",
            "So this is just to get some raw features.",
            "You could use any of your own preferred features or the promise, not video.",
            "It's something else you you might have.",
            "Data, gloves that give you raw measurements, but you're getting this time series of the means and covariances of these three Gaussians as a person is being tracked in line, so you just collect this data set for a few minutes.",
            "And then you form this basic Arma model.",
            "And here's kind of a gimmicky picture of what's going on with the Arma model.",
            "And then we want to maximize likely our model if we really want to be standard.",
            "Bayes Nets practitioners, but it turns out we only need Y from X.",
            "We're not going to try to predict both people, we just want to predict one person from the other.",
            "So it turns out condition.",
            "Likely it does a little bit better and there's a conditional variance of them called conditionally M. Which basically, if all the care."
        ],
        [
            "About his probability of the visible wise given the visible X is you want to really maximize that model, not just the probability of XD, YV, which is maximum likelihood, then should maximize the conditional likelihood, which is this.",
            "Original log likelihood minus the log likelihood of only the X data.",
            "So X, y -- X.",
            "An as an example, if you wanted to do if why was a label and you want to want to classify red circles and Blue Cross is if you just did Yemen maximum like you would get this really bad classifier.",
            "Here's some red data here and here in Blue data here and here.",
            "Read data here in here Blue data here and here.",
            "This is the maximum liquid solution, but it actually splits the data set down the middle.",
            "This way the top half is where the red Gaussians dominate the bottom half with the blue Gaussians dominate.",
            "And that's basically 5050.",
            "Classification accuracy if you take the same modeling, maximizes conditional likelihood.",
            "You get a red, blue or red or blue Gaussian negative perfect classification accuracy.",
            "So if you really only care about making predictions on Y from X, it's better to match some conditional likelihood.",
            "And now."
        ],
        [
            "Now actually, people are moving towards maximum margin, which is some work by Daphne Kohler and Taskbar, which is even better criterion the maximum likelihood.",
            "And here's another example of what.",
            "This looks like in a regression setting.",
            "So if you were to take two Gaussians and try to fit this regression problem, we're trying to predict Y from X, you would get this behavior out of them.",
            "These are two Gaussians and this is basically saying that X&Y are related through some step function.",
            "But the higher the better solutions actually to capture that there's some linear relation between X&Y and that's when you do maximum conditional likelihood and you get from the same starting point.",
            "The fact that there's these two possible answers for each X there's two possible correct answers for why in their linearly related.",
            "That's lossed that when you just do maximum likelihood.",
            "How my doing on time?",
            "You have 5 more minutes, 7 more minutes.",
            "OK, better hurry up wow.",
            "So we did this model and.",
            "I'll have to speed up a little bit.",
            "I'm sorry bout this ranted trained it with conditionally M. This is this length of the short term memory and then we have some simulations of.",
            "The behavior now, if you actually go up to the system and it's it's predicts the most likely.",
            "Why using this model so you can go up and scare an in real time."
        ],
        [
            "It will generate this figure that acts like it's scared of the person in front of it.",
            "You can wave and then it'll wave back and it's just learned this from a few minutes of time series data.",
            "And you can do things like clap, do something silly and it'll start clapping just because I watched a few minutes of people."
        ],
        [
            "Interacting that way, another system which we tried is this wearable system where we try to collect data of the person's face and their audio in response to everything going on in front of them, and what we did there was we applied input output in Markov models, which Samy Bengio did a lot of pioneering work on what we're trying to predict an agent's behavior from the outer world stimulus response, but from both audio and video channels anile there instead of using HM."
        ],
        [
            "We use an input output HMM, where you've got the world's observations and the agents observations, and there's a common state that connects the two, and then we maximize the likelihood of the agent given the world.",
            "So again, this is the dynamic Bayes net.",
            "This is the criterion.",
            "And it's better than doing it regular hidden Markov model because it allows you to distinguish agent from world and say I really want to go from the world inputs to the agents actions.",
            "Here's what the data looks like."
        ],
        [
            "Like this is.",
            "The audio data.",
            "This is some of the video being collected.",
            "These are just.",
            "Coefficients from the spectrograms.",
            "I'll show a little bit of this just in the interest of time.",
            "So we collected a few hours of this type of data just walking around hallways and.",
            "Getting the face image and its response to the audio in the video in its own video response.",
            "Just talking to people in.",
            "Various places in the in the building.",
            "And these are just some of the features we use.",
            "We're not really focused on the features you can do better jobs with features over here.",
            "This is.",
            "The data and then we.",
            "We represented."
        ],
        [
            "This is using some variant of PCA which does better than regular PCA.",
            "Not going to describe this into too much detail.",
            "Basically it allows you to do PCA while connecting the different landmarks of the faces together, so you find that I line up with eyes and nose is lined up with noses before you apply PCA, and that does a lot better.",
            "In fact, it."
        ],
        [
            "As I think 2 1/2 orders of magnitude better reconstruction accuracy than just vanilla PCA applied to the images.",
            "So that's what we did to represent the videos and then we trained this."
        ],
        [
            "Input output hidden Markov models that take the audio and video of this person and.",
            "And this person passed audio and video to predict their next video.",
            "And then to predict their next audio as well.",
            "And, um."
        ],
        [
            "Again, we maximize this with CEM.",
            "We formed two input output models, one which predicts the agents audio given world and World Audio World video and agents video.",
            "Given World Audio World video.",
            "And these are the models look like looks like and then we subtract the log likelihood of the probability of the world to get the conditional, and this was modeled as 60 dimension, 60 State, 82 dimensional hidden Markov model.",
            "And this is the size of the training and testing datasets and we can see the."
        ],
        [
            "All likelihood, so this is the training.",
            "And this is the conditional likelihood.",
            "I think the EMH algorithms in red and on joint likely the end of those great, but on condition likely it doesn't do so well.",
            "You'd rather do this CEM algorithm, which outperforms it and then we can re synthesize audio and video from new test data by training on the actual audio video interactions.",
            "It doesn't really learn anything that deep, it really just learns how to."
        ],
        [
            "Synchronize.",
            "The face to the outside stimulus.",
            "Somebody speaking the face starts to move, otherwise the face doesn't move an it basically Mumbles if there is audio just learns a very simple type of behavior out of the 1st order and I didn't plug in my audio but I'm running low on time anyway and I really want to show this surgical robotics data set.",
            "So here it was showing the simulation and then it does a.",
            "So when the person speaks, the face moves, otherwise it stays pretty still.",
            "I will show some."
        ],
        [
            "Quad quantitative results in a second, so these are all for re synthesis.",
            "We can also move up to intractable dynamic Bayes Nets.",
            "So for example, these things called factorial hidden Markov models that zoom in Garman, he's been working on coupled hidden Markov models at Michael Jordan, and that Brad has been working on that.",
            "When you try to couple of multiple people together, so I might have five people interacting in a couple of this way, or two people interacting.",
            "A couple of them this way through their hidden states.",
            "So these become attractable."
        ],
        [
            "Because the Q distribution over here when we start doing maximum likelihood ATM, the Q distribution made to maximize the optimal setting of that Q distribution is this gigantic distribution again, which we can't write nicely as a product of conditionals.",
            "It explodes on us, so we can't find the queue that optimizes this bound.",
            "So instead will restrict the queue and say, you know, I don't find the best Q.",
            "Find something that's a little bit better than the previous guest and only explore some subset of.",
            "Compact cues while you're maximizing this bound.",
            "So don't take the best step towards the optimal solution.",
            "Take 1/2 step, but just don't make a very complicated Q distribution.",
            "Make one that's still something we can write down on computer, and so that's what this variational EM or generalized them in some Giants does.",
            "You basically maximize this, but you limit the queue that you explore to be factorized.",
            "So it's not this arbitrary Q distribution which might explode on you, and then you maximize that data and you alternate back and forth.",
            "So this takes partial steps and it doesn't push the bound quite up, and railing against the log likelihood.",
            "So."
        ],
        [
            "That means, as you can force the chains in the factory hidden Markov model to be just chains instead of coupling them together and the couple Markov model as well.",
            "This is called structured mean field.",
            "So instead of having all possible couplings, we just separate the chains.",
            "And now we can still do tractable.",
            "Perative EM and maximize like iteratively.",
            "So what we want is something even more flexible.",
            "A couple of many people, as we've we've proposed, this dynamical systems tree when we want a hierarchy of coupling.",
            "So imagine coupling 1000 people in a University from students in different departments in different schools in different universities.",
            "You want to slowly hierarchically couple people because not everybody interacts all at once in homogeneous way.",
            "So instead we have each person over here as a.",
            "As a."
        ],
        [
            "Hmm, or common filter actually switch common filter and we stitch them together in pairs.",
            "So imagine this is a doubles tennis game with one person here.",
            "One person here, one person here in one person.",
            "Here these two people are on the same team there stitched together 1st and then the two teams are playing against each other in the same game in Denver stitched together.",
            "So this is like a doubles tennis type of interaction and now we can again apply the same trick to this which is.",
            "To apply."
        ],
        [
            "I this structured mean field approximation and just make these all just chains and break the links, iteratively maximize the bound for M and it turns out when it's a tree structure connecting these changes, you can write a generic piece of code that does recursive things and solves for any topology.",
            "So people want to download our code.",
            "It's basically any topology connection of chains, just give me how you want to connect your 500 chains or 20 chains into whatever hierarchy.",
            "And the code will just figure it out.",
            "You don't have to re write new code the way these other techniques needed when it was factorial or coupled for each different topology.",
            "Just because it's a tree and we can write it recursively.",
            "Um?",
            "So the basic idea."
        ],
        [
            "As we do variational inference.",
            "This way in and up the tree, so it's a recursive way of applying structured midfield.",
            "And we apply."
        ],
        [
            "This two football play recognition where we have little over a dozen different players and each person as an XY coordinate and we track them.",
            "And then we decided."
        ],
        [
            "Try to classify two different types of plays.",
            "Wham plays and big plays.",
            "And we basically built a model of each and salt on training data.",
            "About half a dozen of each plate, and then for another half dozen test plays, which model overwhelm the other.",
            "And that's how we classify the new query plays, so these are different strategies for the American Football games, and we explore just combining.",
            "Everybody has their own individual common filters or their own individual Markov models, so breaking all the links and you don't classify the queries.",
            "There's about a dozen queries very well at all.",
            "Or you can put everybody into one single time series.",
            "That actually happens to work, but it still makes 1 error on the test.",
            "Combining things in a bad way with a bad topology DST gives you 5 errors, but if you combine people into two teams and then the two teams into one game, then you get 0 errors for the recognition of these plays.",
            "So just all the players merge into one team state and another set of players in the other, and then they all merge.",
            "We've applied this again to things."
        ],
        [
            "Gene networks multiple genes, interacting overtime and that again does much better than slamming things together.",
            "Or keeping things completely apart for their dynamics.",
            "And now we're looking at another multimodal data set, which is haptics and video, and these are the Davinci laparoscopic robots that surgeons use in hundreds of hospitals, at least in the US, and the surgical team."
        ],
        [
            "I'm working with has three surgeons that have over 100 operations each on this robot and they basically work at a console near the patient and the robot mimics what they're doing.",
            "This is what they see while minimizing the invasiveness that they wouldn't otherwise be able to do as well if they were working directly on the patient, but this robot is actually collecting 300 Hertz time series of what the surgeon wants to do, how the robots actuators are moving, and the video data inside.",
            "So we've collected.",
            "We can't collect real operations in real surgeries.",
            "But we collected."
        ],
        [
            "Various drills that look like this and we just have 4 videos.",
            "These are the two cameras that are inside the cavity.",
            "These are two outside cameras.",
            "And then we collected."
        ],
        [
            "Time series, and here there's 64 Dimensional time series for the surgeons doing the console coming into 300 Hertz and the video is coming in at 30 Hertz.",
            "And we can see various.",
            "Sorry, we can see various differentiations between a trace maneuver or Cobar open over.",
            "When an expert does it in a novice, does it an here novices, a student who's just starting to work with the the robot versus an expert is over 100 real operations with it.",
            "And suturing over here.",
            "And here's some initial results."
        ],
        [
            "And we're also going to plan on providing this data set.",
            "Probably link off of my homepage modulo some permissions from the Davinci Company called Surgical into it, Intuitive Surgical.",
            "When we compress the haptics and video data with PCA, and then we did, we actually tried many things.",
            "Input output.",
            "Hmm Cindy Estes, and here are some preliminary results.",
            "This is the minefield gesture, so we asked three offices and three surgeons to do mine fields, and then on another three novices, another three.",
            "Search surgeons we had test data and you can see you can get this with a little bit better than chance.",
            "The minefield misses for different maneuver, different complexities of models, but this other other drill.",
            "This Russian roulette real.",
            "It's a lot more lot more reliable to around 85% accuracy that you can predict.",
            "It's an obvious and expert and then suture is also around 80% accuracy for 5050 random chat problem.",
            "So this is an interesting direction to go to.",
            "Again, a rich data set and all the parameters are in here and the surgeons are actually they actually really do it.",
            "Use these simulators to evaluate when students ready to perform the surgery.",
            "So this is a much closer to reality.",
            "Real robot as opposed to basically a computer game, is what they were."
        ],
        [
            "Before so basically.",
            "Dynamic Bayes Nets are natural way to get out of the HMM world and I'll stop there.",
            "I guess that was my cue and those are all the different modalities we're looking at and.",
            "Hoping to get some of your feedback the next couple of days.",
            "Thank you.",
            "Time for one or two questions.",
            "Questions here.",
            "All questions are correct.",
            "Can you use the microphone please?",
            "Just wondering.",
            "Available for every business.",
            "I have the best textbook is actually yet to be published.",
            "Michael Jordan has an online version, but I think he's still looking for a publisher.",
            "I I think that is the easiest and most successful accessible textbook right now at Berkeley, yeah?",
            "Yes cs.berkeley.edu/tildejordan.",
            "OK, then I guess we can find some details in your book.",
            "Yeah, there's also some details in my book as well.",
            "So then let's thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm happy to chair this next session.",
                    "label": 0
                },
                {
                    "sent": "My name is Ryan Stifel Hagen and it's going to be a very interesting session right after lunch and in particular we will have a very interesting invited talk given by Tony Jebara.",
                    "label": 0
                },
                {
                    "sent": "You can find his very impressive biography and the material handed out at the reception, but I just want to say one or two words.",
                    "label": 0
                },
                {
                    "sent": "He graduated from MIT Media Lab in 2002.",
                    "label": 0
                },
                {
                    "sent": "He is now with Columbia University.",
                    "label": 0
                },
                {
                    "sent": "He received an NSF career Award, and most important, she has just written a very interesting book.",
                    "label": 0
                },
                {
                    "sent": "Which you can assess why his website about on machine learning and so he's talked today will be machine learning applied to multimodal interfaces.",
                    "label": 0
                },
                {
                    "sent": "So with that I'll have to go to you and.",
                    "label": 0
                },
                {
                    "sent": "Great thanks, I'm very excited to be here.",
                    "label": 0
                },
                {
                    "sent": "Still on New York time because I flew in this morning so I'll try to maintain a steady talking pace, but I'm actually also really excited because a lot of the ideas here are novel for us.",
                    "label": 0
                },
                {
                    "sent": "We're looking at new datasets and I think it's great that we have a community here where people are really doing ambitious things with multiple modalities, specially with the very last data set were working on, which is uncharted territory for us, which is looking at surgical drills and surgical procedures would.",
                    "label": 0
                },
                {
                    "sent": "Laparoscopic robots will talk about that in a second, but we really aren't into a new area here where we don't know anything about the signal properties, the noise, the dynamics, and things like that.",
                    "label": 0
                },
                {
                    "sent": "It's a very complicated robot.",
                    "label": 0
                },
                {
                    "sent": "Cost multiple millions of dollars and so on, but it's actually becoming an industry standard.",
                    "label": 0
                },
                {
                    "sent": "So now we're looking forward to sharing this data and sharing some of the code.",
                    "label": 0
                },
                {
                    "sent": "The main approach is going to be this dynamic Bayesian networks approach, which is popular in the machine learning community.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "I'll be quickly talking about.",
                    "label": 0
                },
                {
                    "sent": "What motivated us to go to DBN's and?",
                    "label": 0
                },
                {
                    "sent": "You can model it depends from a multi model Anna multi person perspective and then we'll just review a little bit about the underlying machine which is really just Bayesian networks and then the standard algorithms behind Bayesian networks and how they can be easily applied to variations of the classical Bayesian networks, which are things like hidden Markov model models and common filters and how we can extend things like the expectation maximization algorithm and matching Michael estimation tenyu graphs that are more relevant to our particular multimodal multi purpose.",
                    "label": 0
                },
                {
                    "sent": "Datasets and then I'll show a few examples and there's just scattered examples of different topologies in these networks.",
                    "label": 0
                },
                {
                    "sent": "The first one is just hidden Arma model and will talk about learning that and doing maximum likelihood and how you train it and then use it to learn interactive gestures.",
                    "label": 0
                },
                {
                    "sent": "Another example is input output hidden Markov models, which also fit the DBN framework, and they can be used to do audiovisual interaction learning on a wearable platform.",
                    "label": 1
                },
                {
                    "sent": "And then we'll move up to what's kind of the.",
                    "label": 0
                },
                {
                    "sent": "Current state of yard in the graphical models community, and that's intractable.",
                    "label": 0
                },
                {
                    "sent": "Dynamic Bayes Nets and there are talks coming up on variational estimation.",
                    "label": 0
                },
                {
                    "sent": "Structure me feel and so on.",
                    "label": 0
                },
                {
                    "sent": "What we talk about generalizing am generalizing the learning algorithms in the inference algorithms to handle graphs that are really intractable.",
                    "label": 0
                },
                {
                    "sent": "Where the distributions blow up on us if we try to do exact inference and then we'll show our particular model which appeared about a year ago at UA I 2004, which is this dynamical systems tree, which is a.",
                    "label": 0
                },
                {
                    "sent": "Tree on top of many different dynamical systems.",
                    "label": 0
                },
                {
                    "sent": "So imagine connecting many common filters and Hmm's in a tree structure and then we'll look at that for multi person visual interaction.",
                    "label": 0
                },
                {
                    "sent": "So football plays by tracking football players.",
                    "label": 0
                },
                {
                    "sent": "This is American football, so there's a.",
                    "label": 0
                },
                {
                    "sent": "There's a handful of players being tracked on the field and then we'll talk about how we were trying to apply this now to surgical drills and then getting other people involved in ongoing directions and how.",
                    "label": 0
                },
                {
                    "sent": "There are some synergies between this type of data.",
                    "label": 0
                },
                {
                    "sent": "And the data.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That you're working with.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's just quickly motivate while we wanted to go beyond the simplest dynamical systems and those dynamical systems are hidden Markov models in common filters and actually has a graph that exact same graphical structure.",
                    "label": 0
                },
                {
                    "sent": "The difference is the common filter has a continuous hidden state and the hidden Markov model has a discrete hidden state, but these are really meant to model one single process in time.",
                    "label": 0
                },
                {
                    "sent": "One single dynamic process with a single kind of Markov assumption.",
                    "label": 0
                },
                {
                    "sent": "But it turns out when we start working with multiple modalities, there's multiple processes, and that's true for combining multiple videos, multiple audios and also heterogeneous modalities, audio and video, partly because there are sometimes occurring in different time scales.",
                    "label": 0
                },
                {
                    "sent": "So audio and video and haptics have different scales of time.",
                    "label": 1
                },
                {
                    "sent": "Also, different amplitude scales also different noise characteristics, so they're not all Gaussian noise types of processes, or they're not all common filters, or they're all.",
                    "label": 0
                },
                {
                    "sent": "Multinomial distributions there's all sorts of unusual noise properties, so we can't just slam them all together into one monolithic model, and that's true.",
                    "label": 0
                },
                {
                    "sent": "Also, we start working with multi person data as well for tracking even a single person may have multiple limbs.",
                    "label": 0
                },
                {
                    "sent": "Each limb has different dynamics.",
                    "label": 0
                },
                {
                    "sent": "You can move your hand more quickly and you can move your upper arm.",
                    "label": 0
                },
                {
                    "sent": "Also, two person and group dynamics also have different speeds of dynamics, and you can also imagine their only weakly coupled, so people don't.",
                    "label": 0
                },
                {
                    "sent": "Walk in lockstep when they're walking down the street together, but they're still somewhat synchronized.",
                    "label": 0
                },
                {
                    "sent": "Someone stops.",
                    "label": 0
                },
                {
                    "sent": "The other person will stop as well.",
                    "label": 0
                },
                {
                    "sent": "The other person will continue with.",
                    "label": 0
                },
                {
                    "sent": "If they're talking with having a conversation.",
                    "label": 0
                },
                {
                    "sent": "So there's we couplings and we want to capture these types of different couplings.",
                    "label": 0
                },
                {
                    "sent": "We don't want to just put everything on to one data set, so if we're given two time series.",
                    "label": 0
                },
                {
                    "sent": "So maybe this is one person walking, and here's another person walking or tracking down limbs.",
                    "label": 0
                },
                {
                    "sent": "We don't want to just put this on to one monolithic time series and say recognized this as these two people.",
                    "label": 0
                },
                {
                    "sent": "Do.",
                    "label": 0
                },
                {
                    "sent": "These two people are walking together because this person might change the phase of their walk.",
                    "label": 0
                },
                {
                    "sent": "Let's say in this person might be walking right stop and then catch up.",
                    "label": 0
                },
                {
                    "sent": "That doesn't mean they're not walking together, but if you put them to one time series, you can only do one kind of dynamic time warping.",
                    "label": 0
                },
                {
                    "sent": "Let's say within Markov models, and they really need to align perfectly as templates.",
                    "label": 0
                },
                {
                    "sent": "Otherwise you don't recognize that.",
                    "label": 0
                },
                {
                    "sent": "Really is the same gesture or the same behavior?",
                    "label": 0
                },
                {
                    "sent": "So we're going to be looking at different ways to zipper multiple interacting processes with different graphical models and different Bayesian networks.",
                    "label": 1
                },
                {
                    "sent": "So let's just do a quick review and for men.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you folks, this might be redundant stuff, but let's just talk a little bit about this elegant framework and how it really is very general, and we don't necessarily.",
                    "label": 0
                },
                {
                    "sent": "Sometimes have to reinvent parts of it because it all fits very nicely together, so Bayesian that's also called graphical models really are just combination of statistics and graph theory and just basically simplifying what might be a huge distribution.",
                    "label": 1
                },
                {
                    "sent": "OK, so when we're dealing with many people many variables, many time steps, the distribution over all these variables is this gigantic distribution over thousands of variables in their cross product.",
                    "label": 0
                },
                {
                    "sent": "So what graphical models and base that's do is they let us simplify this gigantic distribution.",
                    "label": 0
                },
                {
                    "sent": "Into a product of smaller simple distributions, these smaller conditionals.",
                    "label": 0
                },
                {
                    "sent": "So if I have 1000 variables, this thing, if they're all binary, could be 2 to the 1000 large, which means I can't store it on a computer, but instead I know some kind of independence of conditional independence structure between the variables.",
                    "label": 0
                },
                {
                    "sent": "For example, this variable generated these two and then this triggered this one and this one, and then there's a Markov chain of triggering over here.",
                    "label": 1
                },
                {
                    "sent": "Then I can just describe the whole distribution as a product of a bunch of simple conditional distributions of each node given its parent variables.",
                    "label": 0
                },
                {
                    "sent": "So instead of having this 2 to the power of 6.",
                    "label": 0
                },
                {
                    "sent": "Hypercube of values.",
                    "label": 0
                },
                {
                    "sent": "I've got a small 2 by 1 table two by two 2 by 2, two by two by two in it and a two by two by two cube and then I can write this distribution down with many fewer numbers than two to the power of 6.",
                    "label": 0
                },
                {
                    "sent": "So that's what really is happening with this graphical modeling.",
                    "label": 0
                },
                {
                    "sent": "And also it lets us intuitively interpret what's going on that there's this kind of.",
                    "label": 0
                },
                {
                    "sent": "Main trigger variable that triggered this causal flow of or this causal chain reaction or Markov chain over here it's hard.",
                    "label": 0
                },
                {
                    "sent": "It's hard to really say anything about causality with these networks, and if people aren't certain pure causality arguments, we should look at causing that.",
                    "label": 0
                },
                {
                    "sent": "Works in Judea Pearl is a great book about that really.",
                    "label": 0
                },
                {
                    "sent": "We're just talking about conditional dependent.",
                    "label": 0
                },
                {
                    "sent": "So a node basically is written in terms of conditional introduction given its parents in the joint.",
                    "label": 0
                },
                {
                    "sent": "Overall variables is just this product.",
                    "label": 0
                },
                {
                    "sent": "And we can imagine these exercise being discreet and in which case these tables are just contingency tables or multinomial distributions or even continuous.",
                    "label": 0
                },
                {
                    "sent": "So X3 and X5 could be a 2D Gaussian.",
                    "label": 0
                },
                {
                    "sent": "There's a 2D Gaussian relationship if their continuous variables and you could use any any member of the exponential family, not just these two.",
                    "label": 0
                },
                {
                    "sent": "And everything is still fine.",
                    "label": 0
                },
                {
                    "sent": "And typically what people do is they split the Bayes net into a series of hidden variables, an observed variables and I use this color for hidden variables in the future slides and.",
                    "label": 1
                },
                {
                    "sent": "This color for observed variables an there are three basic operations we typically do with Bayes Nets.",
                    "label": 1
                },
                {
                    "sent": "You've got this probability of all the nodes, so this is the set of all visible instead of all hidden variables and it splits this way as a product given some parameters that tell you what to put in for these tables, the three basic operations are well in addition to computing the probability for any specific setting, we'd like to compute marginals over the hidden variables given the observed ones, so I observe.",
                    "label": 0
                },
                {
                    "sent": "Certain variables and I want to query some unknown ones and give me a distribution of that conditional or that marginal.",
                    "label": 1
                },
                {
                    "sent": "Another thing I'd like to do is compute the likelihood of the data so for all the hidden variables, I sum them out and I have observed observed these XV visible variables.",
                    "label": 0
                },
                {
                    "sent": "This is the probability of X given the model.",
                    "label": 1
                },
                {
                    "sent": "So how likely was this observation given my model?",
                    "label": 0
                },
                {
                    "sent": "And then another thing I'd like to do is maximize likelihood.",
                    "label": 0
                },
                {
                    "sent": "The model by adjusting this data or modifying the tables here so that I fit the data better so I can evaluate how well I'm doing with that data.",
                    "label": 0
                },
                {
                    "sent": "I can also push the model towards the data and the two key algorithms for steps wanted.",
                    "label": 0
                },
                {
                    "sent": "The two key algorithms are the junction tree algorithm in the EM algorithm and junction.",
                    "label": 0
                },
                {
                    "sent": "She will help you with this inference problem and compute the likelihood and M maximize the likelihood whenever you have hidden variables.",
                    "label": 0
                },
                {
                    "sent": "OK. And so.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The junction tree algorithm and I'll show I'm just showing a quick overview because a lot of people.",
                    "label": 0
                },
                {
                    "sent": "Have more details on this in different papers, but this is the basic workhorse of the junction of Bayesian Nets and we start off with the Bayesian data.",
                    "label": 1
                },
                {
                    "sent": "Typically we go from that to a junction tree and we do is we moralize the graph by marrying parents.",
                    "label": 0
                },
                {
                    "sent": "So apparent of this note is over here, and here's another parent.",
                    "label": 0
                },
                {
                    "sent": "They're both at the root of these arrows.",
                    "label": 0
                },
                {
                    "sent": "In coming to this common child, we marry them by drawing extra link and then we drop the arrowheads and now we have this moral graph which says that there's dependencies between all the connected variables.",
                    "label": 0
                },
                {
                    "sent": "And then from that moral graph we triangulate it, which prevents cycles of of four or more by drawing an extra link here.",
                    "label": 1
                },
                {
                    "sent": "Then we find all the maximal connected cliques.",
                    "label": 0
                },
                {
                    "sent": "So here's a fully connected clique of nodes X one X2X3.",
                    "label": 0
                },
                {
                    "sent": "Here's another one X3X5X2X2 X 5X6 and X2X4, and then we build this thing called the junction tree, which is a set of mega nodes containing all the nodes in one single clicks or X1X2X3 is right here, and it touches X 2X3X5.",
                    "label": 0
                },
                {
                    "sent": "Over here, which touches X2 X 5X6 over here and so here are the three year four cliques and we sort of out here and between the clicks you write separator potentials that connect them and contain the variables that have in common.",
                    "label": 0
                },
                {
                    "sent": "So X one X2X3 shares X 2X3 with this clique.",
                    "label": 0
                },
                {
                    "sent": "So then we have this junction tree and now we can run the junction tree algorithm.",
                    "label": 1
                },
                {
                    "sent": "And the junction tree algorithm is basically going.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Take this complicated distribution and send messages around to make sure everybody is consistent, and we're mean by consistent is I want the various tables in this junction tree and each potential function, so each one of these cliques.",
                    "label": 0
                },
                {
                    "sent": "We saw earlier has a function over these variables and over these variables over these variables.",
                    "label": 0
                },
                {
                    "sent": "So I have a function over X2 X 3M function over X1X2X3.",
                    "label": 0
                },
                {
                    "sent": "I want all these functions to agree on the variables they share.",
                    "label": 0
                },
                {
                    "sent": "OK, if I ask this function, what does it think is a likely value for X2 should actually be true or false?",
                    "label": 0
                },
                {
                    "sent": "Well, this one likes that extrudes true, but this one likes to say X2 is false.",
                    "label": 0
                },
                {
                    "sent": "They're not in agreement, so the junction tree algorithm is going to just all the tables.",
                    "label": 1
                },
                {
                    "sent": "By sending messages from each clique to its neighboring cliques, saying this is what I think the marginal is over this shared variable.",
                    "label": 0
                },
                {
                    "sent": "So maybe will send a message to BC saying if I sum out over my 2 dimensional table to get entries Overby.",
                    "label": 0
                },
                {
                    "sent": "Here's what I think the entry should be.",
                    "label": 0
                },
                {
                    "sent": "An NBC gets that message and then updates its table so that if it were to come out over C you would get the same table for being.",
                    "label": 0
                },
                {
                    "sent": "So basically you get a two by two table here 21 dimensional to buy one table and two by two table and summing over this table should give you the same value.",
                    "label": 0
                },
                {
                    "sent": "Something over this table over C. So that's what we mean by message passing.",
                    "label": 0
                },
                {
                    "sent": "And so the separator gets updated.",
                    "label": 0
                },
                {
                    "sent": "Then you update the clique with this message.",
                    "label": 0
                },
                {
                    "sent": "Then you do the same for the reverse message.",
                    "label": 0
                },
                {
                    "sent": "Then both potentials are in agreement.",
                    "label": 0
                },
                {
                    "sent": "We have a consistent distribution.",
                    "label": 0
                },
                {
                    "sent": "An there's something very valuable about that, because once the junction tree algorithm, this consistency and we have marginal distributions that are correct and on a tree this junction tree algorithm is guaranteed, and you typically proceed in two steps, you initialize all your clique potentials and your separate potentials.",
                    "label": 0
                },
                {
                    "sent": "Then you pull messages up towards the root, everybody updates.",
                    "label": 0
                },
                {
                    "sent": "Let's say upstream until you get to this root.",
                    "label": 0
                },
                {
                    "sent": "Everybody updates their tables and then the root updates and send messages back downstream to all the individual leads and at the end the junction trees done.",
                    "label": 0
                },
                {
                    "sent": "You sent these sum and product messages.",
                    "label": 0
                },
                {
                    "sent": "It's also called the sum product algorithm because of this.",
                    "label": 0
                },
                {
                    "sent": "And at the very end, when you read off your clique tables in your potential tables, you're separated.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "They all become potentials.",
                    "label": 0
                },
                {
                    "sent": "They all become marginals or conditionals over the hidden variables given the data.",
                    "label": 1
                },
                {
                    "sent": "So that's what we wanted for inference.",
                    "label": 0
                },
                {
                    "sent": "So each one of these.",
                    "label": 0
                },
                {
                    "sent": "Clicks now becomes a probability of the variables.",
                    "label": 1
                },
                {
                    "sent": "It's describing, given all the observed variables that you've actually observed before we started running the junction tree algorithm.",
                    "label": 0
                },
                {
                    "sent": "So we have all these marginals telling you what configurations are likely for the hidden variable.",
                    "label": 0
                },
                {
                    "sent": "Here this in variable, then this pair of hidden variables as a 2 dimensional table.",
                    "label": 0
                },
                {
                    "sent": "And also the normalizer for each potential function is just the likelihood of the data, the probability of the data if I were to sum over the hidden variables.",
                    "label": 0
                },
                {
                    "sent": "So for basically running one.",
                    "label": 0
                },
                {
                    "sent": "Very efficient algorithm on a tree just to collect and distribute.",
                    "label": 0
                },
                {
                    "sent": "We have.",
                    "label": 0
                },
                {
                    "sent": "All these interesting marginals and conditionals over hidden variables, given the data and the likelihood of the data given the model.",
                    "label": 0
                },
                {
                    "sent": "OK, so that steps one or two that we like to do with.",
                    "label": 0
                },
                {
                    "sent": "Without Bayes Nets.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then another step is maximizing the likelihood, so I can evaluate that likelihood.",
                    "label": 0
                },
                {
                    "sent": "But I also want to increase it by wiggling some parameters, adjusting entries in my tables or adjusting some Gaussian means and covariances.",
                    "label": 0
                },
                {
                    "sent": "So we'd like to maximize the likelihood by summing over the hidden variables.",
                    "label": 1
                },
                {
                    "sent": "Basically, the probability that the XV given the Theta when you come out over XH, it disappears, so that's what maximum likelihood is trying to do.",
                    "label": 0
                },
                {
                    "sent": "Typically, if this isn't easy.",
                    "label": 0
                },
                {
                    "sent": "No hidden problem maximization, we could just take derivatives and set to zero over Theta and get back the most likely setting.",
                    "label": 0
                },
                {
                    "sent": "But instead if there's there are some issues over hidden variables.",
                    "label": 0
                },
                {
                    "sent": "These derivatives aren't analytic, we can't do them in close form.",
                    "label": 1
                },
                {
                    "sent": "So instead we apply this expectation maximization algorithm EM, which basically maximizes a lower bound on the log likelihood.",
                    "label": 0
                },
                {
                    "sent": "So instead of maximizing this, we maximize the log likelihood.",
                    "label": 0
                },
                {
                    "sent": "That's OK, it's the exact same problem.",
                    "label": 0
                },
                {
                    "sent": "But we apply Jensen's inequality to rewrite this as the expected complete likelihood plus some KL divergent between this proposal distribution and the posterior over the hidden variables given some parameters.",
                    "label": 0
                },
                {
                    "sent": "So this turns out to be a lot easier to work with than this.",
                    "label": 0
                },
                {
                    "sent": "This is a function over some proposal distribution Q over the hidden variable.",
                    "label": 0
                },
                {
                    "sent": "That I want to maximize andsome Theta, so this is a function over 2 variables Q and Theta which is always less than the original log likelihood of data.",
                    "label": 0
                },
                {
                    "sent": "So instead of maximizing in one fell swoop over Theta, the log likelihood I literally maximize this bound, which has two parameters.",
                    "label": 0
                },
                {
                    "sent": "This proposal, distribution of what my guess is, the distribution of the hidden variable should be.",
                    "label": 0
                },
                {
                    "sent": "That's the queue, and then the parameters for this lower bound.",
                    "label": 1
                },
                {
                    "sent": "So this looks like this access parallel optimization we maximize over Q that pushes this lower bound up the maximizing with it, and we keep pushing up or slowly going to increase this thing by iteratively increasing this thing.",
                    "label": 0
                },
                {
                    "sent": "And if we start off here, we first maximize over Q.",
                    "label": 0
                },
                {
                    "sent": "This gives us the best Q, then the best day to the best Q bestatin.",
                    "label": 0
                },
                {
                    "sent": "So we iteratively maximize and this is easy to maximize over Theta and easy to maximize over Q.",
                    "label": 0
                },
                {
                    "sent": "Except I can get stuck in local minima.",
                    "label": 0
                },
                {
                    "sent": "So over here if I started with this state I guess.",
                    "label": 0
                },
                {
                    "sent": "Then I would get this best Q than this best data this best Q.",
                    "label": 0
                },
                {
                    "sent": "So each step is going to give you the best Q and the best data we need to start at a good place to get to the good global optimum.",
                    "label": 0
                },
                {
                    "sent": "And what's going on is in the eastep for maximizing this functional over.",
                    "label": 0
                },
                {
                    "sent": "Distributions of hidden variables and parameters.",
                    "label": 0
                },
                {
                    "sent": "And it turns out the most like the highest value of this function is when we set the Q distribution equal to the.",
                    "label": 0
                },
                {
                    "sent": "Posterior distribution over the hidden variables given the observed variables and current guest of your model parameters.",
                    "label": 0
                },
                {
                    "sent": "OK, so this takes the lower bound.",
                    "label": 0
                },
                {
                    "sent": "And maximizing such that it hits the log likelihood.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                },
                {
                    "sent": "This is the right function.",
                    "label": 0
                },
                {
                    "sent": "Is this L the capital L function it touches makes this lower red bound kiss the log likelihood, and then we maximize over Theta to get to the peak of red lower bound downstairs and I give you the best model parameter.",
                    "label": 0
                },
                {
                    "sent": "So you iterate these two steps and then you train your models on data.",
                    "label": 0
                },
                {
                    "sent": "So that's all there is to Bayesian Nets an at the end of the day those are the three key steps that you need to do, no matter how complicated the network gets.",
                    "label": 0
                },
                {
                    "sent": "Fundamentally, maximizing this and maximizing this.",
                    "label": 0
                },
                {
                    "sent": "And running junction tree algorithm because we need XH given XP and that was the marginals and the conditionals that we got out of Junction tree algorithm.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of a full clause world that we need in terms of Bayes net tools.",
                    "label": 0
                },
                {
                    "sent": "Obviously there's extra extensions of things, So what are dynamic Bayes Nets will?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just to keep things simple, dynamic Bayes Nets are just Bayes Nets that are unrolled in time.",
                    "label": 0
                },
                {
                    "sent": "So instead of just a network of static variables, you've got this timestamp for every single variable, and this is time T = 0 = 1 Tables, 2 tables, three an really.",
                    "label": 0
                },
                {
                    "sent": "What's going on is something is happening in time, but I'm just representing it as eight random variables in.",
                    "label": 0
                },
                {
                    "sent": "Kind of probability distribution and this can continue indefinitely if it's a really long time series.",
                    "label": 0
                },
                {
                    "sent": "And so the two classical examples of dynamic Bayes Nets are the hidden Markov model over here and.",
                    "label": 1
                },
                {
                    "sent": "This is in yellow, which means this is a hidden state that evolves with Markov dynamics, so the current state depends on my past date.",
                    "label": 0
                },
                {
                    "sent": "Once I know my current state, I generate an observation that's related to my current state.",
                    "label": 0
                },
                {
                    "sent": "So imagine this is somebody's mood, we can't directly observe their happy, sad, or neutral, but we can look at how much they're smiling down here.",
                    "label": 0
                },
                {
                    "sent": "And so we typically evolve from happy to neutral and then from neutral, sad and jump back and forth between two extremes.",
                    "label": 0
                },
                {
                    "sent": "There is some kind of Markov assumption here telling you how you evolve between S1 and S2.",
                    "label": 0
                },
                {
                    "sent": "Typically that's described as state transition matrix.",
                    "label": 0
                },
                {
                    "sent": "Given us two, I'll tell you how much the person smiling and then S3 persons frowning etc.",
                    "label": 0
                },
                {
                    "sent": "So these are observations and there's a hidden state and the state evolves as this matrix of values probability of the current state.",
                    "label": 0
                },
                {
                    "sent": "Depends on the past state.",
                    "label": 0
                },
                {
                    "sent": "We have this prior over initial prior distribution over the first set of States and then the emission model is given.",
                    "label": 0
                },
                {
                    "sent": "My states.",
                    "label": 0
                },
                {
                    "sent": "I'm going to sample from another table which gives you my observations or a Gaussian which gives me continuous observations.",
                    "label": 0
                },
                {
                    "sent": "That's all hidden Markov model is, so the hidden states might be phonemes in a speech recognizer and then the samples are the raster vector of coefficients or the capsule coefficient, let's say from a single Gaussian centered around that.",
                    "label": 0
                },
                {
                    "sent": "Pony and then similarly we have the same topology here, but now it's a linear dynamical system or common filter.",
                    "label": 0
                },
                {
                    "sent": "And here we have X 0 being continuous vector instead of a discrete state vector.",
                    "label": 0
                },
                {
                    "sent": "And now this thing also evolves as a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So given my past I sample around my pass X1 with the Gaussian model something like this.",
                    "label": 0
                },
                {
                    "sent": "Which is a linear function, so this is good for things like tracking or.",
                    "label": 0
                },
                {
                    "sent": "Flying planes and things like that.",
                    "label": 0
                },
                {
                    "sent": "It's not good for discrete types of hidden state problems, and then given my hidden state I sample also an admission.",
                    "label": 0
                },
                {
                    "sent": "So those are the two classical things, but in the in the DBN framework they look pretty much the same and it turns out that junction tree steps and the junction tree structure look pretty much the same as well.",
                    "label": 0
                },
                {
                    "sent": "So once we've done one, we've triangulated moralise.",
                    "label": 0
                },
                {
                    "sent": "We figured out how to do.",
                    "label": 0
                },
                {
                    "sent": "Message passing is pretty much the same for the other.",
                    "label": 0
                },
                {
                    "sent": "We just need to do some integration status summation for the Gaussian, yeah?",
                    "label": 0
                },
                {
                    "sent": "Sure.",
                    "label": 0
                },
                {
                    "sent": "Are you limited to the assumption that the state transition model is Gaussian?",
                    "label": 0
                },
                {
                    "sent": "You're a junction tree album.",
                    "label": 0
                },
                {
                    "sent": "You don't have to stick to Gaussians against do other exponential families, but things do sometimes get nasty if you move beyond Gaussian or multi million.",
                    "label": 0
                },
                {
                    "sent": "Mixture.",
                    "label": 0
                },
                {
                    "sent": "If it is a mixture, then things might blow up with a junction tree algorithm, so complexity might grow.",
                    "label": 0
                },
                {
                    "sent": "If you're doing a mixture of Gaussians, each time you propagate a message that mixture of Gaussians might go from 10 Gaussians to 100 to 1000, with each step of the propagation.",
                    "label": 0
                },
                {
                    "sent": "So that's an open problem right now is how to mitigate that exponential growth as you do junction Tree message passing when you have non Gaussian distributions.",
                    "label": 0
                },
                {
                    "sent": "Assumption for your addition model.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you could move away from the Gaussian emission model, and there's things like the kernelized Kalman filter, which is a nonlinear Gaussian in some other space abende Gaussian fuel.",
                    "label": 0
                },
                {
                    "sent": "So people have started doing things like that as well.",
                    "label": 0
                },
                {
                    "sent": "It also helps avoid putting nonlinearities in here, which can cause problems with junction tree.",
                    "label": 0
                },
                {
                    "sent": "So these are all kind of active areas.",
                    "label": 0
                },
                {
                    "sent": "Definitely in modifying these models and so adding these mixtures or nonlinearities extended common filters, and so on.",
                    "label": 0
                },
                {
                    "sent": "An people in the control literature are doing some of this people in the.",
                    "label": 0
                },
                {
                    "sent": "Kind of Markov random fields, literature and so on.",
                    "label": 0
                },
                {
                    "sent": "So there's there's in physics as well.",
                    "label": 0
                },
                {
                    "sent": "Variations of these non Gaussian models.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "It's hard to keep track of everybody, but.",
                    "label": 0
                },
                {
                    "sent": "We're going to.",
                    "label": 0
                },
                {
                    "sent": "We're also going to go in there and modify the graphical models a little bit.",
                    "label": 0
                },
                {
                    "sent": "So one thing we want to do is to look at two person interaction and see if there's two people interacting.",
                    "label": 0
                },
                {
                    "sent": "What can we learn about two people person?",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Let's say in person X.",
                    "label": 0
                },
                {
                    "sent": "So here's person wires, person X.",
                    "label": 0
                },
                {
                    "sent": "They're interacting and we can try.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Back this person and synthesize some.",
                    "label": 0
                },
                {
                    "sent": "Caricature them to this person and then they're just going through this virtual interaction over the Internet.",
                    "label": 0
                },
                {
                    "sent": "Let's say we'd like to watch two people interact, so we get a model of the behavior of Y given the behavior of X, and if we learn enough about this distribution, then I can mimic this person.",
                    "label": 0
                },
                {
                    "sent": "With basically a distribution and predict what's the most likely why?",
                    "label": 0
                },
                {
                    "sent": "Let's say for this human or sorry, so This is why what was the most likely wise response to what X just did, and we can learn these simple kind of stimulus response types of behaviors.",
                    "label": 0
                },
                {
                    "sent": "So what's one way to take these two people?",
                    "label": 0
                },
                {
                    "sent": "And applying Markov models to them?",
                    "label": 0
                },
                {
                    "sent": "Well, we could just take 1 hidden Markov model for each person and then model this person within HMM and model this person within HMM.",
                    "label": 1
                },
                {
                    "sent": "But then there's no coupling.",
                    "label": 0
                },
                {
                    "sent": "So when I try to predict this person's behavior from this one well this person is just their own hidden Markov model.",
                    "label": 0
                },
                {
                    "sent": "Let's say we're tracking their mouth and we're seeing their moods and they have happy neutral.",
                    "label": 0
                },
                {
                    "sent": "Sad well I just model them by themselves.",
                    "label": 0
                },
                {
                    "sent": "I can't synthesize a virtual person.",
                    "label": 0
                },
                {
                    "sent": "So that's no coupling.",
                    "label": 0
                },
                {
                    "sent": "Another way is just to take both their time series data and learn one giant in Markov model for both, but that's too rigid.",
                    "label": 1
                },
                {
                    "sent": "Again, that's the problem of just slamming everything into one time series.",
                    "label": 0
                },
                {
                    "sent": "Then if something happens at slightly out of lockstep, one person smiles one second too late or too early.",
                    "label": 0
                },
                {
                    "sent": "Then it looks like a completely new pattern an you don't recognize it.",
                    "label": 0
                },
                {
                    "sent": "That was the same behavior.",
                    "label": 0
                },
                {
                    "sent": "So what we'd like to do is do something a little more flexible than just put things together or just keeping keeping them apart.",
                    "label": 0
                },
                {
                    "sent": "Some kind of slower coupling.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And So what we're going to do is we're going to try this hidden Arma model first, an arm.",
                    "label": 0
                },
                {
                    "sent": "A stands for autoregressive moving average, and we're going to take the past of this person X.",
                    "label": 0
                },
                {
                    "sent": "In the past of this person Y and predict the next.",
                    "label": 1
                },
                {
                    "sent": "Behavior over here for this person and then instant future.",
                    "label": 0
                },
                {
                    "sent": "So we're going to watch two people talking.",
                    "label": 0
                },
                {
                    "sent": "And as this person moves our lips, this person stays quiet.",
                    "label": 0
                },
                {
                    "sent": "When this person stops talking.",
                    "label": 0
                },
                {
                    "sent": "Typically that's when this person can take the floor, so we're just tracking the openness of the mouth here.",
                    "label": 0
                },
                {
                    "sent": "This person talks and then stops talking.",
                    "label": 0
                },
                {
                    "sent": "That means chances are this person should start.",
                    "label": 0
                },
                {
                    "sent": "So we've learned this two person interaction with the way we're going to do that.",
                    "label": 0
                },
                {
                    "sent": "We're not just going to slam everything together, even though in a sense this is a little slam together.",
                    "label": 0
                },
                {
                    "sent": "We're going to fix that later on, but we're going to learn to predict Y.",
                    "label": 0
                },
                {
                    "sent": "From the past X&Y here.",
                    "label": 0
                },
                {
                    "sent": "The past of this person in the past of this person to predict what this person should do next.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that this can be thought of as an ARMA model.",
                    "label": 1
                },
                {
                    "sent": "We're also going to make it a hidden Arma model.",
                    "label": 0
                },
                {
                    "sent": "And we're going to focus on predicting what person Y will do from both their past and the other person's past an it's a hidden ARMA model because in our model is really a linear model at the moving average auto regressive moving average.",
                    "label": 1
                },
                {
                    "sent": "And instead we're going to have many auto regressive moving averages which capture different regimes of behavior, and we're going to switch between them with this S. Discrete variable.",
                    "label": 0
                },
                {
                    "sent": "So this allows us to make a nonlinear Arma model.",
                    "label": 0
                },
                {
                    "sent": "Basically because instead of having a single ARMA model, we're switching between many different linear ones and that pieces together is.",
                    "label": 0
                },
                {
                    "sent": "And a linear landscape into this nonlinear type of landscape.",
                    "label": 0
                },
                {
                    "sent": "And so here is.",
                    "label": 0
                },
                {
                    "sent": "And we're also going to do PCA to really allow long-term moving averages of windows of 100, let's say, as opposed to really short term moving averages, which is maybe more typical for things like stock market prediction.",
                    "label": 0
                },
                {
                    "sent": "So here's an example of just a second order moving average.",
                    "label": 0
                },
                {
                    "sent": "Here's the person we're trying to predict, evolving in time, and we're going to protect them over here based on what they just did and what they did 2 steps ago and what the other person X1 just did, and let the other.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Person X1X0 did 2 steps ago and I'm going to pick which Arma model to use with S2 so as to is the choice of which ARMA model.",
                    "label": 0
                },
                {
                    "sent": "And it doesn't matter that you just use one normal model, you're just going to pick which one out of my five ARMA models.",
                    "label": 0
                },
                {
                    "sent": "Once I picked it, I just know how to map these four values into a prediction through a linear mapping.",
                    "label": 0
                },
                {
                    "sent": "So now we just see that this classical.",
                    "label": 0
                },
                {
                    "sent": "Time series forecasting method in statistics.",
                    "label": 0
                },
                {
                    "sent": "An in other application areas can be written as a graphical model and folks at Microsoft Research are actually the ones that we're proposing this it was at UA 2004 that you can describe a variety of Arma Arima models as graphical models and be very Bayesian and very Bayesian network about your treatment of them.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the model and the features we've.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for the model or just the head and hand coordinates for each person, two people go out and interact and play these gestural games.",
                    "label": 0
                },
                {
                    "sent": "And the setup looks something like this.",
                    "label": 0
                },
                {
                    "sent": "Let me just go into it.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We've got two people sitting in front of computer screens, just waving at each other and playing little games.",
                    "label": 1
                },
                {
                    "sent": "We get this time series of a few minutes of this type of behavior, probably about 15 minutes.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Each person is tracked by forming again a distribution over RGB by three Gaussians describing the color of skin that gives you these types of images from the video, which are all only the skin colors, and these are clustered with a mixture of spatial Gaussians, three spatial Gaussians in 2D will cluster the coordinates of the head in hands.",
                    "label": 0
                },
                {
                    "sent": "And here's an example of that.",
                    "label": 0
                },
                {
                    "sent": "Looks like as you track the person.",
                    "label": 0
                },
                {
                    "sent": "So this is just to get some raw features.",
                    "label": 0
                },
                {
                    "sent": "You could use any of your own preferred features or the promise, not video.",
                    "label": 0
                },
                {
                    "sent": "It's something else you you might have.",
                    "label": 0
                },
                {
                    "sent": "Data, gloves that give you raw measurements, but you're getting this time series of the means and covariances of these three Gaussians as a person is being tracked in line, so you just collect this data set for a few minutes.",
                    "label": 1
                },
                {
                    "sent": "And then you form this basic Arma model.",
                    "label": 0
                },
                {
                    "sent": "And here's kind of a gimmicky picture of what's going on with the Arma model.",
                    "label": 0
                },
                {
                    "sent": "And then we want to maximize likely our model if we really want to be standard.",
                    "label": 0
                },
                {
                    "sent": "Bayes Nets practitioners, but it turns out we only need Y from X.",
                    "label": 0
                },
                {
                    "sent": "We're not going to try to predict both people, we just want to predict one person from the other.",
                    "label": 0
                },
                {
                    "sent": "So it turns out condition.",
                    "label": 0
                },
                {
                    "sent": "Likely it does a little bit better and there's a conditional variance of them called conditionally M. Which basically, if all the care.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About his probability of the visible wise given the visible X is you want to really maximize that model, not just the probability of XD, YV, which is maximum likelihood, then should maximize the conditional likelihood, which is this.",
                    "label": 0
                },
                {
                    "sent": "Original log likelihood minus the log likelihood of only the X data.",
                    "label": 0
                },
                {
                    "sent": "So X, y -- X.",
                    "label": 0
                },
                {
                    "sent": "An as an example, if you wanted to do if why was a label and you want to want to classify red circles and Blue Cross is if you just did Yemen maximum like you would get this really bad classifier.",
                    "label": 0
                },
                {
                    "sent": "Here's some red data here and here in Blue data here and here.",
                    "label": 0
                },
                {
                    "sent": "Read data here in here Blue data here and here.",
                    "label": 0
                },
                {
                    "sent": "This is the maximum liquid solution, but it actually splits the data set down the middle.",
                    "label": 0
                },
                {
                    "sent": "This way the top half is where the red Gaussians dominate the bottom half with the blue Gaussians dominate.",
                    "label": 0
                },
                {
                    "sent": "And that's basically 5050.",
                    "label": 0
                },
                {
                    "sent": "Classification accuracy if you take the same modeling, maximizes conditional likelihood.",
                    "label": 0
                },
                {
                    "sent": "You get a red, blue or red or blue Gaussian negative perfect classification accuracy.",
                    "label": 0
                },
                {
                    "sent": "So if you really only care about making predictions on Y from X, it's better to match some conditional likelihood.",
                    "label": 0
                },
                {
                    "sent": "And now.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now actually, people are moving towards maximum margin, which is some work by Daphne Kohler and Taskbar, which is even better criterion the maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "And here's another example of what.",
                    "label": 0
                },
                {
                    "sent": "This looks like in a regression setting.",
                    "label": 0
                },
                {
                    "sent": "So if you were to take two Gaussians and try to fit this regression problem, we're trying to predict Y from X, you would get this behavior out of them.",
                    "label": 0
                },
                {
                    "sent": "These are two Gaussians and this is basically saying that X&Y are related through some step function.",
                    "label": 0
                },
                {
                    "sent": "But the higher the better solutions actually to capture that there's some linear relation between X&Y and that's when you do maximum conditional likelihood and you get from the same starting point.",
                    "label": 0
                },
                {
                    "sent": "The fact that there's these two possible answers for each X there's two possible correct answers for why in their linearly related.",
                    "label": 0
                },
                {
                    "sent": "That's lossed that when you just do maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "How my doing on time?",
                    "label": 0
                },
                {
                    "sent": "You have 5 more minutes, 7 more minutes.",
                    "label": 0
                },
                {
                    "sent": "OK, better hurry up wow.",
                    "label": 0
                },
                {
                    "sent": "So we did this model and.",
                    "label": 0
                },
                {
                    "sent": "I'll have to speed up a little bit.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry bout this ranted trained it with conditionally M. This is this length of the short term memory and then we have some simulations of.",
                    "label": 0
                },
                {
                    "sent": "The behavior now, if you actually go up to the system and it's it's predicts the most likely.",
                    "label": 0
                },
                {
                    "sent": "Why using this model so you can go up and scare an in real time.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It will generate this figure that acts like it's scared of the person in front of it.",
                    "label": 0
                },
                {
                    "sent": "You can wave and then it'll wave back and it's just learned this from a few minutes of time series data.",
                    "label": 0
                },
                {
                    "sent": "And you can do things like clap, do something silly and it'll start clapping just because I watched a few minutes of people.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Interacting that way, another system which we tried is this wearable system where we try to collect data of the person's face and their audio in response to everything going on in front of them, and what we did there was we applied input output in Markov models, which Samy Bengio did a lot of pioneering work on what we're trying to predict an agent's behavior from the outer world stimulus response, but from both audio and video channels anile there instead of using HM.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We use an input output HMM, where you've got the world's observations and the agents observations, and there's a common state that connects the two, and then we maximize the likelihood of the agent given the world.",
                    "label": 0
                },
                {
                    "sent": "So again, this is the dynamic Bayes net.",
                    "label": 0
                },
                {
                    "sent": "This is the criterion.",
                    "label": 0
                },
                {
                    "sent": "And it's better than doing it regular hidden Markov model because it allows you to distinguish agent from world and say I really want to go from the world inputs to the agents actions.",
                    "label": 0
                },
                {
                    "sent": "Here's what the data looks like.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like this is.",
                    "label": 0
                },
                {
                    "sent": "The audio data.",
                    "label": 0
                },
                {
                    "sent": "This is some of the video being collected.",
                    "label": 0
                },
                {
                    "sent": "These are just.",
                    "label": 0
                },
                {
                    "sent": "Coefficients from the spectrograms.",
                    "label": 0
                },
                {
                    "sent": "I'll show a little bit of this just in the interest of time.",
                    "label": 0
                },
                {
                    "sent": "So we collected a few hours of this type of data just walking around hallways and.",
                    "label": 0
                },
                {
                    "sent": "Getting the face image and its response to the audio in the video in its own video response.",
                    "label": 0
                },
                {
                    "sent": "Just talking to people in.",
                    "label": 0
                },
                {
                    "sent": "Various places in the in the building.",
                    "label": 0
                },
                {
                    "sent": "And these are just some of the features we use.",
                    "label": 0
                },
                {
                    "sent": "We're not really focused on the features you can do better jobs with features over here.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "The data and then we.",
                    "label": 0
                },
                {
                    "sent": "We represented.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is using some variant of PCA which does better than regular PCA.",
                    "label": 0
                },
                {
                    "sent": "Not going to describe this into too much detail.",
                    "label": 0
                },
                {
                    "sent": "Basically it allows you to do PCA while connecting the different landmarks of the faces together, so you find that I line up with eyes and nose is lined up with noses before you apply PCA, and that does a lot better.",
                    "label": 0
                },
                {
                    "sent": "In fact, it.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As I think 2 1/2 orders of magnitude better reconstruction accuracy than just vanilla PCA applied to the images.",
                    "label": 0
                },
                {
                    "sent": "So that's what we did to represent the videos and then we trained this.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Input output hidden Markov models that take the audio and video of this person and.",
                    "label": 0
                },
                {
                    "sent": "And this person passed audio and video to predict their next video.",
                    "label": 0
                },
                {
                    "sent": "And then to predict their next audio as well.",
                    "label": 0
                },
                {
                    "sent": "And, um.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, we maximize this with CEM.",
                    "label": 0
                },
                {
                    "sent": "We formed two input output models, one which predicts the agents audio given world and World Audio World video and agents video.",
                    "label": 0
                },
                {
                    "sent": "Given World Audio World video.",
                    "label": 0
                },
                {
                    "sent": "And these are the models look like looks like and then we subtract the log likelihood of the probability of the world to get the conditional, and this was modeled as 60 dimension, 60 State, 82 dimensional hidden Markov model.",
                    "label": 0
                },
                {
                    "sent": "And this is the size of the training and testing datasets and we can see the.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All likelihood, so this is the training.",
                    "label": 0
                },
                {
                    "sent": "And this is the conditional likelihood.",
                    "label": 0
                },
                {
                    "sent": "I think the EMH algorithms in red and on joint likely the end of those great, but on condition likely it doesn't do so well.",
                    "label": 0
                },
                {
                    "sent": "You'd rather do this CEM algorithm, which outperforms it and then we can re synthesize audio and video from new test data by training on the actual audio video interactions.",
                    "label": 0
                },
                {
                    "sent": "It doesn't really learn anything that deep, it really just learns how to.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Synchronize.",
                    "label": 0
                },
                {
                    "sent": "The face to the outside stimulus.",
                    "label": 0
                },
                {
                    "sent": "Somebody speaking the face starts to move, otherwise the face doesn't move an it basically Mumbles if there is audio just learns a very simple type of behavior out of the 1st order and I didn't plug in my audio but I'm running low on time anyway and I really want to show this surgical robotics data set.",
                    "label": 0
                },
                {
                    "sent": "So here it was showing the simulation and then it does a.",
                    "label": 0
                },
                {
                    "sent": "So when the person speaks, the face moves, otherwise it stays pretty still.",
                    "label": 0
                },
                {
                    "sent": "I will show some.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Quad quantitative results in a second, so these are all for re synthesis.",
                    "label": 0
                },
                {
                    "sent": "We can also move up to intractable dynamic Bayes Nets.",
                    "label": 1
                },
                {
                    "sent": "So for example, these things called factorial hidden Markov models that zoom in Garman, he's been working on coupled hidden Markov models at Michael Jordan, and that Brad has been working on that.",
                    "label": 1
                },
                {
                    "sent": "When you try to couple of multiple people together, so I might have five people interacting in a couple of this way, or two people interacting.",
                    "label": 0
                },
                {
                    "sent": "A couple of them this way through their hidden states.",
                    "label": 0
                },
                {
                    "sent": "So these become attractable.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because the Q distribution over here when we start doing maximum likelihood ATM, the Q distribution made to maximize the optimal setting of that Q distribution is this gigantic distribution again, which we can't write nicely as a product of conditionals.",
                    "label": 0
                },
                {
                    "sent": "It explodes on us, so we can't find the queue that optimizes this bound.",
                    "label": 0
                },
                {
                    "sent": "So instead will restrict the queue and say, you know, I don't find the best Q.",
                    "label": 0
                },
                {
                    "sent": "Find something that's a little bit better than the previous guest and only explore some subset of.",
                    "label": 0
                },
                {
                    "sent": "Compact cues while you're maximizing this bound.",
                    "label": 0
                },
                {
                    "sent": "So don't take the best step towards the optimal solution.",
                    "label": 0
                },
                {
                    "sent": "Take 1/2 step, but just don't make a very complicated Q distribution.",
                    "label": 0
                },
                {
                    "sent": "Make one that's still something we can write down on computer, and so that's what this variational EM or generalized them in some Giants does.",
                    "label": 0
                },
                {
                    "sent": "You basically maximize this, but you limit the queue that you explore to be factorized.",
                    "label": 0
                },
                {
                    "sent": "So it's not this arbitrary Q distribution which might explode on you, and then you maximize that data and you alternate back and forth.",
                    "label": 0
                },
                {
                    "sent": "So this takes partial steps and it doesn't push the bound quite up, and railing against the log likelihood.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That means, as you can force the chains in the factory hidden Markov model to be just chains instead of coupling them together and the couple Markov model as well.",
                    "label": 1
                },
                {
                    "sent": "This is called structured mean field.",
                    "label": 0
                },
                {
                    "sent": "So instead of having all possible couplings, we just separate the chains.",
                    "label": 0
                },
                {
                    "sent": "And now we can still do tractable.",
                    "label": 0
                },
                {
                    "sent": "Perative EM and maximize like iteratively.",
                    "label": 0
                },
                {
                    "sent": "So what we want is something even more flexible.",
                    "label": 0
                },
                {
                    "sent": "A couple of many people, as we've we've proposed, this dynamical systems tree when we want a hierarchy of coupling.",
                    "label": 0
                },
                {
                    "sent": "So imagine coupling 1000 people in a University from students in different departments in different schools in different universities.",
                    "label": 0
                },
                {
                    "sent": "You want to slowly hierarchically couple people because not everybody interacts all at once in homogeneous way.",
                    "label": 0
                },
                {
                    "sent": "So instead we have each person over here as a.",
                    "label": 0
                },
                {
                    "sent": "As a.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hmm, or common filter actually switch common filter and we stitch them together in pairs.",
                    "label": 0
                },
                {
                    "sent": "So imagine this is a doubles tennis game with one person here.",
                    "label": 0
                },
                {
                    "sent": "One person here, one person here in one person.",
                    "label": 0
                },
                {
                    "sent": "Here these two people are on the same team there stitched together 1st and then the two teams are playing against each other in the same game in Denver stitched together.",
                    "label": 0
                },
                {
                    "sent": "So this is like a doubles tennis type of interaction and now we can again apply the same trick to this which is.",
                    "label": 0
                },
                {
                    "sent": "To apply.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I this structured mean field approximation and just make these all just chains and break the links, iteratively maximize the bound for M and it turns out when it's a tree structure connecting these changes, you can write a generic piece of code that does recursive things and solves for any topology.",
                    "label": 0
                },
                {
                    "sent": "So people want to download our code.",
                    "label": 0
                },
                {
                    "sent": "It's basically any topology connection of chains, just give me how you want to connect your 500 chains or 20 chains into whatever hierarchy.",
                    "label": 0
                },
                {
                    "sent": "And the code will just figure it out.",
                    "label": 0
                },
                {
                    "sent": "You don't have to re write new code the way these other techniques needed when it was factorial or coupled for each different topology.",
                    "label": 0
                },
                {
                    "sent": "Just because it's a tree and we can write it recursively.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So the basic idea.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As we do variational inference.",
                    "label": 0
                },
                {
                    "sent": "This way in and up the tree, so it's a recursive way of applying structured midfield.",
                    "label": 0
                },
                {
                    "sent": "And we apply.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This two football play recognition where we have little over a dozen different players and each person as an XY coordinate and we track them.",
                    "label": 0
                },
                {
                    "sent": "And then we decided.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Try to classify two different types of plays.",
                    "label": 1
                },
                {
                    "sent": "Wham plays and big plays.",
                    "label": 0
                },
                {
                    "sent": "And we basically built a model of each and salt on training data.",
                    "label": 0
                },
                {
                    "sent": "About half a dozen of each plate, and then for another half dozen test plays, which model overwhelm the other.",
                    "label": 0
                },
                {
                    "sent": "And that's how we classify the new query plays, so these are different strategies for the American Football games, and we explore just combining.",
                    "label": 0
                },
                {
                    "sent": "Everybody has their own individual common filters or their own individual Markov models, so breaking all the links and you don't classify the queries.",
                    "label": 0
                },
                {
                    "sent": "There's about a dozen queries very well at all.",
                    "label": 0
                },
                {
                    "sent": "Or you can put everybody into one single time series.",
                    "label": 0
                },
                {
                    "sent": "That actually happens to work, but it still makes 1 error on the test.",
                    "label": 0
                },
                {
                    "sent": "Combining things in a bad way with a bad topology DST gives you 5 errors, but if you combine people into two teams and then the two teams into one game, then you get 0 errors for the recognition of these plays.",
                    "label": 1
                },
                {
                    "sent": "So just all the players merge into one team state and another set of players in the other, and then they all merge.",
                    "label": 0
                },
                {
                    "sent": "We've applied this again to things.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Gene networks multiple genes, interacting overtime and that again does much better than slamming things together.",
                    "label": 0
                },
                {
                    "sent": "Or keeping things completely apart for their dynamics.",
                    "label": 0
                },
                {
                    "sent": "And now we're looking at another multimodal data set, which is haptics and video, and these are the Davinci laparoscopic robots that surgeons use in hundreds of hospitals, at least in the US, and the surgical team.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm working with has three surgeons that have over 100 operations each on this robot and they basically work at a console near the patient and the robot mimics what they're doing.",
                    "label": 0
                },
                {
                    "sent": "This is what they see while minimizing the invasiveness that they wouldn't otherwise be able to do as well if they were working directly on the patient, but this robot is actually collecting 300 Hertz time series of what the surgeon wants to do, how the robots actuators are moving, and the video data inside.",
                    "label": 0
                },
                {
                    "sent": "So we've collected.",
                    "label": 0
                },
                {
                    "sent": "We can't collect real operations in real surgeries.",
                    "label": 0
                },
                {
                    "sent": "But we collected.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Various drills that look like this and we just have 4 videos.",
                    "label": 0
                },
                {
                    "sent": "These are the two cameras that are inside the cavity.",
                    "label": 0
                },
                {
                    "sent": "These are two outside cameras.",
                    "label": 0
                },
                {
                    "sent": "And then we collected.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Time series, and here there's 64 Dimensional time series for the surgeons doing the console coming into 300 Hertz and the video is coming in at 30 Hertz.",
                    "label": 1
                },
                {
                    "sent": "And we can see various.",
                    "label": 0
                },
                {
                    "sent": "Sorry, we can see various differentiations between a trace maneuver or Cobar open over.",
                    "label": 0
                },
                {
                    "sent": "When an expert does it in a novice, does it an here novices, a student who's just starting to work with the the robot versus an expert is over 100 real operations with it.",
                    "label": 0
                },
                {
                    "sent": "And suturing over here.",
                    "label": 0
                },
                {
                    "sent": "And here's some initial results.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we're also going to plan on providing this data set.",
                    "label": 0
                },
                {
                    "sent": "Probably link off of my homepage modulo some permissions from the Davinci Company called Surgical into it, Intuitive Surgical.",
                    "label": 0
                },
                {
                    "sent": "When we compress the haptics and video data with PCA, and then we did, we actually tried many things.",
                    "label": 1
                },
                {
                    "sent": "Input output.",
                    "label": 0
                },
                {
                    "sent": "Hmm Cindy Estes, and here are some preliminary results.",
                    "label": 0
                },
                {
                    "sent": "This is the minefield gesture, so we asked three offices and three surgeons to do mine fields, and then on another three novices, another three.",
                    "label": 0
                },
                {
                    "sent": "Search surgeons we had test data and you can see you can get this with a little bit better than chance.",
                    "label": 0
                },
                {
                    "sent": "The minefield misses for different maneuver, different complexities of models, but this other other drill.",
                    "label": 0
                },
                {
                    "sent": "This Russian roulette real.",
                    "label": 0
                },
                {
                    "sent": "It's a lot more lot more reliable to around 85% accuracy that you can predict.",
                    "label": 0
                },
                {
                    "sent": "It's an obvious and expert and then suture is also around 80% accuracy for 5050 random chat problem.",
                    "label": 0
                },
                {
                    "sent": "So this is an interesting direction to go to.",
                    "label": 0
                },
                {
                    "sent": "Again, a rich data set and all the parameters are in here and the surgeons are actually they actually really do it.",
                    "label": 0
                },
                {
                    "sent": "Use these simulators to evaluate when students ready to perform the surgery.",
                    "label": 0
                },
                {
                    "sent": "So this is a much closer to reality.",
                    "label": 0
                },
                {
                    "sent": "Real robot as opposed to basically a computer game, is what they were.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Before so basically.",
                    "label": 0
                },
                {
                    "sent": "Dynamic Bayes Nets are natural way to get out of the HMM world and I'll stop there.",
                    "label": 0
                },
                {
                    "sent": "I guess that was my cue and those are all the different modalities we're looking at and.",
                    "label": 0
                },
                {
                    "sent": "Hoping to get some of your feedback the next couple of days.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Time for one or two questions.",
                    "label": 0
                },
                {
                    "sent": "Questions here.",
                    "label": 0
                },
                {
                    "sent": "All questions are correct.",
                    "label": 0
                },
                {
                    "sent": "Can you use the microphone please?",
                    "label": 0
                },
                {
                    "sent": "Just wondering.",
                    "label": 0
                },
                {
                    "sent": "Available for every business.",
                    "label": 0
                },
                {
                    "sent": "I have the best textbook is actually yet to be published.",
                    "label": 0
                },
                {
                    "sent": "Michael Jordan has an online version, but I think he's still looking for a publisher.",
                    "label": 0
                },
                {
                    "sent": "I I think that is the easiest and most successful accessible textbook right now at Berkeley, yeah?",
                    "label": 0
                },
                {
                    "sent": "Yes cs.berkeley.edu/tildejordan.",
                    "label": 0
                },
                {
                    "sent": "OK, then I guess we can find some details in your book.",
                    "label": 0
                },
                {
                    "sent": "Yeah, there's also some details in my book as well.",
                    "label": 0
                },
                {
                    "sent": "So then let's thank you.",
                    "label": 0
                }
            ]
        }
    }
}