{
    "id": "clvro676qbiglnbdwwwggsfaqsbjae47",
    "title": "Iterative Learning for Reliable Crowdsourcing Systems",
    "info": {
        "author": [
            "Sewoong Oh, Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign"
        ],
        "published": "Jan. 25, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Crowdsourcing"
        ]
    },
    "url": "http://videolectures.net/nips2011_oh_crowdsourcing/",
    "segmentation": [
        [
            "So this talk is on designing a reliable and cost efficient cost sourcing system and this is joint work with David Karger and above Russia."
        ],
        [
            "So crowdsourcing systems like Amazons Mechanical Turk has emerged as a very important tool for solving problems which involve huge amounts of data sets in which can be decomposed into smaller simple tasks.",
            "So for example, in the year 2007 there was a plane crash in the desert in Nevada and people did not know where exactly this plane crashed.",
            "So defined explain what they did.",
            "Was they released millions of satellite images of the region?",
            "They were the plane might have crashed and they push it.",
            "All the images on their crowdsourcing website like this one here.",
            "And from a pool of workers who ever once 2 picks up these images and indicate if they see anything that looks like a plane crash.",
            "And in this example more than 50,000 people volunteered to search through these images.",
            "So crowdsourcing platforms like this one allows us to use this kind of huge amounts of human power to solve problems, which can be difficult for computers but can be simple and easy for humans to do.",
            "So he."
        ],
        [
            "Here's how we view crowdsourcing, so we have bunch of tasks.",
            "In this example, each task is an image and want to figure out if there's an airplane in an image.",
            "So what we do as a taskmaster is we create."
        ],
        [
            "Patches of task.",
            "Each Patch is going to be a collection of tasks and we post them on a crowdsourcing website, then from a pool of workers."
        ],
        [
            "Who wants to is going to pick up a batch and complete every task in the Patch and on."
        ],
        [
            "The batch is going to be completed by another worker, and if you're using another's mechanical targets, actually the workers who have some control over how many tasks they want to work on and which task they won't work on.",
            "But there are simple ways you can impose so that the does that.",
            "They complete every task in the batch.",
            "For instance, you can condition your payment on completion of a batch, and as you can see these are crossing tasks are typically very tedious and repetitive, so these people are going to make a lot of mistakes.",
            "So we know there too."
        ],
        [
            "Check for this mistake."
        ],
        [
            "So we're gonna collect."
        ],
        [
            "Simple responses for each of these tasks and once we get all the responses then we need to figure out the correct answer is by aggregating all this information and here we are assuming that there is a fixed cost that we need to pay for each response that we get.",
            "So the total cost is going to be proportional to the total number of responses that we can or the total number of edges in this graph here.",
            "And for such systems, there's a fundamental question of interest to every Test match."
        ],
        [
            "So it is.",
            "How can we get some confidence in our answers while using US small number of questions as possible or repeating his task as few times as possible.",
            "So in order to design such a reliable and cost efficient processing system we need to address the following two main tasks.",
            "First we need a good scheme to decide which task is included in which Patch.",
            "'cause as you'll see some task assignment is going to be better than the others.",
            "An once we get the responses then we need to solve the inference problem.",
            "Or figuring out what the correct answer is based on all the responses that we get and in the following we're going to introduce."
        ],
        [
            "Introduce an optimal task assignment scheme based on random graphs and an optimal inference algorithm on this task assignment based on low rank matrix approximations."
        ],
        [
            "So previous work on designing search reliable processing systems.",
            "They typically focus on the inference problem and they do not address the role or a choice of task assignment, and these approaches are mainly based on expectation maximization, which is a greedy algorithm which converges to local minima.",
            "So it's very sensitive to where we start the algorithm, and there's no performance guarantees known for these approaches."
        ],
        [
            "So what we want to do is we want to address both questions of task assignment and inference together, and we're going to show that if use our task assignment scheme with our inference algorithm, then this approach is going to achieve performance which is very close to a fundamental or minimax lower bound, which is a lower bound on what you can achieve using the best possible task assignment with the best possible inference Iris."
        ],
        [
            "So here is our design of our customers in system.",
            "So assigning task is equivalent as as designing a bipartite graph which tells you which task is included in which batch.",
            "So in order to do that we first choose LNRL is how many times we repeat each task and R is how many tasks are included in each batch.",
            "So in practice lol is going to be chosen according to how, how much money or resources you can spend on each task.",
            "And R is going to be chosen depending on the applications according to how many tasks are manageable for a single worker.",
            "And once you have this Ellen, are you going to going to choose a regular bipartite?"
        ],
        [
            "Graph from uniform distribution and the reason we're using a random graph here is that has a couple of good properties.",
            "First of all, in the large system limit or sparse random graph is known to converge locally to a tree, so we're going to use this to analyze our message passing algorithm, improve really sharp bound on the performance, but more importantly, random graphs are known to be good expanders, and what this means is that if you look at the adjacency matrix of a random graph and plot the histogram.",
            "Of the singular values of this matrix, then there's going to be a clear separation between the largest singular value and the rest of the singular values, and what this means for us in our crowdsourcing problem is that this first singular value is going to correspond to the signal that we want to measure, and the rest of the singular values are going to correspond to the noise in the data.",
            "So having a good expander is essentially like having high signal to noise ratio, and both of these notions are going to be made quite clear when I explain the inference."
        ],
        [
            "Once we have this task assignment, then we're going to collect responses according to a probabilistic model.",
            "So here we are assuming that each task is binary, so you can think of it as yes or no questions or binary classification tasks, and each worker is characterized by an unknown quality parameter PJ, such that when task I is assigned to Walker J, the response AIJ is going to be either plus or minus and it's going to be correct with property PJ.",
            "So if a worker has PSA call to one is always giving us the.",
            "The right answers, and if it's a spammer, his PJ could do half, in which case it gives us random answers.",
            "And if it's an adversary, his PT equal to 0 and 1 technical detail meaning to assume here is that it's necessary that we know these PJS on average, whether they're bigger than half or less than half.",
            "And the reason is that if you think of an extreme case where everyone is adversary, then everyone is going to give us the wrong answer.",
            "And unless you know that the PJS are less than half, there's no way you can correct for this.",
            "So in order to resolve this ambiguity of the sign, we need this one bit of information, which tells us whether the PJS on Evers are bigger than half or no."
        ],
        [
            "But and once we get this response is then we need to solve the inference problem so we don't know what the PJS are 'cause we don't know how reliable talker is and we don't, we don't know what the teaser, 'cause that's what we're trying to figure out, but we get to see the response is a ijs and we want to estimate the correct answers T and a very naive and simple algorithm to estimate it is."
        ],
        [
            "Using majority voting and on the other extreme, if you have an Oracle which tells you how reliable each of this worker is, which gives you all the PJS, then there is."
        ],
        [
            "Very simple rule for computing the best possible estimate by taking the weighted sum of the responses, weighted according to reliability to workers.",
            "So you're giving more weight to the response that came from a reliable worker and we can simulate these."
        ],
        [
            "Two extremes and we consider as we more add more workers to each task.",
            "As we spend more money, you can see that the probability of error is going to decay exponentially, but there's a huge gap between one majority.",
            "Voting can do and what the Oracle estimator does.",
            "So you can think of this red line as a lower bound of what any algorithm can achieve.",
            "And in reality we don't have an Oracle, so we don't know what the PJS are.",
            "But what we can do is we can use a nitrated procedure to estimate what the what the correct solution should be, and use this to estimate who's more reliable, and I tried this procedure an get computer effective reliability so that we can use this."
        ],
        [
            "To wait their responses and get an estimate which achieves performance which is very close to the Oracle estimator.",
            "So here."
        ],
        [
            "Is how our estimation works, so we need to figure out what weight we need to use here or how we love each worker is.",
            "So this algorithm is based on message passing algorithm which is inspired in part by lowering approximations like power."
        ],
        [
            "So this algorithm will present two sets of messages that task messages TJS represents how we live.",
            "How likely this task I is to be a positive task and the worker messages WHAS represents how reliable each worker J is and we start with a randomly initialized WHAS and once we have this it's easy to compute the TJS by taking the weighted sum of the incoming messages, waited out coding to the responses on those edges.",
            "And once we have this likelihood, then it's also easy to compute the reliabilities by taking the weighted sum of the incoming messages again weighted according to the responses on these edges.",
            "So we iterate this to procedures updating our belief on how reliable each worker is and how likely it's task it is.",
            "And in the end we use this reliability to compute our final estimate.",
            "So in English if I explain what this update is doing at the task update, we're saying that.",
            "A task I is more likely to be a positive task if reliable workers agree that it's a positive task and work update.",
            "We're saying the worker J is more reliable if TI Jason Aij's have the same sign, meaning that if what this worker said about other tasks agree with what we believe about those tasks."
        ],
        [
            "And perhaps one more intuitive way of understanding how our algorithm works is through singular value decomposition.",
            "So if you look at our message updates, then any titration we're just applying on linear operation to our set of messages.",
            "So what we're computing is the leading singular vector or eigenvector over particular linear operator.",
            "And this linear operator has a special structure.",
            "So instead what we're computing is essentially an approximation of the singular.",
            "Vector of a data matrix A where the rows are the tasks and columns are the workers and the entries of the responses, and the reason that the singular vector, the leading singular vector of this matrix A is gives us good estimates is that if we compute the conditional expectation of this random data matrix conditioned on all the correct answers and the older reliability PJS, then it's going to be a rank one matrix where the left singular vector is exactly the solutions to the tasks.",
            "They were trying to figure out.",
            "So you can think of this data matrix A as a lowering signal which is perturbed by random noise and was singular value decomposition is doing as it's trying to extract this lowering signal from this random noise and because we're using random graphs which are good expanders, this is going to give us good estimates.",
            "And at this point you might wonder we have this data matrix A and the singular vector is so easy to compute.",
            "Why don't we use it directly for inference?",
            "Why do we have to go through this?",
            "I creative and message passing procedure to approximate the singular vectors, and the reason is that the current analysis techniques that we have for analyzing singular vectors of this kind of random matrices does not give us the type found in the performance that we want.",
            "So in order to really prove sharp bounds, we need to exploit the tree like locally tree like structure.",
            "Of random graphs by using a message passing procedure, and I won't have I. I'm not going to have time to explain how we analyze our algorithm, but there's some beautiful mathematics that comes into play and I'd be happy to explain more offline if anyone is interested.",
            "And before we go into the theory."
        ],
        [
            "Let's look at a real example.",
            "So here, let's say you have a bunch of items and you want to figure out which of these items are more similar to one another so that you can use it for for instance, like recommendations or searching so you can use crowdsourcing to ask.",
            "We took these two items are more similar to the one at the top, and this seems easy, but you can easily see easily see how we can use the same thing."
        ],
        [
            "Work to ask much more complex questions using crowdsourcing.",
            "For instance, if you have your favorite shirt, are which type might match it better.",
            "But for us we just wanted to check how good our algorithm is and we wanted to be able to control how difficult these tasks are.",
            "So we used comparisons and colors, so we asked crowdsourcing to tell us which of these two colors."
        ],
        [
            "Similar to the one at the top.",
            "And we created 50 of these tasks and collected 28 responses for his task using Amazon's Mechanical Turk and you can see if we use our algorithm for inference, we have about 6% of error and if majority voting is at about 12% and EMI rhythms are about the 14% and once we have this data we can sub sample subset."
        ],
        [
            "This data and see what simulate what might have happened if we spend less money and you can see that for most values our algorithm is better and there is a transition where our algorithm starts being worth the majority voting and our theorem can predict where such phase transition is going to happen and we give up abandoned the property of error for using our algorithm and here we are using.",
            "We're just comparing the inference algorithm on the same data we're using the same task assignment.",
            "Now if you want to see the effect of task assignment we need to go to."
        ],
        [
            "Bigger problem size, So what we're showing is a numerical simulation using two inference algorithms and this data, or generated using random task assignments as we propose.",
            "And if we had used different task assignment, one that's deterministic has a lot of structure than an has smaller spectral gap."
        ],
        [
            "Then, even if you spend same money, it's easily going to get worse quickly.",
            "Anne."
        ],
        [
            "Now we can.",
            "Analyze the performance.",
            "Our algorithm which is going to depend on the degree of task L that degree of batch are and all of these PJS we can show that the mainly it's going to depend on all these PJS through a single quality parameter Q which captures how good the quality of the crowd is collectively.",
            "So when everyone has PJ code, one Q is going to be equal to 1 when everyone is a spammer, Q is 0, the keys on number between zero and one which captures how good.",
            "The quality of the crowd is and you can see that in terms of Q&L, the probability of error is going to decay exponentially.",
            "And you can see that there's a phase transition below which our algorithm performs worse than majority voting and over.",
            "But our algorithm is significantly better compared to majority voting, and this kind of phase transitions into seems to be universal and fundamental in the sense that we see similar behavior for all the algorithms that we tried, including the EM algorithm.",
            "And as a practitioner you have interest in the reason where you're above the phase transition regime and then."
        ],
        [
            "This reason we can prove an upper bound and the probability of error using our approach.",
            "So if in the large system limit if you're above the phase transition, then if you use our random assignment an use K iterations of our iterative algorithm, then the probability of error is going to be bounded by some quantity which decays exponentially in QL with, which is fully described by LRQ and titrations K. So now to see how good.",
            "This performance found is less simplified."
        ],
        [
            "Little bit, so we're going to assume that the worker degree is bigger than task degree, which is not a big assumption and we're on the above threshold regime, and we're going to take the number of iterations to Infinity and making sure that our approach converges to fixed fixed point unique fixed point and this fixed point solution achieves performance, which decays exponentially in QL and can prove matching minimix lower bound in the regime.",
            "In the scenario where nature chooses the worst case.",
            "The eyes and the worst case PJS from a family of PJ's which are the collective qualities parents advice Q and if you use the best possible task assignment and best possible algorithm then it's still going to be the property of various.",
            "Still going to be lower bounded by something that decays exponentially into L. So we have a matching lower and upper bound which means that no other algorithm can do much better than our algorithm other than improving this constant in the exponent.",
            "And this constant in practice seems to be very close to 1, because this is the slope in this graph here."
        ],
        [
            "So."
        ],
        [
            "2."
        ],
        [
            "Conclude."
        ],
        [
            "So we introduced a novel order optimal scheme for Tesco location.",
            "Another optimal scheme for inference algorithm and we showed that comparing it to the best possible graph and best possible algorithm, this achieves order to.",
            "Performance and we can extend this analysis to show that even if you include all the algorithms that can adaptively choose which task to assign to each worker, our algorithm is still order optimal and there's a natural generalization to more general models for the worker behavior.",
            "And thank you for your attention.",
            "So let me repeat the question first question is in the original problem.",
            "It seems like there are only a few positive tasks and people might have different property of error depending where there's positive negative and that's what I was going to in the conclusion.",
            "Is that our algorithm an approach naturally generalizes to this case where workers have bias, they have different quality of making an error depending on whether the original solution is positive or negative.",
            "And the second question is if you have if you want to do it in multi process like.",
            "If you do it once and then maybe ask more questions and those tasks that you're not sure about, so that's what I was going at with the adaptive thing which I didn't have time to explain.",
            "So even if we include all the best possible adaptive schemes, our algorithm is still order optimal, so it's only the constant.",
            "Fear gets better in terms of scaling, it's still the same, so the optimality of algorithm doesn't change, but in practice you might want to use our approach adaptively.",
            "To reduce the constant in the exponent."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this talk is on designing a reliable and cost efficient cost sourcing system and this is joint work with David Karger and above Russia.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So crowdsourcing systems like Amazons Mechanical Turk has emerged as a very important tool for solving problems which involve huge amounts of data sets in which can be decomposed into smaller simple tasks.",
                    "label": 0
                },
                {
                    "sent": "So for example, in the year 2007 there was a plane crash in the desert in Nevada and people did not know where exactly this plane crashed.",
                    "label": 0
                },
                {
                    "sent": "So defined explain what they did.",
                    "label": 0
                },
                {
                    "sent": "Was they released millions of satellite images of the region?",
                    "label": 0
                },
                {
                    "sent": "They were the plane might have crashed and they push it.",
                    "label": 0
                },
                {
                    "sent": "All the images on their crowdsourcing website like this one here.",
                    "label": 0
                },
                {
                    "sent": "And from a pool of workers who ever once 2 picks up these images and indicate if they see anything that looks like a plane crash.",
                    "label": 0
                },
                {
                    "sent": "And in this example more than 50,000 people volunteered to search through these images.",
                    "label": 0
                },
                {
                    "sent": "So crowdsourcing platforms like this one allows us to use this kind of huge amounts of human power to solve problems, which can be difficult for computers but can be simple and easy for humans to do.",
                    "label": 0
                },
                {
                    "sent": "So he.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's how we view crowdsourcing, so we have bunch of tasks.",
                    "label": 0
                },
                {
                    "sent": "In this example, each task is an image and want to figure out if there's an airplane in an image.",
                    "label": 0
                },
                {
                    "sent": "So what we do as a taskmaster is we create.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Patches of task.",
                    "label": 0
                },
                {
                    "sent": "Each Patch is going to be a collection of tasks and we post them on a crowdsourcing website, then from a pool of workers.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Who wants to is going to pick up a batch and complete every task in the Patch and on.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The batch is going to be completed by another worker, and if you're using another's mechanical targets, actually the workers who have some control over how many tasks they want to work on and which task they won't work on.",
                    "label": 0
                },
                {
                    "sent": "But there are simple ways you can impose so that the does that.",
                    "label": 0
                },
                {
                    "sent": "They complete every task in the batch.",
                    "label": 0
                },
                {
                    "sent": "For instance, you can condition your payment on completion of a batch, and as you can see these are crossing tasks are typically very tedious and repetitive, so these people are going to make a lot of mistakes.",
                    "label": 0
                },
                {
                    "sent": "So we know there too.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Check for this mistake.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we're gonna collect.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Simple responses for each of these tasks and once we get all the responses then we need to figure out the correct answer is by aggregating all this information and here we are assuming that there is a fixed cost that we need to pay for each response that we get.",
                    "label": 0
                },
                {
                    "sent": "So the total cost is going to be proportional to the total number of responses that we can or the total number of edges in this graph here.",
                    "label": 0
                },
                {
                    "sent": "And for such systems, there's a fundamental question of interest to every Test match.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it is.",
                    "label": 0
                },
                {
                    "sent": "How can we get some confidence in our answers while using US small number of questions as possible or repeating his task as few times as possible.",
                    "label": 0
                },
                {
                    "sent": "So in order to design such a reliable and cost efficient processing system we need to address the following two main tasks.",
                    "label": 0
                },
                {
                    "sent": "First we need a good scheme to decide which task is included in which Patch.",
                    "label": 0
                },
                {
                    "sent": "'cause as you'll see some task assignment is going to be better than the others.",
                    "label": 0
                },
                {
                    "sent": "An once we get the responses then we need to solve the inference problem.",
                    "label": 0
                },
                {
                    "sent": "Or figuring out what the correct answer is based on all the responses that we get and in the following we're going to introduce.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Introduce an optimal task assignment scheme based on random graphs and an optimal inference algorithm on this task assignment based on low rank matrix approximations.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So previous work on designing search reliable processing systems.",
                    "label": 0
                },
                {
                    "sent": "They typically focus on the inference problem and they do not address the role or a choice of task assignment, and these approaches are mainly based on expectation maximization, which is a greedy algorithm which converges to local minima.",
                    "label": 0
                },
                {
                    "sent": "So it's very sensitive to where we start the algorithm, and there's no performance guarantees known for these approaches.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we want to do is we want to address both questions of task assignment and inference together, and we're going to show that if use our task assignment scheme with our inference algorithm, then this approach is going to achieve performance which is very close to a fundamental or minimax lower bound, which is a lower bound on what you can achieve using the best possible task assignment with the best possible inference Iris.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is our design of our customers in system.",
                    "label": 0
                },
                {
                    "sent": "So assigning task is equivalent as as designing a bipartite graph which tells you which task is included in which batch.",
                    "label": 0
                },
                {
                    "sent": "So in order to do that we first choose LNRL is how many times we repeat each task and R is how many tasks are included in each batch.",
                    "label": 0
                },
                {
                    "sent": "So in practice lol is going to be chosen according to how, how much money or resources you can spend on each task.",
                    "label": 0
                },
                {
                    "sent": "And R is going to be chosen depending on the applications according to how many tasks are manageable for a single worker.",
                    "label": 0
                },
                {
                    "sent": "And once you have this Ellen, are you going to going to choose a regular bipartite?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Graph from uniform distribution and the reason we're using a random graph here is that has a couple of good properties.",
                    "label": 0
                },
                {
                    "sent": "First of all, in the large system limit or sparse random graph is known to converge locally to a tree, so we're going to use this to analyze our message passing algorithm, improve really sharp bound on the performance, but more importantly, random graphs are known to be good expanders, and what this means is that if you look at the adjacency matrix of a random graph and plot the histogram.",
                    "label": 0
                },
                {
                    "sent": "Of the singular values of this matrix, then there's going to be a clear separation between the largest singular value and the rest of the singular values, and what this means for us in our crowdsourcing problem is that this first singular value is going to correspond to the signal that we want to measure, and the rest of the singular values are going to correspond to the noise in the data.",
                    "label": 0
                },
                {
                    "sent": "So having a good expander is essentially like having high signal to noise ratio, and both of these notions are going to be made quite clear when I explain the inference.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Once we have this task assignment, then we're going to collect responses according to a probabilistic model.",
                    "label": 0
                },
                {
                    "sent": "So here we are assuming that each task is binary, so you can think of it as yes or no questions or binary classification tasks, and each worker is characterized by an unknown quality parameter PJ, such that when task I is assigned to Walker J, the response AIJ is going to be either plus or minus and it's going to be correct with property PJ.",
                    "label": 0
                },
                {
                    "sent": "So if a worker has PSA call to one is always giving us the.",
                    "label": 0
                },
                {
                    "sent": "The right answers, and if it's a spammer, his PJ could do half, in which case it gives us random answers.",
                    "label": 0
                },
                {
                    "sent": "And if it's an adversary, his PT equal to 0 and 1 technical detail meaning to assume here is that it's necessary that we know these PJS on average, whether they're bigger than half or less than half.",
                    "label": 0
                },
                {
                    "sent": "And the reason is that if you think of an extreme case where everyone is adversary, then everyone is going to give us the wrong answer.",
                    "label": 0
                },
                {
                    "sent": "And unless you know that the PJS are less than half, there's no way you can correct for this.",
                    "label": 0
                },
                {
                    "sent": "So in order to resolve this ambiguity of the sign, we need this one bit of information, which tells us whether the PJS on Evers are bigger than half or no.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But and once we get this response is then we need to solve the inference problem so we don't know what the PJS are 'cause we don't know how reliable talker is and we don't, we don't know what the teaser, 'cause that's what we're trying to figure out, but we get to see the response is a ijs and we want to estimate the correct answers T and a very naive and simple algorithm to estimate it is.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using majority voting and on the other extreme, if you have an Oracle which tells you how reliable each of this worker is, which gives you all the PJS, then there is.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very simple rule for computing the best possible estimate by taking the weighted sum of the responses, weighted according to reliability to workers.",
                    "label": 0
                },
                {
                    "sent": "So you're giving more weight to the response that came from a reliable worker and we can simulate these.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two extremes and we consider as we more add more workers to each task.",
                    "label": 0
                },
                {
                    "sent": "As we spend more money, you can see that the probability of error is going to decay exponentially, but there's a huge gap between one majority.",
                    "label": 0
                },
                {
                    "sent": "Voting can do and what the Oracle estimator does.",
                    "label": 0
                },
                {
                    "sent": "So you can think of this red line as a lower bound of what any algorithm can achieve.",
                    "label": 0
                },
                {
                    "sent": "And in reality we don't have an Oracle, so we don't know what the PJS are.",
                    "label": 0
                },
                {
                    "sent": "But what we can do is we can use a nitrated procedure to estimate what the what the correct solution should be, and use this to estimate who's more reliable, and I tried this procedure an get computer effective reliability so that we can use this.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To wait their responses and get an estimate which achieves performance which is very close to the Oracle estimator.",
                    "label": 0
                },
                {
                    "sent": "So here.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is how our estimation works, so we need to figure out what weight we need to use here or how we love each worker is.",
                    "label": 0
                },
                {
                    "sent": "So this algorithm is based on message passing algorithm which is inspired in part by lowering approximations like power.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this algorithm will present two sets of messages that task messages TJS represents how we live.",
                    "label": 0
                },
                {
                    "sent": "How likely this task I is to be a positive task and the worker messages WHAS represents how reliable each worker J is and we start with a randomly initialized WHAS and once we have this it's easy to compute the TJS by taking the weighted sum of the incoming messages, waited out coding to the responses on those edges.",
                    "label": 0
                },
                {
                    "sent": "And once we have this likelihood, then it's also easy to compute the reliabilities by taking the weighted sum of the incoming messages again weighted according to the responses on these edges.",
                    "label": 0
                },
                {
                    "sent": "So we iterate this to procedures updating our belief on how reliable each worker is and how likely it's task it is.",
                    "label": 0
                },
                {
                    "sent": "And in the end we use this reliability to compute our final estimate.",
                    "label": 0
                },
                {
                    "sent": "So in English if I explain what this update is doing at the task update, we're saying that.",
                    "label": 0
                },
                {
                    "sent": "A task I is more likely to be a positive task if reliable workers agree that it's a positive task and work update.",
                    "label": 0
                },
                {
                    "sent": "We're saying the worker J is more reliable if TI Jason Aij's have the same sign, meaning that if what this worker said about other tasks agree with what we believe about those tasks.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And perhaps one more intuitive way of understanding how our algorithm works is through singular value decomposition.",
                    "label": 0
                },
                {
                    "sent": "So if you look at our message updates, then any titration we're just applying on linear operation to our set of messages.",
                    "label": 0
                },
                {
                    "sent": "So what we're computing is the leading singular vector or eigenvector over particular linear operator.",
                    "label": 0
                },
                {
                    "sent": "And this linear operator has a special structure.",
                    "label": 0
                },
                {
                    "sent": "So instead what we're computing is essentially an approximation of the singular.",
                    "label": 0
                },
                {
                    "sent": "Vector of a data matrix A where the rows are the tasks and columns are the workers and the entries of the responses, and the reason that the singular vector, the leading singular vector of this matrix A is gives us good estimates is that if we compute the conditional expectation of this random data matrix conditioned on all the correct answers and the older reliability PJS, then it's going to be a rank one matrix where the left singular vector is exactly the solutions to the tasks.",
                    "label": 0
                },
                {
                    "sent": "They were trying to figure out.",
                    "label": 0
                },
                {
                    "sent": "So you can think of this data matrix A as a lowering signal which is perturbed by random noise and was singular value decomposition is doing as it's trying to extract this lowering signal from this random noise and because we're using random graphs which are good expanders, this is going to give us good estimates.",
                    "label": 0
                },
                {
                    "sent": "And at this point you might wonder we have this data matrix A and the singular vector is so easy to compute.",
                    "label": 0
                },
                {
                    "sent": "Why don't we use it directly for inference?",
                    "label": 0
                },
                {
                    "sent": "Why do we have to go through this?",
                    "label": 0
                },
                {
                    "sent": "I creative and message passing procedure to approximate the singular vectors, and the reason is that the current analysis techniques that we have for analyzing singular vectors of this kind of random matrices does not give us the type found in the performance that we want.",
                    "label": 0
                },
                {
                    "sent": "So in order to really prove sharp bounds, we need to exploit the tree like locally tree like structure.",
                    "label": 0
                },
                {
                    "sent": "Of random graphs by using a message passing procedure, and I won't have I. I'm not going to have time to explain how we analyze our algorithm, but there's some beautiful mathematics that comes into play and I'd be happy to explain more offline if anyone is interested.",
                    "label": 0
                },
                {
                    "sent": "And before we go into the theory.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's look at a real example.",
                    "label": 0
                },
                {
                    "sent": "So here, let's say you have a bunch of items and you want to figure out which of these items are more similar to one another so that you can use it for for instance, like recommendations or searching so you can use crowdsourcing to ask.",
                    "label": 0
                },
                {
                    "sent": "We took these two items are more similar to the one at the top, and this seems easy, but you can easily see easily see how we can use the same thing.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Work to ask much more complex questions using crowdsourcing.",
                    "label": 1
                },
                {
                    "sent": "For instance, if you have your favorite shirt, are which type might match it better.",
                    "label": 1
                },
                {
                    "sent": "But for us we just wanted to check how good our algorithm is and we wanted to be able to control how difficult these tasks are.",
                    "label": 0
                },
                {
                    "sent": "So we used comparisons and colors, so we asked crowdsourcing to tell us which of these two colors.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Similar to the one at the top.",
                    "label": 0
                },
                {
                    "sent": "And we created 50 of these tasks and collected 28 responses for his task using Amazon's Mechanical Turk and you can see if we use our algorithm for inference, we have about 6% of error and if majority voting is at about 12% and EMI rhythms are about the 14% and once we have this data we can sub sample subset.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This data and see what simulate what might have happened if we spend less money and you can see that for most values our algorithm is better and there is a transition where our algorithm starts being worth the majority voting and our theorem can predict where such phase transition is going to happen and we give up abandoned the property of error for using our algorithm and here we are using.",
                    "label": 0
                },
                {
                    "sent": "We're just comparing the inference algorithm on the same data we're using the same task assignment.",
                    "label": 0
                },
                {
                    "sent": "Now if you want to see the effect of task assignment we need to go to.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bigger problem size, So what we're showing is a numerical simulation using two inference algorithms and this data, or generated using random task assignments as we propose.",
                    "label": 0
                },
                {
                    "sent": "And if we had used different task assignment, one that's deterministic has a lot of structure than an has smaller spectral gap.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then, even if you spend same money, it's easily going to get worse quickly.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we can.",
                    "label": 0
                },
                {
                    "sent": "Analyze the performance.",
                    "label": 0
                },
                {
                    "sent": "Our algorithm which is going to depend on the degree of task L that degree of batch are and all of these PJS we can show that the mainly it's going to depend on all these PJS through a single quality parameter Q which captures how good the quality of the crowd is collectively.",
                    "label": 0
                },
                {
                    "sent": "So when everyone has PJ code, one Q is going to be equal to 1 when everyone is a spammer, Q is 0, the keys on number between zero and one which captures how good.",
                    "label": 0
                },
                {
                    "sent": "The quality of the crowd is and you can see that in terms of Q&L, the probability of error is going to decay exponentially.",
                    "label": 0
                },
                {
                    "sent": "And you can see that there's a phase transition below which our algorithm performs worse than majority voting and over.",
                    "label": 0
                },
                {
                    "sent": "But our algorithm is significantly better compared to majority voting, and this kind of phase transitions into seems to be universal and fundamental in the sense that we see similar behavior for all the algorithms that we tried, including the EM algorithm.",
                    "label": 0
                },
                {
                    "sent": "And as a practitioner you have interest in the reason where you're above the phase transition regime and then.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This reason we can prove an upper bound and the probability of error using our approach.",
                    "label": 0
                },
                {
                    "sent": "So if in the large system limit if you're above the phase transition, then if you use our random assignment an use K iterations of our iterative algorithm, then the probability of error is going to be bounded by some quantity which decays exponentially in QL with, which is fully described by LRQ and titrations K. So now to see how good.",
                    "label": 1
                },
                {
                    "sent": "This performance found is less simplified.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Little bit, so we're going to assume that the worker degree is bigger than task degree, which is not a big assumption and we're on the above threshold regime, and we're going to take the number of iterations to Infinity and making sure that our approach converges to fixed fixed point unique fixed point and this fixed point solution achieves performance, which decays exponentially in QL and can prove matching minimix lower bound in the regime.",
                    "label": 0
                },
                {
                    "sent": "In the scenario where nature chooses the worst case.",
                    "label": 0
                },
                {
                    "sent": "The eyes and the worst case PJS from a family of PJ's which are the collective qualities parents advice Q and if you use the best possible task assignment and best possible algorithm then it's still going to be the property of various.",
                    "label": 0
                },
                {
                    "sent": "Still going to be lower bounded by something that decays exponentially into L. So we have a matching lower and upper bound which means that no other algorithm can do much better than our algorithm other than improving this constant in the exponent.",
                    "label": 0
                },
                {
                    "sent": "And this constant in practice seems to be very close to 1, because this is the slope in this graph here.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "2.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Conclude.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we introduced a novel order optimal scheme for Tesco location.",
                    "label": 0
                },
                {
                    "sent": "Another optimal scheme for inference algorithm and we showed that comparing it to the best possible graph and best possible algorithm, this achieves order to.",
                    "label": 0
                },
                {
                    "sent": "Performance and we can extend this analysis to show that even if you include all the algorithms that can adaptively choose which task to assign to each worker, our algorithm is still order optimal and there's a natural generalization to more general models for the worker behavior.",
                    "label": 0
                },
                {
                    "sent": "And thank you for your attention.",
                    "label": 0
                },
                {
                    "sent": "So let me repeat the question first question is in the original problem.",
                    "label": 0
                },
                {
                    "sent": "It seems like there are only a few positive tasks and people might have different property of error depending where there's positive negative and that's what I was going to in the conclusion.",
                    "label": 0
                },
                {
                    "sent": "Is that our algorithm an approach naturally generalizes to this case where workers have bias, they have different quality of making an error depending on whether the original solution is positive or negative.",
                    "label": 0
                },
                {
                    "sent": "And the second question is if you have if you want to do it in multi process like.",
                    "label": 0
                },
                {
                    "sent": "If you do it once and then maybe ask more questions and those tasks that you're not sure about, so that's what I was going at with the adaptive thing which I didn't have time to explain.",
                    "label": 0
                },
                {
                    "sent": "So even if we include all the best possible adaptive schemes, our algorithm is still order optimal, so it's only the constant.",
                    "label": 0
                },
                {
                    "sent": "Fear gets better in terms of scaling, it's still the same, so the optimality of algorithm doesn't change, but in practice you might want to use our approach adaptively.",
                    "label": 0
                },
                {
                    "sent": "To reduce the constant in the exponent.",
                    "label": 0
                }
            ]
        }
    }
}