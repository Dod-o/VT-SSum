{
    "id": "sumuktsdxred4pr2t6ggqjlcl4mswty6",
    "title": "BayesANIL - A Bayesian Model for Handling Approximate, Noisy or Incomplete Labeling in Text Classification",
    "info": {
        "author": [
            "Ganes Ramakrishnan, IBM"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "August 2005",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/icml05_ramakrishnan_bbmha/",
    "segmentation": [
        [
            "Then I will explain the learning component in the invasion and how the basinal para meters could be used in standard classifiers.",
            "I present some rabbit experiments and some conclusions.",
            "So the motivation for this work is basically a hurdle."
        ],
        [
            "In supervised learning of text classifiers where manual labelers it and assign labels to text documents.",
            "But often there are approximations involved in the manual labeling of documents, so you have."
        ],
        [
            "Often to break a tie between certain class labels or very often the documents belong inherently to multiple classes, and so there's a degree of belonging degree to which a document belongs to each class.",
            "There would also be explicit noise in the labeling.",
            "I just give an example.",
            "It's easy to sometimes generate a label data set with some amount of noise in the labeling by firing a query on the web.",
            "Oh ando.",
            "In order to generalize well, you may also want to learn from learning in the presence of unlabeled documents.",
            "But we look upon this as a problem of learning with incomplete labeling.",
            "Oh so there's some related work."
        ],
        [
            "In learning from positive and unlabeled examples, those present in last year's Lyceum lost, lost his ICM will.",
            "And they they cause the problem of learning from positive and unlabeled examples.",
            "The original problem is basically learning from few labeled examples, and this is how they cause the problem.",
            "They they treat the unlabeled set to be a mixture of unlabeled plus few positive, few positive instances.",
            "And they also have an estimate of the proportion of noise in the unlabeled examples.",
            "They also work done by broadly and fiddle in 1996 where they tried to counter class noise by by an iterative process where the remote training instances that could be potentially misclassified.",
            "So this is like a metal owner.",
            "There's also work done by Domingos at all in 1999.",
            "All four datasets with imbalance imbalance classes cost sensitive learning algorithm and they apply for the specific problem datasets with imbalanced classes.",
            "And the method we propose is complementary to this work.",
            "It can be used.",
            "It can be plugged into this work."
        ],
        [
            "Also, this work done by.",
            "Nigam and Xanga, but they learn from labeled and unlabeled.",
            "And there are of course some other people also.",
            "In order to generalize from few label examples.",
            "And we show that our proposed method can very well perform this task of learning, from the unlabeled examples as well.",
            "To induce generalization, but also features smoothing techniques by Laplace, let's tone.",
            "There are some smoothing techniques, but they do not account for empirical distribution of features on label document.",
            "They just take some probability models from the word scene in the label documents and.",
            "Mean words and then distributed amongst unseen words.",
            "They also work done by Hoffman probabilistic latent, latent semantic analysis, blsa.",
            "I need he does most suited for information retrieval and for the particular task we found it not performing well.",
            "So what we propose a model that estimates."
        ],
        [
            "The degree to which each document belongs to or in other words, how well a document D fits into each each of the priests of the specified class labels there.",
            "So we provide this estimate of joint probability of DNZ.",
            "This D is basically an index into the.",
            "Document collection and there is an index into a class label set.",
            "And we use this to Ada traditional text classifiers.",
            "Naive business VM so that they can handle approximate noise or incomplete labeling.",
            "So I'll just plug it into these text classifiers.",
            "And we can look at probability of B given zed as a measure of support.",
            "This notion of support and confidence is borrowed from data mining community, but there is this notion of support and confidence.",
            "How?",
            "How much evidence is there for the label that has been assigned to a particular class and how confident confidently the label has been assigned?",
            "Oh so by marginalizing the joint distribution we get this notion of support and confidence.",
            "So this figure I just tried to explain the."
        ],
        [
            "How business fits.",
            "So you have a document collection of B. Dear Document collection of documents and you have an index into the documents small D and we use capital D as a random variable to represent the random variable which takes values D from their collection.",
            "Calligraphic D. And this document collection can be approximately labeled.",
            "Can be noisy label or there could be only.",
            "It could also be unlabeled in, which means that there might be knows its corresponding to.",
            "Certainties how?",
            "I as an offline task are model based estimates.",
            "The joint distribution of DNZ.",
            "From football to simple Bayesian network, which we show there are model is that there is a random variable Z and arrow from set to D from from D2 W. So W is a collection of words.",
            "Calligraphic W an SM W as an index into this collection.",
            "Similarly, calligraphic said is a collection of class labels and SMS is index into that.",
            "So this is a model that a class generates documents and document generates words.",
            "As a learning phase, we we estimate these joint distributions of document enzed.",
            "I remind again that he's just an index into the collection of documents and there is an index into the class label set and.",
            "We might also provide the class.",
            "We also provide the class labels and assigned to the documents.",
            "These last labels could come from the document collection, or, in the case of unlabeled documents, we use our measure measure of confidence threshold and find the correct class label.",
            "Valuable documents.",
            "And we also provide this joint distribution.",
            "The measure of support and the classifier learning algorithm SVM on IBS called could use these.",
            "Explain how we how we do that.",
            "As if I apply for unsecured unseen documents such at the top.",
            "So."
        ],
        [
            "Annotations.",
            "We continue with the notations of so W is independent of zed.",
            "Given D, it comes from the simple Bayesian network.",
            "All.",
            "And models are class generated document instances, each of which is a bag of words.",
            "History document instance is a bag of words.",
            "And the simple.",
            "Equation follows from the vision network.",
            "We we we compute this.",
            "This is not a parimeter we we compute this as a fraction of times word W occurs across all words in document D. So this is not a parameterized model.",
            "I mean, it's a it's a known parameter.",
            "It's about original parameter and the observables Rd.",
            "The probability of W given D&WZ pairs.",
            "The world class pairs.",
            "This parameter our model tries to estimate.",
            "So.",
            "Continue with the notations."
        ],
        [
            "NWS Ed is account of word W in document D size of diesel total count.",
            "We scale each document to unit length.",
            "This is based on.",
            "Work done by Andrew McCallum.",
            "We scale each document to unit length.",
            "And we have this NWZ which can be expressed in this form.",
            "I'm sorry, we actually scale each document to a common length L. Scalise document to a common link L. To avoid modeling document length.",
            "Now this L can be the LCM of all document links and we can obtain the empirical distribution of world Class D Class Z as follows.",
            "These are is a subset of documents that have labels assigned as Ed.",
            "So this is easy to derive from this.",
            "So our observations become probability of W given Z when scaled to unit length.",
            "And we use empirical distribution in place of end up user.",
            "Our objective function is a."
        ],
        [
            "Is a log likelihood of observing word class pairs from the.",
            "Training set.",
            "And we express this in terms of can be expressed in terms of this.",
            "Now a more general form of a log likelihood objective based on empirical distribution.",
            "Can be derived as follows, where QWZ was shown in the last light the cube that is empirical distribution.",
            "And they proved that.",
            "The generalized generalized form of log likelihood algorithm can be applied to it.",
            "And you have the constraint here that the probabilities over DMZ should sum to one.",
            "So the condition for the."
        ],
        [
            "Maximum value of the objective function is obtained as follows.",
            "How much is a LaGrange multiplier?",
            "And we get the M step and the eastep here.",
            "In the M step we find the joint distribution of Zed ND.",
            "In terms of probability of B, given W, Z and in the eastep probability of B given W, that is estimated from zed comediennes, which is which is estimated from step and W given D is is precomputed.",
            "So the algorithm is as follows."
        ],
        [
            "So we have the input as empirical distribution Q of W, Z and probability of WMD initializer para meters as a uniform distribution.",
            "Um?",
            "And while the stopping criteria is not met, I'll explain the stopping criteria in a second Easter is that we do for each document in the collection, and for each word.",
            "For each class we estimate D, given W, Z.",
            "And then there's a M step.",
            "Just explaining the last late.",
            "However, we find that.",
            "There's no need for storing this very sparse and large matrix probability of B, given W, Z, and also that these two 2 two steps can be efficiently reformulated into a.",
            "Restructured into this step so you have a PR.",
            "Oh is is old probability of B, Z.",
            "And this is the initial, the zero.",
            "The set to 0 for all the documents and classes.",
            "And then we compute the probability of W, zed.",
            "As follows, so believe efficient restructuring.",
            "So we find that there's no need to store D given W, said.",
            "The details of this can be found in the paper.",
            "So the number of steps is reduced and the amount of storage is also reduced."
        ],
        [
            "We have an optional eastep where we can re estimate the empirical distribution for two purposes.",
            "Either we have we want to fold feature evidence from unlabeled documents.",
            "Order we have noise in the labeling and so we want to.",
            "We want to reach.",
            "Recompute the empirical distribution after the EFUM iterations of run.",
            "So we use a smoothing parameter Lambda.",
            "The empirical distribution in the end titration can be given US Q NWS that we had initialize it and this after so many iterations, the probability estimates and then the iteration.",
            "So we have this intuitive modification to this empirical distribution.",
            "1 minus Lambda times.",
            "Q&W, there from the previous iteration.",
            "And the joint distribution of WNZ.",
            "This can will obtain by marginalizing over these.",
            "So the case of learning in presence of classification noise.",
            "Lambda serves as an estimate of the proportion of noise in the training data.",
            "So how do we use the para meters joint distribution W decomposition?",
            "Like base classifier is explained here.",
            "So we have this aperture distribution, which is generally estimated by ways classifier."
        ],
        [
            "By simply taking a fraction of counts, whereas we do it using this formula which we get from the simple vision network we had initially and W given D is known.",
            "This is the.",
            "This is a support support for assign having assigned a class, labels it to D and.",
            "This so each document, while taking the W given D fraction from each document.",
            "We weighed by the corresponding support.",
            "So these.",
            "These two are one para meters.",
            "So we call this class if we use W given service and I was in my base classifier and we call this the weighted my base, so no explicit feature smoothing is performed in this case.",
            "And we also."
        ],
        [
            "Try plugging in output of Basil in SCM.",
            "So there's a cost based SVM learner is available at the site.",
            "That it allows multiple cost settings, cost of mislea, mislabeled, cost of mislabeling documents in a particular class or the class penalty, as well as every instance can be penalty for every instance.",
            "How important is it to classify each document correctly so the cost of misclassification for each document can be set in this?",
            "And we said this using probability of the estimates from our by marginalizing over.",
            "Zajan probability of decomposition.",
            "And it uses error correcting output code for handling multiple classes.",
            "So we called the resulting classifier weighted SVM.",
            "So we just plugged in our joint distribution, our joint distribution estimates and.",
            "In this particular implementation of SVM.",
            "So we put."
        ],
        [
            "Mom.",
            "Four types of experiments completely supervised learning.",
            "We also have access to unlabeled examples and evaluate how well the model performs when unlabeled documents are provided.",
            "We also add in noise class noise, which probably introduces a thermoflask.",
            "Noise is simply mutating class labels in the training set.",
            "And we also show some experiments showing reflecting that probability of B is a good measure of support.",
            "So we show experiments on two datasets, 20 newsgroups and their KB.",
            "The data preparation we use Rainbow standard text classification software written in C. To parse, tokenize and index the documents so as to.",
            "Is to maintain reproducibility.",
            "And stop words are not removed and no stemming was performed.",
            "So we first."
        ],
        [
            "Present the results for supervised learning.",
            "The two datasets were coming in today news groups with and without probability of the commas estimation based as soon.",
            "The convergence criterion used here is that the migrations are stopped when the change in lock like log likelihood difference across two successive iterations is less than 0.01% of that in the previous iteration, the smoothing parameter was set to Lambda is 0.001.",
            "I mean I mean we just arbitrarily chose it.",
            "The train to test ratio was scripted 60 to 40% and results are reported on 20 random test train splits.",
            "So.",
            "We find that.",
            "You can see the difference in my base and waited my base we the by assigning this notion of support for each probability of W given D. Count not counting the fraction, we find that there's a considerable improvement in accuracy for both web KB as well as 20 newsgroups.",
            "Similarly, we find that SVM also we get substantial improvement.",
            "We also present."
        ],
        [
            "Goes on label unlabeled setting where we have access to unlabeled example, we provide unlabeled examples, so setup is similar to that in Nigam, common, ligament, tall, and 2000.",
            "Oh so set up as we set aside one person training 10% test and the unlabeled collection is built from the remaining documents that more details of the experimental setup is in the paper.",
            "Um?",
            "So.",
            "We have this ten person test and we report all the results of this test collection.",
            "But we also vary the number of unlabeled documents across two values of Lambda.",
            "Show that show how by setting the varying value of Lambda, which is an estimate of either a noise or estimate of the fraction of.",
            "And fraction of labeled unlabeled forms.",
            "We get different improvements, so here are results for web TV."
        ],
        [
            "So the.",
            "This line which you see is a result with Lambda of 0.001.",
            "And this is, you see is with Lambda zero Lambda set to 0.",
            "So setting non 0 value of Lambda helps here.",
            "This is for the optional optional step of re estimating the empirical distribution Q&W, there and you find that it produces substantial improvement and there's a percentage on label document.",
            "So as the number of unlabeled documents is increased, we find the test accuracy is.",
            "Accuracy increases, so this proves two things that the value of Lambda the folding in step voting, empirical evidence from legal documents is definitely helping.",
            "At the same time as the number of unlabeled documents is increased.",
            "Also we get better results.",
            "And This is why."
        ],
        [
            "20 newsgroups.",
            "That is, there's an improvement here.",
            "2 four.",
            "We find that with non 0 value of Lambda, of course Lambda should not be set to too much, otherwise it's well known that there can be complete drift.",
            "Complete drift in the land distribution.",
            "And we find that the classification accuracy increases as the number of unlabeled documents increases.",
            "We found that these results are more or less similar to what somewhat better than what, Lingam all reported in their paper.",
            "The more unlabeled coming to him later.",
            "Is it a strange thing?",
            "I mean, this is the accuracy on the on the head of this set.",
            "Hello.",
            "So.",
            "We have ten person test set which is held out and one person is user training and the remaining is unlabeled.",
            "So we pick in pick documents on the unlabeled collection and accuracies are all reported in the test set.",
            "Try to prove that it induces generalization.",
            "Does that.",
            "OK. Also, we report experiments for noise."
        ],
        [
            "The labels we use a set of very exactly the one used by Bing Liu in 2003.",
            "I CML so 50% was used for training.",
            "This is for.",
            "I'm sorry I didn't mention it here, but this is for the 20 newsgroups data set.",
            "Only 20% was used for validation and 30% for testing.",
            "The setup here was that some of the.",
            "Bing reported results on one versus rest setting.",
            "So there are 20 newsgroups for each of the classes and experiment was performed and all the remaining classes were taken as unlabeled set and the chosen class was taken as the positive set and the others are taken as unlabeled negative set and he introduces noise in the unlabeled collection by introducing positive positive instances in design label collection so.",
            "The 13 percent was held out as fixed tested 50% for training and 20% validation.",
            "So the 50% training and the validation set.",
            "The labels are mutated.",
            "So that's how noise was introduced and he had the classification classification noise Alpha and reset Lambda Alpha to counter the noisy labels in the optional Easter which I presented, the results are results tabulated for weighted SVM.",
            "So we used.",
            "We waited SVM with the probability of the estimates and these are results.",
            "We get there by themselves may not mean much, but I'll compare it against.",
            "Bing you so?"
        ],
        [
            "Our results are substantially better than that of this.",
            "Just look at the best column here with the A0 setting.",
            "So Alpha is a noise, so when zero noises there we perform better than them.",
            "And.",
            "You could ignore the these two columns from his experimental results.",
            "These are the best he reports.",
            "And similarly for Alpha is .3 noise.",
            "Also, we get better results.",
            "These are results that are all F1 results reported.",
            "And file size .7 noise two we get better results.",
            "We also performed some experiments to validate."
        ],
        [
            "The notion of support as provided by our classifier.",
            "So this idea, we had 10% labeled rest unlabeled.",
            "And 3030% classification noise was introduced in the label set by mutating the labels.",
            "So mean and standard deviation are plotted.",
            "I'll explain these in the plots.",
            "So label characters if D is labeled, dies in the label collection and the confidence probability of Z given D or you can even look at argmax over said the Max over.",
            "This gives you the correct label, so we plot the mean.",
            "Probabilities PR of D which is which is a measure of support for the label correct.",
            "Similarly we have labeled wrong, unlabeled, correct and unable wrong.",
            "And if we find that for label correct we have overall higher support for labeled wrong, we have slightly lower.",
            "For unlabeled correct, we have next an unlabeled wrong.",
            "Maybe find this consistently in both 20 newsgroups and web datasets.",
            "So somebody in somebody we."
        ],
        [
            "Embase algorithm for estimation of probability of B given Lee, Z from a label or partially labeled document collection.",
            "It provides a measure of support and confidence and effective way to assist relabelling of documents.",
            "Of course, is measure of confidence.",
            "We are not proposing it because this could be used.",
            "Model can be used in any standard classifier start classifier and into the modification T step three estimate empirical distribution was also proposed.",
            "It reinforces feature values in the unlabeled data set.",
            "Also, it reduces the influence of noise we showed through experiments by from the noisy label examples.",
            "Anatomy is a business shown to improve classification accuracy of standard.",
            "My base in history and even in the no noise and no unlabeled setting.",
            "So we showed that there is approximation in the labeling which our model accounts for.",
            "And in in presence of noise also reported results with and without unlabeled examples.",
            "We found that with unlabeled examples and non zero values of Lambda we we get better generalization and the future work is handing multi label documents.",
            "We have not tested are."
        ],
        [
            "Our method on multi label corpora and extending to information retrieval we have.",
            "We had had done some preliminary experiments and we find that we have good results but.",
            "It's very slow.",
            "Implementation is going to be slow and extending the implementation handle multiple feature types is another feature.",
            "Thank you for attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then I will explain the learning component in the invasion and how the basinal para meters could be used in standard classifiers.",
                    "label": 0
                },
                {
                    "sent": "I present some rabbit experiments and some conclusions.",
                    "label": 0
                },
                {
                    "sent": "So the motivation for this work is basically a hurdle.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In supervised learning of text classifiers where manual labelers it and assign labels to text documents.",
                    "label": 0
                },
                {
                    "sent": "But often there are approximations involved in the manual labeling of documents, so you have.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Often to break a tie between certain class labels or very often the documents belong inherently to multiple classes, and so there's a degree of belonging degree to which a document belongs to each class.",
                    "label": 0
                },
                {
                    "sent": "There would also be explicit noise in the labeling.",
                    "label": 1
                },
                {
                    "sent": "I just give an example.",
                    "label": 0
                },
                {
                    "sent": "It's easy to sometimes generate a label data set with some amount of noise in the labeling by firing a query on the web.",
                    "label": 1
                },
                {
                    "sent": "Oh ando.",
                    "label": 1
                },
                {
                    "sent": "In order to generalize well, you may also want to learn from learning in the presence of unlabeled documents.",
                    "label": 0
                },
                {
                    "sent": "But we look upon this as a problem of learning with incomplete labeling.",
                    "label": 0
                },
                {
                    "sent": "Oh so there's some related work.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In learning from positive and unlabeled examples, those present in last year's Lyceum lost, lost his ICM will.",
                    "label": 0
                },
                {
                    "sent": "And they they cause the problem of learning from positive and unlabeled examples.",
                    "label": 1
                },
                {
                    "sent": "The original problem is basically learning from few labeled examples, and this is how they cause the problem.",
                    "label": 0
                },
                {
                    "sent": "They they treat the unlabeled set to be a mixture of unlabeled plus few positive, few positive instances.",
                    "label": 0
                },
                {
                    "sent": "And they also have an estimate of the proportion of noise in the unlabeled examples.",
                    "label": 0
                },
                {
                    "sent": "They also work done by broadly and fiddle in 1996 where they tried to counter class noise by by an iterative process where the remote training instances that could be potentially misclassified.",
                    "label": 1
                },
                {
                    "sent": "So this is like a metal owner.",
                    "label": 1
                },
                {
                    "sent": "There's also work done by Domingos at all in 1999.",
                    "label": 1
                },
                {
                    "sent": "All four datasets with imbalance imbalance classes cost sensitive learning algorithm and they apply for the specific problem datasets with imbalanced classes.",
                    "label": 1
                },
                {
                    "sent": "And the method we propose is complementary to this work.",
                    "label": 0
                },
                {
                    "sent": "It can be used.",
                    "label": 0
                },
                {
                    "sent": "It can be plugged into this work.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Also, this work done by.",
                    "label": 0
                },
                {
                    "sent": "Nigam and Xanga, but they learn from labeled and unlabeled.",
                    "label": 1
                },
                {
                    "sent": "And there are of course some other people also.",
                    "label": 0
                },
                {
                    "sent": "In order to generalize from few label examples.",
                    "label": 0
                },
                {
                    "sent": "And we show that our proposed method can very well perform this task of learning, from the unlabeled examples as well.",
                    "label": 0
                },
                {
                    "sent": "To induce generalization, but also features smoothing techniques by Laplace, let's tone.",
                    "label": 0
                },
                {
                    "sent": "There are some smoothing techniques, but they do not account for empirical distribution of features on label document.",
                    "label": 1
                },
                {
                    "sent": "They just take some probability models from the word scene in the label documents and.",
                    "label": 1
                },
                {
                    "sent": "Mean words and then distributed amongst unseen words.",
                    "label": 1
                },
                {
                    "sent": "They also work done by Hoffman probabilistic latent, latent semantic analysis, blsa.",
                    "label": 0
                },
                {
                    "sent": "I need he does most suited for information retrieval and for the particular task we found it not performing well.",
                    "label": 0
                },
                {
                    "sent": "So what we propose a model that estimates.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The degree to which each document belongs to or in other words, how well a document D fits into each each of the priests of the specified class labels there.",
                    "label": 1
                },
                {
                    "sent": "So we provide this estimate of joint probability of DNZ.",
                    "label": 0
                },
                {
                    "sent": "This D is basically an index into the.",
                    "label": 0
                },
                {
                    "sent": "Document collection and there is an index into a class label set.",
                    "label": 1
                },
                {
                    "sent": "And we use this to Ada traditional text classifiers.",
                    "label": 0
                },
                {
                    "sent": "Naive business VM so that they can handle approximate noise or incomplete labeling.",
                    "label": 0
                },
                {
                    "sent": "So I'll just plug it into these text classifiers.",
                    "label": 1
                },
                {
                    "sent": "And we can look at probability of B given zed as a measure of support.",
                    "label": 0
                },
                {
                    "sent": "This notion of support and confidence is borrowed from data mining community, but there is this notion of support and confidence.",
                    "label": 0
                },
                {
                    "sent": "How?",
                    "label": 0
                },
                {
                    "sent": "How much evidence is there for the label that has been assigned to a particular class and how confident confidently the label has been assigned?",
                    "label": 0
                },
                {
                    "sent": "Oh so by marginalizing the joint distribution we get this notion of support and confidence.",
                    "label": 0
                },
                {
                    "sent": "So this figure I just tried to explain the.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How business fits.",
                    "label": 0
                },
                {
                    "sent": "So you have a document collection of B. Dear Document collection of documents and you have an index into the documents small D and we use capital D as a random variable to represent the random variable which takes values D from their collection.",
                    "label": 0
                },
                {
                    "sent": "Calligraphic D. And this document collection can be approximately labeled.",
                    "label": 0
                },
                {
                    "sent": "Can be noisy label or there could be only.",
                    "label": 0
                },
                {
                    "sent": "It could also be unlabeled in, which means that there might be knows its corresponding to.",
                    "label": 0
                },
                {
                    "sent": "Certainties how?",
                    "label": 0
                },
                {
                    "sent": "I as an offline task are model based estimates.",
                    "label": 0
                },
                {
                    "sent": "The joint distribution of DNZ.",
                    "label": 0
                },
                {
                    "sent": "From football to simple Bayesian network, which we show there are model is that there is a random variable Z and arrow from set to D from from D2 W. So W is a collection of words.",
                    "label": 0
                },
                {
                    "sent": "Calligraphic W an SM W as an index into this collection.",
                    "label": 0
                },
                {
                    "sent": "Similarly, calligraphic said is a collection of class labels and SMS is index into that.",
                    "label": 0
                },
                {
                    "sent": "So this is a model that a class generates documents and document generates words.",
                    "label": 0
                },
                {
                    "sent": "As a learning phase, we we estimate these joint distributions of document enzed.",
                    "label": 0
                },
                {
                    "sent": "I remind again that he's just an index into the collection of documents and there is an index into the class label set and.",
                    "label": 0
                },
                {
                    "sent": "We might also provide the class.",
                    "label": 0
                },
                {
                    "sent": "We also provide the class labels and assigned to the documents.",
                    "label": 0
                },
                {
                    "sent": "These last labels could come from the document collection, or, in the case of unlabeled documents, we use our measure measure of confidence threshold and find the correct class label.",
                    "label": 0
                },
                {
                    "sent": "Valuable documents.",
                    "label": 0
                },
                {
                    "sent": "And we also provide this joint distribution.",
                    "label": 0
                },
                {
                    "sent": "The measure of support and the classifier learning algorithm SVM on IBS called could use these.",
                    "label": 0
                },
                {
                    "sent": "Explain how we how we do that.",
                    "label": 0
                },
                {
                    "sent": "As if I apply for unsecured unseen documents such at the top.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Annotations.",
                    "label": 0
                },
                {
                    "sent": "We continue with the notations of so W is independent of zed.",
                    "label": 0
                },
                {
                    "sent": "Given D, it comes from the simple Bayesian network.",
                    "label": 0
                },
                {
                    "sent": "All.",
                    "label": 0
                },
                {
                    "sent": "And models are class generated document instances, each of which is a bag of words.",
                    "label": 0
                },
                {
                    "sent": "History document instance is a bag of words.",
                    "label": 0
                },
                {
                    "sent": "And the simple.",
                    "label": 0
                },
                {
                    "sent": "Equation follows from the vision network.",
                    "label": 0
                },
                {
                    "sent": "We we we compute this.",
                    "label": 0
                },
                {
                    "sent": "This is not a parimeter we we compute this as a fraction of times word W occurs across all words in document D. So this is not a parameterized model.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's a it's a known parameter.",
                    "label": 0
                },
                {
                    "sent": "It's about original parameter and the observables Rd.",
                    "label": 0
                },
                {
                    "sent": "The probability of W given D&WZ pairs.",
                    "label": 0
                },
                {
                    "sent": "The world class pairs.",
                    "label": 0
                },
                {
                    "sent": "This parameter our model tries to estimate.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Continue with the notations.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "NWS Ed is account of word W in document D size of diesel total count.",
                    "label": 0
                },
                {
                    "sent": "We scale each document to unit length.",
                    "label": 0
                },
                {
                    "sent": "This is based on.",
                    "label": 0
                },
                {
                    "sent": "Work done by Andrew McCallum.",
                    "label": 0
                },
                {
                    "sent": "We scale each document to unit length.",
                    "label": 0
                },
                {
                    "sent": "And we have this NWZ which can be expressed in this form.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry, we actually scale each document to a common length L. Scalise document to a common link L. To avoid modeling document length.",
                    "label": 1
                },
                {
                    "sent": "Now this L can be the LCM of all document links and we can obtain the empirical distribution of world Class D Class Z as follows.",
                    "label": 0
                },
                {
                    "sent": "These are is a subset of documents that have labels assigned as Ed.",
                    "label": 0
                },
                {
                    "sent": "So this is easy to derive from this.",
                    "label": 1
                },
                {
                    "sent": "So our observations become probability of W given Z when scaled to unit length.",
                    "label": 1
                },
                {
                    "sent": "And we use empirical distribution in place of end up user.",
                    "label": 0
                },
                {
                    "sent": "Our objective function is a.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is a log likelihood of observing word class pairs from the.",
                    "label": 0
                },
                {
                    "sent": "Training set.",
                    "label": 0
                },
                {
                    "sent": "And we express this in terms of can be expressed in terms of this.",
                    "label": 0
                },
                {
                    "sent": "Now a more general form of a log likelihood objective based on empirical distribution.",
                    "label": 0
                },
                {
                    "sent": "Can be derived as follows, where QWZ was shown in the last light the cube that is empirical distribution.",
                    "label": 0
                },
                {
                    "sent": "And they proved that.",
                    "label": 0
                },
                {
                    "sent": "The generalized generalized form of log likelihood algorithm can be applied to it.",
                    "label": 0
                },
                {
                    "sent": "And you have the constraint here that the probabilities over DMZ should sum to one.",
                    "label": 0
                },
                {
                    "sent": "So the condition for the.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Maximum value of the objective function is obtained as follows.",
                    "label": 1
                },
                {
                    "sent": "How much is a LaGrange multiplier?",
                    "label": 0
                },
                {
                    "sent": "And we get the M step and the eastep here.",
                    "label": 0
                },
                {
                    "sent": "In the M step we find the joint distribution of Zed ND.",
                    "label": 0
                },
                {
                    "sent": "In terms of probability of B, given W, Z and in the eastep probability of B given W, that is estimated from zed comediennes, which is which is estimated from step and W given D is is precomputed.",
                    "label": 0
                },
                {
                    "sent": "So the algorithm is as follows.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have the input as empirical distribution Q of W, Z and probability of WMD initializer para meters as a uniform distribution.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And while the stopping criteria is not met, I'll explain the stopping criteria in a second Easter is that we do for each document in the collection, and for each word.",
                    "label": 0
                },
                {
                    "sent": "For each class we estimate D, given W, Z.",
                    "label": 0
                },
                {
                    "sent": "And then there's a M step.",
                    "label": 0
                },
                {
                    "sent": "Just explaining the last late.",
                    "label": 0
                },
                {
                    "sent": "However, we find that.",
                    "label": 0
                },
                {
                    "sent": "There's no need for storing this very sparse and large matrix probability of B, given W, Z, and also that these two 2 two steps can be efficiently reformulated into a.",
                    "label": 0
                },
                {
                    "sent": "Restructured into this step so you have a PR.",
                    "label": 0
                },
                {
                    "sent": "Oh is is old probability of B, Z.",
                    "label": 0
                },
                {
                    "sent": "And this is the initial, the zero.",
                    "label": 0
                },
                {
                    "sent": "The set to 0 for all the documents and classes.",
                    "label": 0
                },
                {
                    "sent": "And then we compute the probability of W, zed.",
                    "label": 0
                },
                {
                    "sent": "As follows, so believe efficient restructuring.",
                    "label": 0
                },
                {
                    "sent": "So we find that there's no need to store D given W, said.",
                    "label": 0
                },
                {
                    "sent": "The details of this can be found in the paper.",
                    "label": 0
                },
                {
                    "sent": "So the number of steps is reduced and the amount of storage is also reduced.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have an optional eastep where we can re estimate the empirical distribution for two purposes.",
                    "label": 0
                },
                {
                    "sent": "Either we have we want to fold feature evidence from unlabeled documents.",
                    "label": 0
                },
                {
                    "sent": "Order we have noise in the labeling and so we want to.",
                    "label": 0
                },
                {
                    "sent": "We want to reach.",
                    "label": 0
                },
                {
                    "sent": "Recompute the empirical distribution after the EFUM iterations of run.",
                    "label": 0
                },
                {
                    "sent": "So we use a smoothing parameter Lambda.",
                    "label": 1
                },
                {
                    "sent": "The empirical distribution in the end titration can be given US Q NWS that we had initialize it and this after so many iterations, the probability estimates and then the iteration.",
                    "label": 0
                },
                {
                    "sent": "So we have this intuitive modification to this empirical distribution.",
                    "label": 0
                },
                {
                    "sent": "1 minus Lambda times.",
                    "label": 0
                },
                {
                    "sent": "Q&W, there from the previous iteration.",
                    "label": 0
                },
                {
                    "sent": "And the joint distribution of WNZ.",
                    "label": 0
                },
                {
                    "sent": "This can will obtain by marginalizing over these.",
                    "label": 0
                },
                {
                    "sent": "So the case of learning in presence of classification noise.",
                    "label": 1
                },
                {
                    "sent": "Lambda serves as an estimate of the proportion of noise in the training data.",
                    "label": 1
                },
                {
                    "sent": "So how do we use the para meters joint distribution W decomposition?",
                    "label": 0
                },
                {
                    "sent": "Like base classifier is explained here.",
                    "label": 0
                },
                {
                    "sent": "So we have this aperture distribution, which is generally estimated by ways classifier.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "By simply taking a fraction of counts, whereas we do it using this formula which we get from the simple vision network we had initially and W given D is known.",
                    "label": 0
                },
                {
                    "sent": "This is the.",
                    "label": 0
                },
                {
                    "sent": "This is a support support for assign having assigned a class, labels it to D and.",
                    "label": 0
                },
                {
                    "sent": "This so each document, while taking the W given D fraction from each document.",
                    "label": 0
                },
                {
                    "sent": "We weighed by the corresponding support.",
                    "label": 0
                },
                {
                    "sent": "So these.",
                    "label": 0
                },
                {
                    "sent": "These two are one para meters.",
                    "label": 0
                },
                {
                    "sent": "So we call this class if we use W given service and I was in my base classifier and we call this the weighted my base, so no explicit feature smoothing is performed in this case.",
                    "label": 1
                },
                {
                    "sent": "And we also.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Try plugging in output of Basil in SCM.",
                    "label": 0
                },
                {
                    "sent": "So there's a cost based SVM learner is available at the site.",
                    "label": 1
                },
                {
                    "sent": "That it allows multiple cost settings, cost of mislea, mislabeled, cost of mislabeling documents in a particular class or the class penalty, as well as every instance can be penalty for every instance.",
                    "label": 0
                },
                {
                    "sent": "How important is it to classify each document correctly so the cost of misclassification for each document can be set in this?",
                    "label": 1
                },
                {
                    "sent": "And we said this using probability of the estimates from our by marginalizing over.",
                    "label": 0
                },
                {
                    "sent": "Zajan probability of decomposition.",
                    "label": 0
                },
                {
                    "sent": "And it uses error correcting output code for handling multiple classes.",
                    "label": 1
                },
                {
                    "sent": "So we called the resulting classifier weighted SVM.",
                    "label": 0
                },
                {
                    "sent": "So we just plugged in our joint distribution, our joint distribution estimates and.",
                    "label": 0
                },
                {
                    "sent": "In this particular implementation of SVM.",
                    "label": 0
                },
                {
                    "sent": "So we put.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mom.",
                    "label": 0
                },
                {
                    "sent": "Four types of experiments completely supervised learning.",
                    "label": 1
                },
                {
                    "sent": "We also have access to unlabeled examples and evaluate how well the model performs when unlabeled documents are provided.",
                    "label": 0
                },
                {
                    "sent": "We also add in noise class noise, which probably introduces a thermoflask.",
                    "label": 0
                },
                {
                    "sent": "Noise is simply mutating class labels in the training set.",
                    "label": 0
                },
                {
                    "sent": "And we also show some experiments showing reflecting that probability of B is a good measure of support.",
                    "label": 0
                },
                {
                    "sent": "So we show experiments on two datasets, 20 newsgroups and their KB.",
                    "label": 0
                },
                {
                    "sent": "The data preparation we use Rainbow standard text classification software written in C. To parse, tokenize and index the documents so as to.",
                    "label": 1
                },
                {
                    "sent": "Is to maintain reproducibility.",
                    "label": 1
                },
                {
                    "sent": "And stop words are not removed and no stemming was performed.",
                    "label": 0
                },
                {
                    "sent": "So we first.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Present the results for supervised learning.",
                    "label": 0
                },
                {
                    "sent": "The two datasets were coming in today news groups with and without probability of the commas estimation based as soon.",
                    "label": 0
                },
                {
                    "sent": "The convergence criterion used here is that the migrations are stopped when the change in lock like log likelihood difference across two successive iterations is less than 0.01% of that in the previous iteration, the smoothing parameter was set to Lambda is 0.001.",
                    "label": 1
                },
                {
                    "sent": "I mean I mean we just arbitrarily chose it.",
                    "label": 1
                },
                {
                    "sent": "The train to test ratio was scripted 60 to 40% and results are reported on 20 random test train splits.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We find that.",
                    "label": 0
                },
                {
                    "sent": "You can see the difference in my base and waited my base we the by assigning this notion of support for each probability of W given D. Count not counting the fraction, we find that there's a considerable improvement in accuracy for both web KB as well as 20 newsgroups.",
                    "label": 0
                },
                {
                    "sent": "Similarly, we find that SVM also we get substantial improvement.",
                    "label": 0
                },
                {
                    "sent": "We also present.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Goes on label unlabeled setting where we have access to unlabeled example, we provide unlabeled examples, so setup is similar to that in Nigam, common, ligament, tall, and 2000.",
                    "label": 0
                },
                {
                    "sent": "Oh so set up as we set aside one person training 10% test and the unlabeled collection is built from the remaining documents that more details of the experimental setup is in the paper.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We have this ten person test and we report all the results of this test collection.",
                    "label": 1
                },
                {
                    "sent": "But we also vary the number of unlabeled documents across two values of Lambda.",
                    "label": 0
                },
                {
                    "sent": "Show that show how by setting the varying value of Lambda, which is an estimate of either a noise or estimate of the fraction of.",
                    "label": 0
                },
                {
                    "sent": "And fraction of labeled unlabeled forms.",
                    "label": 0
                },
                {
                    "sent": "We get different improvements, so here are results for web TV.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "This line which you see is a result with Lambda of 0.001.",
                    "label": 0
                },
                {
                    "sent": "And this is, you see is with Lambda zero Lambda set to 0.",
                    "label": 0
                },
                {
                    "sent": "So setting non 0 value of Lambda helps here.",
                    "label": 0
                },
                {
                    "sent": "This is for the optional optional step of re estimating the empirical distribution Q&W, there and you find that it produces substantial improvement and there's a percentage on label document.",
                    "label": 0
                },
                {
                    "sent": "So as the number of unlabeled documents is increased, we find the test accuracy is.",
                    "label": 0
                },
                {
                    "sent": "Accuracy increases, so this proves two things that the value of Lambda the folding in step voting, empirical evidence from legal documents is definitely helping.",
                    "label": 0
                },
                {
                    "sent": "At the same time as the number of unlabeled documents is increased.",
                    "label": 0
                },
                {
                    "sent": "Also we get better results.",
                    "label": 0
                },
                {
                    "sent": "And This is why.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "20 newsgroups.",
                    "label": 0
                },
                {
                    "sent": "That is, there's an improvement here.",
                    "label": 0
                },
                {
                    "sent": "2 four.",
                    "label": 0
                },
                {
                    "sent": "We find that with non 0 value of Lambda, of course Lambda should not be set to too much, otherwise it's well known that there can be complete drift.",
                    "label": 0
                },
                {
                    "sent": "Complete drift in the land distribution.",
                    "label": 0
                },
                {
                    "sent": "And we find that the classification accuracy increases as the number of unlabeled documents increases.",
                    "label": 0
                },
                {
                    "sent": "We found that these results are more or less similar to what somewhat better than what, Lingam all reported in their paper.",
                    "label": 0
                },
                {
                    "sent": "The more unlabeled coming to him later.",
                    "label": 0
                },
                {
                    "sent": "Is it a strange thing?",
                    "label": 0
                },
                {
                    "sent": "I mean, this is the accuracy on the on the head of this set.",
                    "label": 0
                },
                {
                    "sent": "Hello.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We have ten person test set which is held out and one person is user training and the remaining is unlabeled.",
                    "label": 0
                },
                {
                    "sent": "So we pick in pick documents on the unlabeled collection and accuracies are all reported in the test set.",
                    "label": 0
                },
                {
                    "sent": "Try to prove that it induces generalization.",
                    "label": 0
                },
                {
                    "sent": "Does that.",
                    "label": 0
                },
                {
                    "sent": "OK. Also, we report experiments for noise.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The labels we use a set of very exactly the one used by Bing Liu in 2003.",
                    "label": 0
                },
                {
                    "sent": "I CML so 50% was used for training.",
                    "label": 0
                },
                {
                    "sent": "This is for.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry I didn't mention it here, but this is for the 20 newsgroups data set.",
                    "label": 0
                },
                {
                    "sent": "Only 20% was used for validation and 30% for testing.",
                    "label": 1
                },
                {
                    "sent": "The setup here was that some of the.",
                    "label": 0
                },
                {
                    "sent": "Bing reported results on one versus rest setting.",
                    "label": 0
                },
                {
                    "sent": "So there are 20 newsgroups for each of the classes and experiment was performed and all the remaining classes were taken as unlabeled set and the chosen class was taken as the positive set and the others are taken as unlabeled negative set and he introduces noise in the unlabeled collection by introducing positive positive instances in design label collection so.",
                    "label": 0
                },
                {
                    "sent": "The 13 percent was held out as fixed tested 50% for training and 20% validation.",
                    "label": 0
                },
                {
                    "sent": "So the 50% training and the validation set.",
                    "label": 1
                },
                {
                    "sent": "The labels are mutated.",
                    "label": 0
                },
                {
                    "sent": "So that's how noise was introduced and he had the classification classification noise Alpha and reset Lambda Alpha to counter the noisy labels in the optional Easter which I presented, the results are results tabulated for weighted SVM.",
                    "label": 1
                },
                {
                    "sent": "So we used.",
                    "label": 0
                },
                {
                    "sent": "We waited SVM with the probability of the estimates and these are results.",
                    "label": 0
                },
                {
                    "sent": "We get there by themselves may not mean much, but I'll compare it against.",
                    "label": 0
                },
                {
                    "sent": "Bing you so?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our results are substantially better than that of this.",
                    "label": 0
                },
                {
                    "sent": "Just look at the best column here with the A0 setting.",
                    "label": 0
                },
                {
                    "sent": "So Alpha is a noise, so when zero noises there we perform better than them.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "You could ignore the these two columns from his experimental results.",
                    "label": 0
                },
                {
                    "sent": "These are the best he reports.",
                    "label": 0
                },
                {
                    "sent": "And similarly for Alpha is .3 noise.",
                    "label": 0
                },
                {
                    "sent": "Also, we get better results.",
                    "label": 0
                },
                {
                    "sent": "These are results that are all F1 results reported.",
                    "label": 1
                },
                {
                    "sent": "And file size .7 noise two we get better results.",
                    "label": 0
                },
                {
                    "sent": "We also performed some experiments to validate.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The notion of support as provided by our classifier.",
                    "label": 1
                },
                {
                    "sent": "So this idea, we had 10% labeled rest unlabeled.",
                    "label": 1
                },
                {
                    "sent": "And 3030% classification noise was introduced in the label set by mutating the labels.",
                    "label": 0
                },
                {
                    "sent": "So mean and standard deviation are plotted.",
                    "label": 1
                },
                {
                    "sent": "I'll explain these in the plots.",
                    "label": 0
                },
                {
                    "sent": "So label characters if D is labeled, dies in the label collection and the confidence probability of Z given D or you can even look at argmax over said the Max over.",
                    "label": 0
                },
                {
                    "sent": "This gives you the correct label, so we plot the mean.",
                    "label": 1
                },
                {
                    "sent": "Probabilities PR of D which is which is a measure of support for the label correct.",
                    "label": 0
                },
                {
                    "sent": "Similarly we have labeled wrong, unlabeled, correct and unable wrong.",
                    "label": 0
                },
                {
                    "sent": "And if we find that for label correct we have overall higher support for labeled wrong, we have slightly lower.",
                    "label": 0
                },
                {
                    "sent": "For unlabeled correct, we have next an unlabeled wrong.",
                    "label": 0
                },
                {
                    "sent": "Maybe find this consistently in both 20 newsgroups and web datasets.",
                    "label": 0
                },
                {
                    "sent": "So somebody in somebody we.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Embase algorithm for estimation of probability of B given Lee, Z from a label or partially labeled document collection.",
                    "label": 0
                },
                {
                    "sent": "It provides a measure of support and confidence and effective way to assist relabelling of documents.",
                    "label": 1
                },
                {
                    "sent": "Of course, is measure of confidence.",
                    "label": 0
                },
                {
                    "sent": "We are not proposing it because this could be used.",
                    "label": 0
                },
                {
                    "sent": "Model can be used in any standard classifier start classifier and into the modification T step three estimate empirical distribution was also proposed.",
                    "label": 1
                },
                {
                    "sent": "It reinforces feature values in the unlabeled data set.",
                    "label": 1
                },
                {
                    "sent": "Also, it reduces the influence of noise we showed through experiments by from the noisy label examples.",
                    "label": 0
                },
                {
                    "sent": "Anatomy is a business shown to improve classification accuracy of standard.",
                    "label": 1
                },
                {
                    "sent": "My base in history and even in the no noise and no unlabeled setting.",
                    "label": 0
                },
                {
                    "sent": "So we showed that there is approximation in the labeling which our model accounts for.",
                    "label": 0
                },
                {
                    "sent": "And in in presence of noise also reported results with and without unlabeled examples.",
                    "label": 0
                },
                {
                    "sent": "We found that with unlabeled examples and non zero values of Lambda we we get better generalization and the future work is handing multi label documents.",
                    "label": 0
                },
                {
                    "sent": "We have not tested are.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our method on multi label corpora and extending to information retrieval we have.",
                    "label": 1
                },
                {
                    "sent": "We had had done some preliminary experiments and we find that we have good results but.",
                    "label": 0
                },
                {
                    "sent": "It's very slow.",
                    "label": 0
                },
                {
                    "sent": "Implementation is going to be slow and extending the implementation handle multiple feature types is another feature.",
                    "label": 1
                },
                {
                    "sent": "Thank you for attention.",
                    "label": 0
                }
            ]
        }
    }
}