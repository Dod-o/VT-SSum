{
    "id": "5y6rbz7i5paw3noazs6yklzzyxpiittx",
    "title": "Efficient Learning of Simplices",
    "info": {
        "author": [
            "Luis Rademacher, Department of Computer Science and Engineering, Ohio State University"
        ],
        "published": "Aug. 9, 2013",
        "recorded": "June 2013",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/colt2013_rademacher_learning/",
    "segmentation": [
        [
            "OK, so yeah this is joint with yourself Anderson and Bingo yeah and the problem I'm going to talk about is you're giving a finally transform simplex in a dimensional space.",
            "Well, you don't see the simplex, you just see uniformly random samples from this and you want to estimate the simplex."
        ],
        [
            "So at the very high level, the my personal motivation for this problem is on one hand.",
            "Well, you can show that this problem has a low sample complexity by a VC dimension like argument, but no known efficient algorithm was known.",
            "So I want to understand the gap between sample complexity and algorithmic complexity in high dimensional problems and.",
            "The second motivation or approaches to study prototypical problems in estimation problems in high dimension.",
            "In this case, properties of distributions related to high dimensional convex bodies."
        ],
        [
            "So here is the problem more precisely.",
            "You're giving uniformly random points from an dimensional simplex.",
            "And you want to recover the simplex.",
            "So this really means you want to approximately recover the affine transformation that was applied to it.",
            "Yes.",
            "There is, there are no labels exactly.",
            "You only get points from.",
            "If you like, you only get positive samples.",
            "Vision is some kind of.",
            "The distribution is uniform, yes.",
            "So the only thing I need to tell you to specify the problem completely is what's the metric.",
            "I'm going to use and it's very natural in this case to use the total variation distance."
        ],
        [
            "Also known as statistical distance between the uniform distributions on the input and the output right in this new specialized this metric to this specific case, it looks like this.",
            "You find which one is the biggest in volume, and it's just the volume of the larger minus the smaller relative to the volume of the larger one.",
            "So when I write it like that, it looks like the metric is not symmetric, but of course that's just because I wrote it in that way you play with this definition, you'll see it's symmetric."
        ],
        [
            "So here are a few more space on multiple specific motivations for this particular question.",
            "Well, first of all, the to be able to do this sufficiently was left as an open question in a paper in 1986 by Fris German cannon.",
            "As I mentioned before, the VC theory implies that this problem must have polynomial sample complexity.",
            "But this theory itself doesn't give an efficient algorithm.",
            "Um, more specific motivation and also some kind of application that I done my own idea.",
            "If someone else is work, is that the same?",
            "Similar ideas can be used for estimation in latent variable models.",
            "So this is the independent work of an Kumar Foster.",
            "So Canyon, Lou.",
            "For latent initial location, this is.",
            "Generalized version of this problem where you want to estimate the parameters of additional distribution.",
            "I mean getting to the core or the part of the problem that matches what I'm talking about and which is on continuous distribution supported on a simplex.",
            "Generalizing the uniform distribution.",
            "So there you not only want to estimate their fine transformation, you also want to estimate the parameters of the distribution.",
            "But the techniques that apply to the simplex problem with small modifications also applied to the other, and they were so discover independently in the second work of an ankle Margie.",
            "Super car until dusk.",
            "And final motivation is to understand algorithmic techniques that have been used in independent component analysis.",
            "Beyond the assumption of independence and also see how much how much you can push these techniques and also how much you can push them to work with provable guarantees, because there are very successful algorithms there that work very well in practice.",
            "I mean, but they don't have that many theoretical guarantees."
        ],
        [
            "So.",
            "One of the my results.",
            "Our results will establish a connection between the simplex learning problem and ICA, so I want to remind you what the ICA problem is.",
            "The idea of the problem is the following.",
            "Your there is a random vector S that has independent coordinates.",
            "You don't know the distributions of the coordinates.",
            "That could be different.",
            "And what you observe is a linear transformation of this vector.",
            "So let's call that vector X.",
            "Or do we decide you observe samples from this linear transformation you want to recover the linear transformation ultimately to be able to invert this transformation and see the original, untransformed signal.",
            "So I know an observation of how these methods generally work or what this problem is about really.",
            "Is that by just looking at the covariance of the data?",
            "You say suppose that data is there random variable X was every coordinate uniform in an interval.",
            "So then this is uniform in a square.",
            "If this was two dimensional, the affine transformation gives you a parallelogram and just by looking at the covariance I could without loss of generality attack only the problem of finding the rotation induced by this transformation A because the other part of the transformation I can find by looking at the covariance.",
            "So really the novelty in ICA beyond second order methods is to.",
            "Given method to find this rotation."
        ],
        [
            "So here is a summary of our result.",
            "So the first one is a polynomial time algorithm for estimating a simplex.",
            "So it has polynomial sample complexity and running time.",
            "And the 2nd result is an explicit connection between the simplex problem and I see a nameless a direct and fairly simple randomized reduction from the problem of learning a simplex do ICA.",
            "So you get the sample from the simplex problem and somehow there is a randomized reduction here that gives you a sample that is suitable for an ICM problem, and from the solution to the problem you can recover the transformation of the simplex.",
            "And the techniques for the reduction for the simplex problem also give a reduction for the problem of learning an affine transformation of an LP ball to the ICA problem."
        ],
        [
            "So let me mention some work.",
            "Related to this problem.",
            "To put it in a slightly broader context, well, there is the paper of Freeze German and that I mentioned before this paper was about ICA.",
            "Actually, even though they don't call it ICA in that paper, namely, learning a linear transformation of a vector having independent coordinates.",
            "In particular, it gives an algorithm for learning a parallelepiped or an affine transformation of a cube.",
            "Later in, when Andre gave in 2006, gave simplified algorithm for the specific case of parallelepiped.",
            "This was motivated by cryptography and breaking encryption schemes.",
            "Um?",
            "So that's on the side of upper bounds of similar looking problems.",
            "Earlier, upper bounds, on the other hand, on the complexity of the problem of estimating a convex body, that is the work by having gotten myself in 2009 and also closely related paper by clients O'Donnell and Servedio.",
            "Where both papers in slightly different sampling models show a lower bound of C to the root and samples to given a convex body estimate the convex body.",
            "On the side of upper bounds for the general convex body problem, which in some sense is also known as the intersection of spaces problem.",
            "Depending on how you look at it and what the distribution is, the same paper of clients O'Donnell and Servedio.",
            "Gave a polynomial time algorithm to learn the intersection of a constant number of spaces and in slightly different models.",
            "There is also the work by center from Paula.",
            "And say in the agnostic setting in this conference we saw the work of Cain climbers and Mecca.",
            "All these papers give you.",
            "Algorithms that would though be super polynomial if you tried to apply them to the simplex problem, because they're super polynomial in the number of spaces and the simplex you have plus 1/2 spaces.",
            "So.",
            "Independent of our work, but solving a very closely related problem that is the work of unencumber fosters one cadien Lou.",
            "Which gave a different algorithm based on spectral decompositions applied to the matrix decomposition supplied to the third moment of the distribution of the sample.",
            "You see to estimate the parameters of a distribution as I mentioned before, and then the later paper by Anna Kumar Gesu Caliente Garski uses a tensor power method or some iteration to diagonalize a tensor.",
            "And this can also estimate the distribution and solve other latent variable problems.",
            "Their algorithm is very similar to the one we use.",
            "But then again, these were independent.",
            "And finally, there is a work of grave and lesser search.",
            "Nick and Robbins who studied the problem of reconstructing a polytope.",
            "With few vertices or few facets, given the moment.",
            "But in this case they focus also on even understanding how many moments you need for exact recovery and therefore this, not surprisingly in its higher moments.",
            "So there is no polynomial time guarantee for this kind of algorithm.",
            "Play the moments are the moments of the year for distribution on this point.",
            "Oh yes.",
            "Well, unless you are talking about the the problem of the digital distribution which is non uniform right?",
            "But in the last one is at the moment of the uniform distribution, yes."
        ],
        [
            "So here is the the main ingredients for the algorithm for efficient learning a simplex well as it was known in the ICA setting without loss of generality, we can assume that the transformation is a rotation because the rest you can figure out by finding the mean and covariance matrix of your sample, at least approximately.",
            "And then to find the rotation, here is a key observation for the simplex case.",
            "Similar observations were seen in the freezer and canal paper and in the ICA setting, but there was the assumption of independence.",
            "That if you have an isotropic simplex.",
            "And you consider the following functional given a vector U in RN is the directional third moment.",
            "Then when you look at this functional over the unit sphere.",
            "The when you take the vertices of the simplex and normalize them so that they have actually unit length, then this gives you a complete list of local and global Optima of this function.",
            "Um, which?",
            "Well, I think it's pretty interesting 'cause it's.",
            "A somewhat unusual case with finding local optimize actually helpful.",
            "S. You say that rotated simplex, if you like rotated isotropic simplex.",
            "So yes.",
            "And the second observation is that one can do the enumeration problem solving enumeration problem efficiently by just using a variation of gradient descent.",
            "Well, technically, could you could just do gradient descent, but I call it a variation becausw it will actually behave much better than usual gradient descent if you pick the right variation.",
            "Yes.",
            "Rental uniforms right after this scaling, baby, no longer uniform right, you mean the isotopic transformation that brings things back to isotopic position?",
            "Oh, actually, it's just a xenophile transformation, so yeah, uniformity is invariant under that, so it's actually still uniform.",
            "But the measure is involved in the translation and also under under linear transformations."
        ],
        [
            "So let's get a little bit deeper on this claim of the structure of the Maxima.",
            "So how to understand the third moment of anisotropic simplex?",
            "Well, it's easier to think about this by embedding the simplex into one dimension higher.",
            "In particular in the hyperplane sum of XI equals to one, because then we can imagine a. I picked the Canonical rotation that would make it the standard simplex and this is just the convex Hull of the Canonical vectors.",
            "And the rotation just means our rotation inside that plane, so our rotation leaving the old ones vector invariant.",
            "And once you have the simplex there, of course you can ignore the rotation for the purpose of doing this computation, and you can integrate in However way you want, and then you will find at the moment along you is some constant times the complete homogeneous polynomial in U.",
            "So this is the polynomial that has all monomers of degree three with coefficient one.",
            "And once you add well, you only care about directions that are orthogonal to the old ones vector, because that's where the action is happening.",
            "This now is a simplex of dimension in R N + 1, right?",
            "You only care about unit vectors.",
            "This polynomial actually becomes just a sum of cubes of UI times some constant, so that's the final shape of the third moment there, and then you just play with LaGrange multipliers and 2nd order optimality conditions, say to understand the structure of the Maxima.",
            "Which in this case will be projections of the Canonical vectors of the vertices of the lifted simplex."
        ],
        [
            "Now about the problem of finding the vertices.",
            "So the way I said it was going to be a gradient like it are gradient descent like iteration.",
            "This is a variation of of an observation of the paper of and 100 give.",
            "So what we're going to do is that from the current iterate U we will iteration will in any direct way compute the vector of the squares of the coordinates of you.",
            "So how does it do this?",
            "Well, even before I tell you how it does that.",
            "What does that achieve?",
            "Like if you think of this iteration, it's going to converge to a Canonical vector, right?",
            "The first iteration makes everything negative.",
            "If I keep normalizing because the scale doesn't matter, it will converge to a Canonical vector.",
            "In fact, such an iteration converges to a Canonical vector very fast with quadratic convergence.",
            "Now, how does one implement such an iteration?",
            "Will you just look at the gradient so when you try to think of how great the descent would do, you realize that you can do much better than usual linear convergence, as you would expect from gradient ascent.",
            "You write the gradient and it takes the following form.",
            "That is the term we are interested in you with the coordinate squared times some constant.",
            "Then there is some other constant times the old ones vector the sum of the coordinates of you.",
            "I hold square the sum of squares of coordinates and then the again the vector U times the sum of the coordinates.",
            "So the observation is that all the terms except the one we're interested in can be computed.",
            "From the current iteration.",
            "With the gradient and but they can also be done in a coordinate freeway because you may say oh this whole computation you did was in the Canonical standard simplex right where things were not rotated.",
            "But everything here can be computed also after rotations that leave the old ones vector invariant.",
            "For example, the sum of the squares is, well, there's two norm which is invariant under rotation, and the sum of you eyes is just the inner product between you and the old ones vector, and because the rotation left one invariant, this inner product is also invariant under rotation.",
            "So by just solving for the vector having the squares of the coordinates of you.",
            "You can implement this iteration in our rotation invariant way.",
            "In whatever coordinate system was given to you."
        ],
        [
            "So let me tell you now about the reduction the reduction.",
            "For the simplex, I'm not going to explain in detail because it's a slight refinement of the one for the LP bowl, and let's keep the core idea.",
            "The idea is there is the following observation.",
            "Suppose that X is a random point in the N dimensional LP ball, and I take an independent scalar, let's call it T, that is distributed as.",
            "Gamma with parameter N / P + 1, One South, whatever that is.",
            "You can sample from that.",
            "Then we show that if you look at the vector T to the 1 / P * X, then the coordinates of this vector and independent.",
            "And.",
            "So very similar claims are found in this work of checkman and gene and also in a later paper by birthday with an mendelsohn an hour, but not precisely.",
            "This independence.",
            "Of course, once you know what to multiply with.",
            "It's just a matter of playing with the change of variable formula.",
            "So."
        ],
        [
            "So how does this lead to a reduction?",
            "Suppose now X is random in the LP ball.",
            "And why is a linear transformation of this vector?",
            "Then if I multiply by the same scale data described before.",
            "The linear transformation will commute, obviously, so I can also do the multiplication by a scalar.",
            "In the transform LP ball in a sample from the transform LP Bowl and this will still have the independence properties, except that now this is not happening in an orthonormal basis is something is happening in some unknown basis, but we call this in the context of ICA.",
            "The basis of the independent components and ICA will recover these basis.",
            "On this basis is precisely aligned with the axis of the transform LP ball, so this is precisely the transformation.",
            "So the reduction just takes a sample.",
            "And multiplies by this sequence of independent scalars.",
            "And fits this to the ICA algorithm, whatever it is, as a black box."
        ],
        [
            "So let me mention an open problem.",
            "Well, one obvious generalization of this question is in particular understanding of sample versus algorithm complexity is to go to the case of polytopes with a polynomial number of facets, right?",
            "This has low sample complexity.",
            "Again, polynomial in the dimension, but no efficient algorithm is known.",
            "So I'll stop here any questions?"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so yeah this is joint with yourself Anderson and Bingo yeah and the problem I'm going to talk about is you're giving a finally transform simplex in a dimensional space.",
                    "label": 0
                },
                {
                    "sent": "Well, you don't see the simplex, you just see uniformly random samples from this and you want to estimate the simplex.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So at the very high level, the my personal motivation for this problem is on one hand.",
                    "label": 0
                },
                {
                    "sent": "Well, you can show that this problem has a low sample complexity by a VC dimension like argument, but no known efficient algorithm was known.",
                    "label": 0
                },
                {
                    "sent": "So I want to understand the gap between sample complexity and algorithmic complexity in high dimensional problems and.",
                    "label": 1
                },
                {
                    "sent": "The second motivation or approaches to study prototypical problems in estimation problems in high dimension.",
                    "label": 0
                },
                {
                    "sent": "In this case, properties of distributions related to high dimensional convex bodies.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is the problem more precisely.",
                    "label": 0
                },
                {
                    "sent": "You're giving uniformly random points from an dimensional simplex.",
                    "label": 1
                },
                {
                    "sent": "And you want to recover the simplex.",
                    "label": 0
                },
                {
                    "sent": "So this really means you want to approximately recover the affine transformation that was applied to it.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "There is, there are no labels exactly.",
                    "label": 0
                },
                {
                    "sent": "You only get points from.",
                    "label": 0
                },
                {
                    "sent": "If you like, you only get positive samples.",
                    "label": 0
                },
                {
                    "sent": "Vision is some kind of.",
                    "label": 0
                },
                {
                    "sent": "The distribution is uniform, yes.",
                    "label": 0
                },
                {
                    "sent": "So the only thing I need to tell you to specify the problem completely is what's the metric.",
                    "label": 0
                },
                {
                    "sent": "I'm going to use and it's very natural in this case to use the total variation distance.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Also known as statistical distance between the uniform distributions on the input and the output right in this new specialized this metric to this specific case, it looks like this.",
                    "label": 1
                },
                {
                    "sent": "You find which one is the biggest in volume, and it's just the volume of the larger minus the smaller relative to the volume of the larger one.",
                    "label": 0
                },
                {
                    "sent": "So when I write it like that, it looks like the metric is not symmetric, but of course that's just because I wrote it in that way you play with this definition, you'll see it's symmetric.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here are a few more space on multiple specific motivations for this particular question.",
                    "label": 0
                },
                {
                    "sent": "Well, first of all, the to be able to do this sufficiently was left as an open question in a paper in 1986 by Fris German cannon.",
                    "label": 1
                },
                {
                    "sent": "As I mentioned before, the VC theory implies that this problem must have polynomial sample complexity.",
                    "label": 1
                },
                {
                    "sent": "But this theory itself doesn't give an efficient algorithm.",
                    "label": 0
                },
                {
                    "sent": "Um, more specific motivation and also some kind of application that I done my own idea.",
                    "label": 0
                },
                {
                    "sent": "If someone else is work, is that the same?",
                    "label": 1
                },
                {
                    "sent": "Similar ideas can be used for estimation in latent variable models.",
                    "label": 1
                },
                {
                    "sent": "So this is the independent work of an Kumar Foster.",
                    "label": 0
                },
                {
                    "sent": "So Canyon, Lou.",
                    "label": 0
                },
                {
                    "sent": "For latent initial location, this is.",
                    "label": 0
                },
                {
                    "sent": "Generalized version of this problem where you want to estimate the parameters of additional distribution.",
                    "label": 0
                },
                {
                    "sent": "I mean getting to the core or the part of the problem that matches what I'm talking about and which is on continuous distribution supported on a simplex.",
                    "label": 0
                },
                {
                    "sent": "Generalizing the uniform distribution.",
                    "label": 1
                },
                {
                    "sent": "So there you not only want to estimate their fine transformation, you also want to estimate the parameters of the distribution.",
                    "label": 0
                },
                {
                    "sent": "But the techniques that apply to the simplex problem with small modifications also applied to the other, and they were so discover independently in the second work of an ankle Margie.",
                    "label": 0
                },
                {
                    "sent": "Super car until dusk.",
                    "label": 0
                },
                {
                    "sent": "And final motivation is to understand algorithmic techniques that have been used in independent component analysis.",
                    "label": 0
                },
                {
                    "sent": "Beyond the assumption of independence and also see how much how much you can push these techniques and also how much you can push them to work with provable guarantees, because there are very successful algorithms there that work very well in practice.",
                    "label": 0
                },
                {
                    "sent": "I mean, but they don't have that many theoretical guarantees.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "One of the my results.",
                    "label": 0
                },
                {
                    "sent": "Our results will establish a connection between the simplex learning problem and ICA, so I want to remind you what the ICA problem is.",
                    "label": 0
                },
                {
                    "sent": "The idea of the problem is the following.",
                    "label": 0
                },
                {
                    "sent": "Your there is a random vector S that has independent coordinates.",
                    "label": 1
                },
                {
                    "sent": "You don't know the distributions of the coordinates.",
                    "label": 0
                },
                {
                    "sent": "That could be different.",
                    "label": 0
                },
                {
                    "sent": "And what you observe is a linear transformation of this vector.",
                    "label": 0
                },
                {
                    "sent": "So let's call that vector X.",
                    "label": 0
                },
                {
                    "sent": "Or do we decide you observe samples from this linear transformation you want to recover the linear transformation ultimately to be able to invert this transformation and see the original, untransformed signal.",
                    "label": 0
                },
                {
                    "sent": "So I know an observation of how these methods generally work or what this problem is about really.",
                    "label": 0
                },
                {
                    "sent": "Is that by just looking at the covariance of the data?",
                    "label": 0
                },
                {
                    "sent": "You say suppose that data is there random variable X was every coordinate uniform in an interval.",
                    "label": 0
                },
                {
                    "sent": "So then this is uniform in a square.",
                    "label": 0
                },
                {
                    "sent": "If this was two dimensional, the affine transformation gives you a parallelogram and just by looking at the covariance I could without loss of generality attack only the problem of finding the rotation induced by this transformation A because the other part of the transformation I can find by looking at the covariance.",
                    "label": 0
                },
                {
                    "sent": "So really the novelty in ICA beyond second order methods is to.",
                    "label": 0
                },
                {
                    "sent": "Given method to find this rotation.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is a summary of our result.",
                    "label": 0
                },
                {
                    "sent": "So the first one is a polynomial time algorithm for estimating a simplex.",
                    "label": 1
                },
                {
                    "sent": "So it has polynomial sample complexity and running time.",
                    "label": 0
                },
                {
                    "sent": "And the 2nd result is an explicit connection between the simplex problem and I see a nameless a direct and fairly simple randomized reduction from the problem of learning a simplex do ICA.",
                    "label": 0
                },
                {
                    "sent": "So you get the sample from the simplex problem and somehow there is a randomized reduction here that gives you a sample that is suitable for an ICM problem, and from the solution to the problem you can recover the transformation of the simplex.",
                    "label": 1
                },
                {
                    "sent": "And the techniques for the reduction for the simplex problem also give a reduction for the problem of learning an affine transformation of an LP ball to the ICA problem.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me mention some work.",
                    "label": 0
                },
                {
                    "sent": "Related to this problem.",
                    "label": 0
                },
                {
                    "sent": "To put it in a slightly broader context, well, there is the paper of Freeze German and that I mentioned before this paper was about ICA.",
                    "label": 0
                },
                {
                    "sent": "Actually, even though they don't call it ICA in that paper, namely, learning a linear transformation of a vector having independent coordinates.",
                    "label": 0
                },
                {
                    "sent": "In particular, it gives an algorithm for learning a parallelepiped or an affine transformation of a cube.",
                    "label": 0
                },
                {
                    "sent": "Later in, when Andre gave in 2006, gave simplified algorithm for the specific case of parallelepiped.",
                    "label": 0
                },
                {
                    "sent": "This was motivated by cryptography and breaking encryption schemes.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So that's on the side of upper bounds of similar looking problems.",
                    "label": 0
                },
                {
                    "sent": "Earlier, upper bounds, on the other hand, on the complexity of the problem of estimating a convex body, that is the work by having gotten myself in 2009 and also closely related paper by clients O'Donnell and Servedio.",
                    "label": 0
                },
                {
                    "sent": "Where both papers in slightly different sampling models show a lower bound of C to the root and samples to given a convex body estimate the convex body.",
                    "label": 0
                },
                {
                    "sent": "On the side of upper bounds for the general convex body problem, which in some sense is also known as the intersection of spaces problem.",
                    "label": 0
                },
                {
                    "sent": "Depending on how you look at it and what the distribution is, the same paper of clients O'Donnell and Servedio.",
                    "label": 0
                },
                {
                    "sent": "Gave a polynomial time algorithm to learn the intersection of a constant number of spaces and in slightly different models.",
                    "label": 1
                },
                {
                    "sent": "There is also the work by center from Paula.",
                    "label": 0
                },
                {
                    "sent": "And say in the agnostic setting in this conference we saw the work of Cain climbers and Mecca.",
                    "label": 0
                },
                {
                    "sent": "All these papers give you.",
                    "label": 0
                },
                {
                    "sent": "Algorithms that would though be super polynomial if you tried to apply them to the simplex problem, because they're super polynomial in the number of spaces and the simplex you have plus 1/2 spaces.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Independent of our work, but solving a very closely related problem that is the work of unencumber fosters one cadien Lou.",
                    "label": 1
                },
                {
                    "sent": "Which gave a different algorithm based on spectral decompositions applied to the matrix decomposition supplied to the third moment of the distribution of the sample.",
                    "label": 1
                },
                {
                    "sent": "You see to estimate the parameters of a distribution as I mentioned before, and then the later paper by Anna Kumar Gesu Caliente Garski uses a tensor power method or some iteration to diagonalize a tensor.",
                    "label": 0
                },
                {
                    "sent": "And this can also estimate the distribution and solve other latent variable problems.",
                    "label": 0
                },
                {
                    "sent": "Their algorithm is very similar to the one we use.",
                    "label": 0
                },
                {
                    "sent": "But then again, these were independent.",
                    "label": 0
                },
                {
                    "sent": "And finally, there is a work of grave and lesser search.",
                    "label": 0
                },
                {
                    "sent": "Nick and Robbins who studied the problem of reconstructing a polytope.",
                    "label": 1
                },
                {
                    "sent": "With few vertices or few facets, given the moment.",
                    "label": 0
                },
                {
                    "sent": "But in this case they focus also on even understanding how many moments you need for exact recovery and therefore this, not surprisingly in its higher moments.",
                    "label": 1
                },
                {
                    "sent": "So there is no polynomial time guarantee for this kind of algorithm.",
                    "label": 0
                },
                {
                    "sent": "Play the moments are the moments of the year for distribution on this point.",
                    "label": 0
                },
                {
                    "sent": "Oh yes.",
                    "label": 0
                },
                {
                    "sent": "Well, unless you are talking about the the problem of the digital distribution which is non uniform right?",
                    "label": 0
                },
                {
                    "sent": "But in the last one is at the moment of the uniform distribution, yes.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is the the main ingredients for the algorithm for efficient learning a simplex well as it was known in the ICA setting without loss of generality, we can assume that the transformation is a rotation because the rest you can figure out by finding the mean and covariance matrix of your sample, at least approximately.",
                    "label": 0
                },
                {
                    "sent": "And then to find the rotation, here is a key observation for the simplex case.",
                    "label": 0
                },
                {
                    "sent": "Similar observations were seen in the freezer and canal paper and in the ICA setting, but there was the assumption of independence.",
                    "label": 0
                },
                {
                    "sent": "That if you have an isotropic simplex.",
                    "label": 1
                },
                {
                    "sent": "And you consider the following functional given a vector U in RN is the directional third moment.",
                    "label": 1
                },
                {
                    "sent": "Then when you look at this functional over the unit sphere.",
                    "label": 0
                },
                {
                    "sent": "The when you take the vertices of the simplex and normalize them so that they have actually unit length, then this gives you a complete list of local and global Optima of this function.",
                    "label": 1
                },
                {
                    "sent": "Um, which?",
                    "label": 0
                },
                {
                    "sent": "Well, I think it's pretty interesting 'cause it's.",
                    "label": 0
                },
                {
                    "sent": "A somewhat unusual case with finding local optimize actually helpful.",
                    "label": 0
                },
                {
                    "sent": "S. You say that rotated simplex, if you like rotated isotropic simplex.",
                    "label": 0
                },
                {
                    "sent": "So yes.",
                    "label": 1
                },
                {
                    "sent": "And the second observation is that one can do the enumeration problem solving enumeration problem efficiently by just using a variation of gradient descent.",
                    "label": 0
                },
                {
                    "sent": "Well, technically, could you could just do gradient descent, but I call it a variation becausw it will actually behave much better than usual gradient descent if you pick the right variation.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Rental uniforms right after this scaling, baby, no longer uniform right, you mean the isotopic transformation that brings things back to isotopic position?",
                    "label": 0
                },
                {
                    "sent": "Oh, actually, it's just a xenophile transformation, so yeah, uniformity is invariant under that, so it's actually still uniform.",
                    "label": 0
                },
                {
                    "sent": "But the measure is involved in the translation and also under under linear transformations.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's get a little bit deeper on this claim of the structure of the Maxima.",
                    "label": 0
                },
                {
                    "sent": "So how to understand the third moment of anisotropic simplex?",
                    "label": 0
                },
                {
                    "sent": "Well, it's easier to think about this by embedding the simplex into one dimension higher.",
                    "label": 0
                },
                {
                    "sent": "In particular in the hyperplane sum of XI equals to one, because then we can imagine a. I picked the Canonical rotation that would make it the standard simplex and this is just the convex Hull of the Canonical vectors.",
                    "label": 0
                },
                {
                    "sent": "And the rotation just means our rotation inside that plane, so our rotation leaving the old ones vector invariant.",
                    "label": 0
                },
                {
                    "sent": "And once you have the simplex there, of course you can ignore the rotation for the purpose of doing this computation, and you can integrate in However way you want, and then you will find at the moment along you is some constant times the complete homogeneous polynomial in U.",
                    "label": 0
                },
                {
                    "sent": "So this is the polynomial that has all monomers of degree three with coefficient one.",
                    "label": 0
                },
                {
                    "sent": "And once you add well, you only care about directions that are orthogonal to the old ones vector, because that's where the action is happening.",
                    "label": 0
                },
                {
                    "sent": "This now is a simplex of dimension in R N + 1, right?",
                    "label": 0
                },
                {
                    "sent": "You only care about unit vectors.",
                    "label": 0
                },
                {
                    "sent": "This polynomial actually becomes just a sum of cubes of UI times some constant, so that's the final shape of the third moment there, and then you just play with LaGrange multipliers and 2nd order optimality conditions, say to understand the structure of the Maxima.",
                    "label": 1
                },
                {
                    "sent": "Which in this case will be projections of the Canonical vectors of the vertices of the lifted simplex.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now about the problem of finding the vertices.",
                    "label": 0
                },
                {
                    "sent": "So the way I said it was going to be a gradient like it are gradient descent like iteration.",
                    "label": 0
                },
                {
                    "sent": "This is a variation of of an observation of the paper of and 100 give.",
                    "label": 1
                },
                {
                    "sent": "So what we're going to do is that from the current iterate U we will iteration will in any direct way compute the vector of the squares of the coordinates of you.",
                    "label": 0
                },
                {
                    "sent": "So how does it do this?",
                    "label": 0
                },
                {
                    "sent": "Well, even before I tell you how it does that.",
                    "label": 0
                },
                {
                    "sent": "What does that achieve?",
                    "label": 0
                },
                {
                    "sent": "Like if you think of this iteration, it's going to converge to a Canonical vector, right?",
                    "label": 0
                },
                {
                    "sent": "The first iteration makes everything negative.",
                    "label": 0
                },
                {
                    "sent": "If I keep normalizing because the scale doesn't matter, it will converge to a Canonical vector.",
                    "label": 1
                },
                {
                    "sent": "In fact, such an iteration converges to a Canonical vector very fast with quadratic convergence.",
                    "label": 0
                },
                {
                    "sent": "Now, how does one implement such an iteration?",
                    "label": 0
                },
                {
                    "sent": "Will you just look at the gradient so when you try to think of how great the descent would do, you realize that you can do much better than usual linear convergence, as you would expect from gradient ascent.",
                    "label": 0
                },
                {
                    "sent": "You write the gradient and it takes the following form.",
                    "label": 0
                },
                {
                    "sent": "That is the term we are interested in you with the coordinate squared times some constant.",
                    "label": 0
                },
                {
                    "sent": "Then there is some other constant times the old ones vector the sum of the coordinates of you.",
                    "label": 0
                },
                {
                    "sent": "I hold square the sum of squares of coordinates and then the again the vector U times the sum of the coordinates.",
                    "label": 0
                },
                {
                    "sent": "So the observation is that all the terms except the one we're interested in can be computed.",
                    "label": 0
                },
                {
                    "sent": "From the current iteration.",
                    "label": 0
                },
                {
                    "sent": "With the gradient and but they can also be done in a coordinate freeway because you may say oh this whole computation you did was in the Canonical standard simplex right where things were not rotated.",
                    "label": 0
                },
                {
                    "sent": "But everything here can be computed also after rotations that leave the old ones vector invariant.",
                    "label": 0
                },
                {
                    "sent": "For example, the sum of the squares is, well, there's two norm which is invariant under rotation, and the sum of you eyes is just the inner product between you and the old ones vector, and because the rotation left one invariant, this inner product is also invariant under rotation.",
                    "label": 0
                },
                {
                    "sent": "So by just solving for the vector having the squares of the coordinates of you.",
                    "label": 0
                },
                {
                    "sent": "You can implement this iteration in our rotation invariant way.",
                    "label": 0
                },
                {
                    "sent": "In whatever coordinate system was given to you.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me tell you now about the reduction the reduction.",
                    "label": 0
                },
                {
                    "sent": "For the simplex, I'm not going to explain in detail because it's a slight refinement of the one for the LP bowl, and let's keep the core idea.",
                    "label": 0
                },
                {
                    "sent": "The idea is there is the following observation.",
                    "label": 0
                },
                {
                    "sent": "Suppose that X is a random point in the N dimensional LP ball, and I take an independent scalar, let's call it T, that is distributed as.",
                    "label": 0
                },
                {
                    "sent": "Gamma with parameter N / P + 1, One South, whatever that is.",
                    "label": 0
                },
                {
                    "sent": "You can sample from that.",
                    "label": 0
                },
                {
                    "sent": "Then we show that if you look at the vector T to the 1 / P * X, then the coordinates of this vector and independent.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So very similar claims are found in this work of checkman and gene and also in a later paper by birthday with an mendelsohn an hour, but not precisely.",
                    "label": 0
                },
                {
                    "sent": "This independence.",
                    "label": 0
                },
                {
                    "sent": "Of course, once you know what to multiply with.",
                    "label": 0
                },
                {
                    "sent": "It's just a matter of playing with the change of variable formula.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how does this lead to a reduction?",
                    "label": 0
                },
                {
                    "sent": "Suppose now X is random in the LP ball.",
                    "label": 1
                },
                {
                    "sent": "And why is a linear transformation of this vector?",
                    "label": 0
                },
                {
                    "sent": "Then if I multiply by the same scale data described before.",
                    "label": 0
                },
                {
                    "sent": "The linear transformation will commute, obviously, so I can also do the multiplication by a scalar.",
                    "label": 0
                },
                {
                    "sent": "In the transform LP ball in a sample from the transform LP Bowl and this will still have the independence properties, except that now this is not happening in an orthonormal basis is something is happening in some unknown basis, but we call this in the context of ICA.",
                    "label": 1
                },
                {
                    "sent": "The basis of the independent components and ICA will recover these basis.",
                    "label": 0
                },
                {
                    "sent": "On this basis is precisely aligned with the axis of the transform LP ball, so this is precisely the transformation.",
                    "label": 0
                },
                {
                    "sent": "So the reduction just takes a sample.",
                    "label": 1
                },
                {
                    "sent": "And multiplies by this sequence of independent scalars.",
                    "label": 0
                },
                {
                    "sent": "And fits this to the ICA algorithm, whatever it is, as a black box.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me mention an open problem.",
                    "label": 1
                },
                {
                    "sent": "Well, one obvious generalization of this question is in particular understanding of sample versus algorithm complexity is to go to the case of polytopes with a polynomial number of facets, right?",
                    "label": 0
                },
                {
                    "sent": "This has low sample complexity.",
                    "label": 0
                },
                {
                    "sent": "Again, polynomial in the dimension, but no efficient algorithm is known.",
                    "label": 0
                },
                {
                    "sent": "So I'll stop here any questions?",
                    "label": 0
                }
            ]
        }
    }
}