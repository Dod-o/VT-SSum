{
    "id": "pmbqkvhq4fw5nksebqxrpll3caiwdpcg",
    "title": "Large Scale Support Vector Machines",
    "info": {
        "author": [
            "Chih-Jen Lin, Department of Computer Science and Information Engineering, National Taiwan University"
        ],
        "published": "Sept. 1, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods->Support Vector Machines"
        ]
    },
    "url": "http://videolectures.net/icml08_lin_lssvm/",
    "segmentation": [
        [
            "OK.",
            "I'm not crazy.",
            "They send me their slides.",
            "To present their their results on participating in this competition.",
            "So here I will try to.",
            "To describe their approaches and I also tried hard to see how I can be a little bit more like Olivia and to have some French accent.",
            "Proved to be very difficult."
        ],
        [
            "So hardly.",
            "So how do they want to say is that so?",
            "In your company?",
            "I mean, we know that for the machine learning task, the in general the final goal is to get high generalization ability, so usually testing accuracy is the main objective, so so so so far we are minimizing an objective function of an optimization problem that's just part of the story, yeah?",
            "Yeah, because the final scenes still under testing accuracy, so they cite.",
            "A paper in Nice last year by about two and buscuit.",
            "So the title is called the trade of large scale learning.",
            "We want to say visiting that paper they argue that will sometimes you exactly solve the optimization problem is not very useful because you already already achieve certain testing accuracy.",
            "So so so this is how they still updating function value and the training error can be interesting for just for debugging, But the final goal is the test.",
            "So because of this, so so for this competition they mainly work on the wildtrak just how to give good tasting."
        ],
        [
            "Curious.",
            "And they would like to say that the number of examples is an issue is now you have a very large data set, but you don't need to.",
            "So they give an example.",
            "If you have a street dimensional she dimensional datasets and you want to separate them, you don't need 1,000,000 examples.",
            "And then they they found out that for the competition.",
            "So you just use a subset.",
            "Then you can already generate a reasonable linear classifier.",
            "Yeah, I think this is just like sometimes if you do subsampling."
        ],
        [
            "Smart subsampling it is useful.",
            "In the starting condition is an issue because user very strict stopping condition and you may get very long training time.",
            "And they find out that in quite a few situations that using a very loose stopping criterion is usually good enough.",
            "So, so I think Olivia ran an example here.",
            "But this is A and UCI data set with health million instances, so that's the one I I also mentioned in my talk earlier.",
            "So of course it was under certain parameters.",
            "Then they changed the staffing.",
            "The starving parlance of lives here from 10 to minus three to one, then reduce the training time by a factor of 10 but without losing any accuracy so.",
            "So think so they say when comparing different training algorithms then stopping criterion should also be considered."
        ],
        [
            "Yeah.",
            "Well, I think we have said this earlier that.",
            "So in comparing different methods because our final goal is on the testing error, so usually drawing a plot to see type the testing area versus time is useful.",
            "So usually we do agree that, but they're thinking something more complicated, because now there are different things like stopping criterion, number of examples, and other things.",
            "So all those things can give you different curves.",
            "In this figure, so they're thinking that maybe overall we should consider all of them too."
        ],
        [
            "Weather so we give an example here.",
            "For this MNIST data they tried different number of subsets.",
            "So you tried your 700 instances.",
            "Len.",
            "Well, after a certain training time then you get 7% or 6% of testing error.",
            "But if you use, say, up to 3000, then you get another curve.",
            "And if you use more like 10,000 then that's this blue curve."
        ],
        [
            "So hopefully.",
            "We should be able to.",
            "Do city something like this one.",
            "So that's why they mentioned."
        ],
        [
            "Something called a lower envelope, but."
        ],
        [
            "Then of course, I ask them how to get this curve in practice when they say, well, no, they don't have a good way yet, but just conceptually, they're hoping that even enjoying the figure of testing error versus time, then something like this should be considered together."
        ],
        [
            "Then for this competition they would like to use differentiable loss function.",
            "So earlier we have talk about the use either using L1 loss or L2 loans.",
            "So for example like in the linear SVM trick, people are required to use L1 SVN, but because they they work on the the wild trick, so they decided to use the L2 loss function.",
            "Yeah, so instead of use the other one loss the hinge loss.",
            "Then use the squared hinge loss.",
            "So as this one this one is differential."
        ],
        [
            "Lynn, here's their first approach.",
            "So leave the paper in neural computation and I think this waste website includes code or for large paper, so they try to solve SVM optimization in a prime or invalid.",
            "So we want to say something that for this competition they use, they consider linear SVM.",
            "So maybe we should ask them quietly.",
            "They use linear and without considering nonlinear.",
            "So I sent him a Mail and he said he think he did try a little bit but then.",
            "Then blah blah and he decided to use cleaning so.",
            "So yeah, so.",
            "I think so far we.",
            "Yet due to the new user, quite complicated evaluation criterion of the wild trick, then actually know the situation between linear nonlinear is still not very clear.",
            "So here we are saying is that to solve the primer and use linear kernel.",
            "Then so we minimize the primal with the L2 loss function by nonlinear conjugate gradient.",
            "So we need to calculate the gradient if you use the L2 loss function, then the function is actually differentiable, but is not twice differentiable, but it is differentiable.",
            "So you can calculate the gradient so the gradient is like this.",
            "Of.",
            "Yeah, so the complexity of calculating the gradient I think, and is there a number of instances in the DS?",
            "The number of features?",
            "And we don't go through the details of, say, hot nonlinear country gradient is.",
            "But basically you find the country gradient direction and you need to do line search.",
            "In the light search involves function evaluations.",
            "And.",
            "But in order to take you to do function evaluations, you need to do matrix vector multiplication.",
            "Here X is the matrix including all training instances.",
            "But W is the current current primal vector, and it's easier direction.",
            "So you want to do a line search and how they want to say here is because XW&XS can be precomputed, so doing linesearch Israel are cheap.",
            "In the two subprimal is in addition to nonlinear conjugate gradient."
        ],
        [
            "They also consider truncated Newton Mason.",
            "The basic idea is that from the current primal variable you calculate the Hessian matrix as the 2nd order derivative of the SVN objective function and times gradient.",
            "This becomes your Newton direction.",
            "But remember, I just said that.",
            "At least L2 SVM is not twice differentiable.",
            "So how can you get the Hessian matrix mutation means second, second derivative.",
            "So actually H here is the so called.",
            "I guess I guess they use circle generalized Haitian, so you have one for you to lose.",
            "As SVM you can define.",
            "The general is Haitian.",
            "Yeah, so you get the direction.",
            "Yeah.",
            "You're not different.",
            "You're not vice principle and just one spot.",
            "Yeah, yeah, yeah, right right.",
            "So yeah yeah yeah yeah so low so points which are not differentiable.",
            "There are major zero.",
            "That's right, yes?",
            "I can answer this question for you.",
            "He he will watch the video.",
            "Open I forget.",
            "So that's not a big name.",
            "Then he didn't know the first part of this talk.",
            "Yeah, so you notice that.",
            "This is the new ten direction in.",
            "Then similar to nonlinear country gradient, you should do line search for less.",
            "Well, that's right.",
            "So what's your advantage of using Newton direction here?",
            "If you were in my area talk, so remember that I mentioned that for linear SVM there I separate 23 situations.",
            "Not if the number of data is much larger than number of features, or for other situations, but right now for this competition let's this case.",
            "Except one problem which is a document set for others.",
            "The number of data is much larger than the number of features.",
            "So what so?",
            "So the trick here is you can do this inverse even if OK, remember that.",
            "So use W the size of W is the same as the number of features.",
            "Yeah.",
            "So so now we minimize the primal.",
            "We can use some.",
            "So this H is actually a square matrix of D * D. Or if these are the number of features.",
            "So, so you need a D CU to invert location.",
            "That's right, but that's not the main computational bottleneck.",
            "The computational bottleneck is.",
            "This is.",
            "There's a place of doing X transpose X.",
            "So in that place, the computational cost is actually an times the square will not.",
            "If you're not familiar with this and don't worry about what SV means, but it is something related to the number of features.",
            "Number of data.",
            "So this term is related to a number of data.",
            "Then times the square loss remain computational costs of of getting these.",
            "This Haitian matrix.",
            "Yeah, so they already mentioned here that this method is not feasible if your number of features is large, because then.",
            "If these larger than, then doing this is basically impossible, but that's not the case for the competition.",
            "Let's see if this happens because at least there is 1 document data set where they cannot invert."
        ],
        [
            "Patient so in that case they use conjugate linear conjugate gradient to solve the Newton system.",
            "Well, if you're not familiar with."
        ],
        [
            "Can't you agree this is different from nonlinear country gradient where you can consider this just an optimization method to directly minimize the object?"
        ],
        [
            "Function.",
            "But here they still want to use Newton method OK essentially to get this direction is to solve a linear system."
        ],
        [
            "They do that by by by regular country ingredient to solve a linear system.",
            "So.",
            "So this is roughly there, the Newton method.",
            "No, this is a country gradient.",
            "OK, so we see two level 2 levels of the loop, so the 1st loop is the Newton method.",
            "So suppose here we need to get a new.",
            "Get a new ten direction.",
            "So I think this is the country gradient procedure too.",
            "So for Congre Dion, how do you need is to?",
            "I think is to do a matrix vector multiplication, so you need to do.",
            "Yeah.",
            "No, let me think.",
            "So now you don't get the.",
            "You cannot do inverse.",
            "But"
        ],
        [
            "The trick is.",
            "This edge can be written as the product of two vectors as X transpose X.",
            "So if you do conjugate gradient.",
            "All you need is the Hessian vector inner product.",
            "But instead of formulating the whole Haitian.",
            "You multiply the vector with X first, then we may be X transpose, so you can do a Haitian vector product without formulating.",
            "Without storing the."
        ],
        [
            "Cohesion.",
            "Oh yeah, Lily, Lady mentioned that each iteration is a matrix vector multiplication.",
            "And this is good if the training data is sparse.",
            "No.",
            "It is similar to like right now.",
            "What implemented in liblinear for solving the L2 SVM primal."
        ],
        [
            "But for the full competition.",
            "Lady, the only one Newton step."
        ],
        [
            "So what does this mean?",
            "Usually this Newton direction may not be.",
            "OK, may not give you a lower function value.",
            "OK, because you may the direction may be too long or something, so you want to do a line search, but I don't think for the competition late."
        ],
        [
            "Lady Linesearch let's just do.",
            "They just do 1 Newton step and go to the next Newton iteration.",
            "I mean, if in general it converges then that is OK. And as I have said earlier, since the number of dimensions, I mean the number of features is rather small for this competition in general, they do invert location.",
            "So they have a MATLAB implementation because all those operations can be easily implemented in Matlab."
        ],
        [
            "Len.",
            "So.",
            "So now for this one Newton step, you need to calculate the hashing first, then do the in the inversion.",
            "And I said earlier that the inversion is not the main bottleneck, is actually an UN getting the Hessian matrix.",
            "That takes ND squared, right?",
            "So that's the main main computational task, so they try to compare.",
            "Like should we use?",
            "Should we inverse the Hessian matrix or should we use linear conjugate gradient method to solve the linear system?",
            "Yeah."
        ],
        [
            "So they mentioned two kinds of truncated Newton implementations here.",
            "One is that to get H and then then to calculate inverse.",
            "But another is by.",
            "Linear conjugate gradient to get the new ten direction, so they try to have a comparison.",
            "So.",
            "Oh no, that's not what they did.",
            "Sorry, we listen on my slides, but any late ladies."
        ],
        [
            "At least they compare this one with this nonlinear conjugate gradient.",
            "OK, that's what they they did, but this one is similar.",
            "Users we have said this again involves matrix vector products.",
            "The major computational task here is about matrix vector products."
        ],
        [
            "Yeah, so for one approach the truncating the Newton.",
            "They get hash and then do inverse and this is the main computational cost.",
            "And therefore nonlinear conjugate gradient.",
            "Lee they try to take K steps of nonlinear conjugate gradient with K less than the number of features.",
            "So it's like they do a fixed number of nonlinear conjugate gradient steps.",
            "And we said earlier that for country gradient domain costs on matrix vector product.",
            "So let's order ND.",
            "And because you do K steps, so the total complex complexities KN times D. So if you compare this with this order and this square.",
            "Len because K is less, they choose a case smaller than D. So older candy should be smaller, right?",
            "OK.",
            "So you would think that this one may be faster.",
            "Yeah.",
            "However they.",
            "OK, let's hear, but they find out that this one is actually slower.",
            "OK, so that means if you do, if you get Haitian link to inverse, they find out that it's fast and they think the reason is because that matrix matrix multiplications can be better optimized using the computer architecture, taking the advantage of computer architectures.",
            "Exactly what they're saying is using using Matlab and internally MATLAB involves optimized basic linear algebra subroutines for matrix matrix operations, so that's actually very related to the.",
            "Which talk about this talk they mention bless, right?",
            "Yeah, so when only we are used.",
            "Olivia used matlab.",
            "Essentially he used optimized place.",
            "To do that?",
            "To do the job.",
            "So that is very effective in this situation.",
            "Yeah.",
            "Yeah, because I don't know if you are familiar with optimized place, but when you do matrix matrix operations use, you split each matrix to several blocks so you can better push blocks into the higher level of the computer architecture.",
            "So then the CPU will be busy on doing your operations.",
            "Then they mentioned that similar observations was made by you shall bendure last year.",
            "Well, I was not there so I didn't.",
            "I was not aware of this but but I think this kind of things have been known for awhile, and in fact for linear SVM, if you go to check Max Arians Paper will Max.",
            "Aaron is A is an optimization researcher in University of Wisconsin.",
            "So I think in a few years ago he did a lot of.",
            "Optimization methods for suitable for linear SVM and he if you look at his papers he used a lot of Matlab and basically just take this advantage."
        ],
        [
            "Len Solar Flare first method.",
            "the Newton Newton stuff.",
            "Then they have the second method, so Olivia in the currency they have the second method.",
            "But this is essentially liblinear for.",
            "44L2 SVA.",
            "Are so different from my students talk this morning is that my students only work on the linear trick, but they use this kind of technique for the wild trick.",
            "Willie made a mistake here for L2, SV and S -- S one not minus 3 -- 3 is for everyone I think.",
            "Because here they laid well 2.",
            "Yeah, so we have said a few times what this this method is just to do a coordinate dissent, you sequentially pick a deal variable and two.",
            "Minimize with respect to that variable.",
            "They do shrinking as well because shrinking is very effective."
        ],
        [
            "And how about stopping conditions?",
            "So.",
            "So the this dual coding and decent procedure is stopped when one of the following conditions occurs.",
            "So either the deal optimality violation is wasting point what.",
            "I think for the competition the requirement is relative.",
            "Primal Dual Gap is less than .01, so it's not clear whether this is a more strict or more strict one.",
            "Well, I don't know, and another thing is, if the total number of iterations.",
            "Is is more than 20, then the procedure stops.",
            "So here by iteration I don't mean the minimization of 1 variable is.",
            "Then that means you only minimize with respect to 20 components.",
            "I mean 20 outer iteration.",
            "So each outer iteration you actually.",
            "Go through all the.",
            "Older deal variables.",
            "Because they work on a wild trick.",
            "So after the competition they realize that.",
            "If they stop based on the testing, the testing accuracy, let's actually in this competition.",
            "This is this some kind of area under the curve is used, so if they use this then this is actually a more effective one.",
            "Because they work down there while the trick, so they need to do parameter selection in order to get good generalization performance.",
            "So to do parameter selection they they select a subset of 100,000 training samples.",
            "Len latitude of parameter C tilt.",
            "And you see, what's the reason of using CC TLD?",
            "In doing parameter selection you if you use a subset then you select the parameter, then you worry about that when applying.",
            "Well when you go to train the whole training set whether the same selected parameters is suitable.",
            "They don't think so.",
            "They need to do some adjustments due to the size difference between the whole set and the subset.",
            "So this is hot lady.",
            "If C Theater is the best C parameter of training a subset.",
            "After finding it then they multiply it with square root of big and divided by small N. So N is the size of the full training set and N is the 100,000 here.",
            "Yeah, so that's the way."
        ],
        [
            "How they select the C for their final training?",
            "OK, so finally.",
            "So they have.",
            "There are some discussions here, so long as people have observed that the main bottleneck of training linear classifiers is actually on reading the data.",
            "Actually we all suffer from that and it takes a lot of time to read the data, say more than 90%.",
            "So they are suggesting that if for future challenges, then if we consider datasets.",
            "Which do not fit into memory, then we should also include the IO time in the evaluation.",
            "Yeah, well, but I don't know how to do that in practice, but everybody has different different architectures, different machines so.",
            "But even if your data cannot fit into memory, then of course you need to consider your algorithm.",
            "You need.",
            "You need to have a good algorithm which try to minimize the number of passes on the data because you don't want to read your data many times that disk IO is very expensive."
        ],
        [
            "What this sentence is?",
            "Everybody wanted that robust method should be fully automated.",
            "Yeah, so so they are suggesting that the time the total time used for model selection and all other things should be included in the evaluation.",
            "In more extremely, they're they're thinking that for participants they should not see datasets.",
            "OK, so they think ideally for participants they should just submit the layer code so they don't see the data or but of course then hopefully the submitted code has an automatic way to do blah blah blah until together good testing accuracy value then that may be a better way to do a fair comparison.",
            "So hopefully we can do that in the future, so thank you very much.",
            "But I am not old.",
            "Yeah, OK, so you are welcome to ask me something.",
            "Yeah, OK, so Olivia will be very heavy.",
            "Yeah.",
            "So anymore comments.",
            "Yes.",
            "Any idea why?",
            "Page.",
            "Better than online.",
            "Well, I think I have explained.",
            "OK, so I didn't do a good job.",
            "Essentially, that goes back to those situations.",
            "Up so earlier when I talk about this situation, so right now we all use some kind of system case, the gradient decent or dual coordinate dissent.",
            "All those things for such data.",
            "So this one is.",
            "But now we're talking about this situation.",
            "Indeed, for those situations you can take a take the advantage of the something easy small.",
            "So buy something small.",
            "That means you have a rectangular data matrix.",
            "So if you have a rectangular matrix, so usually in a situation you can store large matrix data, that's actually the data, and by doing suitable matrix matrix operation then you get a very small square matrix, so we use less square matrix together your direction.",
            "So so that's very efficient.",
            "So for this kind of situation you should use not only.",
            "So here I mentioned you use Primal based romances, but not only should use matrix based method, that's very effective.",
            "So if so, I mentioned some materials paper.",
            "If you go go back to look at some of his earlier papers or such methods are such such things.",
            "So that's I think that's the reason you want to take this adventure.",
            "Something is small and you have two regular rectangular matrix.",
            "Then you multiply them.",
            "You get a small matrix and just use the small matrix to get your direction.",
            "Lastly well, I don't know if Olivia think I answered that correctly.",
            "No.",
            "OK, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I'm not crazy.",
                    "label": 0
                },
                {
                    "sent": "They send me their slides.",
                    "label": 0
                },
                {
                    "sent": "To present their their results on participating in this competition.",
                    "label": 0
                },
                {
                    "sent": "So here I will try to.",
                    "label": 0
                },
                {
                    "sent": "To describe their approaches and I also tried hard to see how I can be a little bit more like Olivia and to have some French accent.",
                    "label": 0
                },
                {
                    "sent": "Proved to be very difficult.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So hardly.",
                    "label": 0
                },
                {
                    "sent": "So how do they want to say is that so?",
                    "label": 0
                },
                {
                    "sent": "In your company?",
                    "label": 0
                },
                {
                    "sent": "I mean, we know that for the machine learning task, the in general the final goal is to get high generalization ability, so usually testing accuracy is the main objective, so so so so far we are minimizing an objective function of an optimization problem that's just part of the story, yeah?",
                    "label": 0
                },
                {
                    "sent": "Yeah, because the final scenes still under testing accuracy, so they cite.",
                    "label": 0
                },
                {
                    "sent": "A paper in Nice last year by about two and buscuit.",
                    "label": 0
                },
                {
                    "sent": "So the title is called the trade of large scale learning.",
                    "label": 1
                },
                {
                    "sent": "We want to say visiting that paper they argue that will sometimes you exactly solve the optimization problem is not very useful because you already already achieve certain testing accuracy.",
                    "label": 0
                },
                {
                    "sent": "So so so this is how they still updating function value and the training error can be interesting for just for debugging, But the final goal is the test.",
                    "label": 1
                },
                {
                    "sent": "So because of this, so so for this competition they mainly work on the wildtrak just how to give good tasting.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Curious.",
                    "label": 0
                },
                {
                    "sent": "And they would like to say that the number of examples is an issue is now you have a very large data set, but you don't need to.",
                    "label": 1
                },
                {
                    "sent": "So they give an example.",
                    "label": 0
                },
                {
                    "sent": "If you have a street dimensional she dimensional datasets and you want to separate them, you don't need 1,000,000 examples.",
                    "label": 1
                },
                {
                    "sent": "And then they they found out that for the competition.",
                    "label": 0
                },
                {
                    "sent": "So you just use a subset.",
                    "label": 1
                },
                {
                    "sent": "Then you can already generate a reasonable linear classifier.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think this is just like sometimes if you do subsampling.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Smart subsampling it is useful.",
                    "label": 0
                },
                {
                    "sent": "In the starting condition is an issue because user very strict stopping condition and you may get very long training time.",
                    "label": 0
                },
                {
                    "sent": "And they find out that in quite a few situations that using a very loose stopping criterion is usually good enough.",
                    "label": 1
                },
                {
                    "sent": "So, so I think Olivia ran an example here.",
                    "label": 0
                },
                {
                    "sent": "But this is A and UCI data set with health million instances, so that's the one I I also mentioned in my talk earlier.",
                    "label": 0
                },
                {
                    "sent": "So of course it was under certain parameters.",
                    "label": 0
                },
                {
                    "sent": "Then they changed the staffing.",
                    "label": 1
                },
                {
                    "sent": "The starving parlance of lives here from 10 to minus three to one, then reduce the training time by a factor of 10 but without losing any accuracy so.",
                    "label": 0
                },
                {
                    "sent": "So think so they say when comparing different training algorithms then stopping criterion should also be considered.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Well, I think we have said this earlier that.",
                    "label": 0
                },
                {
                    "sent": "So in comparing different methods because our final goal is on the testing error, so usually drawing a plot to see type the testing area versus time is useful.",
                    "label": 0
                },
                {
                    "sent": "So usually we do agree that, but they're thinking something more complicated, because now there are different things like stopping criterion, number of examples, and other things.",
                    "label": 1
                },
                {
                    "sent": "So all those things can give you different curves.",
                    "label": 0
                },
                {
                    "sent": "In this figure, so they're thinking that maybe overall we should consider all of them too.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Weather so we give an example here.",
                    "label": 0
                },
                {
                    "sent": "For this MNIST data they tried different number of subsets.",
                    "label": 0
                },
                {
                    "sent": "So you tried your 700 instances.",
                    "label": 0
                },
                {
                    "sent": "Len.",
                    "label": 0
                },
                {
                    "sent": "Well, after a certain training time then you get 7% or 6% of testing error.",
                    "label": 0
                },
                {
                    "sent": "But if you use, say, up to 3000, then you get another curve.",
                    "label": 0
                },
                {
                    "sent": "And if you use more like 10,000 then that's this blue curve.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So hopefully.",
                    "label": 0
                },
                {
                    "sent": "We should be able to.",
                    "label": 0
                },
                {
                    "sent": "Do city something like this one.",
                    "label": 0
                },
                {
                    "sent": "So that's why they mentioned.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Something called a lower envelope, but.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then of course, I ask them how to get this curve in practice when they say, well, no, they don't have a good way yet, but just conceptually, they're hoping that even enjoying the figure of testing error versus time, then something like this should be considered together.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then for this competition they would like to use differentiable loss function.",
                    "label": 1
                },
                {
                    "sent": "So earlier we have talk about the use either using L1 loss or L2 loans.",
                    "label": 0
                },
                {
                    "sent": "So for example like in the linear SVM trick, people are required to use L1 SVN, but because they they work on the the wild trick, so they decided to use the L2 loss function.",
                    "label": 1
                },
                {
                    "sent": "Yeah, so instead of use the other one loss the hinge loss.",
                    "label": 0
                },
                {
                    "sent": "Then use the squared hinge loss.",
                    "label": 1
                },
                {
                    "sent": "So as this one this one is differential.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Lynn, here's their first approach.",
                    "label": 0
                },
                {
                    "sent": "So leave the paper in neural computation and I think this waste website includes code or for large paper, so they try to solve SVM optimization in a prime or invalid.",
                    "label": 1
                },
                {
                    "sent": "So we want to say something that for this competition they use, they consider linear SVM.",
                    "label": 0
                },
                {
                    "sent": "So maybe we should ask them quietly.",
                    "label": 0
                },
                {
                    "sent": "They use linear and without considering nonlinear.",
                    "label": 0
                },
                {
                    "sent": "So I sent him a Mail and he said he think he did try a little bit but then.",
                    "label": 0
                },
                {
                    "sent": "Then blah blah and he decided to use cleaning so.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so.",
                    "label": 0
                },
                {
                    "sent": "I think so far we.",
                    "label": 0
                },
                {
                    "sent": "Yet due to the new user, quite complicated evaluation criterion of the wild trick, then actually know the situation between linear nonlinear is still not very clear.",
                    "label": 0
                },
                {
                    "sent": "So here we are saying is that to solve the primer and use linear kernel.",
                    "label": 0
                },
                {
                    "sent": "Then so we minimize the primal with the L2 loss function by nonlinear conjugate gradient.",
                    "label": 0
                },
                {
                    "sent": "So we need to calculate the gradient if you use the L2 loss function, then the function is actually differentiable, but is not twice differentiable, but it is differentiable.",
                    "label": 0
                },
                {
                    "sent": "So you can calculate the gradient so the gradient is like this.",
                    "label": 0
                },
                {
                    "sent": "Of.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so the complexity of calculating the gradient I think, and is there a number of instances in the DS?",
                    "label": 1
                },
                {
                    "sent": "The number of features?",
                    "label": 0
                },
                {
                    "sent": "And we don't go through the details of, say, hot nonlinear country gradient is.",
                    "label": 1
                },
                {
                    "sent": "But basically you find the country gradient direction and you need to do line search.",
                    "label": 0
                },
                {
                    "sent": "In the light search involves function evaluations.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "But in order to take you to do function evaluations, you need to do matrix vector multiplication.",
                    "label": 0
                },
                {
                    "sent": "Here X is the matrix including all training instances.",
                    "label": 0
                },
                {
                    "sent": "But W is the current current primal vector, and it's easier direction.",
                    "label": 0
                },
                {
                    "sent": "So you want to do a line search and how they want to say here is because XW&XS can be precomputed, so doing linesearch Israel are cheap.",
                    "label": 0
                },
                {
                    "sent": "In the two subprimal is in addition to nonlinear conjugate gradient.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "They also consider truncated Newton Mason.",
                    "label": 1
                },
                {
                    "sent": "The basic idea is that from the current primal variable you calculate the Hessian matrix as the 2nd order derivative of the SVN objective function and times gradient.",
                    "label": 0
                },
                {
                    "sent": "This becomes your Newton direction.",
                    "label": 0
                },
                {
                    "sent": "But remember, I just said that.",
                    "label": 0
                },
                {
                    "sent": "At least L2 SVM is not twice differentiable.",
                    "label": 0
                },
                {
                    "sent": "So how can you get the Hessian matrix mutation means second, second derivative.",
                    "label": 0
                },
                {
                    "sent": "So actually H here is the so called.",
                    "label": 0
                },
                {
                    "sent": "I guess I guess they use circle generalized Haitian, so you have one for you to lose.",
                    "label": 0
                },
                {
                    "sent": "As SVM you can define.",
                    "label": 0
                },
                {
                    "sent": "The general is Haitian.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so you get the direction.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "You're not different.",
                    "label": 0
                },
                {
                    "sent": "You're not vice principle and just one spot.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, yeah, right right.",
                    "label": 0
                },
                {
                    "sent": "So yeah yeah yeah yeah so low so points which are not differentiable.",
                    "label": 0
                },
                {
                    "sent": "There are major zero.",
                    "label": 0
                },
                {
                    "sent": "That's right, yes?",
                    "label": 0
                },
                {
                    "sent": "I can answer this question for you.",
                    "label": 0
                },
                {
                    "sent": "He he will watch the video.",
                    "label": 0
                },
                {
                    "sent": "Open I forget.",
                    "label": 0
                },
                {
                    "sent": "So that's not a big name.",
                    "label": 0
                },
                {
                    "sent": "Then he didn't know the first part of this talk.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so you notice that.",
                    "label": 0
                },
                {
                    "sent": "This is the new ten direction in.",
                    "label": 0
                },
                {
                    "sent": "Then similar to nonlinear country gradient, you should do line search for less.",
                    "label": 1
                },
                {
                    "sent": "Well, that's right.",
                    "label": 0
                },
                {
                    "sent": "So what's your advantage of using Newton direction here?",
                    "label": 0
                },
                {
                    "sent": "If you were in my area talk, so remember that I mentioned that for linear SVM there I separate 23 situations.",
                    "label": 0
                },
                {
                    "sent": "Not if the number of data is much larger than number of features, or for other situations, but right now for this competition let's this case.",
                    "label": 0
                },
                {
                    "sent": "Except one problem which is a document set for others.",
                    "label": 0
                },
                {
                    "sent": "The number of data is much larger than the number of features.",
                    "label": 0
                },
                {
                    "sent": "So what so?",
                    "label": 0
                },
                {
                    "sent": "So the trick here is you can do this inverse even if OK, remember that.",
                    "label": 0
                },
                {
                    "sent": "So use W the size of W is the same as the number of features.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So so now we minimize the primal.",
                    "label": 0
                },
                {
                    "sent": "We can use some.",
                    "label": 0
                },
                {
                    "sent": "So this H is actually a square matrix of D * D. Or if these are the number of features.",
                    "label": 0
                },
                {
                    "sent": "So, so you need a D CU to invert location.",
                    "label": 0
                },
                {
                    "sent": "That's right, but that's not the main computational bottleneck.",
                    "label": 0
                },
                {
                    "sent": "The computational bottleneck is.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "There's a place of doing X transpose X.",
                    "label": 0
                },
                {
                    "sent": "So in that place, the computational cost is actually an times the square will not.",
                    "label": 0
                },
                {
                    "sent": "If you're not familiar with this and don't worry about what SV means, but it is something related to the number of features.",
                    "label": 0
                },
                {
                    "sent": "Number of data.",
                    "label": 0
                },
                {
                    "sent": "So this term is related to a number of data.",
                    "label": 0
                },
                {
                    "sent": "Then times the square loss remain computational costs of of getting these.",
                    "label": 0
                },
                {
                    "sent": "This Haitian matrix.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so they already mentioned here that this method is not feasible if your number of features is large, because then.",
                    "label": 0
                },
                {
                    "sent": "If these larger than, then doing this is basically impossible, but that's not the case for the competition.",
                    "label": 0
                },
                {
                    "sent": "Let's see if this happens because at least there is 1 document data set where they cannot invert.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Patient so in that case they use conjugate linear conjugate gradient to solve the Newton system.",
                    "label": 0
                },
                {
                    "sent": "Well, if you're not familiar with.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can't you agree this is different from nonlinear country gradient where you can consider this just an optimization method to directly minimize the object?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Function.",
                    "label": 0
                },
                {
                    "sent": "But here they still want to use Newton method OK essentially to get this direction is to solve a linear system.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "They do that by by by regular country ingredient to solve a linear system.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So this is roughly there, the Newton method.",
                    "label": 0
                },
                {
                    "sent": "No, this is a country gradient.",
                    "label": 1
                },
                {
                    "sent": "OK, so we see two level 2 levels of the loop, so the 1st loop is the Newton method.",
                    "label": 0
                },
                {
                    "sent": "So suppose here we need to get a new.",
                    "label": 0
                },
                {
                    "sent": "Get a new ten direction.",
                    "label": 0
                },
                {
                    "sent": "So I think this is the country gradient procedure too.",
                    "label": 0
                },
                {
                    "sent": "So for Congre Dion, how do you need is to?",
                    "label": 0
                },
                {
                    "sent": "I think is to do a matrix vector multiplication, so you need to do.",
                    "label": 1
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "No, let me think.",
                    "label": 0
                },
                {
                    "sent": "So now you don't get the.",
                    "label": 0
                },
                {
                    "sent": "You cannot do inverse.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The trick is.",
                    "label": 0
                },
                {
                    "sent": "This edge can be written as the product of two vectors as X transpose X.",
                    "label": 0
                },
                {
                    "sent": "So if you do conjugate gradient.",
                    "label": 0
                },
                {
                    "sent": "All you need is the Hessian vector inner product.",
                    "label": 0
                },
                {
                    "sent": "But instead of formulating the whole Haitian.",
                    "label": 0
                },
                {
                    "sent": "You multiply the vector with X first, then we may be X transpose, so you can do a Haitian vector product without formulating.",
                    "label": 0
                },
                {
                    "sent": "Without storing the.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Cohesion.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, Lily, Lady mentioned that each iteration is a matrix vector multiplication.",
                    "label": 1
                },
                {
                    "sent": "And this is good if the training data is sparse.",
                    "label": 1
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "It is similar to like right now.",
                    "label": 0
                },
                {
                    "sent": "What implemented in liblinear for solving the L2 SVM primal.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But for the full competition.",
                    "label": 0
                },
                {
                    "sent": "Lady, the only one Newton step.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what does this mean?",
                    "label": 0
                },
                {
                    "sent": "Usually this Newton direction may not be.",
                    "label": 0
                },
                {
                    "sent": "OK, may not give you a lower function value.",
                    "label": 0
                },
                {
                    "sent": "OK, because you may the direction may be too long or something, so you want to do a line search, but I don't think for the competition late.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Lady Linesearch let's just do.",
                    "label": 0
                },
                {
                    "sent": "They just do 1 Newton step and go to the next Newton iteration.",
                    "label": 1
                },
                {
                    "sent": "I mean, if in general it converges then that is OK. And as I have said earlier, since the number of dimensions, I mean the number of features is rather small for this competition in general, they do invert location.",
                    "label": 1
                },
                {
                    "sent": "So they have a MATLAB implementation because all those operations can be easily implemented in Matlab.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Len.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So now for this one Newton step, you need to calculate the hashing first, then do the in the inversion.",
                    "label": 0
                },
                {
                    "sent": "And I said earlier that the inversion is not the main bottleneck, is actually an UN getting the Hessian matrix.",
                    "label": 0
                },
                {
                    "sent": "That takes ND squared, right?",
                    "label": 0
                },
                {
                    "sent": "So that's the main main computational task, so they try to compare.",
                    "label": 0
                },
                {
                    "sent": "Like should we use?",
                    "label": 0
                },
                {
                    "sent": "Should we inverse the Hessian matrix or should we use linear conjugate gradient method to solve the linear system?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So they mentioned two kinds of truncated Newton implementations here.",
                    "label": 0
                },
                {
                    "sent": "One is that to get H and then then to calculate inverse.",
                    "label": 0
                },
                {
                    "sent": "But another is by.",
                    "label": 0
                },
                {
                    "sent": "Linear conjugate gradient to get the new ten direction, so they try to have a comparison.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Oh no, that's not what they did.",
                    "label": 0
                },
                {
                    "sent": "Sorry, we listen on my slides, but any late ladies.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At least they compare this one with this nonlinear conjugate gradient.",
                    "label": 0
                },
                {
                    "sent": "OK, that's what they they did, but this one is similar.",
                    "label": 0
                },
                {
                    "sent": "Users we have said this again involves matrix vector products.",
                    "label": 0
                },
                {
                    "sent": "The major computational task here is about matrix vector products.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, so for one approach the truncating the Newton.",
                    "label": 0
                },
                {
                    "sent": "They get hash and then do inverse and this is the main computational cost.",
                    "label": 0
                },
                {
                    "sent": "And therefore nonlinear conjugate gradient.",
                    "label": 0
                },
                {
                    "sent": "Lee they try to take K steps of nonlinear conjugate gradient with K less than the number of features.",
                    "label": 1
                },
                {
                    "sent": "So it's like they do a fixed number of nonlinear conjugate gradient steps.",
                    "label": 0
                },
                {
                    "sent": "And we said earlier that for country gradient domain costs on matrix vector product.",
                    "label": 0
                },
                {
                    "sent": "So let's order ND.",
                    "label": 0
                },
                {
                    "sent": "And because you do K steps, so the total complex complexities KN times D. So if you compare this with this order and this square.",
                    "label": 0
                },
                {
                    "sent": "Len because K is less, they choose a case smaller than D. So older candy should be smaller, right?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So you would think that this one may be faster.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "However they.",
                    "label": 0
                },
                {
                    "sent": "OK, let's hear, but they find out that this one is actually slower.",
                    "label": 0
                },
                {
                    "sent": "OK, so that means if you do, if you get Haitian link to inverse, they find out that it's fast and they think the reason is because that matrix matrix multiplications can be better optimized using the computer architecture, taking the advantage of computer architectures.",
                    "label": 0
                },
                {
                    "sent": "Exactly what they're saying is using using Matlab and internally MATLAB involves optimized basic linear algebra subroutines for matrix matrix operations, so that's actually very related to the.",
                    "label": 0
                },
                {
                    "sent": "Which talk about this talk they mention bless, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so when only we are used.",
                    "label": 0
                },
                {
                    "sent": "Olivia used matlab.",
                    "label": 0
                },
                {
                    "sent": "Essentially he used optimized place.",
                    "label": 0
                },
                {
                    "sent": "To do that?",
                    "label": 0
                },
                {
                    "sent": "To do the job.",
                    "label": 0
                },
                {
                    "sent": "So that is very effective in this situation.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, because I don't know if you are familiar with optimized place, but when you do matrix matrix operations use, you split each matrix to several blocks so you can better push blocks into the higher level of the computer architecture.",
                    "label": 0
                },
                {
                    "sent": "So then the CPU will be busy on doing your operations.",
                    "label": 0
                },
                {
                    "sent": "Then they mentioned that similar observations was made by you shall bendure last year.",
                    "label": 0
                },
                {
                    "sent": "Well, I was not there so I didn't.",
                    "label": 0
                },
                {
                    "sent": "I was not aware of this but but I think this kind of things have been known for awhile, and in fact for linear SVM, if you go to check Max Arians Paper will Max.",
                    "label": 0
                },
                {
                    "sent": "Aaron is A is an optimization researcher in University of Wisconsin.",
                    "label": 0
                },
                {
                    "sent": "So I think in a few years ago he did a lot of.",
                    "label": 0
                },
                {
                    "sent": "Optimization methods for suitable for linear SVM and he if you look at his papers he used a lot of Matlab and basically just take this advantage.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Len Solar Flare first method.",
                    "label": 0
                },
                {
                    "sent": "the Newton Newton stuff.",
                    "label": 0
                },
                {
                    "sent": "Then they have the second method, so Olivia in the currency they have the second method.",
                    "label": 0
                },
                {
                    "sent": "But this is essentially liblinear for.",
                    "label": 0
                },
                {
                    "sent": "44L2 SVA.",
                    "label": 0
                },
                {
                    "sent": "Are so different from my students talk this morning is that my students only work on the linear trick, but they use this kind of technique for the wild trick.",
                    "label": 0
                },
                {
                    "sent": "Willie made a mistake here for L2, SV and S -- S one not minus 3 -- 3 is for everyone I think.",
                    "label": 0
                },
                {
                    "sent": "Because here they laid well 2.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so we have said a few times what this this method is just to do a coordinate dissent, you sequentially pick a deal variable and two.",
                    "label": 0
                },
                {
                    "sent": "Minimize with respect to that variable.",
                    "label": 0
                },
                {
                    "sent": "They do shrinking as well because shrinking is very effective.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And how about stopping conditions?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So the this dual coding and decent procedure is stopped when one of the following conditions occurs.",
                    "label": 1
                },
                {
                    "sent": "So either the deal optimality violation is wasting point what.",
                    "label": 0
                },
                {
                    "sent": "I think for the competition the requirement is relative.",
                    "label": 1
                },
                {
                    "sent": "Primal Dual Gap is less than .01, so it's not clear whether this is a more strict or more strict one.",
                    "label": 0
                },
                {
                    "sent": "Well, I don't know, and another thing is, if the total number of iterations.",
                    "label": 0
                },
                {
                    "sent": "Is is more than 20, then the procedure stops.",
                    "label": 0
                },
                {
                    "sent": "So here by iteration I don't mean the minimization of 1 variable is.",
                    "label": 0
                },
                {
                    "sent": "Then that means you only minimize with respect to 20 components.",
                    "label": 0
                },
                {
                    "sent": "I mean 20 outer iteration.",
                    "label": 0
                },
                {
                    "sent": "So each outer iteration you actually.",
                    "label": 1
                },
                {
                    "sent": "Go through all the.",
                    "label": 0
                },
                {
                    "sent": "Older deal variables.",
                    "label": 1
                },
                {
                    "sent": "Because they work on a wild trick.",
                    "label": 0
                },
                {
                    "sent": "So after the competition they realize that.",
                    "label": 0
                },
                {
                    "sent": "If they stop based on the testing, the testing accuracy, let's actually in this competition.",
                    "label": 0
                },
                {
                    "sent": "This is this some kind of area under the curve is used, so if they use this then this is actually a more effective one.",
                    "label": 0
                },
                {
                    "sent": "Because they work down there while the trick, so they need to do parameter selection in order to get good generalization performance.",
                    "label": 1
                },
                {
                    "sent": "So to do parameter selection they they select a subset of 100,000 training samples.",
                    "label": 0
                },
                {
                    "sent": "Len latitude of parameter C tilt.",
                    "label": 0
                },
                {
                    "sent": "And you see, what's the reason of using CC TLD?",
                    "label": 0
                },
                {
                    "sent": "In doing parameter selection you if you use a subset then you select the parameter, then you worry about that when applying.",
                    "label": 0
                },
                {
                    "sent": "Well when you go to train the whole training set whether the same selected parameters is suitable.",
                    "label": 0
                },
                {
                    "sent": "They don't think so.",
                    "label": 0
                },
                {
                    "sent": "They need to do some adjustments due to the size difference between the whole set and the subset.",
                    "label": 0
                },
                {
                    "sent": "So this is hot lady.",
                    "label": 0
                },
                {
                    "sent": "If C Theater is the best C parameter of training a subset.",
                    "label": 0
                },
                {
                    "sent": "After finding it then they multiply it with square root of big and divided by small N. So N is the size of the full training set and N is the 100,000 here.",
                    "label": 1
                },
                {
                    "sent": "Yeah, so that's the way.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How they select the C for their final training?",
                    "label": 0
                },
                {
                    "sent": "OK, so finally.",
                    "label": 0
                },
                {
                    "sent": "So they have.",
                    "label": 0
                },
                {
                    "sent": "There are some discussions here, so long as people have observed that the main bottleneck of training linear classifiers is actually on reading the data.",
                    "label": 1
                },
                {
                    "sent": "Actually we all suffer from that and it takes a lot of time to read the data, say more than 90%.",
                    "label": 0
                },
                {
                    "sent": "So they are suggesting that if for future challenges, then if we consider datasets.",
                    "label": 0
                },
                {
                    "sent": "Which do not fit into memory, then we should also include the IO time in the evaluation.",
                    "label": 1
                },
                {
                    "sent": "Yeah, well, but I don't know how to do that in practice, but everybody has different different architectures, different machines so.",
                    "label": 0
                },
                {
                    "sent": "But even if your data cannot fit into memory, then of course you need to consider your algorithm.",
                    "label": 0
                },
                {
                    "sent": "You need.",
                    "label": 0
                },
                {
                    "sent": "You need to have a good algorithm which try to minimize the number of passes on the data because you don't want to read your data many times that disk IO is very expensive.",
                    "label": 1
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What this sentence is?",
                    "label": 0
                },
                {
                    "sent": "Everybody wanted that robust method should be fully automated.",
                    "label": 1
                },
                {
                    "sent": "Yeah, so so they are suggesting that the time the total time used for model selection and all other things should be included in the evaluation.",
                    "label": 0
                },
                {
                    "sent": "In more extremely, they're they're thinking that for participants they should not see datasets.",
                    "label": 0
                },
                {
                    "sent": "OK, so they think ideally for participants they should just submit the layer code so they don't see the data or but of course then hopefully the submitted code has an automatic way to do blah blah blah until together good testing accuracy value then that may be a better way to do a fair comparison.",
                    "label": 0
                },
                {
                    "sent": "So hopefully we can do that in the future, so thank you very much.",
                    "label": 0
                },
                {
                    "sent": "But I am not old.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, so you are welcome to ask me something.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, so Olivia will be very heavy.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So anymore comments.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Any idea why?",
                    "label": 0
                },
                {
                    "sent": "Page.",
                    "label": 0
                },
                {
                    "sent": "Better than online.",
                    "label": 0
                },
                {
                    "sent": "Well, I think I have explained.",
                    "label": 0
                },
                {
                    "sent": "OK, so I didn't do a good job.",
                    "label": 0
                },
                {
                    "sent": "Essentially, that goes back to those situations.",
                    "label": 0
                },
                {
                    "sent": "Up so earlier when I talk about this situation, so right now we all use some kind of system case, the gradient decent or dual coordinate dissent.",
                    "label": 0
                },
                {
                    "sent": "All those things for such data.",
                    "label": 0
                },
                {
                    "sent": "So this one is.",
                    "label": 0
                },
                {
                    "sent": "But now we're talking about this situation.",
                    "label": 0
                },
                {
                    "sent": "Indeed, for those situations you can take a take the advantage of the something easy small.",
                    "label": 0
                },
                {
                    "sent": "So buy something small.",
                    "label": 0
                },
                {
                    "sent": "That means you have a rectangular data matrix.",
                    "label": 0
                },
                {
                    "sent": "So if you have a rectangular matrix, so usually in a situation you can store large matrix data, that's actually the data, and by doing suitable matrix matrix operation then you get a very small square matrix, so we use less square matrix together your direction.",
                    "label": 0
                },
                {
                    "sent": "So so that's very efficient.",
                    "label": 0
                },
                {
                    "sent": "So for this kind of situation you should use not only.",
                    "label": 0
                },
                {
                    "sent": "So here I mentioned you use Primal based romances, but not only should use matrix based method, that's very effective.",
                    "label": 0
                },
                {
                    "sent": "So if so, I mentioned some materials paper.",
                    "label": 0
                },
                {
                    "sent": "If you go go back to look at some of his earlier papers or such methods are such such things.",
                    "label": 0
                },
                {
                    "sent": "So that's I think that's the reason you want to take this adventure.",
                    "label": 0
                },
                {
                    "sent": "Something is small and you have two regular rectangular matrix.",
                    "label": 0
                },
                {
                    "sent": "Then you multiply them.",
                    "label": 0
                },
                {
                    "sent": "You get a small matrix and just use the small matrix to get your direction.",
                    "label": 0
                },
                {
                    "sent": "Lastly well, I don't know if Olivia think I answered that correctly.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                }
            ]
        }
    }
}