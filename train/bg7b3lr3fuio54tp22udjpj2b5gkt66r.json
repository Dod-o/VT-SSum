{
    "id": "bg7b3lr3fuio54tp22udjpj2b5gkt66r",
    "title": "Online-Batch Strongly Convex Multi Kernel Learning",
    "info": {
        "author": [
            "Francesco Orabona, Toyota Technological Institute at Chicago"
        ],
        "published": "July 19, 2010",
        "recorded": "June 2010",
        "category": [
            "Top->Computer Science->Computer Vision->Sparsity & Convex Optimisation"
        ]
    },
    "url": "http://videolectures.net/cvpr2010_orabona_obsc/",
    "segmentation": [
        [
            "So good afternoon everybody.",
            "I am Francesco bona.",
            "This is a joint work with the Loggia Anne Barbara Caputo."
        ],
        [
            "This is an outline of the talk."
        ],
        [
            "And I'll start introducing you.",
            "The problem of multikernel learning.",
            "And this notation."
        ],
        [
            "So let's introduce formally the problem we have N training samples.",
            "Excited why I wear X belongs to some vectorial space and Y is the label between one of the EM labels and we have F different kernels corresponding to different features.",
            "For example, color, shape and what we want to do is to learn as core function as.",
            "That will assign a score to each label and then we predict with the label with the maximum score and we want to use all the kernels."
        ],
        [
            "To learn this score function.",
            "We will also assume that the score function will be basically the sum of F different score function, one for each kernel.",
            "And we define we use the standard linear classification framework.",
            "So we define joint feature Maps fijai on that and labels.",
            "So in this way the score function will be defined simply as another inner product between an hyperplane WJ and the map Fiji.",
            "The last definition that we need is this double bar that is simple.",
            "The stacking of all the.",
            "I I per plane in a long vector and a similar notation will for fee so that we have that the score function is simply defined as the inner product between W bar and free bar."
        ],
        [
            "Now, in principled approach to use multiple kernels in a classification algorithm, is the multikernel learning framework in which we minimize an objective function that is composed by a regularization term weighted by parameter Lambda plus a term that measures the loss over the training samples and the regularization that has been used the widely in a multikernel learning is the Group One group Norm 21.",
            "That basically consistent taking all the hyperplane, calculating separately they all two norm of each one.",
            "They're putting them into a vector and then calculating the one norm of this vector.",
            "Now this regularization is interesting because it induces sparsity into the into the domain of kernels.",
            "So it means that the solution will be expressed just by a subset of the kernels.",
            "And but on the other hand, this problem is particularly difficult to be optimized.",
            "In fact, all the proposed algorithm use an alternating optimization strategy that uses the dual formulation now."
        ],
        [
            "Let's focus on these two things, the sparsity and the dual formulation."
        ],
        [
            "Are we sure that we need sparsity?",
            "Actually, are we sure that we we want to throw away some of the kernels that we give to the algorithm?",
            "Because we have to take into account the fact that most of the time these kernels were the results of years of over search, sometimes even an entire PhD spent to the signing kernel and we just throw it away with the algorithm.",
            "The other thing."
        ],
        [
            "Is that?",
            "Are we sure that we want to use the dual?",
            "So because historically, the dual formulation of SVM has been introduced to have an easier optimization problem and to use kernels.",
            "But it turns out that."
        ],
        [
            "We don't need to know the dual for neither of these two things in fact uses stochastic subgradient descent algorithm directly on the primal.",
            "We can have a faster convergence to the optimal solution, and we can use kernels too."
        ],
        [
            "And Moreover, with this kind of algorithms, we can use any kind of loss, while for dual problems we have to design A loss that is nice enough to have a nice dual formulation.",
            "And this kind of algorithms are computationally efficient for large data sets.",
            "And if the objective function is strongly convex, we can also prove fast convergence rates bound to the optimal solution, and this is not possible in general.",
            "Will alternating optimization mean?"
        ],
        [
            "Words.",
            "However, the group Norm to one that is using multi kernel learning is not strongly convex."
        ],
        [
            "So I will now introduce Lou, a different multikernel formulation and an algorithm."
        ],
        [
            "Could solve it.",
            "What we propose to do is to change the regularization term.",
            "In particular, instead of using the group norm to one, we propose to use the group norm 2P.",
            "So again, we take the hyperplane.",
            "We take the L2 norms of each hyperplane.",
            "We stuck it in into a vector and we calculate the P norm of this vector.",
            "Now of course, if speed is equal to 1, we recovered the previous formulation.",
            "That is sparse, if P equal is equal to two, it's easy to see that this formulation is equivalent to just using the sum of the kernel.",
            "So learning with just one kernel.",
            "And this formulation is nice.",
            "Becausw tuning P we can choose the level of sparsity of our solution and another thing is that for any P between one and two and P different from one, this formulation is strongly convex, even if the strongly convexity is decreasing when P tends to one.",
            "And this will bring to the fact that the problem will be harder to optimize because we return to the other problem."
        ],
        [
            "So we need to introduce another trick.",
            "Suppose that you want to minimize the generic convex function like the one in the figure in the domain minus one and one, and someone tell us that the solution is actually living in a smaller bowl."
        ],
        [
            "Centering around your region.",
            "Something like this.",
            "So it turns out that this problem should be easier to optimize, and in fact we can use approximate regularization methods to include this kind of information inside the algorithm."
        ],
        [
            "Now we need to estimate the Boulevard solution leaves.",
            "And the."
        ],
        [
            "Solution that we propose is to use a fast online algorithm."
        ],
        [
            "So putting all together, this is the algorithm we obtain that we call obskure that is online batch strongly convex multikernel learning, so we have to stage first.",
            "One is a quick online.",
            "2P multikernel learning algorithm that we we can stop at anytime and we obtain an estimate of the radius of the ball where the solution leaves.",
            "Then we start at the second stage.",
            "That is stochastic gradient descent algorithm.",
            "That use the solution find by the previous stage as a starting point and uses the information of the radius of the."
        ],
        [
            "Full now before going into the details of the algorithm, I want to show you the kind of the kind of converter kind of guarantee on the convergence rate that we can prove.",
            "So for any P between one and two and four Q defined as P / P -- 1, whoever that after T iteration of the second stage in expectation the gap from the optimal solution is.",
            "This quantity is the order of magnitude of this quantity.",
            "And here you see that there is a minimum of these two terms.",
            "The first one is Q divided by Lambda T. That is the kind of regret that you expect every time you minimize the strongly convex function.",
            "And the second one is the kind of regret that you expect when you minimize a generic convex function.",
            "Now this one is a better dependency on T, but actually this term can be bigger because when P tends to one, Q can be quite big and Lambda.",
            "That is, the regularization parameter can be also quite small.",
            "So even if this is a better dependency on T, actually this other term can be can be better and is also independent on Lambda.",
            "And there's a better dependency on Q, so the algorithm automatically will will have a convergence rate that will interpolate between these two to have always the optimal convergence."
        ],
        [
            "Great.",
            "Moreover, we can prove that if the problem is linearly separable, that is always the case in multi kernel learning because you use a lot of kernels, the first stage will stop after a finite number of updates and R will overestimate the radius of the ball at at most by a factor of 4."
        ],
        [
            "So let's let's see now how the algorithm is done, and this is just a general draft of the algorithm and the online stage and the stochastic gradients stage.",
            "They're both.",
            "They shared the same structure, so we take a sample at random and then the theoretical sells out to set these two parameter it A T and Alpha T and we just update this vector titabar with the with the subgradient of the loss weighted by.",
            "30 Plus the previous solution, weighted by Alpha T. And then we transform this into T into the hyperplane just with the scaling that depends on the norm of all the T 30.",
            "So the algorithm is extremely simple and easy to implement."
        ],
        [
            "And I will not go into details on how to choose this Alpha, T and 80, but all the details are in the paper, but if you're not interested in theory, you just want to try it.",
            "You can take the source code of our Matlab algorithm called Dogma.",
            "That means discriminative, aligned, good metal of algorithms that is explicitly designed to have easy to understand and easy to modify."
        ],
        [
            "Grits now let's see some results."
        ],
        [
            "We tested obscure on we compared obscure with SLP.",
            "That is a state of the art multi kernel learning solver with the LP beta that it's another way to combine different kernels and with the SVM just using the average of all the kernels and we tested it on Caltech 11 with 39 kernels.",
            "Are there data sets early in the paper?"
        ],
        [
            "This is the the useful graph of four Caltech with classification rate versus number of training samples, and you see that the red line of of the obscure with 39 kernels and with the best five kernels basically as the same performance of LP beta, that is the state of the art currently in combining kernels, and we are always better than the blue line that is multi kernel learning and of the green line that is just the average of the kernel."
        ],
        [
            "From the computational point of view, we have that if you use we will use just 5015 samples.",
            "The time is similar to LP, LP, Beta and SLP, but if we will use 30 samples, we have that we reach basically the final classification rate 7 to 10 times faster than LP, beta and ship and I want to remind you that our implementation is just MATLAB.",
            "While LPB tensile RC."
        ],
        [
            "Another graph is the is the behavior of the algorithm.",
            "Changing the parameter P. So we took just the best for kernels among the 39, and it's very likely that given that there are so few and all of them and they are the best one, it's likely that the sparse solution will not perform so well.",
            "In fact, this parcel sparsest one that is with P = 1.01.",
            "It's here while we have the best solution.",
            "Would be equal 1 one point 10 that is here.",
            "So we gain something like 2%.",
            "Three percent of accuracy.",
            "So it means that this actual parameter that we introduced is.",
            "It's also useful to improve the performance.",
            "And when you consider that with P equal to two, we have worse performance that correspond to just taking the average of the kernel."
        ],
        [
            "The last graph that I want to show you is the behavior of the algorithm when we add more kernels and we have this interesting behavior that adding more kernels, we have a faster convergence rate.",
            "So for example, if you take a given classification rate 72, we have that we reach it with less iteration using 39 kernels compared to using five kernels."
        ],
        [
            "And that's all we have to summarize.",
            "We've introduced a new formulation for multikernel learning problems and an algorithm to solve it that is composed by two stage, the 1st.",
            "It's an online one that quickly estimates the region where the solution leaves, and the second stage reached the solution with a guaranteed convergence rate using the information coming from the first stage.",
            "As a future work, we want to extend obskure to your archical losses.",
            "Given that we use.",
            "A primal formulation so we can use any kind of loss."
        ],
        [
            "So that's all.",
            "Thanks for your attention.",
            "Thanks for the nice talk.",
            "I have a question.",
            "So this parameter P you can treat unit in two ways, one for higher performance of course and one for getting the best convergence rate right there anyway that you could achieve goals.",
            "So could you advise the mechanism that can give us a P that is both optimal in terms of convergence rate and performance?",
            "Well to obtain the faster convergence rate you have to choose B proportional to the logarithm of the number of kernels.",
            "For the performance, well it's problem dependent because you you should know.",
            "You know that many of these kernels are useless.",
            "Probably you want to forward a sparse solution on the other end if you want.",
            "If you know that all of them are useful, you want to bias the solution versus a dense one.",
            "You cannot say this a priority.",
            "Let's thank the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So good afternoon everybody.",
                    "label": 0
                },
                {
                    "sent": "I am Francesco bona.",
                    "label": 0
                },
                {
                    "sent": "This is a joint work with the Loggia Anne Barbara Caputo.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is an outline of the talk.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I'll start introducing you.",
                    "label": 0
                },
                {
                    "sent": "The problem of multikernel learning.",
                    "label": 0
                },
                {
                    "sent": "And this notation.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's introduce formally the problem we have N training samples.",
                    "label": 1
                },
                {
                    "sent": "Excited why I wear X belongs to some vectorial space and Y is the label between one of the EM labels and we have F different kernels corresponding to different features.",
                    "label": 1
                },
                {
                    "sent": "For example, color, shape and what we want to do is to learn as core function as.",
                    "label": 0
                },
                {
                    "sent": "That will assign a score to each label and then we predict with the label with the maximum score and we want to use all the kernels.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To learn this score function.",
                    "label": 0
                },
                {
                    "sent": "We will also assume that the score function will be basically the sum of F different score function, one for each kernel.",
                    "label": 0
                },
                {
                    "sent": "And we define we use the standard linear classification framework.",
                    "label": 0
                },
                {
                    "sent": "So we define joint feature Maps fijai on that and labels.",
                    "label": 1
                },
                {
                    "sent": "So in this way the score function will be defined simply as another inner product between an hyperplane WJ and the map Fiji.",
                    "label": 0
                },
                {
                    "sent": "The last definition that we need is this double bar that is simple.",
                    "label": 0
                },
                {
                    "sent": "The stacking of all the.",
                    "label": 0
                },
                {
                    "sent": "I I per plane in a long vector and a similar notation will for fee so that we have that the score function is simply defined as the inner product between W bar and free bar.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, in principled approach to use multiple kernels in a classification algorithm, is the multikernel learning framework in which we minimize an objective function that is composed by a regularization term weighted by parameter Lambda plus a term that measures the loss over the training samples and the regularization that has been used the widely in a multikernel learning is the Group One group Norm 21.",
                    "label": 0
                },
                {
                    "sent": "That basically consistent taking all the hyperplane, calculating separately they all two norm of each one.",
                    "label": 0
                },
                {
                    "sent": "They're putting them into a vector and then calculating the one norm of this vector.",
                    "label": 0
                },
                {
                    "sent": "Now this regularization is interesting because it induces sparsity into the into the domain of kernels.",
                    "label": 1
                },
                {
                    "sent": "So it means that the solution will be expressed just by a subset of the kernels.",
                    "label": 0
                },
                {
                    "sent": "And but on the other hand, this problem is particularly difficult to be optimized.",
                    "label": 0
                },
                {
                    "sent": "In fact, all the proposed algorithm use an alternating optimization strategy that uses the dual formulation now.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's focus on these two things, the sparsity and the dual formulation.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Are we sure that we need sparsity?",
                    "label": 0
                },
                {
                    "sent": "Actually, are we sure that we we want to throw away some of the kernels that we give to the algorithm?",
                    "label": 1
                },
                {
                    "sent": "Because we have to take into account the fact that most of the time these kernels were the results of years of over search, sometimes even an entire PhD spent to the signing kernel and we just throw it away with the algorithm.",
                    "label": 0
                },
                {
                    "sent": "The other thing.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is that?",
                    "label": 0
                },
                {
                    "sent": "Are we sure that we want to use the dual?",
                    "label": 0
                },
                {
                    "sent": "So because historically, the dual formulation of SVM has been introduced to have an easier optimization problem and to use kernels.",
                    "label": 1
                },
                {
                    "sent": "But it turns out that.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We don't need to know the dual for neither of these two things in fact uses stochastic subgradient descent algorithm directly on the primal.",
                    "label": 0
                },
                {
                    "sent": "We can have a faster convergence to the optimal solution, and we can use kernels too.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And Moreover, with this kind of algorithms, we can use any kind of loss, while for dual problems we have to design A loss that is nice enough to have a nice dual formulation.",
                    "label": 0
                },
                {
                    "sent": "And this kind of algorithms are computationally efficient for large data sets.",
                    "label": 0
                },
                {
                    "sent": "And if the objective function is strongly convex, we can also prove fast convergence rates bound to the optimal solution, and this is not possible in general.",
                    "label": 1
                },
                {
                    "sent": "Will alternating optimization mean?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Words.",
                    "label": 0
                },
                {
                    "sent": "However, the group Norm to one that is using multi kernel learning is not strongly convex.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I will now introduce Lou, a different multikernel formulation and an algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Could solve it.",
                    "label": 0
                },
                {
                    "sent": "What we propose to do is to change the regularization term.",
                    "label": 0
                },
                {
                    "sent": "In particular, instead of using the group norm to one, we propose to use the group norm 2P.",
                    "label": 1
                },
                {
                    "sent": "So again, we take the hyperplane.",
                    "label": 0
                },
                {
                    "sent": "We take the L2 norms of each hyperplane.",
                    "label": 0
                },
                {
                    "sent": "We stuck it in into a vector and we calculate the P norm of this vector.",
                    "label": 0
                },
                {
                    "sent": "Now of course, if speed is equal to 1, we recovered the previous formulation.",
                    "label": 0
                },
                {
                    "sent": "That is sparse, if P equal is equal to two, it's easy to see that this formulation is equivalent to just using the sum of the kernel.",
                    "label": 1
                },
                {
                    "sent": "So learning with just one kernel.",
                    "label": 0
                },
                {
                    "sent": "And this formulation is nice.",
                    "label": 0
                },
                {
                    "sent": "Becausw tuning P we can choose the level of sparsity of our solution and another thing is that for any P between one and two and P different from one, this formulation is strongly convex, even if the strongly convexity is decreasing when P tends to one.",
                    "label": 0
                },
                {
                    "sent": "And this will bring to the fact that the problem will be harder to optimize because we return to the other problem.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we need to introduce another trick.",
                    "label": 0
                },
                {
                    "sent": "Suppose that you want to minimize the generic convex function like the one in the figure in the domain minus one and one, and someone tell us that the solution is actually living in a smaller bowl.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Centering around your region.",
                    "label": 0
                },
                {
                    "sent": "Something like this.",
                    "label": 0
                },
                {
                    "sent": "So it turns out that this problem should be easier to optimize, and in fact we can use approximate regularization methods to include this kind of information inside the algorithm.",
                    "label": 1
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we need to estimate the Boulevard solution leaves.",
                    "label": 0
                },
                {
                    "sent": "And the.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Solution that we propose is to use a fast online algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So putting all together, this is the algorithm we obtain that we call obskure that is online batch strongly convex multikernel learning, so we have to stage first.",
                    "label": 0
                },
                {
                    "sent": "One is a quick online.",
                    "label": 1
                },
                {
                    "sent": "2P multikernel learning algorithm that we we can stop at anytime and we obtain an estimate of the radius of the ball where the solution leaves.",
                    "label": 1
                },
                {
                    "sent": "Then we start at the second stage.",
                    "label": 1
                },
                {
                    "sent": "That is stochastic gradient descent algorithm.",
                    "label": 1
                },
                {
                    "sent": "That use the solution find by the previous stage as a starting point and uses the information of the radius of the.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Full now before going into the details of the algorithm, I want to show you the kind of the kind of converter kind of guarantee on the convergence rate that we can prove.",
                    "label": 0
                },
                {
                    "sent": "So for any P between one and two and four Q defined as P / P -- 1, whoever that after T iteration of the second stage in expectation the gap from the optimal solution is.",
                    "label": 1
                },
                {
                    "sent": "This quantity is the order of magnitude of this quantity.",
                    "label": 0
                },
                {
                    "sent": "And here you see that there is a minimum of these two terms.",
                    "label": 0
                },
                {
                    "sent": "The first one is Q divided by Lambda T. That is the kind of regret that you expect every time you minimize the strongly convex function.",
                    "label": 0
                },
                {
                    "sent": "And the second one is the kind of regret that you expect when you minimize a generic convex function.",
                    "label": 0
                },
                {
                    "sent": "Now this one is a better dependency on T, but actually this term can be bigger because when P tends to one, Q can be quite big and Lambda.",
                    "label": 0
                },
                {
                    "sent": "That is, the regularization parameter can be also quite small.",
                    "label": 0
                },
                {
                    "sent": "So even if this is a better dependency on T, actually this other term can be can be better and is also independent on Lambda.",
                    "label": 0
                },
                {
                    "sent": "And there's a better dependency on Q, so the algorithm automatically will will have a convergence rate that will interpolate between these two to have always the optimal convergence.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Great.",
                    "label": 0
                },
                {
                    "sent": "Moreover, we can prove that if the problem is linearly separable, that is always the case in multi kernel learning because you use a lot of kernels, the first stage will stop after a finite number of updates and R will overestimate the radius of the ball at at most by a factor of 4.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's let's see now how the algorithm is done, and this is just a general draft of the algorithm and the online stage and the stochastic gradients stage.",
                    "label": 0
                },
                {
                    "sent": "They're both.",
                    "label": 0
                },
                {
                    "sent": "They shared the same structure, so we take a sample at random and then the theoretical sells out to set these two parameter it A T and Alpha T and we just update this vector titabar with the with the subgradient of the loss weighted by.",
                    "label": 1
                },
                {
                    "sent": "30 Plus the previous solution, weighted by Alpha T. And then we transform this into T into the hyperplane just with the scaling that depends on the norm of all the T 30.",
                    "label": 0
                },
                {
                    "sent": "So the algorithm is extremely simple and easy to implement.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I will not go into details on how to choose this Alpha, T and 80, but all the details are in the paper, but if you're not interested in theory, you just want to try it.",
                    "label": 1
                },
                {
                    "sent": "You can take the source code of our Matlab algorithm called Dogma.",
                    "label": 1
                },
                {
                    "sent": "That means discriminative, aligned, good metal of algorithms that is explicitly designed to have easy to understand and easy to modify.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Grits now let's see some results.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We tested obscure on we compared obscure with SLP.",
                    "label": 1
                },
                {
                    "sent": "That is a state of the art multi kernel learning solver with the LP beta that it's another way to combine different kernels and with the SVM just using the average of all the kernels and we tested it on Caltech 11 with 39 kernels.",
                    "label": 1
                },
                {
                    "sent": "Are there data sets early in the paper?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the the useful graph of four Caltech with classification rate versus number of training samples, and you see that the red line of of the obscure with 39 kernels and with the best five kernels basically as the same performance of LP beta, that is the state of the art currently in combining kernels, and we are always better than the blue line that is multi kernel learning and of the green line that is just the average of the kernel.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "From the computational point of view, we have that if you use we will use just 5015 samples.",
                    "label": 0
                },
                {
                    "sent": "The time is similar to LP, LP, Beta and SLP, but if we will use 30 samples, we have that we reach basically the final classification rate 7 to 10 times faster than LP, beta and ship and I want to remind you that our implementation is just MATLAB.",
                    "label": 1
                },
                {
                    "sent": "While LPB tensile RC.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another graph is the is the behavior of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "Changing the parameter P. So we took just the best for kernels among the 39, and it's very likely that given that there are so few and all of them and they are the best one, it's likely that the sparse solution will not perform so well.",
                    "label": 1
                },
                {
                    "sent": "In fact, this parcel sparsest one that is with P = 1.01.",
                    "label": 0
                },
                {
                    "sent": "It's here while we have the best solution.",
                    "label": 0
                },
                {
                    "sent": "Would be equal 1 one point 10 that is here.",
                    "label": 0
                },
                {
                    "sent": "So we gain something like 2%.",
                    "label": 0
                },
                {
                    "sent": "Three percent of accuracy.",
                    "label": 0
                },
                {
                    "sent": "So it means that this actual parameter that we introduced is.",
                    "label": 0
                },
                {
                    "sent": "It's also useful to improve the performance.",
                    "label": 0
                },
                {
                    "sent": "And when you consider that with P equal to two, we have worse performance that correspond to just taking the average of the kernel.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The last graph that I want to show you is the behavior of the algorithm when we add more kernels and we have this interesting behavior that adding more kernels, we have a faster convergence rate.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you take a given classification rate 72, we have that we reach it with less iteration using 39 kernels compared to using five kernels.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And that's all we have to summarize.",
                    "label": 0
                },
                {
                    "sent": "We've introduced a new formulation for multikernel learning problems and an algorithm to solve it that is composed by two stage, the 1st.",
                    "label": 1
                },
                {
                    "sent": "It's an online one that quickly estimates the region where the solution leaves, and the second stage reached the solution with a guaranteed convergence rate using the information coming from the first stage.",
                    "label": 1
                },
                {
                    "sent": "As a future work, we want to extend obskure to your archical losses.",
                    "label": 0
                },
                {
                    "sent": "Given that we use.",
                    "label": 0
                },
                {
                    "sent": "A primal formulation so we can use any kind of loss.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's all.",
                    "label": 0
                },
                {
                    "sent": "Thanks for your attention.",
                    "label": 0
                },
                {
                    "sent": "Thanks for the nice talk.",
                    "label": 1
                },
                {
                    "sent": "I have a question.",
                    "label": 0
                },
                {
                    "sent": "So this parameter P you can treat unit in two ways, one for higher performance of course and one for getting the best convergence rate right there anyway that you could achieve goals.",
                    "label": 0
                },
                {
                    "sent": "So could you advise the mechanism that can give us a P that is both optimal in terms of convergence rate and performance?",
                    "label": 0
                },
                {
                    "sent": "Well to obtain the faster convergence rate you have to choose B proportional to the logarithm of the number of kernels.",
                    "label": 0
                },
                {
                    "sent": "For the performance, well it's problem dependent because you you should know.",
                    "label": 0
                },
                {
                    "sent": "You know that many of these kernels are useless.",
                    "label": 0
                },
                {
                    "sent": "Probably you want to forward a sparse solution on the other end if you want.",
                    "label": 0
                },
                {
                    "sent": "If you know that all of them are useful, you want to bias the solution versus a dense one.",
                    "label": 0
                },
                {
                    "sent": "You cannot say this a priority.",
                    "label": 0
                },
                {
                    "sent": "Let's thank the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}