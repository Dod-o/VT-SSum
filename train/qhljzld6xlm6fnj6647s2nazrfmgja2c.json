{
    "id": "qhljzld6xlm6fnj6647s2nazrfmgja2c",
    "title": "On the Complexity of Learning with Kernels",
    "info": {
        "author": [
            "Ohad Shamir, Faculty of Mathematics and Computer Science, Weizmann Institute of Science"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_shamir_complexity_learning/",
    "segmentation": [
        [
            "So a kernel learning is.",
            "Most of you probably know is this very nice technique to learn nonlinear predictors by mapping the data to a high dimensional feature space and learning a linear predictor there.",
            "So doing empirical risk minimization basically boils down to."
        ],
        [
            "Right here, where maybe you had some sort of regularization anaso."
        ],
        [
            "You know using the kernel trick and the Representer theorem, we can.",
            "Write this in equivalent way where now we minimize and with respect to alphas weights over the examples.",
            "Thanks.",
            "And this is a convex problem which we can solve in polynomial time."
        ],
        [
            "However, there is a problem in terms of scalability.",
            "So to solve this we still need to compute an handle an M by N matrix or M is the number of data points and that could be very expensive when M is large.",
            "As a result, there's been a huge literature on making kernel learning more efficient.",
            "I cannot possibly do justice to all of the works.",
            "Is there just a few, maybe representative examples but rough?"
        ],
        [
            "He's speaking, we can divide these methods or the methods that I'm familiar with is generally used one or both of the following techniques.",
            "So either we limit the number of entries of the kernel matrix that we compute, or the do some sort of low rank approximation of the matrix, either explicitly or implicitly.",
            "Now, despite this, a large body of literature, as far as we know, there's been almost no attempt to try and find lower bounds for this problem.",
            "So how can we?",
            "How well can we hope at all to perform using these kinds of approaches?",
            "So this is basically the subject of."
        ],
        [
            "Our paper.",
            "So the first model we're going to look at is when we try to speed up kernel learning by a, forcing ourselves to look at only a small part of the kernel matrix.",
            "So we are going to assume that the target values wise are a given, but we don't know the access nor the kernel matrix.",
            "The only way we can access this data is by a querying B entries in the Matrix, possibly in adaptive way, so how well?",
            "In this model, can we solve the ERM optimization problem?"
        ],
        [
            "And we're going to see some answers for this, but in general, one thing that we observed is that the answer depends not just in the kernel matrix, but also depends a lot on the kind of regularization you use and also the nature of the loss function.",
            "Also, it's important to realize that this is very different question then matrix approximation, so people have looked at this question that I have an unknown matrix.",
            "I can query a certain number of entries out of it, so how well can I approximate the matrix in Frobenius norm?",
            "Spectral norm or something like that?",
            "And this question is a different because here are interested in eventually solving an optimization problem."
        ],
        [
            "Just to give you an example, let's say consider doing kernel ERM using software colorization with respect to the linear loss.",
            "OK, so the loss L is just the product of the prediction times the target value Y.",
            "So how many entries of K do you think we need to observe if we want to solve this up to epsilon error, any ideas?",
            "Guesses.",
            "One over epsilon.",
            "One over epsilon squared.",
            "Well, actually know the true answer is 0.",
            "You do not need to look at the kernel matrix at all to solve this problem, and the reason is that what we need to return our weights over the data points and for the linear loss the values of these weights does not dependent on the kernel matrix.",
            "It just depends on the target values times some scaling.",
            "So again this demonstrates that this is not just the question of matrix approximation, but.",
            "It also depends on other properties of this problem."
        ],
        [
            "OK, so the kind of kernel matrices we use to construct are a lower bounds are basically block diagonal matrices with one inside each block and zero outside.",
            "In fact, our techniques are more general, but we focus on this for simplicity.",
            "By the way, these kinds of matrices are induced by linear kernels, or certain tests are polynomial kernel kernels approximately by Gaussian kernels.",
            "But basically this is the class, so we have at most D blocks of 1 along the diagonal and N by N matrices."
        ],
        [
            "So let's start with the first example where we want to do erm, with respect to the absolute loss, and without any kind of strong convexity, we're just constraining the normal for predictor in the kernel space to be squared norm at most 2.",
            "So one kind of theorem we can show is that given a budget constraint B, as long as it's much smaller than the total sum total kernel size, you won't be able to get an error which scales better than one over the 4th root of a B.",
            "And this is actually optimal because you can get this using the following very simple algorithm.",
            "Just throw away most of your data, just subsample square root of BA training examples, then your kernel matrix would be of size.",
            "Sqrt B * sqrt B.",
            "That's only be entries and do, erm, with respect to that.",
            "And this would be the kind of error you get.",
            "So in some sense this tells us that we won't be able to speed up kernel learning in any kind of nontrivial way, at least in this case.",
            "Please, without throwing away your data.",
            "Um?"
        ],
        [
            "And here is the general proof idea.",
            "So we start with the matrix like the kind of mentioned.",
            "The earlier where we have something like D blocks more or less of equal size.",
            "And then."
        ],
        [
            "For each block, we randomly choose to do one of two things.",
            "Either we leave it."
        ],
        [
            "The same way or we further divide it into a two smaller a blocks so you end up with the matrix, which is a miss mix of small blocks in large blocks and in the end."
        ],
        [
            "You randomly permute the rows and columns, basically permuting the order of your data OK."
        ],
        [
            "A now we pick the target values to be something like one over square root.",
            "The number of blocks.",
            "In this case we can show that there is a predictor in our domain, achieving 0 error.",
            "But to get any kind of error much smaller than 1 / sqrt D, it reduces to a estimating for each of these blocks what is its size?",
            "Is it a small block or large block?",
            "So this is just a two line calculation, so we end up with requiring.",
            "That the alphas for each block there's some would be a 1 / sqrt D and it's important that we won't undershoot or overshoot.",
            "However, because it's a block diagonal matrix, it's relatively sparse and we randomly permuted the rows and columns it's going to be quite difficult to even get any information on most of these blocks because just hitting all of these blocks is going to be difficult and.",
            "We show this formally and using that we eventually get our bound."
        ],
        [
            "A OK, so that's for one specific loss, but can we say something more general for other types of losses, so we have the theorem of that type when we do soft regularization, we just add the graduation term to the objective function."
        ],
        [
            "This is how the general theorem looks like in the next slide.",
            "I'm going to give specific corollary's, but roughly speaking, the lower bound that we get on the error depends on how non linear or non smooth is a our loss.",
            "So roughly speaking it depends on these differences, sorry.",
            "Difference of two times U 1 -- U two where AU one is when you try to minimize the loss with respect to some target value plus some regularization, you two is the same thing, but where we divide the regularization by two.",
            "So for linear loss this will just scale the use by a factor of two and you would get 0.",
            "But if you have any kind of non linearity non smoothness it makes this something which is larger than 0.",
            "EH, OK, so to give some."
        ],
        [
            "Simple corollary's, so for linear loss.",
            "As I said earlier, you get 0 and this is actually the right thing because we saw that we don't need to look at the kernel matrix to get a lower bound for the linear loss.",
            "For the absolute loss, we get something similar to the previous theorem, only now with respect to soft regularization.",
            "So again, it's a lower bound which can be achieved by the trivial algorithm where we throw away most of our data.",
            "But we can also get things for other losses, so a for squared loss which is nonlinear but smooth, we get a bound of the following form, which still tells us that, for instance, the number of queries we need to do scales like one over the strong convexity parameter is squared.",
            "So for instance, if it's too small.",
            "If it's something like one over the data size, which is a reasonable choice, in many situations you won't be able to speed up kernel learning in a.",
            "Meaningful way.",
            "Also, for the hinge loss, we can get things here.",
            "It's non smooth but sort of.",
            "The error is a wine cited, so we get something, but it's weaker than in the case of absolute loss, and again it still identifies the region where you cannot get in ever better than a constant."
        ],
        [
            "In OK, so one approach is to look at the budget constraints, but you can also look at the second kind of general approach where we approximate the kernel matrix by some low rank matrix.",
            "So specifically, we can think of an algorithm which does Colonel Iran, but with respect to any kind of low rank surrogates for the original matrix.",
            "By the way, it may not need to approximate it in any meaningful way.",
            "We can get a lower bound just by the fact that it's low rank and you get a lower bound on this form, which again tells you that a.",
            "It gives you some lower bound on the rank of the low rank approximation that you would need to get small error and again in the regime where Lambda is one over the data size, you won't be able to get in anything meaningful.",
            "OK."
        ],
        [
            "So to summarize a so the bad news are that there are cases of kernel learning which we can't speed up, at least not using the kind of methods that I talked about.",
            "And in general, the kind of computational effort does depend on the loss and also scales with your essentially condition number.",
            "The strong convexity for instance, and in general regression losses which are non smooth, such as the absolute loss.",
            "Appear to be especially difficult.",
            "I mean, our bound is for the absolute laws, but you can take just about any kind of nonsmooth regression loss, and you get the same kind of.",
            "A problem, so these are the bad news.",
            "The good news is that our lower bounds are not the same.",
            "They are actually weaker when we discuss a smooth losses or when you have a lot of strong convexity or 1 sided losses such as the hinge loss and of course the natural question is can we utilize these assumptions to design better algorithms so they have in some specific cases.",
            "So for instance a learning algorithm design for the hinge loss or squared loss but.",
            "Maybe we can get something which is a more general.",
            "Course the reverse could be that maybe our lower bounds are simply not tight enough and they can be further improved, so I'll stop."
        ],
        [
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So a kernel learning is.",
                    "label": 0
                },
                {
                    "sent": "Most of you probably know is this very nice technique to learn nonlinear predictors by mapping the data to a high dimensional feature space and learning a linear predictor there.",
                    "label": 0
                },
                {
                    "sent": "So doing empirical risk minimization basically boils down to.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right here, where maybe you had some sort of regularization anaso.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You know using the kernel trick and the Representer theorem, we can.",
                    "label": 0
                },
                {
                    "sent": "Write this in equivalent way where now we minimize and with respect to alphas weights over the examples.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "And this is a convex problem which we can solve in polynomial time.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, there is a problem in terms of scalability.",
                    "label": 0
                },
                {
                    "sent": "So to solve this we still need to compute an handle an M by N matrix or M is the number of data points and that could be very expensive when M is large.",
                    "label": 0
                },
                {
                    "sent": "As a result, there's been a huge literature on making kernel learning more efficient.",
                    "label": 0
                },
                {
                    "sent": "I cannot possibly do justice to all of the works.",
                    "label": 0
                },
                {
                    "sent": "Is there just a few, maybe representative examples but rough?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "He's speaking, we can divide these methods or the methods that I'm familiar with is generally used one or both of the following techniques.",
                    "label": 1
                },
                {
                    "sent": "So either we limit the number of entries of the kernel matrix that we compute, or the do some sort of low rank approximation of the matrix, either explicitly or implicitly.",
                    "label": 0
                },
                {
                    "sent": "Now, despite this, a large body of literature, as far as we know, there's been almost no attempt to try and find lower bounds for this problem.",
                    "label": 0
                },
                {
                    "sent": "So how can we?",
                    "label": 0
                },
                {
                    "sent": "How well can we hope at all to perform using these kinds of approaches?",
                    "label": 1
                },
                {
                    "sent": "So this is basically the subject of.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our paper.",
                    "label": 0
                },
                {
                    "sent": "So the first model we're going to look at is when we try to speed up kernel learning by a, forcing ourselves to look at only a small part of the kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "So we are going to assume that the target values wise are a given, but we don't know the access nor the kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "The only way we can access this data is by a querying B entries in the Matrix, possibly in adaptive way, so how well?",
                    "label": 0
                },
                {
                    "sent": "In this model, can we solve the ERM optimization problem?",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we're going to see some answers for this, but in general, one thing that we observed is that the answer depends not just in the kernel matrix, but also depends a lot on the kind of regularization you use and also the nature of the loss function.",
                    "label": 1
                },
                {
                    "sent": "Also, it's important to realize that this is very different question then matrix approximation, so people have looked at this question that I have an unknown matrix.",
                    "label": 0
                },
                {
                    "sent": "I can query a certain number of entries out of it, so how well can I approximate the matrix in Frobenius norm?",
                    "label": 0
                },
                {
                    "sent": "Spectral norm or something like that?",
                    "label": 1
                },
                {
                    "sent": "And this question is a different because here are interested in eventually solving an optimization problem.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just to give you an example, let's say consider doing kernel ERM using software colorization with respect to the linear loss.",
                    "label": 0
                },
                {
                    "sent": "OK, so the loss L is just the product of the prediction times the target value Y.",
                    "label": 0
                },
                {
                    "sent": "So how many entries of K do you think we need to observe if we want to solve this up to epsilon error, any ideas?",
                    "label": 1
                },
                {
                    "sent": "Guesses.",
                    "label": 0
                },
                {
                    "sent": "One over epsilon.",
                    "label": 0
                },
                {
                    "sent": "One over epsilon squared.",
                    "label": 0
                },
                {
                    "sent": "Well, actually know the true answer is 0.",
                    "label": 0
                },
                {
                    "sent": "You do not need to look at the kernel matrix at all to solve this problem, and the reason is that what we need to return our weights over the data points and for the linear loss the values of these weights does not dependent on the kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "It just depends on the target values times some scaling.",
                    "label": 0
                },
                {
                    "sent": "So again this demonstrates that this is not just the question of matrix approximation, but.",
                    "label": 0
                },
                {
                    "sent": "It also depends on other properties of this problem.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the kind of kernel matrices we use to construct are a lower bounds are basically block diagonal matrices with one inside each block and zero outside.",
                    "label": 1
                },
                {
                    "sent": "In fact, our techniques are more general, but we focus on this for simplicity.",
                    "label": 0
                },
                {
                    "sent": "By the way, these kinds of matrices are induced by linear kernels, or certain tests are polynomial kernel kernels approximately by Gaussian kernels.",
                    "label": 1
                },
                {
                    "sent": "But basically this is the class, so we have at most D blocks of 1 along the diagonal and N by N matrices.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's start with the first example where we want to do erm, with respect to the absolute loss, and without any kind of strong convexity, we're just constraining the normal for predictor in the kernel space to be squared norm at most 2.",
                    "label": 1
                },
                {
                    "sent": "So one kind of theorem we can show is that given a budget constraint B, as long as it's much smaller than the total sum total kernel size, you won't be able to get an error which scales better than one over the 4th root of a B.",
                    "label": 0
                },
                {
                    "sent": "And this is actually optimal because you can get this using the following very simple algorithm.",
                    "label": 0
                },
                {
                    "sent": "Just throw away most of your data, just subsample square root of BA training examples, then your kernel matrix would be of size.",
                    "label": 1
                },
                {
                    "sent": "Sqrt B * sqrt B.",
                    "label": 0
                },
                {
                    "sent": "That's only be entries and do, erm, with respect to that.",
                    "label": 0
                },
                {
                    "sent": "And this would be the kind of error you get.",
                    "label": 0
                },
                {
                    "sent": "So in some sense this tells us that we won't be able to speed up kernel learning in any kind of nontrivial way, at least in this case.",
                    "label": 0
                },
                {
                    "sent": "Please, without throwing away your data.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here is the general proof idea.",
                    "label": 1
                },
                {
                    "sent": "So we start with the matrix like the kind of mentioned.",
                    "label": 0
                },
                {
                    "sent": "The earlier where we have something like D blocks more or less of equal size.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For each block, we randomly choose to do one of two things.",
                    "label": 0
                },
                {
                    "sent": "Either we leave it.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The same way or we further divide it into a two smaller a blocks so you end up with the matrix, which is a miss mix of small blocks in large blocks and in the end.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You randomly permute the rows and columns, basically permuting the order of your data OK.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A now we pick the target values to be something like one over square root.",
                    "label": 0
                },
                {
                    "sent": "The number of blocks.",
                    "label": 0
                },
                {
                    "sent": "In this case we can show that there is a predictor in our domain, achieving 0 error.",
                    "label": 0
                },
                {
                    "sent": "But to get any kind of error much smaller than 1 / sqrt D, it reduces to a estimating for each of these blocks what is its size?",
                    "label": 1
                },
                {
                    "sent": "Is it a small block or large block?",
                    "label": 0
                },
                {
                    "sent": "So this is just a two line calculation, so we end up with requiring.",
                    "label": 0
                },
                {
                    "sent": "That the alphas for each block there's some would be a 1 / sqrt D and it's important that we won't undershoot or overshoot.",
                    "label": 0
                },
                {
                    "sent": "However, because it's a block diagonal matrix, it's relatively sparse and we randomly permuted the rows and columns it's going to be quite difficult to even get any information on most of these blocks because just hitting all of these blocks is going to be difficult and.",
                    "label": 1
                },
                {
                    "sent": "We show this formally and using that we eventually get our bound.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A OK, so that's for one specific loss, but can we say something more general for other types of losses, so we have the theorem of that type when we do soft regularization, we just add the graduation term to the objective function.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is how the general theorem looks like in the next slide.",
                    "label": 0
                },
                {
                    "sent": "I'm going to give specific corollary's, but roughly speaking, the lower bound that we get on the error depends on how non linear or non smooth is a our loss.",
                    "label": 1
                },
                {
                    "sent": "So roughly speaking it depends on these differences, sorry.",
                    "label": 1
                },
                {
                    "sent": "Difference of two times U 1 -- U two where AU one is when you try to minimize the loss with respect to some target value plus some regularization, you two is the same thing, but where we divide the regularization by two.",
                    "label": 0
                },
                {
                    "sent": "So for linear loss this will just scale the use by a factor of two and you would get 0.",
                    "label": 0
                },
                {
                    "sent": "But if you have any kind of non linearity non smoothness it makes this something which is larger than 0.",
                    "label": 0
                },
                {
                    "sent": "EH, OK, so to give some.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Simple corollary's, so for linear loss.",
                    "label": 1
                },
                {
                    "sent": "As I said earlier, you get 0 and this is actually the right thing because we saw that we don't need to look at the kernel matrix to get a lower bound for the linear loss.",
                    "label": 0
                },
                {
                    "sent": "For the absolute loss, we get something similar to the previous theorem, only now with respect to soft regularization.",
                    "label": 1
                },
                {
                    "sent": "So again, it's a lower bound which can be achieved by the trivial algorithm where we throw away most of our data.",
                    "label": 0
                },
                {
                    "sent": "But we can also get things for other losses, so a for squared loss which is nonlinear but smooth, we get a bound of the following form, which still tells us that, for instance, the number of queries we need to do scales like one over the strong convexity parameter is squared.",
                    "label": 0
                },
                {
                    "sent": "So for instance, if it's too small.",
                    "label": 0
                },
                {
                    "sent": "If it's something like one over the data size, which is a reasonable choice, in many situations you won't be able to speed up kernel learning in a.",
                    "label": 1
                },
                {
                    "sent": "Meaningful way.",
                    "label": 1
                },
                {
                    "sent": "Also, for the hinge loss, we can get things here.",
                    "label": 0
                },
                {
                    "sent": "It's non smooth but sort of.",
                    "label": 0
                },
                {
                    "sent": "The error is a wine cited, so we get something, but it's weaker than in the case of absolute loss, and again it still identifies the region where you cannot get in ever better than a constant.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In OK, so one approach is to look at the budget constraints, but you can also look at the second kind of general approach where we approximate the kernel matrix by some low rank matrix.",
                    "label": 0
                },
                {
                    "sent": "So specifically, we can think of an algorithm which does Colonel Iran, but with respect to any kind of low rank surrogates for the original matrix.",
                    "label": 0
                },
                {
                    "sent": "By the way, it may not need to approximate it in any meaningful way.",
                    "label": 0
                },
                {
                    "sent": "We can get a lower bound just by the fact that it's low rank and you get a lower bound on this form, which again tells you that a.",
                    "label": 0
                },
                {
                    "sent": "It gives you some lower bound on the rank of the low rank approximation that you would need to get small error and again in the regime where Lambda is one over the data size, you won't be able to get in anything meaningful.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to summarize a so the bad news are that there are cases of kernel learning which we can't speed up, at least not using the kind of methods that I talked about.",
                    "label": 0
                },
                {
                    "sent": "And in general, the kind of computational effort does depend on the loss and also scales with your essentially condition number.",
                    "label": 1
                },
                {
                    "sent": "The strong convexity for instance, and in general regression losses which are non smooth, such as the absolute loss.",
                    "label": 1
                },
                {
                    "sent": "Appear to be especially difficult.",
                    "label": 1
                },
                {
                    "sent": "I mean, our bound is for the absolute laws, but you can take just about any kind of nonsmooth regression loss, and you get the same kind of.",
                    "label": 1
                },
                {
                    "sent": "A problem, so these are the bad news.",
                    "label": 1
                },
                {
                    "sent": "The good news is that our lower bounds are not the same.",
                    "label": 0
                },
                {
                    "sent": "They are actually weaker when we discuss a smooth losses or when you have a lot of strong convexity or 1 sided losses such as the hinge loss and of course the natural question is can we utilize these assumptions to design better algorithms so they have in some specific cases.",
                    "label": 0
                },
                {
                    "sent": "So for instance a learning algorithm design for the hinge loss or squared loss but.",
                    "label": 0
                },
                {
                    "sent": "Maybe we can get something which is a more general.",
                    "label": 0
                },
                {
                    "sent": "Course the reverse could be that maybe our lower bounds are simply not tight enough and they can be further improved, so I'll stop.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}