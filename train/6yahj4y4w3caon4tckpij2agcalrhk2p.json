{
    "id": "6yahj4y4w3caon4tckpij2agcalrhk2p",
    "title": "Hierarchical preconditioners for image processing applications",
    "info": {
        "author": [
            "Rick Szeliski, Microsoft Research"
        ],
        "published": "Jan. 13, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Computer Vision"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2010_szeliski_hpi/",
    "segmentation": [
        [
            "Thank you for coming here.",
            "I'm glad to see Alan's in the room 'cause I threw in a couple of extra relations to what he's what he talked about this morning into the talk and I think what I'm talking about is also very closely related to it then discuss, so I'll try to make those connections more obvious."
        ],
        [
            "So when we have, there are a lot of problems in computer vision and computer graphics where we're trying to solve optimization problems on 2 dimensional grid.",
            "Sometimes they're irregular grids, but I'm going to talk about kind of structured semi regularity where you have two dimensional grids, but the connectivity on the grid, the the strength of the connections can be very different.",
            "So and and Sebastian was talking about some of the structured prediction work yesterday, where they had the the conductances were different and so on.",
            "So I'm talking about those kinds of things where, for example if we have sparse data interpolation, it's possible that the amount of smoothness can be different in different parts of the grid.",
            "And this is some work that Dmitri Top listed at MIT along time ago.",
            "So here is what thin plate spline which Alan warned us is a bad idea does when you have a tear in it.",
            "So these are the kinds of problems we might want to solve.",
            "Quest sound blending is another one where you take 2 images and you want to disguise the fact that the exposures were different.",
            "Then there's also colorization where you start with an image and you just put a little seeds of color and you want the color to bleed in a way that fills insensibly, so you don't want it to bleed across intensity edges.",
            "And again when we look at this as a graphical model.",
            "Basically their spatially varying smoothness.",
            "So these problems, although their grid it they're very in Homewood."
        ],
        [
            "Genius and then interactive Tone Mapping is another example where we kind of say the blue strokes in the foreground should be brightened up, but the background shouldn't be, and then it does something like that.",
            "So these are just some examples of where these problems arise Now in mathematics.",
            "If you come from the domain of partial differential equations, you often start with this kind of variational formula.",
            "Continuous function that you're trying to minimize, but typically inside computers we discretize it onto a finite grid using either finite element or finite differences.",
            "We solve the problem.",
            "The terms in blue are somewhat nonstandard, but they're the ones that people who do press on blending use.",
            "So if you ever have a gradient constraint, you're trying to say, please match a gradient, then that's what the blue terms are.",
            "Otherwise they're typically zero if you're just trying to minimize smoothness Now, this may not look that familiar to people in machine learning, but if you just pop up this kind of a diagram, then all of a sudden that looks extremely familiar.",
            "Because this is a graphical model for Markov random field and you can see each of the numbers down below has an exact correspondence in this graphical model.",
            "Right, the S are the smoothness is there basically compatibility's between adjacent nodes in a field and the WSR the weights tying you to measurements you know, so you might have nodes measurements at some nodes but not at others.",
            "And you know Alan is very familiar with this, 'cause you have well logs in some places but not elsewhere.",
            "So it's all the same mathematical formulation.",
            "When I was working at Slumber J many many years ago they called this creating, so there's a huge literature on this and you know, as I'll show you soon.",
            "Classical multigrid is one of the highly used techniques, but I think, well, I have here is something that works better."
        ],
        [
            "In practice, so very simply, you're trying to.",
            "You can think of it as minimizing a quadratic energy function.",
            "It's the same as solving a sparse linear system, and so the question is how do you solve these sparse multi banded systems?",
            "And there are."
        ],
        [
            "Variety of techniques.",
            "So for example, one of them is these direct solvers.",
            "This is what the medicine I think library does right?",
            "There is that for direct techniques for sparse solvers or not.",
            "You were the one who asked about Medas.",
            "Yeah, right?",
            "So their direct.",
            "So there are direct solvers.",
            "They work fine once the problem gets to bid an end dimensional like two or three dimensional, you have too much filling, so people so at some point you stop using those.",
            "Then you have iterative solvers and they can be as simple as things like conjugate gradient.",
            "But in practice works much better.",
            "Are these multigrid techniques?",
            "Both of these are excellent books to read about an.",
            "The idea is Dan just told us is that we sort of work between different levels in a pyramid and there's another set of techniques that were kind of introduced in late 80s, and I wrote an early paper on this in 1990 called Hierarchical preconditioners, which is related to multigrade.",
            "But rather than thinking of it as transferring things between levels, it's more like a basis change.",
            "So little bit again, it's related to what Alan was talking about this morning with tree based priors.",
            "So you can think of it like that.",
            "It's a tree based.",
            "It's a prior that when you transform the system it has a very nice preconditioning effect.",
            "So basically."
        ],
        [
            "What's going on is you start with your initial system and then you say, what if I represented the unknown variables X as some kind of a hierarchical wavelet like representation an.",
            "I try to optimize over the wavelet coefficients and the actual S matrix is typically written as a series of very sparse sort.",
            "Of course defined operations, so it has a very low operation count, and when you do that, you eventually end up with a equivalent system which were the Hessian.",
            "Now has the form SAS, and when you do the computation for small enough grids where you can actually compute the condition number.",
            "You know, because it's small enough, you find that it has a much better condition number.",
            "So the simple take home messages for a lot of these problems.",
            "If you just use a wavelet basis, things get a lot better."
        ],
        [
            "Solve it a lot faster.",
            "Here's an intuitive example of what's going on.",
            "Let's say we're solving a 1 dimensional problem, but we just represented as a bunch of data points, and we're going to do linear interpolation between."
        ],
        [
            "The data points, so the basis functions are local.",
            "They're called nodal basis functions.",
            "But if you represent it as a hierarchal basis so you have some low frequency component in some mid frequency components and so on, you end up with a system that's much better conditioned.",
            "And like I said, late 80s.",
            "There were some numerical analysis work.",
            "I did work in the early 90s and then the computer graphics community in the mid 90s.",
            "People pick this up and started using it.",
            "So these kinds of."
        ],
        [
            "Wavelet Preconditioners work very well, and here's an example.",
            "So we have our little problem.",
            "We're trying to interpolate this thin plate spline with a with a tear in it, and on the left is if we use four bases of four levels in the wavelet.",
            "These are very simple wavelets.",
            "These are just tent wavelets.",
            "Four and six, and you can see it.",
            "The six gets you to the solution initially much faster, but what happens is it tends to stall 'cause it's over smoothing."
        ],
        [
            "If you actually look at the convergence, then for four levels you get something like this and for six levels you start off faster.",
            "You can see it drops quicker, but then it doesn't do as well because it's over smoothing.",
            "So this idea of wave preconditioning on irregular problems is a good idea, but it's not good enough.",
            "It's not actually paying attention to the underlying structure, and that's why people have invented things like algebraic multigrid where you actually look at the elements in a matrix and say how can we solve it.",
            "So the question is how?"
        ],
        [
            "Can we do better, right?",
            "So here in 1990 when I sort of stopped working on this problem, I said, well, looks like you just have to tune the number of levels and that's the best we can do.",
            "And it doesn't really do anything sensible about adapting to things like this continuity's.",
            "So the question is, can we do better and the goal would be if we can to automatically construct a set of basis functions which adapts the domain and gives you good preconditioning so."
        ],
        [
            "Question is, how do we do that?",
            "Well, I've sort of formulate the problem giving you some intuitions on where we're going.",
            "Now let me start with a 1 dimensional problem where I can probably show you what the optimal bases are, and they are so provably optimal that they converge in a single iteration.",
            "They basically preconditioned the system into a diagonal, and let's see how this is done."
        ],
        [
            "So this is a technique.",
            "Let's just reformulate the problem like this in one dimension.",
            "It's the same graphical model, but just restricted to align OK. And now we're going to use a technique that's called cyclic reduction.",
            "How many people have heard of cyclic reduction?",
            "Just Alan?",
            "It's a very old well known technique for doing linear system solving.",
            "When you have a parallel computer, what you do is rather than sort of walking down your tridiagonal system and doing it salesky and then walking back up, you take all the even variables or all the odd variables and you get rid of them and it turns out you can eliminate them and for a problem like this which is tridiagonal, you end up with a tridiagonal system, so you can in parallel basically eliminate things so you can take all the odd variables.",
            "You can write them down as a function of the even variables and notice that here.",
            "We have things like the diagonal element is being used to divide the off diagonal elements, so this should look familiar from dance talk as he has these terms where there's an AIJ divided by the sum of AIIMS.",
            "It's exactly that kind of thing that's going on, but it's not a heuristic.",
            "It's not saying, well, these things are weak.",
            "These things are strong.",
            "This is exact.",
            "This is exactly the right way to solve the prob."
        ],
        [
            "And when you do that substitution, what happens is if you start off with this kind of a red black problem, you're going to basically say you're going to say the red values.",
            "I know if someone gave me the black values, I know exactly what red value each red value should, because it only has to look at its two neighbors.",
            "You can think of it as it's two parents, and so I can do that substitution in terms of linear algebra.",
            "What happens is now I have a smaller problem, which is represents exactly the same energy, just defined over fewer variables.",
            "So we're basically restricting from a larger dimensional.",
            "Space into a smaller one and solving the small one gives us exactly the information we need to solve the fine complete problem.",
            "It's not an approximation.",
            "We aren't going to have to iterate.",
            "We're going to be able to in one step.",
            "Basically go up the pyramid form these very small set of equations.",
            "In theory it could be just one equation and then on the way back down by back substitution we're going."
        ],
        [
            "Yeah, exactly the right answer.",
            "So in terms of matrices, if we permute the variables so that the course variables are in the bottom and define variables are at the top, this interpolation function is this idea of taking the off diagonals and."
        ],
        [
            "Abiding by the diagonal and when you do that, the effective matrix.",
            "Remember we're trying to precondition the matrix so that it has better eigenvalues.",
            "Well, if we can get the matrix turned into a diagonal, then it's trivial to do perfect inversion, right?",
            "Because everyone can invert a diagonal matrix and we're in good shape because the matrix is diagonal in the upper left corner in the lower left corner right corner.",
            "It's still tridiagonal, and we can recurse on the lower right corner and keep going all the way down until we have a single variable.",
            "So this is a very nice construction.",
            "It's not new, but it's just to motivate what we're going to try to do in two dimensions by this exact 1 dimensional algorithm."
        ],
        [
            "Just to show that it really does work, by the way, their relationships here too, with people in machine learning, called the Junction tree algorithm.",
            "I think those of you who know about graphical models, how many people know that in a graphical model you all know that if a graphical model is a tree, it's order N right solution, right?",
            "What if you have a long chain?",
            "How many of you know that you can basically take any element in the chain and just get rid of it right?",
            "Because if you have an element in a chain, you can find the effective relationships between the two, the left and the right neighbors, and so the junction tree algorithm.",
            "One way of thinking about it is you do the easy stuff which is you take the leaf nodes.",
            "And if you have things in the middle of a chain, you just get rid of them and then you get to the harder parts, which are things like triplets and so on where you get clicks.",
            "So it's related to the junction tree algorithm."
        ],
        [
            "And so in 1D, if I set up a problem like this, which is just a bunch of data points with weights and then this is my smoothness function, so I put a little tear in it.",
            "Then when I solve the problem, it's kind of interpolating those weights with a little decay towards 0.",
            "Then it jumps because there's a tear and it solves the next interpolation."
        ],
        [
            "Problem and if we run this in an actual simulator and compare it to the other older work I did in 1990 which has a fixed number of just non adapted linear basis splines, we see that it has a one step convergence.",
            "So this is again mostly just back."
        ],
        [
            "Round, so the really interesting question.",
            "So this is cyclic reduction and it's also related to in the wavelet community there called weighted wavelets.",
            "Although again the way the weights are computed are often heuristic based on certain desired properties.",
            "This is an optimal way to get the waiting.",
            "So the big questions can be extended to 2D Canal and see what's coming.",
            "Or is that yeah OK."
        ],
        [
            "So here is the 2D salute."
        ],
        [
            "OK, so we're going to basically now do this on a checkerboard, and we're going to use the same idea of a red black, so we're going to try to get rid of the red variables and come up with a similar problem on the black variables.",
            "So what we're going to do?"
        ],
        [
            "Is we're going to take these and we're going to eliminate that.",
            "And when you eliminate that, what happens?"
        ],
        [
            "Is you get these connections, the bandwidth now I told you that the bandwidth of that smaller system was try diagonal in the 1 dimensional case in the two dimensional case it gets bigger and so you're in trouble.",
            "There are two reasons why you're in trouble.",
            "One reason is if it gets bigger, you're just going to have to do more computation, but there's a bigger reason that you're in trouble, and the big reason is this that if I wanted then eliminate, let's say this node here, it's no longer tide to only its parents.",
            "'cause these are going to be the parents at the next level.",
            "It's also tide to its siblings, so whenever you have a graphical model where you can't partition between things to be illuminated on things that remain, you have couplings and this is one of the reasons I think, qualitatively why multigrid isn't a perfect preconditioner, because you're basically saying let's do an interpolation function based on parent nodes, But the fine notes actually talk to each other, and we're kind of ignoring that you know there's information that percolates just through the fine nodes, like, even if we took all the course nodes and fix them to particular value, define nodes would still sort of negotiate with each other.",
            "How much they have to smooth?",
            "So the trick is how can we get this behavior?",
            "The nice thing is we start with an original problem where we can just eliminate all the red variables.",
            "But after once a step we're stuck.",
            "We don't know how to recurse, so the trick would be how do we eliminate these things that I'm going to call diagonals in this diagram?",
            "The red things aren't actually diagonal, they look horizontal and vertical, but if you think about the black things as being the variables, the blue connections are between nearest neighbors.",
            "The red connections are between things that are sort of diagonally across from you, and there are the ones that you have to get rid of.",
            "Right, so I'll pause here.",
            "Does anyone have an idea of how we're going to get rid of these?",
            "Any suggestions?",
            "No OK, so if you think about this as a resistive grid, what we're trying to do is basically doing approximation where we want to sort of come up with a resistive grid that is similar, right?",
            "And if you had to resist this bridge, you wouldn't go in here and just snip resistors, right?",
            "'cause you're going to remove the you're going to lower the conductance of the overall thing, so even for very low frequency gradient from one edge to the other, if you snip resistors, you know your overall conductance is gone down, so that's not a good idea.",
            "What we want to do instead.",
            "Is to basically re distribute the weight, so if we think of it as cutting resistors but then putting additional conductance on the links between the resistors that remain like this.",
            "So I went by a little fast notice the little arrows I'm going to remove the red things and I'm going to beef up the things that remain and this is a variant that I haven't seen in the numerical analysis literature.",
            "Per say.",
            "There are things like Cholesky decomposition where you say let's do incomplete Cholesky, which is actually just dropping off diagonal elements.",
            "There are modified incomplete tell you where you beef up the central values to keep them strong, but the idea of tweaking other elements in your matrix when you're zeroing out certain ones is not one that I've seen now.",
            "Maybe in graphical models you know people have done it.",
            "I'd be interested in hearing about this, but this is the basic trick.",
            "It basically says how do you do it and.",
            "As a matter of fact, I'll tell you shortly that we were working with some folks from Yana scooters from Gary Miller's Group at CMU, and this is a trick that's used in linear algebra where they it's called sparsification, where you basically remove certain edges in a graph and beef up other ones so it is.",
            "It is actually known, but I haven't seen it in the multigrid or incomplete sort of factorization literature, so I guess the question is, will this heuristic work right?"
        ],
        [
            "And So what we do did here is we have a simple problem again with a tear.",
            "You can see the solution is on the lower right hand corner and if we run regular conjugate gradient after one iteration it hasn't done anything.",
            "Really.",
            "We're trying to get to the solution on the right with three levels.",
            "It's already spreading.",
            "You can see that white area is spreading, so it's doing good job of smoothing, but with the locally adapted heretical basis functions.",
            "What we call this 'cause there are article wavelet like functions, but they're locally adapted based.",
            "On the strengths in the the Hessian matrix, it's gotten along way towards the solution.",
            "It's actually visually.",
            "It's very hard to tell if it has or hasn't converged, and after five iterations the classical hierarchical basis functions are getting there.",
            "You can start to see a little bit of that error showing up, but it's not working nearly as fast and so much easier."
        ],
        [
            "Creative see this is to just look at the convergence plot and again the more levels we use for preconditioning, the better it gets.",
            "I haven't shown past level equals 4 where starts getting worse.",
            "Other things Ilu 0 is this idea of basically doing the red black coarsening, but then just dropping things off diagonal that are the wrong connections and it's not doing that great.",
            "It's kind of in the middle rils view and Emily LUR is where you drop part of the off diagonal things back on the diagonal.",
            "MLU puts all of it on the diagonal, but then the technique I'm proposing is just."
        ],
        [
            "Much, much faster, and if you look at it in a log plot it's dramatically evident.",
            "So even though it's a heuristic, it turns out to be a very good heuristic.",
            "So the idea is that we know that local relaxation let's Dan told us right is good at smoothing high frequencies.",
            "We want to have a smaller level problem that smoothing course frequencies low low frequencies, right?",
            "And if you have a resistive grid or graphical model and you know the solution is low frequency then you can remove certain connections and put them elsewhere.",
            "That's the intuition.",
            "OK, so that's the basic technique and."
        ],
        [
            "So here's just another visualization of what's going on.",
            "If we start with a grid like this, then when we reduce it by dropping half the variables, we end up with this grid and the light blue.",
            "Things are the things we don't want.",
            "So then we basically get rid of them, and we have the next grid and so on.",
            "So you just keep recursing like that, so that's this.",
            "Is a nice way we've seen to visualize what's going on in these grids.",
            "And as I mentioned already, the previous strategies either just drop the off diagonals or add them back to the diagonal and neither the miss is good as coming up with a better approximation."
        ],
        [
            "So again, this is the sample to the problem."
        ],
        [
            "And this is the summary of the algorithms for those of you who do preconditioned conjugate gradient.",
            "So we basically iterate between computing the residual vector.",
            "Then we compute this hierarchical residual.",
            "We divided by the diagonal.",
            "That's an important step.",
            "We come back down and that matrix SD inverse transpose is an approximation to the actual original a matrix.",
            "It's not exact, but it's a good preconditioner, and then we basically, after we've done that preconditioning step, then we.",
            "Do the rest of the usual conjugate gradient."
        ],
        [
            "OK, so as it turns out, these kinds of algorithms can be easily implemented on a GPU if we want to in practice what you end up doing is you can stop at some level in the pyramid.",
            "This is fairly standard in multigrid and in a lot of other multiresolution models.",
            "Is once the model gets small enough to be tractable, you just do a complete inference over the course level model.",
            "And there are also techniques for very large problems."
        ],
        [
            "Streaming multigrid"
        ],
        [
            "So here are some of the examples on computer graphics.",
            "This is the colorization problem and this is what conjugate gradient does after 10 iterations after 20, so on just keeps going so it gets there eventually, but it's a slow technique and machine learning.",
            "We've seen some people I mean obviously can't grade is much better than just gradient descent, but if you choose the right preconditioner you can do much better.",
            "So this is after one iteration with."
        ],
        [
            "The proper preconditioner, and again we see results like."
        ],
        [
            "Yes, this is the personal blending problem.",
            "That's what the final the final blend looks like.",
            "So originally you see the big intensity differences between the ground after the blending.",
            "It looks like that."
        ],
        [
            "And again it has."
        ],
        [
            "Conversions this is the colorization problem with the interactive tone mapping, and that's the result."
        ],
        [
            "Up there, so there's a whole bunch of other applications in computer graphics.",
            "One I really like a lot is Annette Levins work on closed formatting where she basically ends up solving a large eigensystem by looking at local color differences, but then eventually computing essentially the eigen values in a kind of.",
            "Cut so these are all techniques all applications in computer graphics and computer vision, where these kinds of fast solvers can be used.",
            "So."
        ],
        [
            "So basically what I've just done is a summary with sort of presenting it more for this audience.",
            "It's a summary of our 2006 SIGGRAPH paper presented for this audience.",
            "Now what we've done since then is I've been working with Rob Fergus is student Dilip Krishnan, and we now actually have an analysis of the convergence properties of this, and that's what I want to sort of present in the last little bit of my talk before I start on that.",
            "How much time do I have left?",
            "10 minutes, OK, that's about how much material I have.",
            "OK so."
        ],
        [
            "The question is, how do we actually analyze the convergence and we didn't really know how to do this until Jan is Kutis who is the the second author there?",
            "Ann has just moved to Puerto Rico came and gave a talk at Microsoft Research.",
            "He's been a postdoc at CMU with Gary Miller for a number of years, so I'm going to borrow a few of his slides just to introduce the concept of generalized eigenvalues, which is the main analytical tool.",
            "Be using here.",
            "OK, so."
        ],
        [
            "That's the."
        ],
        [
            "Talk and these are just a few selected slides from very nice talk.",
            "You can think of these problems as weighted Laplacian, so if you're a matrix basically is something like a symmetrically diagonally dominant matrix with negative off diagonals.",
            "That's equivalent to resistive grid, and so you can think of this as having voltages flowing around.",
            "Alan this morning was talking about Random Walk S What was the WS was it works?",
            "Summing right exactly walks something somebody like Grady Leo Grady calls these the random Walker algorithms.",
            "So basically you can start with from one node and basically walk based on these connections and how far you percolate gives you an idea of the sort of connectivity between things and so potential way of solving, for example a seated interpolation problem.",
            "So there are nice analogs between.",
            "These kinds of problems and resistive grids and."
        ],
        [
            "What's also nice is that you have this concept of a support number.",
            "So let's say I take one grid and I approximated with a different grid by sniping things and changing values somewhere else, and I want to know how close are the two grids?",
            "Well, the grids, if if that ratio there is identical for every single value, then basically the grid has exactly the same resistance properties no matter what voltages you put in and the only condition under which that that's the case is if the two graphs, if LA&LB, are the same.",
            "But the ratio being non unity is a nice scalar number that tells you how much energy is dissipated by B.",
            "If it's an approximation to A and if you take that ratio in its inverse, you get something that's called the generalized condition number and it basically says how close are these two grids to each other.",
            "How close are these two problems and you want them to be close in both directions, 'cause otherwise if you just said I want one to be smaller than the other, you could just make all the values small or all of them big.",
            "So this is the basic tool."
        ],
        [
            "We're going to use so again this quadratic form V transpose LV is just the energy dissipation in a network, and So what you're trying to do is come up with two networks that have very similar energy dissipation, both for high frequencies and for low frequencies.",
            "Essentially for all frequencies you want to find the eigenvalue, which is the worst.",
            "The biggest difference between the two, and that tells you how well the approximation is, and the reason that's important is if we're using one graph to be an approximate inverse of the other graph.",
            "In that particular eigenvalue where they disagree is the one that's not going to get cancelled as fast as it needs to be."
        ],
        [
            "So theoretically there are nice results which show that if you do very simple things like just snip things and then re distribute the values as connection.",
            "So for integer graphs like this we have certain end we're embedding B which is more complex, has loops into a non loopy chain.",
            "There the dilation which is saying for each edge that we cut.",
            "What's the maximum length that has to traverse in the similar graph the congestion says for any edge in B, how much traffic is it carrying from the original thing?",
            "And it turns out there's a nice theoretical result that says the condition number is at most the congestion times the dilation.",
            "I'm not going to use it, but it's a very useful concept.",
            "If you've got these graphs you're trying to say, how am I going to approximate one graphical model with another?",
            "Ask yourself the question, how well does this simpler model model the original model and vice versa, right?",
            "So that's the basic."
        ],
        [
            "Thing.",
            "So now we've got all the tools that we need basically to analyze our locally adaptive hierarchical basis functions, which is basically a series of course innings where, because the graph is sparse enough, I can eliminate certain variables and then sparsification because I want to be able to come back to a graph that's simple or thin enough.",
            "So that you can basically do it again."
        ],
        [
            "So let's start with a simple problem like this of just taking our N8.",
            "The eight neighbor graph and approximating with RN for graph.",
            "How well does that work?"
        ],
        [
            "What is the relative condition number so I can do that?",
            "I can basically take an end 9 graph with uniform weights and approximated with an end and for graph, and these are the generalized algen value.",
            "So if you do the approximation right, you know things like low frequencies are approximated perfectly, but the high frequency checkerboard isn't.",
            "There's a gap of exactly oh sorry, this isn't the gap, this is just the original eigenvalues of the problem.",
            "To show you that it's very poorly conditioned, so low frequency has very low spectral or eigenvalue in the high frequency."
        ],
        [
            "These are our low.",
            "This is the approximation, so you approximate low frequencies very accurately.",
            "The checkerboard is off by factor of two, so you lose a factor of two if you go up.",
            "If you basically sparse."
        ],
        [
            "By the graph.",
            "Now what we're really interested in is not the question of 1 sparsification, but if we use this as a multi level preconditioner and we go up the graph and then we solve the course level problem, what is the relative condition number?",
            "Because that will tell us how good the preconditioner."
        ],
        [
            "So we can do this with just doing one step of half octave coarsening and sparsification and we see that the condition numbers are distributed between about 1/4 and 1.7, so that's sort of the eigenvalues and so the condition number."
        ],
        [
            "About five, if we now do this a second time, so we're up a full octave in the hierarchy.",
            "The condition number is now about 7 and I'm sorry."
        ],
        [
            "Spoke this is for a full octave so this is the traditional way you do multigrid.",
            "You basically subsample by two horizontally and vertically.",
            "Then if we."
        ],
        [
            "Do that for two levels.",
            "We get something like."
        ],
        [
            "This an for three levels.",
            "It's like that, so if you just use the three level full octave bilinear interpolation function, which is what I did in 1990, we basically have a condition number of 10.",
            "So means you're going to need about sqrt 10 operations to bring down your error by certain factor.",
            "OK, so the question is, is this half octave adaptive?"
        ],
        [
            "Thing any better if you do a half octave, the half."
        ],
        [
            "Active has a condition number of 2.5, but if you do two of them certain things cancel and the condition number is only two.",
            "The interesting thing also is in the middle there's a flat region and that means you have a bunch of eigenvalues there at one and conjugate gradient likes that it really likes clustered eigenvalues, so there's a certain subspace here where the particular approximation we're doing is doing a very nice job."
        ],
        [
            "And then we can keep going.",
            "This is 2 full."
        ],
        [
            "Octaves and this is 3 full octaves, so you see that basically it's still there's a lot of things that are approximated exactly and only a few things that are off the things you have to worry about are the half of the eigenvalues that are off to the left there, but they're between 0.5 and one.",
            "Oh well, actually there between .26."
        ],
        [
            "And one.",
            "So what's going on?",
            "Basically, it looks like about half the eigenvalues for a simple problem, well approximated, so you get those solved exactly for free.",
            "Most of the rest of them are spread between 1/4 and one, so the condition number is 4, and when you get into small condition numbers, it's good to go back to the real formula, not to remember the square root thing.",
            "If we take sqrt 4 -- 1, that's 2 -- 1 in the numerator 2 + 1 in the denominator.",
            "This means that about every iteration you're getting 1/3 the error, which is very fast convergence.",
            "So."
        ],
        [
            "So, so I guess the question is this.",
            "Is this is very exciting, but how does this relate to multigrid?",
            "I haven't yet talked at all in this presentation about any kind of smoothing, right?",
            "This is just a transformation of basis functions.",
            "So can we use or should we use pre and post smoothing steps?",
            "And how this?",
            "How does multigrid compared to this technique right?"
        ],
        [
            "So we did some simulations anfora fairly smooth problem.",
            "Here's what it looks like if I use a full octave coarsening, which is the blue line HBF, I get that kind of convergence and a simple multigrid V sweep has the same performance.",
            "Now.",
            "The thing to remember is that the V sweep is not a conjugate gradient algorithm, it's just a fixed step size keeps going, so you can do better by either computing adaptive step size or using it as a preconditioner, which is the purple line, but each V sweep takes about three times the amount of work, or maybe twice the amount of work.",
            "A regular preconditioning 'cause doing a relaxation involves comparing yourself with a stencil of your neighbors and then interpolating, doing the prolongation interpolations about the same amount of work.",
            "So these sweeps or multigrid because it has the smoothing is maybe two or three times slower than a simple change of basis things, so they are comparable.",
            "But then when we go to the half octave thing that green thing which is both faster and the sweeping converges faster.",
            "So this seems to be for regular problems, just something that outperforms multigrid we want to.",
            "Do a lot more careful experiments before we claim that, but this may be a technique that's just outperforms classical geometric multigrid."
        ],
        [
            "And then if we use and."
        ],
        [
            "A regular problem like this thing with that error.",
            "Then there is no comparison because classical geometric multigrid is just doesn't work well.",
            "We'd love to compare it with algebraic multigrid, but we're still looking for a good implementation.",
            "Maybe Dan can help me sort of find one that we could benchmark against that.",
            "Let's see how well it works."
        ],
        [
            "OK, let me mention a few other extensions that Dilip and I have discovered as we're doing this richly.",
            "I started this problem.",
            "This talk by saying if you start with a four connected grid, here's how we can basically solve it now.",
            "It turns out if you think about it, we can actually start with the 9 connected grid very easily and just apply this thing that we were using later.",
            "Applied early so you can solve non connected problems.",
            "Now in theory you could start with larger graphs with more connectivity, but I'm not on such firm footing house.",
            "How to do it.",
            "So if we're doing normalized cuts with arbitrary affinity's between distant things.",
            "It's not really clear to me that you can go in and snip resistors and replace them, or at least that's you know significantly more research to figure out how to do that, but the basic intuition of sparsifying graphs seems to be useful.",
            "Another thing we noticed is that actually you don't have to cut all of the diagonals because you only need to cut things that are going to be illuminated in parallel.",
            "So we can actually get by."
        ],
        [
            "With less cutting.",
            "OK, and that's this thing here.",
            "So you only need to eliminate the connections between the Reds and the Reds if you're going to eliminate the Reds then so you can have these extra diagonals between things that are going to remain as parents.",
            "And it also works."
        ],
        [
            "OK, so the final question, which is when I've been trying to work on this week and I didn't get the code done in time is can we add smoothing, tarako preconditioning and the answer is I don't have the the answer yet, but it's possible that.",
            "I mean it's almost certain that the smoothing will help.",
            "The question is whether it will be worth the extra computational effort and that's where I have to implement it to see because the nice thing is with these preconditions you're already getting a fairly clean split between the fine level variables in the course level variables.",
            "Discoursing procedure doesn't introduce any approximation error, unlike wavelets, which might not be perfectly orthogonal or something like that, but it's the coarsening that isn't approximate."
        ],
        [
            "Step.",
            "OK, so this is my last slide before my conclusions an let me mention the limitations.",
            "So the big limitation for this technique is that if your problem becomes highly irregular, it just doesn't work.",
            "You can't take something like for example problem where you embedded a 1 dimensional chain as a spiral and two dimensional problem and expect something that is using a series of checkerboards to approximate it because at some point you can't approximate a 1 dimensional chain with twenty turns on something that's like a four by four grid.",
            "So for very.",
            "Irregular problems we need something like algebraic multigrid, or the stuff that quote system those people are doing, which is called combinatorial multigrid, which is a different agglomeration strategy, right?",
            "So I think basically we need to combine both these approaches, and that's interesting future work, but that's probably could be at least a year or maybe thesis is worth of work to work that out.",
            "I think it's going to be very promising when."
        ],
        [
            "It gets done so."
        ],
        [
            "Other potential future work well.",
            "OK, so extending it to 2nd order problems like thin plates is something that's interesting, but we're not really sure how to do it.",
            "The other thing that I think is interesting for this audience is that this may be a guideline for how to do this for non quadratic non Gaussian Markov random fields, because if you have other potentials like for binary things actually Richard Hartley in the student car showed that for binary Markov random fields you can do this kind of coarsening and he was trying to go from problems that were not submitted.",
            "Modular until the course into one that was submodular, and then he used that as initialization for the more difficult problem.",
            "But the difficulty is that if you have non quadratic energies you can't.",
            "The energies don't collapse in the same nice way, so you may have sort of more complicated interaction potentials and things like that.",
            "Last line I threw in after hearing Alan's talk this morning because he thinks about different kinds of trees and he mentioned thinning, which is I think related to what we're doing here.",
            "So these are all open questions, but I think.",
            "The general idea of trying to be very judicious about eliminating things and then approximating energy so that may have a lot of benefits in other kinds of graphical models."
        ],
        [
            "So this is my conclusion slide.",
            "Basically we've developed something that's a very effective preconditioner.",
            "It's useful in a bunch of applications, and maybe is also useful as a general framework for other graphical models.",
            "So thank you very much.",
            "Yes, Alan.",
            "I got 47 comments.",
            "So going back to early things, we talked about using the tents right?",
            "I talked about the problem with this one collection for grounding.",
            "That's right, midpoint selections.",
            "Data points collected.",
            "That's right, however, you can use the same thing.",
            "The same formulation works for any one dimensional Markov process.",
            "That's right interpolation.",
            "Exactly what you were doing with not non tenant basis and in one D. This works exactly the same, it's just a different elimination order for the inference.",
            "Other comment which is great but not not done things on the second red black, but I talked about the idea of doing cavity models where you eliminate right 1D double correspond to not doing cyclic reduction.",
            "I've got a bunch of places in it.",
            "I'm going to reduce.",
            "Groups of them until they believe that's right, Yep.",
            "Exactly the same problem because you get filled.",
            "Right when I talked about this.",
            "Projection, which is the idea of people going to get in graph and then I'm going to match the correlations that match under the grip Max entropy, right?",
            "I haven't thought about it this way.",
            "In your context, correspond to how do I strengthen those other connectivities, right?",
            "So only other context that I know it's not great, but certainly the things we've done things that have that kind of.",
            "Interesting that I want to understand better.",
            "How do in fact, you're out having to do those kind of things, whether it has an interpretation?",
            "Sure, yeah, I think there could be some very interesting connections there, yeah?",
            "Thank you.",
            "I guess yes.",
            "Something that's going on that is adapting to lose weight, so it's the simple averaging, so the actual right I did I didn't have the formula for the redistribution, but if I bring up the right figure I can kind of show it."
        ],
        [
            "So this OK.",
            "So the formula we have is actually heuristic, which basically by doing some case analysis we said if there are certain connections between nodes that have no resistance, then don't put anything there because those we were working when I did the work in 2006.",
            "I was thinking of this as a physical model of a thin plate with cuts and I said if there's a cut you don't want to bridge over it, because then you're going to get low frequency coupling between things.",
            "So I said don't bridge anything and by sort of into it just by pure intuition I said well if one resistor on one side.",
            "You get to re distribute.",
            "Let's say this unwanted red thing to either.",
            "Well, no, that's not a good example.",
            "So let's take this horizontal red thing.",
            "Here we get to re distributed across the four blue things that are the diamonds.",
            "And if the resistance or these are actually all conductances, if the conductance is much weaker on one side, then you want to push less stuff there because you're basically increasing the smoothness in an area that wasn't very smooth.",
            "So I did a two way split back and then I split something along the way.",
            "Dilip and I started doing an analysis based on the condition numbers and we haven't finished it, but I think the right way to do this is exactly to use what could shows is to just look at a small clique and say what is the theoretically optimal.",
            "If we have just four values that we have to increase, what's the one that has the best local behavior on the conditioning number?",
            "So the answer is I have an old heuristic.",
            "I think there's more principled work to be done.",
            "Yes, but you know, in this work in terms of the set up, if I have to solve a little 4 by 4 system or something like that, I'm hoping that if we do enough of it will have intuitions and there will be some simple algebraic formula that works, but it's work that remains to be done, yeah?",
            "Any other questions?",
            "OK, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you for coming here.",
                    "label": 0
                },
                {
                    "sent": "I'm glad to see Alan's in the room 'cause I threw in a couple of extra relations to what he's what he talked about this morning into the talk and I think what I'm talking about is also very closely related to it then discuss, so I'll try to make those connections more obvious.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So when we have, there are a lot of problems in computer vision and computer graphics where we're trying to solve optimization problems on 2 dimensional grid.",
                    "label": 1
                },
                {
                    "sent": "Sometimes they're irregular grids, but I'm going to talk about kind of structured semi regularity where you have two dimensional grids, but the connectivity on the grid, the the strength of the connections can be very different.",
                    "label": 0
                },
                {
                    "sent": "So and and Sebastian was talking about some of the structured prediction work yesterday, where they had the the conductances were different and so on.",
                    "label": 1
                },
                {
                    "sent": "So I'm talking about those kinds of things where, for example if we have sparse data interpolation, it's possible that the amount of smoothness can be different in different parts of the grid.",
                    "label": 0
                },
                {
                    "sent": "And this is some work that Dmitri Top listed at MIT along time ago.",
                    "label": 0
                },
                {
                    "sent": "So here is what thin plate spline which Alan warned us is a bad idea does when you have a tear in it.",
                    "label": 0
                },
                {
                    "sent": "So these are the kinds of problems we might want to solve.",
                    "label": 0
                },
                {
                    "sent": "Quest sound blending is another one where you take 2 images and you want to disguise the fact that the exposures were different.",
                    "label": 0
                },
                {
                    "sent": "Then there's also colorization where you start with an image and you just put a little seeds of color and you want the color to bleed in a way that fills insensibly, so you don't want it to bleed across intensity edges.",
                    "label": 0
                },
                {
                    "sent": "And again when we look at this as a graphical model.",
                    "label": 0
                },
                {
                    "sent": "Basically their spatially varying smoothness.",
                    "label": 0
                },
                {
                    "sent": "So these problems, although their grid it they're very in Homewood.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Genius and then interactive Tone Mapping is another example where we kind of say the blue strokes in the foreground should be brightened up, but the background shouldn't be, and then it does something like that.",
                    "label": 0
                },
                {
                    "sent": "So these are just some examples of where these problems arise Now in mathematics.",
                    "label": 0
                },
                {
                    "sent": "If you come from the domain of partial differential equations, you often start with this kind of variational formula.",
                    "label": 0
                },
                {
                    "sent": "Continuous function that you're trying to minimize, but typically inside computers we discretize it onto a finite grid using either finite element or finite differences.",
                    "label": 0
                },
                {
                    "sent": "We solve the problem.",
                    "label": 0
                },
                {
                    "sent": "The terms in blue are somewhat nonstandard, but they're the ones that people who do press on blending use.",
                    "label": 0
                },
                {
                    "sent": "So if you ever have a gradient constraint, you're trying to say, please match a gradient, then that's what the blue terms are.",
                    "label": 0
                },
                {
                    "sent": "Otherwise they're typically zero if you're just trying to minimize smoothness Now, this may not look that familiar to people in machine learning, but if you just pop up this kind of a diagram, then all of a sudden that looks extremely familiar.",
                    "label": 0
                },
                {
                    "sent": "Because this is a graphical model for Markov random field and you can see each of the numbers down below has an exact correspondence in this graphical model.",
                    "label": 0
                },
                {
                    "sent": "Right, the S are the smoothness is there basically compatibility's between adjacent nodes in a field and the WSR the weights tying you to measurements you know, so you might have nodes measurements at some nodes but not at others.",
                    "label": 0
                },
                {
                    "sent": "And you know Alan is very familiar with this, 'cause you have well logs in some places but not elsewhere.",
                    "label": 0
                },
                {
                    "sent": "So it's all the same mathematical formulation.",
                    "label": 0
                },
                {
                    "sent": "When I was working at Slumber J many many years ago they called this creating, so there's a huge literature on this and you know, as I'll show you soon.",
                    "label": 0
                },
                {
                    "sent": "Classical multigrid is one of the highly used techniques, but I think, well, I have here is something that works better.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In practice, so very simply, you're trying to.",
                    "label": 0
                },
                {
                    "sent": "You can think of it as minimizing a quadratic energy function.",
                    "label": 1
                },
                {
                    "sent": "It's the same as solving a sparse linear system, and so the question is how do you solve these sparse multi banded systems?",
                    "label": 0
                },
                {
                    "sent": "And there are.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Variety of techniques.",
                    "label": 0
                },
                {
                    "sent": "So for example, one of them is these direct solvers.",
                    "label": 0
                },
                {
                    "sent": "This is what the medicine I think library does right?",
                    "label": 0
                },
                {
                    "sent": "There is that for direct techniques for sparse solvers or not.",
                    "label": 0
                },
                {
                    "sent": "You were the one who asked about Medas.",
                    "label": 0
                },
                {
                    "sent": "Yeah, right?",
                    "label": 0
                },
                {
                    "sent": "So their direct.",
                    "label": 0
                },
                {
                    "sent": "So there are direct solvers.",
                    "label": 1
                },
                {
                    "sent": "They work fine once the problem gets to bid an end dimensional like two or three dimensional, you have too much filling, so people so at some point you stop using those.",
                    "label": 1
                },
                {
                    "sent": "Then you have iterative solvers and they can be as simple as things like conjugate gradient.",
                    "label": 0
                },
                {
                    "sent": "But in practice works much better.",
                    "label": 0
                },
                {
                    "sent": "Are these multigrid techniques?",
                    "label": 0
                },
                {
                    "sent": "Both of these are excellent books to read about an.",
                    "label": 0
                },
                {
                    "sent": "The idea is Dan just told us is that we sort of work between different levels in a pyramid and there's another set of techniques that were kind of introduced in late 80s, and I wrote an early paper on this in 1990 called Hierarchical preconditioners, which is related to multigrade.",
                    "label": 0
                },
                {
                    "sent": "But rather than thinking of it as transferring things between levels, it's more like a basis change.",
                    "label": 0
                },
                {
                    "sent": "So little bit again, it's related to what Alan was talking about this morning with tree based priors.",
                    "label": 0
                },
                {
                    "sent": "So you can think of it like that.",
                    "label": 0
                },
                {
                    "sent": "It's a tree based.",
                    "label": 0
                },
                {
                    "sent": "It's a prior that when you transform the system it has a very nice preconditioning effect.",
                    "label": 0
                },
                {
                    "sent": "So basically.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What's going on is you start with your initial system and then you say, what if I represented the unknown variables X as some kind of a hierarchical wavelet like representation an.",
                    "label": 0
                },
                {
                    "sent": "I try to optimize over the wavelet coefficients and the actual S matrix is typically written as a series of very sparse sort.",
                    "label": 0
                },
                {
                    "sent": "Of course defined operations, so it has a very low operation count, and when you do that, you eventually end up with a equivalent system which were the Hessian.",
                    "label": 0
                },
                {
                    "sent": "Now has the form SAS, and when you do the computation for small enough grids where you can actually compute the condition number.",
                    "label": 0
                },
                {
                    "sent": "You know, because it's small enough, you find that it has a much better condition number.",
                    "label": 0
                },
                {
                    "sent": "So the simple take home messages for a lot of these problems.",
                    "label": 0
                },
                {
                    "sent": "If you just use a wavelet basis, things get a lot better.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Solve it a lot faster.",
                    "label": 0
                },
                {
                    "sent": "Here's an intuitive example of what's going on.",
                    "label": 0
                },
                {
                    "sent": "Let's say we're solving a 1 dimensional problem, but we just represented as a bunch of data points, and we're going to do linear interpolation between.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The data points, so the basis functions are local.",
                    "label": 0
                },
                {
                    "sent": "They're called nodal basis functions.",
                    "label": 0
                },
                {
                    "sent": "But if you represent it as a hierarchal basis so you have some low frequency component in some mid frequency components and so on, you end up with a system that's much better conditioned.",
                    "label": 0
                },
                {
                    "sent": "And like I said, late 80s.",
                    "label": 0
                },
                {
                    "sent": "There were some numerical analysis work.",
                    "label": 0
                },
                {
                    "sent": "I did work in the early 90s and then the computer graphics community in the mid 90s.",
                    "label": 0
                },
                {
                    "sent": "People pick this up and started using it.",
                    "label": 0
                },
                {
                    "sent": "So these kinds of.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Wavelet Preconditioners work very well, and here's an example.",
                    "label": 0
                },
                {
                    "sent": "So we have our little problem.",
                    "label": 0
                },
                {
                    "sent": "We're trying to interpolate this thin plate spline with a with a tear in it, and on the left is if we use four bases of four levels in the wavelet.",
                    "label": 0
                },
                {
                    "sent": "These are very simple wavelets.",
                    "label": 0
                },
                {
                    "sent": "These are just tent wavelets.",
                    "label": 0
                },
                {
                    "sent": "Four and six, and you can see it.",
                    "label": 0
                },
                {
                    "sent": "The six gets you to the solution initially much faster, but what happens is it tends to stall 'cause it's over smoothing.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you actually look at the convergence, then for four levels you get something like this and for six levels you start off faster.",
                    "label": 0
                },
                {
                    "sent": "You can see it drops quicker, but then it doesn't do as well because it's over smoothing.",
                    "label": 0
                },
                {
                    "sent": "So this idea of wave preconditioning on irregular problems is a good idea, but it's not good enough.",
                    "label": 0
                },
                {
                    "sent": "It's not actually paying attention to the underlying structure, and that's why people have invented things like algebraic multigrid where you actually look at the elements in a matrix and say how can we solve it.",
                    "label": 0
                },
                {
                    "sent": "So the question is how?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Can we do better, right?",
                    "label": 0
                },
                {
                    "sent": "So here in 1990 when I sort of stopped working on this problem, I said, well, looks like you just have to tune the number of levels and that's the best we can do.",
                    "label": 1
                },
                {
                    "sent": "And it doesn't really do anything sensible about adapting to things like this continuity's.",
                    "label": 0
                },
                {
                    "sent": "So the question is, can we do better and the goal would be if we can to automatically construct a set of basis functions which adapts the domain and gives you good preconditioning so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Question is, how do we do that?",
                    "label": 0
                },
                {
                    "sent": "Well, I've sort of formulate the problem giving you some intuitions on where we're going.",
                    "label": 0
                },
                {
                    "sent": "Now let me start with a 1 dimensional problem where I can probably show you what the optimal bases are, and they are so provably optimal that they converge in a single iteration.",
                    "label": 0
                },
                {
                    "sent": "They basically preconditioned the system into a diagonal, and let's see how this is done.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is a technique.",
                    "label": 0
                },
                {
                    "sent": "Let's just reformulate the problem like this in one dimension.",
                    "label": 0
                },
                {
                    "sent": "It's the same graphical model, but just restricted to align OK. And now we're going to use a technique that's called cyclic reduction.",
                    "label": 0
                },
                {
                    "sent": "How many people have heard of cyclic reduction?",
                    "label": 0
                },
                {
                    "sent": "Just Alan?",
                    "label": 0
                },
                {
                    "sent": "It's a very old well known technique for doing linear system solving.",
                    "label": 0
                },
                {
                    "sent": "When you have a parallel computer, what you do is rather than sort of walking down your tridiagonal system and doing it salesky and then walking back up, you take all the even variables or all the odd variables and you get rid of them and it turns out you can eliminate them and for a problem like this which is tridiagonal, you end up with a tridiagonal system, so you can in parallel basically eliminate things so you can take all the odd variables.",
                    "label": 0
                },
                {
                    "sent": "You can write them down as a function of the even variables and notice that here.",
                    "label": 0
                },
                {
                    "sent": "We have things like the diagonal element is being used to divide the off diagonal elements, so this should look familiar from dance talk as he has these terms where there's an AIJ divided by the sum of AIIMS.",
                    "label": 0
                },
                {
                    "sent": "It's exactly that kind of thing that's going on, but it's not a heuristic.",
                    "label": 0
                },
                {
                    "sent": "It's not saying, well, these things are weak.",
                    "label": 0
                },
                {
                    "sent": "These things are strong.",
                    "label": 0
                },
                {
                    "sent": "This is exact.",
                    "label": 0
                },
                {
                    "sent": "This is exactly the right way to solve the prob.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And when you do that substitution, what happens is if you start off with this kind of a red black problem, you're going to basically say you're going to say the red values.",
                    "label": 0
                },
                {
                    "sent": "I know if someone gave me the black values, I know exactly what red value each red value should, because it only has to look at its two neighbors.",
                    "label": 0
                },
                {
                    "sent": "You can think of it as it's two parents, and so I can do that substitution in terms of linear algebra.",
                    "label": 0
                },
                {
                    "sent": "What happens is now I have a smaller problem, which is represents exactly the same energy, just defined over fewer variables.",
                    "label": 0
                },
                {
                    "sent": "So we're basically restricting from a larger dimensional.",
                    "label": 0
                },
                {
                    "sent": "Space into a smaller one and solving the small one gives us exactly the information we need to solve the fine complete problem.",
                    "label": 0
                },
                {
                    "sent": "It's not an approximation.",
                    "label": 0
                },
                {
                    "sent": "We aren't going to have to iterate.",
                    "label": 0
                },
                {
                    "sent": "We're going to be able to in one step.",
                    "label": 0
                },
                {
                    "sent": "Basically go up the pyramid form these very small set of equations.",
                    "label": 0
                },
                {
                    "sent": "In theory it could be just one equation and then on the way back down by back substitution we're going.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, exactly the right answer.",
                    "label": 0
                },
                {
                    "sent": "So in terms of matrices, if we permute the variables so that the course variables are in the bottom and define variables are at the top, this interpolation function is this idea of taking the off diagonals and.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Abiding by the diagonal and when you do that, the effective matrix.",
                    "label": 0
                },
                {
                    "sent": "Remember we're trying to precondition the matrix so that it has better eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "Well, if we can get the matrix turned into a diagonal, then it's trivial to do perfect inversion, right?",
                    "label": 0
                },
                {
                    "sent": "Because everyone can invert a diagonal matrix and we're in good shape because the matrix is diagonal in the upper left corner in the lower left corner right corner.",
                    "label": 1
                },
                {
                    "sent": "It's still tridiagonal, and we can recurse on the lower right corner and keep going all the way down until we have a single variable.",
                    "label": 1
                },
                {
                    "sent": "So this is a very nice construction.",
                    "label": 0
                },
                {
                    "sent": "It's not new, but it's just to motivate what we're going to try to do in two dimensions by this exact 1 dimensional algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just to show that it really does work, by the way, their relationships here too, with people in machine learning, called the Junction tree algorithm.",
                    "label": 0
                },
                {
                    "sent": "I think those of you who know about graphical models, how many people know that in a graphical model you all know that if a graphical model is a tree, it's order N right solution, right?",
                    "label": 0
                },
                {
                    "sent": "What if you have a long chain?",
                    "label": 0
                },
                {
                    "sent": "How many of you know that you can basically take any element in the chain and just get rid of it right?",
                    "label": 0
                },
                {
                    "sent": "Because if you have an element in a chain, you can find the effective relationships between the two, the left and the right neighbors, and so the junction tree algorithm.",
                    "label": 0
                },
                {
                    "sent": "One way of thinking about it is you do the easy stuff which is you take the leaf nodes.",
                    "label": 0
                },
                {
                    "sent": "And if you have things in the middle of a chain, you just get rid of them and then you get to the harder parts, which are things like triplets and so on where you get clicks.",
                    "label": 0
                },
                {
                    "sent": "So it's related to the junction tree algorithm.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so in 1D, if I set up a problem like this, which is just a bunch of data points with weights and then this is my smoothness function, so I put a little tear in it.",
                    "label": 0
                },
                {
                    "sent": "Then when I solve the problem, it's kind of interpolating those weights with a little decay towards 0.",
                    "label": 0
                },
                {
                    "sent": "Then it jumps because there's a tear and it solves the next interpolation.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem and if we run this in an actual simulator and compare it to the other older work I did in 1990 which has a fixed number of just non adapted linear basis splines, we see that it has a one step convergence.",
                    "label": 0
                },
                {
                    "sent": "So this is again mostly just back.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Round, so the really interesting question.",
                    "label": 0
                },
                {
                    "sent": "So this is cyclic reduction and it's also related to in the wavelet community there called weighted wavelets.",
                    "label": 1
                },
                {
                    "sent": "Although again the way the weights are computed are often heuristic based on certain desired properties.",
                    "label": 0
                },
                {
                    "sent": "This is an optimal way to get the waiting.",
                    "label": 1
                },
                {
                    "sent": "So the big questions can be extended to 2D Canal and see what's coming.",
                    "label": 0
                },
                {
                    "sent": "Or is that yeah OK.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is the 2D salute.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we're going to basically now do this on a checkerboard, and we're going to use the same idea of a red black, so we're going to try to get rid of the red variables and come up with a similar problem on the black variables.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is we're going to take these and we're going to eliminate that.",
                    "label": 0
                },
                {
                    "sent": "And when you eliminate that, what happens?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is you get these connections, the bandwidth now I told you that the bandwidth of that smaller system was try diagonal in the 1 dimensional case in the two dimensional case it gets bigger and so you're in trouble.",
                    "label": 0
                },
                {
                    "sent": "There are two reasons why you're in trouble.",
                    "label": 0
                },
                {
                    "sent": "One reason is if it gets bigger, you're just going to have to do more computation, but there's a bigger reason that you're in trouble, and the big reason is this that if I wanted then eliminate, let's say this node here, it's no longer tide to only its parents.",
                    "label": 0
                },
                {
                    "sent": "'cause these are going to be the parents at the next level.",
                    "label": 0
                },
                {
                    "sent": "It's also tide to its siblings, so whenever you have a graphical model where you can't partition between things to be illuminated on things that remain, you have couplings and this is one of the reasons I think, qualitatively why multigrid isn't a perfect preconditioner, because you're basically saying let's do an interpolation function based on parent nodes, But the fine notes actually talk to each other, and we're kind of ignoring that you know there's information that percolates just through the fine nodes, like, even if we took all the course nodes and fix them to particular value, define nodes would still sort of negotiate with each other.",
                    "label": 0
                },
                {
                    "sent": "How much they have to smooth?",
                    "label": 0
                },
                {
                    "sent": "So the trick is how can we get this behavior?",
                    "label": 0
                },
                {
                    "sent": "The nice thing is we start with an original problem where we can just eliminate all the red variables.",
                    "label": 0
                },
                {
                    "sent": "But after once a step we're stuck.",
                    "label": 0
                },
                {
                    "sent": "We don't know how to recurse, so the trick would be how do we eliminate these things that I'm going to call diagonals in this diagram?",
                    "label": 0
                },
                {
                    "sent": "The red things aren't actually diagonal, they look horizontal and vertical, but if you think about the black things as being the variables, the blue connections are between nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "The red connections are between things that are sort of diagonally across from you, and there are the ones that you have to get rid of.",
                    "label": 0
                },
                {
                    "sent": "Right, so I'll pause here.",
                    "label": 0
                },
                {
                    "sent": "Does anyone have an idea of how we're going to get rid of these?",
                    "label": 0
                },
                {
                    "sent": "Any suggestions?",
                    "label": 0
                },
                {
                    "sent": "No OK, so if you think about this as a resistive grid, what we're trying to do is basically doing approximation where we want to sort of come up with a resistive grid that is similar, right?",
                    "label": 0
                },
                {
                    "sent": "And if you had to resist this bridge, you wouldn't go in here and just snip resistors, right?",
                    "label": 0
                },
                {
                    "sent": "'cause you're going to remove the you're going to lower the conductance of the overall thing, so even for very low frequency gradient from one edge to the other, if you snip resistors, you know your overall conductance is gone down, so that's not a good idea.",
                    "label": 0
                },
                {
                    "sent": "What we want to do instead.",
                    "label": 0
                },
                {
                    "sent": "Is to basically re distribute the weight, so if we think of it as cutting resistors but then putting additional conductance on the links between the resistors that remain like this.",
                    "label": 0
                },
                {
                    "sent": "So I went by a little fast notice the little arrows I'm going to remove the red things and I'm going to beef up the things that remain and this is a variant that I haven't seen in the numerical analysis literature.",
                    "label": 0
                },
                {
                    "sent": "Per say.",
                    "label": 0
                },
                {
                    "sent": "There are things like Cholesky decomposition where you say let's do incomplete Cholesky, which is actually just dropping off diagonal elements.",
                    "label": 0
                },
                {
                    "sent": "There are modified incomplete tell you where you beef up the central values to keep them strong, but the idea of tweaking other elements in your matrix when you're zeroing out certain ones is not one that I've seen now.",
                    "label": 0
                },
                {
                    "sent": "Maybe in graphical models you know people have done it.",
                    "label": 0
                },
                {
                    "sent": "I'd be interested in hearing about this, but this is the basic trick.",
                    "label": 0
                },
                {
                    "sent": "It basically says how do you do it and.",
                    "label": 0
                },
                {
                    "sent": "As a matter of fact, I'll tell you shortly that we were working with some folks from Yana scooters from Gary Miller's Group at CMU, and this is a trick that's used in linear algebra where they it's called sparsification, where you basically remove certain edges in a graph and beef up other ones so it is.",
                    "label": 0
                },
                {
                    "sent": "It is actually known, but I haven't seen it in the multigrid or incomplete sort of factorization literature, so I guess the question is, will this heuristic work right?",
                    "label": 1
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And So what we do did here is we have a simple problem again with a tear.",
                    "label": 0
                },
                {
                    "sent": "You can see the solution is on the lower right hand corner and if we run regular conjugate gradient after one iteration it hasn't done anything.",
                    "label": 0
                },
                {
                    "sent": "Really.",
                    "label": 0
                },
                {
                    "sent": "We're trying to get to the solution on the right with three levels.",
                    "label": 0
                },
                {
                    "sent": "It's already spreading.",
                    "label": 0
                },
                {
                    "sent": "You can see that white area is spreading, so it's doing good job of smoothing, but with the locally adapted heretical basis functions.",
                    "label": 0
                },
                {
                    "sent": "What we call this 'cause there are article wavelet like functions, but they're locally adapted based.",
                    "label": 0
                },
                {
                    "sent": "On the strengths in the the Hessian matrix, it's gotten along way towards the solution.",
                    "label": 0
                },
                {
                    "sent": "It's actually visually.",
                    "label": 0
                },
                {
                    "sent": "It's very hard to tell if it has or hasn't converged, and after five iterations the classical hierarchical basis functions are getting there.",
                    "label": 0
                },
                {
                    "sent": "You can start to see a little bit of that error showing up, but it's not working nearly as fast and so much easier.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Creative see this is to just look at the convergence plot and again the more levels we use for preconditioning, the better it gets.",
                    "label": 0
                },
                {
                    "sent": "I haven't shown past level equals 4 where starts getting worse.",
                    "label": 0
                },
                {
                    "sent": "Other things Ilu 0 is this idea of basically doing the red black coarsening, but then just dropping things off diagonal that are the wrong connections and it's not doing that great.",
                    "label": 0
                },
                {
                    "sent": "It's kind of in the middle rils view and Emily LUR is where you drop part of the off diagonal things back on the diagonal.",
                    "label": 0
                },
                {
                    "sent": "MLU puts all of it on the diagonal, but then the technique I'm proposing is just.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Much, much faster, and if you look at it in a log plot it's dramatically evident.",
                    "label": 0
                },
                {
                    "sent": "So even though it's a heuristic, it turns out to be a very good heuristic.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that we know that local relaxation let's Dan told us right is good at smoothing high frequencies.",
                    "label": 0
                },
                {
                    "sent": "We want to have a smaller level problem that smoothing course frequencies low low frequencies, right?",
                    "label": 0
                },
                {
                    "sent": "And if you have a resistive grid or graphical model and you know the solution is low frequency then you can remove certain connections and put them elsewhere.",
                    "label": 0
                },
                {
                    "sent": "That's the intuition.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the basic technique and.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's just another visualization of what's going on.",
                    "label": 0
                },
                {
                    "sent": "If we start with a grid like this, then when we reduce it by dropping half the variables, we end up with this grid and the light blue.",
                    "label": 0
                },
                {
                    "sent": "Things are the things we don't want.",
                    "label": 0
                },
                {
                    "sent": "So then we basically get rid of them, and we have the next grid and so on.",
                    "label": 0
                },
                {
                    "sent": "So you just keep recursing like that, so that's this.",
                    "label": 0
                },
                {
                    "sent": "Is a nice way we've seen to visualize what's going on in these grids.",
                    "label": 0
                },
                {
                    "sent": "And as I mentioned already, the previous strategies either just drop the off diagonals or add them back to the diagonal and neither the miss is good as coming up with a better approximation.",
                    "label": 1
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So again, this is the sample to the problem.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is the summary of the algorithms for those of you who do preconditioned conjugate gradient.",
                    "label": 0
                },
                {
                    "sent": "So we basically iterate between computing the residual vector.",
                    "label": 1
                },
                {
                    "sent": "Then we compute this hierarchical residual.",
                    "label": 1
                },
                {
                    "sent": "We divided by the diagonal.",
                    "label": 0
                },
                {
                    "sent": "That's an important step.",
                    "label": 0
                },
                {
                    "sent": "We come back down and that matrix SD inverse transpose is an approximation to the actual original a matrix.",
                    "label": 0
                },
                {
                    "sent": "It's not exact, but it's a good preconditioner, and then we basically, after we've done that preconditioning step, then we.",
                    "label": 0
                },
                {
                    "sent": "Do the rest of the usual conjugate gradient.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so as it turns out, these kinds of algorithms can be easily implemented on a GPU if we want to in practice what you end up doing is you can stop at some level in the pyramid.",
                    "label": 1
                },
                {
                    "sent": "This is fairly standard in multigrid and in a lot of other multiresolution models.",
                    "label": 0
                },
                {
                    "sent": "Is once the model gets small enough to be tractable, you just do a complete inference over the course level model.",
                    "label": 0
                },
                {
                    "sent": "And there are also techniques for very large problems.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Streaming multigrid",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here are some of the examples on computer graphics.",
                    "label": 0
                },
                {
                    "sent": "This is the colorization problem and this is what conjugate gradient does after 10 iterations after 20, so on just keeps going so it gets there eventually, but it's a slow technique and machine learning.",
                    "label": 0
                },
                {
                    "sent": "We've seen some people I mean obviously can't grade is much better than just gradient descent, but if you choose the right preconditioner you can do much better.",
                    "label": 0
                },
                {
                    "sent": "So this is after one iteration with.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The proper preconditioner, and again we see results like.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, this is the personal blending problem.",
                    "label": 0
                },
                {
                    "sent": "That's what the final the final blend looks like.",
                    "label": 0
                },
                {
                    "sent": "So originally you see the big intensity differences between the ground after the blending.",
                    "label": 0
                },
                {
                    "sent": "It looks like that.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And again it has.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Conversions this is the colorization problem with the interactive tone mapping, and that's the result.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Up there, so there's a whole bunch of other applications in computer graphics.",
                    "label": 0
                },
                {
                    "sent": "One I really like a lot is Annette Levins work on closed formatting where she basically ends up solving a large eigensystem by looking at local color differences, but then eventually computing essentially the eigen values in a kind of.",
                    "label": 0
                },
                {
                    "sent": "Cut so these are all techniques all applications in computer graphics and computer vision, where these kinds of fast solvers can be used.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So basically what I've just done is a summary with sort of presenting it more for this audience.",
                    "label": 0
                },
                {
                    "sent": "It's a summary of our 2006 SIGGRAPH paper presented for this audience.",
                    "label": 0
                },
                {
                    "sent": "Now what we've done since then is I've been working with Rob Fergus is student Dilip Krishnan, and we now actually have an analysis of the convergence properties of this, and that's what I want to sort of present in the last little bit of my talk before I start on that.",
                    "label": 0
                },
                {
                    "sent": "How much time do I have left?",
                    "label": 0
                },
                {
                    "sent": "10 minutes, OK, that's about how much material I have.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The question is, how do we actually analyze the convergence and we didn't really know how to do this until Jan is Kutis who is the the second author there?",
                    "label": 0
                },
                {
                    "sent": "Ann has just moved to Puerto Rico came and gave a talk at Microsoft Research.",
                    "label": 0
                },
                {
                    "sent": "He's been a postdoc at CMU with Gary Miller for a number of years, so I'm going to borrow a few of his slides just to introduce the concept of generalized eigenvalues, which is the main analytical tool.",
                    "label": 0
                },
                {
                    "sent": "Be using here.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's the.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Talk and these are just a few selected slides from very nice talk.",
                    "label": 0
                },
                {
                    "sent": "You can think of these problems as weighted Laplacian, so if you're a matrix basically is something like a symmetrically diagonally dominant matrix with negative off diagonals.",
                    "label": 0
                },
                {
                    "sent": "That's equivalent to resistive grid, and so you can think of this as having voltages flowing around.",
                    "label": 0
                },
                {
                    "sent": "Alan this morning was talking about Random Walk S What was the WS was it works?",
                    "label": 0
                },
                {
                    "sent": "Summing right exactly walks something somebody like Grady Leo Grady calls these the random Walker algorithms.",
                    "label": 0
                },
                {
                    "sent": "So basically you can start with from one node and basically walk based on these connections and how far you percolate gives you an idea of the sort of connectivity between things and so potential way of solving, for example a seated interpolation problem.",
                    "label": 0
                },
                {
                    "sent": "So there are nice analogs between.",
                    "label": 0
                },
                {
                    "sent": "These kinds of problems and resistive grids and.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What's also nice is that you have this concept of a support number.",
                    "label": 0
                },
                {
                    "sent": "So let's say I take one grid and I approximated with a different grid by sniping things and changing values somewhere else, and I want to know how close are the two grids?",
                    "label": 0
                },
                {
                    "sent": "Well, the grids, if if that ratio there is identical for every single value, then basically the grid has exactly the same resistance properties no matter what voltages you put in and the only condition under which that that's the case is if the two graphs, if LA&LB, are the same.",
                    "label": 0
                },
                {
                    "sent": "But the ratio being non unity is a nice scalar number that tells you how much energy is dissipated by B.",
                    "label": 0
                },
                {
                    "sent": "If it's an approximation to A and if you take that ratio in its inverse, you get something that's called the generalized condition number and it basically says how close are these two grids to each other.",
                    "label": 0
                },
                {
                    "sent": "How close are these two problems and you want them to be close in both directions, 'cause otherwise if you just said I want one to be smaller than the other, you could just make all the values small or all of them big.",
                    "label": 0
                },
                {
                    "sent": "So this is the basic tool.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're going to use so again this quadratic form V transpose LV is just the energy dissipation in a network, and So what you're trying to do is come up with two networks that have very similar energy dissipation, both for high frequencies and for low frequencies.",
                    "label": 0
                },
                {
                    "sent": "Essentially for all frequencies you want to find the eigenvalue, which is the worst.",
                    "label": 0
                },
                {
                    "sent": "The biggest difference between the two, and that tells you how well the approximation is, and the reason that's important is if we're using one graph to be an approximate inverse of the other graph.",
                    "label": 0
                },
                {
                    "sent": "In that particular eigenvalue where they disagree is the one that's not going to get cancelled as fast as it needs to be.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So theoretically there are nice results which show that if you do very simple things like just snip things and then re distribute the values as connection.",
                    "label": 0
                },
                {
                    "sent": "So for integer graphs like this we have certain end we're embedding B which is more complex, has loops into a non loopy chain.",
                    "label": 0
                },
                {
                    "sent": "There the dilation which is saying for each edge that we cut.",
                    "label": 0
                },
                {
                    "sent": "What's the maximum length that has to traverse in the similar graph the congestion says for any edge in B, how much traffic is it carrying from the original thing?",
                    "label": 0
                },
                {
                    "sent": "And it turns out there's a nice theoretical result that says the condition number is at most the congestion times the dilation.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to use it, but it's a very useful concept.",
                    "label": 0
                },
                {
                    "sent": "If you've got these graphs you're trying to say, how am I going to approximate one graphical model with another?",
                    "label": 0
                },
                {
                    "sent": "Ask yourself the question, how well does this simpler model model the original model and vice versa, right?",
                    "label": 0
                },
                {
                    "sent": "So that's the basic.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thing.",
                    "label": 0
                },
                {
                    "sent": "So now we've got all the tools that we need basically to analyze our locally adaptive hierarchical basis functions, which is basically a series of course innings where, because the graph is sparse enough, I can eliminate certain variables and then sparsification because I want to be able to come back to a graph that's simple or thin enough.",
                    "label": 1
                },
                {
                    "sent": "So that you can basically do it again.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's start with a simple problem like this of just taking our N8.",
                    "label": 0
                },
                {
                    "sent": "The eight neighbor graph and approximating with RN for graph.",
                    "label": 0
                },
                {
                    "sent": "How well does that work?",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What is the relative condition number so I can do that?",
                    "label": 0
                },
                {
                    "sent": "I can basically take an end 9 graph with uniform weights and approximated with an end and for graph, and these are the generalized algen value.",
                    "label": 0
                },
                {
                    "sent": "So if you do the approximation right, you know things like low frequencies are approximated perfectly, but the high frequency checkerboard isn't.",
                    "label": 0
                },
                {
                    "sent": "There's a gap of exactly oh sorry, this isn't the gap, this is just the original eigenvalues of the problem.",
                    "label": 0
                },
                {
                    "sent": "To show you that it's very poorly conditioned, so low frequency has very low spectral or eigenvalue in the high frequency.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are our low.",
                    "label": 0
                },
                {
                    "sent": "This is the approximation, so you approximate low frequencies very accurately.",
                    "label": 0
                },
                {
                    "sent": "The checkerboard is off by factor of two, so you lose a factor of two if you go up.",
                    "label": 0
                },
                {
                    "sent": "If you basically sparse.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "By the graph.",
                    "label": 0
                },
                {
                    "sent": "Now what we're really interested in is not the question of 1 sparsification, but if we use this as a multi level preconditioner and we go up the graph and then we solve the course level problem, what is the relative condition number?",
                    "label": 1
                },
                {
                    "sent": "Because that will tell us how good the preconditioner.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can do this with just doing one step of half octave coarsening and sparsification and we see that the condition numbers are distributed between about 1/4 and 1.7, so that's sort of the eigenvalues and so the condition number.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About five, if we now do this a second time, so we're up a full octave in the hierarchy.",
                    "label": 0
                },
                {
                    "sent": "The condition number is now about 7 and I'm sorry.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Spoke this is for a full octave so this is the traditional way you do multigrid.",
                    "label": 0
                },
                {
                    "sent": "You basically subsample by two horizontally and vertically.",
                    "label": 0
                },
                {
                    "sent": "Then if we.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do that for two levels.",
                    "label": 0
                },
                {
                    "sent": "We get something like.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This an for three levels.",
                    "label": 0
                },
                {
                    "sent": "It's like that, so if you just use the three level full octave bilinear interpolation function, which is what I did in 1990, we basically have a condition number of 10.",
                    "label": 0
                },
                {
                    "sent": "So means you're going to need about sqrt 10 operations to bring down your error by certain factor.",
                    "label": 0
                },
                {
                    "sent": "OK, so the question is, is this half octave adaptive?",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thing any better if you do a half octave, the half.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Active has a condition number of 2.5, but if you do two of them certain things cancel and the condition number is only two.",
                    "label": 0
                },
                {
                    "sent": "The interesting thing also is in the middle there's a flat region and that means you have a bunch of eigenvalues there at one and conjugate gradient likes that it really likes clustered eigenvalues, so there's a certain subspace here where the particular approximation we're doing is doing a very nice job.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we can keep going.",
                    "label": 0
                },
                {
                    "sent": "This is 2 full.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Octaves and this is 3 full octaves, so you see that basically it's still there's a lot of things that are approximated exactly and only a few things that are off the things you have to worry about are the half of the eigenvalues that are off to the left there, but they're between 0.5 and one.",
                    "label": 0
                },
                {
                    "sent": "Oh well, actually there between .26.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And one.",
                    "label": 0
                },
                {
                    "sent": "So what's going on?",
                    "label": 0
                },
                {
                    "sent": "Basically, it looks like about half the eigenvalues for a simple problem, well approximated, so you get those solved exactly for free.",
                    "label": 0
                },
                {
                    "sent": "Most of the rest of them are spread between 1/4 and one, so the condition number is 4, and when you get into small condition numbers, it's good to go back to the real formula, not to remember the square root thing.",
                    "label": 1
                },
                {
                    "sent": "If we take sqrt 4 -- 1, that's 2 -- 1 in the numerator 2 + 1 in the denominator.",
                    "label": 0
                },
                {
                    "sent": "This means that about every iteration you're getting 1/3 the error, which is very fast convergence.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, so I guess the question is this.",
                    "label": 0
                },
                {
                    "sent": "Is this is very exciting, but how does this relate to multigrid?",
                    "label": 0
                },
                {
                    "sent": "I haven't yet talked at all in this presentation about any kind of smoothing, right?",
                    "label": 0
                },
                {
                    "sent": "This is just a transformation of basis functions.",
                    "label": 0
                },
                {
                    "sent": "So can we use or should we use pre and post smoothing steps?",
                    "label": 1
                },
                {
                    "sent": "And how this?",
                    "label": 0
                },
                {
                    "sent": "How does multigrid compared to this technique right?",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we did some simulations anfora fairly smooth problem.",
                    "label": 0
                },
                {
                    "sent": "Here's what it looks like if I use a full octave coarsening, which is the blue line HBF, I get that kind of convergence and a simple multigrid V sweep has the same performance.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "The thing to remember is that the V sweep is not a conjugate gradient algorithm, it's just a fixed step size keeps going, so you can do better by either computing adaptive step size or using it as a preconditioner, which is the purple line, but each V sweep takes about three times the amount of work, or maybe twice the amount of work.",
                    "label": 0
                },
                {
                    "sent": "A regular preconditioning 'cause doing a relaxation involves comparing yourself with a stencil of your neighbors and then interpolating, doing the prolongation interpolations about the same amount of work.",
                    "label": 0
                },
                {
                    "sent": "So these sweeps or multigrid because it has the smoothing is maybe two or three times slower than a simple change of basis things, so they are comparable.",
                    "label": 0
                },
                {
                    "sent": "But then when we go to the half octave thing that green thing which is both faster and the sweeping converges faster.",
                    "label": 0
                },
                {
                    "sent": "So this seems to be for regular problems, just something that outperforms multigrid we want to.",
                    "label": 0
                },
                {
                    "sent": "Do a lot more careful experiments before we claim that, but this may be a technique that's just outperforms classical geometric multigrid.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then if we use and.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A regular problem like this thing with that error.",
                    "label": 0
                },
                {
                    "sent": "Then there is no comparison because classical geometric multigrid is just doesn't work well.",
                    "label": 0
                },
                {
                    "sent": "We'd love to compare it with algebraic multigrid, but we're still looking for a good implementation.",
                    "label": 0
                },
                {
                    "sent": "Maybe Dan can help me sort of find one that we could benchmark against that.",
                    "label": 0
                },
                {
                    "sent": "Let's see how well it works.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, let me mention a few other extensions that Dilip and I have discovered as we're doing this richly.",
                    "label": 0
                },
                {
                    "sent": "I started this problem.",
                    "label": 0
                },
                {
                    "sent": "This talk by saying if you start with a four connected grid, here's how we can basically solve it now.",
                    "label": 0
                },
                {
                    "sent": "It turns out if you think about it, we can actually start with the 9 connected grid very easily and just apply this thing that we were using later.",
                    "label": 0
                },
                {
                    "sent": "Applied early so you can solve non connected problems.",
                    "label": 0
                },
                {
                    "sent": "Now in theory you could start with larger graphs with more connectivity, but I'm not on such firm footing house.",
                    "label": 0
                },
                {
                    "sent": "How to do it.",
                    "label": 0
                },
                {
                    "sent": "So if we're doing normalized cuts with arbitrary affinity's between distant things.",
                    "label": 0
                },
                {
                    "sent": "It's not really clear to me that you can go in and snip resistors and replace them, or at least that's you know significantly more research to figure out how to do that, but the basic intuition of sparsifying graphs seems to be useful.",
                    "label": 0
                },
                {
                    "sent": "Another thing we noticed is that actually you don't have to cut all of the diagonals because you only need to cut things that are going to be illuminated in parallel.",
                    "label": 0
                },
                {
                    "sent": "So we can actually get by.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With less cutting.",
                    "label": 0
                },
                {
                    "sent": "OK, and that's this thing here.",
                    "label": 0
                },
                {
                    "sent": "So you only need to eliminate the connections between the Reds and the Reds if you're going to eliminate the Reds then so you can have these extra diagonals between things that are going to remain as parents.",
                    "label": 0
                },
                {
                    "sent": "And it also works.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the final question, which is when I've been trying to work on this week and I didn't get the code done in time is can we add smoothing, tarako preconditioning and the answer is I don't have the the answer yet, but it's possible that.",
                    "label": 1
                },
                {
                    "sent": "I mean it's almost certain that the smoothing will help.",
                    "label": 0
                },
                {
                    "sent": "The question is whether it will be worth the extra computational effort and that's where I have to implement it to see because the nice thing is with these preconditions you're already getting a fairly clean split between the fine level variables in the course level variables.",
                    "label": 0
                },
                {
                    "sent": "Discoursing procedure doesn't introduce any approximation error, unlike wavelets, which might not be perfectly orthogonal or something like that, but it's the coarsening that isn't approximate.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Step.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is my last slide before my conclusions an let me mention the limitations.",
                    "label": 0
                },
                {
                    "sent": "So the big limitation for this technique is that if your problem becomes highly irregular, it just doesn't work.",
                    "label": 1
                },
                {
                    "sent": "You can't take something like for example problem where you embedded a 1 dimensional chain as a spiral and two dimensional problem and expect something that is using a series of checkerboards to approximate it because at some point you can't approximate a 1 dimensional chain with twenty turns on something that's like a four by four grid.",
                    "label": 0
                },
                {
                    "sent": "So for very.",
                    "label": 0
                },
                {
                    "sent": "Irregular problems we need something like algebraic multigrid, or the stuff that quote system those people are doing, which is called combinatorial multigrid, which is a different agglomeration strategy, right?",
                    "label": 1
                },
                {
                    "sent": "So I think basically we need to combine both these approaches, and that's interesting future work, but that's probably could be at least a year or maybe thesis is worth of work to work that out.",
                    "label": 0
                },
                {
                    "sent": "I think it's going to be very promising when.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It gets done so.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Other potential future work well.",
                    "label": 1
                },
                {
                    "sent": "OK, so extending it to 2nd order problems like thin plates is something that's interesting, but we're not really sure how to do it.",
                    "label": 0
                },
                {
                    "sent": "The other thing that I think is interesting for this audience is that this may be a guideline for how to do this for non quadratic non Gaussian Markov random fields, because if you have other potentials like for binary things actually Richard Hartley in the student car showed that for binary Markov random fields you can do this kind of coarsening and he was trying to go from problems that were not submitted.",
                    "label": 1
                },
                {
                    "sent": "Modular until the course into one that was submodular, and then he used that as initialization for the more difficult problem.",
                    "label": 0
                },
                {
                    "sent": "But the difficulty is that if you have non quadratic energies you can't.",
                    "label": 0
                },
                {
                    "sent": "The energies don't collapse in the same nice way, so you may have sort of more complicated interaction potentials and things like that.",
                    "label": 0
                },
                {
                    "sent": "Last line I threw in after hearing Alan's talk this morning because he thinks about different kinds of trees and he mentioned thinning, which is I think related to what we're doing here.",
                    "label": 0
                },
                {
                    "sent": "So these are all open questions, but I think.",
                    "label": 1
                },
                {
                    "sent": "The general idea of trying to be very judicious about eliminating things and then approximating energy so that may have a lot of benefits in other kinds of graphical models.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is my conclusion slide.",
                    "label": 0
                },
                {
                    "sent": "Basically we've developed something that's a very effective preconditioner.",
                    "label": 0
                },
                {
                    "sent": "It's useful in a bunch of applications, and maybe is also useful as a general framework for other graphical models.",
                    "label": 1
                },
                {
                    "sent": "So thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Yes, Alan.",
                    "label": 0
                },
                {
                    "sent": "I got 47 comments.",
                    "label": 0
                },
                {
                    "sent": "So going back to early things, we talked about using the tents right?",
                    "label": 0
                },
                {
                    "sent": "I talked about the problem with this one collection for grounding.",
                    "label": 0
                },
                {
                    "sent": "That's right, midpoint selections.",
                    "label": 0
                },
                {
                    "sent": "Data points collected.",
                    "label": 0
                },
                {
                    "sent": "That's right, however, you can use the same thing.",
                    "label": 0
                },
                {
                    "sent": "The same formulation works for any one dimensional Markov process.",
                    "label": 0
                },
                {
                    "sent": "That's right interpolation.",
                    "label": 0
                },
                {
                    "sent": "Exactly what you were doing with not non tenant basis and in one D. This works exactly the same, it's just a different elimination order for the inference.",
                    "label": 0
                },
                {
                    "sent": "Other comment which is great but not not done things on the second red black, but I talked about the idea of doing cavity models where you eliminate right 1D double correspond to not doing cyclic reduction.",
                    "label": 0
                },
                {
                    "sent": "I've got a bunch of places in it.",
                    "label": 0
                },
                {
                    "sent": "I'm going to reduce.",
                    "label": 0
                },
                {
                    "sent": "Groups of them until they believe that's right, Yep.",
                    "label": 0
                },
                {
                    "sent": "Exactly the same problem because you get filled.",
                    "label": 0
                },
                {
                    "sent": "Right when I talked about this.",
                    "label": 0
                },
                {
                    "sent": "Projection, which is the idea of people going to get in graph and then I'm going to match the correlations that match under the grip Max entropy, right?",
                    "label": 0
                },
                {
                    "sent": "I haven't thought about it this way.",
                    "label": 0
                },
                {
                    "sent": "In your context, correspond to how do I strengthen those other connectivities, right?",
                    "label": 0
                },
                {
                    "sent": "So only other context that I know it's not great, but certainly the things we've done things that have that kind of.",
                    "label": 0
                },
                {
                    "sent": "Interesting that I want to understand better.",
                    "label": 0
                },
                {
                    "sent": "How do in fact, you're out having to do those kind of things, whether it has an interpretation?",
                    "label": 0
                },
                {
                    "sent": "Sure, yeah, I think there could be some very interesting connections there, yeah?",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "I guess yes.",
                    "label": 0
                },
                {
                    "sent": "Something that's going on that is adapting to lose weight, so it's the simple averaging, so the actual right I did I didn't have the formula for the redistribution, but if I bring up the right figure I can kind of show it.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this OK.",
                    "label": 0
                },
                {
                    "sent": "So the formula we have is actually heuristic, which basically by doing some case analysis we said if there are certain connections between nodes that have no resistance, then don't put anything there because those we were working when I did the work in 2006.",
                    "label": 0
                },
                {
                    "sent": "I was thinking of this as a physical model of a thin plate with cuts and I said if there's a cut you don't want to bridge over it, because then you're going to get low frequency coupling between things.",
                    "label": 0
                },
                {
                    "sent": "So I said don't bridge anything and by sort of into it just by pure intuition I said well if one resistor on one side.",
                    "label": 0
                },
                {
                    "sent": "You get to re distribute.",
                    "label": 0
                },
                {
                    "sent": "Let's say this unwanted red thing to either.",
                    "label": 0
                },
                {
                    "sent": "Well, no, that's not a good example.",
                    "label": 0
                },
                {
                    "sent": "So let's take this horizontal red thing.",
                    "label": 0
                },
                {
                    "sent": "Here we get to re distributed across the four blue things that are the diamonds.",
                    "label": 0
                },
                {
                    "sent": "And if the resistance or these are actually all conductances, if the conductance is much weaker on one side, then you want to push less stuff there because you're basically increasing the smoothness in an area that wasn't very smooth.",
                    "label": 0
                },
                {
                    "sent": "So I did a two way split back and then I split something along the way.",
                    "label": 0
                },
                {
                    "sent": "Dilip and I started doing an analysis based on the condition numbers and we haven't finished it, but I think the right way to do this is exactly to use what could shows is to just look at a small clique and say what is the theoretically optimal.",
                    "label": 0
                },
                {
                    "sent": "If we have just four values that we have to increase, what's the one that has the best local behavior on the conditioning number?",
                    "label": 0
                },
                {
                    "sent": "So the answer is I have an old heuristic.",
                    "label": 0
                },
                {
                    "sent": "I think there's more principled work to be done.",
                    "label": 0
                },
                {
                    "sent": "Yes, but you know, in this work in terms of the set up, if I have to solve a little 4 by 4 system or something like that, I'm hoping that if we do enough of it will have intuitions and there will be some simple algebraic formula that works, but it's work that remains to be done, yeah?",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                }
            ]
        }
    }
}