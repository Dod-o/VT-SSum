{
    "id": "wpltrla2aw5e6koxqlcsqje3w3ioqa6g",
    "title": "Learning Structured Outputs via Kernel Dependency Estimation and Stochastic Grammars",
    "info": {
        "author": [
            "Andrea Passerini, University of Firenze"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "July 2006",
        "category": [
            "Top->Computer Science->Machine Learning->Structured Output"
        ]
    },
    "url": "http://videolectures.net/oh06_passerini_lsovk/",
    "segmentation": [
        [
            "Dealing with structured outputs with structures which are kinda undersea estimation and stochastic grammars.",
            "Oh"
        ],
        [
            "So I will just.",
            "Give the idea in one slide because I think it's very simple, so we talk about sector output prediction and what we assume is that output structures are generated by a stochastic grammar.",
            "OK, but The thing is that the probabilities of this grammar are example dependent.",
            "OK, so you cannot use, let's say, an estimate of these probabilities on the entire data set in order to be able to parse.",
            "It's in the sequence because it won't work.",
            "That would mean that you have something like context sensitiveness in the grammar, something like that, and so the idea is that we rally on the Napa Fisher space, which is simply the space of all production rules of grammar, and we actually map the structure that is, the parts of the sequence into the frequencies of single production rules.",
            "And once we have done that and we can do this quite easily with this vector output.",
            "In prediction map estimation methods like doing the framework of kernel density dependency estimation, then you can do compute the preimage simply by inside.",
            "Outside procedure might be timing procedure using the fact that you have example.",
            "You have estimated example dependent probabilities to attach to the grammar OK.",
            "So."
        ],
        [
            "I'm just playing it a little bit better.",
            "So the framework is that of kind of dependency estimation, which is, I think, let me know.",
            "I would say it's a little bit the alternative to the idea of having like.",
            "Junk fishing space for input and output.",
            "In this case you have OK and input mapping with associated kernel and output mapping from Wild tool FY with the associated output kernel and the process of structured output prediction goes into steps.",
            "The first one estimates the output features from the inputs with the function function G and the second one is to compute the preimage.",
            "From the output feature space to the output and of course this one is not invertible usually, so you have to estimated.",
            "So the fish."
        ],
        [
            "The first step is quite simple.",
            "You have this examples XI side of why I and you have to estimate this this function from XI to Y and what we can do.",
            "What we did was simply to apply Kernel Ridge regression OK, but on the vector output which is simply very simple extension of the individual session for this color output.",
            "Where here this one is the metrics of output features.",
            "Each column is a is the output feature mapping of a single Y in the training set, and this one is a Colonel in the input to plus regularization and this one is the metrics of the weights.",
            "That means that in the solution G of X is simply the sum of the support vectors of these columns of parameters.",
            "Times the Colonel on the input and of course efficient alternatives exist.",
            "I would I would have talked about the maximum margin regression, but Santos made that you have to talk about maximum margin robots, but I mean I think it's an interesting idea.",
            "So."
        ],
        [
            "For the second step, which is the preimage calculation.",
            "It was proposed typically was that of trying to find the way which minimized sort of distance between what you predicted and the image of the Y right, and it was shown that if you have G of X estimated using candidate regression, this problem doesn't for force you to have explicit out feature mapping, but you can use the kernel.",
            "And so the problem is still to find the dark mean of this dysfunction.",
            "And what Karina Talena reason to ice email paper did was to use a graph theoretical algorithm.",
            "When you had outputs which were swings, and you use the playground, output candles on the output.",
            "And our idea was simply to stochastic grammars, that is.",
            "You have the stochastic grammar with.",
            "I would say with the probabilities on rolls."
        ],
        [
            "Which depends on the examples and of course you don't know them on on test examples.",
            "The output feature mapping will be a real vector which encodes such probability for a single example.",
            "And once you have estimated this album official mapping with the multiple regression, you can use the probabilistic parser in order to do that pretty much step that is found in most popular parts.",
            "Given this grammar and estimated probabilities.",
            "Thanks.",
            "And to be more concrete, I will talk about."
        ],
        [
            "Plastic context free grammars.",
            "Because we have parsers, and So what we do here we have production rules on this form from a Cato Alpha L. OK, we're often at least union of nonterminals, terminals and now.",
            "And what you do is that the probabilities associated to the rules of course have to sum to 1 four rules which share the same hand OK. And what we do is as Fisher Vector we use the feature vector which is related to this probabilities by the softmax.",
            "And what we estimate this one that means simply that he used something like.",
            "We do a generalized linear model OK.",
            "Allows us to to have outputs which don't have to be like to sum to one and to be all in 01 course so you don't have to put constraints on the output vector.",
            "And that's it.",
            "I mean, once you you have obtained this, you apply stochastic context free grammar parsers to compute the most probable parts given this probabilities.",
            "So why all this stuff could be interesting.",
            "What do we have to prove is that this can of course work better than a simple parser.",
            "OK, standard parts which has global probabilities and how we try to show it with an artificial example.",
            "We tried to simulate the PP attachment ambiguity resolution problem which is classical in NLP and which easily explained.",
            "With this example.",
            "It this other way the fork we fork is, let's say characteristic of.",
            "Eating OK, so that means the preposition phrase is directly attached to the verb, while if the salad with tomatoes with tomatoes is attached to is a property of the silent and that means the prepositional phrase attached the noun phrase.",
            "And you cannot like discriminate between these."
        ],
        [
            "To simply with this in Vietnam are OK because there is a big booty and so you need the lexicalization in order to resolve this ambiguity, OK?",
            "So."
        ],
        [
            "Wish we just very simple grammar.",
            "OK here alphabetic letters are non terminals, then lowercase R pre terminals and then digits are terminals.",
            "And what is I mean probabilities here are all uniform except for this one which is the sentence which goes with probability 0.2 in a conjunction conjunction of sentences or in the.",
            "0.8 in noun phrase and verb phrase.",
            "But what is interesting in this grammar is simply this rule OK, which says that phrase can go in in this case.",
            "OK, you have the first example.",
            "It the salad with a fork.",
            "OK, so noun phrase and prepositional phrases are attached to our.",
            "Like children of the same phrase or the verb phrase in the second case you are, you have the salad.",
            "The salad with tomatoes, OK?",
            "So we complicated things.",
            "Once we generated data set according to this grammar and this probabilities, we complicated things by collapsing the two oh it terminals for the verbs BMW in a single one, OK?",
            "In this way, the standard parser, which had probabilities estimated on their time data set OK, couldn't really disambiguate within these two cases, right?",
            "And that's actually what happens because."
        ],
        [
            "OK, and how we generated the data set with this simple language generator program?",
            "OK, we randomly generated bunch of sentences.",
            "We post process them in two ways International, one with the simply duplicate input sentences and the unique one.",
            "We made it more difficult because we filtered out sentences which had identical representation on the artificial space FY.",
            "And what happens?",
            "Is that OK, I run into the results, but I have to explain how."
        ],
        [
            "So we compared our solution which we called with this ugly name KD SC FV with the standard parts are we use the spectrum Colonel on the input on there, which is the sequence of terminals with came areas of sites to five.",
            "And we used to have all be program in order to compute the bracketing F measure and the exact parts matches score.",
            "So the first Martin is more loose, the second one is really to get all the parts 3 correct?",
            "We randomly speed the data set in two sets and we use the first one for model selection.",
            "During the five fold transportation and the second one for performance evaluation with another 5, four transportation and what you get is that.",
            "In OK."
        ],
        [
            "This we compared the results for the entire data set and for the short sequences only, so less than 35 terminals OK, and you know cases you have that the for architecture is much better than the simpler parts are, and these differences are really.",
            "Significant much more than 99.5% and that simply shows that you catch something like a little bit of context sensitiveness.",
            "You catch it on an example level, not on a like single production rule level on a single, let's say local level, but this surface is to be better than using global probabilities only, and so this is I think it's very simple idea and it's simply using.",
            "Known.",
            "Quickly, I know you said it already.",
            "What was the difference with the unique?",
            "Why was unique so much right?",
            "The difference is that in the.",
            "A unique case you?",
            "We assured that in the data set you didn't have two sentences which were mapped to the same output feature from one?",
            "Yeah, yeah.",
            "So yeah, performances are much really has to generalize it.",
            "Yes?"
        ],
        [
            "So we I mean we are thinking of applying it more generally to any problems because we would like to see how how well it it's it works or real world data and further extensions we are thinking of is to use probabilistic IRP instead of stochastic country grammars and so.",
            "Programs which, let's say you learn the probabilities with each program, should explain target concept.",
            "Streams.",
            "So let's take."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dealing with structured outputs with structures which are kinda undersea estimation and stochastic grammars.",
                    "label": 0
                },
                {
                    "sent": "Oh",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I will just.",
                    "label": 0
                },
                {
                    "sent": "Give the idea in one slide because I think it's very simple, so we talk about sector output prediction and what we assume is that output structures are generated by a stochastic grammar.",
                    "label": 1
                },
                {
                    "sent": "OK, but The thing is that the probabilities of this grammar are example dependent.",
                    "label": 0
                },
                {
                    "sent": "OK, so you cannot use, let's say, an estimate of these probabilities on the entire data set in order to be able to parse.",
                    "label": 0
                },
                {
                    "sent": "It's in the sequence because it won't work.",
                    "label": 0
                },
                {
                    "sent": "That would mean that you have something like context sensitiveness in the grammar, something like that, and so the idea is that we rally on the Napa Fisher space, which is simply the space of all production rules of grammar, and we actually map the structure that is, the parts of the sequence into the frequencies of single production rules.",
                    "label": 0
                },
                {
                    "sent": "And once we have done that and we can do this quite easily with this vector output.",
                    "label": 0
                },
                {
                    "sent": "In prediction map estimation methods like doing the framework of kernel density dependency estimation, then you can do compute the preimage simply by inside.",
                    "label": 0
                },
                {
                    "sent": "Outside procedure might be timing procedure using the fact that you have example.",
                    "label": 0
                },
                {
                    "sent": "You have estimated example dependent probabilities to attach to the grammar OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm just playing it a little bit better.",
                    "label": 0
                },
                {
                    "sent": "So the framework is that of kind of dependency estimation, which is, I think, let me know.",
                    "label": 0
                },
                {
                    "sent": "I would say it's a little bit the alternative to the idea of having like.",
                    "label": 0
                },
                {
                    "sent": "Junk fishing space for input and output.",
                    "label": 0
                },
                {
                    "sent": "In this case you have OK and input mapping with associated kernel and output mapping from Wild tool FY with the associated output kernel and the process of structured output prediction goes into steps.",
                    "label": 1
                },
                {
                    "sent": "The first one estimates the output features from the inputs with the function function G and the second one is to compute the preimage.",
                    "label": 0
                },
                {
                    "sent": "From the output feature space to the output and of course this one is not invertible usually, so you have to estimated.",
                    "label": 0
                },
                {
                    "sent": "So the fish.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The first step is quite simple.",
                    "label": 0
                },
                {
                    "sent": "You have this examples XI side of why I and you have to estimate this this function from XI to Y and what we can do.",
                    "label": 0
                },
                {
                    "sent": "What we did was simply to apply Kernel Ridge regression OK, but on the vector output which is simply very simple extension of the individual session for this color output.",
                    "label": 1
                },
                {
                    "sent": "Where here this one is the metrics of output features.",
                    "label": 0
                },
                {
                    "sent": "Each column is a is the output feature mapping of a single Y in the training set, and this one is a Colonel in the input to plus regularization and this one is the metrics of the weights.",
                    "label": 0
                },
                {
                    "sent": "That means that in the solution G of X is simply the sum of the support vectors of these columns of parameters.",
                    "label": 0
                },
                {
                    "sent": "Times the Colonel on the input and of course efficient alternatives exist.",
                    "label": 0
                },
                {
                    "sent": "I would I would have talked about the maximum margin regression, but Santos made that you have to talk about maximum margin robots, but I mean I think it's an interesting idea.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For the second step, which is the preimage calculation.",
                    "label": 0
                },
                {
                    "sent": "It was proposed typically was that of trying to find the way which minimized sort of distance between what you predicted and the image of the Y right, and it was shown that if you have G of X estimated using candidate regression, this problem doesn't for force you to have explicit out feature mapping, but you can use the kernel.",
                    "label": 0
                },
                {
                    "sent": "And so the problem is still to find the dark mean of this dysfunction.",
                    "label": 0
                },
                {
                    "sent": "And what Karina Talena reason to ice email paper did was to use a graph theoretical algorithm.",
                    "label": 1
                },
                {
                    "sent": "When you had outputs which were swings, and you use the playground, output candles on the output.",
                    "label": 0
                },
                {
                    "sent": "And our idea was simply to stochastic grammars, that is.",
                    "label": 0
                },
                {
                    "sent": "You have the stochastic grammar with.",
                    "label": 0
                },
                {
                    "sent": "I would say with the probabilities on rolls.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which depends on the examples and of course you don't know them on on test examples.",
                    "label": 0
                },
                {
                    "sent": "The output feature mapping will be a real vector which encodes such probability for a single example.",
                    "label": 1
                },
                {
                    "sent": "And once you have estimated this album official mapping with the multiple regression, you can use the probabilistic parser in order to do that pretty much step that is found in most popular parts.",
                    "label": 0
                },
                {
                    "sent": "Given this grammar and estimated probabilities.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "And to be more concrete, I will talk about.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Plastic context free grammars.",
                    "label": 0
                },
                {
                    "sent": "Because we have parsers, and So what we do here we have production rules on this form from a Cato Alpha L. OK, we're often at least union of nonterminals, terminals and now.",
                    "label": 0
                },
                {
                    "sent": "And what you do is that the probabilities associated to the rules of course have to sum to 1 four rules which share the same hand OK. And what we do is as Fisher Vector we use the feature vector which is related to this probabilities by the softmax.",
                    "label": 1
                },
                {
                    "sent": "And what we estimate this one that means simply that he used something like.",
                    "label": 1
                },
                {
                    "sent": "We do a generalized linear model OK.",
                    "label": 0
                },
                {
                    "sent": "Allows us to to have outputs which don't have to be like to sum to one and to be all in 01 course so you don't have to put constraints on the output vector.",
                    "label": 1
                },
                {
                    "sent": "And that's it.",
                    "label": 0
                },
                {
                    "sent": "I mean, once you you have obtained this, you apply stochastic context free grammar parsers to compute the most probable parts given this probabilities.",
                    "label": 0
                },
                {
                    "sent": "So why all this stuff could be interesting.",
                    "label": 0
                },
                {
                    "sent": "What do we have to prove is that this can of course work better than a simple parser.",
                    "label": 0
                },
                {
                    "sent": "OK, standard parts which has global probabilities and how we try to show it with an artificial example.",
                    "label": 0
                },
                {
                    "sent": "We tried to simulate the PP attachment ambiguity resolution problem which is classical in NLP and which easily explained.",
                    "label": 0
                },
                {
                    "sent": "With this example.",
                    "label": 0
                },
                {
                    "sent": "It this other way the fork we fork is, let's say characteristic of.",
                    "label": 0
                },
                {
                    "sent": "Eating OK, so that means the preposition phrase is directly attached to the verb, while if the salad with tomatoes with tomatoes is attached to is a property of the silent and that means the prepositional phrase attached the noun phrase.",
                    "label": 0
                },
                {
                    "sent": "And you cannot like discriminate between these.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To simply with this in Vietnam are OK because there is a big booty and so you need the lexicalization in order to resolve this ambiguity, OK?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Wish we just very simple grammar.",
                    "label": 0
                },
                {
                    "sent": "OK here alphabetic letters are non terminals, then lowercase R pre terminals and then digits are terminals.",
                    "label": 0
                },
                {
                    "sent": "And what is I mean probabilities here are all uniform except for this one which is the sentence which goes with probability 0.2 in a conjunction conjunction of sentences or in the.",
                    "label": 1
                },
                {
                    "sent": "0.8 in noun phrase and verb phrase.",
                    "label": 0
                },
                {
                    "sent": "But what is interesting in this grammar is simply this rule OK, which says that phrase can go in in this case.",
                    "label": 0
                },
                {
                    "sent": "OK, you have the first example.",
                    "label": 0
                },
                {
                    "sent": "It the salad with a fork.",
                    "label": 0
                },
                {
                    "sent": "OK, so noun phrase and prepositional phrases are attached to our.",
                    "label": 0
                },
                {
                    "sent": "Like children of the same phrase or the verb phrase in the second case you are, you have the salad.",
                    "label": 0
                },
                {
                    "sent": "The salad with tomatoes, OK?",
                    "label": 0
                },
                {
                    "sent": "So we complicated things.",
                    "label": 1
                },
                {
                    "sent": "Once we generated data set according to this grammar and this probabilities, we complicated things by collapsing the two oh it terminals for the verbs BMW in a single one, OK?",
                    "label": 1
                },
                {
                    "sent": "In this way, the standard parser, which had probabilities estimated on their time data set OK, couldn't really disambiguate within these two cases, right?",
                    "label": 0
                },
                {
                    "sent": "And that's actually what happens because.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and how we generated the data set with this simple language generator program?",
                    "label": 1
                },
                {
                    "sent": "OK, we randomly generated bunch of sentences.",
                    "label": 0
                },
                {
                    "sent": "We post process them in two ways International, one with the simply duplicate input sentences and the unique one.",
                    "label": 1
                },
                {
                    "sent": "We made it more difficult because we filtered out sentences which had identical representation on the artificial space FY.",
                    "label": 0
                },
                {
                    "sent": "And what happens?",
                    "label": 0
                },
                {
                    "sent": "Is that OK, I run into the results, but I have to explain how.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we compared our solution which we called with this ugly name KD SC FV with the standard parts are we use the spectrum Colonel on the input on there, which is the sequence of terminals with came areas of sites to five.",
                    "label": 0
                },
                {
                    "sent": "And we used to have all be program in order to compute the bracketing F measure and the exact parts matches score.",
                    "label": 1
                },
                {
                    "sent": "So the first Martin is more loose, the second one is really to get all the parts 3 correct?",
                    "label": 1
                },
                {
                    "sent": "We randomly speed the data set in two sets and we use the first one for model selection.",
                    "label": 1
                },
                {
                    "sent": "During the five fold transportation and the second one for performance evaluation with another 5, four transportation and what you get is that.",
                    "label": 0
                },
                {
                    "sent": "In OK.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This we compared the results for the entire data set and for the short sequences only, so less than 35 terminals OK, and you know cases you have that the for architecture is much better than the simpler parts are, and these differences are really.",
                    "label": 1
                },
                {
                    "sent": "Significant much more than 99.5% and that simply shows that you catch something like a little bit of context sensitiveness.",
                    "label": 0
                },
                {
                    "sent": "You catch it on an example level, not on a like single production rule level on a single, let's say local level, but this surface is to be better than using global probabilities only, and so this is I think it's very simple idea and it's simply using.",
                    "label": 0
                },
                {
                    "sent": "Known.",
                    "label": 0
                },
                {
                    "sent": "Quickly, I know you said it already.",
                    "label": 0
                },
                {
                    "sent": "What was the difference with the unique?",
                    "label": 0
                },
                {
                    "sent": "Why was unique so much right?",
                    "label": 0
                },
                {
                    "sent": "The difference is that in the.",
                    "label": 0
                },
                {
                    "sent": "A unique case you?",
                    "label": 0
                },
                {
                    "sent": "We assured that in the data set you didn't have two sentences which were mapped to the same output feature from one?",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "So yeah, performances are much really has to generalize it.",
                    "label": 0
                },
                {
                    "sent": "Yes?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we I mean we are thinking of applying it more generally to any problems because we would like to see how how well it it's it works or real world data and further extensions we are thinking of is to use probabilistic IRP instead of stochastic country grammars and so.",
                    "label": 1
                },
                {
                    "sent": "Programs which, let's say you learn the probabilities with each program, should explain target concept.",
                    "label": 0
                },
                {
                    "sent": "Streams.",
                    "label": 0
                },
                {
                    "sent": "So let's take.",
                    "label": 0
                }
            ]
        }
    }
}