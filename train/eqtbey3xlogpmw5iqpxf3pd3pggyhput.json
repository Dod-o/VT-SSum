{
    "id": "eqtbey3xlogpmw5iqpxf3pd3pggyhput",
    "title": "Multi-Classification by Categorical Features via Clustering",
    "info": {
        "author": [
            "Yevgeny Seldin, Department of Computer Science, University of Copenhagen"
        ],
        "published": "Aug. 7, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Clustering"
        ]
    },
    "url": "http://videolectures.net/icml08_seldin_mcc/",
    "segmentation": [
        [
            "Well thanks for staying till the last talk of the last session.",
            "I have Jenna, Selden and I will tell you about multi classification by categorical features.",
            "We are clustering and this is a joint work with nationalities, beer and we are both from the Hebrew University of Jerusalem.",
            "And as the title says, the talk is going to be both on classification and clustering, and I hope that both communities will be able to come up with something out of this."
        ],
        [
            "Look so I will start with an example of multi classification by categorical features and the example I give so it's not seen very well, but it's collaborative filtering data, so in collaborative filtering we have viewers and we have movies.",
            "And each year here we have about 1000 viewers by 1500 movies, when a viewer number, for example 36, will give rating of five to movie number 57 will have a red dot within this space and you see that there are relatively few ratings that are present at most of the ratings are missing and our task is to predict the ratings based on Viewer ID and.",
            "Movie idea, so we rented movie already out to categorical parameters of features that were working with and the rating is the label.",
            "And we approach this prob."
        ],
        [
            "Them by clustering the viewers into clusters of yours and clustering movies into clusters of movies.",
            "This kind of approach defines agreed partition of our parameter space and then within each page in this partition we're going to to choose some prediction strategy.",
            "So for example, in this page there are more ratings that are close to five, so we will predict rating for all the page.",
            "And here's the ratings are closer to one.",
            "We will predict one for all the page.",
            "And given when we have to predict the rating for some viewer and movie, will will see in which page this pair of parameters bring us and give the prediction of that page.",
            "And actually this is a real picture from our previous works presented at NIPS 2006.",
            "Our same kind of data appears in many other applications, not limited to collaborative filtering.",
            "For example, gene expression data and text analysis data, and so on.",
            "In some cases, we are more interested in predicting the label.",
            "In other cases, like for example, gene expression data will be more interested in the clustering itself, so may be interested in what are the groups of genes that are related to each other.",
            "And the question."
        ],
        [
            "Ask in our paper is how how to partition this data.",
            "So should we put everything in one big cluster should partition it as shown in the slide previously or should we take some other form of partition and there are two extreme cases of this problem that give a good intuition about what's happening if we put everything in one big cluster we are able to estimate the average rating of everybody.",
            "Quite quite good, but this is a global average that is going to be quite far from every particular entry.",
            "On the other hand, when we put every viewer and every movie in a separate cluster and then we will be focused on that specific entry.",
            "But we will have very little or maybe even know data within that point.",
            "So the question is how to partition?",
            "How to solve this tradeoff between statistical reliability of our estimates and precision or focus of the partition and the approach we take is to relate this tradeoff with generalization properties of the classifier that is based on the clustering.",
            "So now."
        ],
        [
            "I'm going to enter a more formal part of my talk.",
            "So classification based grid grid clustering could have this parameter space.",
            "Here it's X1 by X2 which is clustered into clusters C1 and C2.",
            "Generally it could be more than two parameters, and this grid plus grid clustering is defined by a set of mappings Q1 of C1 given X1 and Q2 of C2 given X2, which map the values of the parameters to their clusters, and a classification rule which Maps the resulted patches.",
            "Into the labels.",
            "It can be some distribution over the labels and we are going to denote both of them collectively as capital Q.",
            "So the manner is."
        ],
        [
            "Out of our paper says that this probability greater than one minus Delta.",
            "The loss of cure.",
            "The expected loss of our prediction strategy is going to be less than the empirical loss of that prediction strategy plus some complexity term.",
            "So first of all, regarding the empirical loss, if we choose some.",
            "Fixed prediction strategy within a partition cell.",
            "For example, we're going to predict with the average of the ratings that fall within that cell.",
            "Certainly not all the ratings are equal, and we're going to have some empirical loss.",
            "Now about the complexity term I'll go in."
        ],
        [
            "So it so NI is the cardinality of feature I."
        ],
        [
            "I you of XI and CI is the information preserved by the mapping of the parameters to the clusters and it is calculated according to QFC I given XI and uniform distribution over X."
        ],
        [
            "As.",
            "K is some value that is independent of the mapping Qi of, say given XI.",
            "I'll show a bit."
        ],
        [
            "Other what hides within K&N is."
        ],
        [
            "Sample size so we are optimizing a tradeoff between empirical loss and effective complexity of a partition.",
            "So for example, if we have those two partitions, the partition on the right uses all the two clusters that are given completely and the partition on the left is closer to a partition that puts all the data in one cluster.",
            "Not exactly, but we have some small clusters and one.",
            "Measure cluster so the partition the complexity of the partition on the right is going to be higher than the complexity of the partition on the left and we are balancing between the trade off of the empirical loss and the complexity of."
        ],
        [
            "Partition now what hides within K, though there is some expression, MI denotes the cardinality of CI and we have some term that is logarithmic in the cardinality of the parameters, which is not so important.",
            "More problematic term is the number of partitions cells that we get in our partition.",
            "This product of the cardinality's of CIS.",
            "Now we will focus mainly on low dimensional problems like two or three dimensions toward the end of the talk.",
            "I will tell you what it is possible to do when we're going to hide dimensions, but this is indeed problematic term.",
            "When we get to higher dimension.",
            "Say for example we have 10 dimensions and make partition only in two clusters along each of the mentions.",
            "We already have two to the power of 10, which is about 1000 partition cells, which means that we need an order of 1000 and more samples to be able to estimate something within that partition.",
            "And there is some usual stuff like one of one over Delta that is present in most of them."
        ],
        [
            "Sounds so just an idea of a proof I won't get into details, but we start with the base and bound suggested by McAllister in 1999 and we use a slightly improved version by Maurer from 2004, which tells that if you have some hypothesis class and.",
            "You predict with the distribution over hypothesis in the class, then the loss of your distribution is less or equal to the empirical loss plus.",
            "Callback label diversions between your distribution and some prior that should be chosen prior to learning.",
            "Plus some some logarithmic terms divided by two times the sample size."
        ],
        [
            "So what we do we design A combinatorial prior, overheard part partitions which are our basic hypothesis."
        ],
        [
            "And then we calculate this pullback, labor, diversions, and shows that it is concentrated around the information mutual information between the clusters and the original values of the."
        ],
        [
            "Monitors and if you are interested in more details, you are welcome to come to the poster or ask me After the."
        ],
        [
            "Gotcha.",
            "So what are the messages that we can get from this bound and in general from our approach?",
            "So for clustering we suggest evaluation of clustering by its generalization properties on the task it is designed for.",
            "And it can serve a meaningful alternative to, for example, stability based clustering evaluation or other.",
            "Regularization.",
            "Approaches to clustering.",
            "For classification, we suggest to unify feature feature values to amplify the statistical reliability of our predictions."
        ],
        [
            "So now I'm going to enter into the problem of classification by single feature and I will show that when we have only a single feature, we can further improve the bound we have.",
            "And then we can apply it to feature ranking."
        ],
        [
            "So if we have a classification by a single feature here is this feature X which is clustered in two some clusters and.",
            "For each cluster we give a prediction over the labels."
        ],
        [
            "We can construct an equivalent direct mapping between X&Y by this formula, which tells that probability of Y given X equals to sum over the clusters of key of C given XQ of Y given C."
        ],
        [
            "And.",
            "We claim that this construction actually doesn't lose generalization power.",
            "So we will denote by QC the classification rule based on clustering came and by Q the classification rule based on direct mapping.",
            "We can actually associate with each label.",
            "We can say that the labels cluster the parameters that each parameter is mapped to a certain.",
            "For example, 50 probabilities to label 0 and 50 probability of 50% label one, then it's in 1/2 in cluster which is associated with label zero.",
            "And by having the cluster that is associated with label one."
        ],
        [
            "What we get is that the loss of classification strategy based on clustering is equal to the loss of stratification strategy based on the direct mapping cause it does the same rule, but the information preserved by the direct mapping is less than the information preserved by the clustering by the information processing equality."
        ],
        [
            "So we get a bound for direct mappings which tells that the loss of a direct mapping is less or equal than it's empirical loss plus N times the information preserved between the parameter and the label.",
            "Plus some term KK Prime which is also slightly improved compared to the you have seen previously.",
            "I will also not get into details of that divided by."
        ],
        [
            "Two times the sample size.",
            "So this Mount is tighter than the bound that would be for classification that."
        ],
        [
            "Going clear clustering.",
            "Important things that this bound holds for any classification rule QFY given X, so you give me your classification rule.",
            "I can tell you what is the generalization bound for your rule using this this bound."
        ],
        [
            "And have.",
            "And I can further optimize this bound with respect to the mapping of X to Y by gradient descent for example, and to obtain an optimal classification rule for a single feature from generalization."
        ],
        [
            "In the field.",
            "And what it comes out that for a single feature there is no need in intermediate clustering.",
            "For a single feature we can work with those direct mappings and find the optimal without building any clustering in the middle.",
            "This is not true for high dimension, but only."
        ],
        [
            "Or a single single dimension.",
            "Now some insights about this bound on a single dimension.",
            "So first of all, the empirical loss is minimized by the maximum likelihood rule, which returns for each feature value X.",
            "The most likely or in the case of 01 loss is the most frequent value of Y that appeared with that X."
        ],
        [
            "If we require that the information between X&Y is 0, which is the minimal values that can be, then the loss is minimized by the marginal maximum likelihood rule, which returns the marginal most likely most frequent value of why that appeared in the trends."
        ],
        [
            "Set.",
            "And this way we see that.",
            "The general loss is minimized by smoothing the maximum likelihood roll toward the marginal maximum likelihood rule dependent on the sample size we have."
        ],
        [
            "Application that we propose for hour for this bound is feature ranking.",
            "So we suggest to rank features by their generalization potential and not, for example, mutual information or correlation with the label.",
            "And this is especially important in cases when we have features with large cardinalities and small sample size.",
            "So for example, if the have questions and we have to predict whether those patients have cancer or not, and we have two parameters.",
            "One parameter says whether the person is smoking or not and the second parameter is the year of birth of that person.",
            "Just because they are much more values for this second parameter there is going to be much higher.",
            "Correlational mutual information within this parameter and the label, but it does not necessarily say that this parameter has a higher generalization."
        ],
        [
            "Power.",
            "A related work in this context is of the venture, but and special of Schwartz presented called 2007.",
            "What they derive there is a generalization bound for prediction rules that takes the empirical distribution of Y given X and predicts labels from that distribution.",
            "Important advantage of our work is that our bound holds for any classification rule, and in particular it holds for the maximum likelihood predictions and maximum likelihood prediction perform much better on practice than the empirical distribution prediction from the empirical distribute."
        ],
        [
            "So here are some examples of application of our bound.",
            "All the data sets are taken from the UCI repository.",
            "The first example is contraception method search data set, which aims to predict which contraception method some womanizer users and there are three options.",
            "No contraception short term and long term.",
            "And the parameters we have is the age of a woman, her education, her husband, education, number of children, religion and so on.",
            "And below I have the cardinality of each of the parameters.",
            "The bottom line corresponds to the train loss of maximum likelihood classifier.",
            "That middle magenta line corresponds to the loss of marginal maximum likelihood classifier, which actually doesn't look at the feature values and therefore it is constant for every feature.",
            "Then we have the test loss which is computed by 5 fold cross validation and.",
            "Above all, that is our bound and an important thing that the distance between the bound and the test loss is less than 0.1, which is very good.",
            "The fact that they can draw the bound and the real test on the same graph is already very good and we have quite.",
            "Quite a small gap between the two and what we see that the form of the bound follows quite well.",
            "The form of the test was and we see, for example, that age which have very many values and have very low empirical loss.",
            "Actually have quite high test loss and this is.",
            "Also expressed in our bound.",
            "And the only relevant feature that appears here is the number of children.",
            "And although it's also parameter with many values but with the bound, see that there is some real correlation between this parameter and the label."
        ],
        [
            "So there are some more examples.",
            "The mushrooms data set we see again here.",
            "The train and test last very close, so the one on top of the other, but our bound follows remarkably well.",
            "The true shape of the test loss, and again the distance between the bound and the actual test loss is quite small."
        ],
        [
            "Another example of letters, data set.",
            "Once again, we see that the shape of the bound follows very well the shape of the actual test loss and the distance once again is even less than 0.1.",
            "Here we have.",
            "The number of labels is 26 is much larger than in the pre."
        ],
        [
            "So sets.",
            "And what we also did, we compared the performance on future ranking on those three and one more data set.",
            "In selecting top one, top two and top three feature subsets based on their power of predictions, test power of prediction.",
            "So we see that.",
            "Here in blue it's normalized correlation.",
            "In green it's mutual information, and in red it's our bound.",
            "We see that on two data sets CMC and later in selection of top One feature feature subset the other two methods completely fail and we have quite significant level of correlation for two top two feature subset where still superior to the other two methods and top three.",
            "Feature subset were somehow close to the mutual information, but still superior to the normalized correlation."
        ],
        [
            "So let me summarize my talk.",
            "I represented the generalization bounds for multi classification based on grid clustering.",
            "We suggest to unify future values to amplify the statistics.",
            "And to improve the predictions, but also suggested evaluation of glass drink.",
            "By its generalization properties on the task that it is designed for.",
            "The Sean and application of our approach to feature ranking and where it was quite good.",
            "And finally there was a limitation that we could not handle high dimensions that they promised to tell how.",
            "We plan to cope with it, so there are two things that can be done.",
            "First of all, we can rank the features and use the top two or three features to solve that."
        ],
        [
            "Ask but another more interesting approach is to derive generalization bound for general graphical models.",
            "So great clustering can be described as a graphical model where each parameter is mapped directly to a cluster and then we have this clique of clusters and the label which actually generates this exploding parameter of the number of pages.",
            "We can transform it to a factor model, where latent variables we will control the complexity of the model and the size of the clicks in this model is only two.",
            "Compared to the slash, click here and it can.",
            "Possibly give some better results.",
            "And this in general, if we drive abound for general graphical models, it can also change the approach to evaluation of graphical models from driving graphical models that give maximum likelihood to the observed data to looking for graphical models that give good prediction will give good predictions on the unseen data.",
            "Send thank you for attention.",
            "So if I send you only apply to questioning that works by clustering each feature separately and then taking the product of those classrooms.",
            "Yeah, at the moment yes.",
            "So what is with what can you say about the metal structure that people use?",
            "Well.",
            "Future work.",
            "Yeah, actually, great clustering is applied and on practice and the slide I've shown you the beginning of the talk is.",
            "It's quite typical, for example, in core clustering you are doing the same type of clustering.",
            "The difference that is, in core clustering you are looking for repetition so that the two features will preserve information about each other.",
            "We look for a partition so that the both features will preserve information in the label, which is indeed what is interesting for you and not the information between the features.",
            "So the question I have is you know in the decision tree literature in split selection decision trees, there's been quite an extensive series of papers on trying to correct for features that have a large number of values.",
            "OK, and it would be interesting to see whether the criterion and come up with is equivalent to any of those.",
            "That might give some guidance about which one could be used in those cases to make women just divide by the entropy of the of each attribute, and that's pretty clearly biased, and so there's been a lot of work on unbiased measures.",
            "Very here, but there are some people in this meeting that I contributed to that leadership.",
            "So I think it's an interesting point that at least the box I've seen and I'll be glad to see more works if you can give me the pointers but the works I've seen, and for example the work of Sabatin Schwartz, they look at the prediction power of the predictors that are based on the empirical empirical distribution of label given the features.",
            "And our bond holds for any classification rule, and in particular for the maximum likelihood rule that I don't have a graph here, but it performs significantly better than this empirical distribution role.",
            "So.",
            "OK, well thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well thanks for staying till the last talk of the last session.",
                    "label": 0
                },
                {
                    "sent": "I have Jenna, Selden and I will tell you about multi classification by categorical features.",
                    "label": 0
                },
                {
                    "sent": "We are clustering and this is a joint work with nationalities, beer and we are both from the Hebrew University of Jerusalem.",
                    "label": 1
                },
                {
                    "sent": "And as the title says, the talk is going to be both on classification and clustering, and I hope that both communities will be able to come up with something out of this.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Look so I will start with an example of multi classification by categorical features and the example I give so it's not seen very well, but it's collaborative filtering data, so in collaborative filtering we have viewers and we have movies.",
                    "label": 1
                },
                {
                    "sent": "And each year here we have about 1000 viewers by 1500 movies, when a viewer number, for example 36, will give rating of five to movie number 57 will have a red dot within this space and you see that there are relatively few ratings that are present at most of the ratings are missing and our task is to predict the ratings based on Viewer ID and.",
                    "label": 0
                },
                {
                    "sent": "Movie idea, so we rented movie already out to categorical parameters of features that were working with and the rating is the label.",
                    "label": 0
                },
                {
                    "sent": "And we approach this prob.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Them by clustering the viewers into clusters of yours and clustering movies into clusters of movies.",
                    "label": 0
                },
                {
                    "sent": "This kind of approach defines agreed partition of our parameter space and then within each page in this partition we're going to to choose some prediction strategy.",
                    "label": 0
                },
                {
                    "sent": "So for example, in this page there are more ratings that are close to five, so we will predict rating for all the page.",
                    "label": 0
                },
                {
                    "sent": "And here's the ratings are closer to one.",
                    "label": 0
                },
                {
                    "sent": "We will predict one for all the page.",
                    "label": 0
                },
                {
                    "sent": "And given when we have to predict the rating for some viewer and movie, will will see in which page this pair of parameters bring us and give the prediction of that page.",
                    "label": 0
                },
                {
                    "sent": "And actually this is a real picture from our previous works presented at NIPS 2006.",
                    "label": 0
                },
                {
                    "sent": "Our same kind of data appears in many other applications, not limited to collaborative filtering.",
                    "label": 0
                },
                {
                    "sent": "For example, gene expression data and text analysis data, and so on.",
                    "label": 0
                },
                {
                    "sent": "In some cases, we are more interested in predicting the label.",
                    "label": 0
                },
                {
                    "sent": "In other cases, like for example, gene expression data will be more interested in the clustering itself, so may be interested in what are the groups of genes that are related to each other.",
                    "label": 0
                },
                {
                    "sent": "And the question.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ask in our paper is how how to partition this data.",
                    "label": 0
                },
                {
                    "sent": "So should we put everything in one big cluster should partition it as shown in the slide previously or should we take some other form of partition and there are two extreme cases of this problem that give a good intuition about what's happening if we put everything in one big cluster we are able to estimate the average rating of everybody.",
                    "label": 0
                },
                {
                    "sent": "Quite quite good, but this is a global average that is going to be quite far from every particular entry.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, when we put every viewer and every movie in a separate cluster and then we will be focused on that specific entry.",
                    "label": 0
                },
                {
                    "sent": "But we will have very little or maybe even know data within that point.",
                    "label": 0
                },
                {
                    "sent": "So the question is how to partition?",
                    "label": 0
                },
                {
                    "sent": "How to solve this tradeoff between statistical reliability of our estimates and precision or focus of the partition and the approach we take is to relate this tradeoff with generalization properties of the classifier that is based on the clustering.",
                    "label": 1
                },
                {
                    "sent": "So now.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm going to enter a more formal part of my talk.",
                    "label": 0
                },
                {
                    "sent": "So classification based grid grid clustering could have this parameter space.",
                    "label": 0
                },
                {
                    "sent": "Here it's X1 by X2 which is clustered into clusters C1 and C2.",
                    "label": 0
                },
                {
                    "sent": "Generally it could be more than two parameters, and this grid plus grid clustering is defined by a set of mappings Q1 of C1 given X1 and Q2 of C2 given X2, which map the values of the parameters to their clusters, and a classification rule which Maps the resulted patches.",
                    "label": 1
                },
                {
                    "sent": "Into the labels.",
                    "label": 0
                },
                {
                    "sent": "It can be some distribution over the labels and we are going to denote both of them collectively as capital Q.",
                    "label": 0
                },
                {
                    "sent": "So the manner is.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Out of our paper says that this probability greater than one minus Delta.",
                    "label": 0
                },
                {
                    "sent": "The loss of cure.",
                    "label": 0
                },
                {
                    "sent": "The expected loss of our prediction strategy is going to be less than the empirical loss of that prediction strategy plus some complexity term.",
                    "label": 1
                },
                {
                    "sent": "So first of all, regarding the empirical loss, if we choose some.",
                    "label": 0
                },
                {
                    "sent": "Fixed prediction strategy within a partition cell.",
                    "label": 0
                },
                {
                    "sent": "For example, we're going to predict with the average of the ratings that fall within that cell.",
                    "label": 0
                },
                {
                    "sent": "Certainly not all the ratings are equal, and we're going to have some empirical loss.",
                    "label": 0
                },
                {
                    "sent": "Now about the complexity term I'll go in.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it so NI is the cardinality of feature I.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I you of XI and CI is the information preserved by the mapping of the parameters to the clusters and it is calculated according to QFC I given XI and uniform distribution over X.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As.",
                    "label": 0
                },
                {
                    "sent": "K is some value that is independent of the mapping Qi of, say given XI.",
                    "label": 0
                },
                {
                    "sent": "I'll show a bit.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other what hides within K&N is.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sample size so we are optimizing a tradeoff between empirical loss and effective complexity of a partition.",
                    "label": 0
                },
                {
                    "sent": "So for example, if we have those two partitions, the partition on the right uses all the two clusters that are given completely and the partition on the left is closer to a partition that puts all the data in one cluster.",
                    "label": 0
                },
                {
                    "sent": "Not exactly, but we have some small clusters and one.",
                    "label": 0
                },
                {
                    "sent": "Measure cluster so the partition the complexity of the partition on the right is going to be higher than the complexity of the partition on the left and we are balancing between the trade off of the empirical loss and the complexity of.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Partition now what hides within K, though there is some expression, MI denotes the cardinality of CI and we have some term that is logarithmic in the cardinality of the parameters, which is not so important.",
                    "label": 0
                },
                {
                    "sent": "More problematic term is the number of partitions cells that we get in our partition.",
                    "label": 0
                },
                {
                    "sent": "This product of the cardinality's of CIS.",
                    "label": 0
                },
                {
                    "sent": "Now we will focus mainly on low dimensional problems like two or three dimensions toward the end of the talk.",
                    "label": 0
                },
                {
                    "sent": "I will tell you what it is possible to do when we're going to hide dimensions, but this is indeed problematic term.",
                    "label": 0
                },
                {
                    "sent": "When we get to higher dimension.",
                    "label": 0
                },
                {
                    "sent": "Say for example we have 10 dimensions and make partition only in two clusters along each of the mentions.",
                    "label": 0
                },
                {
                    "sent": "We already have two to the power of 10, which is about 1000 partition cells, which means that we need an order of 1000 and more samples to be able to estimate something within that partition.",
                    "label": 0
                },
                {
                    "sent": "And there is some usual stuff like one of one over Delta that is present in most of them.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sounds so just an idea of a proof I won't get into details, but we start with the base and bound suggested by McAllister in 1999 and we use a slightly improved version by Maurer from 2004, which tells that if you have some hypothesis class and.",
                    "label": 1
                },
                {
                    "sent": "You predict with the distribution over hypothesis in the class, then the loss of your distribution is less or equal to the empirical loss plus.",
                    "label": 0
                },
                {
                    "sent": "Callback label diversions between your distribution and some prior that should be chosen prior to learning.",
                    "label": 0
                },
                {
                    "sent": "Plus some some logarithmic terms divided by two times the sample size.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we do we design A combinatorial prior, overheard part partitions which are our basic hypothesis.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we calculate this pullback, labor, diversions, and shows that it is concentrated around the information mutual information between the clusters and the original values of the.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Monitors and if you are interested in more details, you are welcome to come to the poster or ask me After the.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Gotcha.",
                    "label": 0
                },
                {
                    "sent": "So what are the messages that we can get from this bound and in general from our approach?",
                    "label": 0
                },
                {
                    "sent": "So for clustering we suggest evaluation of clustering by its generalization properties on the task it is designed for.",
                    "label": 1
                },
                {
                    "sent": "And it can serve a meaningful alternative to, for example, stability based clustering evaluation or other.",
                    "label": 0
                },
                {
                    "sent": "Regularization.",
                    "label": 0
                },
                {
                    "sent": "Approaches to clustering.",
                    "label": 1
                },
                {
                    "sent": "For classification, we suggest to unify feature feature values to amplify the statistical reliability of our predictions.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now I'm going to enter into the problem of classification by single feature and I will show that when we have only a single feature, we can further improve the bound we have.",
                    "label": 0
                },
                {
                    "sent": "And then we can apply it to feature ranking.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we have a classification by a single feature here is this feature X which is clustered in two some clusters and.",
                    "label": 0
                },
                {
                    "sent": "For each cluster we give a prediction over the labels.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can construct an equivalent direct mapping between X&Y by this formula, which tells that probability of Y given X equals to sum over the clusters of key of C given XQ of Y given C.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We claim that this construction actually doesn't lose generalization power.",
                    "label": 0
                },
                {
                    "sent": "So we will denote by QC the classification rule based on clustering came and by Q the classification rule based on direct mapping.",
                    "label": 0
                },
                {
                    "sent": "We can actually associate with each label.",
                    "label": 0
                },
                {
                    "sent": "We can say that the labels cluster the parameters that each parameter is mapped to a certain.",
                    "label": 0
                },
                {
                    "sent": "For example, 50 probabilities to label 0 and 50 probability of 50% label one, then it's in 1/2 in cluster which is associated with label zero.",
                    "label": 0
                },
                {
                    "sent": "And by having the cluster that is associated with label one.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we get is that the loss of classification strategy based on clustering is equal to the loss of stratification strategy based on the direct mapping cause it does the same rule, but the information preserved by the direct mapping is less than the information preserved by the clustering by the information processing equality.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we get a bound for direct mappings which tells that the loss of a direct mapping is less or equal than it's empirical loss plus N times the information preserved between the parameter and the label.",
                    "label": 1
                },
                {
                    "sent": "Plus some term KK Prime which is also slightly improved compared to the you have seen previously.",
                    "label": 0
                },
                {
                    "sent": "I will also not get into details of that divided by.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two times the sample size.",
                    "label": 0
                },
                {
                    "sent": "So this Mount is tighter than the bound that would be for classification that.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Going clear clustering.",
                    "label": 0
                },
                {
                    "sent": "Important things that this bound holds for any classification rule QFY given X, so you give me your classification rule.",
                    "label": 1
                },
                {
                    "sent": "I can tell you what is the generalization bound for your rule using this this bound.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And have.",
                    "label": 0
                },
                {
                    "sent": "And I can further optimize this bound with respect to the mapping of X to Y by gradient descent for example, and to obtain an optimal classification rule for a single feature from generalization.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the field.",
                    "label": 0
                },
                {
                    "sent": "And what it comes out that for a single feature there is no need in intermediate clustering.",
                    "label": 1
                },
                {
                    "sent": "For a single feature we can work with those direct mappings and find the optimal without building any clustering in the middle.",
                    "label": 0
                },
                {
                    "sent": "This is not true for high dimension, but only.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or a single single dimension.",
                    "label": 0
                },
                {
                    "sent": "Now some insights about this bound on a single dimension.",
                    "label": 1
                },
                {
                    "sent": "So first of all, the empirical loss is minimized by the maximum likelihood rule, which returns for each feature value X.",
                    "label": 1
                },
                {
                    "sent": "The most likely or in the case of 01 loss is the most frequent value of Y that appeared with that X.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we require that the information between X&Y is 0, which is the minimal values that can be, then the loss is minimized by the marginal maximum likelihood rule, which returns the marginal most likely most frequent value of why that appeared in the trends.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Set.",
                    "label": 0
                },
                {
                    "sent": "And this way we see that.",
                    "label": 0
                },
                {
                    "sent": "The general loss is minimized by smoothing the maximum likelihood roll toward the marginal maximum likelihood rule dependent on the sample size we have.",
                    "label": 1
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Application that we propose for hour for this bound is feature ranking.",
                    "label": 0
                },
                {
                    "sent": "So we suggest to rank features by their generalization potential and not, for example, mutual information or correlation with the label.",
                    "label": 1
                },
                {
                    "sent": "And this is especially important in cases when we have features with large cardinalities and small sample size.",
                    "label": 0
                },
                {
                    "sent": "So for example, if the have questions and we have to predict whether those patients have cancer or not, and we have two parameters.",
                    "label": 0
                },
                {
                    "sent": "One parameter says whether the person is smoking or not and the second parameter is the year of birth of that person.",
                    "label": 0
                },
                {
                    "sent": "Just because they are much more values for this second parameter there is going to be much higher.",
                    "label": 0
                },
                {
                    "sent": "Correlational mutual information within this parameter and the label, but it does not necessarily say that this parameter has a higher generalization.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Power.",
                    "label": 0
                },
                {
                    "sent": "A related work in this context is of the venture, but and special of Schwartz presented called 2007.",
                    "label": 0
                },
                {
                    "sent": "What they derive there is a generalization bound for prediction rules that takes the empirical distribution of Y given X and predicts labels from that distribution.",
                    "label": 1
                },
                {
                    "sent": "Important advantage of our work is that our bound holds for any classification rule, and in particular it holds for the maximum likelihood predictions and maximum likelihood prediction perform much better on practice than the empirical distribution prediction from the empirical distribute.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here are some examples of application of our bound.",
                    "label": 0
                },
                {
                    "sent": "All the data sets are taken from the UCI repository.",
                    "label": 0
                },
                {
                    "sent": "The first example is contraception method search data set, which aims to predict which contraception method some womanizer users and there are three options.",
                    "label": 0
                },
                {
                    "sent": "No contraception short term and long term.",
                    "label": 0
                },
                {
                    "sent": "And the parameters we have is the age of a woman, her education, her husband, education, number of children, religion and so on.",
                    "label": 0
                },
                {
                    "sent": "And below I have the cardinality of each of the parameters.",
                    "label": 0
                },
                {
                    "sent": "The bottom line corresponds to the train loss of maximum likelihood classifier.",
                    "label": 0
                },
                {
                    "sent": "That middle magenta line corresponds to the loss of marginal maximum likelihood classifier, which actually doesn't look at the feature values and therefore it is constant for every feature.",
                    "label": 0
                },
                {
                    "sent": "Then we have the test loss which is computed by 5 fold cross validation and.",
                    "label": 0
                },
                {
                    "sent": "Above all, that is our bound and an important thing that the distance between the bound and the test loss is less than 0.1, which is very good.",
                    "label": 0
                },
                {
                    "sent": "The fact that they can draw the bound and the real test on the same graph is already very good and we have quite.",
                    "label": 0
                },
                {
                    "sent": "Quite a small gap between the two and what we see that the form of the bound follows quite well.",
                    "label": 0
                },
                {
                    "sent": "The form of the test was and we see, for example, that age which have very many values and have very low empirical loss.",
                    "label": 0
                },
                {
                    "sent": "Actually have quite high test loss and this is.",
                    "label": 0
                },
                {
                    "sent": "Also expressed in our bound.",
                    "label": 0
                },
                {
                    "sent": "And the only relevant feature that appears here is the number of children.",
                    "label": 0
                },
                {
                    "sent": "And although it's also parameter with many values but with the bound, see that there is some real correlation between this parameter and the label.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there are some more examples.",
                    "label": 0
                },
                {
                    "sent": "The mushrooms data set we see again here.",
                    "label": 0
                },
                {
                    "sent": "The train and test last very close, so the one on top of the other, but our bound follows remarkably well.",
                    "label": 0
                },
                {
                    "sent": "The true shape of the test loss, and again the distance between the bound and the actual test loss is quite small.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another example of letters, data set.",
                    "label": 0
                },
                {
                    "sent": "Once again, we see that the shape of the bound follows very well the shape of the actual test loss and the distance once again is even less than 0.1.",
                    "label": 0
                },
                {
                    "sent": "Here we have.",
                    "label": 0
                },
                {
                    "sent": "The number of labels is 26 is much larger than in the pre.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So sets.",
                    "label": 0
                },
                {
                    "sent": "And what we also did, we compared the performance on future ranking on those three and one more data set.",
                    "label": 0
                },
                {
                    "sent": "In selecting top one, top two and top three feature subsets based on their power of predictions, test power of prediction.",
                    "label": 0
                },
                {
                    "sent": "So we see that.",
                    "label": 0
                },
                {
                    "sent": "Here in blue it's normalized correlation.",
                    "label": 0
                },
                {
                    "sent": "In green it's mutual information, and in red it's our bound.",
                    "label": 1
                },
                {
                    "sent": "We see that on two data sets CMC and later in selection of top One feature feature subset the other two methods completely fail and we have quite significant level of correlation for two top two feature subset where still superior to the other two methods and top three.",
                    "label": 1
                },
                {
                    "sent": "Feature subset were somehow close to the mutual information, but still superior to the normalized correlation.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me summarize my talk.",
                    "label": 0
                },
                {
                    "sent": "I represented the generalization bounds for multi classification based on grid clustering.",
                    "label": 1
                },
                {
                    "sent": "We suggest to unify future values to amplify the statistics.",
                    "label": 0
                },
                {
                    "sent": "And to improve the predictions, but also suggested evaluation of glass drink.",
                    "label": 1
                },
                {
                    "sent": "By its generalization properties on the task that it is designed for.",
                    "label": 0
                },
                {
                    "sent": "The Sean and application of our approach to feature ranking and where it was quite good.",
                    "label": 0
                },
                {
                    "sent": "And finally there was a limitation that we could not handle high dimensions that they promised to tell how.",
                    "label": 0
                },
                {
                    "sent": "We plan to cope with it, so there are two things that can be done.",
                    "label": 0
                },
                {
                    "sent": "First of all, we can rank the features and use the top two or three features to solve that.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ask but another more interesting approach is to derive generalization bound for general graphical models.",
                    "label": 1
                },
                {
                    "sent": "So great clustering can be described as a graphical model where each parameter is mapped directly to a cluster and then we have this clique of clusters and the label which actually generates this exploding parameter of the number of pages.",
                    "label": 0
                },
                {
                    "sent": "We can transform it to a factor model, where latent variables we will control the complexity of the model and the size of the clicks in this model is only two.",
                    "label": 0
                },
                {
                    "sent": "Compared to the slash, click here and it can.",
                    "label": 0
                },
                {
                    "sent": "Possibly give some better results.",
                    "label": 0
                },
                {
                    "sent": "And this in general, if we drive abound for general graphical models, it can also change the approach to evaluation of graphical models from driving graphical models that give maximum likelihood to the observed data to looking for graphical models that give good prediction will give good predictions on the unseen data.",
                    "label": 0
                },
                {
                    "sent": "Send thank you for attention.",
                    "label": 0
                },
                {
                    "sent": "So if I send you only apply to questioning that works by clustering each feature separately and then taking the product of those classrooms.",
                    "label": 0
                },
                {
                    "sent": "Yeah, at the moment yes.",
                    "label": 0
                },
                {
                    "sent": "So what is with what can you say about the metal structure that people use?",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "Future work.",
                    "label": 0
                },
                {
                    "sent": "Yeah, actually, great clustering is applied and on practice and the slide I've shown you the beginning of the talk is.",
                    "label": 0
                },
                {
                    "sent": "It's quite typical, for example, in core clustering you are doing the same type of clustering.",
                    "label": 0
                },
                {
                    "sent": "The difference that is, in core clustering you are looking for repetition so that the two features will preserve information about each other.",
                    "label": 0
                },
                {
                    "sent": "We look for a partition so that the both features will preserve information in the label, which is indeed what is interesting for you and not the information between the features.",
                    "label": 0
                },
                {
                    "sent": "So the question I have is you know in the decision tree literature in split selection decision trees, there's been quite an extensive series of papers on trying to correct for features that have a large number of values.",
                    "label": 0
                },
                {
                    "sent": "OK, and it would be interesting to see whether the criterion and come up with is equivalent to any of those.",
                    "label": 0
                },
                {
                    "sent": "That might give some guidance about which one could be used in those cases to make women just divide by the entropy of the of each attribute, and that's pretty clearly biased, and so there's been a lot of work on unbiased measures.",
                    "label": 0
                },
                {
                    "sent": "Very here, but there are some people in this meeting that I contributed to that leadership.",
                    "label": 0
                },
                {
                    "sent": "So I think it's an interesting point that at least the box I've seen and I'll be glad to see more works if you can give me the pointers but the works I've seen, and for example the work of Sabatin Schwartz, they look at the prediction power of the predictors that are based on the empirical empirical distribution of label given the features.",
                    "label": 0
                },
                {
                    "sent": "And our bond holds for any classification rule, and in particular for the maximum likelihood rule that I don't have a graph here, but it performs significantly better than this empirical distribution role.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK, well thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}