{
    "id": "bz3fx5kqmmftlem6kqtpv3hzosyyec3j",
    "title": "Learning from Weakly Labeled Data",
    "info": {
        "author": [
            "James Kwok, Department of Computer Science and Engineering, The Hong Kong University of Science and Technology"
        ],
        "published": "Aug. 26, 2013",
        "recorded": "July 2013",
        "category": [
            "Top->Computer Science->Optimization Methods",
            "Top->Computer Science->Machine Learning->Kernel Methods->Support Vector Machines",
            "Top->Computer Science->Compressed Sensing",
            "Top->Computer Science->Machine Learning->Regularization"
        ]
    },
    "url": "http://videolectures.net/roks2013_kwok_data/",
    "segmentation": [
        [
            "But today I will talk about learning found wiki labels data, so this is joint work with evenly after John and Joe."
        ],
        [
            "So this is the outline of my presentation, so first I will give an introduction.",
            "Then I will present the proposed algorithm which is called about SVM, which stands for weakly labeled SVM.",
            "And we will then apply this to three example applications, namely semi supervised learning, multiple instance learning and the maximum margin cluster.",
            "And then finally there's the conclusion."
        ],
        [
            "Who?",
            "First of all, what do I mean by I quickly labeled learning, learning, so we know that are obtaining labeled data is often very hot.",
            "OK, because it is expensive.",
            "It involves a lot of Labor, involves time, OK, involves expertise, and so on.",
            "So in recent years there has been a lot of interest or not studying various forms of learning algorithms in with the problems in which the labels are incomplete or only partially known.",
            "So for example, in this talk I will focus on this at summer, supplies, supervised learning, multiple instance learning, and maximum marking clustering.",
            "So."
        ],
        [
            "I guess everybody knows what some of us learning is.",
            "So in this case we have very few labeled data, so just one blue point and one black phones belonging to two different classes.",
            "But we have a lot of unlabeled data.",
            "So the goal is that by utilizing this abundance of unlabeled data, we can get better prediction.",
            "So of course if we just use these two labeled points, then we may position the same boundary here, whereas if you use the unlabeled data distribution then we may have a better solution.",
            "So semi supervised learning have a lot of applications.",
            "So for example you know documents processing, image processing, NLP, computer vision and so on and so on.",
            "So in this case, we say that the labels are only partially known, so universe I know the labels of the labeled data, but not those of the unlabeled data."
        ],
        [
            "So another scenario you wish you only have partial label information is our multiple instance learning.",
            "So some of you might not be very familiar with this, so I give.",
            "Yeah, more detailed introduction.",
            "So multiple instance learning well was first introduced by our company Trish in 97, but the example application there was the drop activity prediction.",
            "So in this application you're given a drug molecule and then you want to predict whether it can bind to a certain targets.",
            "OK, so this looks like standard supervised learning.",
            "OK, basically a classification problem, however, but drop protection because it's complicated.",
            "In the sense that each molecule can have multiple confirmations.",
            "So for example, here all these corresponds both.",
            "These correspond to just one molecule.",
            "OK, but as you can see so say for example the born here can rip up or down.",
            "OK, so each one of these corresponds to our confirmation, OK?",
            "And then.",
            "When you want to know whether this molecule can bind to a drug combined to a certain target, OK, so you can buy at least one of these confirmations.",
            "Combine OK forever, unfortunately, but the power chemist can tell the binding capability of the whole molecule OK.",
            "However, they do not know which particular information.",
            "In fact spines OK, so in this case we have a weaker form of.",
            "Labor information more."
        ],
        [
            "In multiple instance learning terminologies, so usually this shape.",
            "OK that is a each one of these confirmations.",
            "OK will then correspond to a certain feature representation and then each one of these is called an instance and then the whole molecule which may correspond to a number of different instances.",
            "This called back OK.",
            "So as I said earlier, so for a molecule you only know that either whether this molecule can bind or not, but not whether each of these confirmations combine OK, so universe.",
            "You know that's a bag is labeled positive when it contains at least one positive instance, OK, However, you do not know which one.",
            "And of course, if all the instances here cannot bind to the target, then the bag will be labeled negative.",
            "And that positive instance in the case when you have a positive fact.",
            "Usually it is called the key instance.",
            "OK, so this is different from traditional supervised learning in the way that's for supervised learning.",
            "For each pattern, OK, you have the corresponding label forever.",
            "In multi instance learning.",
            "So the information is big in the sense that one object or one pattern or one bag can contain a number of instances and the label information is only associated with this.",
            "But not with the individual instances.",
            "OK, so in this case we say that the labels are only implicitly known, OK."
        ],
        [
            "And the third application.",
            "These are of course, clustering.",
            "But in that case the labels are totally unknown because you only have unlabeled data.",
            "OK, and in particular will be interested in the so called maximize maximum margin clustering, which is proposed by.",
            "Sherman and his group in NIPS and not in 2005.",
            "OK, so the idea is that we notice the SVM is doing very well in supervised learning OK.",
            "However, in that case we have the class labels, so you maximum margin clustering, but we want to do is that we want to extend this large margin concept to unsupervised learning.",
            "So we want to assign labels to these unlabeled patterns.",
            "While at the same time try to have a large margin for the resultant decision boundary.",
            "But of course, this problem is much harder because you don't have the labels."
        ],
        [
            "So as we can see, so in the so called weak label learning.",
            "OK, so as in standard supervised learning, we have to learn the parameters.",
            "Say for example the SVM, the double parameter and besides that cause the label information is only partial or incomplete.",
            "So we also need to infer the labels.",
            "OK, so as a whole this will give us a mixed integer programming problem.",
            "So we're just more complicated."
        ],
        [
            "So how to do this?",
            "Though there are several existing ways of solving this kind of learning problems, so one approach is to use a global optimization techniques.",
            "OK, so there's a screw, so for example, a number of people have proposed different use.",
            "The difference optimization techniques, so such as branch and bound with ministering and so on.",
            "But as we know well, typically these techniques are not so scalable.",
            "OK, so typically you can live with very small datasets.",
            "And another major paradigm is used.",
            "CarMax relaxations in particular SDP relaxations?",
            "So the good thing about this approach is that, well, you have a convex problem, so you don't have local minimum problem.",
            "However, as the piece as we know where you're still not so scalable.",
            "OK, so it can only be used or is more or at most say samples worth 1000 examples OK?",
            "Another approach is that we forget about complexity and we just use nonconvex optimization.",
            "OK, in this case you can use our coordinate descent or automating minimization or commas concave procedures and so on.",
            "So usually they're pretty efficient.",
            "OK.",
            "However, because they are based on comics approaches so.",
            "There is the possibility that you may get stuck in local minima.",
            "So the goal of this talk is that we want to find a way so that.",
            "But optimization procedure is CarMax so that you don't have local minimum problems and Moreover it is scalable much more scalable than these SDP relaxations."
        ],
        [
            "OK, so how do we do it so?",
            "The proposed algorithm is called the weakly labeled SBM or, whereas the MOK.",
            "So in fact, it is just a variant of the SVM with something with and other so called.",
            "Mabel generation process.",
            "OK so how to do this?",
            "So this is just an overview.",
            "So for example if we talk about think about the clustering problems so we don't have the labels.",
            "OK so this is half.",
            "So what we want to do is that we try to 1st generate some labels for these unlabeled patterns.",
            "So for example.",
            "I may generate, say for example, are these two are one and then these three are minus one OK and then feed this to SPM and the next and the next run I made gas at this.",
            "This is a -- 1 and then these are one and so on and so on.",
            "So of course later on we'll see how well the label vectors are generated and then after generating the label vectors OK, so each round will have one label vector.",
            "So at the end if you will have a number of Labor factors.",
            "So the learning algorithms also.",
            "Ha.",
            "Combine them together, and in particular that we use multiple learning, not multiple kernel learning algorithms to do it OK, and details will be given later and depending on the which will be specific to the particular example application.",
            "So what's the advantage?",
            "So the advantages are first, while it can be shown but later on will show that OK.",
            "This is a tight convex relaxation of the original message integer programming problem.",
            "OK, now in fact, but it is at least as tight as the existing Seiko Max SDP relaxations, so hopefully that will give us a more accurate solution.",
            "An morova during the procedure.",
            "OK, this is iterative.",
            "The iterative procedure and each iteration, while it can be reduced to standard SVM training.",
            "OK, So what that means is that we can make use of advances in the development of SVM solvers OK. Or you can just plug in your favorite server OK, and we know that nowadays many of these servers are scalable and efficient.",
            "OK, so at the end the whole problem is much more scalable than, say, using SDP.",
            "So of course I'm global optimization problems algorithms."
        ],
        [
            "So let me be more precise.",
            "So this is so we are given a data set OK D. So we have say N samples.",
            "Excise the input.",
            "Why is the output?",
            "And as the standard machine learning, so we want to find a function F that Maps from the inputs with the output.",
            "And by minimizing this structural risk, so Omega is the regularizer, an LF is the loss on the data set, and see of course is the parameter that controls the tradeoff.",
            "These two terms and.",
            "As in the SVM nature, so we assume that the Omega and loss oppose comics.",
            "OK, so this is the standard supervised learning, so in this status of ice learning.",
            "You know case record this.",
            "We talk about weak label learning, so some of the labels may not be available.",
            "OK so they have to be learned.",
            "So instead of just learning the F so we have to minimize both the F and also the unknown labels in the white hat.",
            "OK, so that's why we have an extra minimization on the outside.",
            "So here, for notational simplicity I just write down Whitehead here.",
            "But in fact inside my head, some of the labels are known, say for example in semi supervised learning you have some labeled samples, so in fact we don't need to learn those labels.",
            "OK, but here I'm a bit lazy so I just put down white heads here.",
            "Amber Alpha.",
            "Often you can conference white hat to be in some domain B, which is the set of our candidates label assignments.",
            "So again, the setting of the B depends on your application.",
            "So say for example.",
            "But the clustering, though you have two classes, an my priority, you think that the positive and negative samples should be approximately balanced.",
            "At this stage, the two clusters should be of approximately equal size.",
            "Then what you can do is to define B this way.",
            "OK, so use because why I is either plus one or minus one.",
            "So the sum of all the why I should if they're really balanced then it should be equal to zero.",
            "OK, if they're only approximately balanced, then well, you can set this to be within minus beta in some N plus beta.",
            "OK.",
            "So this is the problem that we want to solve."
        ],
        [
            "So let's come back to the standard SVM first.",
            "Because this is easier.",
            "So the SPM we know.",
            "That's why the primal is this OK and then you can rise the corresponding dual with the offer.",
            "Is the dual variables, K is the kernel matrix OK?",
            "Recall that.",
            "So here in the standard SVM, you only need to maximize over the dual variables OK.",
            "However, in our case because some of the labels may be missing.",
            "So besides maximizing the dual the dual variables, we also need to minimize the wise labels.",
            "OK, so more generally we will consider this.",
            "Optimization problem.",
            "OK, so that means that instead of just fixed restricting to the SVM well in principle you can park in some other SVN variance or some of the models.",
            "So here we only require Alpha to be in a convex set to say for example in the standard SVM this is the convex set a an this G is concave in Alpha.",
            "So just like a standard SVM objective or any fix it why hat?",
            "Anna Moreover, sometimes it is convenience to rise this G. Ha, as a Jeep A we have replaced this label vector by some label matrix.",
            "OK so say for example you can simply define M as YY prime, OK and then you and then you can revise this objective.",
            "Is this one OK and then we call this the qipao.",
            "And of course this G bar is concave in Alpha and linear in the matrix M. OK, so."
        ],
        [
            "This is the problem that we want to solve.",
            "OK, so recorders are why head is an integer labeled integer valued?",
            "OK, so in general it is quite difficult to solve.",
            "So what we do is that we do relaxation OK.",
            "So instead of solving this problem first, first of all we trying to win to change the order of the Max and min.",
            "OK, so we know that that will give us another bomb.",
            "OK on this problem and now we look at the inner optimization problem.",
            "So we want to minimize this objective G over the White House.",
            "OK, so you can also write it this way.",
            "Basically you want to maximize.",
            "A lower ban on G OK, but all the whitey's in this be OK.",
            "The two are the same an but this well you can also derive the dual for this.",
            "Some problem OK babe, by introducing do variables new T OK. Then it is easy to see that this.",
            "The dual of this problem is this one OK where mu is now lying in a simplex.",
            "OK.",
            "In out loud, the last step.",
            "Is that OK?",
            "Now I notice that our this objective OK, this convex immune.",
            "And I can't get in Alpha.",
            "OK, sorry, that means that I can again exchange the order of the maximin, and so finally this is the final optimization problem that's I attain.",
            "So you can see that well originally I have to minimize over this Whitehead here, but now.",
            "I move the white hat inside a.",
            "Moreover, minimization becomes a summation OK submission over all the possible Y label vectors OK, and then also I.",
            "This is mu recorded.",
            "This is a simplex, so this is a convex combination.",
            "OK, so now the question is are why is this easier to solve?",
            "But before that, OK.",
            "So first I showed this record that this is a relaxation.",
            "So first I will show that.",
            "Well, this is a tight relaxation."
        ],
        [
            "So recall this.",
            "This is the original problem.",
            "OK, we have a mean.",
            "Before, why maximal the Alpha and then the objective?",
            "And as I said earlier, so you can rewrite this objective OK, instead of using the label vectors, you can use the label matrix M. OK, so you can equivalently minimize over EM.",
            "Both are in this RY zero.",
            "OK, so where Y zero is just this one.",
            "OK, so this is the original problem.",
            "So now let's look at our relaxation.",
            "So this is our relaxed optimization problem.",
            "So again, well using the same reformulation so.",
            "I don't want to have the label vector here, so I right in terms of the label matrix OK and then you have summation, you can move the summation inside OK 'cause this G this bar is supposed to be linear in the matrix argument.",
            "OK so you have this and so you can equivalently define another set by one.",
            "OK, but this.",
            "Argument OK, so you basically minimizing M in Y, one OK, and if you compare this.",
            "Problem with this problem.",
            "So basically they are the same except that the original problem OK minimizes M in Y, zero.",
            "OK, murderous, we minimize M / y one OK, and Moreover it is easy to see that's why one is just the convex Hull of why not?",
            "OK, so we know that the convex how is the tightest comics object.",
            "OK, so that gives us so that means that this one is in fact the tightest CarMax relaxation that we can get an so we can be sure that's well.",
            "Well, this is at least as tight as the existing STP convex relaxations.",
            "OK, so hopefully we can get accuracy solutions.",
            "Thought this is tight, this is good."
        ],
        [
            "OK, but then the question is how to solve this.",
            "So recall that this problem OK originates from this problem.",
            "OK, you have a optimization problem.",
            "You want to find the upper bomb, the largest lower bound.",
            "OK of all the G Alpha whitey's.",
            "OK for all the variety in the candidate set.",
            "So now the problem is that well you can have.",
            "Explain exponential number of a.",
            "Elements in B. Becausw B contains all the possible label combinations.",
            "OK, so if you have any patterns that you can have is possible that you can have two power N label combinations.",
            "So that means that you can have an exponential number of constraints here.",
            "OK, so obviously a direct optimization will be computationally expensive.",
            "But the good thing is that a.",
            "Typically at optimality, while we don't expect to have an exponential number of active constraints, OK, so typically we only have a small subset of constraints, so this suggests well we can just find a small subset, hopefully very small OK, and that will still give us a very good approximation.",
            "OK, so in fact this is called the cutting plane method.",
            "OK, so essentially what it does is that well.",
            "You can replace this B which contains all the possible label vector combinations by holding a much smaller working set C. OK, but still this is not the end of the story.",
            "So now the question is how are you going to solve this problem?",
            "OK, so as I said, we're going to use cutting play."
        ],
        [
            "So this is the standard cutting plane.",
            "So first you do initialization.",
            "You set refined one Y hat OK.",
            "Initially the working set is empty.",
            "Then you add this initial one, head into your working set.",
            "So your wife has now contains one wine.",
            "And then you obtain.",
            "The Alpha from this optimization problem OK, and then you generate.",
            "Could be playing essentially.",
            "In this case you generates are violated label vector OK, and then you keep repeating until it terminates.",
            "Say for example well the objective or the.",
            "So this essentially measures the violation of the new label vector.",
            "OK, so if the violation is not too big that you can stop, or if the objective the decrease of the objective value is small, then you can stop.",
            "OK, so this is the kind of basic cutting plane algorithm.",
            "But of course, well you carrying us this cutting plane algorithm, we have to make sure that you know how to solve the two important subsets.",
            "Namely, that is that full.",
            "You will have a current working set and then you have to solve this optimization problem and then.",
            "You also need to know how to generate a violated label vector.",
            "OK, so this is what we call label generation.",
            "So as you can see we will generate a lot of label, so one iteration will give you one label vector.",
            "OK so at the end you will have a number of our label factors.",
            "So before talking about how to solve these issues, OK, we will talk."
        ],
        [
            "But some are general properties of this algorithm.",
            "So let's PT denote the optimal objective value at that iteration.",
            "Then it can be shown that, well, recall that we are doing a minimizations problem.",
            "Then it can be shown that the T + 1 iteration, the objective value will be smaller than the original.",
            "The previous iterations objective value by a certain eater.",
            "OK, so we either depends on that assignment and also depends on some properties of the G. So here there is a C. So C is the.",
            "So here is the richest constant and also the strong competitive constant.",
            "OK so we can be sure that the objective is decreasing across different iterations.",
            "And if P star is the optimal objective then obviously.",
            "With the conversion.",
            "OK, that's many iterations, and more generally so.",
            "Initial iteration, the magnitude of the violation may be different.",
            "OK, so instead of a common assignment, you can have someone out dependent for iteration.",
            "Then a similar analysis shows that the algorithm will converge in our iterations.",
            "Where are satisfies this?",
            "Inequality OK, so essentially what it says is that if you spend a lot of effort define in finding a very violated label, OK, then they have signed an hour will be large, and then hopefully.",
            "These are, that is the number of iterations will be small.",
            "So this represents a tradeoff between the effort our spending in each iteration versus the number of iterations.",
            "OK, so this is a reasonable."
        ],
        [
            "OK, so how to do the cutting plane?",
            "So let's first look at the semi supervised learning problem.",
            "OK so in this case it's a set, so we only have labels on the labeled data DL and we have our labeled patterns and we have a total of N patterns.",
            "OK so for patterns L + 1 to N they are enabled.",
            "And we use Ellen do to denote the two indexes for the labeled and enabled data an we like SVM.",
            "So we use each loss and L2 regularizer.",
            "So this is the standard SVM for semi supervised learning.",
            "So this is the regularizer.",
            "This is the loss on the on the labeled parts and this is for the enabled part.",
            "And recall that we have this B here.",
            "OK, so you can set this P depending on some domain knowledge.",
            "So say for example.",
            "You may want so you have to label the data and you may want the proportions of the two classes in the unlabeled part to be similar to that of the labeled data.",
            "OK, so you can require the whole label infected whiteheads OK to satisfy this constraint.",
            "OK, you can plug in some of the things if you like.",
            "OK, but this is the constraints that we used here."
        ],
        [
            "So how to solve this problem?",
            "So following the.",
            "The previous are sliced OK, so the first step we do is that we convert this, you know optimization to a dual OK, so this is the dual problem.",
            "So as I said, so this is a concave in Alpha and so on and so on.",
            "OK, so this is the comics eight.",
            "OK, this is a standard and then as I said so using the tracks by interchanging the order of the min and Max and then converting the dual and then change the order again.",
            "This is the final objective optimization problem that we obtain.",
            "OK, and then Moreover, as I said, I would use cutting plane.",
            "So I will use a working set to see to replace.",
            "The set B which contains all the possible label assignments."
        ],
        [
            "OK, so how to solve this problem?",
            "So as I said, so the first issue is that using the cutting plane this does for particular see OK you need to know how to solve this optimization problem and then the second issue is that you need to know how to generate a new violated my hat."
        ],
        [
            "So how to do this so?",
            "But the first issue.",
            "OK, so this is the problem that we have to solve.",
            "OK so if we focus on the inner optimization problem then you may notice that this is very similar to the dual of a standard SVM, in particular for the standard SVM.",
            "This part will be of the Form K and then why?",
            "Why prime OK so here.",
            "Instead of having this, we have a summation.",
            "OK, so.",
            "You can regard this summation.",
            "There's a universe you can regard that well.",
            "This is a big kernel, OK, and then this big kernel is our target and it's a combination of the base kernels here.",
            "Hammer over because this meal is in this simplex, so here we have a CarMax combination of kernels.",
            "OK, so as I said so diagrammatically this is what I said before, so you generate some label vectors OK and then each label vector will give you a kernel OK and then iteration you have one kernel.",
            "So at the end you combine all these kernels together, OK?",
            "So how to combine this so to combine these two curves?",
            "These kernels together means that we have to learn the.",
            "Beauties OK so this is nothing special but multiple kernel learning OK, but of course there is some slight difference, which with the standard multiple kernel learning or MCL so usually is standard MCL.",
            "You know the labels why OK and you have a lot of kernels Katie.",
            "And then you try to find the best combination of these kitties.",
            "So here it is slightly different.",
            "So typically you only have one kernel K, But then you have a lot of wise OK so you try to combine all these Ky primes together.",
            "OK, so the form is slightly different but mathematically well you can still write this as a MTL problem."
        ],
        [
            "So.",
            "You can use any MCL algorithm that you like.",
            "OK, in this work we use this particular MKR Group last two algorithm.",
            "So the advantage of this algorithm is that if formulas, the MPL problem is minimalization is adjoins minimization problem over all the variables OK, and so we can use alternating minimization to do the optimization.",
            "So to be more precise, so suppose that at current iteration you have a T label vectors in your working set C. And I so.",
            "It is easy to convert this problem OK into the primal form.",
            "OK, we're just like this.",
            "OK, so this is a joint minimization problem and so we do alternating minimization of block code or code innocence.",
            "So at first step we fix the view OK and then we solve for the WS.",
            "So notice that.",
            "This is in fact a standard SVM OK, or you can also rewrite this way.",
            "OK, so this is a standard SVM, so you can plug in your favorite SVM solver OK and then once you solve you have solved for the eater, then you fix them and solve for the meal and the good thing is that the view is a simplex, so there is a closed form solution for the beauties OK which is given by this one.",
            "And then you reiterate.",
            "So this is how we solve.",
            "This optimization problem OK.",
            "So we're done with the first step."
        ],
        [
            "Now we move on to the second step, so the next issue is that we have to find a violated label assignment.",
            "We have to find a way to add to the working set.",
            "OK, so how to do this?",
            "So recall that.",
            "This is our problem and we have mentioned earlier that it can be written into this form.",
            "OK so.",
            "If you want to find a violated, the most violated labor assignment, that means that you want to find.",
            "So this is.",
            "This data is supposed to be the lower bound.",
            "OK, so if you want to find the violated label assignments, then what you can do is you find the minimum of.",
            "This G objective OK over all the wax.",
            "OK so this G is recall test is of this form.",
            "OK for semi supervised learning and this term is independent of one hand.",
            "So we can drop it and then you can simply revise this as another matrix H. OK so now what you are doing is that you try to maximize.",
            "This convex function.",
            "OK, so ever so this is Max, so this is quite difficult OK?",
            "Wherever the good thing about the cutting plane is that in fact you don't need to find the most my little label assignments.",
            "OK, in fact, the cutting plane algorithm works as long as you add are violated assignments.",
            "OK, are violated constraint OK?",
            "So you don't need to find the most file at the constraints, you just need to find one of the valid constraints.",
            "So here we suggest a method of finding valid constraints.",
            "Which is very simple.",
            "So as I said, so instead of maximizing this function OK over the whole set B. I just want to find."
        ],
        [
            "Awai such that.",
            "This value is greater than the current estimate from the working set.",
            "OK, so how to do this?",
            "But the idea is simple so essentially so this is a quadratic function, so I basically do a linear approximation and then maximize along this line.",
            "OK, it can be shown that, but it is easy to show that this Weinstein is a valid label assignment.",
            "If this value is not equal to this matter.",
            "OK, so essentially what it does is that the converse.",
            "The maximization bulfer quadratic function into the maximization of a linear function.",
            "OK, so of course well, as you may expect, maximizing a median function is easier.",
            "So in fact it is the case so.",
            "Recall that we want to maximize this linear function.",
            "We still want to maximize over the whole set speed, OK?",
            "And nasty knock this buy out OK?",
            "To record the speed is the contains the balance constraints.",
            "OK, so you can also rewrite this way.",
            "So this is a linear function.",
            "OK, so it is easy to show that its optimality.",
            "The yiz here will be aligned with the arise.",
            "That is, if a particular entry our eyes large they are eye is larger than RJ RJ, then the corresponding why I should be also be larger than my J?",
            "OK, So what that means is that if you want to solve for this OK, it's very simple.",
            "You just do a sorting OK.",
            "Arranged our eyes OK being say ascending order and then for this more eyes the corresponding wise should be assigned minus one and then for the larger eyes the corresponding why I should be assigned one?",
            "That's it.",
            "OK, so is very fast."
        ],
        [
            "So this is the whole algorithm, so as I said that you obtain the view and W by solving an MCL problem and then you obtain.",
            "The Vita constraints OK by sorting OK and then you re iterate until it converges."
        ],
        [
            "So first I show you some experience.",
            "OK, so we perform experiments on these standard benchmark is so we compare the proposed development with a number of our standard semi supervised learning SVM.",
            "So so this is standard VM supervised learning.",
            "So this is translated from the process via universal them and so on and so on.",
            "And recall that for the vast near SVM you can plug in your favorite SVM solver.",
            "So in our case, but nonlinear kernels we use the lips VM and then for the linear kernel we use the linear OK. We also tried to compare with the SDP relaxation.",
            "Of the SBM variant which is supposed to Sherman and be in the standard supplies so much Barcelona book OK.",
            "However, this SDP relaxations are very expensive and we cannot conversion OK even on the smallest datasets.",
            "So that's why we only compare with visa faster SVM variants."
        ],
        [
            "So these are the results.",
            "OK, so we use 5% labeled samples an these are the accuracies OK from zero to 1.",
            "So as you can see the SVM is a very competitive and if you compare that with the SPM is better than SVM most of the time.",
            "OK in terms of the average accuracy.",
            "So the proposed algorithm also is the best.",
            "So.",
            "This shows that the proposed algorithm is accurate.",
            "And."
        ],
        [
            "Moreover, it is fast.",
            "OK, so here.",
            "We showed the CPU time so.",
            "In fact, the PSB M is the slowest OK, and the universal SVM is the fastest."
        ],
        [
            "However, the universal mass media.",
            "He's not so accurate in comparison with the other matters."
        ],
        [
            "So.",
            "The last part OK is our well SBM.",
            "OK, so as you can see from all the datasets, OK, it is much faster than the.",
            "SVM and also very compatible with the Laplacian SVM, but it's more accurate."
        ],
        [
            "Anne.",
            "Recall that our algorithm is iterative in nature, so empirically, so the number of iterations is.",
            "So the number of iterations is typically smaller than 25.",
            "OK, so this is pretty fast."
        ],
        [
            "And I'll be also work on some larger than this.",
            "OK, say the real semen MCV, one of our many instances, an becausw.",
            "These are much bigger than this, so we cannot compare with the previous RTS BMS, Anna USPS there so slow.",
            "So we compare with the SDM OK, which is a linear kernel based semi supervised SVM proposal Simonian quality.",
            "Then as you can see.",
            "So there should be ending is not so accurate.",
            "OK so.",
            "Even worse than the SVM.",
            "OK, various hours is more accurate.",
            "An much faster than the SVM."
        ],
        [
            "So we also compare with some other standard SVM.",
            "Some of the methods.",
            "OK, so this is a in the standard SVM semester vice learning book.",
            "OK, chappelle.",
            "Finish your coffee and Alex scene.",
            "So here we welcome the datasets there and then as you can see the performance in terms of the test error is very comperable.",
            "But we believe that our proposed algorithm is faster."
        ],
        [
            "Man, this is comparison with the SDP based benchmark.",
            "OK again, this is much more is compareable but again much fast."
        ],
        [
            "So that's for semi supervised learning.",
            "So now for multiple instance learning so.",
            "We use a similar approach, so we have a data set B OK which contains M packs.",
            "And in each bag you have a number of instances OK, and as I said, so only you only have the back labels but not the instance neighbors.",
            "Hannah.",
            "The multiple instance learning as I said, so we want to isolate the key instance for positive back.",
            "OK, so in fact the you can define the function on the bank.",
            "That's the maximum over border instance evaluations."
        ],
        [
            "So again, we use SVM's.",
            "OK, so.",
            "Hey, my alma mater is this learning the SBM formulation is usually like this so this is the loss for the Apostle facts.",
            "This is the last of the negative facts so.",
            "The green bags are the positive ones.",
            "OK, so recall that.",
            "The protection of Deposit Bank is only based on the key instance.",
            "So suppose that this is the most positive instance.",
            "This is the most possible instance for this back.",
            "OK, so we only look at these two instances.",
            "Where is for lack of fact or the instances should be negative.",
            "OK, so we try to separate the positive key instance.",
            "From the elective instances by a heart plane.",
            "So in that case this will be the best boundary.",
            "OK, so mathematically.",
            "So as I said, so this is the why and then this is the prediction on the bank OK?",
            "So as I said in this case we only have the back labels, but we don't have the instance labels, so we have to estimate.",
            "So the way we do it is that we define.",
            "Vector booty and vector VI for each bag OK. Anna, we assume that each Pacific has only one key instance, so we also put in this constraint and then for the language packs.",
            "OK, all the instances should be negative.",
            "So by using this requirement."
        ],
        [
            "We can rewrite.",
            "The constraints here this way.",
            "OK, so we put in.",
            "All those are the factors which corresponds to the Boolean label assignments.",
            "So for positive back, so we need to only look at the one which is the key instance.",
            "Virus for the like the fact we have to consider all the negative body instances because they're all negative.",
            "OK, so now of course we have to minimize over the Boolean vector D. So.",
            "How to do this so we follow the same procedure.",
            "So first we convert this from the prime."
        ],
        [
            "So to do OK, so this is what we get, so it's a bit complicated, but you can easily verify this.",
            "So.",
            "This is the dual and then we also need to minimize over door those Boolean markers.",
            "So and then we use the same trick.",
            "Well, exchange the order of the minimize minimization and maximization operators converted to do an exchange again.",
            "Then this is the CarMax realization that we obtain.",
            "Ben Ben"
        ],
        [
            "We use cutting playing OK.",
            "So record as the first tab is that you have to fix the working set and then obtain the parameters.",
            "And it is easy to see that this is still my MCL problem.",
            "OK, so now basically the kernels of this form the base kernels of this form.",
            "OK, so this is an MPL, so we're game use MPL, GL.",
            "So after some so we use.",
            "Primal and then plug it in OK."
        ],
        [
            "And then for the.",
            "A second issue, which is on finding the most valid constraints.",
            "So again, if you want to find the most valid constraints, there will be expensive becausw.",
            "To this maximization problem.",
            "So this is a convex OK, so again difficult.",
            "So again, instead of finding the most violated, we.",
            "We are less aggressive.",
            "OK, we just find are violated.",
            "Label assignment.",
            "OK, so the way we do is that again we do linear approximation and then maximize along that direction and so on.",
            "OK and then they can be shown that this.",
            "These are valid label assignment.",
            "If this is larger than this, OK an as in the previous case, this is just a linear function.",
            "So maximize this linear function over the whole domain is still easy.",
            "OK.",
            "In fact by the previous approach we can also do a sorting OK.",
            "So the details I will give it here OK.",
            "So basically doing something OK and then you just record that in each bag you only have one key instance.",
            "So after doing the sorting you just pick the largest elements, sets the corresponding instance to one and then the rest to minus one."
        ],
        [
            "Hi Sarah, OK.",
            "So the experiments we.",
            "Do experiments on CPI are content based image retrieval so?",
            "In some in the multiple instance, learning something.",
            "So the idea is that.",
            "I would love to have this car.",
            "I would have both cars.",
            "So this is a nice girl.",
            "OK, so the question is how about this is the image.",
            "This is back.",
            "Well you can divide us you mesh into a number of different segments.",
            "OK, so the question you say I love this picture.",
            "Why maybe you love the car?",
            "Or maybe you just love to see.",
            "OK you love the rocks.",
            "OK I don't know you love to this guy and so on.",
            "OK so each of these image segments corresponds to an instance.",
            "OK, so in Cpl when we use multiple instance learning we want to protect.",
            "Wow, what is of real interest to you, OK?",
            "So."
        ],
        [
            "We compared with the number of ISBN variance for multiple instance learning these are by Andrew Sattel in 2003 and also there is something called the multiple instance kernel.",
            "OK by Thomas scanner.",
            "So again we compared with this and then there are some known SVM methods.",
            "OK so pretty well along pretty standard so diverse density and diversity and so on and so on.",
            "So these are results.",
            "So performance measure is higher better.",
            "So as you can see we.",
            "Price competitive.",
            "OK."
        ],
        [
            "And here as I said so.",
            "You can use.",
            "You can regard the key instance OK as the region of interest.",
            "OK, so here I show some examples.",
            "So this is our results.",
            "So say for example we label this particular image segment as the region of interest.",
            "In this case we label this one and this One South.",
            "Of course this is somehow subjective, but it seems that the results here are quite reasonable.",
            "OK seems more reasonable than say for example this one and this one.",
            "OK."
        ],
        [
            "But the last application yes are multiple maximum margin clustering.",
            "So the idea is basically the same.",
            "OK, so of course here we have a difference.",
            "Primal problem.",
            "OK, we have a difference.",
            "Be so as I said, well you may want the positive and negative samples to be approximately of the same size.",
            "So use this B or balance constraints.",
            "So you write the dual for this problem OK, which is this one and then you follow the procedure.",
            "You obtain this convex relaxation.",
            "And then you use cut."
        ],
        [
            "Plain pandas step one is still a MCL problem.",
            "Step two.",
            "Well again, we don't want to find the most violent constraints, we just want to find one model constraints.",
            "So we solve this.",
            "This maximization problem was just media.",
            "OK, so again, like the previous two problems, this can be found by some simple sorting.",
            "OK, so I don't go into the details here."
        ],
        [
            "So I just show you some experiments be compared with the number of so these are SDP relaxations.",
            "So there's a standard normalized cut and these are some long comics approaches, iterative SCRN GPMC."
        ],
        [
            "OK, so as you can see from here so the clustering accuracies are quite good.",
            "OK, in comparison with the automatic."
        ],
        [
            "And it is fast, OK?",
            "Then, as I said, the long convex methods also very fast.",
            "Two OK, but ours is quite compareable Ann.",
            "Our method is a much faster than GMC GMC GMC which is based on Sep. OK, so on average is 10 times faster."
        ],
        [
            "So some logical experiments well also performs quite well."
        ],
        [
            "So conclusion is that are well, we learn from weakly labeled later OK, where you may have some missing or incomplete label information, and the proposed algorithm is CarMax.",
            "So based on the so-called label generation procedure.",
            "And at this convex relaxation is tight.",
            "OK, it's the tightest in fact, because it corresponds to the convex Hull.",
            "Optimization problem reduces tool a number of MKL problems.",
            "OK, which can then be solved by standard SVM's office and we know that.",
            "Many SVM solvers are scalable.",
            "OK, an experimental results on these three learning tasks promising.",
            "OK, so the take home message is that if you have.",
            "An optimization problem of this form.",
            "Then maybe you can try to convert that to this problem and then use cutting OK."
        ],
        [
            "So for references.",
            "So these are the references.",
            "OK, thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But today I will talk about learning found wiki labels data, so this is joint work with evenly after John and Joe.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the outline of my presentation, so first I will give an introduction.",
                    "label": 0
                },
                {
                    "sent": "Then I will present the proposed algorithm which is called about SVM, which stands for weakly labeled SVM.",
                    "label": 0
                },
                {
                    "sent": "And we will then apply this to three example applications, namely semi supervised learning, multiple instance learning and the maximum margin cluster.",
                    "label": 1
                },
                {
                    "sent": "And then finally there's the conclusion.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Who?",
                    "label": 0
                },
                {
                    "sent": "First of all, what do I mean by I quickly labeled learning, learning, so we know that are obtaining labeled data is often very hot.",
                    "label": 1
                },
                {
                    "sent": "OK, because it is expensive.",
                    "label": 1
                },
                {
                    "sent": "It involves a lot of Labor, involves time, OK, involves expertise, and so on.",
                    "label": 1
                },
                {
                    "sent": "So in recent years there has been a lot of interest or not studying various forms of learning algorithms in with the problems in which the labels are incomplete or only partially known.",
                    "label": 0
                },
                {
                    "sent": "So for example, in this talk I will focus on this at summer, supplies, supervised learning, multiple instance learning, and maximum marking clustering.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I guess everybody knows what some of us learning is.",
                    "label": 0
                },
                {
                    "sent": "So in this case we have very few labeled data, so just one blue point and one black phones belonging to two different classes.",
                    "label": 1
                },
                {
                    "sent": "But we have a lot of unlabeled data.",
                    "label": 1
                },
                {
                    "sent": "So the goal is that by utilizing this abundance of unlabeled data, we can get better prediction.",
                    "label": 0
                },
                {
                    "sent": "So of course if we just use these two labeled points, then we may position the same boundary here, whereas if you use the unlabeled data distribution then we may have a better solution.",
                    "label": 0
                },
                {
                    "sent": "So semi supervised learning have a lot of applications.",
                    "label": 0
                },
                {
                    "sent": "So for example you know documents processing, image processing, NLP, computer vision and so on and so on.",
                    "label": 0
                },
                {
                    "sent": "So in this case, we say that the labels are only partially known, so universe I know the labels of the labeled data, but not those of the unlabeled data.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So another scenario you wish you only have partial label information is our multiple instance learning.",
                    "label": 0
                },
                {
                    "sent": "So some of you might not be very familiar with this, so I give.",
                    "label": 0
                },
                {
                    "sent": "Yeah, more detailed introduction.",
                    "label": 0
                },
                {
                    "sent": "So multiple instance learning well was first introduced by our company Trish in 97, but the example application there was the drop activity prediction.",
                    "label": 0
                },
                {
                    "sent": "So in this application you're given a drug molecule and then you want to predict whether it can bind to a certain targets.",
                    "label": 1
                },
                {
                    "sent": "OK, so this looks like standard supervised learning.",
                    "label": 0
                },
                {
                    "sent": "OK, basically a classification problem, however, but drop protection because it's complicated.",
                    "label": 1
                },
                {
                    "sent": "In the sense that each molecule can have multiple confirmations.",
                    "label": 0
                },
                {
                    "sent": "So for example, here all these corresponds both.",
                    "label": 0
                },
                {
                    "sent": "These correspond to just one molecule.",
                    "label": 0
                },
                {
                    "sent": "OK, but as you can see so say for example the born here can rip up or down.",
                    "label": 0
                },
                {
                    "sent": "OK, so each one of these corresponds to our confirmation, OK?",
                    "label": 1
                },
                {
                    "sent": "And then.",
                    "label": 1
                },
                {
                    "sent": "When you want to know whether this molecule can bind to a drug combined to a certain target, OK, so you can buy at least one of these confirmations.",
                    "label": 0
                },
                {
                    "sent": "Combine OK forever, unfortunately, but the power chemist can tell the binding capability of the whole molecule OK.",
                    "label": 0
                },
                {
                    "sent": "However, they do not know which particular information.",
                    "label": 0
                },
                {
                    "sent": "In fact spines OK, so in this case we have a weaker form of.",
                    "label": 0
                },
                {
                    "sent": "Labor information more.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In multiple instance learning terminologies, so usually this shape.",
                    "label": 0
                },
                {
                    "sent": "OK that is a each one of these confirmations.",
                    "label": 0
                },
                {
                    "sent": "OK will then correspond to a certain feature representation and then each one of these is called an instance and then the whole molecule which may correspond to a number of different instances.",
                    "label": 0
                },
                {
                    "sent": "This called back OK.",
                    "label": 0
                },
                {
                    "sent": "So as I said earlier, so for a molecule you only know that either whether this molecule can bind or not, but not whether each of these confirmations combine OK, so universe.",
                    "label": 0
                },
                {
                    "sent": "You know that's a bag is labeled positive when it contains at least one positive instance, OK, However, you do not know which one.",
                    "label": 1
                },
                {
                    "sent": "And of course, if all the instances here cannot bind to the target, then the bag will be labeled negative.",
                    "label": 0
                },
                {
                    "sent": "And that positive instance in the case when you have a positive fact.",
                    "label": 0
                },
                {
                    "sent": "Usually it is called the key instance.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is different from traditional supervised learning in the way that's for supervised learning.",
                    "label": 0
                },
                {
                    "sent": "For each pattern, OK, you have the corresponding label forever.",
                    "label": 0
                },
                {
                    "sent": "In multi instance learning.",
                    "label": 0
                },
                {
                    "sent": "So the information is big in the sense that one object or one pattern or one bag can contain a number of instances and the label information is only associated with this.",
                    "label": 1
                },
                {
                    "sent": "But not with the individual instances.",
                    "label": 0
                },
                {
                    "sent": "OK, so in this case we say that the labels are only implicitly known, OK.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the third application.",
                    "label": 0
                },
                {
                    "sent": "These are of course, clustering.",
                    "label": 0
                },
                {
                    "sent": "But in that case the labels are totally unknown because you only have unlabeled data.",
                    "label": 1
                },
                {
                    "sent": "OK, and in particular will be interested in the so called maximize maximum margin clustering, which is proposed by.",
                    "label": 0
                },
                {
                    "sent": "Sherman and his group in NIPS and not in 2005.",
                    "label": 0
                },
                {
                    "sent": "OK, so the idea is that we notice the SVM is doing very well in supervised learning OK.",
                    "label": 0
                },
                {
                    "sent": "However, in that case we have the class labels, so you maximum margin clustering, but we want to do is that we want to extend this large margin concept to unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "So we want to assign labels to these unlabeled patterns.",
                    "label": 0
                },
                {
                    "sent": "While at the same time try to have a large margin for the resultant decision boundary.",
                    "label": 0
                },
                {
                    "sent": "But of course, this problem is much harder because you don't have the labels.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as we can see, so in the so called weak label learning.",
                    "label": 0
                },
                {
                    "sent": "OK, so as in standard supervised learning, we have to learn the parameters.",
                    "label": 1
                },
                {
                    "sent": "Say for example the SVM, the double parameter and besides that cause the label information is only partial or incomplete.",
                    "label": 0
                },
                {
                    "sent": "So we also need to infer the labels.",
                    "label": 1
                },
                {
                    "sent": "OK, so as a whole this will give us a mixed integer programming problem.",
                    "label": 0
                },
                {
                    "sent": "So we're just more complicated.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how to do this?",
                    "label": 0
                },
                {
                    "sent": "Though there are several existing ways of solving this kind of learning problems, so one approach is to use a global optimization techniques.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's a screw, so for example, a number of people have proposed different use.",
                    "label": 0
                },
                {
                    "sent": "The difference optimization techniques, so such as branch and bound with ministering and so on.",
                    "label": 1
                },
                {
                    "sent": "But as we know well, typically these techniques are not so scalable.",
                    "label": 0
                },
                {
                    "sent": "OK, so typically you can live with very small datasets.",
                    "label": 0
                },
                {
                    "sent": "And another major paradigm is used.",
                    "label": 1
                },
                {
                    "sent": "CarMax relaxations in particular SDP relaxations?",
                    "label": 0
                },
                {
                    "sent": "So the good thing about this approach is that, well, you have a convex problem, so you don't have local minimum problem.",
                    "label": 0
                },
                {
                    "sent": "However, as the piece as we know where you're still not so scalable.",
                    "label": 0
                },
                {
                    "sent": "OK, so it can only be used or is more or at most say samples worth 1000 examples OK?",
                    "label": 0
                },
                {
                    "sent": "Another approach is that we forget about complexity and we just use nonconvex optimization.",
                    "label": 0
                },
                {
                    "sent": "OK, in this case you can use our coordinate descent or automating minimization or commas concave procedures and so on.",
                    "label": 0
                },
                {
                    "sent": "So usually they're pretty efficient.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "However, because they are based on comics approaches so.",
                    "label": 0
                },
                {
                    "sent": "There is the possibility that you may get stuck in local minima.",
                    "label": 1
                },
                {
                    "sent": "So the goal of this talk is that we want to find a way so that.",
                    "label": 0
                },
                {
                    "sent": "But optimization procedure is CarMax so that you don't have local minimum problems and Moreover it is scalable much more scalable than these SDP relaxations.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so how do we do it so?",
                    "label": 0
                },
                {
                    "sent": "The proposed algorithm is called the weakly labeled SBM or, whereas the MOK.",
                    "label": 0
                },
                {
                    "sent": "So in fact, it is just a variant of the SVM with something with and other so called.",
                    "label": 1
                },
                {
                    "sent": "Mabel generation process.",
                    "label": 0
                },
                {
                    "sent": "OK so how to do this?",
                    "label": 0
                },
                {
                    "sent": "So this is just an overview.",
                    "label": 0
                },
                {
                    "sent": "So for example if we talk about think about the clustering problems so we don't have the labels.",
                    "label": 0
                },
                {
                    "sent": "OK so this is half.",
                    "label": 0
                },
                {
                    "sent": "So what we want to do is that we try to 1st generate some labels for these unlabeled patterns.",
                    "label": 0
                },
                {
                    "sent": "So for example.",
                    "label": 0
                },
                {
                    "sent": "I may generate, say for example, are these two are one and then these three are minus one OK and then feed this to SPM and the next and the next run I made gas at this.",
                    "label": 0
                },
                {
                    "sent": "This is a -- 1 and then these are one and so on and so on.",
                    "label": 0
                },
                {
                    "sent": "So of course later on we'll see how well the label vectors are generated and then after generating the label vectors OK, so each round will have one label vector.",
                    "label": 0
                },
                {
                    "sent": "So at the end if you will have a number of Labor factors.",
                    "label": 0
                },
                {
                    "sent": "So the learning algorithms also.",
                    "label": 0
                },
                {
                    "sent": "Ha.",
                    "label": 0
                },
                {
                    "sent": "Combine them together, and in particular that we use multiple learning, not multiple kernel learning algorithms to do it OK, and details will be given later and depending on the which will be specific to the particular example application.",
                    "label": 0
                },
                {
                    "sent": "So what's the advantage?",
                    "label": 0
                },
                {
                    "sent": "So the advantages are first, while it can be shown but later on will show that OK.",
                    "label": 0
                },
                {
                    "sent": "This is a tight convex relaxation of the original message integer programming problem.",
                    "label": 1
                },
                {
                    "sent": "OK, now in fact, but it is at least as tight as the existing Seiko Max SDP relaxations, so hopefully that will give us a more accurate solution.",
                    "label": 0
                },
                {
                    "sent": "An morova during the procedure.",
                    "label": 0
                },
                {
                    "sent": "OK, this is iterative.",
                    "label": 0
                },
                {
                    "sent": "The iterative procedure and each iteration, while it can be reduced to standard SVM training.",
                    "label": 0
                },
                {
                    "sent": "OK, So what that means is that we can make use of advances in the development of SVM solvers OK. Or you can just plug in your favorite server OK, and we know that nowadays many of these servers are scalable and efficient.",
                    "label": 0
                },
                {
                    "sent": "OK, so at the end the whole problem is much more scalable than, say, using SDP.",
                    "label": 0
                },
                {
                    "sent": "So of course I'm global optimization problems algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me be more precise.",
                    "label": 0
                },
                {
                    "sent": "So this is so we are given a data set OK D. So we have say N samples.",
                    "label": 0
                },
                {
                    "sent": "Excise the input.",
                    "label": 0
                },
                {
                    "sent": "Why is the output?",
                    "label": 0
                },
                {
                    "sent": "And as the standard machine learning, so we want to find a function F that Maps from the inputs with the output.",
                    "label": 0
                },
                {
                    "sent": "And by minimizing this structural risk, so Omega is the regularizer, an LF is the loss on the data set, and see of course is the parameter that controls the tradeoff.",
                    "label": 1
                },
                {
                    "sent": "These two terms and.",
                    "label": 0
                },
                {
                    "sent": "As in the SVM nature, so we assume that the Omega and loss oppose comics.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the standard supervised learning, so in this status of ice learning.",
                    "label": 0
                },
                {
                    "sent": "You know case record this.",
                    "label": 0
                },
                {
                    "sent": "We talk about weak label learning, so some of the labels may not be available.",
                    "label": 0
                },
                {
                    "sent": "OK so they have to be learned.",
                    "label": 1
                },
                {
                    "sent": "So instead of just learning the F so we have to minimize both the F and also the unknown labels in the white hat.",
                    "label": 1
                },
                {
                    "sent": "OK, so that's why we have an extra minimization on the outside.",
                    "label": 0
                },
                {
                    "sent": "So here, for notational simplicity I just write down Whitehead here.",
                    "label": 0
                },
                {
                    "sent": "But in fact inside my head, some of the labels are known, say for example in semi supervised learning you have some labeled samples, so in fact we don't need to learn those labels.",
                    "label": 0
                },
                {
                    "sent": "OK, but here I'm a bit lazy so I just put down white heads here.",
                    "label": 0
                },
                {
                    "sent": "Amber Alpha.",
                    "label": 1
                },
                {
                    "sent": "Often you can conference white hat to be in some domain B, which is the set of our candidates label assignments.",
                    "label": 0
                },
                {
                    "sent": "So again, the setting of the B depends on your application.",
                    "label": 0
                },
                {
                    "sent": "So say for example.",
                    "label": 0
                },
                {
                    "sent": "But the clustering, though you have two classes, an my priority, you think that the positive and negative samples should be approximately balanced.",
                    "label": 0
                },
                {
                    "sent": "At this stage, the two clusters should be of approximately equal size.",
                    "label": 0
                },
                {
                    "sent": "Then what you can do is to define B this way.",
                    "label": 0
                },
                {
                    "sent": "OK, so use because why I is either plus one or minus one.",
                    "label": 0
                },
                {
                    "sent": "So the sum of all the why I should if they're really balanced then it should be equal to zero.",
                    "label": 0
                },
                {
                    "sent": "OK, if they're only approximately balanced, then well, you can set this to be within minus beta in some N plus beta.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this is the problem that we want to solve.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's come back to the standard SVM first.",
                    "label": 0
                },
                {
                    "sent": "Because this is easier.",
                    "label": 0
                },
                {
                    "sent": "So the SPM we know.",
                    "label": 0
                },
                {
                    "sent": "That's why the primal is this OK and then you can rise the corresponding dual with the offer.",
                    "label": 0
                },
                {
                    "sent": "Is the dual variables, K is the kernel matrix OK?",
                    "label": 1
                },
                {
                    "sent": "Recall that.",
                    "label": 0
                },
                {
                    "sent": "So here in the standard SVM, you only need to maximize over the dual variables OK.",
                    "label": 0
                },
                {
                    "sent": "However, in our case because some of the labels may be missing.",
                    "label": 0
                },
                {
                    "sent": "So besides maximizing the dual the dual variables, we also need to minimize the wise labels.",
                    "label": 0
                },
                {
                    "sent": "OK, so more generally we will consider this.",
                    "label": 1
                },
                {
                    "sent": "Optimization problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so that means that instead of just fixed restricting to the SVM well in principle you can park in some other SVN variance or some of the models.",
                    "label": 0
                },
                {
                    "sent": "So here we only require Alpha to be in a convex set to say for example in the standard SVM this is the convex set a an this G is concave in Alpha.",
                    "label": 1
                },
                {
                    "sent": "So just like a standard SVM objective or any fix it why hat?",
                    "label": 0
                },
                {
                    "sent": "Anna Moreover, sometimes it is convenience to rise this G. Ha, as a Jeep A we have replaced this label vector by some label matrix.",
                    "label": 0
                },
                {
                    "sent": "OK so say for example you can simply define M as YY prime, OK and then you and then you can revise this objective.",
                    "label": 0
                },
                {
                    "sent": "Is this one OK and then we call this the qipao.",
                    "label": 1
                },
                {
                    "sent": "And of course this G bar is concave in Alpha and linear in the matrix M. OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is the problem that we want to solve.",
                    "label": 0
                },
                {
                    "sent": "OK, so recorders are why head is an integer labeled integer valued?",
                    "label": 0
                },
                {
                    "sent": "OK, so in general it is quite difficult to solve.",
                    "label": 0
                },
                {
                    "sent": "So what we do is that we do relaxation OK.",
                    "label": 0
                },
                {
                    "sent": "So instead of solving this problem first, first of all we trying to win to change the order of the Max and min.",
                    "label": 1
                },
                {
                    "sent": "OK, so we know that that will give us another bomb.",
                    "label": 0
                },
                {
                    "sent": "OK on this problem and now we look at the inner optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So we want to minimize this objective G over the White House.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can also write it this way.",
                    "label": 0
                },
                {
                    "sent": "Basically you want to maximize.",
                    "label": 0
                },
                {
                    "sent": "A lower ban on G OK, but all the whitey's in this be OK.",
                    "label": 0
                },
                {
                    "sent": "The two are the same an but this well you can also derive the dual for this.",
                    "label": 0
                },
                {
                    "sent": "Some problem OK babe, by introducing do variables new T OK. Then it is easy to see that this.",
                    "label": 0
                },
                {
                    "sent": "The dual of this problem is this one OK where mu is now lying in a simplex.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "In out loud, the last step.",
                    "label": 0
                },
                {
                    "sent": "Is that OK?",
                    "label": 0
                },
                {
                    "sent": "Now I notice that our this objective OK, this convex immune.",
                    "label": 0
                },
                {
                    "sent": "And I can't get in Alpha.",
                    "label": 0
                },
                {
                    "sent": "OK, sorry, that means that I can again exchange the order of the maximin, and so finally this is the final optimization problem that's I attain.",
                    "label": 0
                },
                {
                    "sent": "So you can see that well originally I have to minimize over this Whitehead here, but now.",
                    "label": 0
                },
                {
                    "sent": "I move the white hat inside a.",
                    "label": 0
                },
                {
                    "sent": "Moreover, minimization becomes a summation OK submission over all the possible Y label vectors OK, and then also I.",
                    "label": 0
                },
                {
                    "sent": "This is mu recorded.",
                    "label": 0
                },
                {
                    "sent": "This is a simplex, so this is a convex combination.",
                    "label": 0
                },
                {
                    "sent": "OK, so now the question is are why is this easier to solve?",
                    "label": 0
                },
                {
                    "sent": "But before that, OK.",
                    "label": 0
                },
                {
                    "sent": "So first I showed this record that this is a relaxation.",
                    "label": 0
                },
                {
                    "sent": "So first I will show that.",
                    "label": 0
                },
                {
                    "sent": "Well, this is a tight relaxation.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So recall this.",
                    "label": 0
                },
                {
                    "sent": "This is the original problem.",
                    "label": 1
                },
                {
                    "sent": "OK, we have a mean.",
                    "label": 0
                },
                {
                    "sent": "Before, why maximal the Alpha and then the objective?",
                    "label": 0
                },
                {
                    "sent": "And as I said earlier, so you can rewrite this objective OK, instead of using the label vectors, you can use the label matrix M. OK, so you can equivalently minimize over EM.",
                    "label": 0
                },
                {
                    "sent": "Both are in this RY zero.",
                    "label": 0
                },
                {
                    "sent": "OK, so where Y zero is just this one.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the original problem.",
                    "label": 1
                },
                {
                    "sent": "So now let's look at our relaxation.",
                    "label": 0
                },
                {
                    "sent": "So this is our relaxed optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So again, well using the same reformulation so.",
                    "label": 0
                },
                {
                    "sent": "I don't want to have the label vector here, so I right in terms of the label matrix OK and then you have summation, you can move the summation inside OK 'cause this G this bar is supposed to be linear in the matrix argument.",
                    "label": 0
                },
                {
                    "sent": "OK so you have this and so you can equivalently define another set by one.",
                    "label": 0
                },
                {
                    "sent": "OK, but this.",
                    "label": 0
                },
                {
                    "sent": "Argument OK, so you basically minimizing M in Y, one OK, and if you compare this.",
                    "label": 0
                },
                {
                    "sent": "Problem with this problem.",
                    "label": 1
                },
                {
                    "sent": "So basically they are the same except that the original problem OK minimizes M in Y, zero.",
                    "label": 0
                },
                {
                    "sent": "OK, murderous, we minimize M / y one OK, and Moreover it is easy to see that's why one is just the convex Hull of why not?",
                    "label": 0
                },
                {
                    "sent": "OK, so we know that the convex how is the tightest comics object.",
                    "label": 0
                },
                {
                    "sent": "OK, so that gives us so that means that this one is in fact the tightest CarMax relaxation that we can get an so we can be sure that's well.",
                    "label": 0
                },
                {
                    "sent": "Well, this is at least as tight as the existing STP convex relaxations.",
                    "label": 1
                },
                {
                    "sent": "OK, so hopefully we can get accuracy solutions.",
                    "label": 0
                },
                {
                    "sent": "Thought this is tight, this is good.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, but then the question is how to solve this.",
                    "label": 1
                },
                {
                    "sent": "So recall that this problem OK originates from this problem.",
                    "label": 0
                },
                {
                    "sent": "OK, you have a optimization problem.",
                    "label": 0
                },
                {
                    "sent": "You want to find the upper bomb, the largest lower bound.",
                    "label": 0
                },
                {
                    "sent": "OK of all the G Alpha whitey's.",
                    "label": 0
                },
                {
                    "sent": "OK for all the variety in the candidate set.",
                    "label": 0
                },
                {
                    "sent": "So now the problem is that well you can have.",
                    "label": 0
                },
                {
                    "sent": "Explain exponential number of a.",
                    "label": 1
                },
                {
                    "sent": "Elements in B. Becausw B contains all the possible label combinations.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you have any patterns that you can have is possible that you can have two power N label combinations.",
                    "label": 0
                },
                {
                    "sent": "So that means that you can have an exponential number of constraints here.",
                    "label": 1
                },
                {
                    "sent": "OK, so obviously a direct optimization will be computationally expensive.",
                    "label": 0
                },
                {
                    "sent": "But the good thing is that a.",
                    "label": 1
                },
                {
                    "sent": "Typically at optimality, while we don't expect to have an exponential number of active constraints, OK, so typically we only have a small subset of constraints, so this suggests well we can just find a small subset, hopefully very small OK, and that will still give us a very good approximation.",
                    "label": 1
                },
                {
                    "sent": "OK, so in fact this is called the cutting plane method.",
                    "label": 0
                },
                {
                    "sent": "OK, so essentially what it does is that well.",
                    "label": 0
                },
                {
                    "sent": "You can replace this B which contains all the possible label vector combinations by holding a much smaller working set C. OK, but still this is not the end of the story.",
                    "label": 0
                },
                {
                    "sent": "So now the question is how are you going to solve this problem?",
                    "label": 0
                },
                {
                    "sent": "OK, so as I said, we're going to use cutting play.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the standard cutting plane.",
                    "label": 0
                },
                {
                    "sent": "So first you do initialization.",
                    "label": 0
                },
                {
                    "sent": "You set refined one Y hat OK.",
                    "label": 0
                },
                {
                    "sent": "Initially the working set is empty.",
                    "label": 0
                },
                {
                    "sent": "Then you add this initial one, head into your working set.",
                    "label": 0
                },
                {
                    "sent": "So your wife has now contains one wine.",
                    "label": 0
                },
                {
                    "sent": "And then you obtain.",
                    "label": 0
                },
                {
                    "sent": "The Alpha from this optimization problem OK, and then you generate.",
                    "label": 1
                },
                {
                    "sent": "Could be playing essentially.",
                    "label": 0
                },
                {
                    "sent": "In this case you generates are violated label vector OK, and then you keep repeating until it terminates.",
                    "label": 0
                },
                {
                    "sent": "Say for example well the objective or the.",
                    "label": 1
                },
                {
                    "sent": "So this essentially measures the violation of the new label vector.",
                    "label": 0
                },
                {
                    "sent": "OK, so if the violation is not too big that you can stop, or if the objective the decrease of the objective value is small, then you can stop.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is the kind of basic cutting plane algorithm.",
                    "label": 0
                },
                {
                    "sent": "But of course, well you carrying us this cutting plane algorithm, we have to make sure that you know how to solve the two important subsets.",
                    "label": 1
                },
                {
                    "sent": "Namely, that is that full.",
                    "label": 1
                },
                {
                    "sent": "You will have a current working set and then you have to solve this optimization problem and then.",
                    "label": 0
                },
                {
                    "sent": "You also need to know how to generate a violated label vector.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is what we call label generation.",
                    "label": 0
                },
                {
                    "sent": "So as you can see we will generate a lot of label, so one iteration will give you one label vector.",
                    "label": 0
                },
                {
                    "sent": "OK so at the end you will have a number of our label factors.",
                    "label": 0
                },
                {
                    "sent": "So before talking about how to solve these issues, OK, we will talk.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But some are general properties of this algorithm.",
                    "label": 0
                },
                {
                    "sent": "So let's PT denote the optimal objective value at that iteration.",
                    "label": 1
                },
                {
                    "sent": "Then it can be shown that, well, recall that we are doing a minimizations problem.",
                    "label": 0
                },
                {
                    "sent": "Then it can be shown that the T + 1 iteration, the objective value will be smaller than the original.",
                    "label": 0
                },
                {
                    "sent": "The previous iterations objective value by a certain eater.",
                    "label": 0
                },
                {
                    "sent": "OK, so we either depends on that assignment and also depends on some properties of the G. So here there is a C. So C is the.",
                    "label": 0
                },
                {
                    "sent": "So here is the richest constant and also the strong competitive constant.",
                    "label": 0
                },
                {
                    "sent": "OK so we can be sure that the objective is decreasing across different iterations.",
                    "label": 0
                },
                {
                    "sent": "And if P star is the optimal objective then obviously.",
                    "label": 0
                },
                {
                    "sent": "With the conversion.",
                    "label": 0
                },
                {
                    "sent": "OK, that's many iterations, and more generally so.",
                    "label": 1
                },
                {
                    "sent": "Initial iteration, the magnitude of the violation may be different.",
                    "label": 1
                },
                {
                    "sent": "OK, so instead of a common assignment, you can have someone out dependent for iteration.",
                    "label": 0
                },
                {
                    "sent": "Then a similar analysis shows that the algorithm will converge in our iterations.",
                    "label": 0
                },
                {
                    "sent": "Where are satisfies this?",
                    "label": 1
                },
                {
                    "sent": "Inequality OK, so essentially what it says is that if you spend a lot of effort define in finding a very violated label, OK, then they have signed an hour will be large, and then hopefully.",
                    "label": 0
                },
                {
                    "sent": "These are, that is the number of iterations will be small.",
                    "label": 0
                },
                {
                    "sent": "So this represents a tradeoff between the effort our spending in each iteration versus the number of iterations.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a reasonable.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so how to do the cutting plane?",
                    "label": 0
                },
                {
                    "sent": "So let's first look at the semi supervised learning problem.",
                    "label": 0
                },
                {
                    "sent": "OK so in this case it's a set, so we only have labels on the labeled data DL and we have our labeled patterns and we have a total of N patterns.",
                    "label": 0
                },
                {
                    "sent": "OK so for patterns L + 1 to N they are enabled.",
                    "label": 0
                },
                {
                    "sent": "And we use Ellen do to denote the two indexes for the labeled and enabled data an we like SVM.",
                    "label": 0
                },
                {
                    "sent": "So we use each loss and L2 regularizer.",
                    "label": 0
                },
                {
                    "sent": "So this is the standard SVM for semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "So this is the regularizer.",
                    "label": 0
                },
                {
                    "sent": "This is the loss on the on the labeled parts and this is for the enabled part.",
                    "label": 0
                },
                {
                    "sent": "And recall that we have this B here.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can set this P depending on some domain knowledge.",
                    "label": 0
                },
                {
                    "sent": "So say for example.",
                    "label": 0
                },
                {
                    "sent": "You may want so you have to label the data and you may want the proportions of the two classes in the unlabeled part to be similar to that of the labeled data.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can require the whole label infected whiteheads OK to satisfy this constraint.",
                    "label": 0
                },
                {
                    "sent": "OK, you can plug in some of the things if you like.",
                    "label": 0
                },
                {
                    "sent": "OK, but this is the constraints that we used here.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how to solve this problem?",
                    "label": 0
                },
                {
                    "sent": "So following the.",
                    "label": 0
                },
                {
                    "sent": "The previous are sliced OK, so the first step we do is that we convert this, you know optimization to a dual OK, so this is the dual problem.",
                    "label": 0
                },
                {
                    "sent": "So as I said, so this is a concave in Alpha and so on and so on.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the comics eight.",
                    "label": 0
                },
                {
                    "sent": "OK, this is a standard and then as I said so using the tracks by interchanging the order of the min and Max and then converting the dual and then change the order again.",
                    "label": 0
                },
                {
                    "sent": "This is the final objective optimization problem that we obtain.",
                    "label": 0
                },
                {
                    "sent": "OK, and then Moreover, as I said, I would use cutting plane.",
                    "label": 0
                },
                {
                    "sent": "So I will use a working set to see to replace.",
                    "label": 0
                },
                {
                    "sent": "The set B which contains all the possible label assignments.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so how to solve this problem?",
                    "label": 0
                },
                {
                    "sent": "So as I said, so the first issue is that using the cutting plane this does for particular see OK you need to know how to solve this optimization problem and then the second issue is that you need to know how to generate a new violated my hat.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how to do this so?",
                    "label": 0
                },
                {
                    "sent": "But the first issue.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the problem that we have to solve.",
                    "label": 0
                },
                {
                    "sent": "OK so if we focus on the inner optimization problem then you may notice that this is very similar to the dual of a standard SVM, in particular for the standard SVM.",
                    "label": 0
                },
                {
                    "sent": "This part will be of the Form K and then why?",
                    "label": 0
                },
                {
                    "sent": "Why prime OK so here.",
                    "label": 0
                },
                {
                    "sent": "Instead of having this, we have a summation.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "You can regard this summation.",
                    "label": 0
                },
                {
                    "sent": "There's a universe you can regard that well.",
                    "label": 0
                },
                {
                    "sent": "This is a big kernel, OK, and then this big kernel is our target and it's a combination of the base kernels here.",
                    "label": 0
                },
                {
                    "sent": "Hammer over because this meal is in this simplex, so here we have a CarMax combination of kernels.",
                    "label": 0
                },
                {
                    "sent": "OK, so as I said so diagrammatically this is what I said before, so you generate some label vectors OK and then each label vector will give you a kernel OK and then iteration you have one kernel.",
                    "label": 0
                },
                {
                    "sent": "So at the end you combine all these kernels together, OK?",
                    "label": 0
                },
                {
                    "sent": "So how to combine this so to combine these two curves?",
                    "label": 0
                },
                {
                    "sent": "These kernels together means that we have to learn the.",
                    "label": 0
                },
                {
                    "sent": "Beauties OK so this is nothing special but multiple kernel learning OK, but of course there is some slight difference, which with the standard multiple kernel learning or MCL so usually is standard MCL.",
                    "label": 0
                },
                {
                    "sent": "You know the labels why OK and you have a lot of kernels Katie.",
                    "label": 0
                },
                {
                    "sent": "And then you try to find the best combination of these kitties.",
                    "label": 0
                },
                {
                    "sent": "So here it is slightly different.",
                    "label": 0
                },
                {
                    "sent": "So typically you only have one kernel K, But then you have a lot of wise OK so you try to combine all these Ky primes together.",
                    "label": 0
                },
                {
                    "sent": "OK, so the form is slightly different but mathematically well you can still write this as a MTL problem.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "You can use any MCL algorithm that you like.",
                    "label": 0
                },
                {
                    "sent": "OK, in this work we use this particular MKR Group last two algorithm.",
                    "label": 0
                },
                {
                    "sent": "So the advantage of this algorithm is that if formulas, the MPL problem is minimalization is adjoins minimization problem over all the variables OK, and so we can use alternating minimization to do the optimization.",
                    "label": 0
                },
                {
                    "sent": "So to be more precise, so suppose that at current iteration you have a T label vectors in your working set C. And I so.",
                    "label": 1
                },
                {
                    "sent": "It is easy to convert this problem OK into the primal form.",
                    "label": 0
                },
                {
                    "sent": "OK, we're just like this.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a joint minimization problem and so we do alternating minimization of block code or code innocence.",
                    "label": 1
                },
                {
                    "sent": "So at first step we fix the view OK and then we solve for the WS.",
                    "label": 0
                },
                {
                    "sent": "So notice that.",
                    "label": 0
                },
                {
                    "sent": "This is in fact a standard SVM OK, or you can also rewrite this way.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a standard SVM, so you can plug in your favorite SVM solver OK and then once you solve you have solved for the eater, then you fix them and solve for the meal and the good thing is that the view is a simplex, so there is a closed form solution for the beauties OK which is given by this one.",
                    "label": 0
                },
                {
                    "sent": "And then you reiterate.",
                    "label": 0
                },
                {
                    "sent": "So this is how we solve.",
                    "label": 0
                },
                {
                    "sent": "This optimization problem OK.",
                    "label": 0
                },
                {
                    "sent": "So we're done with the first step.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we move on to the second step, so the next issue is that we have to find a violated label assignment.",
                    "label": 1
                },
                {
                    "sent": "We have to find a way to add to the working set.",
                    "label": 0
                },
                {
                    "sent": "OK, so how to do this?",
                    "label": 0
                },
                {
                    "sent": "So recall that.",
                    "label": 0
                },
                {
                    "sent": "This is our problem and we have mentioned earlier that it can be written into this form.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "If you want to find a violated, the most violated labor assignment, that means that you want to find.",
                    "label": 1
                },
                {
                    "sent": "So this is.",
                    "label": 1
                },
                {
                    "sent": "This data is supposed to be the lower bound.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you want to find the violated label assignments, then what you can do is you find the minimum of.",
                    "label": 0
                },
                {
                    "sent": "This G objective OK over all the wax.",
                    "label": 0
                },
                {
                    "sent": "OK so this G is recall test is of this form.",
                    "label": 0
                },
                {
                    "sent": "OK for semi supervised learning and this term is independent of one hand.",
                    "label": 0
                },
                {
                    "sent": "So we can drop it and then you can simply revise this as another matrix H. OK so now what you are doing is that you try to maximize.",
                    "label": 0
                },
                {
                    "sent": "This convex function.",
                    "label": 0
                },
                {
                    "sent": "OK, so ever so this is Max, so this is quite difficult OK?",
                    "label": 1
                },
                {
                    "sent": "Wherever the good thing about the cutting plane is that in fact you don't need to find the most my little label assignments.",
                    "label": 0
                },
                {
                    "sent": "OK, in fact, the cutting plane algorithm works as long as you add are violated assignments.",
                    "label": 0
                },
                {
                    "sent": "OK, are violated constraint OK?",
                    "label": 0
                },
                {
                    "sent": "So you don't need to find the most file at the constraints, you just need to find one of the valid constraints.",
                    "label": 0
                },
                {
                    "sent": "So here we suggest a method of finding valid constraints.",
                    "label": 0
                },
                {
                    "sent": "Which is very simple.",
                    "label": 0
                },
                {
                    "sent": "So as I said, so instead of maximizing this function OK over the whole set B. I just want to find.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Awai such that.",
                    "label": 0
                },
                {
                    "sent": "This value is greater than the current estimate from the working set.",
                    "label": 0
                },
                {
                    "sent": "OK, so how to do this?",
                    "label": 0
                },
                {
                    "sent": "But the idea is simple so essentially so this is a quadratic function, so I basically do a linear approximation and then maximize along this line.",
                    "label": 0
                },
                {
                    "sent": "OK, it can be shown that, but it is easy to show that this Weinstein is a valid label assignment.",
                    "label": 1
                },
                {
                    "sent": "If this value is not equal to this matter.",
                    "label": 0
                },
                {
                    "sent": "OK, so essentially what it does is that the converse.",
                    "label": 0
                },
                {
                    "sent": "The maximization bulfer quadratic function into the maximization of a linear function.",
                    "label": 0
                },
                {
                    "sent": "OK, so of course well, as you may expect, maximizing a median function is easier.",
                    "label": 0
                },
                {
                    "sent": "So in fact it is the case so.",
                    "label": 0
                },
                {
                    "sent": "Recall that we want to maximize this linear function.",
                    "label": 0
                },
                {
                    "sent": "We still want to maximize over the whole set speed, OK?",
                    "label": 0
                },
                {
                    "sent": "And nasty knock this buy out OK?",
                    "label": 0
                },
                {
                    "sent": "To record the speed is the contains the balance constraints.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can also rewrite this way.",
                    "label": 0
                },
                {
                    "sent": "So this is a linear function.",
                    "label": 0
                },
                {
                    "sent": "OK, so it is easy to show that its optimality.",
                    "label": 0
                },
                {
                    "sent": "The yiz here will be aligned with the arise.",
                    "label": 1
                },
                {
                    "sent": "That is, if a particular entry our eyes large they are eye is larger than RJ RJ, then the corresponding why I should be also be larger than my J?",
                    "label": 0
                },
                {
                    "sent": "OK, So what that means is that if you want to solve for this OK, it's very simple.",
                    "label": 0
                },
                {
                    "sent": "You just do a sorting OK.",
                    "label": 0
                },
                {
                    "sent": "Arranged our eyes OK being say ascending order and then for this more eyes the corresponding wise should be assigned minus one and then for the larger eyes the corresponding why I should be assigned one?",
                    "label": 0
                },
                {
                    "sent": "That's it.",
                    "label": 0
                },
                {
                    "sent": "OK, so is very fast.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the whole algorithm, so as I said that you obtain the view and W by solving an MCL problem and then you obtain.",
                    "label": 0
                },
                {
                    "sent": "The Vita constraints OK by sorting OK and then you re iterate until it converges.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first I show you some experience.",
                    "label": 0
                },
                {
                    "sent": "OK, so we perform experiments on these standard benchmark is so we compare the proposed development with a number of our standard semi supervised learning SVM.",
                    "label": 0
                },
                {
                    "sent": "So so this is standard VM supervised learning.",
                    "label": 0
                },
                {
                    "sent": "So this is translated from the process via universal them and so on and so on.",
                    "label": 0
                },
                {
                    "sent": "And recall that for the vast near SVM you can plug in your favorite SVM solver.",
                    "label": 0
                },
                {
                    "sent": "So in our case, but nonlinear kernels we use the lips VM and then for the linear kernel we use the linear OK. We also tried to compare with the SDP relaxation.",
                    "label": 0
                },
                {
                    "sent": "Of the SBM variant which is supposed to Sherman and be in the standard supplies so much Barcelona book OK.",
                    "label": 0
                },
                {
                    "sent": "However, this SDP relaxations are very expensive and we cannot conversion OK even on the smallest datasets.",
                    "label": 0
                },
                {
                    "sent": "So that's why we only compare with visa faster SVM variants.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these are the results.",
                    "label": 0
                },
                {
                    "sent": "OK, so we use 5% labeled samples an these are the accuracies OK from zero to 1.",
                    "label": 0
                },
                {
                    "sent": "So as you can see the SVM is a very competitive and if you compare that with the SPM is better than SVM most of the time.",
                    "label": 0
                },
                {
                    "sent": "OK in terms of the average accuracy.",
                    "label": 0
                },
                {
                    "sent": "So the proposed algorithm also is the best.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This shows that the proposed algorithm is accurate.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Moreover, it is fast.",
                    "label": 0
                },
                {
                    "sent": "OK, so here.",
                    "label": 0
                },
                {
                    "sent": "We showed the CPU time so.",
                    "label": 0
                },
                {
                    "sent": "In fact, the PSB M is the slowest OK, and the universal SVM is the fastest.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, the universal mass media.",
                    "label": 0
                },
                {
                    "sent": "He's not so accurate in comparison with the other matters.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The last part OK is our well SBM.",
                    "label": 0
                },
                {
                    "sent": "OK, so as you can see from all the datasets, OK, it is much faster than the.",
                    "label": 0
                },
                {
                    "sent": "SVM and also very compatible with the Laplacian SVM, but it's more accurate.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Recall that our algorithm is iterative in nature, so empirically, so the number of iterations is.",
                    "label": 0
                },
                {
                    "sent": "So the number of iterations is typically smaller than 25.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is pretty fast.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I'll be also work on some larger than this.",
                    "label": 0
                },
                {
                    "sent": "OK, say the real semen MCV, one of our many instances, an becausw.",
                    "label": 0
                },
                {
                    "sent": "These are much bigger than this, so we cannot compare with the previous RTS BMS, Anna USPS there so slow.",
                    "label": 0
                },
                {
                    "sent": "So we compare with the SDM OK, which is a linear kernel based semi supervised SVM proposal Simonian quality.",
                    "label": 0
                },
                {
                    "sent": "Then as you can see.",
                    "label": 0
                },
                {
                    "sent": "So there should be ending is not so accurate.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "Even worse than the SVM.",
                    "label": 0
                },
                {
                    "sent": "OK, various hours is more accurate.",
                    "label": 0
                },
                {
                    "sent": "An much faster than the SVM.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we also compare with some other standard SVM.",
                    "label": 0
                },
                {
                    "sent": "Some of the methods.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a in the standard SVM semester vice learning book.",
                    "label": 0
                },
                {
                    "sent": "OK, chappelle.",
                    "label": 0
                },
                {
                    "sent": "Finish your coffee and Alex scene.",
                    "label": 0
                },
                {
                    "sent": "So here we welcome the datasets there and then as you can see the performance in terms of the test error is very comperable.",
                    "label": 0
                },
                {
                    "sent": "But we believe that our proposed algorithm is faster.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Man, this is comparison with the SDP based benchmark.",
                    "label": 0
                },
                {
                    "sent": "OK again, this is much more is compareable but again much fast.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's for semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "So now for multiple instance learning so.",
                    "label": 0
                },
                {
                    "sent": "We use a similar approach, so we have a data set B OK which contains M packs.",
                    "label": 1
                },
                {
                    "sent": "And in each bag you have a number of instances OK, and as I said, so only you only have the back labels but not the instance neighbors.",
                    "label": 1
                },
                {
                    "sent": "Hannah.",
                    "label": 0
                },
                {
                    "sent": "The multiple instance learning as I said, so we want to isolate the key instance for positive back.",
                    "label": 1
                },
                {
                    "sent": "OK, so in fact the you can define the function on the bank.",
                    "label": 0
                },
                {
                    "sent": "That's the maximum over border instance evaluations.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So again, we use SVM's.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Hey, my alma mater is this learning the SBM formulation is usually like this so this is the loss for the Apostle facts.",
                    "label": 0
                },
                {
                    "sent": "This is the last of the negative facts so.",
                    "label": 0
                },
                {
                    "sent": "The green bags are the positive ones.",
                    "label": 0
                },
                {
                    "sent": "OK, so recall that.",
                    "label": 0
                },
                {
                    "sent": "The protection of Deposit Bank is only based on the key instance.",
                    "label": 0
                },
                {
                    "sent": "So suppose that this is the most positive instance.",
                    "label": 0
                },
                {
                    "sent": "This is the most possible instance for this back.",
                    "label": 0
                },
                {
                    "sent": "OK, so we only look at these two instances.",
                    "label": 0
                },
                {
                    "sent": "Where is for lack of fact or the instances should be negative.",
                    "label": 0
                },
                {
                    "sent": "OK, so we try to separate the positive key instance.",
                    "label": 1
                },
                {
                    "sent": "From the elective instances by a heart plane.",
                    "label": 0
                },
                {
                    "sent": "So in that case this will be the best boundary.",
                    "label": 0
                },
                {
                    "sent": "OK, so mathematically.",
                    "label": 0
                },
                {
                    "sent": "So as I said, so this is the why and then this is the prediction on the bank OK?",
                    "label": 0
                },
                {
                    "sent": "So as I said in this case we only have the back labels, but we don't have the instance labels, so we have to estimate.",
                    "label": 0
                },
                {
                    "sent": "So the way we do it is that we define.",
                    "label": 0
                },
                {
                    "sent": "Vector booty and vector VI for each bag OK. Anna, we assume that each Pacific has only one key instance, so we also put in this constraint and then for the language packs.",
                    "label": 1
                },
                {
                    "sent": "OK, all the instances should be negative.",
                    "label": 0
                },
                {
                    "sent": "So by using this requirement.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can rewrite.",
                    "label": 0
                },
                {
                    "sent": "The constraints here this way.",
                    "label": 0
                },
                {
                    "sent": "OK, so we put in.",
                    "label": 0
                },
                {
                    "sent": "All those are the factors which corresponds to the Boolean label assignments.",
                    "label": 0
                },
                {
                    "sent": "So for positive back, so we need to only look at the one which is the key instance.",
                    "label": 0
                },
                {
                    "sent": "Virus for the like the fact we have to consider all the negative body instances because they're all negative.",
                    "label": 0
                },
                {
                    "sent": "OK, so now of course we have to minimize over the Boolean vector D. So.",
                    "label": 0
                },
                {
                    "sent": "How to do this so we follow the same procedure.",
                    "label": 0
                },
                {
                    "sent": "So first we convert this from the prime.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to do OK, so this is what we get, so it's a bit complicated, but you can easily verify this.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is the dual and then we also need to minimize over door those Boolean markers.",
                    "label": 0
                },
                {
                    "sent": "So and then we use the same trick.",
                    "label": 0
                },
                {
                    "sent": "Well, exchange the order of the minimize minimization and maximization operators converted to do an exchange again.",
                    "label": 0
                },
                {
                    "sent": "Then this is the CarMax realization that we obtain.",
                    "label": 0
                },
                {
                    "sent": "Ben Ben",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We use cutting playing OK.",
                    "label": 0
                },
                {
                    "sent": "So record as the first tab is that you have to fix the working set and then obtain the parameters.",
                    "label": 0
                },
                {
                    "sent": "And it is easy to see that this is still my MCL problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so now basically the kernels of this form the base kernels of this form.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is an MPL, so we're game use MPL, GL.",
                    "label": 0
                },
                {
                    "sent": "So after some so we use.",
                    "label": 0
                },
                {
                    "sent": "Primal and then plug it in OK.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then for the.",
                    "label": 0
                },
                {
                    "sent": "A second issue, which is on finding the most valid constraints.",
                    "label": 0
                },
                {
                    "sent": "So again, if you want to find the most valid constraints, there will be expensive becausw.",
                    "label": 1
                },
                {
                    "sent": "To this maximization problem.",
                    "label": 0
                },
                {
                    "sent": "So this is a convex OK, so again difficult.",
                    "label": 1
                },
                {
                    "sent": "So again, instead of finding the most violated, we.",
                    "label": 1
                },
                {
                    "sent": "We are less aggressive.",
                    "label": 0
                },
                {
                    "sent": "OK, we just find are violated.",
                    "label": 0
                },
                {
                    "sent": "Label assignment.",
                    "label": 0
                },
                {
                    "sent": "OK, so the way we do is that again we do linear approximation and then maximize along that direction and so on.",
                    "label": 0
                },
                {
                    "sent": "OK and then they can be shown that this.",
                    "label": 1
                },
                {
                    "sent": "These are valid label assignment.",
                    "label": 0
                },
                {
                    "sent": "If this is larger than this, OK an as in the previous case, this is just a linear function.",
                    "label": 0
                },
                {
                    "sent": "So maximize this linear function over the whole domain is still easy.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "In fact by the previous approach we can also do a sorting OK.",
                    "label": 0
                },
                {
                    "sent": "So the details I will give it here OK.",
                    "label": 0
                },
                {
                    "sent": "So basically doing something OK and then you just record that in each bag you only have one key instance.",
                    "label": 0
                },
                {
                    "sent": "So after doing the sorting you just pick the largest elements, sets the corresponding instance to one and then the rest to minus one.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi Sarah, OK.",
                    "label": 0
                },
                {
                    "sent": "So the experiments we.",
                    "label": 0
                },
                {
                    "sent": "Do experiments on CPI are content based image retrieval so?",
                    "label": 0
                },
                {
                    "sent": "In some in the multiple instance, learning something.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that.",
                    "label": 0
                },
                {
                    "sent": "I would love to have this car.",
                    "label": 0
                },
                {
                    "sent": "I would have both cars.",
                    "label": 0
                },
                {
                    "sent": "So this is a nice girl.",
                    "label": 0
                },
                {
                    "sent": "OK, so the question is how about this is the image.",
                    "label": 0
                },
                {
                    "sent": "This is back.",
                    "label": 0
                },
                {
                    "sent": "Well you can divide us you mesh into a number of different segments.",
                    "label": 0
                },
                {
                    "sent": "OK, so the question you say I love this picture.",
                    "label": 0
                },
                {
                    "sent": "Why maybe you love the car?",
                    "label": 0
                },
                {
                    "sent": "Or maybe you just love to see.",
                    "label": 0
                },
                {
                    "sent": "OK you love the rocks.",
                    "label": 0
                },
                {
                    "sent": "OK I don't know you love to this guy and so on.",
                    "label": 0
                },
                {
                    "sent": "OK so each of these image segments corresponds to an instance.",
                    "label": 0
                },
                {
                    "sent": "OK, so in Cpl when we use multiple instance learning we want to protect.",
                    "label": 0
                },
                {
                    "sent": "Wow, what is of real interest to you, OK?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We compared with the number of ISBN variance for multiple instance learning these are by Andrew Sattel in 2003 and also there is something called the multiple instance kernel.",
                    "label": 0
                },
                {
                    "sent": "OK by Thomas scanner.",
                    "label": 0
                },
                {
                    "sent": "So again we compared with this and then there are some known SVM methods.",
                    "label": 0
                },
                {
                    "sent": "OK so pretty well along pretty standard so diverse density and diversity and so on and so on.",
                    "label": 0
                },
                {
                    "sent": "So these are results.",
                    "label": 0
                },
                {
                    "sent": "So performance measure is higher better.",
                    "label": 0
                },
                {
                    "sent": "So as you can see we.",
                    "label": 0
                },
                {
                    "sent": "Price competitive.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here as I said so.",
                    "label": 0
                },
                {
                    "sent": "You can use.",
                    "label": 0
                },
                {
                    "sent": "You can regard the key instance OK as the region of interest.",
                    "label": 1
                },
                {
                    "sent": "OK, so here I show some examples.",
                    "label": 0
                },
                {
                    "sent": "So this is our results.",
                    "label": 0
                },
                {
                    "sent": "So say for example we label this particular image segment as the region of interest.",
                    "label": 0
                },
                {
                    "sent": "In this case we label this one and this One South.",
                    "label": 0
                },
                {
                    "sent": "Of course this is somehow subjective, but it seems that the results here are quite reasonable.",
                    "label": 0
                },
                {
                    "sent": "OK seems more reasonable than say for example this one and this one.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But the last application yes are multiple maximum margin clustering.",
                    "label": 1
                },
                {
                    "sent": "So the idea is basically the same.",
                    "label": 0
                },
                {
                    "sent": "OK, so of course here we have a difference.",
                    "label": 0
                },
                {
                    "sent": "Primal problem.",
                    "label": 0
                },
                {
                    "sent": "OK, we have a difference.",
                    "label": 0
                },
                {
                    "sent": "Be so as I said, well you may want the positive and negative samples to be approximately of the same size.",
                    "label": 0
                },
                {
                    "sent": "So use this B or balance constraints.",
                    "label": 0
                },
                {
                    "sent": "So you write the dual for this problem OK, which is this one and then you follow the procedure.",
                    "label": 1
                },
                {
                    "sent": "You obtain this convex relaxation.",
                    "label": 0
                },
                {
                    "sent": "And then you use cut.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Plain pandas step one is still a MCL problem.",
                    "label": 0
                },
                {
                    "sent": "Step two.",
                    "label": 0
                },
                {
                    "sent": "Well again, we don't want to find the most violent constraints, we just want to find one model constraints.",
                    "label": 0
                },
                {
                    "sent": "So we solve this.",
                    "label": 0
                },
                {
                    "sent": "This maximization problem was just media.",
                    "label": 0
                },
                {
                    "sent": "OK, so again, like the previous two problems, this can be found by some simple sorting.",
                    "label": 0
                },
                {
                    "sent": "OK, so I don't go into the details here.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I just show you some experiments be compared with the number of so these are SDP relaxations.",
                    "label": 0
                },
                {
                    "sent": "So there's a standard normalized cut and these are some long comics approaches, iterative SCRN GPMC.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so as you can see from here so the clustering accuracies are quite good.",
                    "label": 0
                },
                {
                    "sent": "OK, in comparison with the automatic.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And it is fast, OK?",
                    "label": 0
                },
                {
                    "sent": "Then, as I said, the long convex methods also very fast.",
                    "label": 0
                },
                {
                    "sent": "Two OK, but ours is quite compareable Ann.",
                    "label": 0
                },
                {
                    "sent": "Our method is a much faster than GMC GMC GMC which is based on Sep. OK, so on average is 10 times faster.",
                    "label": 1
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So some logical experiments well also performs quite well.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So conclusion is that are well, we learn from weakly labeled later OK, where you may have some missing or incomplete label information, and the proposed algorithm is CarMax.",
                    "label": 1
                },
                {
                    "sent": "So based on the so-called label generation procedure.",
                    "label": 1
                },
                {
                    "sent": "And at this convex relaxation is tight.",
                    "label": 0
                },
                {
                    "sent": "OK, it's the tightest in fact, because it corresponds to the convex Hull.",
                    "label": 0
                },
                {
                    "sent": "Optimization problem reduces tool a number of MKL problems.",
                    "label": 0
                },
                {
                    "sent": "OK, which can then be solved by standard SVM's office and we know that.",
                    "label": 0
                },
                {
                    "sent": "Many SVM solvers are scalable.",
                    "label": 1
                },
                {
                    "sent": "OK, an experimental results on these three learning tasks promising.",
                    "label": 0
                },
                {
                    "sent": "OK, so the take home message is that if you have.",
                    "label": 0
                },
                {
                    "sent": "An optimization problem of this form.",
                    "label": 0
                },
                {
                    "sent": "Then maybe you can try to convert that to this problem and then use cutting OK.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for references.",
                    "label": 0
                },
                {
                    "sent": "So these are the references.",
                    "label": 0
                },
                {
                    "sent": "OK, thanks.",
                    "label": 0
                }
            ]
        }
    }
}