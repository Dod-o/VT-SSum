{
    "id": "bajkfiy3l2iblt4xcuwz35koxuenujia",
    "title": "Tractable Nonparametric Bayesian Inference in Poisson Processes with Gaussian Process Intensities",
    "info": {
        "author": [
            "Ryan Prescott Adams, Department of Computer Science, University of Toronto"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_adams_tnbi/",
    "segmentation": [
        [
            "This is this is work with Anne Murray at Toronto, an my supervisor David Mackay.",
            "Cambridge and I know this is a big obnoxious sounding title, but in fact hopefully you'll see it.",
            "It's really not that not that bad.",
            "I mean, the main idea is pretty simple.",
            "So what we want to do is model A plus on process.",
            "Somebody hands us a chunk of data that are events say in time or space.",
            "We want to model with the person.",
            "Process this post."
        ],
        [
            "Process has an intensity function.",
            "Would like to put a GP prior on that intensity function and then be able to do inference in a tractable way, and that's that's the big idea.",
            "So we have some events here and we'd like to find the distribution of the posterior distribution over over the rate functions that generated this, and using a GP prior on that.",
            "So that's the big idea here."
        ],
        [
            "Just to give you a quick overview, I'm going to talk a little bit about posaunen Cox processes, and then I'll talk about our model, which we call the sigmoid Gaussian Cox process and I'll tell you a little bit about inference and then hopefully show you that doing something useful."
        ],
        [
            "Data, but first let's talk a little bit about the Sonic cash processes."
        ],
        [
            "So your basic point process is a random subset or finite subset of of what we think of as events.",
            "So time is the thing which which were maybe most familiar, so you're standing at the bus stop and these events are coming along and you know the bus appears random.",
            "Maybe if you don't know much about schedules or if the bus is really unreliable.",
            "Or maybe it would you know these points would be."
        ],
        [
            "The injuries of your favorite football player or something along these lines.",
            "We could also have."
        ],
        [
            "Spatial point processes, so these might be trees and forests or something like that.",
            "And then we can have also things like space."
        ],
        [
            "Show temporal for some processes, and there's such a neat example of this that I'm going to show you this real fast 'cause this is one of my favorite examples of a spatial temporal mark person process.",
            "So zappos.com is the like the Amazon.com of shoes in the United States, and they've created this real time shoe purchasing map and so hopefully it will be working.",
            "But basically what you're saying is events popping up as people buy things and they're having it.",
            "Having a random place at a random time and then they have a random distribution over shoes or whatever.",
            "They are an I love this, 'cause it's a really good example of sort of.",
            "A real life person process or not, plus unnecessarily but but point process in space and time."
        ],
        [
            "And I just wanted to show you that 'cause I think it's neat.",
            "Alright, so the really the."
        ],
        [
            "Basic kind of point processes the homogeneous plus on process and this is where we have some constant intensity that will use Lambda to denote and the two big sort of properties of a homogeneous and process are that disjoint intervals.",
            "The number of events we expect to get within disjoint intervals are independent and then also if we grab a particular interval and we want to know how many days are going to be in it, and that's going to be a person distributed number and it's going to relate to volume of that interval.",
            "Multiplied by the rate graphically, we can take a."
        ],
        [
            "Get the idea by imagining that we have some some person process with a constant rate here and then we have some events and we wanted to ask the question in this little window.",
            "Here, how many events were going to get?",
            "And that's going to be put on distributed, so the probability mass function looks something like this and that's more or less the whole story with."
        ],
        [
            "With the homogeneous Wilson process, but of course that's a really limiting assumption and the real in real life are interesting models.",
            "We want to have an homogeneous rate, something that varies with time, and so for that we extend it so that we have instead of a constant rate.",
            "We have this, this Lambda T as a function of time, and instead of looking at just the size of the interval."
        ],
        [
            "We're going to do now is we're going to integrate the rate function so we have some little more complex rate function.",
            "And now if we want to know how many how many events were going to get inside some little window, then we integrate this thing.",
            "And again, we gotta push."
        ],
        [
            "Distribution, so the question is, if we're actually looking at real data, do we do we want to make strong assumptions about the about this intensity function and the answer is often we don't because we maybe don't know with this specific parametric functional form when we do non Bayesian inference in these kind of models, then a stochastic process is a really nice thing to use as a prior on that on that intensity function.",
            "And when we do that when we have a stochastic process generating our intensity that then generates Wilson data, we call that a Cox process.",
            "Now Cox called this doubly doubly stochastic person process but.",
            "Like all really good ideas, that kind of needs, you know, a good proper noun for it, and then in the specific case where that latent, where that latent stochastic processes a Gaussian process, then we call it a Gaussian Cox process.",
            "So just to be clear, 'cause I've said process like 40 times in the last five minutes then.",
            "So we have some Gaussian process that's generating a random intensity and then that random intensity is generating posts on data.",
            "OK, and this talk is about constructing a Gaussian Cox process.",
            "That's going to allow us to do tractable inference via Markov chain Monte Carlo so.",
            "Now, if you were just going to construct one of these from scratch, you'd look at the post."
        ],
        [
            "Process and say that the major requirement that you had is that the person intensity function needs to be non negative, so you know a lot of times the machine learning when we want to have some real value thing we want to make it negative, then we squeeze it through an exponential function and now it's positive.",
            "So to do that we would create what's called the log Gaussian Cox process and this has been around for a little while.",
            "So if G here is some draw from GP then we could get to a Lambda T just by exponentiating it and this is pretty easy.",
            "The problem is that."
        ],
        [
            "In general, this model is intractable, so for one thing, if we want to do inference, this object G is uncountably infinite in dimension, and we can't realistically hold this thing in memory, but also the posterior distribution that we would get when we tried to perform inference with this object is called doubly intractable.",
            "Now, what that means is that the likelihood function has a constant that we can't evaluate, and you can see that right here by."
        ],
        [
            "If we if we look at the likelihood function, then of you know some chunk of K events and G is this infinite dimensional function?",
            "That's our.",
            "That's our parameter.",
            "Then we need to in order to evaluate the likelihood we need to integrate this.",
            "Integrate this function that we just drew from the GP.",
            "And in general this is unknowable innocence.",
            "So if we have this infinite dimensional object, we need to evaluate it everywhere to in order to determine this interval and so realistically we can't know the likelihood function except within a constant.",
            "Now it happens all the time."
        ],
        [
            "Of course, in Bayesian inference that we run into intractable integrals and interesting models, the marginal likelihood, the bit that's on the bottom of the posterior distribution is almost never able to be evaluated.",
            "But we have a very large toolkit for dealing with this situation specifically, Markov chain Monte Carlo.",
            "And the reason this all kind of works is because the parameter, the marginal likelihood doesn't depend on the parameters.",
            "But in a doubly intractable distribution it's the likelihood function.",
            "The thing that's telling us that's telling us about the relationship between parameters and data that have that has the intractability.",
            "The good news is, though, that there's been some recent developments in Markov chain Monte Carlo, and we can now perform inference in these models under some of the under specific circumstances that you can generate exact data.",
            "Now, when I say exactly what I mean is that if you somebody hand you one of these models and a particular set of parameters that you're going to be able to generate fantasies from it that are not biased, so you couldn't, for example, just run a finite naive Markov chain, and because that would be biased by the starting state.",
            "So what this is about is constructing a Gaussian."
        ],
        [
            "Cox process from which we're going to be able to generate this kind of exact data.",
            "And the way we do it is actually really simple.",
            "So rather than."
        ],
        [
            "This is actually extremely simple, so rather than.",
            "Just exponentiating our function or that we just drew from the GP.",
            "What we're going to do instead do is squash it with the sigmoid function so instead of exponentiating, we're just going to say that there's some upper bound and then we multiply the sigmoid thing by Lambda star, which is just some upper bounding some upper bound on our rate and the random functions that we get out.",
            "You're used to looking at at these kind of wiggly lines that we get from the GPS.",
            "Well these are going to be non negative upper bounded wiggly lines now, so so this is kind of the picture of the kind of intensity functions we get out.",
            "And what's neat about upper bounding this is that we're going to be able to generate exact boson data from random intensity functions with the priors."
        ],
        [
            "The way that we do that is by using a method called thinning and setting is a lot like rejection sampling, but it's for the person process and it goes like this.",
            "So this is our upper bounding intensity Lambda star here, and it turns out that it's very easy to generate pulse on data from a homogeneous case, so we can just do what I said earlier and figure out what the width of this thing is so 100 and we've got some rate.",
            "This point six that gives us the parameters for our focus on distribution we draw from the post."
        ],
        [
            "And then we distribute those events uniformly and randomly within the interval, so very easily we've gone from this homogeneous thing to a set of a set of events that are drawn from that.",
            "Now because of the nice marginalization properties of the Gaussian process, we can come along and sample this."
        ],
        [
            "Function at exactly these initial events, and this is something we can do without having to ask questions about the random sample at all.",
            "The other points I mean, this is the GPS in.",
            "Are you guys have not done this all before?",
            "But now what the next step is the interesting bit, and we're going to perform what's called."
        ],
        [
            "Now this is where we come along and we're going to remove some subset of these events.",
            "According to this blue function here.",
            "So what you can do is you can interpret the partition of the blue function of this interval as being a Bernoulli coin flip bias.",
            "So we're going to go along.",
            "We're going to take each one of these events, and we're going to flip a coin according to the Blue line about whether not we keep it or throw it away, and that's what these axes are so."
        ],
        [
            "What I've done is I've.",
            "In here where it was very unlikely to keep an event, I've chopped out the events that corresponded to those axes make sense.",
            "And it turns out that the events that we keep."
        ],
        [
            "Are drawn exactly from a pass on process that is consistent with the random intensity function that we just drew.",
            "So what this means is that this data that we have, these data that we have are consistent with having drawn a random intensity function as though we knew it everywhere and then drew poson data from that.",
            "And because we're able to get this exact data out of this model, we're now going to be able to do tractable Markov chain Monte Carlo inference without having to perform that intractable integral, and so just."
        ],
        [
            "Sort of make the point.",
            "What's interesting here is we did this.",
            "We got this data exactly and it's exactly so not biased from this model.",
            "We only had to ask for the function at a finite number of locations and we didn't have to evaluate that difficult interval.",
            "So."
        ],
        [
            "So let's talk about inference.",
            "So everything up until now I've just talked about."
        ],
        [
            "How we go from a model to a bunch of fantasies from data that you know data that are drawn from this model.",
            "So of course what we're really interested in is going from the data that we've actually seen to the intensity function underlying this process on which we put the GP prior.",
            "And it's worth pointing out that we haven't really avoided this doubly intractable problem, that if we write down the naive likelihood, we still can't do this over this random function.",
            "Even.",
            "I mean, we upper bounded it, but that's it.",
            "However, what's really in?"
        ],
        [
            "Testing is that if someone hands you some data and they also and they say, use our sigmoidal Gaussian Cox process prior, use this generative prior innocence.",
            "What they're doing is asserting that this data came about as a result of running this generative process.",
            "And if you just knew what the general process was that that was beneath the data, then you would kind of know everything there was to know and you could perform.",
            "That would be like knowing what the intensity function was, and So what we're going to do is just create a latent variable model that is the history of that generative process.",
            "So we create some latent variables that were going to include in this model, and so we don't know how many events we thinned out on the way.",
            "So we generated initial set and then we removed a chunk of them so we don't know how many of those there were, and we also don't know where.",
            "Don't know where they were, so we had this scattering appoints someone removed, and then we saw the rest and then there was also a function that we evaluated from the GP both at the data that we saw an also with these data that we sent away.",
            "And even though we don't know these things, we do have a well defined joint distribution over this whole object.",
            "And so we can sample from this with Markov chain Monte Carlo and so we can right?",
            "And that just gives us this big ugly joint distribution, so it's ugly, but it's not intractable, and in fact it's not even really that bad if you look closely at it.",
            "So this is a joint distribution over the data that we saw, so there are K events with T here.",
            "There are these latent very."
        ],
        [
            "So we're introducing these are the thin events that we threw away.",
            "So there, Emma, Emma, them we don't know M and we don't know where they are and they were writing the joint distribution over this random function and conditioning on Lambda staran.",
            "Any hyperparameters for GP?",
            "And it's just really breaks down into four big components that we have.",
            "This.",
            "This likelihood that corresponds to the homogeneous the initial homogeneous and process.",
            "So if we take the data that we saw plus the thin things, these just came from a person process that was homogenous with with with intensity, Lambda, Lambda, star and then this.",
            "Just me was just saying the volume of this space in which in which this thing is going on.",
            "Then conditioned on these points."
        ],
        [
            "We got then we drew from the GP with whatever hyperparameters we specify for our GP, and this again is just the marginal likelihood.",
            "The thing that we're used to dealing with with GPS all the time.",
            "And."
        ],
        [
            "If we integrate out those little X is I drew on that on that previous figure, then we have the probability determined by the sigmoid of keeping the data that we saw.",
            "These were all just independent Bernoulli trials and throwing away the data that we."
        ],
        [
            "That didn't end up getting seen, and these are also Bernoulli, but just with the 1 minus the sigmoid.",
            "So even though it's kind of ugly joint distribution, this is something that's really straightforward to evaluate and doesn't involve any intractable integrals.",
            "It's just a larger Markov chain with more state, but by including all this stuff, we managed to avoid having to do that difficult."
        ],
        [
            "That find that difficult constant just to give you an overview of the way that we do the sampling.",
            "I mean if you understood things up to this point, you would have enough.",
            "I mean, this is kind of just a little bit of detail, so we kind of define a birthday.",
            "We define a birth death process on these latent events because we need to include and remove them.",
            "We would do that with Metropolis Hastings proposing these things, proposing their locations and then accepting or rejecting that we also need to sample from the actual locations of the thin events conditioned on the number of them.",
            "And we again do that with Metropolis Hastings.",
            "And then conditioned on all of this in events and all of the observed data, we'd like to discover the function.",
            "This being kind of the thing that we're really interested in.",
            "What is the latent intensity function?",
            "And we can do that with Hamiltonian Monte Carlo, and that's quite efficient.",
            "We can also do hyperparameter inference as well, and include that in the Markov chain, because of course we have this Lambda star.",
            "This bounding intensity, and we'd like that not to be a constraint on our on our model, so we can infer that as well.",
            "And of course, the Gaussian process hyperparameters as well that determine the length scale, say of the of the intensity functions that were interested.",
            "Right, so let me just show you a couple of pretty pictures in this and these last couple minutes."
        ],
        [
            "So one of the datasets we looked at this just to show this through something useful.",
            "You can look at the paper where we looked at synthetic data and show that is competitive with other methods, But this is kind of an interesting data set to look at, so these are the dates of coal mine disasters between 1851 and 1962.",
            "A coal mine disaster being defined as 10 or more people killed in a coal mine disaster, so nine people doesn't make the disaster cut, but it's a good example of a non stationary person process.",
            "And."
        ],
        [
            "Here's the data along here, and you can see that this is running from 1850 till about 1960.",
            "You can interpret this Y axis basically as being the Bernoulli probability of one of these disasters happening on a particular day.",
            "So at the end of the 19th century, then you're seeing that the probability of one of these accidents happening is about one in 100, which is pretty high.",
            "I think these are these are error bars, just essentially two signatures on this.",
            "You can obviously do post hoc analysis on this kind of thing and people have.",
            "Because it's a well studied changepoint or or non stationary person process there was there was legislation passed in the UK about coal mine safety right here and then this ramp up being for the World Wars.",
            "Although it's interesting to note that it turns out I've discovered recently that the peak UK Coal production was actually in 1910 so it even though the peak of coal production overall in the UK actually corresponds to the to the truth of this thing."
        ],
        [
            "So looking just a little bit to see what the Markov chain Sampler is doing while this is running, these are histograms of the of what Lambda star inferred along the way and how many thinned events it needed to include in the model.",
            "So this is a histogram of that M. How many things do we need to explain the data?",
            "And the dominating intensity is about .1, so it's finding something that's right about here, which is pleasant, because that's what we'd hoped it would do.",
            "And then the number of send events that it's keeping around is about 200.",
            "And that makes a lot of sense, because if we imagine that the bounding intensity is floating around about here, and we imagine the partition of this is about 5050, and there are 200 data that we saw, then we would expect to need about 200 projections to explain it.",
            "We also just."
        ],
        [
            "So this works in a spatial setting.",
            "Looked at this well studied Redwood tree data, so these red these red dots are locations of trees.",
            "They've been scaled to the unit square and take away from this is hey look, it puts bright bits around where you expect to see trees, but more interesting is looking."
        ],
        [
            "The histogram over the locations of the thin events, so it's moving these center events around trying to explain the data and of course with the constant pounding intensity you would expect it to look essentially like a negative of the previous picture and it more or less does.",
            "Although they're not lined up and so that sort of looks like you know it's doing the right thing.",
            "Right, so just to summarize, some process is a useful process.",
            "There's a lot of different."
        ],
        [
            "Point process data in the world that we'd like to be able to model, this is one way to do it.",
            "Gaussian processes are a natural way to include nonparametric to make that model nonparametric.",
            "But but in general, these models have always been intractable.",
            "This is the first model that allows us to do completely tractable inference by Markov chain Monte Carlo without having to make intermediate approximations of this of this difficult interval, it seems to be competitive in practice.",
            "And of course, the bad news about using GPS for this kind of thing is that we inherit the cubic scaling that.",
            "Everyone does.",
            "When they do Jeep stuff.",
            "This is actually in some ways even worse than normal, because this is an this N is in the number of data that we've seen and also the number of thin events that we're carrying around with us in the latent variable model.",
            "So thank you all for listening."
        ],
        [
            "And I just like to think collaboration and some other people who contributed this."
        ],
        [
            "You go back to the coal mining.",
            "Yeah, yeah so.",
            "To the untrained eye, it looks like there's a subsidiary peak just around 1910 in the past.",
            "Yeah, yeah.",
            "You know it's a good question.",
            "I should say that that humanizing Tori sleep added trying to do this sort of inference, but but it certainly does look that way and it it's it's.",
            "I don't have a good answer for why that why that's not there.",
            "The link scale would seem to be what seemed to be longer than that, so maybe what it's doing is trying to explain these these gaps with with less data it's hard to say for sure.",
            "I mean, I would be interesting to look at some of the other analysis of this and see if they.",
            "If anybody gets a bump like that and the ones I've seen that the curves went up looking very similar to this.",
            "Yeah, you didn't.",
            "Discount if you wanted to normalize data with the level of production, then you probably need a weighted version.",
            "So there are different ways that you could imagine, including covariate data you, and that's something we haven't explored.",
            "We've been looking at this entirely in the context of where the inputs or GP were, just the that's not my people there, it's more like a waiting.",
            "So if we did you mean if we are, if we considered an event?",
            "If you wanted to normalize the number of events by level of production, right?",
            "So it's not clear that that still corresponds to a point process, so that.",
            "So the question is.",
            "We wanted we normalize this data by taking into account the fact that I mentioned production peaked actually here right?",
            "That is that what you're saying?",
            "So so it's not clear that an event per production corresponds to to a reasonable person process still.",
            "So you could imagine looking at the rate function after you did the inference and saying what is our rate function as normalizing that, but it's.",
            "There's not really a reasonable time unit, if that makes sense.",
            "It basically showed it very good properties.",
            "So they just quit this up.",
            "I mean, why don't you just quit the lineup and everything?",
            "Very easy, simple and people are neuroscience.",
            "So that so the question is, why don't we just just discretize the space?",
            "Well?",
            "And that's almost certainly completely reasonable thing to do.",
            "So this is about doing it non parametrically, so that's a finite dimensional approximation and this is about doing it exactly correctly, if that makes sense, so that's a free parameter that you would have to choose, right?",
            "I wanna know this process of these points right?",
            "And then I say I wanna know another point.",
            "You have to sample again.",
            "So at each point you're getting, you're getting a sample from the GP and it's consistent to then sample any new point from GP at each step in the Markov chain.",
            "But now you're right.",
            "So discredit discrimination.",
            "In the paper.",
            "We compared to the Gaussian Cox process with discretize intervals, which is kind of the state of the art, and so you're absolutely correct though, that that's a reasonable thing to do.",
            "This is just about doing it without having to make a finite dimensional approximation.",
            "For example, these guys will have to get to very small deltas.",
            "Then I mean, would there be a Delta from which you can say that yours is possible?",
            "Well, so you could imagine I've presented this with with a homogeneous bounding rate, but you could imagine much more sophisticated models for varying that bounding rate in space, right?",
            "So saying rather than having a big flat big flat liners are bounding intensity, we could imagine something more sophisticated or parameters that we would infer, and that would correspond to having.",
            "A discretization that varied in space in that model and could potentially allow us to deal with very Peaky with very Peaky point process data, but that's just kind of a special thing to do, because that would be quite a different prior than modules, right?",
            "When is the next speaker here is coming set up.",
            "We just use the squared exponential kernel for this.",
            "We haven't done anything sophisticated with the kernels.",
            "It actually with Hamiltonian Monte Carlo, it converges within a couple of 100 iterations.",
            "Actually it's quite fast.",
            "I mean it's I shouldn't say it's quite fast because each step of course is in cube, but in terms of the Monte Carlo steps themselves, it's fairly efficient.",
            "How long does it take the coal mining data takes on the order minutes so it's."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is this is work with Anne Murray at Toronto, an my supervisor David Mackay.",
                    "label": 1
                },
                {
                    "sent": "Cambridge and I know this is a big obnoxious sounding title, but in fact hopefully you'll see it.",
                    "label": 0
                },
                {
                    "sent": "It's really not that not that bad.",
                    "label": 0
                },
                {
                    "sent": "I mean, the main idea is pretty simple.",
                    "label": 0
                },
                {
                    "sent": "So what we want to do is model A plus on process.",
                    "label": 0
                },
                {
                    "sent": "Somebody hands us a chunk of data that are events say in time or space.",
                    "label": 0
                },
                {
                    "sent": "We want to model with the person.",
                    "label": 0
                },
                {
                    "sent": "Process this post.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Process has an intensity function.",
                    "label": 0
                },
                {
                    "sent": "Would like to put a GP prior on that intensity function and then be able to do inference in a tractable way, and that's that's the big idea.",
                    "label": 0
                },
                {
                    "sent": "So we have some events here and we'd like to find the distribution of the posterior distribution over over the rate functions that generated this, and using a GP prior on that.",
                    "label": 0
                },
                {
                    "sent": "So that's the big idea here.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just to give you a quick overview, I'm going to talk a little bit about posaunen Cox processes, and then I'll talk about our model, which we call the sigmoid Gaussian Cox process and I'll tell you a little bit about inference and then hopefully show you that doing something useful.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Data, but first let's talk a little bit about the Sonic cash processes.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So your basic point process is a random subset or finite subset of of what we think of as events.",
                    "label": 0
                },
                {
                    "sent": "So time is the thing which which were maybe most familiar, so you're standing at the bus stop and these events are coming along and you know the bus appears random.",
                    "label": 0
                },
                {
                    "sent": "Maybe if you don't know much about schedules or if the bus is really unreliable.",
                    "label": 0
                },
                {
                    "sent": "Or maybe it would you know these points would be.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The injuries of your favorite football player or something along these lines.",
                    "label": 0
                },
                {
                    "sent": "We could also have.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Spatial point processes, so these might be trees and forests or something like that.",
                    "label": 0
                },
                {
                    "sent": "And then we can have also things like space.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Show temporal for some processes, and there's such a neat example of this that I'm going to show you this real fast 'cause this is one of my favorite examples of a spatial temporal mark person process.",
                    "label": 0
                },
                {
                    "sent": "So zappos.com is the like the Amazon.com of shoes in the United States, and they've created this real time shoe purchasing map and so hopefully it will be working.",
                    "label": 0
                },
                {
                    "sent": "But basically what you're saying is events popping up as people buy things and they're having it.",
                    "label": 0
                },
                {
                    "sent": "Having a random place at a random time and then they have a random distribution over shoes or whatever.",
                    "label": 0
                },
                {
                    "sent": "They are an I love this, 'cause it's a really good example of sort of.",
                    "label": 0
                },
                {
                    "sent": "A real life person process or not, plus unnecessarily but but point process in space and time.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I just wanted to show you that 'cause I think it's neat.",
                    "label": 0
                },
                {
                    "sent": "Alright, so the really the.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Basic kind of point processes the homogeneous plus on process and this is where we have some constant intensity that will use Lambda to denote and the two big sort of properties of a homogeneous and process are that disjoint intervals.",
                    "label": 0
                },
                {
                    "sent": "The number of events we expect to get within disjoint intervals are independent and then also if we grab a particular interval and we want to know how many days are going to be in it, and that's going to be a person distributed number and it's going to relate to volume of that interval.",
                    "label": 1
                },
                {
                    "sent": "Multiplied by the rate graphically, we can take a.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Get the idea by imagining that we have some some person process with a constant rate here and then we have some events and we wanted to ask the question in this little window.",
                    "label": 0
                },
                {
                    "sent": "Here, how many events were going to get?",
                    "label": 0
                },
                {
                    "sent": "And that's going to be put on distributed, so the probability mass function looks something like this and that's more or less the whole story with.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With the homogeneous Wilson process, but of course that's a really limiting assumption and the real in real life are interesting models.",
                    "label": 0
                },
                {
                    "sent": "We want to have an homogeneous rate, something that varies with time, and so for that we extend it so that we have instead of a constant rate.",
                    "label": 0
                },
                {
                    "sent": "We have this, this Lambda T as a function of time, and instead of looking at just the size of the interval.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're going to do now is we're going to integrate the rate function so we have some little more complex rate function.",
                    "label": 0
                },
                {
                    "sent": "And now if we want to know how many how many events were going to get inside some little window, then we integrate this thing.",
                    "label": 0
                },
                {
                    "sent": "And again, we gotta push.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Distribution, so the question is, if we're actually looking at real data, do we do we want to make strong assumptions about the about this intensity function and the answer is often we don't because we maybe don't know with this specific parametric functional form when we do non Bayesian inference in these kind of models, then a stochastic process is a really nice thing to use as a prior on that on that intensity function.",
                    "label": 0
                },
                {
                    "sent": "And when we do that when we have a stochastic process generating our intensity that then generates Wilson data, we call that a Cox process.",
                    "label": 0
                },
                {
                    "sent": "Now Cox called this doubly doubly stochastic person process but.",
                    "label": 0
                },
                {
                    "sent": "Like all really good ideas, that kind of needs, you know, a good proper noun for it, and then in the specific case where that latent, where that latent stochastic processes a Gaussian process, then we call it a Gaussian Cox process.",
                    "label": 0
                },
                {
                    "sent": "So just to be clear, 'cause I've said process like 40 times in the last five minutes then.",
                    "label": 0
                },
                {
                    "sent": "So we have some Gaussian process that's generating a random intensity and then that random intensity is generating posts on data.",
                    "label": 0
                },
                {
                    "sent": "OK, and this talk is about constructing a Gaussian Cox process.",
                    "label": 1
                },
                {
                    "sent": "That's going to allow us to do tractable inference via Markov chain Monte Carlo so.",
                    "label": 0
                },
                {
                    "sent": "Now, if you were just going to construct one of these from scratch, you'd look at the post.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Process and say that the major requirement that you had is that the person intensity function needs to be non negative, so you know a lot of times the machine learning when we want to have some real value thing we want to make it negative, then we squeeze it through an exponential function and now it's positive.",
                    "label": 0
                },
                {
                    "sent": "So to do that we would create what's called the log Gaussian Cox process and this has been around for a little while.",
                    "label": 1
                },
                {
                    "sent": "So if G here is some draw from GP then we could get to a Lambda T just by exponentiating it and this is pretty easy.",
                    "label": 0
                },
                {
                    "sent": "The problem is that.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In general, this model is intractable, so for one thing, if we want to do inference, this object G is uncountably infinite in dimension, and we can't realistically hold this thing in memory, but also the posterior distribution that we would get when we tried to perform inference with this object is called doubly intractable.",
                    "label": 0
                },
                {
                    "sent": "Now, what that means is that the likelihood function has a constant that we can't evaluate, and you can see that right here by.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we if we look at the likelihood function, then of you know some chunk of K events and G is this infinite dimensional function?",
                    "label": 0
                },
                {
                    "sent": "That's our.",
                    "label": 0
                },
                {
                    "sent": "That's our parameter.",
                    "label": 0
                },
                {
                    "sent": "Then we need to in order to evaluate the likelihood we need to integrate this.",
                    "label": 0
                },
                {
                    "sent": "Integrate this function that we just drew from the GP.",
                    "label": 0
                },
                {
                    "sent": "And in general this is unknowable innocence.",
                    "label": 0
                },
                {
                    "sent": "So if we have this infinite dimensional object, we need to evaluate it everywhere to in order to determine this interval and so realistically we can't know the likelihood function except within a constant.",
                    "label": 0
                },
                {
                    "sent": "Now it happens all the time.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of course, in Bayesian inference that we run into intractable integrals and interesting models, the marginal likelihood, the bit that's on the bottom of the posterior distribution is almost never able to be evaluated.",
                    "label": 0
                },
                {
                    "sent": "But we have a very large toolkit for dealing with this situation specifically, Markov chain Monte Carlo.",
                    "label": 0
                },
                {
                    "sent": "And the reason this all kind of works is because the parameter, the marginal likelihood doesn't depend on the parameters.",
                    "label": 0
                },
                {
                    "sent": "But in a doubly intractable distribution it's the likelihood function.",
                    "label": 0
                },
                {
                    "sent": "The thing that's telling us that's telling us about the relationship between parameters and data that have that has the intractability.",
                    "label": 0
                },
                {
                    "sent": "The good news is, though, that there's been some recent developments in Markov chain Monte Carlo, and we can now perform inference in these models under some of the under specific circumstances that you can generate exact data.",
                    "label": 1
                },
                {
                    "sent": "Now, when I say exactly what I mean is that if you somebody hand you one of these models and a particular set of parameters that you're going to be able to generate fantasies from it that are not biased, so you couldn't, for example, just run a finite naive Markov chain, and because that would be biased by the starting state.",
                    "label": 0
                },
                {
                    "sent": "So what this is about is constructing a Gaussian.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Cox process from which we're going to be able to generate this kind of exact data.",
                    "label": 1
                },
                {
                    "sent": "And the way we do it is actually really simple.",
                    "label": 0
                },
                {
                    "sent": "So rather than.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is actually extremely simple, so rather than.",
                    "label": 0
                },
                {
                    "sent": "Just exponentiating our function or that we just drew from the GP.",
                    "label": 1
                },
                {
                    "sent": "What we're going to do instead do is squash it with the sigmoid function so instead of exponentiating, we're just going to say that there's some upper bound and then we multiply the sigmoid thing by Lambda star, which is just some upper bounding some upper bound on our rate and the random functions that we get out.",
                    "label": 0
                },
                {
                    "sent": "You're used to looking at at these kind of wiggly lines that we get from the GPS.",
                    "label": 0
                },
                {
                    "sent": "Well these are going to be non negative upper bounded wiggly lines now, so so this is kind of the picture of the kind of intensity functions we get out.",
                    "label": 0
                },
                {
                    "sent": "And what's neat about upper bounding this is that we're going to be able to generate exact boson data from random intensity functions with the priors.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The way that we do that is by using a method called thinning and setting is a lot like rejection sampling, but it's for the person process and it goes like this.",
                    "label": 0
                },
                {
                    "sent": "So this is our upper bounding intensity Lambda star here, and it turns out that it's very easy to generate pulse on data from a homogeneous case, so we can just do what I said earlier and figure out what the width of this thing is so 100 and we've got some rate.",
                    "label": 0
                },
                {
                    "sent": "This point six that gives us the parameters for our focus on distribution we draw from the post.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we distribute those events uniformly and randomly within the interval, so very easily we've gone from this homogeneous thing to a set of a set of events that are drawn from that.",
                    "label": 0
                },
                {
                    "sent": "Now because of the nice marginalization properties of the Gaussian process, we can come along and sample this.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Function at exactly these initial events, and this is something we can do without having to ask questions about the random sample at all.",
                    "label": 0
                },
                {
                    "sent": "The other points I mean, this is the GPS in.",
                    "label": 0
                },
                {
                    "sent": "Are you guys have not done this all before?",
                    "label": 0
                },
                {
                    "sent": "But now what the next step is the interesting bit, and we're going to perform what's called.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now this is where we come along and we're going to remove some subset of these events.",
                    "label": 0
                },
                {
                    "sent": "According to this blue function here.",
                    "label": 0
                },
                {
                    "sent": "So what you can do is you can interpret the partition of the blue function of this interval as being a Bernoulli coin flip bias.",
                    "label": 0
                },
                {
                    "sent": "So we're going to go along.",
                    "label": 0
                },
                {
                    "sent": "We're going to take each one of these events, and we're going to flip a coin according to the Blue line about whether not we keep it or throw it away, and that's what these axes are so.",
                    "label": 1
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What I've done is I've.",
                    "label": 0
                },
                {
                    "sent": "In here where it was very unlikely to keep an event, I've chopped out the events that corresponded to those axes make sense.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that the events that we keep.",
                    "label": 1
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are drawn exactly from a pass on process that is consistent with the random intensity function that we just drew.",
                    "label": 0
                },
                {
                    "sent": "So what this means is that this data that we have, these data that we have are consistent with having drawn a random intensity function as though we knew it everywhere and then drew poson data from that.",
                    "label": 0
                },
                {
                    "sent": "And because we're able to get this exact data out of this model, we're now going to be able to do tractable Markov chain Monte Carlo inference without having to perform that intractable integral, and so just.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sort of make the point.",
                    "label": 0
                },
                {
                    "sent": "What's interesting here is we did this.",
                    "label": 1
                },
                {
                    "sent": "We got this data exactly and it's exactly so not biased from this model.",
                    "label": 0
                },
                {
                    "sent": "We only had to ask for the function at a finite number of locations and we didn't have to evaluate that difficult interval.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's talk about inference.",
                    "label": 0
                },
                {
                    "sent": "So everything up until now I've just talked about.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How we go from a model to a bunch of fantasies from data that you know data that are drawn from this model.",
                    "label": 0
                },
                {
                    "sent": "So of course what we're really interested in is going from the data that we've actually seen to the intensity function underlying this process on which we put the GP prior.",
                    "label": 0
                },
                {
                    "sent": "And it's worth pointing out that we haven't really avoided this doubly intractable problem, that if we write down the naive likelihood, we still can't do this over this random function.",
                    "label": 0
                },
                {
                    "sent": "Even.",
                    "label": 0
                },
                {
                    "sent": "I mean, we upper bounded it, but that's it.",
                    "label": 0
                },
                {
                    "sent": "However, what's really in?",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Testing is that if someone hands you some data and they also and they say, use our sigmoidal Gaussian Cox process prior, use this generative prior innocence.",
                    "label": 0
                },
                {
                    "sent": "What they're doing is asserting that this data came about as a result of running this generative process.",
                    "label": 0
                },
                {
                    "sent": "And if you just knew what the general process was that that was beneath the data, then you would kind of know everything there was to know and you could perform.",
                    "label": 0
                },
                {
                    "sent": "That would be like knowing what the intensity function was, and So what we're going to do is just create a latent variable model that is the history of that generative process.",
                    "label": 0
                },
                {
                    "sent": "So we create some latent variables that were going to include in this model, and so we don't know how many events we thinned out on the way.",
                    "label": 0
                },
                {
                    "sent": "So we generated initial set and then we removed a chunk of them so we don't know how many of those there were, and we also don't know where.",
                    "label": 0
                },
                {
                    "sent": "Don't know where they were, so we had this scattering appoints someone removed, and then we saw the rest and then there was also a function that we evaluated from the GP both at the data that we saw an also with these data that we sent away.",
                    "label": 0
                },
                {
                    "sent": "And even though we don't know these things, we do have a well defined joint distribution over this whole object.",
                    "label": 0
                },
                {
                    "sent": "And so we can sample from this with Markov chain Monte Carlo and so we can right?",
                    "label": 0
                },
                {
                    "sent": "And that just gives us this big ugly joint distribution, so it's ugly, but it's not intractable, and in fact it's not even really that bad if you look closely at it.",
                    "label": 1
                },
                {
                    "sent": "So this is a joint distribution over the data that we saw, so there are K events with T here.",
                    "label": 0
                },
                {
                    "sent": "There are these latent very.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we're introducing these are the thin events that we threw away.",
                    "label": 0
                },
                {
                    "sent": "So there, Emma, Emma, them we don't know M and we don't know where they are and they were writing the joint distribution over this random function and conditioning on Lambda staran.",
                    "label": 0
                },
                {
                    "sent": "Any hyperparameters for GP?",
                    "label": 0
                },
                {
                    "sent": "And it's just really breaks down into four big components that we have.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "This likelihood that corresponds to the homogeneous the initial homogeneous and process.",
                    "label": 0
                },
                {
                    "sent": "So if we take the data that we saw plus the thin things, these just came from a person process that was homogenous with with with intensity, Lambda, Lambda, star and then this.",
                    "label": 0
                },
                {
                    "sent": "Just me was just saying the volume of this space in which in which this thing is going on.",
                    "label": 0
                },
                {
                    "sent": "Then conditioned on these points.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We got then we drew from the GP with whatever hyperparameters we specify for our GP, and this again is just the marginal likelihood.",
                    "label": 0
                },
                {
                    "sent": "The thing that we're used to dealing with with GPS all the time.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we integrate out those little X is I drew on that on that previous figure, then we have the probability determined by the sigmoid of keeping the data that we saw.",
                    "label": 0
                },
                {
                    "sent": "These were all just independent Bernoulli trials and throwing away the data that we.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That didn't end up getting seen, and these are also Bernoulli, but just with the 1 minus the sigmoid.",
                    "label": 0
                },
                {
                    "sent": "So even though it's kind of ugly joint distribution, this is something that's really straightforward to evaluate and doesn't involve any intractable integrals.",
                    "label": 0
                },
                {
                    "sent": "It's just a larger Markov chain with more state, but by including all this stuff, we managed to avoid having to do that difficult.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That find that difficult constant just to give you an overview of the way that we do the sampling.",
                    "label": 0
                },
                {
                    "sent": "I mean if you understood things up to this point, you would have enough.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is kind of just a little bit of detail, so we kind of define a birthday.",
                    "label": 0
                },
                {
                    "sent": "We define a birth death process on these latent events because we need to include and remove them.",
                    "label": 0
                },
                {
                    "sent": "We would do that with Metropolis Hastings proposing these things, proposing their locations and then accepting or rejecting that we also need to sample from the actual locations of the thin events conditioned on the number of them.",
                    "label": 1
                },
                {
                    "sent": "And we again do that with Metropolis Hastings.",
                    "label": 0
                },
                {
                    "sent": "And then conditioned on all of this in events and all of the observed data, we'd like to discover the function.",
                    "label": 0
                },
                {
                    "sent": "This being kind of the thing that we're really interested in.",
                    "label": 0
                },
                {
                    "sent": "What is the latent intensity function?",
                    "label": 1
                },
                {
                    "sent": "And we can do that with Hamiltonian Monte Carlo, and that's quite efficient.",
                    "label": 0
                },
                {
                    "sent": "We can also do hyperparameter inference as well, and include that in the Markov chain, because of course we have this Lambda star.",
                    "label": 0
                },
                {
                    "sent": "This bounding intensity, and we'd like that not to be a constraint on our on our model, so we can infer that as well.",
                    "label": 0
                },
                {
                    "sent": "And of course, the Gaussian process hyperparameters as well that determine the length scale, say of the of the intensity functions that were interested.",
                    "label": 0
                },
                {
                    "sent": "Right, so let me just show you a couple of pretty pictures in this and these last couple minutes.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one of the datasets we looked at this just to show this through something useful.",
                    "label": 0
                },
                {
                    "sent": "You can look at the paper where we looked at synthetic data and show that is competitive with other methods, But this is kind of an interesting data set to look at, so these are the dates of coal mine disasters between 1851 and 1962.",
                    "label": 0
                },
                {
                    "sent": "A coal mine disaster being defined as 10 or more people killed in a coal mine disaster, so nine people doesn't make the disaster cut, but it's a good example of a non stationary person process.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's the data along here, and you can see that this is running from 1850 till about 1960.",
                    "label": 0
                },
                {
                    "sent": "You can interpret this Y axis basically as being the Bernoulli probability of one of these disasters happening on a particular day.",
                    "label": 0
                },
                {
                    "sent": "So at the end of the 19th century, then you're seeing that the probability of one of these accidents happening is about one in 100, which is pretty high.",
                    "label": 0
                },
                {
                    "sent": "I think these are these are error bars, just essentially two signatures on this.",
                    "label": 0
                },
                {
                    "sent": "You can obviously do post hoc analysis on this kind of thing and people have.",
                    "label": 0
                },
                {
                    "sent": "Because it's a well studied changepoint or or non stationary person process there was there was legislation passed in the UK about coal mine safety right here and then this ramp up being for the World Wars.",
                    "label": 0
                },
                {
                    "sent": "Although it's interesting to note that it turns out I've discovered recently that the peak UK Coal production was actually in 1910 so it even though the peak of coal production overall in the UK actually corresponds to the to the truth of this thing.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So looking just a little bit to see what the Markov chain Sampler is doing while this is running, these are histograms of the of what Lambda star inferred along the way and how many thinned events it needed to include in the model.",
                    "label": 0
                },
                {
                    "sent": "So this is a histogram of that M. How many things do we need to explain the data?",
                    "label": 0
                },
                {
                    "sent": "And the dominating intensity is about .1, so it's finding something that's right about here, which is pleasant, because that's what we'd hoped it would do.",
                    "label": 0
                },
                {
                    "sent": "And then the number of send events that it's keeping around is about 200.",
                    "label": 0
                },
                {
                    "sent": "And that makes a lot of sense, because if we imagine that the bounding intensity is floating around about here, and we imagine the partition of this is about 5050, and there are 200 data that we saw, then we would expect to need about 200 projections to explain it.",
                    "label": 0
                },
                {
                    "sent": "We also just.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this works in a spatial setting.",
                    "label": 0
                },
                {
                    "sent": "Looked at this well studied Redwood tree data, so these red these red dots are locations of trees.",
                    "label": 0
                },
                {
                    "sent": "They've been scaled to the unit square and take away from this is hey look, it puts bright bits around where you expect to see trees, but more interesting is looking.",
                    "label": 1
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The histogram over the locations of the thin events, so it's moving these center events around trying to explain the data and of course with the constant pounding intensity you would expect it to look essentially like a negative of the previous picture and it more or less does.",
                    "label": 0
                },
                {
                    "sent": "Although they're not lined up and so that sort of looks like you know it's doing the right thing.",
                    "label": 0
                },
                {
                    "sent": "Right, so just to summarize, some process is a useful process.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of different.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Point process data in the world that we'd like to be able to model, this is one way to do it.",
                    "label": 0
                },
                {
                    "sent": "Gaussian processes are a natural way to include nonparametric to make that model nonparametric.",
                    "label": 0
                },
                {
                    "sent": "But but in general, these models have always been intractable.",
                    "label": 1
                },
                {
                    "sent": "This is the first model that allows us to do completely tractable inference by Markov chain Monte Carlo without having to make intermediate approximations of this of this difficult interval, it seems to be competitive in practice.",
                    "label": 1
                },
                {
                    "sent": "And of course, the bad news about using GPS for this kind of thing is that we inherit the cubic scaling that.",
                    "label": 0
                },
                {
                    "sent": "Everyone does.",
                    "label": 0
                },
                {
                    "sent": "When they do Jeep stuff.",
                    "label": 0
                },
                {
                    "sent": "This is actually in some ways even worse than normal, because this is an this N is in the number of data that we've seen and also the number of thin events that we're carrying around with us in the latent variable model.",
                    "label": 0
                },
                {
                    "sent": "So thank you all for listening.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I just like to think collaboration and some other people who contributed this.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You go back to the coal mining.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah so.",
                    "label": 0
                },
                {
                    "sent": "To the untrained eye, it looks like there's a subsidiary peak just around 1910 in the past.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "You know it's a good question.",
                    "label": 0
                },
                {
                    "sent": "I should say that that humanizing Tori sleep added trying to do this sort of inference, but but it certainly does look that way and it it's it's.",
                    "label": 0
                },
                {
                    "sent": "I don't have a good answer for why that why that's not there.",
                    "label": 0
                },
                {
                    "sent": "The link scale would seem to be what seemed to be longer than that, so maybe what it's doing is trying to explain these these gaps with with less data it's hard to say for sure.",
                    "label": 0
                },
                {
                    "sent": "I mean, I would be interesting to look at some of the other analysis of this and see if they.",
                    "label": 0
                },
                {
                    "sent": "If anybody gets a bump like that and the ones I've seen that the curves went up looking very similar to this.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you didn't.",
                    "label": 0
                },
                {
                    "sent": "Discount if you wanted to normalize data with the level of production, then you probably need a weighted version.",
                    "label": 0
                },
                {
                    "sent": "So there are different ways that you could imagine, including covariate data you, and that's something we haven't explored.",
                    "label": 0
                },
                {
                    "sent": "We've been looking at this entirely in the context of where the inputs or GP were, just the that's not my people there, it's more like a waiting.",
                    "label": 0
                },
                {
                    "sent": "So if we did you mean if we are, if we considered an event?",
                    "label": 0
                },
                {
                    "sent": "If you wanted to normalize the number of events by level of production, right?",
                    "label": 0
                },
                {
                    "sent": "So it's not clear that that still corresponds to a point process, so that.",
                    "label": 0
                },
                {
                    "sent": "So the question is.",
                    "label": 0
                },
                {
                    "sent": "We wanted we normalize this data by taking into account the fact that I mentioned production peaked actually here right?",
                    "label": 0
                },
                {
                    "sent": "That is that what you're saying?",
                    "label": 0
                },
                {
                    "sent": "So so it's not clear that an event per production corresponds to to a reasonable person process still.",
                    "label": 0
                },
                {
                    "sent": "So you could imagine looking at the rate function after you did the inference and saying what is our rate function as normalizing that, but it's.",
                    "label": 0
                },
                {
                    "sent": "There's not really a reasonable time unit, if that makes sense.",
                    "label": 0
                },
                {
                    "sent": "It basically showed it very good properties.",
                    "label": 0
                },
                {
                    "sent": "So they just quit this up.",
                    "label": 0
                },
                {
                    "sent": "I mean, why don't you just quit the lineup and everything?",
                    "label": 0
                },
                {
                    "sent": "Very easy, simple and people are neuroscience.",
                    "label": 0
                },
                {
                    "sent": "So that so the question is, why don't we just just discretize the space?",
                    "label": 0
                },
                {
                    "sent": "Well?",
                    "label": 0
                },
                {
                    "sent": "And that's almost certainly completely reasonable thing to do.",
                    "label": 0
                },
                {
                    "sent": "So this is about doing it non parametrically, so that's a finite dimensional approximation and this is about doing it exactly correctly, if that makes sense, so that's a free parameter that you would have to choose, right?",
                    "label": 0
                },
                {
                    "sent": "I wanna know this process of these points right?",
                    "label": 0
                },
                {
                    "sent": "And then I say I wanna know another point.",
                    "label": 0
                },
                {
                    "sent": "You have to sample again.",
                    "label": 0
                },
                {
                    "sent": "So at each point you're getting, you're getting a sample from the GP and it's consistent to then sample any new point from GP at each step in the Markov chain.",
                    "label": 0
                },
                {
                    "sent": "But now you're right.",
                    "label": 0
                },
                {
                    "sent": "So discredit discrimination.",
                    "label": 0
                },
                {
                    "sent": "In the paper.",
                    "label": 0
                },
                {
                    "sent": "We compared to the Gaussian Cox process with discretize intervals, which is kind of the state of the art, and so you're absolutely correct though, that that's a reasonable thing to do.",
                    "label": 0
                },
                {
                    "sent": "This is just about doing it without having to make a finite dimensional approximation.",
                    "label": 0
                },
                {
                    "sent": "For example, these guys will have to get to very small deltas.",
                    "label": 0
                },
                {
                    "sent": "Then I mean, would there be a Delta from which you can say that yours is possible?",
                    "label": 0
                },
                {
                    "sent": "Well, so you could imagine I've presented this with with a homogeneous bounding rate, but you could imagine much more sophisticated models for varying that bounding rate in space, right?",
                    "label": 0
                },
                {
                    "sent": "So saying rather than having a big flat big flat liners are bounding intensity, we could imagine something more sophisticated or parameters that we would infer, and that would correspond to having.",
                    "label": 0
                },
                {
                    "sent": "A discretization that varied in space in that model and could potentially allow us to deal with very Peaky with very Peaky point process data, but that's just kind of a special thing to do, because that would be quite a different prior than modules, right?",
                    "label": 0
                },
                {
                    "sent": "When is the next speaker here is coming set up.",
                    "label": 0
                },
                {
                    "sent": "We just use the squared exponential kernel for this.",
                    "label": 0
                },
                {
                    "sent": "We haven't done anything sophisticated with the kernels.",
                    "label": 0
                },
                {
                    "sent": "It actually with Hamiltonian Monte Carlo, it converges within a couple of 100 iterations.",
                    "label": 0
                },
                {
                    "sent": "Actually it's quite fast.",
                    "label": 0
                },
                {
                    "sent": "I mean it's I shouldn't say it's quite fast because each step of course is in cube, but in terms of the Monte Carlo steps themselves, it's fairly efficient.",
                    "label": 0
                },
                {
                    "sent": "How long does it take the coal mining data takes on the order minutes so it's.",
                    "label": 0
                }
            ]
        }
    }
}