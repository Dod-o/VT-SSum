{
    "id": "vlnrj6kgcx5xtfzzd22q6k36wmaudpnp",
    "title": "Probabilistic Dyadic Data Analysis with Local and Global Consistency",
    "info": {
        "author": [
            "Vikas Raykar, Department of Computer Science, University of Maryland"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/icml09_raykar_pdd/",
    "segmentation": [
        [
            "This paper could not make it to the conference because of the visa issues, so I'm trying to present it on their behalf and this is not really my area and I read the paper last night so be gentle on the questions."
        ],
        [
            "So to summarize, the main.",
            "What the authors are trying to do here is that they show some impressive results that by if you use the manifold assumption you can get very good results on topic modeling approaches.",
            "So generally in topic modeling you're given."
        ],
        [
            "Collection of text documents and you want to build a model for the words, for example.",
            "Let's say a given text as these words and I want to say what is the probability that this word occurred in that document.",
            "So I'm trying to build a model for words in the text.",
            "So on topic models are known to be a powerful tool for various text applications, like topic discovery, summarization, opinion, mining and many more.",
            "So one."
        ],
        [
            "The most popular approaches for topic modeling is what is known as the probabilistic latent semantic analysis.",
            "So here you're given a bunch of documents.",
            "Anne.",
            "Every document is a lot of words, so we are this bunch of words.",
            "It belong to all the documents and very simply I'm interested in learning what is the probability that a given word occurs in a particular document.",
            "This is all I'm interested to build a model for this task."
        ],
        [
            "So the naive approach would be just to count the number of occurrences of each word in that document and just do a frequency of that.",
            "So essentially or frequency of that particular word occurring in a particular document.",
            "But the problem with this approach is that it's what is known as a zero frequency problem.",
            "So a lot of terms which are not occurring in a document, you get a zero probability.",
            "So."
        ],
        [
            "What probabilistic latent semantic analysis does is so you have this.",
            "At this left you have the documents and you have the words in between.",
            "It introduced something called as a latent concept.",
            "So the way our document is generated is first you pick a document.",
            "Then you speak up concept and then once this concept is picked, this concept generates the words.",
            "So given a document, you have a distribution over the concept Z.",
            "So what is the probability of Z given a document and once you pick a particular concept, you have, what is the probability that you can generate a particular word given that concept?",
            "So essentially, instead of summarizing a document by a bunch of words, you're saying we're going to summarize it by a bunch of concepts and trying to learn a model for the concepts rather than the words.",
            "Just to give an example, let's say the article is about trade.",
            "So and there are three concepts.",
            "So probability of trade given the document would be, let's say .8 and once you pick the concept rate, you're going to generate words like economic imports and trade.",
            "So with this model, what you can write if you can just now probability of a given word occurring in a document can be written as the summation over all possible concepts.",
            "So you have the probability of Z given the document.",
            "What is the probability that a particular concept occurs.",
            "And given this concept, what is the probability that word appears in that document and you just sum it over all possible documents?",
            "So so this.",
            "So what we observe here is just the documents and the words.",
            "So that's our observation.",
            "What I'm trying to learn is the latent concepts.",
            "Or more precisely, I want to learn these two para meters.",
            "What is the probability given a document, what is the probability that a particular it comes belongs to a particular concept and what is the probability that that word appears given that concept.",
            "So these are the models of my parameters and my observation is just the documents and the words.",
            "So how do I find those parameters?"
        ],
        [
            "So the most popular approach as I mentioned is what is known as P, LSI probabilistic latent semantic analysis.",
            "An there is also the latent dislocation.",
            "This also the I don't know this patching or location and many more.",
            "So all this isn't all these models, what they're trying to do is they're trying to build a model on the ambient Euclidean space, so they're trying to build a probabilistic model on your words probability of Z given D. So the DSD lies in some Euclidean space.",
            "So the main contribution of this talk is that.",
            "We can show that if you take the geometric structure of the data, you can get a significant improvement in the topic models and this paper mainly talks about only the first method, which is latent semantic indexing.",
            "And how can you bring in the geometric structure of the manifold into the latent semantic indexing model."
        ],
        [
            "So I think I shouldn't spend too much on this diagram, so the key assumption is that you assume that documents could lie on this underlying, probably smooth low dimensional manifold.",
            "And now I want to build a model.",
            "How do I exploit this structure?",
            "So the main assumption with this exploit is what is known as the locally consistent assumption that the nearby points should share similar properties.",
            "So the challenge here is how do we incorporate this nearby information in terms of probability distributions and try to solve the latent semantic indexing problem."
        ],
        [
            "OK, as usual you start with a graph.",
            "So you have each document.",
            "It's a point.",
            "In some high dimensional Euclidean space.",
            "So each document is represented by the word frequencies.",
            "And on these documents you can't reconstruct AP nearest neighbor graph, so every document is connected to P of its nearest neighbors.",
            "So what P LSA that's the normal topic modeling approach does is at every point on this graph there is.",
            "You have a distribution or probability of Z given D. So.",
            "So the key idea here is that you have at every document where you have at every point in this Euclidian space, I have a probability distribution.",
            "So now I want to smooth this probability distribution to encode the geometric structure in the data.",
            "So essentially in the end you would look something like this.",
            "You take the normal Prob topic model, then you impose a constraint that if two documents are closed by you want the topic the probability distributions to be smooth."
        ],
        [
            "So this would be the normal typical objective function for latent semantic indexing, where here this is the number of times that document word occurs in a document, and generally we're trying to maximize this.",
            "So this is a typical log likelihood.",
            "There's nothing new here.",
            "The part comes here is how do we regularize this objective function to encode the smoothness constraint and Lambda is your regularization parameter.",
            "I want to maximize this term so that at the same time I want to penalize if our distributions learn are not smooth.",
            "So the key now issue is how do you measure the smoothness of a distribution P of Z given D over the geometric structure of the data.",
            "So there are two things.",
            "How do you measure the smoothness and how do you encode the geometric structure of the data?",
            "The to measure the smoothness, the authors use the callback library divergance between two probability distributions.",
            "So I&J are your two documents at point I, you have a distribution of P of Z.",
            "Given D, There is a distribution of your latent semantic concepts and similarly at Jay you have this distribution.",
            "Now you want to measure how close these two distributions are.",
            "So one of the commonly used metric is the color black library vergence.",
            "Since it is not symmetric.",
            "They kind of make it more symmetric by using IJ&J and then take a square of this term.",
            "So and now this this term can measure how smooth your distribution is.",
            "Now you want to make sure this is not doesn't smooth everywhere.",
            "You want to smooth only on your graph.",
            "So essentially this graph is constructed by using WI J, which says that don't bother to smooth if your point is too far, so only care about only nearby points nearby documents.",
            "So WI J brings in your.",
            "What do you call the manifold assumption?",
            "And this brings you the smoothness assumption of your distribution.",
            "So with this now we have all the tools to crank out the math.",
            "So this is what you want to maximize.",
            "And you can, so this is essentially a regularised log likelihood."
        ],
        [
            "And the standard way latent semantic analysis does is just as am algorithm and tries to estimate the parameters and you can see that most almost all of the parameters remain the same except for one particular term.",
            "So the eastep essentially calculates the posterior probability of the latent variables.",
            "Which essentially happens to be exactly same as in latent semantic indexing.",
            "So you want to calculate what is the probability of that semantic concept given the word and a document.",
            "The M step is once you have this positive probabilities, you want to estimate your para meters that essentially you're learning the multinomial distribution.",
            "So here there are two things in this.",
            "There are two para meters.",
            "What is the probability of observing a particular word given a particular latent concept, and this update happens to be same exactly as latent semantic indexing.",
            "The only thing we need to change is what is the probability of ZK given die.",
            "So given this document, what is the probability that you observe this semantic concept and this update differs because of our regularization to encode the manifold assumption."
        ],
        [
            "So if you see in the paper they show that this update can essentially be got by solving this linear certification with Sigma plus land.",
            "I'll inverse into this big vector.",
            "So Sigma is defined like this and it happens to be your graph Laplacian.",
            "So if you solve this, this gets your update equations and it's very easy to see that if you just set Lambda equal to 0, you fall back to latent semantic indexing.",
            "So.",
            "Now I'll describe some experiments with this.",
            "So essentially what we have done is we have taken latent semantic indexing, put the manifold assumption and then we have a new algorithms which.",
            "Takes the manifold assumption into the data."
        ],
        [
            "So they describe two sets of experiments.",
            "One is for text clustering and text classification.",
            "In text clustering.",
            "The taker data set, which has 30 categories and we have around 8000 documents and around 18,000 distinct words.",
            "So here what we're trying to do is I want to cluster the data all the text documents.",
            "So the way you do here is that we're building a model for P of Z given D, which is for the latent semantic concepts.",
            "And if you find which one has the maximum, that gives you an idea of the cluster of the data.",
            "That's one approach.",
            "That is the one application and the second application is for text classification, which will come back later.",
            "As they say that the algorithms and datasets are available on their website."
        ],
        [
            "OK, let's see the results.",
            "So as I said, P of Z given D can be used to indicate the cluster, and they're comparing 6 algorithms.",
            "The first 2 is the conventional topic modeling approaches, which is latent semantic indexing and latent dislocation.",
            "The third one is the proposed method, which is essentially P LSA plus the manifold assumption.",
            "The last three are simple three clustering algorithms, which is K means and cut and NMF.",
            "And here it is.",
            "There is some measure of clustering accuracy.",
            "And what OK?",
            "Let's first see the last row overall.",
            "You are taking the average over a different kind of different number of clusters.",
            "So one OK.",
            "So what you want to notice first is that.",
            "The expected the clustering algorithms, surprisingly do much better than latent semantic index.",
            "An LDA, even though we had specific models here.",
            "But once you put the manifold assumption into this latent semantic analysis, you have much better results.",
            "So the."
        ],
        [
            "There are two parameters in their algorithm.",
            "One is the regularization.",
            "How much do you regularize on your manifold assumption and the 2nd is how much, how many nearest neighbors do you choose to build the graph?",
            "And this graph shows that it shows you as a function of your Lambda, which is your regularization parameter.",
            "Essentially what they're trying to say is that.",
            "It's a bit pretty much stable after certain Lambda and.",
            "Kind of day, I think in the experiments they use a fixed value of Lambda, which is something."
        ],
        [
            "1000 and this shows as a function of the parameter P, which is the number of nearest neighbors you pick to build the graph, and it's kind of relatively stable as a function of the number of nearest neighbors."
        ],
        [
            "OK, the second set is for classification.",
            "So how do we use topic models to do document classification?",
            "So what you can do is first OK. Then I've approaches, just use the word as the features.",
            "The frequency of the different words and then learning SVM as the classifier, and that's the first row.",
            "Use first column you see.",
            "Here in topic modeling what you do is you build a model for the latent concepts which are P of Z given the document.",
            "So now you use these distributions Z as a feature to build a classifier instead of using the actual words.",
            "So you're kind of doing a dimension reduction in terms of the head hidden semantic concept.",
            "And then I'm going to use an SVM to train a classifier.",
            "So this is with the word feature.",
            "Then your latent semantic indexing, and this is zygarde by LDA, and this is their approach.",
            "So in general they get the minimum error with this approach and then they just do another experiment where they show that you could always so we have a lot of freedom in building this graph here in the first topic modeling, you thought all the documents is just one group and threw it and then did a clustering.",
            "But if you know the labels, which one is the positive classic negative class, you could encode this in building your graph and they show that by encoding this kind of information you get much better results than without using that labels."
        ],
        [
            "So one key issue which people I think work on how many topic model?",
            "How many hidden topics are there?",
            "How many hidden topics to use?",
            "And there is no.",
            "I mean there's no answer for this, but there is a simulation which shows as a function of number of topics.",
            "How does your classification error rate change?",
            "And they say that for their method, it's kind of pretty the black line you see is kind of pretty flat and very much pretty robust to the number of topics."
        ],
        [
            "OK. To summarize, so they took the probabilistic latent semantic analysis method, and they showed that.",
            "How do you incorporate?",
            "The manifold assumption and they.",
            "Use and the proposed an EM algorithm which can exactly solve it.",
            "And as Lambda goes to zero, it kind of defaults to latent semantic analysis.",
            "And the experimental results on text clustering and classification show that they get pretty impressive results without taking the manifold assumptions.",
            "So the future work you can ask the authors further, and if you anymore comments or anything I suggest I let you write some comments in the discussion board so that someone should be able to answer to you.",
            "But anyway, I can take any other questions you want based on my understanding of the paper."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This paper could not make it to the conference because of the visa issues, so I'm trying to present it on their behalf and this is not really my area and I read the paper last night so be gentle on the questions.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to summarize, the main.",
                    "label": 0
                },
                {
                    "sent": "What the authors are trying to do here is that they show some impressive results that by if you use the manifold assumption you can get very good results on topic modeling approaches.",
                    "label": 1
                },
                {
                    "sent": "So generally in topic modeling you're given.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Collection of text documents and you want to build a model for the words, for example.",
                    "label": 0
                },
                {
                    "sent": "Let's say a given text as these words and I want to say what is the probability that this word occurred in that document.",
                    "label": 0
                },
                {
                    "sent": "So I'm trying to build a model for words in the text.",
                    "label": 0
                },
                {
                    "sent": "So on topic models are known to be a powerful tool for various text applications, like topic discovery, summarization, opinion, mining and many more.",
                    "label": 1
                },
                {
                    "sent": "So one.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The most popular approaches for topic modeling is what is known as the probabilistic latent semantic analysis.",
                    "label": 1
                },
                {
                    "sent": "So here you're given a bunch of documents.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Every document is a lot of words, so we are this bunch of words.",
                    "label": 0
                },
                {
                    "sent": "It belong to all the documents and very simply I'm interested in learning what is the probability that a given word occurs in a particular document.",
                    "label": 0
                },
                {
                    "sent": "This is all I'm interested to build a model for this task.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the naive approach would be just to count the number of occurrences of each word in that document and just do a frequency of that.",
                    "label": 1
                },
                {
                    "sent": "So essentially or frequency of that particular word occurring in a particular document.",
                    "label": 1
                },
                {
                    "sent": "But the problem with this approach is that it's what is known as a zero frequency problem.",
                    "label": 0
                },
                {
                    "sent": "So a lot of terms which are not occurring in a document, you get a zero probability.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What probabilistic latent semantic analysis does is so you have this.",
                    "label": 1
                },
                {
                    "sent": "At this left you have the documents and you have the words in between.",
                    "label": 0
                },
                {
                    "sent": "It introduced something called as a latent concept.",
                    "label": 0
                },
                {
                    "sent": "So the way our document is generated is first you pick a document.",
                    "label": 0
                },
                {
                    "sent": "Then you speak up concept and then once this concept is picked, this concept generates the words.",
                    "label": 0
                },
                {
                    "sent": "So given a document, you have a distribution over the concept Z.",
                    "label": 0
                },
                {
                    "sent": "So what is the probability of Z given a document and once you pick a particular concept, you have, what is the probability that you can generate a particular word given that concept?",
                    "label": 0
                },
                {
                    "sent": "So essentially, instead of summarizing a document by a bunch of words, you're saying we're going to summarize it by a bunch of concepts and trying to learn a model for the concepts rather than the words.",
                    "label": 0
                },
                {
                    "sent": "Just to give an example, let's say the article is about trade.",
                    "label": 0
                },
                {
                    "sent": "So and there are three concepts.",
                    "label": 0
                },
                {
                    "sent": "So probability of trade given the document would be, let's say .8 and once you pick the concept rate, you're going to generate words like economic imports and trade.",
                    "label": 0
                },
                {
                    "sent": "So with this model, what you can write if you can just now probability of a given word occurring in a document can be written as the summation over all possible concepts.",
                    "label": 0
                },
                {
                    "sent": "So you have the probability of Z given the document.",
                    "label": 0
                },
                {
                    "sent": "What is the probability that a particular concept occurs.",
                    "label": 0
                },
                {
                    "sent": "And given this concept, what is the probability that word appears in that document and you just sum it over all possible documents?",
                    "label": 0
                },
                {
                    "sent": "So so this.",
                    "label": 0
                },
                {
                    "sent": "So what we observe here is just the documents and the words.",
                    "label": 0
                },
                {
                    "sent": "So that's our observation.",
                    "label": 1
                },
                {
                    "sent": "What I'm trying to learn is the latent concepts.",
                    "label": 0
                },
                {
                    "sent": "Or more precisely, I want to learn these two para meters.",
                    "label": 0
                },
                {
                    "sent": "What is the probability given a document, what is the probability that a particular it comes belongs to a particular concept and what is the probability that that word appears given that concept.",
                    "label": 0
                },
                {
                    "sent": "So these are the models of my parameters and my observation is just the documents and the words.",
                    "label": 0
                },
                {
                    "sent": "So how do I find those parameters?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the most popular approach as I mentioned is what is known as P, LSI probabilistic latent semantic analysis.",
                    "label": 1
                },
                {
                    "sent": "An there is also the latent dislocation.",
                    "label": 0
                },
                {
                    "sent": "This also the I don't know this patching or location and many more.",
                    "label": 0
                },
                {
                    "sent": "So all this isn't all these models, what they're trying to do is they're trying to build a model on the ambient Euclidean space, so they're trying to build a probabilistic model on your words probability of Z given D. So the DSD lies in some Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "So the main contribution of this talk is that.",
                    "label": 0
                },
                {
                    "sent": "We can show that if you take the geometric structure of the data, you can get a significant improvement in the topic models and this paper mainly talks about only the first method, which is latent semantic indexing.",
                    "label": 0
                },
                {
                    "sent": "And how can you bring in the geometric structure of the manifold into the latent semantic indexing model.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I think I shouldn't spend too much on this diagram, so the key assumption is that you assume that documents could lie on this underlying, probably smooth low dimensional manifold.",
                    "label": 0
                },
                {
                    "sent": "And now I want to build a model.",
                    "label": 0
                },
                {
                    "sent": "How do I exploit this structure?",
                    "label": 0
                },
                {
                    "sent": "So the main assumption with this exploit is what is known as the locally consistent assumption that the nearby points should share similar properties.",
                    "label": 1
                },
                {
                    "sent": "So the challenge here is how do we incorporate this nearby information in terms of probability distributions and try to solve the latent semantic indexing problem.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, as usual you start with a graph.",
                    "label": 0
                },
                {
                    "sent": "So you have each document.",
                    "label": 0
                },
                {
                    "sent": "It's a point.",
                    "label": 0
                },
                {
                    "sent": "In some high dimensional Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "So each document is represented by the word frequencies.",
                    "label": 0
                },
                {
                    "sent": "And on these documents you can't reconstruct AP nearest neighbor graph, so every document is connected to P of its nearest neighbors.",
                    "label": 1
                },
                {
                    "sent": "So what P LSA that's the normal topic modeling approach does is at every point on this graph there is.",
                    "label": 0
                },
                {
                    "sent": "You have a distribution or probability of Z given D. So.",
                    "label": 0
                },
                {
                    "sent": "So the key idea here is that you have at every document where you have at every point in this Euclidian space, I have a probability distribution.",
                    "label": 1
                },
                {
                    "sent": "So now I want to smooth this probability distribution to encode the geometric structure in the data.",
                    "label": 0
                },
                {
                    "sent": "So essentially in the end you would look something like this.",
                    "label": 0
                },
                {
                    "sent": "You take the normal Prob topic model, then you impose a constraint that if two documents are closed by you want the topic the probability distributions to be smooth.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this would be the normal typical objective function for latent semantic indexing, where here this is the number of times that document word occurs in a document, and generally we're trying to maximize this.",
                    "label": 0
                },
                {
                    "sent": "So this is a typical log likelihood.",
                    "label": 0
                },
                {
                    "sent": "There's nothing new here.",
                    "label": 0
                },
                {
                    "sent": "The part comes here is how do we regularize this objective function to encode the smoothness constraint and Lambda is your regularization parameter.",
                    "label": 0
                },
                {
                    "sent": "I want to maximize this term so that at the same time I want to penalize if our distributions learn are not smooth.",
                    "label": 0
                },
                {
                    "sent": "So the key now issue is how do you measure the smoothness of a distribution P of Z given D over the geometric structure of the data.",
                    "label": 1
                },
                {
                    "sent": "So there are two things.",
                    "label": 0
                },
                {
                    "sent": "How do you measure the smoothness and how do you encode the geometric structure of the data?",
                    "label": 0
                },
                {
                    "sent": "The to measure the smoothness, the authors use the callback library divergance between two probability distributions.",
                    "label": 0
                },
                {
                    "sent": "So I&J are your two documents at point I, you have a distribution of P of Z.",
                    "label": 0
                },
                {
                    "sent": "Given D, There is a distribution of your latent semantic concepts and similarly at Jay you have this distribution.",
                    "label": 0
                },
                {
                    "sent": "Now you want to measure how close these two distributions are.",
                    "label": 0
                },
                {
                    "sent": "So one of the commonly used metric is the color black library vergence.",
                    "label": 0
                },
                {
                    "sent": "Since it is not symmetric.",
                    "label": 0
                },
                {
                    "sent": "They kind of make it more symmetric by using IJ&J and then take a square of this term.",
                    "label": 0
                },
                {
                    "sent": "So and now this this term can measure how smooth your distribution is.",
                    "label": 0
                },
                {
                    "sent": "Now you want to make sure this is not doesn't smooth everywhere.",
                    "label": 0
                },
                {
                    "sent": "You want to smooth only on your graph.",
                    "label": 0
                },
                {
                    "sent": "So essentially this graph is constructed by using WI J, which says that don't bother to smooth if your point is too far, so only care about only nearby points nearby documents.",
                    "label": 0
                },
                {
                    "sent": "So WI J brings in your.",
                    "label": 0
                },
                {
                    "sent": "What do you call the manifold assumption?",
                    "label": 0
                },
                {
                    "sent": "And this brings you the smoothness assumption of your distribution.",
                    "label": 0
                },
                {
                    "sent": "So with this now we have all the tools to crank out the math.",
                    "label": 0
                },
                {
                    "sent": "So this is what you want to maximize.",
                    "label": 0
                },
                {
                    "sent": "And you can, so this is essentially a regularised log likelihood.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the standard way latent semantic analysis does is just as am algorithm and tries to estimate the parameters and you can see that most almost all of the parameters remain the same except for one particular term.",
                    "label": 0
                },
                {
                    "sent": "So the eastep essentially calculates the posterior probability of the latent variables.",
                    "label": 1
                },
                {
                    "sent": "Which essentially happens to be exactly same as in latent semantic indexing.",
                    "label": 0
                },
                {
                    "sent": "So you want to calculate what is the probability of that semantic concept given the word and a document.",
                    "label": 0
                },
                {
                    "sent": "The M step is once you have this positive probabilities, you want to estimate your para meters that essentially you're learning the multinomial distribution.",
                    "label": 0
                },
                {
                    "sent": "So here there are two things in this.",
                    "label": 0
                },
                {
                    "sent": "There are two para meters.",
                    "label": 0
                },
                {
                    "sent": "What is the probability of observing a particular word given a particular latent concept, and this update happens to be same exactly as latent semantic indexing.",
                    "label": 0
                },
                {
                    "sent": "The only thing we need to change is what is the probability of ZK given die.",
                    "label": 0
                },
                {
                    "sent": "So given this document, what is the probability that you observe this semantic concept and this update differs because of our regularization to encode the manifold assumption.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you see in the paper they show that this update can essentially be got by solving this linear certification with Sigma plus land.",
                    "label": 0
                },
                {
                    "sent": "I'll inverse into this big vector.",
                    "label": 0
                },
                {
                    "sent": "So Sigma is defined like this and it happens to be your graph Laplacian.",
                    "label": 0
                },
                {
                    "sent": "So if you solve this, this gets your update equations and it's very easy to see that if you just set Lambda equal to 0, you fall back to latent semantic indexing.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Now I'll describe some experiments with this.",
                    "label": 0
                },
                {
                    "sent": "So essentially what we have done is we have taken latent semantic indexing, put the manifold assumption and then we have a new algorithms which.",
                    "label": 0
                },
                {
                    "sent": "Takes the manifold assumption into the data.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So they describe two sets of experiments.",
                    "label": 0
                },
                {
                    "sent": "One is for text clustering and text classification.",
                    "label": 1
                },
                {
                    "sent": "In text clustering.",
                    "label": 1
                },
                {
                    "sent": "The taker data set, which has 30 categories and we have around 8000 documents and around 18,000 distinct words.",
                    "label": 0
                },
                {
                    "sent": "So here what we're trying to do is I want to cluster the data all the text documents.",
                    "label": 0
                },
                {
                    "sent": "So the way you do here is that we're building a model for P of Z given D, which is for the latent semantic concepts.",
                    "label": 0
                },
                {
                    "sent": "And if you find which one has the maximum, that gives you an idea of the cluster of the data.",
                    "label": 0
                },
                {
                    "sent": "That's one approach.",
                    "label": 0
                },
                {
                    "sent": "That is the one application and the second application is for text classification, which will come back later.",
                    "label": 1
                },
                {
                    "sent": "As they say that the algorithms and datasets are available on their website.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, let's see the results.",
                    "label": 0
                },
                {
                    "sent": "So as I said, P of Z given D can be used to indicate the cluster, and they're comparing 6 algorithms.",
                    "label": 1
                },
                {
                    "sent": "The first 2 is the conventional topic modeling approaches, which is latent semantic indexing and latent dislocation.",
                    "label": 0
                },
                {
                    "sent": "The third one is the proposed method, which is essentially P LSA plus the manifold assumption.",
                    "label": 0
                },
                {
                    "sent": "The last three are simple three clustering algorithms, which is K means and cut and NMF.",
                    "label": 0
                },
                {
                    "sent": "And here it is.",
                    "label": 0
                },
                {
                    "sent": "There is some measure of clustering accuracy.",
                    "label": 0
                },
                {
                    "sent": "And what OK?",
                    "label": 0
                },
                {
                    "sent": "Let's first see the last row overall.",
                    "label": 0
                },
                {
                    "sent": "You are taking the average over a different kind of different number of clusters.",
                    "label": 0
                },
                {
                    "sent": "So one OK.",
                    "label": 0
                },
                {
                    "sent": "So what you want to notice first is that.",
                    "label": 0
                },
                {
                    "sent": "The expected the clustering algorithms, surprisingly do much better than latent semantic index.",
                    "label": 0
                },
                {
                    "sent": "An LDA, even though we had specific models here.",
                    "label": 0
                },
                {
                    "sent": "But once you put the manifold assumption into this latent semantic analysis, you have much better results.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are two parameters in their algorithm.",
                    "label": 0
                },
                {
                    "sent": "One is the regularization.",
                    "label": 0
                },
                {
                    "sent": "How much do you regularize on your manifold assumption and the 2nd is how much, how many nearest neighbors do you choose to build the graph?",
                    "label": 0
                },
                {
                    "sent": "And this graph shows that it shows you as a function of your Lambda, which is your regularization parameter.",
                    "label": 0
                },
                {
                    "sent": "Essentially what they're trying to say is that.",
                    "label": 0
                },
                {
                    "sent": "It's a bit pretty much stable after certain Lambda and.",
                    "label": 0
                },
                {
                    "sent": "Kind of day, I think in the experiments they use a fixed value of Lambda, which is something.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "1000 and this shows as a function of the parameter P, which is the number of nearest neighbors you pick to build the graph, and it's kind of relatively stable as a function of the number of nearest neighbors.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, the second set is for classification.",
                    "label": 0
                },
                {
                    "sent": "So how do we use topic models to do document classification?",
                    "label": 0
                },
                {
                    "sent": "So what you can do is first OK. Then I've approaches, just use the word as the features.",
                    "label": 0
                },
                {
                    "sent": "The frequency of the different words and then learning SVM as the classifier, and that's the first row.",
                    "label": 0
                },
                {
                    "sent": "Use first column you see.",
                    "label": 0
                },
                {
                    "sent": "Here in topic modeling what you do is you build a model for the latent concepts which are P of Z given the document.",
                    "label": 0
                },
                {
                    "sent": "So now you use these distributions Z as a feature to build a classifier instead of using the actual words.",
                    "label": 0
                },
                {
                    "sent": "So you're kind of doing a dimension reduction in terms of the head hidden semantic concept.",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to use an SVM to train a classifier.",
                    "label": 0
                },
                {
                    "sent": "So this is with the word feature.",
                    "label": 0
                },
                {
                    "sent": "Then your latent semantic indexing, and this is zygarde by LDA, and this is their approach.",
                    "label": 0
                },
                {
                    "sent": "So in general they get the minimum error with this approach and then they just do another experiment where they show that you could always so we have a lot of freedom in building this graph here in the first topic modeling, you thought all the documents is just one group and threw it and then did a clustering.",
                    "label": 0
                },
                {
                    "sent": "But if you know the labels, which one is the positive classic negative class, you could encode this in building your graph and they show that by encoding this kind of information you get much better results than without using that labels.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one key issue which people I think work on how many topic model?",
                    "label": 0
                },
                {
                    "sent": "How many hidden topics are there?",
                    "label": 1
                },
                {
                    "sent": "How many hidden topics to use?",
                    "label": 0
                },
                {
                    "sent": "And there is no.",
                    "label": 1
                },
                {
                    "sent": "I mean there's no answer for this, but there is a simulation which shows as a function of number of topics.",
                    "label": 0
                },
                {
                    "sent": "How does your classification error rate change?",
                    "label": 0
                },
                {
                    "sent": "And they say that for their method, it's kind of pretty the black line you see is kind of pretty flat and very much pretty robust to the number of topics.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. To summarize, so they took the probabilistic latent semantic analysis method, and they showed that.",
                    "label": 0
                },
                {
                    "sent": "How do you incorporate?",
                    "label": 0
                },
                {
                    "sent": "The manifold assumption and they.",
                    "label": 0
                },
                {
                    "sent": "Use and the proposed an EM algorithm which can exactly solve it.",
                    "label": 1
                },
                {
                    "sent": "And as Lambda goes to zero, it kind of defaults to latent semantic analysis.",
                    "label": 0
                },
                {
                    "sent": "And the experimental results on text clustering and classification show that they get pretty impressive results without taking the manifold assumptions.",
                    "label": 1
                },
                {
                    "sent": "So the future work you can ask the authors further, and if you anymore comments or anything I suggest I let you write some comments in the discussion board so that someone should be able to answer to you.",
                    "label": 0
                },
                {
                    "sent": "But anyway, I can take any other questions you want based on my understanding of the paper.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}