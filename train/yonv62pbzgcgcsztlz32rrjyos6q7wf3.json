{
    "id": "yonv62pbzgcgcsztlz32rrjyos6q7wf3",
    "title": "On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient",
    "info": {
        "author": [
            "Jie Tang, Department of Electrical Engineering and Computer Sciences, UC Berkeley"
        ],
        "published": "March 25, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Reinforcement Learning"
        ]
    },
    "url": "http://videolectures.net/nips2010_tang_cbi/",
    "segmentation": [
        [
            "Hi, I'm Jay and this is joint work with Peter Beale at UC Berkeley.",
            "So."
        ],
        [
            "Good ratio policy.",
            "Gradient methods have been some of the most successful reinforcement learning algorithms, especially for learning on physical systems.",
            "These algorithms have been used to tune walking gaits for leg robots hit baseballs with robot arms and do motion planning for humanoid robotics.",
            "These algorithms are especially good at fine tuning the parameters of stochastic control policies for complex complex tasks in continuous state action spaces.",
            "The idea behind them is to directly optimize the expected return as a function of the policy parameters Theta.",
            "This can be done as follows.",
            "Given a policy Pi Theta, you execute it, obtain sample state action sequences and observe the associated rewards.",
            "You then adjust the policy parameters Theta to make state action sequences that give high reward more likely.",
            "This is done by following the gradient of the expected return.",
            "It turns out you can compute the gradient of the expected return function from sample trajectories even without access to a dynamics model.",
            "One limitation of these methods is that after every parameter update.",
            "New samples need to be gathered from the environment and you discard samples gathered under past policy parameters.",
            "Now, ideally we'd like to make use of all the data that we've gathered."
        ],
        [
            "Important sampling provides a way to evaluate an expectation under one distribution using samples drawn from a different distribution.",
            "In our case, we can estimate the expected return function under policy parameters, Theta one using samples gathered under a different policy parameters Theta 2.",
            "Our first result establishes that the sample estimate of the gradient of this expected important sample expected return function.",
            "You had a Theta evaluated using only samples directory is drawn under Pius Theta is equal to the likelihood ratio based sample estimate of the gradient of the expected return function U of Theta.",
            "In other words, the likelihood ratio based gradient estimate is a special case of an important sample based grading system.",
            "This implies that likelihood ratio based policy gradient methods are not making full use of the data in two ways.",
            "First, there artificially restricting themselves to samples gathered under a single set of policy parameters.",
            "And second, they are using only gradient information contained in the important sampled expected return function.",
            "Not making use of the function information itself.",
            "So this suggests that important sampling methods should outperform likelihood ratio methods.",
            "However, past work on important sampling and likelihood ratio policy gradients has not performed well enough to justify including this as a common part of the policy gradient toolbox."
        ],
        [
            "We found that there exist several techniques which have been applied to likelihood ratio methods.",
            "Which are necessary to generalize to be generalized.",
            "The important sampling, in order to guarantee good performance?",
            "One of these commonly used techniques that we generalize, the important sampling setting is idea of optimal baselines.",
            "Optimal baselines for likelihood ratio policy gradient methods improve convergence speeds by reducing the variance of the gradient estimates.",
            "You can think of the baseline term B as the average expected reward, and intuition is that we want to make sample to directories which give more than average more likely and sample directories, which gives less rewarding average less likely.",
            "We generalize this baseline technique.",
            "To any expectation, overparameterized probability distribution, P, Theta of X.",
            "So for any distribution P, Theta X, any scalar valued function F of X and any fixed vector B, you can introduce an unbiased baseline term which is not change the value of the final expectation.",
            "Again, this is a result that applies to any expectation and a wide range of estimators.",
            "So we can set B to minimize the variance of the resulting estimator.",
            "If we use this baseline technique to estimate the important sampled expected return function, you have data, we can get performance improvements similar to the ones we see from optimal baselines for likely iteration methods.",
            "If we do this plus a few other techniques that I'll describe at the poster, we can get better performance and likelihood ratio methods on a variety of simulated test beds.",
            "If you'd like to learn more, please come by poster number W 19 tonight."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi, I'm Jay and this is joint work with Peter Beale at UC Berkeley.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good ratio policy.",
                    "label": 0
                },
                {
                    "sent": "Gradient methods have been some of the most successful reinforcement learning algorithms, especially for learning on physical systems.",
                    "label": 1
                },
                {
                    "sent": "These algorithms have been used to tune walking gaits for leg robots hit baseballs with robot arms and do motion planning for humanoid robotics.",
                    "label": 0
                },
                {
                    "sent": "These algorithms are especially good at fine tuning the parameters of stochastic control policies for complex complex tasks in continuous state action spaces.",
                    "label": 0
                },
                {
                    "sent": "The idea behind them is to directly optimize the expected return as a function of the policy parameters Theta.",
                    "label": 0
                },
                {
                    "sent": "This can be done as follows.",
                    "label": 0
                },
                {
                    "sent": "Given a policy Pi Theta, you execute it, obtain sample state action sequences and observe the associated rewards.",
                    "label": 0
                },
                {
                    "sent": "You then adjust the policy parameters Theta to make state action sequences that give high reward more likely.",
                    "label": 0
                },
                {
                    "sent": "This is done by following the gradient of the expected return.",
                    "label": 1
                },
                {
                    "sent": "It turns out you can compute the gradient of the expected return function from sample trajectories even without access to a dynamics model.",
                    "label": 0
                },
                {
                    "sent": "One limitation of these methods is that after every parameter update.",
                    "label": 0
                },
                {
                    "sent": "New samples need to be gathered from the environment and you discard samples gathered under past policy parameters.",
                    "label": 0
                },
                {
                    "sent": "Now, ideally we'd like to make use of all the data that we've gathered.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Important sampling provides a way to evaluate an expectation under one distribution using samples drawn from a different distribution.",
                    "label": 0
                },
                {
                    "sent": "In our case, we can estimate the expected return function under policy parameters, Theta one using samples gathered under a different policy parameters Theta 2.",
                    "label": 0
                },
                {
                    "sent": "Our first result establishes that the sample estimate of the gradient of this expected important sample expected return function.",
                    "label": 1
                },
                {
                    "sent": "You had a Theta evaluated using only samples directory is drawn under Pius Theta is equal to the likelihood ratio based sample estimate of the gradient of the expected return function U of Theta.",
                    "label": 1
                },
                {
                    "sent": "In other words, the likelihood ratio based gradient estimate is a special case of an important sample based grading system.",
                    "label": 1
                },
                {
                    "sent": "This implies that likelihood ratio based policy gradient methods are not making full use of the data in two ways.",
                    "label": 0
                },
                {
                    "sent": "First, there artificially restricting themselves to samples gathered under a single set of policy parameters.",
                    "label": 0
                },
                {
                    "sent": "And second, they are using only gradient information contained in the important sampled expected return function.",
                    "label": 1
                },
                {
                    "sent": "Not making use of the function information itself.",
                    "label": 0
                },
                {
                    "sent": "So this suggests that important sampling methods should outperform likelihood ratio methods.",
                    "label": 0
                },
                {
                    "sent": "However, past work on important sampling and likelihood ratio policy gradients has not performed well enough to justify including this as a common part of the policy gradient toolbox.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We found that there exist several techniques which have been applied to likelihood ratio methods.",
                    "label": 0
                },
                {
                    "sent": "Which are necessary to generalize to be generalized.",
                    "label": 0
                },
                {
                    "sent": "The important sampling, in order to guarantee good performance?",
                    "label": 0
                },
                {
                    "sent": "One of these commonly used techniques that we generalize, the important sampling setting is idea of optimal baselines.",
                    "label": 0
                },
                {
                    "sent": "Optimal baselines for likelihood ratio policy gradient methods improve convergence speeds by reducing the variance of the gradient estimates.",
                    "label": 1
                },
                {
                    "sent": "You can think of the baseline term B as the average expected reward, and intuition is that we want to make sample to directories which give more than average more likely and sample directories, which gives less rewarding average less likely.",
                    "label": 0
                },
                {
                    "sent": "We generalize this baseline technique.",
                    "label": 0
                },
                {
                    "sent": "To any expectation, overparameterized probability distribution, P, Theta of X.",
                    "label": 0
                },
                {
                    "sent": "So for any distribution P, Theta X, any scalar valued function F of X and any fixed vector B, you can introduce an unbiased baseline term which is not change the value of the final expectation.",
                    "label": 1
                },
                {
                    "sent": "Again, this is a result that applies to any expectation and a wide range of estimators.",
                    "label": 1
                },
                {
                    "sent": "So we can set B to minimize the variance of the resulting estimator.",
                    "label": 0
                },
                {
                    "sent": "If we use this baseline technique to estimate the important sampled expected return function, you have data, we can get performance improvements similar to the ones we see from optimal baselines for likely iteration methods.",
                    "label": 0
                },
                {
                    "sent": "If we do this plus a few other techniques that I'll describe at the poster, we can get better performance and likelihood ratio methods on a variety of simulated test beds.",
                    "label": 0
                },
                {
                    "sent": "If you'd like to learn more, please come by poster number W 19 tonight.",
                    "label": 0
                }
            ]
        }
    }
}