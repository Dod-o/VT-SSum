{
    "id": "dqkeiwftebf23uoa3a7utx5wzfcdsqvn",
    "title": "Which Supervised Learning Method Works Best for What? An Empirical Comparison of Learning Methods and Metrics",
    "info": {
        "author": [
            "Rich Caruana, Cornell University"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "May 2006",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/solomon_caruana_wslmw/",
    "segmentation": [
        [
            "What's kind of calculators should be used for certain problems and how to evaluate this is partly, so I saw that it is from from certain.",
            "Somehow I connect this.",
            "Text from the upset also to the work that you did love this Kitty car little bit, or with this some more intensive work.",
            "So we organised together with Torsten your claims.",
            "Two years ago I think.",
            "2004 and so this was very interesting challenge I guess for the whole community through how to compare algorithms, ideas on couple of datasets.",
            "So this will be also seem a little bit here.",
            "So we have roughly whatever one hour and you have more less open discussion.",
            "So if you have if you have any questions, I guess you can just drop it.",
            "One more thing got it.",
            "Research visit sponsored by Pascal.",
            "Excellent.",
            "This is one of several visits visits, which we will also feature so.",
            "So thank you to Marco Engineer for inviting me to come in for Pascal for funding it, and especially to you for signature for about 45 minutes from here.",
            "So sorry, I was so late.",
            "So I got more slides and I'll be able to go through in the hour hour and a half at work to do this.",
            "So feel free to stop in this questions and I can always break at different times during the talk, so this is joint work with some of my graduate students.",
            "Allison give us to Christian Mercia and arguments and all identify something."
        ],
        [
            "OK, so how many people here already know machine learning?",
            "OK, so pretty much everybody knows machine learning.",
            "So more less at this almost everybody.",
            "I think her course of being good.",
            "Do you all know what supervised learning is?",
            "Basically it's regression on steroids so you know how."
        ],
        [
            "We have a sort of sad state of affairs in machine learning and that such that it appears that in some sense we've been too successful.",
            "So over the last 30 years we've developed, now count is about 35 different machine learning algorithms out there.",
            "Most of them I'm sure you've heard of, so we've got things like regression K, nearest neighbor, Decision tree, support vector machines, graphical models, methods, things like boosting and bagging, and random forests, and then maybe some more things that you haven't played with death.",
            "I'm an extra Gaussian processes yet, so we've got all these different learning methods."
        ],
        [
            "And I think.",
            "My impression is that you guys already know what came nearest neighbor is right.",
            "But how many people don't know what Kangaroos neighbors?"
        ],
        [
            "Windows 10 years neighbor everybody."
        ],
        [
            "This decision trees that."
        ],
        [
            "Everybody knows neural Nets.",
            "How much neural net.",
            "So do they still teach parents?",
            "OK, you have a couple of interesting so so this talk will surprise you then, especially in the beginning, because neural Nets are going to feature heavily in it, so neural Nets of these things which take a vector of inputs.",
            "They go through a bunch of connection rates to some hidden units that learner even presentation of the data and then through another set of ways to save the output unit you can have multiple hidden layers here and then.",
            "The thing that you're predicting predicting on this applet unit, and we train them in the very simple procedure, just gradient descent backdrop is just a fancy name for efficient.",
            "Set."
        ],
        [
            "We're going to see neural network show up later.",
            "Neural networks.",
            "One of the nice things about them is that they are very flexible models.",
            "I can pick a large vector of inputs and as well a large vector and outputs.",
            "Although we're not going to take any advantage in the talk today.",
            "Um, this vector values.",
            "We're just going to have one output."
        ],
        [
            "A lot of my work has been in the past on training networks that have multiple outputs, so if you're interested in that, talk to me later in the week will be here.",
            "Linear regression is just a special case.",
            "It's a very simple neural network.",
            "We have just the inputs.",
            "Just these wings going to 1 unit.",
            "That's the output unit and this is just a linear transfer function in this album unit.",
            "So that's what linear regression is expression."
        ],
        [
            "Network and then logistic regression.",
            "If you're familiar with this progression, is also just a simple network where we've taken that out.",
            "Looking at which used to be a linear unit, and now we replace it with the same point function so that that's logistic regression.",
            "And then there are ways of regularising that and that gives you things like rich aggression."
        ],
        [
            "So everybody must know support vector machines then right?",
            "So so you learn the newer methods of machine learning, so of course in support vector machines we're trying to maximize this margin that separates the two classes in the binary classification."
        ],
        [
            "OK, so we've got all these different learning methods all talk about some of these others, such as a bagging boost.",
            "And random forests as we get further into talk, how many people already know value?",
            "So everybody knows boosting nice soon random forests.",
            "OK, good good.",
            "So many of you know random parts as well, so I'll talk about them a little more later later in the talk.",
            "Is there also going to turn out to be very competitive?",
            "You got a problem though.",
            "Not only do we have 30 odd message over here on the left, we have no prescription about when to use these things or when not to use them, what problems they're good for, what problems they're bad for, and it sounds like already you stop teaching some of them, such as neural Nets, because there are some.",
            "How old are inferior to newer things like support vector machines.",
            "We'll see if that's true.",
            "There's an even bigger problem though, so each of these things you know I say support vector machines or decision trees as if that's just a single algorithm.",
            "And of course that's not the case right?",
            "With support vector machines, there's lots of different kernels that you could decide to use.",
            "There are different losses that you can decide to train to write.",
            "You don't just have to use hinge loss, you could use the one norm.",
            "This log loss for SPMS, so SVN find themselves are really a whole family of algorithms, and in fact there are many different heuristic procedures for how to train these SPN.",
            "The whole family advantage here.",
            "Similarly, neural Nets in the hands of a master are very flexible, kind of play that can be made to represent many different things.",
            "There are a full class of algorithms.",
            "Decision trees are also a full class of algorithms.",
            "Turns out there's a decent dozen different major ways of doing decision trees in anyone textbook.",
            "You'll only learn about one of them.",
            "Maybe they'll mention one or two of the others, but but this is also a whole family of trees, tree types, and similarly for K nearest neighbors.",
            "There's a lot of different memory based learning techniques.",
            "When we say painters neighbor, there really is just one hour and there is a whole family of algorithms that have been developed over the past 40 years.",
            "So so, not only are the sort of 30 different major categories of algorithms, but each of these things has all these different flavors and parameters that you have to set.",
            "So in some sense, there's actually hundreds of different algorithms, and we really don't have anywhere in a textbook printer paper.",
            "This sort of rule which says, oh, if your data looks like this.",
            "And if you're trying to optimize this, and this is what's important, you should use either this algorithm or this algorithm and ignore the other 200 right, so we don't have that sort of thing, so that's really a bit of a problem.",
            "Is optimizing your tribe all these methods and then optimize each method to make it best in your problem?",
            "That's a lot of work, so so we're actually going to do some of that here to see what happens, but we don't recommend that you do that unless you really, really need to squeeze out the last drop form."
        ],
        [
            "So questions you might be interested in well.",
            "Maybe one of these algorithms just dominates all the others, right?",
            "Maybe support vector machines really are just the best thing you can ignore all these other things and just use them all the time.",
            "That would be great to find out about this trip.",
            "Or maybe it will turn out that something a little simpler than that is true.",
            "Maybe it'll just turn out that you know you're doing regression using neural Nets.",
            "If you're trying to do classification, especially binary classification, then use SPS.",
            "Maybe it'll be something simple like that.",
            "That would be fine.",
            "If that's not true, maybe no one method is going to dominate.",
            "Maybe it's not gonna be.",
            "The Testament is best for classification in general, but maybe at least a number of these methods won't be very good and we can sort of flipping the rest.",
            "So, just as you're not learning neural Nets anymore, maybe it's safer not learn neural Nets anymore, because they're just not going to be competitive.",
            "So be nice to know that neural Nets and K nearest neighbor where things you didn't have to learn about something dusty textbooks.",
            "Well maybe it's gonna turn out that some methods are doing some loss, and yet that very same method would be bad on a different kind of loss.",
            "That's sort of interesting just to be good on one loss, it means you in some sense have to be a smart algorithm.",
            "And yet, when it comes to a different loss, you're not very good.",
            "So you're doing something stupid on the very same problem.",
            "What you're doing something smart.",
            "It just depends on how you're being evaluated, so we'll talk a little bit about that.",
            "And then if we have time, we'll talk about how different ways of measuring loss related to each other, and ultimately which loss might be more robust than other losses.",
            "You know, there are now dozens of different kinds of losses out there that you see show up in papers, maybe one or two of these are better than the others, and you should just focus on those, because other ones are too high.",
            "Parents, for example.",
            "Optimized ultimately though, what it all comes down to is what should you use?",
            "You got a problem.",
            "What learning as it should you use?",
            "What flavor that learning method should you use?",
            "And maybe what loss function should you so those are the kind of questions but remains."
        ],
        [
            "So I'm going to present.",
            "This is an empirical talk, so so it's a bunch of results of experiments that we've run.",
            "It's not a theoretical analysis of these algorithms at all, so we're now up to this is older now.",
            "211 datasets that fairly expensive experiment, so you'll see why we don't have 50 days since here and most of the results in a presenter going to be on these eight datasets and half of these were taken from UC Irvine repository.",
            "I've abused that repository just like everybody else but the other half are real problems, for which I have real collaborators.",
            "In some sense I have customers who are genuinely interested in the results of that, yet so those tend to be problems in in particle physics or problems in pneumonia.",
            "Medical prediction seems like that, and this is important, so all these permits I'm going to do are going to be done with fairly modest training sets, just 5000 points being used for training.",
            "So we're gonna typically train the models with 4000 plus points and then hold aside 1000 of the training points for some sort of validation set to do parameter selection for the model.",
            "You know, maybe to figure out what kernel we should use for the SVM or do early stopping or weight decay within neural net or something like that so, but this is all training data and then most of these datasets are moderately large.",
            "Most of them are larger than 20,000.",
            "Only a few of them are smaller than that, so typically we can afford to have very large final Test sets, which means that when I report numbers you can sort of trust him to the second, possibly 3rd decimal place on the test.",
            "So notice that I've intentionally I've got a lot of data.",
            "And I'm intentionally not used at all for training, so one of the reasons I'm doing this for computational efficiency one is about I can have these large final Test sets so I don't have to do hundreds of applications across my numbers, But this really is perhaps the key limitation of this study.",
            "So if you're working on a problem where you've got 100,000 points in your training set for 10,000,000 points in your training set, it's very possible that the results I'm going to show here just going to apply it all to that regime.",
            "So, so going.",
            "Overgeneralize are overfit to any of the results that I present.",
            "So the other thing is, all of these datasets are sort of modest number of dimensions.",
            "There are from 20 to 300 dimensions, so these are not bags of words.",
            "Text problems where we might have 10,000 dimensions.",
            "So there's another way in which we could overfit.",
            "It's very possible that works very well on 100 dimensional.",
            "Problem isn't what works well in at 10,000 mention problems, so and I haven't got any 10,000 dimensional problems in the results of the report today."
        ],
        [
            "OK, so that's the empirical.",
            "Background and then we're going to do is.",
            "You know it's always painful to see a comparison of two algorithms made with one metric accuracy.",
            "Or square or something, but just one metric, right?",
            "Right?",
            "You don't know whether you really should trust.",
            "That is is, is that really indicative of what the model is learning about the problem in general, or is this something that's telling you more about that learning method modulo that metric, as opposed to how well that learning methods work on that problem?",
            "So what we're going to do is be agnostic about performance measures, at least in the first 2/3 of the talk, and we're just going to evaluate these learning methods on, as many metrics seem reasonable.",
            "OK, so three of these metrics are going to depend on comparing the prediction.",
            "All of the methods were using one way or another.",
            "Gonna be asking make a continuous value prediction.",
            "OK, so they're going to spit out a number, either minus Infinity plus Infinity, or a number between zero and one or something like that, as opposed to just the Boolean classification.",
            "And then what we'll do is, we'll we'll make it class classification out of it by putting some threshold on that prediction.",
            "And if you're below the threshold you're predicting plasty review above that threshold recruiting class one and everything I'm doing is just.",
            "Volume classification.",
            "So, so that's accuracy, which of course everybody knows how many people are familiar with F score for lift.",
            "OK, so so nobody is familiar with left maybe or just OK so so F scores based on precision and recall which is used a lot in information retrieval so I won't dwell on that list.",
            "Let me just mention briefly with Lift is.",
            "Suppose you have to send out a marketing brochure to your customer base, and what you'd like to do is.",
            "You'd like to only send the brochure to the fraction of the customer base that would actually respond to it, right?",
            "Most people just throw it in trash.",
            "You'd really like to focus your your marketing as much as possible.",
            "Will suppose if you randomly send a brochure out to 10% of your population.",
            "Suppose.",
            "If you just randomly pick who those people were, your response would have been 5%.",
            "OK, that means on average 5% of customers would respond to your brochure.",
            "But instead suppose you send it to 10%.",
            "That's carefully selected.",
            "OK, so you use the learning models predict who would respond and suppose now you get a 20% response rate on that 5% you sent it.",
            "Now got a lift of four because you've done 4 times better than random prediction soul is used a lot in marketing so it's gonna get it.",
            "There's a threshold in the left curve, but we won't talk.",
            "A feel free afterwards to ask more about these metrics if you want.",
            "We've got these warming metrics.",
            "It sounds like most people are familiar with precision and recall, so average precision is just like.",
            "If you calculate the full precision curve as you sweep the threshold, average position is just the average of that curve.",
            "There's some difficulties in there, but that's basically what it is.",
            "The precision, recall, break, even point is where these things hit the 45 degree diagonal line precision is equal to recall and then RC area.",
            "How many people are familiar with the area under your secret?",
            "OK, great, great so.",
            "So that's also very similar to precision recall RC area in some senses, perhaps more statistically sound measure of these sorts of things.",
            "The important thing about these measures, as they only depend.",
            "There's no comparison now of your predicted values for threshold.",
            "It just depends you have this case and you have this case.",
            "Maybe this is a positive and this is a negative with council nails.",
            "The relative value that you predicted for this case is if you ordering that it creates on the test.",
            "So all three of these metrics just depend on ordering and then we have three more metrics and these are the probability metrics and the probability metrics.",
            "Those are just good old friends like squared error and log loss, which I assume everybody knows about.",
            "It was one more than explaining about Pirate 10 slide.",
            "It's a measure of calibration model so that all equipment."
        ],
        [
            "We have a problem.",
            "Some of these metrics, like squared error zero is optimal.",
            "OK in a high value of .5 would be bad.",
            "For others like RC, .5 is in fact sort of random prediction and one would be optimal for other things like lift.",
            "Remember, I said you can get a lift of four.",
            "OK, so so that means lift can even be above 1.",
            "It can go up to a fraction that depends on the number of positives and negatives in your data set.",
            "Anyway, it's going to make comparing things across all these different metrics.",
            "Difficult, So what we're going to do is normalize all these scores and make the scales all the similar as possible.",
            "So in everything I'm going to talk about, we're going to have scale things in such a way that zero is baseline performance.",
            "So if you see a score of 0, that's the kind of performance any simple model should be able to achieve just by knowing the numbers of positives and negatives.",
            "Undateables.",
            "OK, so that's based on performance and one is going to be the best performance we think you can achieve in this problem.",
            "We really like to be the base optimal rate.",
            "We don't know the base operating because these are real problems.",
            "So what we're going to do is we sort of cheap train with a little extra data.",
            "We use use the big test set to do model selection and we sort of set the top of the scale up there by cheating basically, and then if any of these things achieve that kind of performance, they're going to support near 1 mean they're doing extremely well doing as well as you think.",
            "You have to cheat to do so just basically promised everything I present zero is sort of baseline performance.",
            "It's not bad performance, but it's not.",
            "Not meaningful performance.",
            "One is excellent performance.",
            "We like you."
        ],
        [
            "It's a pretty big set of experiment, so we've got 10 different learning methods and in fact, remember I said there's lots of different ways of running into these learning methods.",
            "So in fact there's hundreds of different parameter settings for some of these learning methods.",
            "For example, between 500 different neural Nets, and we're going to train hundreds of decision trees, and we train 120 SPM's and things like that.",
            "We're going to do 5 fold cross validation, so that means we're going to train over, say, 10,000 models for each of our problems, and we're up to 11 problems now.",
            "So we're going to train over 100,000 different models here.",
            "90 thousand of which tend to be very fast to train in the last 20,000 always seem to take all the time and never going to valuate on these nine performance metrics.",
            "So we're going to do something on the order of 1,000,000 evaluations of models, so it's a pretty big set of experiments.",
            "You can serve.",
            "Run this kind of experiment on one problem on a cluster of Saint 30.",
            "Linux nodes in a week.",
            "So if you got any problems between take sort of 10 weeks on the plus button to give you an idea what it takes to do this."
        ],
        [
            "I'm gonna start off just talking about probabilities first.",
            "Anna couple different reasons for doing that.",
            "Well, I don't want it.",
            "I'm gonna hit tables.",
            "Lots of numbers in it and I don't want to hit with big tables in the very beginning that have all these metrics 'cause it'll just be harder to make any sense of it.",
            "Also let's basically give it predict the right conditional probability for each test case.",
            "You'd be done.",
            "I mean, that's the best prediction you're going to be able to do any reasonable performance metric should be optimized.",
            "Conditional probabilities, so getting the right probabilities is really a sufficient condition for excellent performance on all metrics.",
            "If metrics are reasonably well engaged, so so that's one reason.",
            "Look at it and then it also turns out there's just a fun story to tell.",
            "If we look at probabilities and then my coauthor else dealing with students, he got a best paper award for this probability work, then I'm going to talk about in the next 2020 slides yourself."
        ],
        [
            "So it's interesting stuff.",
            "OK, so here is the first table of results.",
            "So let me walk through this so that you have.",
            "Here I thought the different learning methods on the left hand side.",
            "Here neural Nets, bag trees, random forests, K, your neighbor, all sorts of memory based learning, logistic regression, vanilla decision trees, boosted decision trees, SPM that have been scaled a certain way.",
            "We're going to talk a lot about that boosted stumps.",
            "Boosted stumps are just boosted decision trees that are one level trees and 90 phase, and these are the three performance measures for looking at right now.",
            "Squared error, cross entropy.",
            "It is just log loss.",
            "I am just measure calibration of all explained in a couple slides.",
            "Again these are all normal sports numbers up near one or this performance and this is the meaning of those three performance figures and the way this works is now realize each of these cells is the average.",
            "Well, first of all, over the five cross validation trials, but it's the average over eight, 910 or 11 problems.",
            "OK, so, So what does that mean for this?",
            "To be the average performance of neural Nets?",
            "On these say 11 problems with five crossovers across, well, here's what we've done.",
            "We gotta problem one.",
            "We train the neural net on its cross validation sample.",
            "We used to help out 1K to pick of all the neural Nets.",
            "Retraining which ones best that he's doing early, stopping weight decay, figuring out the number of hidden units, what learning rate to use, all that sort of stuff.",
            "Figure out for problem one for trial one the neural net that looks best, and then we write down this performance on the final Test.",
            "Now we go to trial two.",
            "We figure out the neural net.",
            "That's best right now.",
            "This performance on the final Test set.",
            "It could be a slightly different neural net, and the one that was best on trial one that's possible could have different different learning rate.",
            "Who knows what.",
            "When we get to problem too, it could be a very different grown up.",
            "It's possible that a problem when you really needed a large neural net with 256 hidden units is possible.",
            "That on problem to you only need one, can you?",
            "And that would be awesome.",
            "OK, so so we're averaging over the best neural Nets that you could train on each of these problems.",
            "They're not all the same neural net.",
            "OK, it's the same thing with SVN.",
            "We ask you answer trying all the different kernels that are available in SPM light we're trying.",
            "All the different parameter settings for those kernels that are reasonable, and then we're picking the one that's best using validation set.",
            "So you can think of this as being suppose you've given.",
            "An assignment which is here's a problem tonight.",
            "Go figure out how best training SPM on this problem.",
            "We're trying to simulate that kind of kind of thing.",
            "OK, so so we're doing exactly that.",
            "So we say this is the overall performance of SPS mean that's the best we could get SPS to do on every problem.",
            "Every trial.",
            "That's the average performance.",
            "OK, so now let's look at some of these numbers.",
            "So these things are sorted by their mean performance.",
            "So an interesting link.",
            "The thing that I guess you no longer get taught neural Nets is at the top of the table, so so that's sort of fun, right?",
            "And and some of these things that I know you can lead to believe or just the best things on the planet aren't at the top of the table.",
            "Don't worry, this is going to rise like a Phoenix to get very near the top of the table.",
            "And we'll talk about why it's not there right now, but here are some things that you might not have expected, so neural Nets right now at the top of the table, back trees in rainforests, music you may not have spent much time using them there also doing extremely well at the top of the table, and in fact would fold means is that for this column these things were statistically indistinguishable and bold here, and so three things were statistically indistinguishable for calibration, which I haven't spent that is yet.",
            "It turns out neural Nets are statistically better than these other things, and for the mean.",
            "Positive calibration it turns up.",
            "Enrollments are often also played good, but notice that the neural Nets aren't necessarily the best.",
            "It's not a clean sweep for y'all.",
            "Next.",
            "In fact, the best thing for Square tower is rainforest.",
            "By a little bit over bad trees in romance.",
            "And the best thing for cross entropy is bad trees followed by rainforest, followed by moments.",
            "It's just the fact that we are going across the city that is putting on its so tenuously in first place.",
            "OK, so some interesting things were not competitive.",
            "Probably the two that should really kick you in the face or boosting isn't doing very well.",
            "Boosting sure.",
            "Sounds like it's a high performing method and SPMS aren't doing very well.",
            "Well, why is that?",
            "Well, we've done something really stupid with SPMS, so all you did was, you know, as we entered the distance to the separating hyperplane.",
            "So that's a number of months.",
            "Plus Infinity and what we've done is we've just had a very stupid thing.",
            "We took the smallest value we ever saw, push that to zero.",
            "The largest value, push that to 1:00, and then just scale everything else linearly in between.",
            "And that's a really bad thing to do if you're interested in predicting probabilities, and we had to do something to squash things to 01 because things like squared R and loss.",
            "But you know, you have to have numbers look like probabilities to calculate loss.",
            "So so let's go in."
        ],
        [
            "Before before I talk about that, so everybody knows banking.",
            "So just in case, it sounds like more people probably know boosting and bagging, let me briefly remind you with padding is better than random parcel.",
            "Make more sense to bang is this incredibly simple brainchild of Leo Bryman.",
            "Here's the algorithm we're going to bag decision trees so we can buy anything.",
            "We take a bootstrap sample of the data that's just a sample with replacement.",
            "We train a model.",
            "We take another bootstrap sample of the same data.",
            "We train another model, do it again, do it again, do it's 100 times.",
            "Without trying to 100 models all on different bootstrap samples from the very same training set, and now you just take the average prediction of the summer models.",
            "Nothing could be simpler.",
            "OK, this is the kind of code that you know when you write a shell script.",
            "It's this long.",
            "It takes me 10 minutes to get it right and it can be run in parallel.",
            "If you've got mobile processors, it's just amazingly simple thing.",
            "It really works very well.",
            "So so it's a.",
            "It's a good algorithm and then random forests is going to work even better."
        ],
        [
            "So we just give you an example of of bagging.",
            "So here we got iterations of bagging.",
            "So we're going out for 100 iterations.",
            "This is the first tree we train, and this is accuracy.",
            "OK, so this isn't normal scores, but up is good and this is a problem particle physics prediction.",
            "So here's the performance of a single trade.",
            "OK, so you know just below 6, nine, 5%.",
            "That's the performance of a single tree has a very smart symmetry.",
            "Actually we did a lot of searching.",
            "Best single tree.",
            "Here's what happens if you train that tree and a second tree and thrown together in a bag.",
            "Suddenly your performance goes up by about point along that added Third Tree goes up quite another .01, but it goes up at a four 3/5, three my talking about 2025 trees.",
            "You sort of, you know, with with 20% of the work we've got 80% of the benefit that the usual 2080 rule, but you can see the curve is sort of still noisy.",
            "We going up by time.",
            "We get to say 100.",
            "You sort of run out of steam.",
            "Rarely do you need to do more than 100 of these.",
            "Performance increase, right?",
            "We've got at least a .03 increase in accuracy here just by doing them.",
            "Examples of the data, training multiple models and taking the average.",
            "And this works well.",
            "We can value neural net chicken bag SPMS, but there are some method works better with in some works.",
            "Worse, but it's just a very flexible thing.",
            "The neat thing about banking, by the way, is it almost never goes bad.",
            "It very rarely falls down gives you worse performance than if you will not pass that model, and that's not true of other things such as boosting.",
            "So boosting Canon does overfit one out of four times you use boosting, you shouldn't use it.",
            "That's not true."
        ],
        [
            "Ready for some more recent idea from from Leo Bryman?",
            "I mean, the truth is.",
            "You know why you came out with bagging in the early 90s?",
            "Boosting sort of hit the scene also.",
            "In the early 90s, I mean the original theoretical work was done in 89 and 90, but it didn't empirically start start getting anybody's interest in social 9394, and boosting is so cool, it just sort of, you know.",
            "Took over, I mean not too many people paid attention to bang later was justifiably pissed off at this and decided he would try to come up with something even better because he didn't believe boosting was working for the reasons they said they thought it was working and he was partly right about that.",
            "And he thought that some of the analysis of why bagging work was too simplistic and he was right about that too.",
            "So he came up with this thing called rainforest which is just sort of take bagging and make it even more stupid.",
            "And that's what random forests are.",
            "And it works even better.",
            "So so that that tells you this genius network, right?",
            "You take something that's pretty good.",
            "You make it even Dumber.",
            "Enforce.",
            "It works better.",
            "So, so random forests.",
            "One thing that random force you can bag anything but random forest is now custom designed to trees.",
            "OK, so you really you really have to do forests.",
            "And here's how we're aiming for us is we draw Bootstrap sample data just like in bagging.",
            "And now what we do is we have to decide what route tests put in the tree.",
            "We draw a small sample of the available attributes, maybe have 100 attributes.",
            "We draw sample 10.",
            "Now you're only allowed to pick the root test from those 10.",
            "You figure out which of those 10 is best put it.",
            "Now you have to decide.",
            "Then there's two branches.",
            "Now you have to decide what tests to put in each of those branches.",
            "When you get to each of those tests that you have to install, you draw a new random sample from the available attributes you 10 out of 100, and then you just get to pick the best from that, so it really.",
            "It really limits what the tree is allowed to look at in terms of the attributes, and he often gets great performance with this with very small samples from the attributes like 3 out of 100, things like that.",
            "It's amazing that this works so well because you would think that the trees just couldn't possibly become good models.",
            "What this does is the nice thing is if the tree is forced to install an attribute that doesn't help it very much.",
            "It still has a chance.",
            "As it goes further down the tree to see attributes that are really needed to use so it sort of keeps getting 2nd and 3rd and 4th and 5th chances it goes further down the tree.",
            "The only problem it has it's the sort of keep running out of data data keeps getting recursively partitioned into smaller and smaller groups.",
            "So the beauty of this and then turns out that's critical as well for the algorithm to work.",
            "The amazing thing is that what it does is it tends to increase the variance of the trees without very much hurting their performance.",
            "And that's exactly what banking is better.",
            "Averaging over many models reduces variance and as long as the average expected performance of those models is good that you can do amazing things.",
            "So it tends to do that.",
            "So basically you can think of banging is being if you're familiar everybody, hopefully with the bias variance decomposition, right?",
            "So?",
            "So bagging is doing is it's?",
            "It's reducing the bias of the trees by throwing an even more random stuff and making trees near producing the bias.",
            "It's doing much higher variance, but then what you do is you typically do maybe 1000 to 4000 iterations of random far, so you really you really hammer down the variance that result from the process, and it works extremely well."
        ],
        [
            "OK, so that's what random forests are.",
            "Let me tell you what these calibration diagrams are.",
            "So this is the third column in the table calibration.",
            "Suppose the weather forecast today was a rain with probability .5, right?",
            "In fact, I if I've got it right in May or June it rains 50% of the time here.",
            "It's actually a little better than Pittsburgh.",
            "Which were like 20 or 30.",
            "OK, So what we mean by a weather forecaster being well calibrated?",
            "Well suppose suppose there's 100 days in the year where the weather forecaster has predicted the probability of rain is .4.",
            "You would say there well calibrated when they predict .4 if on 40 out of those 100 days of rain.",
            "Right, that's what it means for probability to be well calibrated.",
            "That means if you say probability P. Then over many trials, sort of frequentist POV the adventure occur about the fraction of the time, right?",
            "So so that's what it means to be well calibrated.",
            "Well, how do we?",
            "How do we measure that for a model?",
            "Well, these are things known as reliability diagram.",
            "So what we're going to do is your model makes a bunch of different predictions were going to take all the predictions your model makes between point 3.4, and we're going to put in a bid.",
            "So I think this is being like a histogram bin.",
            "We put them in that pig and then what we do is we plot on the graph.",
            "We plot the mean predictive value that then it will be something like .35 probably.",
            "So we plot the mean predicted value of .35 and then we for all the items that fell in that bin.",
            "All those test points.",
            "We know what the observed rate is as well.",
            "How often did it actually rain on this 100 days?",
            "And if it turns out that the mean predicted value is comparable to the fraction of positives, then you're well calibrated in that bid.",
            "Right, that sort of makes sense, and if you for all bins full along this diagonal, then you must be well calibrated everywhere.",
            "That means you're you're been, that's predicting the real low P values.",
            "0.1 is also well calibrated.",
            "Those things are happening somewhere around .05% of the time, so that would be an ideal diagram.",
            "Would sort of go straight along the diagonal.",
            "That mean your model was extremely well calibrated no matter where it made predictions.",
            "This model is pretty well calibrated, it's predicting.",
            "A little off here, crossing a little low here, it tends to follow the diagonal so it's not too bad.",
            "What does it mean to be predicting this?",
            "So here we've got a point.",
            "This mean predicted value is, say, .015.",
            "But the observed rate is .2, so that means it's under predicted is predicting things like .015.",
            "I'm sorry .15 when the Observer 8.2 so it's not predicting this high values and should in this part of space exact opposite is happening over there is predicting a somewhat too high value over there and the observed rate is less OK.",
            "So so the number we're using in our tables is actually sort of is a measure of the average absolute deviation of these diagrams from the diagonal.",
            "Turns out that's not a great way to summarize these diagrams, which you really should do that's not robustly area in the RC curve is robust.",
            "It turns out you really should do is.",
            "You should look at the diagram.",
            "The diagram tells you what will look at a bunch of these diagrams, but we are going to summarize these numbers.",
            "Take those that last column where we use a single number to summarize these diagrams.",
            "Take that with a little green assault.",
            "OK, so let's."
        ],
        [
            "Get back to these results and remember take this column with a grain of salt and if you take that come with a grain of salt.",
            "It means these other things are just misguided neural network problem.",
            "Right, so it's wrong that they do.",
            "Neural Nets are so commandingly in first place.",
            "Let's talk about SBS.",
            "Remember we did something really stupid.",
            "We scale them to 0 to one so."
        ],
        [
            "Now let's look at a reliability diagram.",
            "For SPMS, this is on just six of our problems.",
            "Remember, if you want your reliability diagram to look like the diagonal line, that would be good calibration.",
            "None of these look like the diagonal line.",
            "In fact, if you're familiar with that, they all look like the sigmoid function.",
            "Classic sigmoid shape.",
            "So these things are very distorted, we just squashed them linearly to zero to 1.",
            "Yes, we're fixing their sort of range.",
            "But we're doing nothing to fix this distortion that comes out of tradition, and all these SPN so far have been trained with just standard hinge loss.",
            "OK, so this is just the usual way that you would we train in SVM.",
            "If you react for accuracy as opposed to Alpha squared up and you can see we've got a big problem here that we need to fix and that's why this car is so so poor you wouldn't expect."
        ],
        [
            "Well, this isn't something that we recognize that John Platt recognizes, back in 98 were very nice paper in 99 method, which we're now calling class method under John's work, and basically what you do is you plot this diagram.",
            "You fit a sigmoid to the data and you use that signaling to undo the transformation.",
            "So you just invert the sigmoid and now you get prediction that should be essentially straight line.",
            "If you can do that, well, John's method is more sophisticated than that, but that's essentially what it is.",
            "I mean, it's more sophisticated in the sense that.",
            "There's ways to do cross validation.",
            "Use out of samples to to do the fitting and things like that, but that's essentially what it is and this this works very well as we're going to see first of all, notice it's a good fit, right?",
            "Here's a sigmoid fit to this data.",
            "It's a pretty good fit today."
        ],
        [
            "So.",
            "Alright, so let's apply pipe scaling to SPN's that have been trained with hinge loss and all the sudden there now type the 1st place right?",
            "These are both bold numbers.",
            "This is the old performance way down here.",
            "That's what happens as soon as we correct for distortion of your predictions.",
            "And I mean look at this is actually better than the neural net by tiny bit.",
            "That's better than the neural net.",
            "By very tiny bit.",
            "This is a little bit versions calibration number, but remember that's the number where least confident in.",
            "So to this, and it's already time to me anyway, so this essentially says that SPMS are just as good at predicting probabilities, as long as you correct for the characteristic distortion that you're going to get if you use hinge loss to do the usual Max margin training of the SBM.",
            "OK, so so that's great news.",
            "It says that the algorithm that you've been told is really good actually is really good as long as if you want probabilities you do something extra step in order to transform it."
        ],
        [
            "So now let's pull back and then we've just been looking at these three probability measures, so, so here's the three probability measures in these last three columns.",
            "This is now the mean across all nine metrics to remember.",
            "We've got accuracy aswell if threshold metrics.",
            "We've got the ordering metrics here.",
            "So now I've got the performance on all these metrics.",
            "And again, bold means that you've done best in that column for indistinguishable from best.",
            "So Interestingly, neural Nets didn't have to be in the 1st place, because now we're taking an average over 9 metrics, not just those three.",
            "I mean, I think everybody who uses neural Nets would have thought that they would do extremely well on squared or I mean and log loss is typically how you optimize.",
            "The neural net is to minimize log, also squared error, and then they probably pretty well calibrated because of that.",
            "But it's interesting that they're also doing quite well on all these other measures, and in fact they do happen to sort to be in first place and that is after SPM's have been corrected with flat flat scale.",
            "So so after that correction has been plot but take.",
            "This bowl very seriously.",
            "I mean, there's now a bunch of different numbers in each of these cells, and this is the average of nine different numbers.",
            "No, immediately, all these performance measures are populated with each other, so these aren't independent readings of performance here, and this is the average over a lot of things.",
            "That's all bowl all.",
            "Four of those methods are essentially in four way tie for first place.",
            "So we really can't distinguish them, so so SPMS batteries in rain before.",
            "So all just doing extremely well.",
            "OK, the only thing that's been pot scale though is is SPS.",
            "Now something interesting.",
            "Look at boost boosted decision trees.",
            "Bold, bold, bold, bold, bold, bold.",
            "And then lousy performance predicting probabilities.",
            "So in fact, if you look at these numbers 861, well that's the best accuracy.",
            "85 or less.",
            "The best score 95600 it's close to being the best left 977 tie for first place in our city.",
            "958 well by her is the best average precision 952.",
            "It's the best right?",
            "It's doing.",
            "Boosted trees are doing incredibly well on these other measures.",
            "The reason why their average performance is so low in the tables, because they're just doing a terrible job.",
            "Probabilities what we've seen that before right SPMS were doing so well.",
            "So so.",
            "Of course we're going to play a similar game for boosted trees.",
            "Let me just summarize that table in the following way.",
            "So here what I've done is I've just taken for every problem.",
            "Here's the state of our problems for each problem and measure.",
            "I just sort of figured out what learning method gave the best performance.",
            "So this means that boosted stumps invest there.",
            "This means neural net."
        ],
        [
            "The best there.",
            "This means boosted decision trees request there.",
            "This means kainer Shaver digest there, right?",
            "So so that's just the table telling us for each problem metric.",
            "What happened to be best?",
            "This is an awkward table, right?",
            "This is possible thing that was second best was you know.",
            "Second only the 5th decimal place somebody wins so we."
        ],
        [
            "So here's a summary so you can see that last column.",
            "Hopefully Allmusic color very much anymore.",
            "Here's a count of how often these different methods came.",
            "Invest in that previous table.",
            "There are less pain.",
            "Invest 17 times right?",
            "That's pretty pretty good performance.",
            "Who's the trees came in best 19.",
            "That's more than a sandwich.",
            "Amazing about that is basically we don't think boosted trees can even compete in this part of the table.",
            "They only get to compete in 2/3 of the table because they're doing a bad thing over here.",
            "Just like SPMS, we're doing a bad thing, so the fact that boosted decision trees are doing so well, 19 count over here is amazing, because they only get to compete on, say, 2/3 of the problem.",
            "So so that means they're really doing extreme."
        ],
        [
            "Well.",
            "So let's let's think about that for saying so.",
            "So first of all, it's gotta be clear that you can have a method that's incredibly smart.",
            "Say boosted trees for SPMS, and that method can still predict error probability.",
            "Predicting good probabilities is something different.",
            "It's more demanding than, say, predicting a good RC or having good accuracy it requires.",
            "Yet this other thing which is like calibration.",
            "So that's an important thing to."
        ],
        [
            "So boosting seems like everybody knows that boosting, so I'm not going to.",
            "I'm not going.",
            "Once we explain it, maybe you could say sure.",
            "So, so boosting is really hard for materials is very, very pretty.",
            "So in boosting what we're going to do is on the training set, we train a model on me.",
            "We say that model and we look at where that model makes errors on me on the training set.",
            "And here's the counter intuitive thing.",
            "The next time we train another model, we put more weight on the cases where the first model made mistakes and less weight on the case is it got right?",
            "So it's kind of like taking a dog and sort of pushing its nose into the places where it's having the most trouble, right?",
            "You're taking the model your steering it to those cases that it is not yet getting right and focusing the attention of the model on the case is that it continues to not yet get right and eventually.",
            "If the learner is powerful enough and if the training data is consistent, eventually it will make that your predictions on those points as well, and then the cool thing is so you keep doing this so you train a series of models.",
            "So this is a sort of inherently sequential process.",
            "You also keep every time you do this, you keep a prescribed weight.",
            "It's based on the accuracy of the model and when you combine all these models votes in the end, you wait them by that term, which is effectively directors.",
            "So, so all the models house not like bagging, where you just sort of training all the things independently and then just make a simple average of them.",
            "Now you have the serial process where you keep focusing the attention of the model on places that it keeps getting wrong and we have a very carefully defined set of weights that tell us how to combine those those models.",
            "In the end very interesting technique, but it has some wonderful properties and one interpretation is that it is effectively it's like a quasi maximum margin method.",
            "It's not quite national margin, but it's close, so is that.",
            "So I'll say the math math problem would be constructed in two minutes in 1/2 an hour.",
            "The math is very nice."
        ],
        [
            "OK, so why is boosting not well calibrated?",
            "Why is it doing poorly predicting probabilities?",
            "Well, let me show you what boosting does.",
            "Here's 17 boosting, so that's a no boosting.",
            "That's the first model train.",
            "Here's four steps.",
            "830, Two, 120, eighteen, 1024 solution.",
            "So this doesn't do more boosting, and I've just picked up the problems here.",
            "Now what is this document?",
            "This is just a histogram of the probabilities that come out of our model.",
            "OK, so so you know, here's.",
            "This model is predicting a number of things down to 0.",
            "Fewer things near point 2.4.",
            "Even less one 6.8 and more things you want, so it's just a histogram of the predictions.",
            "Come out of your model adanih test, not on trains.",
            "Now we do a little boosting.",
            "Notice that the probability.",
            "There's a lot of probability mass and two tails.",
            "Notice it's got a little flatter.",
            "We do more boosting.",
            "It's actually getting a little peek in the center and the little loss of mass there.",
            "And maybe that's going down to.",
            "And then we do more stages of boosting, and sure enough, there's even less mass there and there, and almost all the probabilities getting pushed to the center.",
            "And that keeps happening and happening OK, so so we're losing the ability to make predictions that are very close to close to 0 or close to one.",
            "We're making far more predictions between sort of .3 and .7 or so.",
            "So that's what boosting is doing as we go.",
            "Let's look at the reliability diagrams as that process happens.",
            "So here's a reliable that diagram.",
            "Actually saw before.",
            "OK, so that's one that's no boosting.",
            "You haven't done anything yet.",
            "You've done this.",
            "This one actually looks pretty good, right?",
            "It doesn't?",
            "It's not up and below here now.",
            "It's just roughly along the diagonal.",
            "Well, that's interesting.",
            "We've overshot now we're below and above below even more in above, even more below even more above and below a lot above a lot.",
            "And hopefully these things look exactly like the SPM reliability diagrams that assault, they are sigmoidal shape.",
            "So boosting yields a distortion in its probability prediction histogram.",
            "That's very much like the kind of thing you can get it finished loss in SPMS.",
            "So of course we should try using Platt's method, which works so well in SPMS.",
            "We should try that on Bruce."
        ],
        [
            "This is consisted by the way for those of you who know the theory of boosting this is.",
            "I mean, this is something we observed, but the truth is the theoreticians already expected this to happen.",
            "They have good explanations for it.",
            "My my favorite explanation actually is Leo Breiman's explanation, which is that boost is an equalizer.",
            "Basically fix the things that keeps getting right and sort of keeps hurting those things and takes the things that keeps getting wrong.",
            "He's trying to help those things so that roughly speaking it has the same error rate across all cases.",
            "That's not exactly that.",
            "Right, and that would explain it.",
            "Turns out why you get that signal shape curve is not quite imagine margin method, but there's certainly a strong flavor of maximum margin optimization going on boosting, and that's really probably with uses the sigmoid curves.",
            "I mean SPMS are maximal margin methods, boosting he's causing maximum margin, so so that's why I feel like that and then the statisticians in Stanford.",
            "I mean they basically said you know it's trying to fit the logic of the model, so of course you're getting sigmoid.",
            "So anyway, I just wanted to put this up because I think even though we're the first ones to sort of empirically look at what happens with boosting long data set I I just wanted to give credit to these other people who really knew that this was coming."
        ],
        [
            "OK, so let me just show you what had to be applied class method.",
            "So here's I think it's just 7 problems across the top, so that might be the problem we looked at before we had 1020 stages of boosting.",
            "So each of these have been heavily boosted.",
            "Is the sigmoid curves you get OK with all the boosting?",
            "And then if you apply flat scaling to those models, these are the reliability diagrams which you get.",
            "Notice these reliability diagrams, essentially hunger, diagonal line.",
            "They're really good.",
            "Reliability diagrams, and sure enough, we've got predictions that are backing the tails of the distribution again, except maybe in this problem.",
            "It turns out from what we know about particle physics, problem probably shouldn't be anything in those tails, so we have some sort of side information to verify.",
            "The impact is probably correct not to have put predictions back in the scales here, and you can see it's it's actually pretty good.",
            "I'm so so that histogram of predictions can be too long without dying.",
            "Would be some nice.",
            "OK, so plants method is doing.",
            "Very nice job of taking this weird thing that boosting does pushing the probabilities all towards the middle and away from the two tails.",
            "It does a great job of doing that.",
            "Now let's"
        ],
        [
            "Apply that and look these probability metrics.",
            "All of a sudden boost is in first place.",
            "So neural Nets are longer in first place.",
            "And boosting is commanding first box, right?",
            "That's not.",
            "I wouldn't call that a 1, two or three way tie for first one, which I don't want.",
            "That is, I would call that a 2, three or four way tie for first place.",
            "Boosting really is in first place and it's only on this calibration measure that anything is hiding these other two measures is just clearly in the lead here.",
            "And just to let you know what we've done, here is.",
            "You know first we did class going on this weekend and we did classifying in SPMS and boosting.",
            "I'm not going to sort of walk through every learning method and do that.",
            "So we did.",
            "We just pass scaled everything, every model we trained with Pascal.",
            "Did we use the validation set to figure out whether it was better to plot scale for to not like scale and a flat scaling help?",
            "That's what we used if it hurt we didn't use it.",
            "Help something to help boosting.",
            "SPS, they help him for us a little bit.",
            "Help boosted stumps alot.",
            "It helped a plain decision.",
            "Trees and help my phase.",
            "Of course.",
            "90 days older couple can get pretty problems right?",
            "So it's it's the craziest probability predicted.",
            "The risk it didn't help the things that other one didn't help their own.",
            "Expat trees, painters, neighbor or logistic regression.",
            "Be careful here.",
            "So cater statement.",
            "This does not mean every flavor parameter setting that you put down for 10 years neighborhoods.",
            "Well calibrated models.",
            "That's definitely not true.",
            "What this means is that for each problem there was a setting that yielded pretty good calibration.",
            "OK, there was a parameter setting that work for you.",
            "OK, so it helps some things and it suddenly put put boosting really in first place, so so that's kind of fun."
        ],
        [
            "Now let's go back to all the other measures.",
            "Remember, boosting is already doing incredibly well in these six.",
            "It was just doing very poorly in those three.",
            "It now really is commandingly first place, so these are huge differences.",
            "Let me give you an idea.",
            "Yeah, these normalized scores that were using can be misleading.",
            "Remember, baseline is 0 on this scale, so suppose baseline in your problem is a 50% accuracy right?",
            "It's a problem, it's 5050 positives in there, so everybody should be able to achieve 50% accuracy.",
            "Let's say you're achieving.",
            "95% accuracy with the smart learning method on that problem.",
            "Well, what's your normalized score gonna be if it's a problem for what you actually think you put it you 100% accuracy, which you can.",
            "Those problems were normalized worldly .9 because you've gone from zero at 50% accuracy, right?",
            "You've gone 9/10 of the way up to 100% so so put you at .9.",
            "So what's the difference of point 92.9 one near me?",
            "Right, so so it means that your accuracy has gone from 95% to 195.5%.",
            "To get that right.",
            "Yeah, so so difference here at .01.",
            "Music your accuracy has increased .05%.",
            "Right, which is which is.",
            "I guess you know.",
            "10% production loss but.",
            "So sometimes what look like big differences here.",
            "If you go back and you think in the original scales can be can be a rather small thing.",
            "So so just keep that in mind.",
            "But still will be.",
            "God is essentially boosting in first place and now four methods, random forests, neural Nets, SPMS in bag trees, two of which need or benefit from this calibration plots method, and two of which don't all sort of ties for 2nd place.",
            "That really is a type of 2nd place.",
            "Don't take the order of this too seriously.",
            "I'll give you some information later to make it clear that you really should ignore that order.",
            "OK, so that's that's kind of interesting.",
            "What's interesting?",
            "I mean, I think decision trees really hot in the 80s, but time Euronext came around.",
            "People were tired of them.",
            "They're ready to move on, something more coolness.",
            "We're definitely all.",
            "And then I think we're all sort of ready to senior.",
            "You know, decision trees get put in history books and just be things that you use when you need to understand your model.",
            "Show model to an expert.",
            "Maybe then you want to use the decision tree.",
            "The cool thing is not that boosted decision trees are intelligible, but humble decision trees which have a lot of nice properties that handle missing values.",
            "They are easy to grow fast, they can handle any kind of activity type without any careful manipulation.",
            "I mean, they're they're pretty impressive models in terms of ease of use.",
            "It's really interesting to see that things like banking and boosting have sort of put them back on the top of the map, so so, so that's fine."
        ],
        [
            "Now I've been using pipes method.",
            "Turns out there's lots of different ways of achieving calibration.",
            "Patch method is just one of those ways, so let's talk a little bit about these other things.",
            "Well, first of all, you can boost a log loss.",
            "Instead of the way we normally do boosting in eight groups so so you can actually do boosting that aims are predicting probabilities directly.",
            "Similarly, you can train SVM is to maximize the likelihood instead of using hinge loss.",
            "And now you'll also be essentially optimizing toward well, so there are ways of training things like boosting SVM.",
            "These magical margin methods so that you actually are optimizing loss directly.",
            "The interesting thing is, it doesn't work so well in practice, so which is kind of unexpected.",
            "We we thought that that would work very well.",
            "It turns out all is going to play with my share similar results.",
            "There are other ways to do it too.",
            "There's a logistic correction.",
            "This comes from the guys at Stanford.",
            "Basically, they said, hey.",
            "You rock Washington logic of something.",
            "Then you should just use analytic formula to undo the logic and you'll get back the right thing.",
            "It turns out that doesn't work very well in practice.",
            "There's touch scaling, which we already used.",
            "Another spacetime regression model which I'm going to talk about 'cause you might actually want to use this instead of plat scaling to depending on how much data."
        ],
        [
            "So.",
            "Going to show you boosting with log loss, so here's boosted stumps.",
            "Optimize the log loss.",
            "And here's boosted stumps that have been optimized the normal way, but then you plant scale right?",
            "So, so here you optimize the wrong thing and then you try to fix it after the fact and he tried to optimize to the right thing.",
            "And I'm talking to the right thing.",
            "Doesn't work as well and we think in this case this is still.",
            "It.",
            "Turns out a lot of benefit.",
            "Report we think one of the reasons why this happens here is when you're doing this plot scaling method, you're automatically using certain health side validation data to do the fitting, so you're much less likely to overfit, and it turns out boosting is very prone to everything.",
            "So it's even more prone to overfitting if you try to do boosting to log loss.",
            "It turns out that makes boosting even more powerful.",
            "It overfits even more and it really just hangs itself.",
            "So this sort of not letting it hang itself and then doing something afterwards.",
            "That also isn't going to hang itself turns out to be a better procedure, so crossing that point out the case, if you had a million training points here.",
            "But it's certainly the case with small sample.",
            "A similar thing for boosted trees cares, boosting poultries with by correction at the top of the table.",
            "Here's boosted full trees that have been optimized.",
            "Log loss taken very, very poorly compared to that, and here it really is overfitting.",
            "It turns out that basically if the underlying model classes too powerful, you just end up getting nearly perfect prediction on every test set.",
            "Every training set that you get when you almost blood loss and it thinks this goes S right?",
            "So so anyway, I just wanted to point out I mean.",
            "I don't want you to leave the talk and say class failing is interesting.",
            "Isotonic regression is interesting, but if they just done the right thing you wouldn't have to do any of this jump.",
            "It turns out if you do the quote right thing, it doesn't doesn't always work with this."
        ],
        [
            "Let me see you there."
        ],
        [
            "Algorithm.",
            "Let me just tell you what we're doing.",
            "What is logistic correction Plattsmouth?",
            "Basically we're looking for a logistic function that's a monotonically rising smooth function that shapes kind of like that, so that's what we're doing with classmethod.",
            "Isotonic regression is more powerful.",
            "Isotonic regression says give me any monotonically increasing function.",
            "I don't care what the shape is.",
            "Give me any monotonically increasing function, and if you can find the function that will actually give me the optimal squared error on some training center.",
            "So it's a much more powerful method, is not restricted to sigmoid shaped curves.",
            "That means it can really perhaps do better things when sigmoid shape correction is beyond your correction, so it's a much more powerful thing, But then you have to worry that maybe it'll be too powerful and overfit, and what will see sometimes it does, and sometimes it doesn't.",
            "So here's a here's a bunch of problems before some of our problems.",
            "This is the sort of fitting isotonic curve if you squint so that you don't see those little edges.",
            "It might look kinda like a sigmoid curve.",
            "But it actually is sort of a stair step.",
            "You could think of.",
            "Isotonic regression is being like an optimal bidding of the data so that the number of violations between adjacent bins is minimized.",
            "So that's that's one way to think about it.",
            "And here's the beautiful reliability diagrams that you get after doing this.",
            "Time progression on those things is really very, very nice.",
            "It's still new upgrade."
        ],
        [
            "So here's a comparison.",
            "Here's Platt method on those problems.",
            "There's a lot of older donors.",
            "Here's isotonic regression on those methods.",
            "Reliability buggers.",
            "They're both very, very very good.",
            "So now you might think, well, we should use.",
            "By the way, this time of regression is nice.",
            "It's a linear time algorithm.",
            "Well, isn't isn't in log in factor in there, but it's actually a very efficient algorithm.",
            "It's easy to code, and it is awful for square, so so it hasn't really nice properties, so we should.",
            "We don't get the credit for this.",
            "Iconic regression was invented by statisticians.",
            "75 years ago and the people who first recognized it could be used for counting or Charles opening beyond.",
            "This is addressing social sort of follow up."
        ],
        [
            "Here's some years boosted decision trees.",
            "SBS neural Nets in rainforest and this is squared error, so I'm no longer looking these normal scores, so down is good in these graphs.",
            "This is for boosted trees.",
            "That's the square you get.",
            "It can do no calibration.",
            "It's not a very good squared error.",
            "This is the squared error.",
            "We're varying amount of data we're using to do the calibration here, so this is a log scale, so this is 128 points used to do calibration.",
            "There's 1024 used to do calibration out about 8000 points used in calibration.",
            "So what you see is here is isotonic regression, so it really doesn't do very well if you have a small data set.",
            "But it does better, better, better, better.",
            "Better.",
            "In fact, it does extremely well if you give it a large data set to do corrections.",
            "This green line it's hard to see that plasma, and I think what you're seeing all these graphs is if your data poor pass method dominates.",
            "I make sense.",
            "It's the simpler, smoother fitting function.",
            "It can't overfit to the data so easily, and sequences are reasonable fitting function, so it actually does pretty well when you don't have much data, but you can see there's a crossing point.",
            "And once you pass that crossing point, possible to structure like so, it's like behind a little bit there right behind there and left behind there.",
            "Once you have enough data is better to use more powerful method.",
            "So, so that's the moral of the story, and it's kind of fun.",
            "You can see you get a lot of improvement on boosted trees by doing any form calibration with SVN is in fact you get so much benefit you can't see this horizontal line for the estimates on calories off the top of the chart.",
            "There random forest.",
            "They're already pretty well calibrated, but they do benefit from calibration.",
            "Here's neural Nets.",
            "Neural Nets are incredibly well calibrated to begin with, and it turns out only if you got tons of data to do the calibration.",
            "Slightly better than you.",
            "Well, now that's the way we're training them.",
            "Have a signal unit in their output layers so so they're already doing something like Classmethod internal automatically.",
            "That's the standard way.",
            "So, OK, so the moral of the story basically is.",
            "Both methods work well if you have lots of data, it's better to use more powerful isotonic regression method if you don't have much data, though, then it's better to use the the smoother simpler plasma fittings."
        ],
        [
            "Yeah, another so we don't.",
            "This is by the time you do.",
            "Calibration.",
            "You've already trained your models on all the attributes and now the calibration step knows nothing about the number of attributes because all this is doing is taking a prediction that comes from your model number, like .6.",
            "And it's looking at the other predictions your model has made and is looking at the true labels from those things.",
            "Zeros and ones.",
            "And it's just finding a way to correct those predictions.",
            "So at this point it doesn't know anything about it, but there isn't a deeper, more interesting question which I think they're looking to, which is.",
            "The more attributes, the more likely the underlying model is overfit.",
            "And then the more likely the underlying model is overfit.",
            "Perhaps the harder it would be for any of these calibration methods to undo that overfitting, and I think that's probably true, but we haven't examined it carefully, so that's interesting.",
            "OK, I'm going to skip this summary slide.",
            "It doesn't say anything you don't."
        ],
        [
            "No.",
            "So calibration works break and it takes 2 of what we thought were our favorite methods and based on our favorite methods again, so it takes boosted trees, an SVM and put some essentially in the top.",
            "It also helps random forests and neural Nets were already doing pretty well.",
            "Thanks for everything pretty well.",
            "So maybe remember the big questions at the very beginning of the talk seems like those years ago, questions like, well, what learning method should you use?",
            "When can we ignore a bunch of these learning methods and we put a bunch of pastor?",
            "Maybe we should just include the trees, calibrate it and, um, you know, that's like the supermarket."
        ],
        [
            "That's not the case, so I've got one more line this table, the top.",
            "Let me tell you what that line is.",
            "What we've done is for every problem.",
            "For every trial we just get to pick from all of thousands of models to be trained.",
            "We just get to pick whichever 1 looks best.",
            "You know, maybe I want trial.",
            "It looks like in years neighbor model may be on the next trial to boost trade next miles and SPM.",
            "Who knows why?",
            "So this line is sort of the best of the best of the best.",
            "It's it's we get to pick for every problem.",
            "What thing that looks like is performing best and the interesting thing is not that you know it does better than you expect it to.",
            "Never do any worse.",
            "Spend the best of these methods right 'cause it gets to pick the best way for the only way.",
            "2.9119 there.",
            "First program yes yes.",
            "So I was just going to say you only way you can do worse is if it overfits to the validation set that's being used to pick the best model.",
            "And in one case in the table just so.",
            "So I'm glad you so.",
            "But overall, you shouldn't expect it to do much worse and less.",
            "Your validation is just very noisy.",
            "Right, but here's the interesting thing is how much better it does.",
            "9533 versus 914 by distinguished trees for everything.",
            "No notice that the difference between these top five methods right is all the difference of about 1.04 at most less than .04, and we're getting another .04.",
            "Just by picking the best model in every one of these things, so it turns out if you dig deeper we looking here.",
            "There are some problems in some metrics for which logistic regression, which is terrible, does best.",
            "In fact, it's really annoying boosted stumps, so one of the interesting things of the study is you might have heard that it's better to boost weak models.",
            "Well, there's no evidence of that.",
            "Boosting full trees is easily outperforming boosting the weaker stumps so so we don't have any evidence.",
            "It turns out if you look at the details, though, there are two problems from boosted stumps easily outperform boosted full decision trees, and that's because the booster for decision trees massively overfits the data.",
            "Where is the boosted stumps can so so the fact that boosting has done so well up here even though it falls down one or two problems, it's interesting that means it really does well in the other problems that enough to compensate for the air, but sometimes so.",
            "But if you wanted the details, I mean everyone of these learning.",
            "As it turns out, is best on some problems.",
            "So you can't safely eliminate any of these things.",
            "I even humble old methods you know, like like a nearest neighbor or simple decision trees.",
            "They occasionally turn out to be just the right model or some problem in somebody that's really sort of disappointed, right?",
            "It's the worst case result.",
            "I mean, if you gotta just train one thing and get good results in hurry, go for boost decision trees do it will do very, very well.",
            "But if you really need the best performance and you have the time to do it.",
            "You really should try everything you can under the sun.",
            "You know, you know, get back to run everything you can think of in waka and pick one that seems best and you'll do better than if you pick whatever your favorite method is in just for that.",
            "So that's kind of a surprise."
        ],
        [
            "So.",
            "That's kind of disappointing.",
            "It says that anytime we want to do really well, we have to just sort of train everything.",
            "It's best can we do any better than that?",
            "I mean, if you're going to work training thousands of models in every problem, can you do something fancier?",
            "Well, well, yeah, so.",
            "While we try building ensemble out of these thousands of models, but some of these thousands of models are themselves in samples.",
            "But that's OK. You can do an ensemble of ensembles and decimal problem, so I mean all we need is that we have a bunch of models.",
            "Which are accurate and that they're different?",
            "And it turns out these learning methods are quite different from each other.",
            "Who's the trees make very different errors in neural Nets, so they're very diverse models, and it turns out a lot of them are very active, so maybe we can just build a smart on top of these things.",
            "I'm going to spend a few minutes going through a method we've got for doing that, and then I'm going to stop.",
            "I'm going to point out a problem that and then I'll just speak of Popeye forbids, because, OK."
        ],
        [
            "So.",
            "Let me show you some existing ensemble methods that you might try using any people for stacking stacking stuff so popular these days.",
            "It's basically trying to build a model on top of the other models.",
            "There's just taking the simple average of all those models with picking the best of the best of the best, which is what we've done on the previous table and then paging model averaging, which basically says put more weight on the walls that appear to be better and less weight on.",
            "The models appear to be weaker but take an average of all of them.",
            "None of this works really invented, just picking the best single model and equation model averaging is ever so slightly better and we're working to make that go up, but but basically just picking the best single model, get you what you're going to get.",
            "So we were discouraged by that.",
            "'cause we know there's gotta be a way of taking average of these models and getting to performance so, so we've developed."
        ],
        [
            "Here's me, I'm just going to walk into the office.",
            "It's easier than reading text, so here's a bunch of balls and we've actually trained thousands of models."
        ],
        [
            "So we're trying to optimize the area under the RC curve, so we want this number to be as large as possible on some tests.",
            "So here we've got the RC of each of these models on that test set, and one of these is best, right?",
            "So that one has the highest RC, so we're going to greedy forward stepwise feature selection.",
            "But now we're doing greedy forward stepwise model selection, so we're going."
        ],
        [
            "Best modeling in sambol.",
            "Now there's just one model in the example.",
            "We're going to ask.",
            "Suppose you were to add this model to that model in the ensemble.",
            "What would be our FCB would be that if we add that all about the RCD could be that this model, it would be that so.",
            "So this is sort of just trial and error.",
            "See which of these things if we add it to Gonzalo making Ensemble performed.",
            "As it turns out, model line if you add it to Model 3 raise the performance even more or not got the best single model like 964.",
            "This raises it to 9384.",
            "Play the game again."
        ],
        [
            "We find something that raises an even more and we just keep doing it, so we keep greedily Hill climbing in the performance of the ensemble space.",
            "Cool thing about this is we can optimize any performance measure, any number you can compute fast enough we can optimize to it because it's just trial and error help, right?",
            "So you can do anything you don't need.",
            "You don't need differentiability or anything."
        ],
        [
            "This works extremely well.",
            "Let me show you three slots in the gun.",
            "It really does yield some extra performance that just best invest in the masturbation model out.",
            "You can't do so.",
            "9777 is a notch above those things.",
            "If you're going to go to the work of training all of these models to pick the thing that's best, that's a great thing to do.",
            "You should do that.",
            "Yes, you want to do a little more.",
            "Work hard to find an ensemble of these things and you can do a little better.",
            "And the cool thing is you optimize many metric by doing that, whereas we don't have training ronetta optimized RC.",
            "But we can optimize ensemble that uses the neural net inside document starts, so that's kind."
        ],
        [
            "Covers really well.",
            "It works even in the hands of other people who are doing other things, which is kind of nice."
        ],
        [
            "Just working."
        ],
        [
            "It's very expensive.",
            "But all the expenses training the thousands of models, it turns out the ensemble selection hillclimbing that I can do in a minute on the laptop.",
            "So so that's the easy part.",
            "As long as your metric can be computed quickly."
        ],
        [
            "I'm just going to tell you there is a problem.",
            "OK, these are as far as we know the best performing models we know how to train right now, and you know that shouldn't be a surprise wireless rumble.",
            "They're building on top of things.",
            "Boosted trees in SPMS in factories around for so, so.",
            "Of course they're good.",
            "You know they're standing on the shoulders of Giants, so that makes sense.",
            "They're very slow big."
        ],
        [
            "Bird models, one of these models has 20,000 decision trees.",
            "Another thousand decision Trees, 2200 hidden units, thousands of support vectors.",
            "Basically it takes a GB, destroyed this model and it takes a second or two to execute it.",
            "On one test case.",
            "So there are all sorts of problems for which you'll never going to be able to use this model.",
            "You never put in your PDA while not enough room while you're not going to be able to apply to Google scale problems, there are all sorts of problems."
        ],
        [
            "So one of the things we're working on is a way of compressing these models into a much simpler model of.",
            "A much more compact, faster model that still achieves the same performance as the ensemble, and I think if I'm giving those talk later in the week, that might be what I'll talk about his model compression.",
            "I'm here in this talk.",
            "Anyway, I wasn't planning to do it today, it's just a second.",
            "OK, so they're pushing this out so we could be around this place.",
            "Be my guest.",
            "Topic people, so unfortunately it doesn't this time I'm afraid so, but.",
            "Basically, you got basically put against for president discussions these days he rounds till Friday and then Saturday something good with some free stuff basically Friday.",
            "Casa at least we're talking, it's not."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What's kind of calculators should be used for certain problems and how to evaluate this is partly, so I saw that it is from from certain.",
                    "label": 0
                },
                {
                    "sent": "Somehow I connect this.",
                    "label": 0
                },
                {
                    "sent": "Text from the upset also to the work that you did love this Kitty car little bit, or with this some more intensive work.",
                    "label": 0
                },
                {
                    "sent": "So we organised together with Torsten your claims.",
                    "label": 0
                },
                {
                    "sent": "Two years ago I think.",
                    "label": 0
                },
                {
                    "sent": "2004 and so this was very interesting challenge I guess for the whole community through how to compare algorithms, ideas on couple of datasets.",
                    "label": 0
                },
                {
                    "sent": "So this will be also seem a little bit here.",
                    "label": 0
                },
                {
                    "sent": "So we have roughly whatever one hour and you have more less open discussion.",
                    "label": 0
                },
                {
                    "sent": "So if you have if you have any questions, I guess you can just drop it.",
                    "label": 0
                },
                {
                    "sent": "One more thing got it.",
                    "label": 0
                },
                {
                    "sent": "Research visit sponsored by Pascal.",
                    "label": 0
                },
                {
                    "sent": "Excellent.",
                    "label": 0
                },
                {
                    "sent": "This is one of several visits visits, which we will also feature so.",
                    "label": 0
                },
                {
                    "sent": "So thank you to Marco Engineer for inviting me to come in for Pascal for funding it, and especially to you for signature for about 45 minutes from here.",
                    "label": 0
                },
                {
                    "sent": "So sorry, I was so late.",
                    "label": 0
                },
                {
                    "sent": "So I got more slides and I'll be able to go through in the hour hour and a half at work to do this.",
                    "label": 0
                },
                {
                    "sent": "So feel free to stop in this questions and I can always break at different times during the talk, so this is joint work with some of my graduate students.",
                    "label": 1
                },
                {
                    "sent": "Allison give us to Christian Mercia and arguments and all identify something.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so how many people here already know machine learning?",
                    "label": 0
                },
                {
                    "sent": "OK, so pretty much everybody knows machine learning.",
                    "label": 0
                },
                {
                    "sent": "So more less at this almost everybody.",
                    "label": 0
                },
                {
                    "sent": "I think her course of being good.",
                    "label": 0
                },
                {
                    "sent": "Do you all know what supervised learning is?",
                    "label": 1
                },
                {
                    "sent": "Basically it's regression on steroids so you know how.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have a sort of sad state of affairs in machine learning and that such that it appears that in some sense we've been too successful.",
                    "label": 1
                },
                {
                    "sent": "So over the last 30 years we've developed, now count is about 35 different machine learning algorithms out there.",
                    "label": 1
                },
                {
                    "sent": "Most of them I'm sure you've heard of, so we've got things like regression K, nearest neighbor, Decision tree, support vector machines, graphical models, methods, things like boosting and bagging, and random forests, and then maybe some more things that you haven't played with death.",
                    "label": 0
                },
                {
                    "sent": "I'm an extra Gaussian processes yet, so we've got all these different learning methods.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I think.",
                    "label": 0
                },
                {
                    "sent": "My impression is that you guys already know what came nearest neighbor is right.",
                    "label": 0
                },
                {
                    "sent": "But how many people don't know what Kangaroos neighbors?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Windows 10 years neighbor everybody.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This decision trees that.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Everybody knows neural Nets.",
                    "label": 0
                },
                {
                    "sent": "How much neural net.",
                    "label": 0
                },
                {
                    "sent": "So do they still teach parents?",
                    "label": 0
                },
                {
                    "sent": "OK, you have a couple of interesting so so this talk will surprise you then, especially in the beginning, because neural Nets are going to feature heavily in it, so neural Nets of these things which take a vector of inputs.",
                    "label": 0
                },
                {
                    "sent": "They go through a bunch of connection rates to some hidden units that learner even presentation of the data and then through another set of ways to save the output unit you can have multiple hidden layers here and then.",
                    "label": 0
                },
                {
                    "sent": "The thing that you're predicting predicting on this applet unit, and we train them in the very simple procedure, just gradient descent backdrop is just a fancy name for efficient.",
                    "label": 0
                },
                {
                    "sent": "Set.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're going to see neural network show up later.",
                    "label": 0
                },
                {
                    "sent": "Neural networks.",
                    "label": 0
                },
                {
                    "sent": "One of the nice things about them is that they are very flexible models.",
                    "label": 0
                },
                {
                    "sent": "I can pick a large vector of inputs and as well a large vector and outputs.",
                    "label": 0
                },
                {
                    "sent": "Although we're not going to take any advantage in the talk today.",
                    "label": 0
                },
                {
                    "sent": "Um, this vector values.",
                    "label": 0
                },
                {
                    "sent": "We're just going to have one output.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A lot of my work has been in the past on training networks that have multiple outputs, so if you're interested in that, talk to me later in the week will be here.",
                    "label": 0
                },
                {
                    "sent": "Linear regression is just a special case.",
                    "label": 1
                },
                {
                    "sent": "It's a very simple neural network.",
                    "label": 0
                },
                {
                    "sent": "We have just the inputs.",
                    "label": 0
                },
                {
                    "sent": "Just these wings going to 1 unit.",
                    "label": 0
                },
                {
                    "sent": "That's the output unit and this is just a linear transfer function in this album unit.",
                    "label": 0
                },
                {
                    "sent": "So that's what linear regression is expression.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Network and then logistic regression.",
                    "label": 1
                },
                {
                    "sent": "If you're familiar with this progression, is also just a simple network where we've taken that out.",
                    "label": 0
                },
                {
                    "sent": "Looking at which used to be a linear unit, and now we replace it with the same point function so that that's logistic regression.",
                    "label": 0
                },
                {
                    "sent": "And then there are ways of regularising that and that gives you things like rich aggression.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So everybody must know support vector machines then right?",
                    "label": 0
                },
                {
                    "sent": "So so you learn the newer methods of machine learning, so of course in support vector machines we're trying to maximize this margin that separates the two classes in the binary classification.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we've got all these different learning methods all talk about some of these others, such as a bagging boost.",
                    "label": 0
                },
                {
                    "sent": "And random forests as we get further into talk, how many people already know value?",
                    "label": 0
                },
                {
                    "sent": "So everybody knows boosting nice soon random forests.",
                    "label": 0
                },
                {
                    "sent": "OK, good good.",
                    "label": 0
                },
                {
                    "sent": "So many of you know random parts as well, so I'll talk about them a little more later later in the talk.",
                    "label": 0
                },
                {
                    "sent": "Is there also going to turn out to be very competitive?",
                    "label": 0
                },
                {
                    "sent": "You got a problem though.",
                    "label": 0
                },
                {
                    "sent": "Not only do we have 30 odd message over here on the left, we have no prescription about when to use these things or when not to use them, what problems they're good for, what problems they're bad for, and it sounds like already you stop teaching some of them, such as neural Nets, because there are some.",
                    "label": 0
                },
                {
                    "sent": "How old are inferior to newer things like support vector machines.",
                    "label": 0
                },
                {
                    "sent": "We'll see if that's true.",
                    "label": 0
                },
                {
                    "sent": "There's an even bigger problem though, so each of these things you know I say support vector machines or decision trees as if that's just a single algorithm.",
                    "label": 0
                },
                {
                    "sent": "And of course that's not the case right?",
                    "label": 0
                },
                {
                    "sent": "With support vector machines, there's lots of different kernels that you could decide to use.",
                    "label": 0
                },
                {
                    "sent": "There are different losses that you can decide to train to write.",
                    "label": 0
                },
                {
                    "sent": "You don't just have to use hinge loss, you could use the one norm.",
                    "label": 0
                },
                {
                    "sent": "This log loss for SPMS, so SVN find themselves are really a whole family of algorithms, and in fact there are many different heuristic procedures for how to train these SPN.",
                    "label": 0
                },
                {
                    "sent": "The whole family advantage here.",
                    "label": 0
                },
                {
                    "sent": "Similarly, neural Nets in the hands of a master are very flexible, kind of play that can be made to represent many different things.",
                    "label": 0
                },
                {
                    "sent": "There are a full class of algorithms.",
                    "label": 0
                },
                {
                    "sent": "Decision trees are also a full class of algorithms.",
                    "label": 0
                },
                {
                    "sent": "Turns out there's a decent dozen different major ways of doing decision trees in anyone textbook.",
                    "label": 0
                },
                {
                    "sent": "You'll only learn about one of them.",
                    "label": 0
                },
                {
                    "sent": "Maybe they'll mention one or two of the others, but but this is also a whole family of trees, tree types, and similarly for K nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of different memory based learning techniques.",
                    "label": 0
                },
                {
                    "sent": "When we say painters neighbor, there really is just one hour and there is a whole family of algorithms that have been developed over the past 40 years.",
                    "label": 0
                },
                {
                    "sent": "So so, not only are the sort of 30 different major categories of algorithms, but each of these things has all these different flavors and parameters that you have to set.",
                    "label": 0
                },
                {
                    "sent": "So in some sense, there's actually hundreds of different algorithms, and we really don't have anywhere in a textbook printer paper.",
                    "label": 0
                },
                {
                    "sent": "This sort of rule which says, oh, if your data looks like this.",
                    "label": 0
                },
                {
                    "sent": "And if you're trying to optimize this, and this is what's important, you should use either this algorithm or this algorithm and ignore the other 200 right, so we don't have that sort of thing, so that's really a bit of a problem.",
                    "label": 0
                },
                {
                    "sent": "Is optimizing your tribe all these methods and then optimize each method to make it best in your problem?",
                    "label": 0
                },
                {
                    "sent": "That's a lot of work, so so we're actually going to do some of that here to see what happens, but we don't recommend that you do that unless you really, really need to squeeze out the last drop form.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So questions you might be interested in well.",
                    "label": 0
                },
                {
                    "sent": "Maybe one of these algorithms just dominates all the others, right?",
                    "label": 1
                },
                {
                    "sent": "Maybe support vector machines really are just the best thing you can ignore all these other things and just use them all the time.",
                    "label": 0
                },
                {
                    "sent": "That would be great to find out about this trip.",
                    "label": 0
                },
                {
                    "sent": "Or maybe it will turn out that something a little simpler than that is true.",
                    "label": 0
                },
                {
                    "sent": "Maybe it'll just turn out that you know you're doing regression using neural Nets.",
                    "label": 0
                },
                {
                    "sent": "If you're trying to do classification, especially binary classification, then use SPS.",
                    "label": 0
                },
                {
                    "sent": "Maybe it'll be something simple like that.",
                    "label": 0
                },
                {
                    "sent": "That would be fine.",
                    "label": 0
                },
                {
                    "sent": "If that's not true, maybe no one method is going to dominate.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's not gonna be.",
                    "label": 0
                },
                {
                    "sent": "The Testament is best for classification in general, but maybe at least a number of these methods won't be very good and we can sort of flipping the rest.",
                    "label": 1
                },
                {
                    "sent": "So, just as you're not learning neural Nets anymore, maybe it's safer not learn neural Nets anymore, because they're just not going to be competitive.",
                    "label": 0
                },
                {
                    "sent": "So be nice to know that neural Nets and K nearest neighbor where things you didn't have to learn about something dusty textbooks.",
                    "label": 0
                },
                {
                    "sent": "Well maybe it's gonna turn out that some methods are doing some loss, and yet that very same method would be bad on a different kind of loss.",
                    "label": 0
                },
                {
                    "sent": "That's sort of interesting just to be good on one loss, it means you in some sense have to be a smart algorithm.",
                    "label": 0
                },
                {
                    "sent": "And yet, when it comes to a different loss, you're not very good.",
                    "label": 0
                },
                {
                    "sent": "So you're doing something stupid on the very same problem.",
                    "label": 0
                },
                {
                    "sent": "What you're doing something smart.",
                    "label": 0
                },
                {
                    "sent": "It just depends on how you're being evaluated, so we'll talk a little bit about that.",
                    "label": 0
                },
                {
                    "sent": "And then if we have time, we'll talk about how different ways of measuring loss related to each other, and ultimately which loss might be more robust than other losses.",
                    "label": 0
                },
                {
                    "sent": "You know, there are now dozens of different kinds of losses out there that you see show up in papers, maybe one or two of these are better than the others, and you should just focus on those, because other ones are too high.",
                    "label": 0
                },
                {
                    "sent": "Parents, for example.",
                    "label": 0
                },
                {
                    "sent": "Optimized ultimately though, what it all comes down to is what should you use?",
                    "label": 1
                },
                {
                    "sent": "You got a problem.",
                    "label": 0
                },
                {
                    "sent": "What learning as it should you use?",
                    "label": 0
                },
                {
                    "sent": "What flavor that learning method should you use?",
                    "label": 0
                },
                {
                    "sent": "And maybe what loss function should you so those are the kind of questions but remains.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm going to present.",
                    "label": 0
                },
                {
                    "sent": "This is an empirical talk, so so it's a bunch of results of experiments that we've run.",
                    "label": 0
                },
                {
                    "sent": "It's not a theoretical analysis of these algorithms at all, so we're now up to this is older now.",
                    "label": 0
                },
                {
                    "sent": "211 datasets that fairly expensive experiment, so you'll see why we don't have 50 days since here and most of the results in a presenter going to be on these eight datasets and half of these were taken from UC Irvine repository.",
                    "label": 0
                },
                {
                    "sent": "I've abused that repository just like everybody else but the other half are real problems, for which I have real collaborators.",
                    "label": 0
                },
                {
                    "sent": "In some sense I have customers who are genuinely interested in the results of that, yet so those tend to be problems in in particle physics or problems in pneumonia.",
                    "label": 0
                },
                {
                    "sent": "Medical prediction seems like that, and this is important, so all these permits I'm going to do are going to be done with fairly modest training sets, just 5000 points being used for training.",
                    "label": 0
                },
                {
                    "sent": "So we're gonna typically train the models with 4000 plus points and then hold aside 1000 of the training points for some sort of validation set to do parameter selection for the model.",
                    "label": 0
                },
                {
                    "sent": "You know, maybe to figure out what kernel we should use for the SVM or do early stopping or weight decay within neural net or something like that so, but this is all training data and then most of these datasets are moderately large.",
                    "label": 0
                },
                {
                    "sent": "Most of them are larger than 20,000.",
                    "label": 0
                },
                {
                    "sent": "Only a few of them are smaller than that, so typically we can afford to have very large final Test sets, which means that when I report numbers you can sort of trust him to the second, possibly 3rd decimal place on the test.",
                    "label": 1
                },
                {
                    "sent": "So notice that I've intentionally I've got a lot of data.",
                    "label": 0
                },
                {
                    "sent": "And I'm intentionally not used at all for training, so one of the reasons I'm doing this for computational efficiency one is about I can have these large final Test sets so I don't have to do hundreds of applications across my numbers, But this really is perhaps the key limitation of this study.",
                    "label": 0
                },
                {
                    "sent": "So if you're working on a problem where you've got 100,000 points in your training set for 10,000,000 points in your training set, it's very possible that the results I'm going to show here just going to apply it all to that regime.",
                    "label": 0
                },
                {
                    "sent": "So, so going.",
                    "label": 0
                },
                {
                    "sent": "Overgeneralize are overfit to any of the results that I present.",
                    "label": 0
                },
                {
                    "sent": "So the other thing is, all of these datasets are sort of modest number of dimensions.",
                    "label": 0
                },
                {
                    "sent": "There are from 20 to 300 dimensions, so these are not bags of words.",
                    "label": 0
                },
                {
                    "sent": "Text problems where we might have 10,000 dimensions.",
                    "label": 0
                },
                {
                    "sent": "So there's another way in which we could overfit.",
                    "label": 0
                },
                {
                    "sent": "It's very possible that works very well on 100 dimensional.",
                    "label": 0
                },
                {
                    "sent": "Problem isn't what works well in at 10,000 mention problems, so and I haven't got any 10,000 dimensional problems in the results of the report today.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so that's the empirical.",
                    "label": 0
                },
                {
                    "sent": "Background and then we're going to do is.",
                    "label": 0
                },
                {
                    "sent": "You know it's always painful to see a comparison of two algorithms made with one metric accuracy.",
                    "label": 0
                },
                {
                    "sent": "Or square or something, but just one metric, right?",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "You don't know whether you really should trust.",
                    "label": 0
                },
                {
                    "sent": "That is is, is that really indicative of what the model is learning about the problem in general, or is this something that's telling you more about that learning method modulo that metric, as opposed to how well that learning methods work on that problem?",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do is be agnostic about performance measures, at least in the first 2/3 of the talk, and we're just going to evaluate these learning methods on, as many metrics seem reasonable.",
                    "label": 0
                },
                {
                    "sent": "OK, so three of these metrics are going to depend on comparing the prediction.",
                    "label": 0
                },
                {
                    "sent": "All of the methods were using one way or another.",
                    "label": 0
                },
                {
                    "sent": "Gonna be asking make a continuous value prediction.",
                    "label": 0
                },
                {
                    "sent": "OK, so they're going to spit out a number, either minus Infinity plus Infinity, or a number between zero and one or something like that, as opposed to just the Boolean classification.",
                    "label": 0
                },
                {
                    "sent": "And then what we'll do is, we'll we'll make it class classification out of it by putting some threshold on that prediction.",
                    "label": 0
                },
                {
                    "sent": "And if you're below the threshold you're predicting plasty review above that threshold recruiting class one and everything I'm doing is just.",
                    "label": 0
                },
                {
                    "sent": "Volume classification.",
                    "label": 0
                },
                {
                    "sent": "So, so that's accuracy, which of course everybody knows how many people are familiar with F score for lift.",
                    "label": 0
                },
                {
                    "sent": "OK, so so nobody is familiar with left maybe or just OK so so F scores based on precision and recall which is used a lot in information retrieval so I won't dwell on that list.",
                    "label": 0
                },
                {
                    "sent": "Let me just mention briefly with Lift is.",
                    "label": 0
                },
                {
                    "sent": "Suppose you have to send out a marketing brochure to your customer base, and what you'd like to do is.",
                    "label": 0
                },
                {
                    "sent": "You'd like to only send the brochure to the fraction of the customer base that would actually respond to it, right?",
                    "label": 0
                },
                {
                    "sent": "Most people just throw it in trash.",
                    "label": 0
                },
                {
                    "sent": "You'd really like to focus your your marketing as much as possible.",
                    "label": 0
                },
                {
                    "sent": "Will suppose if you randomly send a brochure out to 10% of your population.",
                    "label": 0
                },
                {
                    "sent": "Suppose.",
                    "label": 0
                },
                {
                    "sent": "If you just randomly pick who those people were, your response would have been 5%.",
                    "label": 0
                },
                {
                    "sent": "OK, that means on average 5% of customers would respond to your brochure.",
                    "label": 0
                },
                {
                    "sent": "But instead suppose you send it to 10%.",
                    "label": 0
                },
                {
                    "sent": "That's carefully selected.",
                    "label": 0
                },
                {
                    "sent": "OK, so you use the learning models predict who would respond and suppose now you get a 20% response rate on that 5% you sent it.",
                    "label": 0
                },
                {
                    "sent": "Now got a lift of four because you've done 4 times better than random prediction soul is used a lot in marketing so it's gonna get it.",
                    "label": 0
                },
                {
                    "sent": "There's a threshold in the left curve, but we won't talk.",
                    "label": 0
                },
                {
                    "sent": "A feel free afterwards to ask more about these metrics if you want.",
                    "label": 0
                },
                {
                    "sent": "We've got these warming metrics.",
                    "label": 0
                },
                {
                    "sent": "It sounds like most people are familiar with precision and recall, so average precision is just like.",
                    "label": 1
                },
                {
                    "sent": "If you calculate the full precision curve as you sweep the threshold, average position is just the average of that curve.",
                    "label": 0
                },
                {
                    "sent": "There's some difficulties in there, but that's basically what it is.",
                    "label": 1
                },
                {
                    "sent": "The precision, recall, break, even point is where these things hit the 45 degree diagonal line precision is equal to recall and then RC area.",
                    "label": 0
                },
                {
                    "sent": "How many people are familiar with the area under your secret?",
                    "label": 0
                },
                {
                    "sent": "OK, great, great so.",
                    "label": 0
                },
                {
                    "sent": "So that's also very similar to precision recall RC area in some senses, perhaps more statistically sound measure of these sorts of things.",
                    "label": 0
                },
                {
                    "sent": "The important thing about these measures, as they only depend.",
                    "label": 0
                },
                {
                    "sent": "There's no comparison now of your predicted values for threshold.",
                    "label": 0
                },
                {
                    "sent": "It just depends you have this case and you have this case.",
                    "label": 0
                },
                {
                    "sent": "Maybe this is a positive and this is a negative with council nails.",
                    "label": 0
                },
                {
                    "sent": "The relative value that you predicted for this case is if you ordering that it creates on the test.",
                    "label": 0
                },
                {
                    "sent": "So all three of these metrics just depend on ordering and then we have three more metrics and these are the probability metrics and the probability metrics.",
                    "label": 1
                },
                {
                    "sent": "Those are just good old friends like squared error and log loss, which I assume everybody knows about.",
                    "label": 0
                },
                {
                    "sent": "It was one more than explaining about Pirate 10 slide.",
                    "label": 0
                },
                {
                    "sent": "It's a measure of calibration model so that all equipment.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have a problem.",
                    "label": 0
                },
                {
                    "sent": "Some of these metrics, like squared error zero is optimal.",
                    "label": 0
                },
                {
                    "sent": "OK in a high value of .5 would be bad.",
                    "label": 0
                },
                {
                    "sent": "For others like RC, .5 is in fact sort of random prediction and one would be optimal for other things like lift.",
                    "label": 0
                },
                {
                    "sent": "Remember, I said you can get a lift of four.",
                    "label": 0
                },
                {
                    "sent": "OK, so so that means lift can even be above 1.",
                    "label": 0
                },
                {
                    "sent": "It can go up to a fraction that depends on the number of positives and negatives in your data set.",
                    "label": 0
                },
                {
                    "sent": "Anyway, it's going to make comparing things across all these different metrics.",
                    "label": 0
                },
                {
                    "sent": "Difficult, So what we're going to do is normalize all these scores and make the scales all the similar as possible.",
                    "label": 0
                },
                {
                    "sent": "So in everything I'm going to talk about, we're going to have scale things in such a way that zero is baseline performance.",
                    "label": 0
                },
                {
                    "sent": "So if you see a score of 0, that's the kind of performance any simple model should be able to achieve just by knowing the numbers of positives and negatives.",
                    "label": 0
                },
                {
                    "sent": "Undateables.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's based on performance and one is going to be the best performance we think you can achieve in this problem.",
                    "label": 0
                },
                {
                    "sent": "We really like to be the base optimal rate.",
                    "label": 0
                },
                {
                    "sent": "We don't know the base operating because these are real problems.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do is we sort of cheap train with a little extra data.",
                    "label": 0
                },
                {
                    "sent": "We use use the big test set to do model selection and we sort of set the top of the scale up there by cheating basically, and then if any of these things achieve that kind of performance, they're going to support near 1 mean they're doing extremely well doing as well as you think.",
                    "label": 0
                },
                {
                    "sent": "You have to cheat to do so just basically promised everything I present zero is sort of baseline performance.",
                    "label": 0
                },
                {
                    "sent": "It's not bad performance, but it's not.",
                    "label": 0
                },
                {
                    "sent": "Not meaningful performance.",
                    "label": 0
                },
                {
                    "sent": "One is excellent performance.",
                    "label": 0
                },
                {
                    "sent": "We like you.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's a pretty big set of experiment, so we've got 10 different learning methods and in fact, remember I said there's lots of different ways of running into these learning methods.",
                    "label": 0
                },
                {
                    "sent": "So in fact there's hundreds of different parameter settings for some of these learning methods.",
                    "label": 1
                },
                {
                    "sent": "For example, between 500 different neural Nets, and we're going to train hundreds of decision trees, and we train 120 SPM's and things like that.",
                    "label": 1
                },
                {
                    "sent": "We're going to do 5 fold cross validation, so that means we're going to train over, say, 10,000 models for each of our problems, and we're up to 11 problems now.",
                    "label": 0
                },
                {
                    "sent": "So we're going to train over 100,000 different models here.",
                    "label": 0
                },
                {
                    "sent": "90 thousand of which tend to be very fast to train in the last 20,000 always seem to take all the time and never going to valuate on these nine performance metrics.",
                    "label": 0
                },
                {
                    "sent": "So we're going to do something on the order of 1,000,000 evaluations of models, so it's a pretty big set of experiments.",
                    "label": 0
                },
                {
                    "sent": "You can serve.",
                    "label": 0
                },
                {
                    "sent": "Run this kind of experiment on one problem on a cluster of Saint 30.",
                    "label": 0
                },
                {
                    "sent": "Linux nodes in a week.",
                    "label": 0
                },
                {
                    "sent": "So if you got any problems between take sort of 10 weeks on the plus button to give you an idea what it takes to do this.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm gonna start off just talking about probabilities first.",
                    "label": 0
                },
                {
                    "sent": "Anna couple different reasons for doing that.",
                    "label": 0
                },
                {
                    "sent": "Well, I don't want it.",
                    "label": 0
                },
                {
                    "sent": "I'm gonna hit tables.",
                    "label": 0
                },
                {
                    "sent": "Lots of numbers in it and I don't want to hit with big tables in the very beginning that have all these metrics 'cause it'll just be harder to make any sense of it.",
                    "label": 1
                },
                {
                    "sent": "Also let's basically give it predict the right conditional probability for each test case.",
                    "label": 0
                },
                {
                    "sent": "You'd be done.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's the best prediction you're going to be able to do any reasonable performance metric should be optimized.",
                    "label": 0
                },
                {
                    "sent": "Conditional probabilities, so getting the right probabilities is really a sufficient condition for excellent performance on all metrics.",
                    "label": 0
                },
                {
                    "sent": "If metrics are reasonably well engaged, so so that's one reason.",
                    "label": 0
                },
                {
                    "sent": "Look at it and then it also turns out there's just a fun story to tell.",
                    "label": 0
                },
                {
                    "sent": "If we look at probabilities and then my coauthor else dealing with students, he got a best paper award for this probability work, then I'm going to talk about in the next 2020 slides yourself.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it's interesting stuff.",
                    "label": 0
                },
                {
                    "sent": "OK, so here is the first table of results.",
                    "label": 0
                },
                {
                    "sent": "So let me walk through this so that you have.",
                    "label": 0
                },
                {
                    "sent": "Here I thought the different learning methods on the left hand side.",
                    "label": 0
                },
                {
                    "sent": "Here neural Nets, bag trees, random forests, K, your neighbor, all sorts of memory based learning, logistic regression, vanilla decision trees, boosted decision trees, SPM that have been scaled a certain way.",
                    "label": 1
                },
                {
                    "sent": "We're going to talk a lot about that boosted stumps.",
                    "label": 0
                },
                {
                    "sent": "Boosted stumps are just boosted decision trees that are one level trees and 90 phase, and these are the three performance measures for looking at right now.",
                    "label": 0
                },
                {
                    "sent": "Squared error, cross entropy.",
                    "label": 0
                },
                {
                    "sent": "It is just log loss.",
                    "label": 0
                },
                {
                    "sent": "I am just measure calibration of all explained in a couple slides.",
                    "label": 0
                },
                {
                    "sent": "Again these are all normal sports numbers up near one or this performance and this is the meaning of those three performance figures and the way this works is now realize each of these cells is the average.",
                    "label": 0
                },
                {
                    "sent": "Well, first of all, over the five cross validation trials, but it's the average over eight, 910 or 11 problems.",
                    "label": 0
                },
                {
                    "sent": "OK, so, So what does that mean for this?",
                    "label": 0
                },
                {
                    "sent": "To be the average performance of neural Nets?",
                    "label": 0
                },
                {
                    "sent": "On these say 11 problems with five crossovers across, well, here's what we've done.",
                    "label": 0
                },
                {
                    "sent": "We gotta problem one.",
                    "label": 0
                },
                {
                    "sent": "We train the neural net on its cross validation sample.",
                    "label": 0
                },
                {
                    "sent": "We used to help out 1K to pick of all the neural Nets.",
                    "label": 0
                },
                {
                    "sent": "Retraining which ones best that he's doing early, stopping weight decay, figuring out the number of hidden units, what learning rate to use, all that sort of stuff.",
                    "label": 0
                },
                {
                    "sent": "Figure out for problem one for trial one the neural net that looks best, and then we write down this performance on the final Test.",
                    "label": 0
                },
                {
                    "sent": "Now we go to trial two.",
                    "label": 0
                },
                {
                    "sent": "We figure out the neural net.",
                    "label": 0
                },
                {
                    "sent": "That's best right now.",
                    "label": 0
                },
                {
                    "sent": "This performance on the final Test set.",
                    "label": 0
                },
                {
                    "sent": "It could be a slightly different neural net, and the one that was best on trial one that's possible could have different different learning rate.",
                    "label": 0
                },
                {
                    "sent": "Who knows what.",
                    "label": 0
                },
                {
                    "sent": "When we get to problem too, it could be a very different grown up.",
                    "label": 0
                },
                {
                    "sent": "It's possible that a problem when you really needed a large neural net with 256 hidden units is possible.",
                    "label": 0
                },
                {
                    "sent": "That on problem to you only need one, can you?",
                    "label": 0
                },
                {
                    "sent": "And that would be awesome.",
                    "label": 0
                },
                {
                    "sent": "OK, so so we're averaging over the best neural Nets that you could train on each of these problems.",
                    "label": 0
                },
                {
                    "sent": "They're not all the same neural net.",
                    "label": 0
                },
                {
                    "sent": "OK, it's the same thing with SVN.",
                    "label": 0
                },
                {
                    "sent": "We ask you answer trying all the different kernels that are available in SPM light we're trying.",
                    "label": 0
                },
                {
                    "sent": "All the different parameter settings for those kernels that are reasonable, and then we're picking the one that's best using validation set.",
                    "label": 0
                },
                {
                    "sent": "So you can think of this as being suppose you've given.",
                    "label": 0
                },
                {
                    "sent": "An assignment which is here's a problem tonight.",
                    "label": 0
                },
                {
                    "sent": "Go figure out how best training SPM on this problem.",
                    "label": 0
                },
                {
                    "sent": "We're trying to simulate that kind of kind of thing.",
                    "label": 0
                },
                {
                    "sent": "OK, so so we're doing exactly that.",
                    "label": 0
                },
                {
                    "sent": "So we say this is the overall performance of SPS mean that's the best we could get SPS to do on every problem.",
                    "label": 0
                },
                {
                    "sent": "Every trial.",
                    "label": 0
                },
                {
                    "sent": "That's the average performance.",
                    "label": 0
                },
                {
                    "sent": "OK, so now let's look at some of these numbers.",
                    "label": 0
                },
                {
                    "sent": "So these things are sorted by their mean performance.",
                    "label": 0
                },
                {
                    "sent": "So an interesting link.",
                    "label": 0
                },
                {
                    "sent": "The thing that I guess you no longer get taught neural Nets is at the top of the table, so so that's sort of fun, right?",
                    "label": 0
                },
                {
                    "sent": "And and some of these things that I know you can lead to believe or just the best things on the planet aren't at the top of the table.",
                    "label": 0
                },
                {
                    "sent": "Don't worry, this is going to rise like a Phoenix to get very near the top of the table.",
                    "label": 0
                },
                {
                    "sent": "And we'll talk about why it's not there right now, but here are some things that you might not have expected, so neural Nets right now at the top of the table, back trees in rainforests, music you may not have spent much time using them there also doing extremely well at the top of the table, and in fact would fold means is that for this column these things were statistically indistinguishable and bold here, and so three things were statistically indistinguishable for calibration, which I haven't spent that is yet.",
                    "label": 0
                },
                {
                    "sent": "It turns out neural Nets are statistically better than these other things, and for the mean.",
                    "label": 0
                },
                {
                    "sent": "Positive calibration it turns up.",
                    "label": 0
                },
                {
                    "sent": "Enrollments are often also played good, but notice that the neural Nets aren't necessarily the best.",
                    "label": 0
                },
                {
                    "sent": "It's not a clean sweep for y'all.",
                    "label": 0
                },
                {
                    "sent": "Next.",
                    "label": 0
                },
                {
                    "sent": "In fact, the best thing for Square tower is rainforest.",
                    "label": 0
                },
                {
                    "sent": "By a little bit over bad trees in romance.",
                    "label": 0
                },
                {
                    "sent": "And the best thing for cross entropy is bad trees followed by rainforest, followed by moments.",
                    "label": 0
                },
                {
                    "sent": "It's just the fact that we are going across the city that is putting on its so tenuously in first place.",
                    "label": 0
                },
                {
                    "sent": "OK, so some interesting things were not competitive.",
                    "label": 0
                },
                {
                    "sent": "Probably the two that should really kick you in the face or boosting isn't doing very well.",
                    "label": 0
                },
                {
                    "sent": "Boosting sure.",
                    "label": 0
                },
                {
                    "sent": "Sounds like it's a high performing method and SPMS aren't doing very well.",
                    "label": 0
                },
                {
                    "sent": "Well, why is that?",
                    "label": 0
                },
                {
                    "sent": "Well, we've done something really stupid with SPMS, so all you did was, you know, as we entered the distance to the separating hyperplane.",
                    "label": 0
                },
                {
                    "sent": "So that's a number of months.",
                    "label": 0
                },
                {
                    "sent": "Plus Infinity and what we've done is we've just had a very stupid thing.",
                    "label": 0
                },
                {
                    "sent": "We took the smallest value we ever saw, push that to zero.",
                    "label": 0
                },
                {
                    "sent": "The largest value, push that to 1:00, and then just scale everything else linearly in between.",
                    "label": 0
                },
                {
                    "sent": "And that's a really bad thing to do if you're interested in predicting probabilities, and we had to do something to squash things to 01 because things like squared R and loss.",
                    "label": 0
                },
                {
                    "sent": "But you know, you have to have numbers look like probabilities to calculate loss.",
                    "label": 0
                },
                {
                    "sent": "So so let's go in.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Before before I talk about that, so everybody knows banking.",
                    "label": 0
                },
                {
                    "sent": "So just in case, it sounds like more people probably know boosting and bagging, let me briefly remind you with padding is better than random parcel.",
                    "label": 0
                },
                {
                    "sent": "Make more sense to bang is this incredibly simple brainchild of Leo Bryman.",
                    "label": 0
                },
                {
                    "sent": "Here's the algorithm we're going to bag decision trees so we can buy anything.",
                    "label": 1
                },
                {
                    "sent": "We take a bootstrap sample of the data that's just a sample with replacement.",
                    "label": 0
                },
                {
                    "sent": "We train a model.",
                    "label": 0
                },
                {
                    "sent": "We take another bootstrap sample of the same data.",
                    "label": 0
                },
                {
                    "sent": "We train another model, do it again, do it again, do it's 100 times.",
                    "label": 0
                },
                {
                    "sent": "Without trying to 100 models all on different bootstrap samples from the very same training set, and now you just take the average prediction of the summer models.",
                    "label": 1
                },
                {
                    "sent": "Nothing could be simpler.",
                    "label": 0
                },
                {
                    "sent": "OK, this is the kind of code that you know when you write a shell script.",
                    "label": 0
                },
                {
                    "sent": "It's this long.",
                    "label": 0
                },
                {
                    "sent": "It takes me 10 minutes to get it right and it can be run in parallel.",
                    "label": 0
                },
                {
                    "sent": "If you've got mobile processors, it's just amazingly simple thing.",
                    "label": 0
                },
                {
                    "sent": "It really works very well.",
                    "label": 0
                },
                {
                    "sent": "So so it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a good algorithm and then random forests is going to work even better.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we just give you an example of of bagging.",
                    "label": 0
                },
                {
                    "sent": "So here we got iterations of bagging.",
                    "label": 0
                },
                {
                    "sent": "So we're going out for 100 iterations.",
                    "label": 0
                },
                {
                    "sent": "This is the first tree we train, and this is accuracy.",
                    "label": 0
                },
                {
                    "sent": "OK, so this isn't normal scores, but up is good and this is a problem particle physics prediction.",
                    "label": 0
                },
                {
                    "sent": "So here's the performance of a single trade.",
                    "label": 0
                },
                {
                    "sent": "OK, so you know just below 6, nine, 5%.",
                    "label": 0
                },
                {
                    "sent": "That's the performance of a single tree has a very smart symmetry.",
                    "label": 1
                },
                {
                    "sent": "Actually we did a lot of searching.",
                    "label": 0
                },
                {
                    "sent": "Best single tree.",
                    "label": 0
                },
                {
                    "sent": "Here's what happens if you train that tree and a second tree and thrown together in a bag.",
                    "label": 0
                },
                {
                    "sent": "Suddenly your performance goes up by about point along that added Third Tree goes up quite another .01, but it goes up at a four 3/5, three my talking about 2025 trees.",
                    "label": 0
                },
                {
                    "sent": "You sort of, you know, with with 20% of the work we've got 80% of the benefit that the usual 2080 rule, but you can see the curve is sort of still noisy.",
                    "label": 0
                },
                {
                    "sent": "We going up by time.",
                    "label": 0
                },
                {
                    "sent": "We get to say 100.",
                    "label": 0
                },
                {
                    "sent": "You sort of run out of steam.",
                    "label": 0
                },
                {
                    "sent": "Rarely do you need to do more than 100 of these.",
                    "label": 0
                },
                {
                    "sent": "Performance increase, right?",
                    "label": 0
                },
                {
                    "sent": "We've got at least a .03 increase in accuracy here just by doing them.",
                    "label": 0
                },
                {
                    "sent": "Examples of the data, training multiple models and taking the average.",
                    "label": 0
                },
                {
                    "sent": "And this works well.",
                    "label": 0
                },
                {
                    "sent": "We can value neural net chicken bag SPMS, but there are some method works better with in some works.",
                    "label": 0
                },
                {
                    "sent": "Worse, but it's just a very flexible thing.",
                    "label": 0
                },
                {
                    "sent": "The neat thing about banking, by the way, is it almost never goes bad.",
                    "label": 0
                },
                {
                    "sent": "It very rarely falls down gives you worse performance than if you will not pass that model, and that's not true of other things such as boosting.",
                    "label": 0
                },
                {
                    "sent": "So boosting Canon does overfit one out of four times you use boosting, you shouldn't use it.",
                    "label": 0
                },
                {
                    "sent": "That's not true.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ready for some more recent idea from from Leo Bryman?",
                    "label": 0
                },
                {
                    "sent": "I mean, the truth is.",
                    "label": 0
                },
                {
                    "sent": "You know why you came out with bagging in the early 90s?",
                    "label": 0
                },
                {
                    "sent": "Boosting sort of hit the scene also.",
                    "label": 0
                },
                {
                    "sent": "In the early 90s, I mean the original theoretical work was done in 89 and 90, but it didn't empirically start start getting anybody's interest in social 9394, and boosting is so cool, it just sort of, you know.",
                    "label": 0
                },
                {
                    "sent": "Took over, I mean not too many people paid attention to bang later was justifiably pissed off at this and decided he would try to come up with something even better because he didn't believe boosting was working for the reasons they said they thought it was working and he was partly right about that.",
                    "label": 0
                },
                {
                    "sent": "And he thought that some of the analysis of why bagging work was too simplistic and he was right about that too.",
                    "label": 0
                },
                {
                    "sent": "So he came up with this thing called rainforest which is just sort of take bagging and make it even more stupid.",
                    "label": 0
                },
                {
                    "sent": "And that's what random forests are.",
                    "label": 1
                },
                {
                    "sent": "And it works even better.",
                    "label": 0
                },
                {
                    "sent": "So so that that tells you this genius network, right?",
                    "label": 0
                },
                {
                    "sent": "You take something that's pretty good.",
                    "label": 0
                },
                {
                    "sent": "You make it even Dumber.",
                    "label": 0
                },
                {
                    "sent": "Enforce.",
                    "label": 0
                },
                {
                    "sent": "It works better.",
                    "label": 0
                },
                {
                    "sent": "So, so random forests.",
                    "label": 0
                },
                {
                    "sent": "One thing that random force you can bag anything but random forest is now custom designed to trees.",
                    "label": 0
                },
                {
                    "sent": "OK, so you really you really have to do forests.",
                    "label": 0
                },
                {
                    "sent": "And here's how we're aiming for us is we draw Bootstrap sample data just like in bagging.",
                    "label": 0
                },
                {
                    "sent": "And now what we do is we have to decide what route tests put in the tree.",
                    "label": 0
                },
                {
                    "sent": "We draw a small sample of the available attributes, maybe have 100 attributes.",
                    "label": 1
                },
                {
                    "sent": "We draw sample 10.",
                    "label": 0
                },
                {
                    "sent": "Now you're only allowed to pick the root test from those 10.",
                    "label": 0
                },
                {
                    "sent": "You figure out which of those 10 is best put it.",
                    "label": 0
                },
                {
                    "sent": "Now you have to decide.",
                    "label": 0
                },
                {
                    "sent": "Then there's two branches.",
                    "label": 0
                },
                {
                    "sent": "Now you have to decide what tests to put in each of those branches.",
                    "label": 0
                },
                {
                    "sent": "When you get to each of those tests that you have to install, you draw a new random sample from the available attributes you 10 out of 100, and then you just get to pick the best from that, so it really.",
                    "label": 0
                },
                {
                    "sent": "It really limits what the tree is allowed to look at in terms of the attributes, and he often gets great performance with this with very small samples from the attributes like 3 out of 100, things like that.",
                    "label": 0
                },
                {
                    "sent": "It's amazing that this works so well because you would think that the trees just couldn't possibly become good models.",
                    "label": 0
                },
                {
                    "sent": "What this does is the nice thing is if the tree is forced to install an attribute that doesn't help it very much.",
                    "label": 0
                },
                {
                    "sent": "It still has a chance.",
                    "label": 0
                },
                {
                    "sent": "As it goes further down the tree to see attributes that are really needed to use so it sort of keeps getting 2nd and 3rd and 4th and 5th chances it goes further down the tree.",
                    "label": 0
                },
                {
                    "sent": "The only problem it has it's the sort of keep running out of data data keeps getting recursively partitioned into smaller and smaller groups.",
                    "label": 0
                },
                {
                    "sent": "So the beauty of this and then turns out that's critical as well for the algorithm to work.",
                    "label": 0
                },
                {
                    "sent": "The amazing thing is that what it does is it tends to increase the variance of the trees without very much hurting their performance.",
                    "label": 0
                },
                {
                    "sent": "And that's exactly what banking is better.",
                    "label": 0
                },
                {
                    "sent": "Averaging over many models reduces variance and as long as the average expected performance of those models is good that you can do amazing things.",
                    "label": 0
                },
                {
                    "sent": "So it tends to do that.",
                    "label": 0
                },
                {
                    "sent": "So basically you can think of banging is being if you're familiar everybody, hopefully with the bias variance decomposition, right?",
                    "label": 0
                },
                {
                    "sent": "So?",
                    "label": 0
                },
                {
                    "sent": "So bagging is doing is it's?",
                    "label": 0
                },
                {
                    "sent": "It's reducing the bias of the trees by throwing an even more random stuff and making trees near producing the bias.",
                    "label": 0
                },
                {
                    "sent": "It's doing much higher variance, but then what you do is you typically do maybe 1000 to 4000 iterations of random far, so you really you really hammer down the variance that result from the process, and it works extremely well.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so that's what random forests are.",
                    "label": 0
                },
                {
                    "sent": "Let me tell you what these calibration diagrams are.",
                    "label": 0
                },
                {
                    "sent": "So this is the third column in the table calibration.",
                    "label": 0
                },
                {
                    "sent": "Suppose the weather forecast today was a rain with probability .5, right?",
                    "label": 0
                },
                {
                    "sent": "In fact, I if I've got it right in May or June it rains 50% of the time here.",
                    "label": 0
                },
                {
                    "sent": "It's actually a little better than Pittsburgh.",
                    "label": 0
                },
                {
                    "sent": "Which were like 20 or 30.",
                    "label": 0
                },
                {
                    "sent": "OK, So what we mean by a weather forecaster being well calibrated?",
                    "label": 0
                },
                {
                    "sent": "Well suppose suppose there's 100 days in the year where the weather forecaster has predicted the probability of rain is .4.",
                    "label": 0
                },
                {
                    "sent": "You would say there well calibrated when they predict .4 if on 40 out of those 100 days of rain.",
                    "label": 1
                },
                {
                    "sent": "Right, that's what it means for probability to be well calibrated.",
                    "label": 0
                },
                {
                    "sent": "That means if you say probability P. Then over many trials, sort of frequentist POV the adventure occur about the fraction of the time, right?",
                    "label": 0
                },
                {
                    "sent": "So so that's what it means to be well calibrated.",
                    "label": 0
                },
                {
                    "sent": "Well, how do we?",
                    "label": 0
                },
                {
                    "sent": "How do we measure that for a model?",
                    "label": 0
                },
                {
                    "sent": "Well, these are things known as reliability diagram.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do is your model makes a bunch of different predictions were going to take all the predictions your model makes between point 3.4, and we're going to put in a bid.",
                    "label": 0
                },
                {
                    "sent": "So I think this is being like a histogram bin.",
                    "label": 0
                },
                {
                    "sent": "We put them in that pig and then what we do is we plot on the graph.",
                    "label": 0
                },
                {
                    "sent": "We plot the mean predictive value that then it will be something like .35 probably.",
                    "label": 0
                },
                {
                    "sent": "So we plot the mean predicted value of .35 and then we for all the items that fell in that bin.",
                    "label": 0
                },
                {
                    "sent": "All those test points.",
                    "label": 0
                },
                {
                    "sent": "We know what the observed rate is as well.",
                    "label": 1
                },
                {
                    "sent": "How often did it actually rain on this 100 days?",
                    "label": 0
                },
                {
                    "sent": "And if it turns out that the mean predicted value is comparable to the fraction of positives, then you're well calibrated in that bid.",
                    "label": 1
                },
                {
                    "sent": "Right, that sort of makes sense, and if you for all bins full along this diagonal, then you must be well calibrated everywhere.",
                    "label": 0
                },
                {
                    "sent": "That means you're you're been, that's predicting the real low P values.",
                    "label": 0
                },
                {
                    "sent": "0.1 is also well calibrated.",
                    "label": 0
                },
                {
                    "sent": "Those things are happening somewhere around .05% of the time, so that would be an ideal diagram.",
                    "label": 0
                },
                {
                    "sent": "Would sort of go straight along the diagonal.",
                    "label": 0
                },
                {
                    "sent": "That mean your model was extremely well calibrated no matter where it made predictions.",
                    "label": 0
                },
                {
                    "sent": "This model is pretty well calibrated, it's predicting.",
                    "label": 0
                },
                {
                    "sent": "A little off here, crossing a little low here, it tends to follow the diagonal so it's not too bad.",
                    "label": 0
                },
                {
                    "sent": "What does it mean to be predicting this?",
                    "label": 0
                },
                {
                    "sent": "So here we've got a point.",
                    "label": 0
                },
                {
                    "sent": "This mean predicted value is, say, .015.",
                    "label": 0
                },
                {
                    "sent": "But the observed rate is .2, so that means it's under predicted is predicting things like .015.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry .15 when the Observer 8.2 so it's not predicting this high values and should in this part of space exact opposite is happening over there is predicting a somewhat too high value over there and the observed rate is less OK.",
                    "label": 0
                },
                {
                    "sent": "So so the number we're using in our tables is actually sort of is a measure of the average absolute deviation of these diagrams from the diagonal.",
                    "label": 0
                },
                {
                    "sent": "Turns out that's not a great way to summarize these diagrams, which you really should do that's not robustly area in the RC curve is robust.",
                    "label": 0
                },
                {
                    "sent": "It turns out you really should do is.",
                    "label": 0
                },
                {
                    "sent": "You should look at the diagram.",
                    "label": 0
                },
                {
                    "sent": "The diagram tells you what will look at a bunch of these diagrams, but we are going to summarize these numbers.",
                    "label": 0
                },
                {
                    "sent": "Take those that last column where we use a single number to summarize these diagrams.",
                    "label": 0
                },
                {
                    "sent": "Take that with a little green assault.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Get back to these results and remember take this column with a grain of salt and if you take that come with a grain of salt.",
                    "label": 0
                },
                {
                    "sent": "It means these other things are just misguided neural network problem.",
                    "label": 0
                },
                {
                    "sent": "Right, so it's wrong that they do.",
                    "label": 0
                },
                {
                    "sent": "Neural Nets are so commandingly in first place.",
                    "label": 0
                },
                {
                    "sent": "Let's talk about SBS.",
                    "label": 0
                },
                {
                    "sent": "Remember we did something really stupid.",
                    "label": 0
                },
                {
                    "sent": "We scale them to 0 to one so.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's look at a reliability diagram.",
                    "label": 0
                },
                {
                    "sent": "For SPMS, this is on just six of our problems.",
                    "label": 0
                },
                {
                    "sent": "Remember, if you want your reliability diagram to look like the diagonal line, that would be good calibration.",
                    "label": 0
                },
                {
                    "sent": "None of these look like the diagonal line.",
                    "label": 0
                },
                {
                    "sent": "In fact, if you're familiar with that, they all look like the sigmoid function.",
                    "label": 0
                },
                {
                    "sent": "Classic sigmoid shape.",
                    "label": 0
                },
                {
                    "sent": "So these things are very distorted, we just squashed them linearly to zero to 1.",
                    "label": 0
                },
                {
                    "sent": "Yes, we're fixing their sort of range.",
                    "label": 0
                },
                {
                    "sent": "But we're doing nothing to fix this distortion that comes out of tradition, and all these SPN so far have been trained with just standard hinge loss.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is just the usual way that you would we train in SVM.",
                    "label": 0
                },
                {
                    "sent": "If you react for accuracy as opposed to Alpha squared up and you can see we've got a big problem here that we need to fix and that's why this car is so so poor you wouldn't expect.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, this isn't something that we recognize that John Platt recognizes, back in 98 were very nice paper in 99 method, which we're now calling class method under John's work, and basically what you do is you plot this diagram.",
                    "label": 0
                },
                {
                    "sent": "You fit a sigmoid to the data and you use that signaling to undo the transformation.",
                    "label": 1
                },
                {
                    "sent": "So you just invert the sigmoid and now you get prediction that should be essentially straight line.",
                    "label": 0
                },
                {
                    "sent": "If you can do that, well, John's method is more sophisticated than that, but that's essentially what it is.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's more sophisticated in the sense that.",
                    "label": 0
                },
                {
                    "sent": "There's ways to do cross validation.",
                    "label": 0
                },
                {
                    "sent": "Use out of samples to to do the fitting and things like that, but that's essentially what it is and this this works very well as we're going to see first of all, notice it's a good fit, right?",
                    "label": 0
                },
                {
                    "sent": "Here's a sigmoid fit to this data.",
                    "label": 1
                },
                {
                    "sent": "It's a pretty good fit today.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Alright, so let's apply pipe scaling to SPN's that have been trained with hinge loss and all the sudden there now type the 1st place right?",
                    "label": 0
                },
                {
                    "sent": "These are both bold numbers.",
                    "label": 0
                },
                {
                    "sent": "This is the old performance way down here.",
                    "label": 0
                },
                {
                    "sent": "That's what happens as soon as we correct for distortion of your predictions.",
                    "label": 0
                },
                {
                    "sent": "And I mean look at this is actually better than the neural net by tiny bit.",
                    "label": 0
                },
                {
                    "sent": "That's better than the neural net.",
                    "label": 0
                },
                {
                    "sent": "By very tiny bit.",
                    "label": 0
                },
                {
                    "sent": "This is a little bit versions calibration number, but remember that's the number where least confident in.",
                    "label": 0
                },
                {
                    "sent": "So to this, and it's already time to me anyway, so this essentially says that SPMS are just as good at predicting probabilities, as long as you correct for the characteristic distortion that you're going to get if you use hinge loss to do the usual Max margin training of the SBM.",
                    "label": 0
                },
                {
                    "sent": "OK, so so that's great news.",
                    "label": 0
                },
                {
                    "sent": "It says that the algorithm that you've been told is really good actually is really good as long as if you want probabilities you do something extra step in order to transform it.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now let's pull back and then we've just been looking at these three probability measures, so, so here's the three probability measures in these last three columns.",
                    "label": 0
                },
                {
                    "sent": "This is now the mean across all nine metrics to remember.",
                    "label": 0
                },
                {
                    "sent": "We've got accuracy aswell if threshold metrics.",
                    "label": 0
                },
                {
                    "sent": "We've got the ordering metrics here.",
                    "label": 0
                },
                {
                    "sent": "So now I've got the performance on all these metrics.",
                    "label": 0
                },
                {
                    "sent": "And again, bold means that you've done best in that column for indistinguishable from best.",
                    "label": 0
                },
                {
                    "sent": "So Interestingly, neural Nets didn't have to be in the 1st place, because now we're taking an average over 9 metrics, not just those three.",
                    "label": 0
                },
                {
                    "sent": "I mean, I think everybody who uses neural Nets would have thought that they would do extremely well on squared or I mean and log loss is typically how you optimize.",
                    "label": 0
                },
                {
                    "sent": "The neural net is to minimize log, also squared error, and then they probably pretty well calibrated because of that.",
                    "label": 0
                },
                {
                    "sent": "But it's interesting that they're also doing quite well on all these other measures, and in fact they do happen to sort to be in first place and that is after SPM's have been corrected with flat flat scale.",
                    "label": 0
                },
                {
                    "sent": "So so after that correction has been plot but take.",
                    "label": 0
                },
                {
                    "sent": "This bowl very seriously.",
                    "label": 0
                },
                {
                    "sent": "I mean, there's now a bunch of different numbers in each of these cells, and this is the average of nine different numbers.",
                    "label": 0
                },
                {
                    "sent": "No, immediately, all these performance measures are populated with each other, so these aren't independent readings of performance here, and this is the average over a lot of things.",
                    "label": 0
                },
                {
                    "sent": "That's all bowl all.",
                    "label": 0
                },
                {
                    "sent": "Four of those methods are essentially in four way tie for first place.",
                    "label": 0
                },
                {
                    "sent": "So we really can't distinguish them, so so SPMS batteries in rain before.",
                    "label": 0
                },
                {
                    "sent": "So all just doing extremely well.",
                    "label": 0
                },
                {
                    "sent": "OK, the only thing that's been pot scale though is is SPS.",
                    "label": 0
                },
                {
                    "sent": "Now something interesting.",
                    "label": 0
                },
                {
                    "sent": "Look at boost boosted decision trees.",
                    "label": 0
                },
                {
                    "sent": "Bold, bold, bold, bold, bold, bold.",
                    "label": 0
                },
                {
                    "sent": "And then lousy performance predicting probabilities.",
                    "label": 0
                },
                {
                    "sent": "So in fact, if you look at these numbers 861, well that's the best accuracy.",
                    "label": 0
                },
                {
                    "sent": "85 or less.",
                    "label": 0
                },
                {
                    "sent": "The best score 95600 it's close to being the best left 977 tie for first place in our city.",
                    "label": 0
                },
                {
                    "sent": "958 well by her is the best average precision 952.",
                    "label": 0
                },
                {
                    "sent": "It's the best right?",
                    "label": 0
                },
                {
                    "sent": "It's doing.",
                    "label": 0
                },
                {
                    "sent": "Boosted trees are doing incredibly well on these other measures.",
                    "label": 0
                },
                {
                    "sent": "The reason why their average performance is so low in the tables, because they're just doing a terrible job.",
                    "label": 0
                },
                {
                    "sent": "Probabilities what we've seen that before right SPMS were doing so well.",
                    "label": 0
                },
                {
                    "sent": "So so.",
                    "label": 0
                },
                {
                    "sent": "Of course we're going to play a similar game for boosted trees.",
                    "label": 0
                },
                {
                    "sent": "Let me just summarize that table in the following way.",
                    "label": 0
                },
                {
                    "sent": "So here what I've done is I've just taken for every problem.",
                    "label": 0
                },
                {
                    "sent": "Here's the state of our problems for each problem and measure.",
                    "label": 0
                },
                {
                    "sent": "I just sort of figured out what learning method gave the best performance.",
                    "label": 0
                },
                {
                    "sent": "So this means that boosted stumps invest there.",
                    "label": 0
                },
                {
                    "sent": "This means neural net.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The best there.",
                    "label": 0
                },
                {
                    "sent": "This means boosted decision trees request there.",
                    "label": 0
                },
                {
                    "sent": "This means kainer Shaver digest there, right?",
                    "label": 0
                },
                {
                    "sent": "So so that's just the table telling us for each problem metric.",
                    "label": 0
                },
                {
                    "sent": "What happened to be best?",
                    "label": 0
                },
                {
                    "sent": "This is an awkward table, right?",
                    "label": 0
                },
                {
                    "sent": "This is possible thing that was second best was you know.",
                    "label": 0
                },
                {
                    "sent": "Second only the 5th decimal place somebody wins so we.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's a summary so you can see that last column.",
                    "label": 0
                },
                {
                    "sent": "Hopefully Allmusic color very much anymore.",
                    "label": 0
                },
                {
                    "sent": "Here's a count of how often these different methods came.",
                    "label": 0
                },
                {
                    "sent": "Invest in that previous table.",
                    "label": 0
                },
                {
                    "sent": "There are less pain.",
                    "label": 0
                },
                {
                    "sent": "Invest 17 times right?",
                    "label": 0
                },
                {
                    "sent": "That's pretty pretty good performance.",
                    "label": 0
                },
                {
                    "sent": "Who's the trees came in best 19.",
                    "label": 0
                },
                {
                    "sent": "That's more than a sandwich.",
                    "label": 0
                },
                {
                    "sent": "Amazing about that is basically we don't think boosted trees can even compete in this part of the table.",
                    "label": 0
                },
                {
                    "sent": "They only get to compete in 2/3 of the table because they're doing a bad thing over here.",
                    "label": 0
                },
                {
                    "sent": "Just like SPMS, we're doing a bad thing, so the fact that boosted decision trees are doing so well, 19 count over here is amazing, because they only get to compete on, say, 2/3 of the problem.",
                    "label": 0
                },
                {
                    "sent": "So so that means they're really doing extreme.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "So let's let's think about that for saying so.",
                    "label": 0
                },
                {
                    "sent": "So first of all, it's gotta be clear that you can have a method that's incredibly smart.",
                    "label": 0
                },
                {
                    "sent": "Say boosted trees for SPMS, and that method can still predict error probability.",
                    "label": 0
                },
                {
                    "sent": "Predicting good probabilities is something different.",
                    "label": 0
                },
                {
                    "sent": "It's more demanding than, say, predicting a good RC or having good accuracy it requires.",
                    "label": 0
                },
                {
                    "sent": "Yet this other thing which is like calibration.",
                    "label": 0
                },
                {
                    "sent": "So that's an important thing to.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So boosting seems like everybody knows that boosting, so I'm not going to.",
                    "label": 0
                },
                {
                    "sent": "I'm not going.",
                    "label": 0
                },
                {
                    "sent": "Once we explain it, maybe you could say sure.",
                    "label": 0
                },
                {
                    "sent": "So, so boosting is really hard for materials is very, very pretty.",
                    "label": 0
                },
                {
                    "sent": "So in boosting what we're going to do is on the training set, we train a model on me.",
                    "label": 0
                },
                {
                    "sent": "We say that model and we look at where that model makes errors on me on the training set.",
                    "label": 0
                },
                {
                    "sent": "And here's the counter intuitive thing.",
                    "label": 0
                },
                {
                    "sent": "The next time we train another model, we put more weight on the cases where the first model made mistakes and less weight on the case is it got right?",
                    "label": 0
                },
                {
                    "sent": "So it's kind of like taking a dog and sort of pushing its nose into the places where it's having the most trouble, right?",
                    "label": 0
                },
                {
                    "sent": "You're taking the model your steering it to those cases that it is not yet getting right and focusing the attention of the model on the case is that it continues to not yet get right and eventually.",
                    "label": 0
                },
                {
                    "sent": "If the learner is powerful enough and if the training data is consistent, eventually it will make that your predictions on those points as well, and then the cool thing is so you keep doing this so you train a series of models.",
                    "label": 0
                },
                {
                    "sent": "So this is a sort of inherently sequential process.",
                    "label": 0
                },
                {
                    "sent": "You also keep every time you do this, you keep a prescribed weight.",
                    "label": 0
                },
                {
                    "sent": "It's based on the accuracy of the model and when you combine all these models votes in the end, you wait them by that term, which is effectively directors.",
                    "label": 0
                },
                {
                    "sent": "So, so all the models house not like bagging, where you just sort of training all the things independently and then just make a simple average of them.",
                    "label": 0
                },
                {
                    "sent": "Now you have the serial process where you keep focusing the attention of the model on places that it keeps getting wrong and we have a very carefully defined set of weights that tell us how to combine those those models.",
                    "label": 0
                },
                {
                    "sent": "In the end very interesting technique, but it has some wonderful properties and one interpretation is that it is effectively it's like a quasi maximum margin method.",
                    "label": 0
                },
                {
                    "sent": "It's not quite national margin, but it's close, so is that.",
                    "label": 0
                },
                {
                    "sent": "So I'll say the math math problem would be constructed in two minutes in 1/2 an hour.",
                    "label": 0
                },
                {
                    "sent": "The math is very nice.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so why is boosting not well calibrated?",
                    "label": 1
                },
                {
                    "sent": "Why is it doing poorly predicting probabilities?",
                    "label": 0
                },
                {
                    "sent": "Well, let me show you what boosting does.",
                    "label": 0
                },
                {
                    "sent": "Here's 17 boosting, so that's a no boosting.",
                    "label": 0
                },
                {
                    "sent": "That's the first model train.",
                    "label": 0
                },
                {
                    "sent": "Here's four steps.",
                    "label": 0
                },
                {
                    "sent": "830, Two, 120, eighteen, 1024 solution.",
                    "label": 0
                },
                {
                    "sent": "So this doesn't do more boosting, and I've just picked up the problems here.",
                    "label": 0
                },
                {
                    "sent": "Now what is this document?",
                    "label": 0
                },
                {
                    "sent": "This is just a histogram of the probabilities that come out of our model.",
                    "label": 0
                },
                {
                    "sent": "OK, so so you know, here's.",
                    "label": 0
                },
                {
                    "sent": "This model is predicting a number of things down to 0.",
                    "label": 0
                },
                {
                    "sent": "Fewer things near point 2.4.",
                    "label": 0
                },
                {
                    "sent": "Even less one 6.8 and more things you want, so it's just a histogram of the predictions.",
                    "label": 0
                },
                {
                    "sent": "Come out of your model adanih test, not on trains.",
                    "label": 0
                },
                {
                    "sent": "Now we do a little boosting.",
                    "label": 0
                },
                {
                    "sent": "Notice that the probability.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of probability mass and two tails.",
                    "label": 0
                },
                {
                    "sent": "Notice it's got a little flatter.",
                    "label": 0
                },
                {
                    "sent": "We do more boosting.",
                    "label": 0
                },
                {
                    "sent": "It's actually getting a little peek in the center and the little loss of mass there.",
                    "label": 0
                },
                {
                    "sent": "And maybe that's going down to.",
                    "label": 0
                },
                {
                    "sent": "And then we do more stages of boosting, and sure enough, there's even less mass there and there, and almost all the probabilities getting pushed to the center.",
                    "label": 0
                },
                {
                    "sent": "And that keeps happening and happening OK, so so we're losing the ability to make predictions that are very close to close to 0 or close to one.",
                    "label": 0
                },
                {
                    "sent": "We're making far more predictions between sort of .3 and .7 or so.",
                    "label": 0
                },
                {
                    "sent": "So that's what boosting is doing as we go.",
                    "label": 0
                },
                {
                    "sent": "Let's look at the reliability diagrams as that process happens.",
                    "label": 0
                },
                {
                    "sent": "So here's a reliable that diagram.",
                    "label": 0
                },
                {
                    "sent": "Actually saw before.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's one that's no boosting.",
                    "label": 0
                },
                {
                    "sent": "You haven't done anything yet.",
                    "label": 0
                },
                {
                    "sent": "You've done this.",
                    "label": 0
                },
                {
                    "sent": "This one actually looks pretty good, right?",
                    "label": 0
                },
                {
                    "sent": "It doesn't?",
                    "label": 0
                },
                {
                    "sent": "It's not up and below here now.",
                    "label": 0
                },
                {
                    "sent": "It's just roughly along the diagonal.",
                    "label": 0
                },
                {
                    "sent": "Well, that's interesting.",
                    "label": 0
                },
                {
                    "sent": "We've overshot now we're below and above below even more in above, even more below even more above and below a lot above a lot.",
                    "label": 0
                },
                {
                    "sent": "And hopefully these things look exactly like the SPM reliability diagrams that assault, they are sigmoidal shape.",
                    "label": 0
                },
                {
                    "sent": "So boosting yields a distortion in its probability prediction histogram.",
                    "label": 0
                },
                {
                    "sent": "That's very much like the kind of thing you can get it finished loss in SPMS.",
                    "label": 0
                },
                {
                    "sent": "So of course we should try using Platt's method, which works so well in SPMS.",
                    "label": 0
                },
                {
                    "sent": "We should try that on Bruce.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is consisted by the way for those of you who know the theory of boosting this is.",
                    "label": 1
                },
                {
                    "sent": "I mean, this is something we observed, but the truth is the theoreticians already expected this to happen.",
                    "label": 0
                },
                {
                    "sent": "They have good explanations for it.",
                    "label": 0
                },
                {
                    "sent": "My my favorite explanation actually is Leo Breiman's explanation, which is that boost is an equalizer.",
                    "label": 1
                },
                {
                    "sent": "Basically fix the things that keeps getting right and sort of keeps hurting those things and takes the things that keeps getting wrong.",
                    "label": 0
                },
                {
                    "sent": "He's trying to help those things so that roughly speaking it has the same error rate across all cases.",
                    "label": 0
                },
                {
                    "sent": "That's not exactly that.",
                    "label": 0
                },
                {
                    "sent": "Right, and that would explain it.",
                    "label": 0
                },
                {
                    "sent": "Turns out why you get that signal shape curve is not quite imagine margin method, but there's certainly a strong flavor of maximum margin optimization going on boosting, and that's really probably with uses the sigmoid curves.",
                    "label": 0
                },
                {
                    "sent": "I mean SPMS are maximal margin methods, boosting he's causing maximum margin, so so that's why I feel like that and then the statisticians in Stanford.",
                    "label": 0
                },
                {
                    "sent": "I mean they basically said you know it's trying to fit the logic of the model, so of course you're getting sigmoid.",
                    "label": 1
                },
                {
                    "sent": "So anyway, I just wanted to put this up because I think even though we're the first ones to sort of empirically look at what happens with boosting long data set I I just wanted to give credit to these other people who really knew that this was coming.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let me just show you what had to be applied class method.",
                    "label": 0
                },
                {
                    "sent": "So here's I think it's just 7 problems across the top, so that might be the problem we looked at before we had 1020 stages of boosting.",
                    "label": 0
                },
                {
                    "sent": "So each of these have been heavily boosted.",
                    "label": 0
                },
                {
                    "sent": "Is the sigmoid curves you get OK with all the boosting?",
                    "label": 0
                },
                {
                    "sent": "And then if you apply flat scaling to those models, these are the reliability diagrams which you get.",
                    "label": 0
                },
                {
                    "sent": "Notice these reliability diagrams, essentially hunger, diagonal line.",
                    "label": 0
                },
                {
                    "sent": "They're really good.",
                    "label": 0
                },
                {
                    "sent": "Reliability diagrams, and sure enough, we've got predictions that are backing the tails of the distribution again, except maybe in this problem.",
                    "label": 0
                },
                {
                    "sent": "It turns out from what we know about particle physics, problem probably shouldn't be anything in those tails, so we have some sort of side information to verify.",
                    "label": 0
                },
                {
                    "sent": "The impact is probably correct not to have put predictions back in the scales here, and you can see it's it's actually pretty good.",
                    "label": 0
                },
                {
                    "sent": "I'm so so that histogram of predictions can be too long without dying.",
                    "label": 0
                },
                {
                    "sent": "Would be some nice.",
                    "label": 0
                },
                {
                    "sent": "OK, so plants method is doing.",
                    "label": 0
                },
                {
                    "sent": "Very nice job of taking this weird thing that boosting does pushing the probabilities all towards the middle and away from the two tails.",
                    "label": 0
                },
                {
                    "sent": "It does a great job of doing that.",
                    "label": 0
                },
                {
                    "sent": "Now let's",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Apply that and look these probability metrics.",
                    "label": 0
                },
                {
                    "sent": "All of a sudden boost is in first place.",
                    "label": 0
                },
                {
                    "sent": "So neural Nets are longer in first place.",
                    "label": 0
                },
                {
                    "sent": "And boosting is commanding first box, right?",
                    "label": 0
                },
                {
                    "sent": "That's not.",
                    "label": 0
                },
                {
                    "sent": "I wouldn't call that a 1, two or three way tie for first one, which I don't want.",
                    "label": 0
                },
                {
                    "sent": "That is, I would call that a 2, three or four way tie for first place.",
                    "label": 0
                },
                {
                    "sent": "Boosting really is in first place and it's only on this calibration measure that anything is hiding these other two measures is just clearly in the lead here.",
                    "label": 0
                },
                {
                    "sent": "And just to let you know what we've done, here is.",
                    "label": 0
                },
                {
                    "sent": "You know first we did class going on this weekend and we did classifying in SPMS and boosting.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to sort of walk through every learning method and do that.",
                    "label": 0
                },
                {
                    "sent": "So we did.",
                    "label": 0
                },
                {
                    "sent": "We just pass scaled everything, every model we trained with Pascal.",
                    "label": 0
                },
                {
                    "sent": "Did we use the validation set to figure out whether it was better to plot scale for to not like scale and a flat scaling help?",
                    "label": 0
                },
                {
                    "sent": "That's what we used if it hurt we didn't use it.",
                    "label": 0
                },
                {
                    "sent": "Help something to help boosting.",
                    "label": 0
                },
                {
                    "sent": "SPS, they help him for us a little bit.",
                    "label": 0
                },
                {
                    "sent": "Help boosted stumps alot.",
                    "label": 0
                },
                {
                    "sent": "It helped a plain decision.",
                    "label": 0
                },
                {
                    "sent": "Trees and help my phase.",
                    "label": 0
                },
                {
                    "sent": "Of course.",
                    "label": 0
                },
                {
                    "sent": "90 days older couple can get pretty problems right?",
                    "label": 0
                },
                {
                    "sent": "So it's it's the craziest probability predicted.",
                    "label": 0
                },
                {
                    "sent": "The risk it didn't help the things that other one didn't help their own.",
                    "label": 0
                },
                {
                    "sent": "Expat trees, painters, neighbor or logistic regression.",
                    "label": 0
                },
                {
                    "sent": "Be careful here.",
                    "label": 0
                },
                {
                    "sent": "So cater statement.",
                    "label": 0
                },
                {
                    "sent": "This does not mean every flavor parameter setting that you put down for 10 years neighborhoods.",
                    "label": 0
                },
                {
                    "sent": "Well calibrated models.",
                    "label": 0
                },
                {
                    "sent": "That's definitely not true.",
                    "label": 0
                },
                {
                    "sent": "What this means is that for each problem there was a setting that yielded pretty good calibration.",
                    "label": 0
                },
                {
                    "sent": "OK, there was a parameter setting that work for you.",
                    "label": 0
                },
                {
                    "sent": "OK, so it helps some things and it suddenly put put boosting really in first place, so so that's kind of fun.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's go back to all the other measures.",
                    "label": 0
                },
                {
                    "sent": "Remember, boosting is already doing incredibly well in these six.",
                    "label": 0
                },
                {
                    "sent": "It was just doing very poorly in those three.",
                    "label": 0
                },
                {
                    "sent": "It now really is commandingly first place, so these are huge differences.",
                    "label": 0
                },
                {
                    "sent": "Let me give you an idea.",
                    "label": 0
                },
                {
                    "sent": "Yeah, these normalized scores that were using can be misleading.",
                    "label": 0
                },
                {
                    "sent": "Remember, baseline is 0 on this scale, so suppose baseline in your problem is a 50% accuracy right?",
                    "label": 0
                },
                {
                    "sent": "It's a problem, it's 5050 positives in there, so everybody should be able to achieve 50% accuracy.",
                    "label": 0
                },
                {
                    "sent": "Let's say you're achieving.",
                    "label": 0
                },
                {
                    "sent": "95% accuracy with the smart learning method on that problem.",
                    "label": 0
                },
                {
                    "sent": "Well, what's your normalized score gonna be if it's a problem for what you actually think you put it you 100% accuracy, which you can.",
                    "label": 0
                },
                {
                    "sent": "Those problems were normalized worldly .9 because you've gone from zero at 50% accuracy, right?",
                    "label": 0
                },
                {
                    "sent": "You've gone 9/10 of the way up to 100% so so put you at .9.",
                    "label": 0
                },
                {
                    "sent": "So what's the difference of point 92.9 one near me?",
                    "label": 0
                },
                {
                    "sent": "Right, so so it means that your accuracy has gone from 95% to 195.5%.",
                    "label": 0
                },
                {
                    "sent": "To get that right.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so so difference here at .01.",
                    "label": 0
                },
                {
                    "sent": "Music your accuracy has increased .05%.",
                    "label": 0
                },
                {
                    "sent": "Right, which is which is.",
                    "label": 0
                },
                {
                    "sent": "I guess you know.",
                    "label": 0
                },
                {
                    "sent": "10% production loss but.",
                    "label": 0
                },
                {
                    "sent": "So sometimes what look like big differences here.",
                    "label": 0
                },
                {
                    "sent": "If you go back and you think in the original scales can be can be a rather small thing.",
                    "label": 0
                },
                {
                    "sent": "So so just keep that in mind.",
                    "label": 0
                },
                {
                    "sent": "But still will be.",
                    "label": 0
                },
                {
                    "sent": "God is essentially boosting in first place and now four methods, random forests, neural Nets, SPMS in bag trees, two of which need or benefit from this calibration plots method, and two of which don't all sort of ties for 2nd place.",
                    "label": 0
                },
                {
                    "sent": "That really is a type of 2nd place.",
                    "label": 0
                },
                {
                    "sent": "Don't take the order of this too seriously.",
                    "label": 0
                },
                {
                    "sent": "I'll give you some information later to make it clear that you really should ignore that order.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's that's kind of interesting.",
                    "label": 0
                },
                {
                    "sent": "What's interesting?",
                    "label": 0
                },
                {
                    "sent": "I mean, I think decision trees really hot in the 80s, but time Euronext came around.",
                    "label": 0
                },
                {
                    "sent": "People were tired of them.",
                    "label": 0
                },
                {
                    "sent": "They're ready to move on, something more coolness.",
                    "label": 0
                },
                {
                    "sent": "We're definitely all.",
                    "label": 0
                },
                {
                    "sent": "And then I think we're all sort of ready to senior.",
                    "label": 0
                },
                {
                    "sent": "You know, decision trees get put in history books and just be things that you use when you need to understand your model.",
                    "label": 0
                },
                {
                    "sent": "Show model to an expert.",
                    "label": 0
                },
                {
                    "sent": "Maybe then you want to use the decision tree.",
                    "label": 0
                },
                {
                    "sent": "The cool thing is not that boosted decision trees are intelligible, but humble decision trees which have a lot of nice properties that handle missing values.",
                    "label": 0
                },
                {
                    "sent": "They are easy to grow fast, they can handle any kind of activity type without any careful manipulation.",
                    "label": 0
                },
                {
                    "sent": "I mean, they're they're pretty impressive models in terms of ease of use.",
                    "label": 0
                },
                {
                    "sent": "It's really interesting to see that things like banking and boosting have sort of put them back on the top of the map, so so, so that's fine.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now I've been using pipes method.",
                    "label": 0
                },
                {
                    "sent": "Turns out there's lots of different ways of achieving calibration.",
                    "label": 0
                },
                {
                    "sent": "Patch method is just one of those ways, so let's talk a little bit about these other things.",
                    "label": 0
                },
                {
                    "sent": "Well, first of all, you can boost a log loss.",
                    "label": 0
                },
                {
                    "sent": "Instead of the way we normally do boosting in eight groups so so you can actually do boosting that aims are predicting probabilities directly.",
                    "label": 0
                },
                {
                    "sent": "Similarly, you can train SVM is to maximize the likelihood instead of using hinge loss.",
                    "label": 0
                },
                {
                    "sent": "And now you'll also be essentially optimizing toward well, so there are ways of training things like boosting SVM.",
                    "label": 0
                },
                {
                    "sent": "These magical margin methods so that you actually are optimizing loss directly.",
                    "label": 0
                },
                {
                    "sent": "The interesting thing is, it doesn't work so well in practice, so which is kind of unexpected.",
                    "label": 0
                },
                {
                    "sent": "We we thought that that would work very well.",
                    "label": 0
                },
                {
                    "sent": "It turns out all is going to play with my share similar results.",
                    "label": 0
                },
                {
                    "sent": "There are other ways to do it too.",
                    "label": 0
                },
                {
                    "sent": "There's a logistic correction.",
                    "label": 0
                },
                {
                    "sent": "This comes from the guys at Stanford.",
                    "label": 0
                },
                {
                    "sent": "Basically, they said, hey.",
                    "label": 0
                },
                {
                    "sent": "You rock Washington logic of something.",
                    "label": 0
                },
                {
                    "sent": "Then you should just use analytic formula to undo the logic and you'll get back the right thing.",
                    "label": 0
                },
                {
                    "sent": "It turns out that doesn't work very well in practice.",
                    "label": 0
                },
                {
                    "sent": "There's touch scaling, which we already used.",
                    "label": 0
                },
                {
                    "sent": "Another spacetime regression model which I'm going to talk about 'cause you might actually want to use this instead of plat scaling to depending on how much data.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Going to show you boosting with log loss, so here's boosted stumps.",
                    "label": 0
                },
                {
                    "sent": "Optimize the log loss.",
                    "label": 0
                },
                {
                    "sent": "And here's boosted stumps that have been optimized the normal way, but then you plant scale right?",
                    "label": 0
                },
                {
                    "sent": "So, so here you optimize the wrong thing and then you try to fix it after the fact and he tried to optimize to the right thing.",
                    "label": 0
                },
                {
                    "sent": "And I'm talking to the right thing.",
                    "label": 0
                },
                {
                    "sent": "Doesn't work as well and we think in this case this is still.",
                    "label": 0
                },
                {
                    "sent": "It.",
                    "label": 0
                },
                {
                    "sent": "Turns out a lot of benefit.",
                    "label": 0
                },
                {
                    "sent": "Report we think one of the reasons why this happens here is when you're doing this plot scaling method, you're automatically using certain health side validation data to do the fitting, so you're much less likely to overfit, and it turns out boosting is very prone to everything.",
                    "label": 0
                },
                {
                    "sent": "So it's even more prone to overfitting if you try to do boosting to log loss.",
                    "label": 0
                },
                {
                    "sent": "It turns out that makes boosting even more powerful.",
                    "label": 0
                },
                {
                    "sent": "It overfits even more and it really just hangs itself.",
                    "label": 0
                },
                {
                    "sent": "So this sort of not letting it hang itself and then doing something afterwards.",
                    "label": 0
                },
                {
                    "sent": "That also isn't going to hang itself turns out to be a better procedure, so crossing that point out the case, if you had a million training points here.",
                    "label": 0
                },
                {
                    "sent": "But it's certainly the case with small sample.",
                    "label": 0
                },
                {
                    "sent": "A similar thing for boosted trees cares, boosting poultries with by correction at the top of the table.",
                    "label": 0
                },
                {
                    "sent": "Here's boosted full trees that have been optimized.",
                    "label": 0
                },
                {
                    "sent": "Log loss taken very, very poorly compared to that, and here it really is overfitting.",
                    "label": 0
                },
                {
                    "sent": "It turns out that basically if the underlying model classes too powerful, you just end up getting nearly perfect prediction on every test set.",
                    "label": 0
                },
                {
                    "sent": "Every training set that you get when you almost blood loss and it thinks this goes S right?",
                    "label": 0
                },
                {
                    "sent": "So so anyway, I just wanted to point out I mean.",
                    "label": 0
                },
                {
                    "sent": "I don't want you to leave the talk and say class failing is interesting.",
                    "label": 0
                },
                {
                    "sent": "Isotonic regression is interesting, but if they just done the right thing you wouldn't have to do any of this jump.",
                    "label": 0
                },
                {
                    "sent": "It turns out if you do the quote right thing, it doesn't doesn't always work with this.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me see you there.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Algorithm.",
                    "label": 0
                },
                {
                    "sent": "Let me just tell you what we're doing.",
                    "label": 0
                },
                {
                    "sent": "What is logistic correction Plattsmouth?",
                    "label": 0
                },
                {
                    "sent": "Basically we're looking for a logistic function that's a monotonically rising smooth function that shapes kind of like that, so that's what we're doing with classmethod.",
                    "label": 0
                },
                {
                    "sent": "Isotonic regression is more powerful.",
                    "label": 1
                },
                {
                    "sent": "Isotonic regression says give me any monotonically increasing function.",
                    "label": 0
                },
                {
                    "sent": "I don't care what the shape is.",
                    "label": 0
                },
                {
                    "sent": "Give me any monotonically increasing function, and if you can find the function that will actually give me the optimal squared error on some training center.",
                    "label": 0
                },
                {
                    "sent": "So it's a much more powerful method, is not restricted to sigmoid shaped curves.",
                    "label": 0
                },
                {
                    "sent": "That means it can really perhaps do better things when sigmoid shape correction is beyond your correction, so it's a much more powerful thing, But then you have to worry that maybe it'll be too powerful and overfit, and what will see sometimes it does, and sometimes it doesn't.",
                    "label": 0
                },
                {
                    "sent": "So here's a here's a bunch of problems before some of our problems.",
                    "label": 0
                },
                {
                    "sent": "This is the sort of fitting isotonic curve if you squint so that you don't see those little edges.",
                    "label": 0
                },
                {
                    "sent": "It might look kinda like a sigmoid curve.",
                    "label": 0
                },
                {
                    "sent": "But it actually is sort of a stair step.",
                    "label": 0
                },
                {
                    "sent": "You could think of.",
                    "label": 0
                },
                {
                    "sent": "Isotonic regression is being like an optimal bidding of the data so that the number of violations between adjacent bins is minimized.",
                    "label": 0
                },
                {
                    "sent": "So that's that's one way to think about it.",
                    "label": 0
                },
                {
                    "sent": "And here's the beautiful reliability diagrams that you get after doing this.",
                    "label": 0
                },
                {
                    "sent": "Time progression on those things is really very, very nice.",
                    "label": 0
                },
                {
                    "sent": "It's still new upgrade.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's a comparison.",
                    "label": 0
                },
                {
                    "sent": "Here's Platt method on those problems.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of older donors.",
                    "label": 0
                },
                {
                    "sent": "Here's isotonic regression on those methods.",
                    "label": 1
                },
                {
                    "sent": "Reliability buggers.",
                    "label": 0
                },
                {
                    "sent": "They're both very, very very good.",
                    "label": 0
                },
                {
                    "sent": "So now you might think, well, we should use.",
                    "label": 0
                },
                {
                    "sent": "By the way, this time of regression is nice.",
                    "label": 0
                },
                {
                    "sent": "It's a linear time algorithm.",
                    "label": 0
                },
                {
                    "sent": "Well, isn't isn't in log in factor in there, but it's actually a very efficient algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's easy to code, and it is awful for square, so so it hasn't really nice properties, so we should.",
                    "label": 0
                },
                {
                    "sent": "We don't get the credit for this.",
                    "label": 0
                },
                {
                    "sent": "Iconic regression was invented by statisticians.",
                    "label": 0
                },
                {
                    "sent": "75 years ago and the people who first recognized it could be used for counting or Charles opening beyond.",
                    "label": 0
                },
                {
                    "sent": "This is addressing social sort of follow up.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's some years boosted decision trees.",
                    "label": 0
                },
                {
                    "sent": "SBS neural Nets in rainforest and this is squared error, so I'm no longer looking these normal scores, so down is good in these graphs.",
                    "label": 0
                },
                {
                    "sent": "This is for boosted trees.",
                    "label": 0
                },
                {
                    "sent": "That's the square you get.",
                    "label": 0
                },
                {
                    "sent": "It can do no calibration.",
                    "label": 0
                },
                {
                    "sent": "It's not a very good squared error.",
                    "label": 0
                },
                {
                    "sent": "This is the squared error.",
                    "label": 0
                },
                {
                    "sent": "We're varying amount of data we're using to do the calibration here, so this is a log scale, so this is 128 points used to do calibration.",
                    "label": 0
                },
                {
                    "sent": "There's 1024 used to do calibration out about 8000 points used in calibration.",
                    "label": 0
                },
                {
                    "sent": "So what you see is here is isotonic regression, so it really doesn't do very well if you have a small data set.",
                    "label": 1
                },
                {
                    "sent": "But it does better, better, better, better.",
                    "label": 0
                },
                {
                    "sent": "Better.",
                    "label": 0
                },
                {
                    "sent": "In fact, it does extremely well if you give it a large data set to do corrections.",
                    "label": 0
                },
                {
                    "sent": "This green line it's hard to see that plasma, and I think what you're seeing all these graphs is if your data poor pass method dominates.",
                    "label": 0
                },
                {
                    "sent": "I make sense.",
                    "label": 0
                },
                {
                    "sent": "It's the simpler, smoother fitting function.",
                    "label": 0
                },
                {
                    "sent": "It can't overfit to the data so easily, and sequences are reasonable fitting function, so it actually does pretty well when you don't have much data, but you can see there's a crossing point.",
                    "label": 0
                },
                {
                    "sent": "And once you pass that crossing point, possible to structure like so, it's like behind a little bit there right behind there and left behind there.",
                    "label": 0
                },
                {
                    "sent": "Once you have enough data is better to use more powerful method.",
                    "label": 0
                },
                {
                    "sent": "So, so that's the moral of the story, and it's kind of fun.",
                    "label": 0
                },
                {
                    "sent": "You can see you get a lot of improvement on boosted trees by doing any form calibration with SVN is in fact you get so much benefit you can't see this horizontal line for the estimates on calories off the top of the chart.",
                    "label": 0
                },
                {
                    "sent": "There random forest.",
                    "label": 0
                },
                {
                    "sent": "They're already pretty well calibrated, but they do benefit from calibration.",
                    "label": 0
                },
                {
                    "sent": "Here's neural Nets.",
                    "label": 0
                },
                {
                    "sent": "Neural Nets are incredibly well calibrated to begin with, and it turns out only if you got tons of data to do the calibration.",
                    "label": 0
                },
                {
                    "sent": "Slightly better than you.",
                    "label": 0
                },
                {
                    "sent": "Well, now that's the way we're training them.",
                    "label": 0
                },
                {
                    "sent": "Have a signal unit in their output layers so so they're already doing something like Classmethod internal automatically.",
                    "label": 0
                },
                {
                    "sent": "That's the standard way.",
                    "label": 0
                },
                {
                    "sent": "So, OK, so the moral of the story basically is.",
                    "label": 0
                },
                {
                    "sent": "Both methods work well if you have lots of data, it's better to use more powerful isotonic regression method if you don't have much data, though, then it's better to use the the smoother simpler plasma fittings.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, another so we don't.",
                    "label": 0
                },
                {
                    "sent": "This is by the time you do.",
                    "label": 0
                },
                {
                    "sent": "Calibration.",
                    "label": 0
                },
                {
                    "sent": "You've already trained your models on all the attributes and now the calibration step knows nothing about the number of attributes because all this is doing is taking a prediction that comes from your model number, like .6.",
                    "label": 0
                },
                {
                    "sent": "And it's looking at the other predictions your model has made and is looking at the true labels from those things.",
                    "label": 0
                },
                {
                    "sent": "Zeros and ones.",
                    "label": 0
                },
                {
                    "sent": "And it's just finding a way to correct those predictions.",
                    "label": 0
                },
                {
                    "sent": "So at this point it doesn't know anything about it, but there isn't a deeper, more interesting question which I think they're looking to, which is.",
                    "label": 0
                },
                {
                    "sent": "The more attributes, the more likely the underlying model is overfit.",
                    "label": 0
                },
                {
                    "sent": "And then the more likely the underlying model is overfit.",
                    "label": 0
                },
                {
                    "sent": "Perhaps the harder it would be for any of these calibration methods to undo that overfitting, and I think that's probably true, but we haven't examined it carefully, so that's interesting.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm going to skip this summary slide.",
                    "label": 0
                },
                {
                    "sent": "It doesn't say anything you don't.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "So calibration works break and it takes 2 of what we thought were our favorite methods and based on our favorite methods again, so it takes boosted trees, an SVM and put some essentially in the top.",
                    "label": 0
                },
                {
                    "sent": "It also helps random forests and neural Nets were already doing pretty well.",
                    "label": 0
                },
                {
                    "sent": "Thanks for everything pretty well.",
                    "label": 0
                },
                {
                    "sent": "So maybe remember the big questions at the very beginning of the talk seems like those years ago, questions like, well, what learning method should you use?",
                    "label": 0
                },
                {
                    "sent": "When can we ignore a bunch of these learning methods and we put a bunch of pastor?",
                    "label": 0
                },
                {
                    "sent": "Maybe we should just include the trees, calibrate it and, um, you know, that's like the supermarket.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's not the case, so I've got one more line this table, the top.",
                    "label": 0
                },
                {
                    "sent": "Let me tell you what that line is.",
                    "label": 0
                },
                {
                    "sent": "What we've done is for every problem.",
                    "label": 0
                },
                {
                    "sent": "For every trial we just get to pick from all of thousands of models to be trained.",
                    "label": 0
                },
                {
                    "sent": "We just get to pick whichever 1 looks best.",
                    "label": 0
                },
                {
                    "sent": "You know, maybe I want trial.",
                    "label": 0
                },
                {
                    "sent": "It looks like in years neighbor model may be on the next trial to boost trade next miles and SPM.",
                    "label": 0
                },
                {
                    "sent": "Who knows why?",
                    "label": 0
                },
                {
                    "sent": "So this line is sort of the best of the best of the best.",
                    "label": 1
                },
                {
                    "sent": "It's it's we get to pick for every problem.",
                    "label": 0
                },
                {
                    "sent": "What thing that looks like is performing best and the interesting thing is not that you know it does better than you expect it to.",
                    "label": 0
                },
                {
                    "sent": "Never do any worse.",
                    "label": 1
                },
                {
                    "sent": "Spend the best of these methods right 'cause it gets to pick the best way for the only way.",
                    "label": 0
                },
                {
                    "sent": "2.9119 there.",
                    "label": 0
                },
                {
                    "sent": "First program yes yes.",
                    "label": 0
                },
                {
                    "sent": "So I was just going to say you only way you can do worse is if it overfits to the validation set that's being used to pick the best model.",
                    "label": 0
                },
                {
                    "sent": "And in one case in the table just so.",
                    "label": 0
                },
                {
                    "sent": "So I'm glad you so.",
                    "label": 0
                },
                {
                    "sent": "But overall, you shouldn't expect it to do much worse and less.",
                    "label": 0
                },
                {
                    "sent": "Your validation is just very noisy.",
                    "label": 0
                },
                {
                    "sent": "Right, but here's the interesting thing is how much better it does.",
                    "label": 0
                },
                {
                    "sent": "9533 versus 914 by distinguished trees for everything.",
                    "label": 0
                },
                {
                    "sent": "No notice that the difference between these top five methods right is all the difference of about 1.04 at most less than .04, and we're getting another .04.",
                    "label": 0
                },
                {
                    "sent": "Just by picking the best model in every one of these things, so it turns out if you dig deeper we looking here.",
                    "label": 0
                },
                {
                    "sent": "There are some problems in some metrics for which logistic regression, which is terrible, does best.",
                    "label": 0
                },
                {
                    "sent": "In fact, it's really annoying boosted stumps, so one of the interesting things of the study is you might have heard that it's better to boost weak models.",
                    "label": 0
                },
                {
                    "sent": "Well, there's no evidence of that.",
                    "label": 0
                },
                {
                    "sent": "Boosting full trees is easily outperforming boosting the weaker stumps so so we don't have any evidence.",
                    "label": 0
                },
                {
                    "sent": "It turns out if you look at the details, though, there are two problems from boosted stumps easily outperform boosted full decision trees, and that's because the booster for decision trees massively overfits the data.",
                    "label": 0
                },
                {
                    "sent": "Where is the boosted stumps can so so the fact that boosting has done so well up here even though it falls down one or two problems, it's interesting that means it really does well in the other problems that enough to compensate for the air, but sometimes so.",
                    "label": 0
                },
                {
                    "sent": "But if you wanted the details, I mean everyone of these learning.",
                    "label": 0
                },
                {
                    "sent": "As it turns out, is best on some problems.",
                    "label": 0
                },
                {
                    "sent": "So you can't safely eliminate any of these things.",
                    "label": 0
                },
                {
                    "sent": "I even humble old methods you know, like like a nearest neighbor or simple decision trees.",
                    "label": 0
                },
                {
                    "sent": "They occasionally turn out to be just the right model or some problem in somebody that's really sort of disappointed, right?",
                    "label": 0
                },
                {
                    "sent": "It's the worst case result.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you gotta just train one thing and get good results in hurry, go for boost decision trees do it will do very, very well.",
                    "label": 0
                },
                {
                    "sent": "But if you really need the best performance and you have the time to do it.",
                    "label": 0
                },
                {
                    "sent": "You really should try everything you can under the sun.",
                    "label": 0
                },
                {
                    "sent": "You know, you know, get back to run everything you can think of in waka and pick one that seems best and you'll do better than if you pick whatever your favorite method is in just for that.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of a surprise.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "That's kind of disappointing.",
                    "label": 0
                },
                {
                    "sent": "It says that anytime we want to do really well, we have to just sort of train everything.",
                    "label": 0
                },
                {
                    "sent": "It's best can we do any better than that?",
                    "label": 1
                },
                {
                    "sent": "I mean, if you're going to work training thousands of models in every problem, can you do something fancier?",
                    "label": 0
                },
                {
                    "sent": "Well, well, yeah, so.",
                    "label": 0
                },
                {
                    "sent": "While we try building ensemble out of these thousands of models, but some of these thousands of models are themselves in samples.",
                    "label": 1
                },
                {
                    "sent": "But that's OK. You can do an ensemble of ensembles and decimal problem, so I mean all we need is that we have a bunch of models.",
                    "label": 1
                },
                {
                    "sent": "Which are accurate and that they're different?",
                    "label": 0
                },
                {
                    "sent": "And it turns out these learning methods are quite different from each other.",
                    "label": 0
                },
                {
                    "sent": "Who's the trees make very different errors in neural Nets, so they're very diverse models, and it turns out a lot of them are very active, so maybe we can just build a smart on top of these things.",
                    "label": 0
                },
                {
                    "sent": "I'm going to spend a few minutes going through a method we've got for doing that, and then I'm going to stop.",
                    "label": 0
                },
                {
                    "sent": "I'm going to point out a problem that and then I'll just speak of Popeye forbids, because, OK.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let me show you some existing ensemble methods that you might try using any people for stacking stacking stuff so popular these days.",
                    "label": 0
                },
                {
                    "sent": "It's basically trying to build a model on top of the other models.",
                    "label": 0
                },
                {
                    "sent": "There's just taking the simple average of all those models with picking the best of the best of the best, which is what we've done on the previous table and then paging model averaging, which basically says put more weight on the walls that appear to be better and less weight on.",
                    "label": 0
                },
                {
                    "sent": "The models appear to be weaker but take an average of all of them.",
                    "label": 0
                },
                {
                    "sent": "None of this works really invented, just picking the best single model and equation model averaging is ever so slightly better and we're working to make that go up, but but basically just picking the best single model, get you what you're going to get.",
                    "label": 0
                },
                {
                    "sent": "So we were discouraged by that.",
                    "label": 0
                },
                {
                    "sent": "'cause we know there's gotta be a way of taking average of these models and getting to performance so, so we've developed.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's me, I'm just going to walk into the office.",
                    "label": 0
                },
                {
                    "sent": "It's easier than reading text, so here's a bunch of balls and we've actually trained thousands of models.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we're trying to optimize the area under the RC curve, so we want this number to be as large as possible on some tests.",
                    "label": 0
                },
                {
                    "sent": "So here we've got the RC of each of these models on that test set, and one of these is best, right?",
                    "label": 0
                },
                {
                    "sent": "So that one has the highest RC, so we're going to greedy forward stepwise feature selection.",
                    "label": 0
                },
                {
                    "sent": "But now we're doing greedy forward stepwise model selection, so we're going.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Best modeling in sambol.",
                    "label": 0
                },
                {
                    "sent": "Now there's just one model in the example.",
                    "label": 0
                },
                {
                    "sent": "We're going to ask.",
                    "label": 0
                },
                {
                    "sent": "Suppose you were to add this model to that model in the ensemble.",
                    "label": 0
                },
                {
                    "sent": "What would be our FCB would be that if we add that all about the RCD could be that this model, it would be that so.",
                    "label": 0
                },
                {
                    "sent": "So this is sort of just trial and error.",
                    "label": 0
                },
                {
                    "sent": "See which of these things if we add it to Gonzalo making Ensemble performed.",
                    "label": 0
                },
                {
                    "sent": "As it turns out, model line if you add it to Model 3 raise the performance even more or not got the best single model like 964.",
                    "label": 0
                },
                {
                    "sent": "This raises it to 9384.",
                    "label": 0
                },
                {
                    "sent": "Play the game again.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We find something that raises an even more and we just keep doing it, so we keep greedily Hill climbing in the performance of the ensemble space.",
                    "label": 0
                },
                {
                    "sent": "Cool thing about this is we can optimize any performance measure, any number you can compute fast enough we can optimize to it because it's just trial and error help, right?",
                    "label": 0
                },
                {
                    "sent": "So you can do anything you don't need.",
                    "label": 0
                },
                {
                    "sent": "You don't need differentiability or anything.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This works extremely well.",
                    "label": 0
                },
                {
                    "sent": "Let me show you three slots in the gun.",
                    "label": 0
                },
                {
                    "sent": "It really does yield some extra performance that just best invest in the masturbation model out.",
                    "label": 0
                },
                {
                    "sent": "You can't do so.",
                    "label": 0
                },
                {
                    "sent": "9777 is a notch above those things.",
                    "label": 0
                },
                {
                    "sent": "If you're going to go to the work of training all of these models to pick the thing that's best, that's a great thing to do.",
                    "label": 0
                },
                {
                    "sent": "You should do that.",
                    "label": 0
                },
                {
                    "sent": "Yes, you want to do a little more.",
                    "label": 0
                },
                {
                    "sent": "Work hard to find an ensemble of these things and you can do a little better.",
                    "label": 0
                },
                {
                    "sent": "And the cool thing is you optimize many metric by doing that, whereas we don't have training ronetta optimized RC.",
                    "label": 0
                },
                {
                    "sent": "But we can optimize ensemble that uses the neural net inside document starts, so that's kind.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Covers really well.",
                    "label": 0
                },
                {
                    "sent": "It works even in the hands of other people who are doing other things, which is kind of nice.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just working.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's very expensive.",
                    "label": 0
                },
                {
                    "sent": "But all the expenses training the thousands of models, it turns out the ensemble selection hillclimbing that I can do in a minute on the laptop.",
                    "label": 0
                },
                {
                    "sent": "So so that's the easy part.",
                    "label": 0
                },
                {
                    "sent": "As long as your metric can be computed quickly.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm just going to tell you there is a problem.",
                    "label": 0
                },
                {
                    "sent": "OK, these are as far as we know the best performing models we know how to train right now, and you know that shouldn't be a surprise wireless rumble.",
                    "label": 0
                },
                {
                    "sent": "They're building on top of things.",
                    "label": 0
                },
                {
                    "sent": "Boosted trees in SPMS in factories around for so, so.",
                    "label": 0
                },
                {
                    "sent": "Of course they're good.",
                    "label": 0
                },
                {
                    "sent": "You know they're standing on the shoulders of Giants, so that makes sense.",
                    "label": 0
                },
                {
                    "sent": "They're very slow big.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Bird models, one of these models has 20,000 decision trees.",
                    "label": 0
                },
                {
                    "sent": "Another thousand decision Trees, 2200 hidden units, thousands of support vectors.",
                    "label": 1
                },
                {
                    "sent": "Basically it takes a GB, destroyed this model and it takes a second or two to execute it.",
                    "label": 0
                },
                {
                    "sent": "On one test case.",
                    "label": 0
                },
                {
                    "sent": "So there are all sorts of problems for which you'll never going to be able to use this model.",
                    "label": 0
                },
                {
                    "sent": "You never put in your PDA while not enough room while you're not going to be able to apply to Google scale problems, there are all sorts of problems.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one of the things we're working on is a way of compressing these models into a much simpler model of.",
                    "label": 1
                },
                {
                    "sent": "A much more compact, faster model that still achieves the same performance as the ensemble, and I think if I'm giving those talk later in the week, that might be what I'll talk about his model compression.",
                    "label": 1
                },
                {
                    "sent": "I'm here in this talk.",
                    "label": 0
                },
                {
                    "sent": "Anyway, I wasn't planning to do it today, it's just a second.",
                    "label": 0
                },
                {
                    "sent": "OK, so they're pushing this out so we could be around this place.",
                    "label": 0
                },
                {
                    "sent": "Be my guest.",
                    "label": 0
                },
                {
                    "sent": "Topic people, so unfortunately it doesn't this time I'm afraid so, but.",
                    "label": 0
                },
                {
                    "sent": "Basically, you got basically put against for president discussions these days he rounds till Friday and then Saturday something good with some free stuff basically Friday.",
                    "label": 0
                },
                {
                    "sent": "Casa at least we're talking, it's not.",
                    "label": 0
                }
            ]
        }
    }
}