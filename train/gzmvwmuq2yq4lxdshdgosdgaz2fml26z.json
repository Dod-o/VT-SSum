{
    "id": "gzmvwmuq2yq4lxdshdgosdgaz2fml26z",
    "title": "Non-Local Evidence for Expert Finding",
    "info": {
        "author": [
            "Krisztian Balog, Dept. of Social Science Informatics, University of Amsterdam"
        ],
        "published": "Nov. 19, 2008",
        "recorded": "October 2008",
        "category": [
            "Top->Computer Science"
        ]
    },
    "url": "http://videolectures.net/cikm08_balog_nlefef/",
    "segmentation": [
        [
            "OK, this should be."
        ],
        [
            "Better now.",
            "OK, welcome.",
            "My name is Christian blog and this is joint work with the Martin Riker.",
            "I start this talk by explaining the title 1st and in doing that I'm going to proceed in backwards order."
        ],
        [
            "So let's see what the expert finding is about.",
            "Expert finding is the task of finding the right person with the appropriate skills and knowledge.",
            "Put it simply, we are given a query and instead of ranking documents, we want to rank people with respect to their expertise.",
            "Given this topic and we refer to these people as candidates in the rest of the talk."
        ],
        [
            "Now moving on to the local evidence for expert finding part.",
            "Existing approaches for expert finding.",
            "Very curvy by computing associations between candidates and topics and these associations are computed based on Co occurrence of terms and names either in the document level or either in text snippets or windows of text.",
            "Since it's a knowledge management session, I find it important to mention that we are not modeling expertise directly as if we don't model the concept of expertise, we simply assume that people strongly associated with the topic are experts on that topic."
        ],
        [
            "OK, so finally the nonlocal evidence there.",
            "That is the aim of this work is to identify and integrate sources of evidence.",
            "Into existing models that are not available locali in an individual page or in text snippet."
        ],
        [
            "The remainder of the talk goes as follows.",
            "1st I'm going to introduce retrieval model that we use for finding experts.",
            "Then I will discuss the experimental setting and then I will identify and discuss ways of integrating these nonlocal features."
        ],
        [
            "We use probabilistic approach for expert finding and state the problem as follows.",
            "What is the probability of a candidate being an expert given a topic?",
            "Vic what we want is compute the probability of a candidate given the query and we can rewrite it using Bayes rule.",
            "I promise I won't use more formulas than necessary.",
            "I won't walk you through all the equations.",
            "You can check this in the paper.",
            "I will just give you.",
            "The intuition is behind these models since our goal is to rank candidates, we can ignore the probability of a query.",
            "And this leaves us with two components, one is.",
            "Probability of candidate.",
            "This is a priori belief that candidate is an expert for any given topic.",
            "Let's query independent part and we have a query dependent part.",
            "How likely the candidate would produce the query."
        ],
        [
            "Next I will discuss how to estimate the probability of a query given a candidate.",
            "The employee generative language modeling approach and under this approach we represent both candidates and the query has probably distributions over terms and refer to these.",
            "I said candidate model and the query model and given these two distributions we measure their similarity using the KL Divergent.",
            "So how can we construct this candidate model?"
        ],
        [
            "Since we are working in a language modeling setting, it is standard to employ smoothing.",
            "I don't want to talk much about smoothing, rather I would like to focus on how to estimate the empirical probability of a term given a candidate.",
            "So the process goes as follows.",
            "We have a set of documents associated with the person from this set of documents, we select document with the probability they given the candidate, and then from this document we sample terms and then by doing that over all documents associated with the person, we obtain the probability distribution.",
            "So this process.",
            "Involves 2."
        ],
        [
            "Probability is to be estimated.",
            "One is what we call the document candidate associations, and at this point I will use.",
            "The simplest possible choice that one could make it is called the Boolean model.",
            "So document and candidate.",
            "Associated if the name of the person is mentioned in the document or not."
        ],
        [
            "The other.",
            "Probability the probability of a term given a document and the candidate can be estimated two ways.",
            "One is document based model.",
            "In prior work we refer to this model of 1.",
            "Here we assume that document and the candidate are conditionally independent and we simply take all terms from the document and associated with the.",
            "Person, the other one.",
            "The proximity based model.",
            "It takes only terms that could occur with the candidate mentions within.",
            "Window of text.",
            "So this was the candidate model."
        ],
        [
            "Now we're moving on to the query model again.",
            "I'm going to present you with with the simplest possible choice.",
            "So given an example query in this case, cane toads.",
            "What we do is assign the probability mass uniformly across the query terms so both terms get the weight 0.5."
        ],
        [
            "OK, next I will.",
            "Discussed the experimental setting just to make matters more concrete, before moving on to the known law."
        ],
        [
            "Killer part.",
            "Views the enterprise track.",
            "Two platform from the tracks.",
            "3 trivia conference 2007.",
            "Addition Document collection is avec role of Syro cider is an Australian science organization.",
            "This collection comprises 307 thousand document.",
            "We have 50 topics with relevance judgment and non trivial part.",
            "Offered the task is identifying candidates people mentioned in document.",
            "I'm not going to discuss this part.",
            "What we can assume at this point that this candidate occurrences are identified and are replaced with a unique ID in documents and can be treated much as like terms."
        ],
        [
            "So again, this is our baseline retrieval model, where we have Boolean document candidate associations.",
            "We have baseline query and we have uniform priors.",
            "So you remember the probability of a candidate I mentioned earlier.",
            "We assume that that is uniform and that simply means that all candidates are equally likely to be experts.",
            "Now I'm going to."
        ],
        [
            "Integrate nonlocal evidence into the same three components.",
            "And this caused before and I will start with the."
        ],
        [
            "Acument candidate associations part.",
            "So intuitively, not all candidate mentions are equally important given document and vice versa.",
            "Not all candidate documents are equally important given a candidate, imagine that person that is associated with many documents.",
            "That may not be really important for one given document, and similarly document that is mentioning many many different names terms from these documents should not contribute too much to the profiles of all these people.",
            "So we want to estimate the strength of the associations based on how many times the candidate is mentioned in the given document and how many other documents this candidate.",
            "Is related to."
        ],
        [
            "One way of looking at this problem is term importance estimation problem where we can view candidates in documents as terms.",
            "Here if user document representation, that means that we document contain only candidate mentions and all other terms are filtered out and then we can use to innovating schemes like TF IDF to estimate this importance this probability.",
            "I don't think I have to talk too much about TF IDF, but one thing remains to be defined.",
            "What is the probability of what is the frequency of a candy?"
        ],
        [
            "Get in a document.",
            "In TF IDF we have this metrics with documents and along one dimension and candidates along the order.",
            "So we can use the number of candidate mentions as the frequency of.",
            "Offer candidate given document or we can use the semantic relatedness.",
            "What I mean by this is we create a language model of the document and we also create a language model of the candidate.",
            "And instead of using simply the number of mentions, we use the KL Divergent.",
            "So if.",
            "The topics that the candidate is likely to talk about are similar to the topics discussed in this document.",
            "Then the document then.",
            "The value will be higher in this cell."
        ],
        [
            "Let's see how it performs.",
            "We have two sets of bloods.",
            "Here reporting on mean average precision, the performance of the document based model on the left hand side and the proximity based model on the right hand side first as to the document based model, they find that using these global statistics for computing document candidates associations improves significantly over the baseline.",
            "Then moving to semantic relatedness improves further, but.",
            "Only marginally, and it's not significant.",
            "In case of the proximity based model we observe minor but known significance improvement.",
            "This is not a surprise since in case of the proximity based model we only add text to the candidate model that.",
            "She runs the candidate mentions and this window size is relatively small in our setting.",
            "It's 125 words, so we would expect to see more differences.",
            "More improvement when using larger."
        ],
        [
            "Window size.",
            "Next, moving on to the query models.",
            "I.",
            "Would like to mention another.",
            "Feature of the Trek Enterprise Track in 2007.",
            "They simulated a type of click based system.",
            "That means that a number of key documents were provided along with the topic description.",
            "And.",
            "Here I'm not going into details.",
            "In previous work we proposed method for estimating very models using these example documents, and the intuition is that these example documents provide additional aspects to the original query."
        ],
        [
            "Will give you an example to to make it more concrete, so this is an example topic where the query is cane toads and we have a narrative that we don't use and this talks about these frogs introduced into Australia and then the last sentence is interesting so.",
            "Wish good terms should be well past management, biological control and so on.",
            "And then in addition we have three example pages.",
            "We see the ideas of these pages."
        ],
        [
            "So again, this is a baseline query and using this method what we get is something like this, it's.",
            "More fine grained allocation of the probability mass across the terms, and indeed we see pass animal management.",
            "So the terms that we want."
        ],
        [
            "To see.",
            "Looking at the results when we move to the extended query models, we observe improvement across the board and these improvements are significant in all cases."
        ],
        [
            "Finally.",
            "The third type of local evidence is what we incorporate in forms of candidate priors.",
            "To make to explain it, I will give an example.",
            "Again, this is a typical page from Syro.",
            "Talking about genome health and we have primary contact in this page who is the director of some Department and here is another page about food science and again we have a contact in case a science communicator.",
            "Task definition they originally wanted systems to return key Contacts and key Contacts are experts and not science communicators.",
            "Science communicators are the users of the system who create these.",
            "Overview Pages and for these overview pages they also wanted the key Contacts, so they don't want to see each other return."
        ],
        [
            "So.",
            "His candidate priors.",
            "Our way of encoding the organizational knowledge.",
            "Encoding these Harris ticks.",
            "In this case we did is extracted names and positions from these contact boxes and identified science communicators based on position information.",
            "Science communicators are often code communication Officer Manager, Advisor, and we simply assign zero prior to this paper."
        ],
        [
            "Who we applied this science communicators prior only to the best performing configurations.",
            "So the.",
            "The right hand side it's shown in yellow and we see that it improves further on in case of both models.",
            "So one question one might ask is."
        ],
        [
            "How good is it in absolute terms, in this table we compare our baseline models to numbers reported at track and reported in the literature so far, and.",
            "Conclusions is that the baselines are very competitive.",
            "It's important to note that there are three types of runs and track.",
            "The automatic runs that use only the query field.",
            "The feedback runs that use the example documents, the sample documents and the manual runs that.",
            "Queries are constructed manually, so our automatic run is close to best actually."
        ],
        [
            "And then if we if we start adding these nonlocal features then we see that the improvement gained with either nicely add up and.",
            "And result in.",
            "Best performance are on this on this task."
        ],
        [
            "So the conclusion is this very identified in number of non local sources for.",
            "Expertise for expert finding and then we complemented existing methods to incorporate these, and we showed significant improvements over competitive baseline."
        ],
        [
            "As do further work we can, we could also look at non local evidence within document and make use of the Internet."
        ],
        [
            "Document structure.",
            "What I mean here is.",
            "We haven't used.",
            "Blocks on this page the primary contact the editors choice, and we also have the.",
            "The menus and the header and the footer, which are not that meaningful is a natural next step would be to calculate these associations between candidates and.",
            "And blocks of text."
        ],
        [
            "That concludes my talk.",
            "OK, any questions?",
            "Have you ever thought about?",
            "You're bored.",
            "This is a very nice submissions being thousand almonds.",
            "That's my sophomore year last year, so you can also find this.",
            "But I need relationships between the.",
            "He is so large in the dark.",
            "Well, one.",
            "One way of incorporating semantics is what we do with these semantic document candidate associations.",
            "So not just simply looking at the presence or absence or the number of times these are mentioned, but actually looking at.",
            "Model.",
            "Looking deeper, looking at the terms.",
            "Do the language models.",
            "Think about that for one language, for I mean.",
            "Subway marking up knowledge which inside with your friends.",
            "I'm not sure what to what you mean by that.",
            "If you will is only at mark up the desserts involving assisting the powerful information with the airport, so maybe you can also as well.",
            "Anything else pays the normalization ships inside inside.",
            "For example, your documents that you you are in the versus Norman knows about, like some foreign object, again without me.",
            "I mean, in the in I love my discarding itaska you search for some rules about programming.",
            "In general, by using symantics you can like the lovely love station that maybe you were purchased another counter.",
            "Yeah, yeah, that's right.",
            "Well, we try to make up for this by using these credit models.",
            "So in in your example we would expect.",
            "Let's say we use the proximity based approach.",
            "So not only the.",
            "Object oriented programming.",
            "These terms should be present with higher weight in the in the in the candidates language model and hopefully the same terms set of terms they should be in in the query model but.",
            "No, we didn't do it yet.",
            "What you are referring to.",
            "OK, only one more question.",
            "I like Santa speakers.",
            "And the next paper is the."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, this should be.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Better now.",
                    "label": 0
                },
                {
                    "sent": "OK, welcome.",
                    "label": 0
                },
                {
                    "sent": "My name is Christian blog and this is joint work with the Martin Riker.",
                    "label": 0
                },
                {
                    "sent": "I start this talk by explaining the title 1st and in doing that I'm going to proceed in backwards order.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's see what the expert finding is about.",
                    "label": 0
                },
                {
                    "sent": "Expert finding is the task of finding the right person with the appropriate skills and knowledge.",
                    "label": 1
                },
                {
                    "sent": "Put it simply, we are given a query and instead of ranking documents, we want to rank people with respect to their expertise.",
                    "label": 0
                },
                {
                    "sent": "Given this topic and we refer to these people as candidates in the rest of the talk.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now moving on to the local evidence for expert finding part.",
                    "label": 0
                },
                {
                    "sent": "Existing approaches for expert finding.",
                    "label": 1
                },
                {
                    "sent": "Very curvy by computing associations between candidates and topics and these associations are computed based on Co occurrence of terms and names either in the document level or either in text snippets or windows of text.",
                    "label": 0
                },
                {
                    "sent": "Since it's a knowledge management session, I find it important to mention that we are not modeling expertise directly as if we don't model the concept of expertise, we simply assume that people strongly associated with the topic are experts on that topic.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so finally the nonlocal evidence there.",
                    "label": 1
                },
                {
                    "sent": "That is the aim of this work is to identify and integrate sources of evidence.",
                    "label": 0
                },
                {
                    "sent": "Into existing models that are not available locali in an individual page or in text snippet.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The remainder of the talk goes as follows.",
                    "label": 0
                },
                {
                    "sent": "1st I'm going to introduce retrieval model that we use for finding experts.",
                    "label": 1
                },
                {
                    "sent": "Then I will discuss the experimental setting and then I will identify and discuss ways of integrating these nonlocal features.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We use probabilistic approach for expert finding and state the problem as follows.",
                    "label": 0
                },
                {
                    "sent": "What is the probability of a candidate being an expert given a topic?",
                    "label": 1
                },
                {
                    "sent": "Vic what we want is compute the probability of a candidate given the query and we can rewrite it using Bayes rule.",
                    "label": 0
                },
                {
                    "sent": "I promise I won't use more formulas than necessary.",
                    "label": 0
                },
                {
                    "sent": "I won't walk you through all the equations.",
                    "label": 0
                },
                {
                    "sent": "You can check this in the paper.",
                    "label": 0
                },
                {
                    "sent": "I will just give you.",
                    "label": 0
                },
                {
                    "sent": "The intuition is behind these models since our goal is to rank candidates, we can ignore the probability of a query.",
                    "label": 0
                },
                {
                    "sent": "And this leaves us with two components, one is.",
                    "label": 0
                },
                {
                    "sent": "Probability of candidate.",
                    "label": 1
                },
                {
                    "sent": "This is a priori belief that candidate is an expert for any given topic.",
                    "label": 0
                },
                {
                    "sent": "Let's query independent part and we have a query dependent part.",
                    "label": 1
                },
                {
                    "sent": "How likely the candidate would produce the query.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Next I will discuss how to estimate the probability of a query given a candidate.",
                    "label": 0
                },
                {
                    "sent": "The employee generative language modeling approach and under this approach we represent both candidates and the query has probably distributions over terms and refer to these.",
                    "label": 1
                },
                {
                    "sent": "I said candidate model and the query model and given these two distributions we measure their similarity using the KL Divergent.",
                    "label": 0
                },
                {
                    "sent": "So how can we construct this candidate model?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Since we are working in a language modeling setting, it is standard to employ smoothing.",
                    "label": 0
                },
                {
                    "sent": "I don't want to talk much about smoothing, rather I would like to focus on how to estimate the empirical probability of a term given a candidate.",
                    "label": 0
                },
                {
                    "sent": "So the process goes as follows.",
                    "label": 0
                },
                {
                    "sent": "We have a set of documents associated with the person from this set of documents, we select document with the probability they given the candidate, and then from this document we sample terms and then by doing that over all documents associated with the person, we obtain the probability distribution.",
                    "label": 0
                },
                {
                    "sent": "So this process.",
                    "label": 0
                },
                {
                    "sent": "Involves 2.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Probability is to be estimated.",
                    "label": 0
                },
                {
                    "sent": "One is what we call the document candidate associations, and at this point I will use.",
                    "label": 0
                },
                {
                    "sent": "The simplest possible choice that one could make it is called the Boolean model.",
                    "label": 0
                },
                {
                    "sent": "So document and candidate.",
                    "label": 0
                },
                {
                    "sent": "Associated if the name of the person is mentioned in the document or not.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The other.",
                    "label": 0
                },
                {
                    "sent": "Probability the probability of a term given a document and the candidate can be estimated two ways.",
                    "label": 0
                },
                {
                    "sent": "One is document based model.",
                    "label": 0
                },
                {
                    "sent": "In prior work we refer to this model of 1.",
                    "label": 0
                },
                {
                    "sent": "Here we assume that document and the candidate are conditionally independent and we simply take all terms from the document and associated with the.",
                    "label": 0
                },
                {
                    "sent": "Person, the other one.",
                    "label": 0
                },
                {
                    "sent": "The proximity based model.",
                    "label": 0
                },
                {
                    "sent": "It takes only terms that could occur with the candidate mentions within.",
                    "label": 0
                },
                {
                    "sent": "Window of text.",
                    "label": 0
                },
                {
                    "sent": "So this was the candidate model.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we're moving on to the query model again.",
                    "label": 1
                },
                {
                    "sent": "I'm going to present you with with the simplest possible choice.",
                    "label": 0
                },
                {
                    "sent": "So given an example query in this case, cane toads.",
                    "label": 1
                },
                {
                    "sent": "What we do is assign the probability mass uniformly across the query terms so both terms get the weight 0.5.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, next I will.",
                    "label": 0
                },
                {
                    "sent": "Discussed the experimental setting just to make matters more concrete, before moving on to the known law.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Killer part.",
                    "label": 0
                },
                {
                    "sent": "Views the enterprise track.",
                    "label": 0
                },
                {
                    "sent": "Two platform from the tracks.",
                    "label": 0
                },
                {
                    "sent": "3 trivia conference 2007.",
                    "label": 0
                },
                {
                    "sent": "Addition Document collection is avec role of Syro cider is an Australian science organization.",
                    "label": 0
                },
                {
                    "sent": "This collection comprises 307 thousand document.",
                    "label": 0
                },
                {
                    "sent": "We have 50 topics with relevance judgment and non trivial part.",
                    "label": 0
                },
                {
                    "sent": "Offered the task is identifying candidates people mentioned in document.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to discuss this part.",
                    "label": 0
                },
                {
                    "sent": "What we can assume at this point that this candidate occurrences are identified and are replaced with a unique ID in documents and can be treated much as like terms.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So again, this is our baseline retrieval model, where we have Boolean document candidate associations.",
                    "label": 0
                },
                {
                    "sent": "We have baseline query and we have uniform priors.",
                    "label": 1
                },
                {
                    "sent": "So you remember the probability of a candidate I mentioned earlier.",
                    "label": 0
                },
                {
                    "sent": "We assume that that is uniform and that simply means that all candidates are equally likely to be experts.",
                    "label": 1
                },
                {
                    "sent": "Now I'm going to.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Integrate nonlocal evidence into the same three components.",
                    "label": 0
                },
                {
                    "sent": "And this caused before and I will start with the.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Acument candidate associations part.",
                    "label": 0
                },
                {
                    "sent": "So intuitively, not all candidate mentions are equally important given document and vice versa.",
                    "label": 0
                },
                {
                    "sent": "Not all candidate documents are equally important given a candidate, imagine that person that is associated with many documents.",
                    "label": 0
                },
                {
                    "sent": "That may not be really important for one given document, and similarly document that is mentioning many many different names terms from these documents should not contribute too much to the profiles of all these people.",
                    "label": 0
                },
                {
                    "sent": "So we want to estimate the strength of the associations based on how many times the candidate is mentioned in the given document and how many other documents this candidate.",
                    "label": 1
                },
                {
                    "sent": "Is related to.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One way of looking at this problem is term importance estimation problem where we can view candidates in documents as terms.",
                    "label": 0
                },
                {
                    "sent": "Here if user document representation, that means that we document contain only candidate mentions and all other terms are filtered out and then we can use to innovating schemes like TF IDF to estimate this importance this probability.",
                    "label": 1
                },
                {
                    "sent": "I don't think I have to talk too much about TF IDF, but one thing remains to be defined.",
                    "label": 0
                },
                {
                    "sent": "What is the probability of what is the frequency of a candy?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Get in a document.",
                    "label": 0
                },
                {
                    "sent": "In TF IDF we have this metrics with documents and along one dimension and candidates along the order.",
                    "label": 0
                },
                {
                    "sent": "So we can use the number of candidate mentions as the frequency of.",
                    "label": 1
                },
                {
                    "sent": "Offer candidate given document or we can use the semantic relatedness.",
                    "label": 1
                },
                {
                    "sent": "What I mean by this is we create a language model of the document and we also create a language model of the candidate.",
                    "label": 1
                },
                {
                    "sent": "And instead of using simply the number of mentions, we use the KL Divergent.",
                    "label": 0
                },
                {
                    "sent": "So if.",
                    "label": 0
                },
                {
                    "sent": "The topics that the candidate is likely to talk about are similar to the topics discussed in this document.",
                    "label": 0
                },
                {
                    "sent": "Then the document then.",
                    "label": 0
                },
                {
                    "sent": "The value will be higher in this cell.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's see how it performs.",
                    "label": 0
                },
                {
                    "sent": "We have two sets of bloods.",
                    "label": 0
                },
                {
                    "sent": "Here reporting on mean average precision, the performance of the document based model on the left hand side and the proximity based model on the right hand side first as to the document based model, they find that using these global statistics for computing document candidates associations improves significantly over the baseline.",
                    "label": 0
                },
                {
                    "sent": "Then moving to semantic relatedness improves further, but.",
                    "label": 0
                },
                {
                    "sent": "Only marginally, and it's not significant.",
                    "label": 0
                },
                {
                    "sent": "In case of the proximity based model we observe minor but known significance improvement.",
                    "label": 0
                },
                {
                    "sent": "This is not a surprise since in case of the proximity based model we only add text to the candidate model that.",
                    "label": 0
                },
                {
                    "sent": "She runs the candidate mentions and this window size is relatively small in our setting.",
                    "label": 0
                },
                {
                    "sent": "It's 125 words, so we would expect to see more differences.",
                    "label": 0
                },
                {
                    "sent": "More improvement when using larger.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Window size.",
                    "label": 0
                },
                {
                    "sent": "Next, moving on to the query models.",
                    "label": 1
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "Would like to mention another.",
                    "label": 1
                },
                {
                    "sent": "Feature of the Trek Enterprise Track in 2007.",
                    "label": 0
                },
                {
                    "sent": "They simulated a type of click based system.",
                    "label": 1
                },
                {
                    "sent": "That means that a number of key documents were provided along with the topic description.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Here I'm not going into details.",
                    "label": 0
                },
                {
                    "sent": "In previous work we proposed method for estimating very models using these example documents, and the intuition is that these example documents provide additional aspects to the original query.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Will give you an example to to make it more concrete, so this is an example topic where the query is cane toads and we have a narrative that we don't use and this talks about these frogs introduced into Australia and then the last sentence is interesting so.",
                    "label": 1
                },
                {
                    "sent": "Wish good terms should be well past management, biological control and so on.",
                    "label": 0
                },
                {
                    "sent": "And then in addition we have three example pages.",
                    "label": 0
                },
                {
                    "sent": "We see the ideas of these pages.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So again, this is a baseline query and using this method what we get is something like this, it's.",
                    "label": 0
                },
                {
                    "sent": "More fine grained allocation of the probability mass across the terms, and indeed we see pass animal management.",
                    "label": 0
                },
                {
                    "sent": "So the terms that we want.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To see.",
                    "label": 0
                },
                {
                    "sent": "Looking at the results when we move to the extended query models, we observe improvement across the board and these improvements are significant in all cases.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finally.",
                    "label": 0
                },
                {
                    "sent": "The third type of local evidence is what we incorporate in forms of candidate priors.",
                    "label": 0
                },
                {
                    "sent": "To make to explain it, I will give an example.",
                    "label": 0
                },
                {
                    "sent": "Again, this is a typical page from Syro.",
                    "label": 0
                },
                {
                    "sent": "Talking about genome health and we have primary contact in this page who is the director of some Department and here is another page about food science and again we have a contact in case a science communicator.",
                    "label": 0
                },
                {
                    "sent": "Task definition they originally wanted systems to return key Contacts and key Contacts are experts and not science communicators.",
                    "label": 0
                },
                {
                    "sent": "Science communicators are the users of the system who create these.",
                    "label": 0
                },
                {
                    "sent": "Overview Pages and for these overview pages they also wanted the key Contacts, so they don't want to see each other return.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "His candidate priors.",
                    "label": 0
                },
                {
                    "sent": "Our way of encoding the organizational knowledge.",
                    "label": 0
                },
                {
                    "sent": "Encoding these Harris ticks.",
                    "label": 0
                },
                {
                    "sent": "In this case we did is extracted names and positions from these contact boxes and identified science communicators based on position information.",
                    "label": 1
                },
                {
                    "sent": "Science communicators are often code communication Officer Manager, Advisor, and we simply assign zero prior to this paper.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Who we applied this science communicators prior only to the best performing configurations.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "The right hand side it's shown in yellow and we see that it improves further on in case of both models.",
                    "label": 0
                },
                {
                    "sent": "So one question one might ask is.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How good is it in absolute terms, in this table we compare our baseline models to numbers reported at track and reported in the literature so far, and.",
                    "label": 1
                },
                {
                    "sent": "Conclusions is that the baselines are very competitive.",
                    "label": 0
                },
                {
                    "sent": "It's important to note that there are three types of runs and track.",
                    "label": 0
                },
                {
                    "sent": "The automatic runs that use only the query field.",
                    "label": 0
                },
                {
                    "sent": "The feedback runs that use the example documents, the sample documents and the manual runs that.",
                    "label": 0
                },
                {
                    "sent": "Queries are constructed manually, so our automatic run is close to best actually.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then if we if we start adding these nonlocal features then we see that the improvement gained with either nicely add up and.",
                    "label": 0
                },
                {
                    "sent": "And result in.",
                    "label": 0
                },
                {
                    "sent": "Best performance are on this on this task.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the conclusion is this very identified in number of non local sources for.",
                    "label": 0
                },
                {
                    "sent": "Expertise for expert finding and then we complemented existing methods to incorporate these, and we showed significant improvements over competitive baseline.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As do further work we can, we could also look at non local evidence within document and make use of the Internet.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Document structure.",
                    "label": 0
                },
                {
                    "sent": "What I mean here is.",
                    "label": 0
                },
                {
                    "sent": "We haven't used.",
                    "label": 0
                },
                {
                    "sent": "Blocks on this page the primary contact the editors choice, and we also have the.",
                    "label": 0
                },
                {
                    "sent": "The menus and the header and the footer, which are not that meaningful is a natural next step would be to calculate these associations between candidates and.",
                    "label": 0
                },
                {
                    "sent": "And blocks of text.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That concludes my talk.",
                    "label": 0
                },
                {
                    "sent": "OK, any questions?",
                    "label": 0
                },
                {
                    "sent": "Have you ever thought about?",
                    "label": 0
                },
                {
                    "sent": "You're bored.",
                    "label": 0
                },
                {
                    "sent": "This is a very nice submissions being thousand almonds.",
                    "label": 0
                },
                {
                    "sent": "That's my sophomore year last year, so you can also find this.",
                    "label": 0
                },
                {
                    "sent": "But I need relationships between the.",
                    "label": 0
                },
                {
                    "sent": "He is so large in the dark.",
                    "label": 0
                },
                {
                    "sent": "Well, one.",
                    "label": 0
                },
                {
                    "sent": "One way of incorporating semantics is what we do with these semantic document candidate associations.",
                    "label": 0
                },
                {
                    "sent": "So not just simply looking at the presence or absence or the number of times these are mentioned, but actually looking at.",
                    "label": 0
                },
                {
                    "sent": "Model.",
                    "label": 0
                },
                {
                    "sent": "Looking deeper, looking at the terms.",
                    "label": 0
                },
                {
                    "sent": "Do the language models.",
                    "label": 0
                },
                {
                    "sent": "Think about that for one language, for I mean.",
                    "label": 0
                },
                {
                    "sent": "Subway marking up knowledge which inside with your friends.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure what to what you mean by that.",
                    "label": 0
                },
                {
                    "sent": "If you will is only at mark up the desserts involving assisting the powerful information with the airport, so maybe you can also as well.",
                    "label": 0
                },
                {
                    "sent": "Anything else pays the normalization ships inside inside.",
                    "label": 0
                },
                {
                    "sent": "For example, your documents that you you are in the versus Norman knows about, like some foreign object, again without me.",
                    "label": 0
                },
                {
                    "sent": "I mean, in the in I love my discarding itaska you search for some rules about programming.",
                    "label": 0
                },
                {
                    "sent": "In general, by using symantics you can like the lovely love station that maybe you were purchased another counter.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, that's right.",
                    "label": 0
                },
                {
                    "sent": "Well, we try to make up for this by using these credit models.",
                    "label": 0
                },
                {
                    "sent": "So in in your example we would expect.",
                    "label": 0
                },
                {
                    "sent": "Let's say we use the proximity based approach.",
                    "label": 0
                },
                {
                    "sent": "So not only the.",
                    "label": 0
                },
                {
                    "sent": "Object oriented programming.",
                    "label": 0
                },
                {
                    "sent": "These terms should be present with higher weight in the in the in the candidates language model and hopefully the same terms set of terms they should be in in the query model but.",
                    "label": 0
                },
                {
                    "sent": "No, we didn't do it yet.",
                    "label": 0
                },
                {
                    "sent": "What you are referring to.",
                    "label": 0
                },
                {
                    "sent": "OK, only one more question.",
                    "label": 0
                },
                {
                    "sent": "I like Santa speakers.",
                    "label": 0
                },
                {
                    "sent": "And the next paper is the.",
                    "label": 0
                }
            ]
        }
    }
}