{
    "id": "r2lqfo6etemxrecpvvv2twy6pyicutac",
    "title": "Probabilistic Deterministic Infinite Automata",
    "info": {
        "author": [
            "David Pfau, Neuroscience, Columbia University Medical Center, Columbia University"
        ],
        "published": "March 25, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/nips2010_pfau_pdi/",
    "segmentation": [
        [
            "Thank you very much in this work.",
            "We take a nonparametric Bayesian approach to discrete sequence modeling.",
            "By that we mean given a string of binary digits or DNA or text, we want to assign a probability to each.",
            "This can be used for conditional prediction.",
            "So given the string of fourscore and seven, how likely is it that the next word is years?",
            "Or we can also use it to assess the typicality of strings which can be used in clustering.",
            "As well as other applications.",
            "So we call our model the probabilistic deterministic infinite automata and."
        ],
        [
            "It's a unattractive tradeoff between generalization performance, space complexity of the model, and speed of prediction, particularly compared to hidden Markov models or NTH order Markov models, also known as N gram models.",
            "This is illustrated qualitatively in the figures below.",
            "For both of them, the sweet spot is to the lower right.",
            "Um, comparing generalization and space complexity.",
            "The PDA does as well as some ngram models, not as well as the best, but with significantly lower space complexity and forward prediction is faster than for hidden Markov MoD."
        ],
        [
            "So what is this thing that we're learning, given a finite amount of training data learned model can be approximated as a finite mixture of probabilistic deterministic finite automata.",
            "Roughly, this can be thought of as a hidden Markov model which has only one possible path through the states given data.",
            "This is what makes forward prediction faster, so the set of PDF is.",
            "Is a strict subset of the space of hidden Markov models, but it is strictly larger than the space of ngram models.",
            "This is illustrated in the figure on the right.",
            "I want to emphasize that this is not a graphical model, it is an automata.",
            "In particular, it is a Markov model over binary sequences, so we have states we have transitions between those states.",
            "Given a state, or I'll say the state zero, we have a probability illustrated in red of committing a particular symbol.",
            "This is the probabilistic part.",
            "However, given a symbol emitted in a particular state, the transition is deterministic, so that's the second word in PDF A. Um?",
            "So our model.",
            "Is a PDF A with an infinite number of states?",
            "That's the third word in the model, but it is biased towards state re use in such a way that inference is still tractable."
        ],
        [
            "So how does it do when trained on simple PDF as it recovers their topology accurately when trained character by character?",
            "On natural language it generalizes as well as a third order Markov model, but with only a 10th, the number of states.",
            "And averaging the predictions from multiple learned PDF phase leads to better prediction than using a single PDF A using only one topology.",
            "So to recap, we take a nonparametric Bayesian approach to learning discrete sequence models.",
            "Forward prediction is faster than hidden Markov models and we think it presents an attractive tradeoff between model size and generalization performance.",
            "For more details, check out poster 81.",
            "Now let's eat thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thank you very much in this work.",
                    "label": 0
                },
                {
                    "sent": "We take a nonparametric Bayesian approach to discrete sequence modeling.",
                    "label": 1
                },
                {
                    "sent": "By that we mean given a string of binary digits or DNA or text, we want to assign a probability to each.",
                    "label": 1
                },
                {
                    "sent": "This can be used for conditional prediction.",
                    "label": 0
                },
                {
                    "sent": "So given the string of fourscore and seven, how likely is it that the next word is years?",
                    "label": 0
                },
                {
                    "sent": "Or we can also use it to assess the typicality of strings which can be used in clustering.",
                    "label": 0
                },
                {
                    "sent": "As well as other applications.",
                    "label": 0
                },
                {
                    "sent": "So we call our model the probabilistic deterministic infinite automata and.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's a unattractive tradeoff between generalization performance, space complexity of the model, and speed of prediction, particularly compared to hidden Markov models or NTH order Markov models, also known as N gram models.",
                    "label": 1
                },
                {
                    "sent": "This is illustrated qualitatively in the figures below.",
                    "label": 0
                },
                {
                    "sent": "For both of them, the sweet spot is to the lower right.",
                    "label": 0
                },
                {
                    "sent": "Um, comparing generalization and space complexity.",
                    "label": 0
                },
                {
                    "sent": "The PDA does as well as some ngram models, not as well as the best, but with significantly lower space complexity and forward prediction is faster than for hidden Markov MoD.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what is this thing that we're learning, given a finite amount of training data learned model can be approximated as a finite mixture of probabilistic deterministic finite automata.",
                    "label": 0
                },
                {
                    "sent": "Roughly, this can be thought of as a hidden Markov model which has only one possible path through the states given data.",
                    "label": 1
                },
                {
                    "sent": "This is what makes forward prediction faster, so the set of PDF is.",
                    "label": 0
                },
                {
                    "sent": "Is a strict subset of the space of hidden Markov models, but it is strictly larger than the space of ngram models.",
                    "label": 0
                },
                {
                    "sent": "This is illustrated in the figure on the right.",
                    "label": 0
                },
                {
                    "sent": "I want to emphasize that this is not a graphical model, it is an automata.",
                    "label": 0
                },
                {
                    "sent": "In particular, it is a Markov model over binary sequences, so we have states we have transitions between those states.",
                    "label": 0
                },
                {
                    "sent": "Given a state, or I'll say the state zero, we have a probability illustrated in red of committing a particular symbol.",
                    "label": 0
                },
                {
                    "sent": "This is the probabilistic part.",
                    "label": 0
                },
                {
                    "sent": "However, given a symbol emitted in a particular state, the transition is deterministic, so that's the second word in PDF A. Um?",
                    "label": 0
                },
                {
                    "sent": "So our model.",
                    "label": 0
                },
                {
                    "sent": "Is a PDF A with an infinite number of states?",
                    "label": 0
                },
                {
                    "sent": "That's the third word in the model, but it is biased towards state re use in such a way that inference is still tractable.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how does it do when trained on simple PDF as it recovers their topology accurately when trained character by character?",
                    "label": 0
                },
                {
                    "sent": "On natural language it generalizes as well as a third order Markov model, but with only a 10th, the number of states.",
                    "label": 1
                },
                {
                    "sent": "And averaging the predictions from multiple learned PDF phase leads to better prediction than using a single PDF A using only one topology.",
                    "label": 0
                },
                {
                    "sent": "So to recap, we take a nonparametric Bayesian approach to learning discrete sequence models.",
                    "label": 0
                },
                {
                    "sent": "Forward prediction is faster than hidden Markov models and we think it presents an attractive tradeoff between model size and generalization performance.",
                    "label": 1
                },
                {
                    "sent": "For more details, check out poster 81.",
                    "label": 0
                },
                {
                    "sent": "Now let's eat thank you.",
                    "label": 0
                }
            ]
        }
    }
}