{
    "id": "udyu34g7ruktk2icaihlcx7dg7u5wn3n",
    "title": "Experiments with Non-parametric Topic Models",
    "info": {
        "author": [
            "Swapnil Mishra, NICTA, Australia's ICT Research Centre of Excellence",
            "Wray Buntine, NICTA, Australia's ICT Research Centre of Excellence"
        ],
        "published": "Oct. 7, 2014",
        "recorded": "August 2014",
        "category": [
            "Top->Computer Science->Data Mining",
            "Top->Computer Science->Knowledge Extraction"
        ]
    },
    "url": "http://videolectures.net/kdd2014_buntine_mishra_topic_models/",
    "segmentation": [
        [
            "I'm Ray Benton and this is Swapnil Mishra.",
            "Garments with non parametric topic models?",
            "Um?",
            "G."
        ],
        [
            "We're hiring so good time for Nadra.",
            "Our net ioni set a good example yesterday."
        ],
        [
            "So background."
        ],
        [
            "So you know topic models are good for discovering hidden themes.",
            "We've seen some good examples.",
            "Light and reallocation is the best one.",
            "But recent research is being all people are developing different ways of improving topic models."
        ],
        [
            "And my question is why should you care?",
            "You know you can download Mal.",
            "It works pretty well.",
            "It's a good system, so why bother doing a bit better?"
        ],
        [
            "Well, one one reason we have is that there's lots of different topic models people are trying.",
            "It's no longer just simple topic models, so for instance, this is a model for segmentation of documents.",
            "Works very well.",
            "One of the best, but in the middle is a topic model, so there's all different tasks we have.",
            "We want high Fidelity topic models."
        ],
        [
            "Secondly, you can build huge amount of topics.",
            "So for 1000 documents you can build 100 topics on it and.",
            "When you've got that Fidelity, people are exploring visualization to to look at it, and so there's advantages in infidelity."
        ],
        [
            "So one of the things in our model is we're introducing or where using a technical burstiness that we got from Doyle and Elkin they published in 2009.",
            "So this is just to explain what burstiness is.",
            "So at the top you see snippet of a news article at the bottom.",
            "It's been bagged up and the thing to notice is that ignoring the stop words, the word cabinet appears twice.",
            "Now if you were to just to get a.",
            "A small selection of words.",
            "How often would a word appear twice?",
            "It's just not going to happen for a somewhat infrequent word like cabinet.",
            "So what's happening is for topical documents, for news articles.",
            "For typical things, there is a lot of repetition in there.",
            "And in fact, that's what TF IDF theory is all about in information retrieval is dealing with this single burstiness.",
            "So Doyle and Elkin discovered if they put burstiness into their topic models, it led to a dramatic improvement, but the algorithms weren't there, so so not a lot of people have used it since.",
            "Here's some of the previous work, and with color coded it green is the stuff that really we're building on, and blue is some of the stuff we're comparing against.",
            "And I'll get to our own sampling method."
        ],
        [
            "In a bit, but there's been a really good sequence of work in, in particular hierarchal delay process LDA, so that's one of the main things we're comparing against."
        ],
        [
            "So one thing we've done is is better sampling methods for hierarchical display processes.",
            "So we don't use the Chinese restaurant process or the the tables that people usually use, and we see quite often in conferences.",
            "Now we've developed a method that Chang Yueqin published and is back in 2011 at EC MLP KDD and that's really good because it doesn't require dynamic memory.",
            "It has more rapid mixing and in general it leads to much better models.",
            "In fact, originally it led to such better models that I thought that you might, as original theory was wrong because we were always better than him.",
            "I thought consistently must be something wrong with his algorithm.",
            "Stupid me actually thinking that UI TE would have errors in his statistics."
        ],
        [
            "So.",
            "Let's have a look at our model."
        ],
        [
            "Um?",
            "We start off with original LDA and this is where you've got symmetric.",
            "Darish lies on the two sides."
        ],
        [
            "Odds what a number of groups did, but the best known is Wallach and no put in asymmetric on the the document proportion side and that's implemented in mallet.",
            "Which means there's a version of truncated HDP LDA in Mallet and it's been one of the best implementations for quite awhile."
        ],
        [
            "Then you've got this full hierarchical display process, LDA, which is a nonparametric version of LDA.",
            "Whether you can have an expandable number of topics.",
            "Now."
        ],
        [
            "In 2010, Sato and colleagues put the same nandish same nonparametric machinery, moved it from the from the topic proportion side down to the word side.",
            "There you see on the right is the non parametric modeling of the the actual topics, the word doc, the topic by word matrix itself.",
            "Now that's a bit we've got that as a Pitman Yor rather than a Dirichlet process."
        ],
        [
            "So now we've got one more bit to add, which is the burstiness part.",
            "When I first saw this, I thought Charles Elkan was crazy, and later on I realized he was brilliant for having Bob Doyle or whoever thought of it.",
            "And what this does is each topic has an instance.",
            "In the actual document which modifies it, and this deals with the burstiness very nicely."
        ],
        [
            "So that's our model."
        ],
        [
            "Do some comments on the burstiness, the modeling we do to make that work.",
            "It's written up well in the paper.",
            "I'm not going to explain the algorithm here.",
            "It basically acts as a front end to any.",
            "Any Gibbs sampler for an LDA style model, so we've added it to dynamic topic models.",
            "We've added it to an author author topic model.",
            "And it's quite simple to add what basically you take your Gibbs sampler and you plug it into a function and this will pop.",
            "Bursting this on the top.",
            "But there's a lot of trickery in the implementation, and I'd, um, I'd hate anyone to have to look at my code or modify it.",
            "It's pretty hairy stuff."
        ],
        [
            "So there we go.",
            "That's the first part.",
            "Now Swapnil will have a chat on the experiments we've done in this, so now I'll talk about the experiments and how we compared against 'cause the one of the major things that we have in we had in our mind was that we should have a full process of comparing in what's in there and how we are.",
            "And where are we heading home to so."
        ],
        [
            "1st just have a look at the runtime characteristics, so the basic idea was we wanted to see if you're building heavy model.",
            "How heavy are your model?",
            "Is how much time it takes and we also wanted to see scalability.",
            "So the data set that we use here for the runtime sectors is the latimes data set from the from the track CDs.",
            "So here if you see in general like NB, LDA is our full model and the things with the burst out the burstiness model related to all models.",
            "So like.",
            "Burst idea stands for business version of Indian Everything.",
            "Now if you see the business version is almost like 10 is like it takes.",
            "It runs with more 50.",
            "It takes 50% more time and the memory increase that's in there is not much like if you as you have seen in the model.",
            "If you generally see you would have thought that the memory would have increased by like by two folds or like a force but that doesn't happen which is good and the other thing that we observed was.",
            "Like you had an online HD version which was supposed to run online version and it's supposed to be fast, but actually to go through the whole latimes data set.",
            "It just takes much more time.",
            "Although of late we realize that there is a C++ version which is a bit faster than the normal code, but it is still not as fast as this."
        ],
        [
            "Now talking about, but those were only runtime data success.",
            "Now what you need to see is how you are going to have your performance.",
            "So for our performance we used the public city and we use the perplexity as being mentioned by the volatile in their 2009 paper and the other metric that we used is a PMI which is a pointwise mutual information.",
            "It was developed by new material in 2010 and it's basically about human compliance with the PM is basically for.",
            "Human comprehensibility thing?"
        ],
        [
            "So the results shown here are only for one of the datasets, but if you go on paper we have shown it for like 7 datasets.",
            "So here for the Reuters Reuters data set.",
            "So the results in the red line in the upper red line is the normal LDA.",
            "The download line is the burst LDA for publicity, and similarly the blue line which is at top is for the nonparametric India and the blue line which is at the bottom is for the bossiness nonparametric.",
            "Something which is very evident here is the burstiness has improved your performance by just choose margins.",
            "An yeah.",
            "The nonparametric stuff also improves your performance as the number of topics increases with the like.",
            "As you increase the number of topics that you want to get from your datasets.",
            "That's why."
        ],
        [
            "And then we thought about that because we have a full sampler, so we should see how HDP is working.",
            "As Ray said, we we always thought that we had best good samplers for her for the things.",
            "So we compared against the mallet for HTTP code.",
            "People might think why against Mallett is just because we have found through our experiments.",
            "That measure has been one of the best implements for truncated SDP LDA.",
            "People never earlier realize this because they never said with their implementation.",
            "So if you again see here, the red line is their version of HDP, LDA.",
            "The blue bars are our version of HTP and the green bars are our full model.",
            "So again, apart from this one case, we generally always beat them.",
            "But again, if you see the place where we have not beaten them is actually a very small is a very smaller data set.",
            "Once you go through the details of that data set."
        ],
        [
            "Then we came and then we realized it's not just about the Gibbs sampling world like people have.",
            "People have lots of variation algorithms, so let's compare against them to our findings.",
            "The set ideal paper in Katy Trail is one of the best versions for Variational Bayes algorithm, which doesn't have split and merge in it.",
            "The screen merge is something that came that's not in there, so again here if you see the red is the version the Blues is our HTP version and the green is the full version.",
            "Again, we consistently beat them over your bed."
        ],
        [
            "And then as I said about Split and Merge so split and Merge was a new technique that was developed by Brandon so that in 2012 lips paper and it's a new way of bringing with the whole process is so we said, OK, let's compare against them.",
            "So if you see here.",
            "This blue line is what's their best performance for HDP LDA method with K equal to 300 topics and we just did our experiments with the same way around.",
            "And here the things which are at down are the normal versions and the things which are at a bar, the Boston's versions.",
            "And here the Y axis is.",
            "Negative log scale.",
            "So here basically the higher the value is better because actually it's a lower value.",
            "Now again, as you see, like burstiness has no comparison.",
            "So like it's just too big and like these things are if you finally will see those are actually increases of thousands rather than only with point force an with the normal versions we are like they are still improving.",
            "We have we have almost saturated at the values where they cover showing, but the point to be noted here is.",
            "Our algorithm is almost like five times faster than theirs for each document, so probably we if you just give us more time, probably will do better."
        ],
        [
            "Now one other thing that we wanted to have a look at is like a lot of people when they do their experiments.",
            "They do not talk about hyperparameters.",
            "Anas reset.",
            "In our versions, we actually sample hyperparameters throughout, so are generally all our hyperparameters sample rather than choosing them directly.",
            "And this is what you see that this light blue line is the beta being sampled.",
            "In our case an.",
            "If you see, the best perplexity is given by bedazzled.",
            ".001 and we're almost tracking it always for all number of topics.",
            "Four other bidders you see, first of all, that that purpose.",
            "Plus it is not that good.",
            "And even then what we mean here is by number of topics there is.",
            "If you had put that as a bitter, you could not find more topics than then, like all other topics that it produced was actually empty.",
            "Or like we had only one or two words.",
            "So basically when people say they're trying to find topics, if you fix your beta at the number of topics that you can find from your versions is actually decided by the hyperparameter.",
            "So it's not just that you can, so you should not just fix your hyperparameters by some here.",
            "You should do experiments or at least sample it, and similarly is with the PM eyes but with PMI is the higher beta gives you better values which is just cause if there is a larger data set and you're trying to compress it with some small number of topics, you're going to get topics which make more sense for smaller topics, But that's the idea."
        ],
        [
            "So finally, So what we have shown here?"
        ],
        [
            "Is there is you can have full nonparametric versions which which is about 50% times lower than the normal GP earlier.",
            "But the performance is better than this is the first time we're saying perform better because volatile in their paper always said that the symmetric symmetric setting doesn't work that well and we have shown that it works well if you sample it properly."
        ],
        [
            "1st is Boston's improves your performance topic.",
            "Conference abilities are improved by few margins, but the test properties is like there's no comparison with the step process cities.",
            "And now finally we have developed a theory where you can use Gibbs samplers to get busting is working, so you have a model that can handle data and that can handle the burst."
        ],
        [
            "This stuff and then again as we have said you, we have got Gibbs sampling algorithm from China DL which which are block table indicators, Gibbs sampling with its which just work much better than the normal Chinese restaurant process which were earlier."
        ],
        [
            "An hyperparameter adaptation is very important for a high performance model.",
            "That's one of the."
        ],
        [
            "And finally we have been also able to actually paralyze algorithms for for a personal computer multiple course, and we have got some into 75% performance increase."
        ],
        [
            "Thank you and you can actually grab bar code like it's available open source so you can grab from either from loss or from the GitHub.",
            "Questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm Ray Benton and this is Swapnil Mishra.",
                    "label": 1
                },
                {
                    "sent": "Garments with non parametric topic models?",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "G.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're hiring so good time for Nadra.",
                    "label": 0
                },
                {
                    "sent": "Our net ioni set a good example yesterday.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So background.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you know topic models are good for discovering hidden themes.",
                    "label": 1
                },
                {
                    "sent": "We've seen some good examples.",
                    "label": 0
                },
                {
                    "sent": "Light and reallocation is the best one.",
                    "label": 1
                },
                {
                    "sent": "But recent research is being all people are developing different ways of improving topic models.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And my question is why should you care?",
                    "label": 1
                },
                {
                    "sent": "You know you can download Mal.",
                    "label": 0
                },
                {
                    "sent": "It works pretty well.",
                    "label": 0
                },
                {
                    "sent": "It's a good system, so why bother doing a bit better?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, one one reason we have is that there's lots of different topic models people are trying.",
                    "label": 0
                },
                {
                    "sent": "It's no longer just simple topic models, so for instance, this is a model for segmentation of documents.",
                    "label": 0
                },
                {
                    "sent": "Works very well.",
                    "label": 0
                },
                {
                    "sent": "One of the best, but in the middle is a topic model, so there's all different tasks we have.",
                    "label": 1
                },
                {
                    "sent": "We want high Fidelity topic models.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Secondly, you can build huge amount of topics.",
                    "label": 1
                },
                {
                    "sent": "So for 1000 documents you can build 100 topics on it and.",
                    "label": 0
                },
                {
                    "sent": "When you've got that Fidelity, people are exploring visualization to to look at it, and so there's advantages in infidelity.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one of the things in our model is we're introducing or where using a technical burstiness that we got from Doyle and Elkin they published in 2009.",
                    "label": 0
                },
                {
                    "sent": "So this is just to explain what burstiness is.",
                    "label": 0
                },
                {
                    "sent": "So at the top you see snippet of a news article at the bottom.",
                    "label": 0
                },
                {
                    "sent": "It's been bagged up and the thing to notice is that ignoring the stop words, the word cabinet appears twice.",
                    "label": 0
                },
                {
                    "sent": "Now if you were to just to get a.",
                    "label": 0
                },
                {
                    "sent": "A small selection of words.",
                    "label": 0
                },
                {
                    "sent": "How often would a word appear twice?",
                    "label": 0
                },
                {
                    "sent": "It's just not going to happen for a somewhat infrequent word like cabinet.",
                    "label": 0
                },
                {
                    "sent": "So what's happening is for topical documents, for news articles.",
                    "label": 0
                },
                {
                    "sent": "For typical things, there is a lot of repetition in there.",
                    "label": 0
                },
                {
                    "sent": "And in fact, that's what TF IDF theory is all about in information retrieval is dealing with this single burstiness.",
                    "label": 0
                },
                {
                    "sent": "So Doyle and Elkin discovered if they put burstiness into their topic models, it led to a dramatic improvement, but the algorithms weren't there, so so not a lot of people have used it since.",
                    "label": 0
                },
                {
                    "sent": "Here's some of the previous work, and with color coded it green is the stuff that really we're building on, and blue is some of the stuff we're comparing against.",
                    "label": 0
                },
                {
                    "sent": "And I'll get to our own sampling method.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In a bit, but there's been a really good sequence of work in, in particular hierarchal delay process LDA, so that's one of the main things we're comparing against.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one thing we've done is is better sampling methods for hierarchical display processes.",
                    "label": 1
                },
                {
                    "sent": "So we don't use the Chinese restaurant process or the the tables that people usually use, and we see quite often in conferences.",
                    "label": 0
                },
                {
                    "sent": "Now we've developed a method that Chang Yueqin published and is back in 2011 at EC MLP KDD and that's really good because it doesn't require dynamic memory.",
                    "label": 1
                },
                {
                    "sent": "It has more rapid mixing and in general it leads to much better models.",
                    "label": 0
                },
                {
                    "sent": "In fact, originally it led to such better models that I thought that you might, as original theory was wrong because we were always better than him.",
                    "label": 0
                },
                {
                    "sent": "I thought consistently must be something wrong with his algorithm.",
                    "label": 0
                },
                {
                    "sent": "Stupid me actually thinking that UI TE would have errors in his statistics.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let's have a look at our model.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "We start off with original LDA and this is where you've got symmetric.",
                    "label": 0
                },
                {
                    "sent": "Darish lies on the two sides.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Odds what a number of groups did, but the best known is Wallach and no put in asymmetric on the the document proportion side and that's implemented in mallet.",
                    "label": 0
                },
                {
                    "sent": "Which means there's a version of truncated HDP LDA in Mallet and it's been one of the best implementations for quite awhile.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then you've got this full hierarchical display process, LDA, which is a nonparametric version of LDA.",
                    "label": 0
                },
                {
                    "sent": "Whether you can have an expandable number of topics.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In 2010, Sato and colleagues put the same nandish same nonparametric machinery, moved it from the from the topic proportion side down to the word side.",
                    "label": 0
                },
                {
                    "sent": "There you see on the right is the non parametric modeling of the the actual topics, the word doc, the topic by word matrix itself.",
                    "label": 1
                },
                {
                    "sent": "Now that's a bit we've got that as a Pitman Yor rather than a Dirichlet process.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we've got one more bit to add, which is the burstiness part.",
                    "label": 0
                },
                {
                    "sent": "When I first saw this, I thought Charles Elkan was crazy, and later on I realized he was brilliant for having Bob Doyle or whoever thought of it.",
                    "label": 0
                },
                {
                    "sent": "And what this does is each topic has an instance.",
                    "label": 0
                },
                {
                    "sent": "In the actual document which modifies it, and this deals with the burstiness very nicely.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's our model.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do some comments on the burstiness, the modeling we do to make that work.",
                    "label": 0
                },
                {
                    "sent": "It's written up well in the paper.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to explain the algorithm here.",
                    "label": 0
                },
                {
                    "sent": "It basically acts as a front end to any.",
                    "label": 1
                },
                {
                    "sent": "Any Gibbs sampler for an LDA style model, so we've added it to dynamic topic models.",
                    "label": 0
                },
                {
                    "sent": "We've added it to an author author topic model.",
                    "label": 0
                },
                {
                    "sent": "And it's quite simple to add what basically you take your Gibbs sampler and you plug it into a function and this will pop.",
                    "label": 0
                },
                {
                    "sent": "Bursting this on the top.",
                    "label": 0
                },
                {
                    "sent": "But there's a lot of trickery in the implementation, and I'd, um, I'd hate anyone to have to look at my code or modify it.",
                    "label": 0
                },
                {
                    "sent": "It's pretty hairy stuff.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there we go.",
                    "label": 0
                },
                {
                    "sent": "That's the first part.",
                    "label": 0
                },
                {
                    "sent": "Now Swapnil will have a chat on the experiments we've done in this, so now I'll talk about the experiments and how we compared against 'cause the one of the major things that we have in we had in our mind was that we should have a full process of comparing in what's in there and how we are.",
                    "label": 0
                },
                {
                    "sent": "And where are we heading home to so.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "1st just have a look at the runtime characteristics, so the basic idea was we wanted to see if you're building heavy model.",
                    "label": 1
                },
                {
                    "sent": "How heavy are your model?",
                    "label": 0
                },
                {
                    "sent": "Is how much time it takes and we also wanted to see scalability.",
                    "label": 1
                },
                {
                    "sent": "So the data set that we use here for the runtime sectors is the latimes data set from the from the track CDs.",
                    "label": 0
                },
                {
                    "sent": "So here if you see in general like NB, LDA is our full model and the things with the burst out the burstiness model related to all models.",
                    "label": 1
                },
                {
                    "sent": "So like.",
                    "label": 0
                },
                {
                    "sent": "Burst idea stands for business version of Indian Everything.",
                    "label": 0
                },
                {
                    "sent": "Now if you see the business version is almost like 10 is like it takes.",
                    "label": 0
                },
                {
                    "sent": "It runs with more 50.",
                    "label": 0
                },
                {
                    "sent": "It takes 50% more time and the memory increase that's in there is not much like if you as you have seen in the model.",
                    "label": 0
                },
                {
                    "sent": "If you generally see you would have thought that the memory would have increased by like by two folds or like a force but that doesn't happen which is good and the other thing that we observed was.",
                    "label": 1
                },
                {
                    "sent": "Like you had an online HD version which was supposed to run online version and it's supposed to be fast, but actually to go through the whole latimes data set.",
                    "label": 0
                },
                {
                    "sent": "It just takes much more time.",
                    "label": 0
                },
                {
                    "sent": "Although of late we realize that there is a C++ version which is a bit faster than the normal code, but it is still not as fast as this.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now talking about, but those were only runtime data success.",
                    "label": 0
                },
                {
                    "sent": "Now what you need to see is how you are going to have your performance.",
                    "label": 0
                },
                {
                    "sent": "So for our performance we used the public city and we use the perplexity as being mentioned by the volatile in their 2009 paper and the other metric that we used is a PMI which is a pointwise mutual information.",
                    "label": 1
                },
                {
                    "sent": "It was developed by new material in 2010 and it's basically about human compliance with the PM is basically for.",
                    "label": 0
                },
                {
                    "sent": "Human comprehensibility thing?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the results shown here are only for one of the datasets, but if you go on paper we have shown it for like 7 datasets.",
                    "label": 0
                },
                {
                    "sent": "So here for the Reuters Reuters data set.",
                    "label": 0
                },
                {
                    "sent": "So the results in the red line in the upper red line is the normal LDA.",
                    "label": 0
                },
                {
                    "sent": "The download line is the burst LDA for publicity, and similarly the blue line which is at top is for the nonparametric India and the blue line which is at the bottom is for the bossiness nonparametric.",
                    "label": 0
                },
                {
                    "sent": "Something which is very evident here is the burstiness has improved your performance by just choose margins.",
                    "label": 0
                },
                {
                    "sent": "An yeah.",
                    "label": 0
                },
                {
                    "sent": "The nonparametric stuff also improves your performance as the number of topics increases with the like.",
                    "label": 0
                },
                {
                    "sent": "As you increase the number of topics that you want to get from your datasets.",
                    "label": 0
                },
                {
                    "sent": "That's why.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we thought about that because we have a full sampler, so we should see how HDP is working.",
                    "label": 0
                },
                {
                    "sent": "As Ray said, we we always thought that we had best good samplers for her for the things.",
                    "label": 0
                },
                {
                    "sent": "So we compared against the mallet for HTTP code.",
                    "label": 0
                },
                {
                    "sent": "People might think why against Mallett is just because we have found through our experiments.",
                    "label": 0
                },
                {
                    "sent": "That measure has been one of the best implements for truncated SDP LDA.",
                    "label": 0
                },
                {
                    "sent": "People never earlier realize this because they never said with their implementation.",
                    "label": 0
                },
                {
                    "sent": "So if you again see here, the red line is their version of HDP, LDA.",
                    "label": 0
                },
                {
                    "sent": "The blue bars are our version of HTP and the green bars are our full model.",
                    "label": 0
                },
                {
                    "sent": "So again, apart from this one case, we generally always beat them.",
                    "label": 0
                },
                {
                    "sent": "But again, if you see the place where we have not beaten them is actually a very small is a very smaller data set.",
                    "label": 0
                },
                {
                    "sent": "Once you go through the details of that data set.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we came and then we realized it's not just about the Gibbs sampling world like people have.",
                    "label": 0
                },
                {
                    "sent": "People have lots of variation algorithms, so let's compare against them to our findings.",
                    "label": 0
                },
                {
                    "sent": "The set ideal paper in Katy Trail is one of the best versions for Variational Bayes algorithm, which doesn't have split and merge in it.",
                    "label": 0
                },
                {
                    "sent": "The screen merge is something that came that's not in there, so again here if you see the red is the version the Blues is our HTP version and the green is the full version.",
                    "label": 0
                },
                {
                    "sent": "Again, we consistently beat them over your bed.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then as I said about Split and Merge so split and Merge was a new technique that was developed by Brandon so that in 2012 lips paper and it's a new way of bringing with the whole process is so we said, OK, let's compare against them.",
                    "label": 0
                },
                {
                    "sent": "So if you see here.",
                    "label": 0
                },
                {
                    "sent": "This blue line is what's their best performance for HDP LDA method with K equal to 300 topics and we just did our experiments with the same way around.",
                    "label": 0
                },
                {
                    "sent": "And here the things which are at down are the normal versions and the things which are at a bar, the Boston's versions.",
                    "label": 0
                },
                {
                    "sent": "And here the Y axis is.",
                    "label": 0
                },
                {
                    "sent": "Negative log scale.",
                    "label": 0
                },
                {
                    "sent": "So here basically the higher the value is better because actually it's a lower value.",
                    "label": 0
                },
                {
                    "sent": "Now again, as you see, like burstiness has no comparison.",
                    "label": 0
                },
                {
                    "sent": "So like it's just too big and like these things are if you finally will see those are actually increases of thousands rather than only with point force an with the normal versions we are like they are still improving.",
                    "label": 0
                },
                {
                    "sent": "We have we have almost saturated at the values where they cover showing, but the point to be noted here is.",
                    "label": 0
                },
                {
                    "sent": "Our algorithm is almost like five times faster than theirs for each document, so probably we if you just give us more time, probably will do better.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now one other thing that we wanted to have a look at is like a lot of people when they do their experiments.",
                    "label": 0
                },
                {
                    "sent": "They do not talk about hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "Anas reset.",
                    "label": 0
                },
                {
                    "sent": "In our versions, we actually sample hyperparameters throughout, so are generally all our hyperparameters sample rather than choosing them directly.",
                    "label": 0
                },
                {
                    "sent": "And this is what you see that this light blue line is the beta being sampled.",
                    "label": 0
                },
                {
                    "sent": "In our case an.",
                    "label": 0
                },
                {
                    "sent": "If you see, the best perplexity is given by bedazzled.",
                    "label": 0
                },
                {
                    "sent": ".001 and we're almost tracking it always for all number of topics.",
                    "label": 1
                },
                {
                    "sent": "Four other bidders you see, first of all, that that purpose.",
                    "label": 0
                },
                {
                    "sent": "Plus it is not that good.",
                    "label": 0
                },
                {
                    "sent": "And even then what we mean here is by number of topics there is.",
                    "label": 0
                },
                {
                    "sent": "If you had put that as a bitter, you could not find more topics than then, like all other topics that it produced was actually empty.",
                    "label": 0
                },
                {
                    "sent": "Or like we had only one or two words.",
                    "label": 0
                },
                {
                    "sent": "So basically when people say they're trying to find topics, if you fix your beta at the number of topics that you can find from your versions is actually decided by the hyperparameter.",
                    "label": 0
                },
                {
                    "sent": "So it's not just that you can, so you should not just fix your hyperparameters by some here.",
                    "label": 0
                },
                {
                    "sent": "You should do experiments or at least sample it, and similarly is with the PM eyes but with PMI is the higher beta gives you better values which is just cause if there is a larger data set and you're trying to compress it with some small number of topics, you're going to get topics which make more sense for smaller topics, But that's the idea.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So finally, So what we have shown here?",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is there is you can have full nonparametric versions which which is about 50% times lower than the normal GP earlier.",
                    "label": 0
                },
                {
                    "sent": "But the performance is better than this is the first time we're saying perform better because volatile in their paper always said that the symmetric symmetric setting doesn't work that well and we have shown that it works well if you sample it properly.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "1st is Boston's improves your performance topic.",
                    "label": 0
                },
                {
                    "sent": "Conference abilities are improved by few margins, but the test properties is like there's no comparison with the step process cities.",
                    "label": 0
                },
                {
                    "sent": "And now finally we have developed a theory where you can use Gibbs samplers to get busting is working, so you have a model that can handle data and that can handle the burst.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This stuff and then again as we have said you, we have got Gibbs sampling algorithm from China DL which which are block table indicators, Gibbs sampling with its which just work much better than the normal Chinese restaurant process which were earlier.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An hyperparameter adaptation is very important for a high performance model.",
                    "label": 0
                },
                {
                    "sent": "That's one of the.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finally we have been also able to actually paralyze algorithms for for a personal computer multiple course, and we have got some into 75% performance increase.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you and you can actually grab bar code like it's available open source so you can grab from either from loss or from the GitHub.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                }
            ]
        }
    }
}