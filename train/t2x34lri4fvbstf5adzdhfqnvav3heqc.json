{
    "id": "t2x34lri4fvbstf5adzdhfqnvav3heqc",
    "title": "Characterizing Microblogs with Topic Models",
    "info": {
        "author": [
            "Daniel Ramage, Stanford Natural Language Processing Group, Stanford University"
        ],
        "published": "June 29, 2010",
        "recorded": "May 2010",
        "category": [
            "Top->Computers->Social Networking",
            "Top->Social Sciences->Journalism->Blogging"
        ]
    },
    "url": "http://videolectures.net/icwsm2010_ramage_cmt/",
    "segmentation": [
        [
            "I'm Daniel ramage.",
            "I'm a PhD student in the natural Language Processing Group at Stanford.",
            "I'm going to describe some joint work I did this fall at Microsoft Research with Sue Dumais and Dan Liebling.",
            "So we're interested in characterizing microblogs with topic models, and like the other talks in this session, we're actually talking to."
        ],
        [
            "Weekly about microblogs on Twitter.",
            "You're all familiar with the basics here.",
            "Messages are short.",
            "You can reply to people.",
            "You can tag things in line.",
            "We're going to use both of those facts a little bit later.",
            "And, importantly, following is not equal to friending as was just discussed that asymmetry makes Twitter an interesting platform for information distribution beyond the standard sort of symmetrical friending, social style relationship, and that's important."
        ],
        [
            "Come because.",
            "We're we want to know what people actually like in the feeds that they see.",
            "They're making choices of who to follow for reasons that are not purely social.",
            "So we did a study internally at Microsoft.",
            "We built a little interface for people to rate the quality of posts that were in the public timelines of people that they followed.",
            "So this was actually posts that they would have seen on their own Twitter homepage we asked."
        ],
        [
            "And to make those judgments on the three point scale from not really worth reading may be worth the time spent reading or worth the time spent reading.",
            "And of course, these posts are 140 characters, so that's not really a lot of the time investment for one of those thumbs down style ratings when we aggregated AKR."
        ],
        [
            "Last users, we found that the number of users whose average judgment was not really worth the time reading was actually 12 of our 43 users may be worth the time.",
            "Reading was 31 of our 43 users an if you're doing the math here, that means actually there was no one whose average rating was worth the time spent reading.",
            "Now that doesn't mean each individual rating was really poor.",
            "Everyone had at least one post.",
            "That was maybe or definitely worth the time spent reading."
        ],
        [
            "So this sort of points to the fundamental problem that we face when talking about these kinds of information networks, which is that people that we follow isn't exactly the same thing as tweets worth reading.",
            "These are tweets that people are already better from the the posters that our raters have already decided post a worthwhile subset of information.",
            "This this problem actually has two sides to it.",
            "Well, first there is the fact that.",
            "Not everything that people post that is worth is worth reading, but at the same time we might find that there's other posts out there that weren't written by people that we follow that are worth reading, that there may be more worthwhile than the ones in our current set.",
            "Depending on the person and their interests."
        ],
        [
            "So to get a better sense of what factors people had when deciding whom to follow and what was worth reading, we spent some time talking with some heavy users of Twitter within Microsoft.",
            "And we also did a broader survey based on those intuitions and our own that 56 users in Microsoft.",
            "So of course this isn't a representative distribution of Twitter users overall, but we're trying to get a sense of the sort of space of reasons that people have.",
            "For finding something worthwhile, and so we think that to that extent this represent these results are informative.",
            "So we categorize those results into a few different buckets, which I'm just going to introduce to you.",
            "We found that one of the reasons people choose to follow others was for substantive reasons.",
            "So something in the substance of what that poster writes about if they have a shared interest in technology, a hobby, something about politics that they find interesting.",
            "There were also a lot of different social reasons and types of communication that were found to be worthwhile, so those would be things related to professional networking or staying in touch making plans.",
            "We can't talk about Twitter without talking about those status update messages.",
            "The Canonical what are you doing right now?",
            "Style answers.",
            "So we found that people tended overall not to like too many updates about meals and hygiene, but actually that wasn't that wasn't uniformly true.",
            "Depending on the relationship between the two people involved, those kinds of status updates might be important and at the same time, depending on the style in which those posts are written, as well as the posts about.",
            "Substance or social, if those are written with a particular worldview or a particular humor or wit that some.",
            "Some readers would find those worth worthwhile even, even if they were just purely status updates.",
            "So we call these 4 dimensions the four S dimension, substance status, social and style, and we think they provide, not.",
            "They don't provide an exhaustive list of the reasons we got in our survey and our interviews, but they provide a nice sort of framing of the major groupings of reasons that people had."
        ],
        [
            "So.",
            "Those types of reasons really dig into the fact that we need to start looking at the content.",
            "In Twitter, we need to look at the words and figure out why are people interested in getting at what they want.",
            "It's not just network features that tell us everything about it, those four dimensions.",
            "We're hoping to be able to discover with a sort of content oriented analysis, which is what the focus of this talk is going to be about.",
            "We analyzed about 8 million posts from Spritzers Twitters Spritzer feed from November.",
            "It's a subset of all of the public posts on Twitter.",
            "We filtered out some of the very common, very rare words and did some tokenization that keeps together emoticons.",
            "Stuff like that.",
            "We ended up with about 13 words proposed.",
            "So when we talk about content modeling."
        ],
        [
            "On these types of short tweets, we have a few options.",
            "First place to start for anyone who's got some background in information retrieval or vector space models would be to take a look at the surface word features in the TF IDF sort of cosine similarity space and.",
            "So we're going to.",
            "We're going to work with those.",
            "You might think that we could try to dig a little deeper using some natural language processing techniques like parsing or part of speech tagging, coreference resolution.",
            "Of course, the best models in that space tend to be trained on text that looks a lot like the Wall Street Journal.",
            "Or is the Wall Street Journal.",
            "In the case of parsing?",
            "And that tends not to work very well on some of the well.",
            "Let's not say some a lot of the content that's out there on Twitter, and so we're just not going to look at that kind of technique in this work.",
            "Although I think there's a lot of really cool directions in future work for how to do deeper linguistic analysis of this."
        ],
        [
            "To text.",
            "So we've got our surface word features.",
            "The problem with these surface features is that.",
            "If you have 13 words out of vocabulary of about 5 million in our case, you're going to end up with very sparse feature vectors which are going to be hard to work with.",
            "So one way to.",
            "A standard way to work a little in a little bit more, less sparse space is where the dimensionality reduction technique, like topic models like in fiercely location and derivatives.",
            "That's what we're going to do here.",
            "In this work, latent semantic analysis would give you something similar.",
            "And also there are some kinds of supervised classification that you might be able to apply in this space.",
            "In particular, we know things about hash tags, emoticons, questions retweets, those types of metadata that are part of the examples we're looking at are built into every single tweet, and if you know that hashtags are there, it would nice to be able to model what those hashtags are about explicitly.",
            "So the model that we're using."
        ],
        [
            "Is called labeled LDA.",
            "It's a topic model that lets us account for the sort of a dimensionality reduction as well as some of the supervised classification type techniques and later in some ranking experiments will find that the best model actually incorporates these three kinds of features.",
            "So labeled LDA is a is a technique that I worked on previously and that's why I'm working on it here too.",
            "It does two things.",
            "It discovers the unlabeled topics that are in our collection, and it also is going to model some of the common labels at the same time.",
            "The common labels being things like hashtags.",
            "So on the unlabeled topic front we give we ask the algorithm to find 200 latent topic dimensions, where each of those topic dimensions is going to basically be a multinomial distribution over all of the words in the vocabulary.",
            "And here we've shown two in these boxes.",
            "The first one we're showing some of the highest scoring words within that topic.",
            "So the most important words in that dimension are going to be Obama, President, American America.",
            "So this is something that we might say is about politics, or more specifically, we could think of this as sort of a substance topic below will find also some of the major trends.",
            "I'm sorry some of the major dimensions of language usage include things that are much more staticy about going out tonight or going to sleep.",
            "On the common labels, we end up with about 500 two 1000 dimensions that are allocated for describing specific labeled features in our text, so the number changes here because it depends, we're only modeling the hashtags that are common enough to be important in whichever data set we're looking at, and so, for instance, the hash tag jobs will end up with words like jobs feature manager associated with it that are with respect to.",
            "The words that tend to go with that hashtag, net of the fact that all of the words in Twitter might better be described by one of the latent topics.",
            "So this model sort of doing both of those things at the same time, figuring out the latent dimensions and which words go with each of these labels.",
            "And we also have some more fun things like figuring out different categories of Smiley faces.",
            "So there's this sort of good morning ones, and there's this sort of more general excitement.",
            "Can't wait type ones.",
            "You can see the paper for some more examples of these types of distributions, but I'm not going to talk about much."
        ],
        [
            "More of that in this talk.",
            "So the way this works is that that I'm not going to talk about the algorithm generative process for it, and you have to do inference.",
            "We have a scalable inference algorithm for it that runs in parallel and works at near real time, but the intuitive way we're going to use this model to get it.",
            "Some of those questions of how to model content on Twitter is by making use of the fact that each one of the words in our corpus.",
            "So here we have three posts from different puppets.",
            "Is going to be coming from one of the topic dimensions or one of the label dimensions and the algorithm is essentially going to sign for each word actually a distribution.",
            "But here we show one of the topics numbered 411 for the latent topics or labeled topic like cookie and yummy.",
            "That is the topic that best describes that particular word.",
            "Once we have those topics figured out, we can get a description of that set of posts by sort of creating histogram of the topic assignments that we saw on each word within that collection of posts.",
            "So we could say that if we wanted to know what these puppets talk about, we'd say that they talk mostly about topics one in two.",
            "They talk a little bit about 3, five and yummy, and they talk a little bit less about four.",
            "So."
        ],
        [
            "Once we've mapped our posts into this sort of lower dimensional space, we still haven't gotten yet to the to the higher level questions that we're interested in, so I hinted before when I showed you some of those latent topics that some of the topics we find really look like substance, and some of them really look like status.",
            "And So what we did was we manually annotated the latent topic dimensions with one or more of substance status, social or style.",
            "As basically saying this, when this topic is used, it is used in a way that evokes language that looks like substance, language, status, language, social language or stylistic language use.",
            "So I'll give you some examples of what that looks like in a second, but we have fairly good Inter annotator agreement about those labels and we also said that things like hashtags are substance represents substance, because people are usually making them to be found and re indexed by others.",
            "But of course, all of these are shortcuts and any individual dimension is probably going to be some mix of all four of those."
        ],
        [
            "OK, So what can we do now that we have these topics labeled with those categories?",
            "Well, we can take the whole corpus of tweets and we can do the same trick that I just showed you for The Muppets.",
            "A couple slides back and we can look at how often each topic is used and then once we know how often each topic is used, we can see well how often are the substance topics used, the status ones, the social ones in the style ones.",
            "So as far as we know, we think this is the first picture that looks at.",
            "Looks like this for trying to describe overall on Twitter.",
            "Is language used in a social, substantive, stylistic, or status E way so despite my own impression, sometimes that most of 80% of Twitter is is about sandwiches.",
            "It turns out that only about sort of 12% is sort of that Canonical status update style language.",
            "Stylistic language is sort of a large category here because it reflects a lot of common words.",
            "You'll end up with people, so this one on the left is.",
            "Peoples representations of things that that are funny.",
            "So different ways of laughing this other one represents sort of a particular dialect, perhaps of the way people might talk in English.",
            "And of course there's some overlap, because all of these have some social intent, or many of them have some social intent as well.",
            "So the but the social ones that we, the ones that we clearly marked as social or things like asking for help in the top left or saying what's up next to that."
        ],
        [
            "So I just gave you a bit of an overview of the kind of technique we're going to use.",
            "I'm going to show you some pictures now of the types of ways interfaces we built for helping to characterize sets of posts into a space that looks like that, and then some applications for recommending and filtering based on it."
        ],
        [
            "So this is a screenshot of a Now live demo built by Dan Liebling, one of our Co.",
            "Authors and and it is.",
            "I'll give you the URL at the end of the talk.",
            "What you do is you type in a query or username in that box in the top and it pulls from Twitter's public API set of posts and for that set of posts it Maps the words in those posts into these dimensions.",
            "The topic dimensions from a model that we learned and then it shows that.",
            "So we see here for the Microsoft Research Twitter account that it's actually mostly substance.",
            "The Big Blue box on the left is the substance topics that are mostly used by Microsoft.",
            "So the largest one.",
            "Of course within there is web and then within web we see the words that are used, shaded by how often this set of posts uses that word.",
            "So the light Gray ones are words that were used by other people who use that topic, but not necessarily.",
            "This person who used that topic."
        ],
        [
            "By contrast, we can see something like the Microsoft Bing account actually is a lot of social language usage.",
            "It's about half social and half substance, and that's because this account actually talks more directly, interacts with other users on Twitter, spends a lot of time replying to people who've spoken to it."
        ],
        [
            "And so then we just to bring things back home.",
            "We are interested in those problems about finding and filtering.",
            "How do we?",
            "How do we recommend more people to follow and filter content?",
            "Because we know that we're not doing a good job of that right now, just by showing all the posts that come from a particular person."
        ],
        [
            "So we used the data set that I mentioned earlier.",
            "There are about 4000 judgments that people had made of posts from people that they follow.",
            "We split for each Raider their judgments into a training set and test set, and so to get a sense of how we might perform at a filtering task.",
            "We basically say that.",
            "We re rank the test set according to its distance to the positive examples in the training set and so."
        ],
        [
            "What we mean is if you take the posts that were if you take the post that the person had rated, and if you consider the may be worth the time spent reading, or definitely worth the time spent reading as good examples.",
            "We want to re rank the remainder of those posts, the remaining 30% to put those ones that we know that the person liked up at the top.",
            "If we were to then take that rank ranking and threshold it, it would be something like a filter.",
            "You could also think of this as a multi as a classification task, but we wanted to keep.",
            "Evaluation simple in this context, just to be able to compare feature spaces."
        ],
        [
            "The feature spaces we want to compare our the raw cosine similarity based on the word features that we see in those posts will reduce dimensional space from the labeled LDA model and a weighted combination of the two."
        ],
        [
            "And so, um.",
            "This is the in the paper.",
            "There's other sorts of valuation metrics in more details here, but I'm just showing you the mean reciprocal rank of the first relevant item.",
            "The random baseline labeled LDA feature space does only a smidge better than the random baseline TF.",
            "IDF does a bunch better, but this combination of the two actually does a lot better than either independent individually.",
            "The reason for that is because if you have two posts, or where they have some words in common.",
            "Then the cosine similarity is actually going to give you a pretty high score, because they're going to have those.",
            "If there's a lot of overlap in their 13 words in a particular post with the centroid of the positive examples, then you want to know that that's going to be a good example of something that's very likely to be positive, and you don't want to lose it, whereas.",
            "Whereas if you don't, if you have no overlap, that doesn't necessarily mean it's not relevant.",
            "If you have no overlap, it might just be because you picked a different 13 terms, and you probably should have used some.",
            "You could have used some synonyms or something and gotten some good results, so when that that kind of example is where the labeled LDA reduced feature space would do well for you.",
            "But sometimes you'll get some false conflations.",
            "So in this third column, what you're doing is you're saying.",
            "When the cosine similarity score is fired, you should use it.",
            "There's good overlap in the words, and when it doesn't fire, fall back to the labeled LDA model."
        ],
        [
            "We see something similar in the finding task where the goal is to take the posts from the some subset of the Raiders so our rater has has.",
            "Labeled examples from several different posters and seven in this case, and we're going to ask the we're going to take six of those seven as the training set, and we're going to ask our model to try to rank the remaining user that they actually do follow.",
            "We're going to try to rank that user highly in a set of eight people that they don't follow, so the goal here is to basically try to recommend a new user to follow that we know they actually do follow.",
            "We're just not training on it right now and defined it among the set.",
            "That they don't follow.",
            "Similarly, we find that TF IDF by itself beats labeled LDA in a weighted combination, does really well.",
            "This actually is.",
            "Nearly perfect at finding the.",
            "Test Raider or finding the test post are among the set of the nine possible examples."
        ],
        [
            "So just to conclude, we spent some.",
            "We think that content analysis on microblogs is sort of in the way to go with meeting some unmet information needs.",
            "There's a lot.",
            "We've only scratched the surface in this kind of work.",
            "There's lots more techniques that people could do in this space, and but we think that for characterizing, recommending, and filtering models like this can be useful.",
            "Labeled LDA model comes up with some dimensions that correspond nicely with our intuitions for why people make.",
            "Following decisions and it can provide some insight into language use about large scale trends of language in general as well as what individual hashtags and emoticons are about within that context.",
            "There's lots of next steps we'd like to do, including temporal dynamics, include incorporating network effects and sorts of flow features, and so this is.",
            "This is our URL we would love for you to play with it and send us feedback.",
            "It's just gone live, and so for those of you who hit it now, you have the unique opportunity to be the first people to tell us all the things that are wrong with it.",
            "Thanks very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm Daniel ramage.",
                    "label": 0
                },
                {
                    "sent": "I'm a PhD student in the natural Language Processing Group at Stanford.",
                    "label": 0
                },
                {
                    "sent": "I'm going to describe some joint work I did this fall at Microsoft Research with Sue Dumais and Dan Liebling.",
                    "label": 0
                },
                {
                    "sent": "So we're interested in characterizing microblogs with topic models, and like the other talks in this session, we're actually talking to.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Weekly about microblogs on Twitter.",
                    "label": 1
                },
                {
                    "sent": "You're all familiar with the basics here.",
                    "label": 0
                },
                {
                    "sent": "Messages are short.",
                    "label": 1
                },
                {
                    "sent": "You can reply to people.",
                    "label": 0
                },
                {
                    "sent": "You can tag things in line.",
                    "label": 0
                },
                {
                    "sent": "We're going to use both of those facts a little bit later.",
                    "label": 0
                },
                {
                    "sent": "And, importantly, following is not equal to friending as was just discussed that asymmetry makes Twitter an interesting platform for information distribution beyond the standard sort of symmetrical friending, social style relationship, and that's important.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Come because.",
                    "label": 0
                },
                {
                    "sent": "We're we want to know what people actually like in the feeds that they see.",
                    "label": 1
                },
                {
                    "sent": "They're making choices of who to follow for reasons that are not purely social.",
                    "label": 0
                },
                {
                    "sent": "So we did a study internally at Microsoft.",
                    "label": 1
                },
                {
                    "sent": "We built a little interface for people to rate the quality of posts that were in the public timelines of people that they followed.",
                    "label": 0
                },
                {
                    "sent": "So this was actually posts that they would have seen on their own Twitter homepage we asked.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And to make those judgments on the three point scale from not really worth reading may be worth the time spent reading or worth the time spent reading.",
                    "label": 0
                },
                {
                    "sent": "And of course, these posts are 140 characters, so that's not really a lot of the time investment for one of those thumbs down style ratings when we aggregated AKR.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Last users, we found that the number of users whose average judgment was not really worth the time reading was actually 12 of our 43 users may be worth the time.",
                    "label": 1
                },
                {
                    "sent": "Reading was 31 of our 43 users an if you're doing the math here, that means actually there was no one whose average rating was worth the time spent reading.",
                    "label": 0
                },
                {
                    "sent": "Now that doesn't mean each individual rating was really poor.",
                    "label": 0
                },
                {
                    "sent": "Everyone had at least one post.",
                    "label": 1
                },
                {
                    "sent": "That was maybe or definitely worth the time spent reading.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this sort of points to the fundamental problem that we face when talking about these kinds of information networks, which is that people that we follow isn't exactly the same thing as tweets worth reading.",
                    "label": 1
                },
                {
                    "sent": "These are tweets that people are already better from the the posters that our raters have already decided post a worthwhile subset of information.",
                    "label": 0
                },
                {
                    "sent": "This this problem actually has two sides to it.",
                    "label": 0
                },
                {
                    "sent": "Well, first there is the fact that.",
                    "label": 0
                },
                {
                    "sent": "Not everything that people post that is worth is worth reading, but at the same time we might find that there's other posts out there that weren't written by people that we follow that are worth reading, that there may be more worthwhile than the ones in our current set.",
                    "label": 0
                },
                {
                    "sent": "Depending on the person and their interests.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to get a better sense of what factors people had when deciding whom to follow and what was worth reading, we spent some time talking with some heavy users of Twitter within Microsoft.",
                    "label": 1
                },
                {
                    "sent": "And we also did a broader survey based on those intuitions and our own that 56 users in Microsoft.",
                    "label": 0
                },
                {
                    "sent": "So of course this isn't a representative distribution of Twitter users overall, but we're trying to get a sense of the sort of space of reasons that people have.",
                    "label": 0
                },
                {
                    "sent": "For finding something worthwhile, and so we think that to that extent this represent these results are informative.",
                    "label": 0
                },
                {
                    "sent": "So we categorize those results into a few different buckets, which I'm just going to introduce to you.",
                    "label": 0
                },
                {
                    "sent": "We found that one of the reasons people choose to follow others was for substantive reasons.",
                    "label": 0
                },
                {
                    "sent": "So something in the substance of what that poster writes about if they have a shared interest in technology, a hobby, something about politics that they find interesting.",
                    "label": 0
                },
                {
                    "sent": "There were also a lot of different social reasons and types of communication that were found to be worthwhile, so those would be things related to professional networking or staying in touch making plans.",
                    "label": 1
                },
                {
                    "sent": "We can't talk about Twitter without talking about those status update messages.",
                    "label": 0
                },
                {
                    "sent": "The Canonical what are you doing right now?",
                    "label": 1
                },
                {
                    "sent": "Style answers.",
                    "label": 1
                },
                {
                    "sent": "So we found that people tended overall not to like too many updates about meals and hygiene, but actually that wasn't that wasn't uniformly true.",
                    "label": 0
                },
                {
                    "sent": "Depending on the relationship between the two people involved, those kinds of status updates might be important and at the same time, depending on the style in which those posts are written, as well as the posts about.",
                    "label": 0
                },
                {
                    "sent": "Substance or social, if those are written with a particular worldview or a particular humor or wit that some.",
                    "label": 0
                },
                {
                    "sent": "Some readers would find those worth worthwhile even, even if they were just purely status updates.",
                    "label": 0
                },
                {
                    "sent": "So we call these 4 dimensions the four S dimension, substance status, social and style, and we think they provide, not.",
                    "label": 0
                },
                {
                    "sent": "They don't provide an exhaustive list of the reasons we got in our survey and our interviews, but they provide a nice sort of framing of the major groupings of reasons that people had.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Those types of reasons really dig into the fact that we need to start looking at the content.",
                    "label": 0
                },
                {
                    "sent": "In Twitter, we need to look at the words and figure out why are people interested in getting at what they want.",
                    "label": 0
                },
                {
                    "sent": "It's not just network features that tell us everything about it, those four dimensions.",
                    "label": 0
                },
                {
                    "sent": "We're hoping to be able to discover with a sort of content oriented analysis, which is what the focus of this talk is going to be about.",
                    "label": 0
                },
                {
                    "sent": "We analyzed about 8 million posts from Spritzers Twitters Spritzer feed from November.",
                    "label": 1
                },
                {
                    "sent": "It's a subset of all of the public posts on Twitter.",
                    "label": 0
                },
                {
                    "sent": "We filtered out some of the very common, very rare words and did some tokenization that keeps together emoticons.",
                    "label": 0
                },
                {
                    "sent": "Stuff like that.",
                    "label": 1
                },
                {
                    "sent": "We ended up with about 13 words proposed.",
                    "label": 0
                },
                {
                    "sent": "So when we talk about content modeling.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On these types of short tweets, we have a few options.",
                    "label": 0
                },
                {
                    "sent": "First place to start for anyone who's got some background in information retrieval or vector space models would be to take a look at the surface word features in the TF IDF sort of cosine similarity space and.",
                    "label": 1
                },
                {
                    "sent": "So we're going to.",
                    "label": 0
                },
                {
                    "sent": "We're going to work with those.",
                    "label": 1
                },
                {
                    "sent": "You might think that we could try to dig a little deeper using some natural language processing techniques like parsing or part of speech tagging, coreference resolution.",
                    "label": 0
                },
                {
                    "sent": "Of course, the best models in that space tend to be trained on text that looks a lot like the Wall Street Journal.",
                    "label": 0
                },
                {
                    "sent": "Or is the Wall Street Journal.",
                    "label": 0
                },
                {
                    "sent": "In the case of parsing?",
                    "label": 0
                },
                {
                    "sent": "And that tends not to work very well on some of the well.",
                    "label": 0
                },
                {
                    "sent": "Let's not say some a lot of the content that's out there on Twitter, and so we're just not going to look at that kind of technique in this work.",
                    "label": 0
                },
                {
                    "sent": "Although I think there's a lot of really cool directions in future work for how to do deeper linguistic analysis of this.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To text.",
                    "label": 0
                },
                {
                    "sent": "So we've got our surface word features.",
                    "label": 1
                },
                {
                    "sent": "The problem with these surface features is that.",
                    "label": 0
                },
                {
                    "sent": "If you have 13 words out of vocabulary of about 5 million in our case, you're going to end up with very sparse feature vectors which are going to be hard to work with.",
                    "label": 0
                },
                {
                    "sent": "So one way to.",
                    "label": 1
                },
                {
                    "sent": "A standard way to work a little in a little bit more, less sparse space is where the dimensionality reduction technique, like topic models like in fiercely location and derivatives.",
                    "label": 1
                },
                {
                    "sent": "That's what we're going to do here.",
                    "label": 0
                },
                {
                    "sent": "In this work, latent semantic analysis would give you something similar.",
                    "label": 0
                },
                {
                    "sent": "And also there are some kinds of supervised classification that you might be able to apply in this space.",
                    "label": 0
                },
                {
                    "sent": "In particular, we know things about hash tags, emoticons, questions retweets, those types of metadata that are part of the examples we're looking at are built into every single tweet, and if you know that hashtags are there, it would nice to be able to model what those hashtags are about explicitly.",
                    "label": 1
                },
                {
                    "sent": "So the model that we're using.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is called labeled LDA.",
                    "label": 0
                },
                {
                    "sent": "It's a topic model that lets us account for the sort of a dimensionality reduction as well as some of the supervised classification type techniques and later in some ranking experiments will find that the best model actually incorporates these three kinds of features.",
                    "label": 0
                },
                {
                    "sent": "So labeled LDA is a is a technique that I worked on previously and that's why I'm working on it here too.",
                    "label": 1
                },
                {
                    "sent": "It does two things.",
                    "label": 0
                },
                {
                    "sent": "It discovers the unlabeled topics that are in our collection, and it also is going to model some of the common labels at the same time.",
                    "label": 0
                },
                {
                    "sent": "The common labels being things like hashtags.",
                    "label": 1
                },
                {
                    "sent": "So on the unlabeled topic front we give we ask the algorithm to find 200 latent topic dimensions, where each of those topic dimensions is going to basically be a multinomial distribution over all of the words in the vocabulary.",
                    "label": 1
                },
                {
                    "sent": "And here we've shown two in these boxes.",
                    "label": 0
                },
                {
                    "sent": "The first one we're showing some of the highest scoring words within that topic.",
                    "label": 0
                },
                {
                    "sent": "So the most important words in that dimension are going to be Obama, President, American America.",
                    "label": 0
                },
                {
                    "sent": "So this is something that we might say is about politics, or more specifically, we could think of this as sort of a substance topic below will find also some of the major trends.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry some of the major dimensions of language usage include things that are much more staticy about going out tonight or going to sleep.",
                    "label": 0
                },
                {
                    "sent": "On the common labels, we end up with about 500 two 1000 dimensions that are allocated for describing specific labeled features in our text, so the number changes here because it depends, we're only modeling the hashtags that are common enough to be important in whichever data set we're looking at, and so, for instance, the hash tag jobs will end up with words like jobs feature manager associated with it that are with respect to.",
                    "label": 0
                },
                {
                    "sent": "The words that tend to go with that hashtag, net of the fact that all of the words in Twitter might better be described by one of the latent topics.",
                    "label": 0
                },
                {
                    "sent": "So this model sort of doing both of those things at the same time, figuring out the latent dimensions and which words go with each of these labels.",
                    "label": 0
                },
                {
                    "sent": "And we also have some more fun things like figuring out different categories of Smiley faces.",
                    "label": 0
                },
                {
                    "sent": "So there's this sort of good morning ones, and there's this sort of more general excitement.",
                    "label": 0
                },
                {
                    "sent": "Can't wait type ones.",
                    "label": 0
                },
                {
                    "sent": "You can see the paper for some more examples of these types of distributions, but I'm not going to talk about much.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "More of that in this talk.",
                    "label": 0
                },
                {
                    "sent": "So the way this works is that that I'm not going to talk about the algorithm generative process for it, and you have to do inference.",
                    "label": 0
                },
                {
                    "sent": "We have a scalable inference algorithm for it that runs in parallel and works at near real time, but the intuitive way we're going to use this model to get it.",
                    "label": 0
                },
                {
                    "sent": "Some of those questions of how to model content on Twitter is by making use of the fact that each one of the words in our corpus.",
                    "label": 0
                },
                {
                    "sent": "So here we have three posts from different puppets.",
                    "label": 0
                },
                {
                    "sent": "Is going to be coming from one of the topic dimensions or one of the label dimensions and the algorithm is essentially going to sign for each word actually a distribution.",
                    "label": 0
                },
                {
                    "sent": "But here we show one of the topics numbered 411 for the latent topics or labeled topic like cookie and yummy.",
                    "label": 1
                },
                {
                    "sent": "That is the topic that best describes that particular word.",
                    "label": 0
                },
                {
                    "sent": "Once we have those topics figured out, we can get a description of that set of posts by sort of creating histogram of the topic assignments that we saw on each word within that collection of posts.",
                    "label": 1
                },
                {
                    "sent": "So we could say that if we wanted to know what these puppets talk about, we'd say that they talk mostly about topics one in two.",
                    "label": 1
                },
                {
                    "sent": "They talk a little bit about 3, five and yummy, and they talk a little bit less about four.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Once we've mapped our posts into this sort of lower dimensional space, we still haven't gotten yet to the to the higher level questions that we're interested in, so I hinted before when I showed you some of those latent topics that some of the topics we find really look like substance, and some of them really look like status.",
                    "label": 0
                },
                {
                    "sent": "And So what we did was we manually annotated the latent topic dimensions with one or more of substance status, social or style.",
                    "label": 1
                },
                {
                    "sent": "As basically saying this, when this topic is used, it is used in a way that evokes language that looks like substance, language, status, language, social language or stylistic language use.",
                    "label": 0
                },
                {
                    "sent": "So I'll give you some examples of what that looks like in a second, but we have fairly good Inter annotator agreement about those labels and we also said that things like hashtags are substance represents substance, because people are usually making them to be found and re indexed by others.",
                    "label": 1
                },
                {
                    "sent": "But of course, all of these are shortcuts and any individual dimension is probably going to be some mix of all four of those.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what can we do now that we have these topics labeled with those categories?",
                    "label": 0
                },
                {
                    "sent": "Well, we can take the whole corpus of tweets and we can do the same trick that I just showed you for The Muppets.",
                    "label": 0
                },
                {
                    "sent": "A couple slides back and we can look at how often each topic is used and then once we know how often each topic is used, we can see well how often are the substance topics used, the status ones, the social ones in the style ones.",
                    "label": 0
                },
                {
                    "sent": "So as far as we know, we think this is the first picture that looks at.",
                    "label": 0
                },
                {
                    "sent": "Looks like this for trying to describe overall on Twitter.",
                    "label": 0
                },
                {
                    "sent": "Is language used in a social, substantive, stylistic, or status E way so despite my own impression, sometimes that most of 80% of Twitter is is about sandwiches.",
                    "label": 0
                },
                {
                    "sent": "It turns out that only about sort of 12% is sort of that Canonical status update style language.",
                    "label": 0
                },
                {
                    "sent": "Stylistic language is sort of a large category here because it reflects a lot of common words.",
                    "label": 0
                },
                {
                    "sent": "You'll end up with people, so this one on the left is.",
                    "label": 0
                },
                {
                    "sent": "Peoples representations of things that that are funny.",
                    "label": 0
                },
                {
                    "sent": "So different ways of laughing this other one represents sort of a particular dialect, perhaps of the way people might talk in English.",
                    "label": 0
                },
                {
                    "sent": "And of course there's some overlap, because all of these have some social intent, or many of them have some social intent as well.",
                    "label": 0
                },
                {
                    "sent": "So the but the social ones that we, the ones that we clearly marked as social or things like asking for help in the top left or saying what's up next to that.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I just gave you a bit of an overview of the kind of technique we're going to use.",
                    "label": 0
                },
                {
                    "sent": "I'm going to show you some pictures now of the types of ways interfaces we built for helping to characterize sets of posts into a space that looks like that, and then some applications for recommending and filtering based on it.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is a screenshot of a Now live demo built by Dan Liebling, one of our Co.",
                    "label": 0
                },
                {
                    "sent": "Authors and and it is.",
                    "label": 0
                },
                {
                    "sent": "I'll give you the URL at the end of the talk.",
                    "label": 0
                },
                {
                    "sent": "What you do is you type in a query or username in that box in the top and it pulls from Twitter's public API set of posts and for that set of posts it Maps the words in those posts into these dimensions.",
                    "label": 0
                },
                {
                    "sent": "The topic dimensions from a model that we learned and then it shows that.",
                    "label": 0
                },
                {
                    "sent": "So we see here for the Microsoft Research Twitter account that it's actually mostly substance.",
                    "label": 0
                },
                {
                    "sent": "The Big Blue box on the left is the substance topics that are mostly used by Microsoft.",
                    "label": 0
                },
                {
                    "sent": "So the largest one.",
                    "label": 0
                },
                {
                    "sent": "Of course within there is web and then within web we see the words that are used, shaded by how often this set of posts uses that word.",
                    "label": 0
                },
                {
                    "sent": "So the light Gray ones are words that were used by other people who use that topic, but not necessarily.",
                    "label": 0
                },
                {
                    "sent": "This person who used that topic.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By contrast, we can see something like the Microsoft Bing account actually is a lot of social language usage.",
                    "label": 0
                },
                {
                    "sent": "It's about half social and half substance, and that's because this account actually talks more directly, interacts with other users on Twitter, spends a lot of time replying to people who've spoken to it.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so then we just to bring things back home.",
                    "label": 0
                },
                {
                    "sent": "We are interested in those problems about finding and filtering.",
                    "label": 1
                },
                {
                    "sent": "How do we?",
                    "label": 0
                },
                {
                    "sent": "How do we recommend more people to follow and filter content?",
                    "label": 0
                },
                {
                    "sent": "Because we know that we're not doing a good job of that right now, just by showing all the posts that come from a particular person.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we used the data set that I mentioned earlier.",
                    "label": 0
                },
                {
                    "sent": "There are about 4000 judgments that people had made of posts from people that they follow.",
                    "label": 0
                },
                {
                    "sent": "We split for each Raider their judgments into a training set and test set, and so to get a sense of how we might perform at a filtering task.",
                    "label": 0
                },
                {
                    "sent": "We basically say that.",
                    "label": 0
                },
                {
                    "sent": "We re rank the test set according to its distance to the positive examples in the training set and so.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we mean is if you take the posts that were if you take the post that the person had rated, and if you consider the may be worth the time spent reading, or definitely worth the time spent reading as good examples.",
                    "label": 0
                },
                {
                    "sent": "We want to re rank the remainder of those posts, the remaining 30% to put those ones that we know that the person liked up at the top.",
                    "label": 0
                },
                {
                    "sent": "If we were to then take that rank ranking and threshold it, it would be something like a filter.",
                    "label": 0
                },
                {
                    "sent": "You could also think of this as a multi as a classification task, but we wanted to keep.",
                    "label": 0
                },
                {
                    "sent": "Evaluation simple in this context, just to be able to compare feature spaces.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The feature spaces we want to compare our the raw cosine similarity based on the word features that we see in those posts will reduce dimensional space from the labeled LDA model and a weighted combination of the two.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so, um.",
                    "label": 0
                },
                {
                    "sent": "This is the in the paper.",
                    "label": 0
                },
                {
                    "sent": "There's other sorts of valuation metrics in more details here, but I'm just showing you the mean reciprocal rank of the first relevant item.",
                    "label": 0
                },
                {
                    "sent": "The random baseline labeled LDA feature space does only a smidge better than the random baseline TF.",
                    "label": 0
                },
                {
                    "sent": "IDF does a bunch better, but this combination of the two actually does a lot better than either independent individually.",
                    "label": 0
                },
                {
                    "sent": "The reason for that is because if you have two posts, or where they have some words in common.",
                    "label": 0
                },
                {
                    "sent": "Then the cosine similarity is actually going to give you a pretty high score, because they're going to have those.",
                    "label": 0
                },
                {
                    "sent": "If there's a lot of overlap in their 13 words in a particular post with the centroid of the positive examples, then you want to know that that's going to be a good example of something that's very likely to be positive, and you don't want to lose it, whereas.",
                    "label": 0
                },
                {
                    "sent": "Whereas if you don't, if you have no overlap, that doesn't necessarily mean it's not relevant.",
                    "label": 0
                },
                {
                    "sent": "If you have no overlap, it might just be because you picked a different 13 terms, and you probably should have used some.",
                    "label": 0
                },
                {
                    "sent": "You could have used some synonyms or something and gotten some good results, so when that that kind of example is where the labeled LDA reduced feature space would do well for you.",
                    "label": 0
                },
                {
                    "sent": "But sometimes you'll get some false conflations.",
                    "label": 0
                },
                {
                    "sent": "So in this third column, what you're doing is you're saying.",
                    "label": 0
                },
                {
                    "sent": "When the cosine similarity score is fired, you should use it.",
                    "label": 0
                },
                {
                    "sent": "There's good overlap in the words, and when it doesn't fire, fall back to the labeled LDA model.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We see something similar in the finding task where the goal is to take the posts from the some subset of the Raiders so our rater has has.",
                    "label": 1
                },
                {
                    "sent": "Labeled examples from several different posters and seven in this case, and we're going to ask the we're going to take six of those seven as the training set, and we're going to ask our model to try to rank the remaining user that they actually do follow.",
                    "label": 0
                },
                {
                    "sent": "We're going to try to rank that user highly in a set of eight people that they don't follow, so the goal here is to basically try to recommend a new user to follow that we know they actually do follow.",
                    "label": 0
                },
                {
                    "sent": "We're just not training on it right now and defined it among the set.",
                    "label": 0
                },
                {
                    "sent": "That they don't follow.",
                    "label": 0
                },
                {
                    "sent": "Similarly, we find that TF IDF by itself beats labeled LDA in a weighted combination, does really well.",
                    "label": 1
                },
                {
                    "sent": "This actually is.",
                    "label": 0
                },
                {
                    "sent": "Nearly perfect at finding the.",
                    "label": 1
                },
                {
                    "sent": "Test Raider or finding the test post are among the set of the nine possible examples.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just to conclude, we spent some.",
                    "label": 0
                },
                {
                    "sent": "We think that content analysis on microblogs is sort of in the way to go with meeting some unmet information needs.",
                    "label": 1
                },
                {
                    "sent": "There's a lot.",
                    "label": 0
                },
                {
                    "sent": "We've only scratched the surface in this kind of work.",
                    "label": 1
                },
                {
                    "sent": "There's lots more techniques that people could do in this space, and but we think that for characterizing, recommending, and filtering models like this can be useful.",
                    "label": 1
                },
                {
                    "sent": "Labeled LDA model comes up with some dimensions that correspond nicely with our intuitions for why people make.",
                    "label": 0
                },
                {
                    "sent": "Following decisions and it can provide some insight into language use about large scale trends of language in general as well as what individual hashtags and emoticons are about within that context.",
                    "label": 0
                },
                {
                    "sent": "There's lots of next steps we'd like to do, including temporal dynamics, include incorporating network effects and sorts of flow features, and so this is.",
                    "label": 0
                },
                {
                    "sent": "This is our URL we would love for you to play with it and send us feedback.",
                    "label": 0
                },
                {
                    "sent": "It's just gone live, and so for those of you who hit it now, you have the unique opportunity to be the first people to tell us all the things that are wrong with it.",
                    "label": 0
                },
                {
                    "sent": "Thanks very much.",
                    "label": 0
                }
            ]
        }
    }
}