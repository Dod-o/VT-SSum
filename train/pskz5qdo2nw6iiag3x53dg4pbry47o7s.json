{
    "id": "pskz5qdo2nw6iiag3x53dg4pbry47o7s",
    "title": "Sample Complexity of Testing the Manifold Hypothesis",
    "info": {
        "author": [
            "Hariharan Narayanan, Laboratory for Information and Decision Systems, Massachusetts Institute of Technology, MIT"
        ],
        "published": "March 25, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Mathematics"
        ]
    },
    "url": "http://videolectures.net/nips2010_narayanan_sct/",
    "segmentation": [
        [
            "The basis of manifold learning is that high dimensional data often lies in the vicinity of a low dimensional manifold, and the rational usually given for this is that.",
            "High dimensional data is often generated by some process which has few essential degrees of freedom and this is then reflected in the nature of data.",
            "So it's of interest to be able to test this hypothesis, since that would tell us something about the regime in which these methods are expected to be successful.",
            "And an actual question is, can we test this using limited data that does not depend on the ambient dimension, since that's the whole purpose.",
            "So this is the."
        ],
        [
            "In the address.",
            "So before I.",
            "Go into that.",
            "Let me talk about the class of manifolds will be interested in, so we're going to look at the set of all manifolds of small dimension contained in a high dimensional Euclidean ball whose curvature is bounded above by some koppa, and volume is bounded above by some V. And we're going to try to minimize the least squared loss of fitting such a manifold to some data points.",
            "An ask how many data samples do I need to choose before fitting a manifold from this class is not going to result in overfitting.",
            "And we have."
        ],
        [
            "Positive results there, which says that the number of samples that you need does not depend on the ambient dimension, but only on certain complexity measures of the low dimensional manifolds one is attempting to fit.",
            "The dependence happens to be exponential in the.",
            "Intrinsic dimension polynomial in the curvature and linear in the volume."
        ],
        [
            "So finally, so along the way we also prove some.",
            "Sample complexity bounds for K means the best upper bound that's known for fitting for K means clustering in terms of generalization.",
            "Error is used to be quadratic in K and the lower bound is linear in K, so we improve this bound by.",
            "By having some term that depends both on Karen Epsilon and Secondly the way this is done is by.",
            "Using fat shattering dimension by bounding the fat shattering dimension of the certain function class by first doing a random projection into a low dimensional subspace and then applying VC dimension on the low dimensional subspace so that the complexity measures that matter are only low dimensional.",
            "My poster will be on 46, so you're welcome to come and ask any questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The basis of manifold learning is that high dimensional data often lies in the vicinity of a low dimensional manifold, and the rational usually given for this is that.",
                    "label": 1
                },
                {
                    "sent": "High dimensional data is often generated by some process which has few essential degrees of freedom and this is then reflected in the nature of data.",
                    "label": 0
                },
                {
                    "sent": "So it's of interest to be able to test this hypothesis, since that would tell us something about the regime in which these methods are expected to be successful.",
                    "label": 0
                },
                {
                    "sent": "And an actual question is, can we test this using limited data that does not depend on the ambient dimension, since that's the whole purpose.",
                    "label": 0
                },
                {
                    "sent": "So this is the.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the address.",
                    "label": 0
                },
                {
                    "sent": "So before I.",
                    "label": 0
                },
                {
                    "sent": "Go into that.",
                    "label": 0
                },
                {
                    "sent": "Let me talk about the class of manifolds will be interested in, so we're going to look at the set of all manifolds of small dimension contained in a high dimensional Euclidean ball whose curvature is bounded above by some koppa, and volume is bounded above by some V. And we're going to try to minimize the least squared loss of fitting such a manifold to some data points.",
                    "label": 0
                },
                {
                    "sent": "An ask how many data samples do I need to choose before fitting a manifold from this class is not going to result in overfitting.",
                    "label": 0
                },
                {
                    "sent": "And we have.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Positive results there, which says that the number of samples that you need does not depend on the ambient dimension, but only on certain complexity measures of the low dimensional manifolds one is attempting to fit.",
                    "label": 1
                },
                {
                    "sent": "The dependence happens to be exponential in the.",
                    "label": 1
                },
                {
                    "sent": "Intrinsic dimension polynomial in the curvature and linear in the volume.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So finally, so along the way we also prove some.",
                    "label": 0
                },
                {
                    "sent": "Sample complexity bounds for K means the best upper bound that's known for fitting for K means clustering in terms of generalization.",
                    "label": 1
                },
                {
                    "sent": "Error is used to be quadratic in K and the lower bound is linear in K, so we improve this bound by.",
                    "label": 0
                },
                {
                    "sent": "By having some term that depends both on Karen Epsilon and Secondly the way this is done is by.",
                    "label": 0
                },
                {
                    "sent": "Using fat shattering dimension by bounding the fat shattering dimension of the certain function class by first doing a random projection into a low dimensional subspace and then applying VC dimension on the low dimensional subspace so that the complexity measures that matter are only low dimensional.",
                    "label": 0
                },
                {
                    "sent": "My poster will be on 46, so you're welcome to come and ask any questions.",
                    "label": 0
                }
            ]
        }
    }
}