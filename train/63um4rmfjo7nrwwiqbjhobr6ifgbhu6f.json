{
    "id": "63um4rmfjo7nrwwiqbjhobr6ifgbhu6f",
    "title": "Cost-sensitive Classification: Algorithms and Advances",
    "info": {
        "author": [
            "Hsuan-Tien Lin, Department of Computer Science and Information Engineering, National Taiwan University"
        ],
        "published": "March 27, 2014",
        "recorded": "November 2013",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/acml2013_lin_cost_sensitive_classification/",
    "segmentation": [
        [
            "So today I'm going to talk about cost sensitive classification with the algorithms that I know and some of the advances that we see in the past few years.",
            "Anne."
        ],
        [
            "Originally I prepare something to introduce myself, but listening has done a very good job and telling people that so we have so that the team, National University, and I'm honored to be part of them have warm some past KDD cops and also I'm doing research on many interesting new problems in machine learning including multi label classification, ranking and also active learning.",
            "I've been doing cost sensitive classification research starting from 2007 that was back to the day when I was a grad student an.",
            "Also I'm currently serving as the Secretary General of the Attorneys Association of Artificial Intelligence.",
            "So if you happen to come to Taiwan and want to discuss with communist researchers in artificial intelligence or machine learning.",
            "Please feel free to let me know an isolation mentioned.",
            "I'm going to teach Mandarin based massive online courses in machine learning starting November 26, so that's the address that's going to be on Kucera.",
            "So and he has collaborated with Kozyra in presenting measuring based machine learning courses, and I'm going to be one of the first professors to be teaching on that platform in.",
            "Mandarin, so if you happen to be having students being measured, speaking, or yourself like to learn a little bit more Mandarin, please feel free to go through this course."
        ],
        [
            "OK, so for today I'm going to start from the easiest product.",
            "So many of you may have known that we can do binary classification in the cost sensitive mode and we will need those for the US.",
            "The basic tools for the multi class case.",
            "So I'll start from the binary case, probably slightly faster and then go to the multiclass case which would be the core of the talk today.",
            "And if you have any questions please feel free to just interrupt me.",
            "And we can discuss on the way."
        ],
        [
            "OK, so let's start with a simple problem.",
            "OK, is this your fingerprint?",
            "So let's say we're going to build a fingerprint recognition system.",
            "Let's say on your laptop or whatever place in which you need to be identified as the right user of the laptop before using it.",
            "And there are two cases.",
            "Of course one is U and the other is the intruder who tries to steal your data for anything bad.",
            "And this is a binary classification problem, as many of you know.",
            "And if you see this, you are probably wondering, this is a machine learning conference.",
            "OK, you're talking about binary classification.",
            "We all know too well about it.",
            "Well, the reason I'm going to talk about this is just to set up some of the notations I'm going to use throughout the talk, so that will be on the same line."
        ],
        [
            "So what we will do is basically in a supervised machine learning setting OK an because of the time constraints.",
            "I'm not going to touch other settings like unsupervised or reinforcement or some other settings in which actually you can use cost sensitive classification also there, but we're going to talk about the basic one, the supervised setting.",
            "And as we know, supervised setting is much like teaching the student or teaching the kit.",
            "OK, we present.",
            "And the kid with a bunch of examples that contains labels or or the intended meanings.",
            "And the kid tries to figure out from a bunch of possibilities the best hypothesis, the best function to be used for the future and in notation we're going to denote this.",
            "True OK, the thing that we want the key to know by FOK.",
            "So F would mean our target function and we're going to learn from a bunch of hypothesis that we call edge right away.",
            "I can use this.",
            "This works better, OK?",
            "OK, we have edge here.",
            "That's our learning model.",
            "We're going to learn from a bunch of hypothesis.",
            "And to get the final decision function that we call G. OK, so those are the notation that we use and for the examples we present to the learning algorithm, we're going to use X as the inputs and then the Y as the output part.",
            "So that's the typical supervised learning setting.",
            "And one key question, in this supervised setting is how we're going to evaluate whether the decision function that we get really corresponds or it really equals the target function that we want.",
            "OK, this is the core question of supervised machine learning.",
            "An will be the main topic we discussed today."
        ],
        [
            "So let's look at this fingerprint verification problem OK. You have a target function.",
            "It's going to say positive one it's you and it's going to say negative one.",
            "It's the intruder.",
            "OK, so we don't use positive for the good cases and negative for the bad cases.",
            "And by the way, OK, this example is actually barreled from the textbooks that lacing just mentioned learning from data book.",
            "It's on Amazon and I'm happy to be one of the Co authors of the book.",
            "It's a very, very short book on machine learning.",
            "It's actually only five chapters.",
            "The story is that we started planning for much longer, but then realized that we only have the energy to finish a very very core part, but for that core part we think it's proper for people entering machine learning in all kinds of fields like biology or some other fields to understand machine learning, and that's how the book was out.",
            "OK, and with so following this example you see that there are two types of error when we call the false accept an one calling the false reject.",
            "So for first accept actually mean OK, we want.",
            "OK, that that's you.",
            "OK sorry, that's not you OK, but accidentally the machine says yes and accept the intruder from using your machine.",
            "So that's false.",
            "Accept the actual label.",
            "It's not you, but the machine accepted that.",
            "The other case is false.",
            "Reject OK in which case, OK, that's you.",
            "But when you put your fingerprints there, the machine somehow says no.",
            "That's not you try again or something like that.",
            "And simplest choice, as we all do in binary classification is to penalize these two kinds of errors equally OK, and something together.",
            "OK, so that's how we calculate the usual error rate when we calculate the user error rate.",
            "OK, whether it's false positive or false, negative or false, accept false reject.",
            "We calculated the same an average day.",
            "That's the simple case in binary classification."
        ],
        [
            "The case we are going to talk about goes beyond that.",
            "Let's say we're going to use this fingerprint system for the supermarket OK for deciding whether we would give the same customers some discount.",
            "If you are a frequent customer of the supermarkets, then you'll get some more discounts.",
            "If you're not, then then the the supermarket would just give you a user price, so that's the use for fingerprint verification.",
            "In the supermarket problem.",
            "Then what happens between the two cases?",
            "Well, let's say you are truly a loyal customer to the supermarket, but then you go to the system, put your fingerprints there, and the system says no reject OK, you are you are you are you are really not our loyal customer.",
            "Blah blah blah.",
            "You probably would feel very angry about it.",
            "OK?",
            "You probably would say wow, what a bad Superman.",
            "OK, it's recording so I cannot say all the bad word and so on.",
            "But let's say OK, you you?",
            "Probably say OK, this is a bad system.",
            "I wouldn't come to this supermarket again, blah blah blah.",
            "You would be left with a really unhappy customer for the supermarket, possibly losing some future business with the customer.",
            "So the first reject for the supermarket may be of high penalty.",
            "On the other hand, let's say the system false accept someone.",
            "OK, so basically that's not a loyal customer, but then the system just says yes, let's offer the customer some discount, and so on for the.",
            "For the supermarket it's a relatively small loss, so maybe it's just one kind of loss or or something and kindly and after the discount, probably the supermarket still makes money, just less money.",
            "OK, but it's it's.",
            "Not really that serious and also if that's truly an intruder for the system and ensure the left the fingerprint on the machine well you may have some way to catch them or or something so relatively.",
            "This is not a serious mistake.",
            "So in the supermarket case we view the first reject.",
            "Uh, this is serious mistake, but the false accept mass of Sears.",
            "If we put them into numbers OK, I mean, those are just artificial numbers, but let's say OK, maybe we view the false reject could be 10 times more serious than the false accept in the supermarket case."
        ],
        [
            "Let's look at another case.",
            "Let's say we are using this fingerprint system for the CIA for something or confidential, and so on.",
            "No, let's look at the CIA.",
            "Use for the fingerprint application.",
            "If our system gives a false accept, which means OK, really the person should not be authorized to to use these parts, but then your assistance is yes you can.",
            "You can get in there, you can do whatever confidential seeing that you originally were not supposed to do.",
            "That's a serious mistake.",
            "On the other hand, if it's a false reject, so let's say, OK, I go there.",
            "I'm a CIA worker.",
            "I go there, I got rejected.",
            "I just try again.",
            "I work in CI, should accept this kind of mistakes and so on for security reasons.",
            "So basically there will be unhappy employee from the CSA, not get it, but it's less serious then leaking out confidential information to people who should not access that.",
            "So in the CIA case, probably we would evaluate the performance like this.",
            "Basically a false reject.",
            "Much less serious or false.",
            "Accept very, very serious.",
            "Let's denote it by 1000 or or.",
            "Something like that.",
            "So we see that even just for binary classification for the fingerprints verification case, if we put the system into different applications, we will have to consider different kinds of costs to satisfy the need of the system."
        ],
        [
            "Now the original problem that the simplest one that we consider was binary classification, in which the regular one in which we just penalize all the errors equally.",
            "OK, so we have one here.",
            "For the false reject, one here for the false accept equally.",
            "And in that case, OK in our example.",
            "In our training example, we will calculate our error.",
            "We call it in, meaning the in sample error we calculate the error of the hypothesis by considering whether the label OK.",
            "The desert label equals the prediction of the hypothesis and if it's not equal.",
            "I'm using a Boolean operation.",
            "Here we will cut count the error by one if it's not equal.",
            "We count by the group, so that's how we calculate our in sample error.",
            "And similarly we can count our out of sample error by picking an expectation over the distribution that our data is generated from.",
            "That's the typical setting of statistical learning, and for this regular binary classification problem it's well studied.",
            "You probably all know lots of algorithms about regular binary classification.",
            "Yeah, you know."
        ],
        [
            "Now we're going to talk about the cost sensitive one, much like the supermarket case and the first one.",
            "We're going to talk about is called the class weighted case for the class weighted case.",
            "We have OK cost metrics that we list here, let's see.",
            "We have this matrix.",
            "OK, for this cost metrics, it's encodes our intention OK, which kind of error should be penalized more?",
            "Which should be penalized less, and so on.",
            "And naturally we assume that the diagonal part of this cost metrics to be 0, meaning no cost is the correct prediction.",
            "We want positive the system predicts positive.",
            "We want negative, the system predicts negative, and for the other two cases we can fill in the number 2.",
            "Encode our dimension of whether to charge this kind of cost more or less.",
            "And in that case we're going to calculate the in sample errors differently.",
            "OK, so you see that I list the equivalent formulas here.",
            "There are basically 2 cases.",
            "If.",
            "There's something wrong.",
            "Which is similar to the previous case of regular classification.",
            "Originally, when there's something wrong, we basically add a penalty of what, now, when there's something wrong, we check the two different cases, one being a false reject case.",
            "We see here.",
            "Then basically we add a penalty of 10.",
            "And then if it's false accept case, we add a penalty of what.",
            "So for the two cases, we add different penalties.",
            "Similarly, we can list the out of sample performance formula.",
            "It's just the same.",
            "OK, just replacing those incident called XY.",
            "The data that we have on hand with the out of sample data that we get from the distribution of interest.",
            "And for this setting it's usually called class weighted cost sensitive classification.",
            "So class weighted meaning we really have different weights of our different penalties for examples from different classes.",
            "If the example is from the positive class, the penalty is 10 of.",
            "The example is from the negative class, the penalty is.",
            "What this penalty corresponds to.",
            "Wait an and there are two kinds of weighted binary classification problem that we will see.",
            "And this is the simpler one.",
            "Just wait by the class."
        ],
        [
            "Let's look at the formal setting of this problem.",
            "Basically, in this problem we're given a bunch of examples.",
            "We're given a bunch of examples, and in addition to the examples we have, those cost information.",
            "Basically, we have a whole cost table, but because of our assumption all we really care are the two entries that we called W plus and W minus, and that's representing the two entries of the cost metrics, so we can view the cost us an additional piece of.",
            "Information to guide the machine on how we're going to evaluate.",
            "If you're teaching this is much like telling your students OK which parts of your lecture would be counted more in the exam?",
            "Which part of the lecture will be counted less in the exam so they can be prepared accordingly?",
            "So we are giving the machine an additional piece of information.",
            "And what we want?",
            "Well, when the machine sees this information in addition to the data, it can use the information to guide the learning process to give us a decision function that pays a small cost.",
            "So we use the same formula to do evaluation during the testing phase.",
            "OK on future unseen example following our definition in this static statistical sense, it actually means we want the machine to reach a load out in this cost sensitive.",
            "Or weighted definition.",
            "And for regular classification you see that it's just a special case in which we have W plus and W minus both equals what?"
        ],
        [
            "OK, so let's revisit the supermarket case.",
            "When we have the false accept the false reject and so on.",
            "Previously we say OK, we have two kinds of customers, one being the loyal customers we want to be careful not to reject those, one being the intruders, and basically it's OK if we occasionally accept them.",
            "No, there may be some customers who are like super loyal customers or something, or we called in the big customers.",
            "Maybe they buy a lot from the supermarket or something, and if you follow my argument previously you will see that we definitely don't want to reject those customers.",
            "OK, we can reject the occasional ones.",
            "OK, it doesn't matter, but if it's a big customer who comes to the supermarket once a day, buying lots of grocery or something.",
            "The supermarket may say, OK, I really don't want to reject it, so there are different kinds of customers, big ones, or the usual ones.",
            "And for those customers may be OK. Our desire to really accept it or reject it would be different.",
            "In that case, maybe for the big customers, the penalty we should charge for rejecting them is higher then the usual customer.",
            "OK, on the other hand, for the usual customer may be OK.",
            "It's high but maybe not as high as the big one.",
            "So you see that OK for different customers?",
            "Although OK, they are both customers both for both of them.",
            "We probably should accept them rather than reject him, but we really should charge the penalties differently."
        ],
        [
            "OK, so in that case we see that and we we have three kinds of costs, vectors and now we call the cost vectors which just corresponds to previously the roofs of the cost metrics.",
            "We only need the rope to consider the importance of each customer.",
            "And in that case that important.",
            "I mean, we really only have 1 degree of freedom.",
            "The authority under the correct case will always assume to be having 0 cost and for the other case we say OK, the importance will be called WN.",
            "For the examples that we have on here.",
            "OK, so if we know the importance of each customer that we have on hand and we will use similar importance to evaluate the future performance of the system.",
            "How can we do well and that is usually called an example weighted cost sensitive classification.",
            "OK, so for each example we have the weights OK for even for examples of the same class they can carry different weights and so on.",
            "K so different W 4 different X&Y for people who have learned about Adaboost, you have probably seen this.",
            "OK, this is actually those weights are exactly what we feed to the weak learners of the edibles.",
            "To get diverse weak classifiers and so on for basically combining them in the future.",
            "So if you have used edibles, you probably have used example weighted binary classification internally in your weekly."
        ],
        [
            "OK, so let's look at the setting here OK and example weighted setting for binary cost sensitive classification in addition to the data that we give to the machines, we have an additional part of the data that we call the weight.",
            "OK, so for this weight.",
            "So basically we actually intend to mean each example is of different importance and that important is used to calculate the in sample error.",
            "The training area that we have on hand.",
            "How about for the future?",
            "For evaluating the test performance, we will use some weights to evaluate the performance of our decision function RG as well.",
            "And for this unseen example we have the usual X.",
            "That our decision function would know, but we assume that the label and the weights are hidden from the decision function.",
            "So basically we generate examples.",
            "Now with an additional entry called W. OK, but when making a decision we don't really give the digital functions adopted.",
            "We just say OK, there's X, make your decision.",
            "I only guarantee you that the W would be generated similarly to what you have seen in the training phase.",
            "OK, so following those settings you probably see that we discuss about this.",
            "The regular classification is a special case of the class waited setting by setting both ways to what and the class waited.",
            "Setting is a special case of the example weighted settings example where this setting more general.",
            "You can have different weights for different examples.",
            "Class weighted setting.",
            "You just set the positive examples to one particular wait.",
            "The negative examples too.",
            "Another part of it."
        ],
        [
            "OK, so how do we do binary cost sensitive classification and the lecture I'm going to show you follow some historical trace on the popular methods for binary weighted cost sensitive classification and the earliest one starts from the patient perspective."
        ],
        [
            "In the patient perspective, OK, we start asking ourselves the question.",
            "We want a classifier that pays a small cost.",
            "Now if we look at the patients and so everything should prove to be put under probability and so on, we see that the expected error that we are going to pay.",
            "For predicting positive one on a particular example X.",
            "So let's say we fix one particular X1 factor fingerprint and say OK, what's the expected error?",
            "We see that the price we're going to pay is W -- * P of minus given X.",
            "So if this particular example is labeled negative one by your target function plus the noise distribution, so you really get a label.",
            "Of negative one.",
            "And then you pretty positive.",
            "OK, so that's a false accept and in that force accept case, on average we pace this much.",
            "The other case is if we insist on predicting negative if we predict negative OK, then we pay for the false reject cases and for the false reject case.",
            "On average we pay this much.",
            "OK, so you have two choices.",
            "You are given one fingerprint.",
            "You have two choices, positive one or negative one and the average error you receive are those two numbers.",
            "So now.",
            "Of course, in the general machine learning, we assume that those P OK the conditional probability are unknown.",
            "But let's say, what if we know the conditional probability?",
            "If you know the conditional probability, then you can make the base optimal decision by saying OK, let me look at the expected error for predicting negative one expected error for predicting positive one, choose the smaller one.",
            "Right, OK, so you can only choose one and the other part of the error or the cost is unavoidable.",
            "So basically choose the smaller one.",
            "That's the equation that you get if OK, you know the true probability that's the base optimal decision.",
            "But now we don't know about the the true probability and so how can we do well what we can do is to take another function to estimate the conditional probability.",
            "So that's an estimate, and if our estimates is accurate to some sense, then and approximately good rule approximately good decision function would be just using this approximation.",
            "To replace the true probability that we see here.",
            "So how do we get a good conditional probability estimator?",
            "Well, you guys know more than me.",
            "Probably you can use logistic regression, naive Bayes, or whatever methods.",
            "Let's say maximum likelihood, maximum posterior blah blah blah for estimating the conditional probability.",
            "But I mean we have all the good tools.",
            "If you are going in the probabilistic direction."
        ],
        [
            "So this is 1 very simple algorithm I found in the literature that it doesn't really have a name, so I gave it a name approximate based optimal decision.",
            "OK, the name is my OK not from any literature.",
            "So basically what we want to do is to approximate a good solution using a good approximate approximator of the conditional probability.",
            "So back to 2001.",
            "That's from Malcolm and there's the derivation of the rule saying.",
            "OK, so we have this P. And basically if we follow this rule then we will predict a positive one.",
            "If OK.",
            "This inner part is positive.",
            "If this inner part is positive, really OK if we just reorganized the terms and so on.",
            "It corresponds to saying P of X.",
            "Which is our estimate of the conditional probability is greater than a threshold that's computed from the blue plus and W minus.",
            "It's a very simple calculation comes from switching the terms and so on, but I'm not going to the mathematical detail, but let's look at the numbers.",
            "If I substitute the numbers that we have OK to this stressful, we see that it evaluates to 1 / 11.",
            "For the supermarket case, which means we would easily accept OK if OK, the conditional probability is greater than 1 / 11 we would accept if the conditional probability is less than 1 / 11 we would reject, so we would easily accept.",
            "On the other hand, for the CIA case, we would easily reject OK, oh, so that's a typo here.",
            "Actually, that should be 1000 / 1001.",
            "OK, so we would easily reject only if we are highly confident that this is you.",
            "We would accept otherwise we would just view it as a intruded.",
            "So you see that the rule corresponds to our common sense of what we should do in the supermarket and the CIK supermarket case.",
            "Accept more easily ancis case reject more easily.",
            "Now, so for this approach you can just use your favourites algorithm to estimate this conditional probability and then for each new prediction you just basically form your prediction using a new threats code.",
            "That we just calculated above.",
            "So that's basically the approximate base optimal solution.",
            "You see that for the training situation, basically nothing is changed.",
            "OK if you are originally a big fan of estimating conditional probabilities, nothing is changed from the training part.",
            "All you need to do is to defer all the cost sensitive decision-making to the prediction part, calculate a new threshold, and use that source code for your future prediction.",
            "That's arguably one of the simplest approach of binary cost sensitive classification.",
            "You do probability, estimate and then you do thresholds changing."
        ],
        [
            "OK, so let me show you a picture of how things work in this case.",
            "Actually I just formed in artificial data OK with all those examples we have the positive ones that I highlight in circle.",
            "And the negative ones I highlight in cross.",
            "So basically this is the two dimensional data we feed to the machine learning algorithms.",
            "If you want to use this approach for cost sensitive binary classification and the training part, I say OK, there's nothing you need to do.",
            "You just run your favorite algorithm on conditional probability estimate.",
            "Let's say logistic regression and then you get some some estimates like this.",
            "So I'm using different colors.",
            "Here, if things gets blue, it means higher probability of being positive.",
            "If things get red, then it means more probability of getting negative.",
            "So get this picture.",
            "Now what you do during decision time is to set your decision boundary based on the cost metrics that you get.",
            "OK, so in the supermarket case, OK, this is the cost metrics you get.",
            "You use this cost metrics, there's W plus you're starting minus you do the calculation.",
            "You get that the stress cold is 1 / 11.",
            "You said the source code, two 1 / 11.",
            "Then this part is where you claim to be positive.",
            "This part is where you claim to be negative.",
            "You see that this OK setting the new stress called give you a shifted boundary from your urgent why?",
            "So this is what you do for regular classification.",
            "For regular classification.",
            "Remember we have W plus and minus both equal 1, so therefore the source code you are going to be setting in this case is 1/2.",
            "So if you set the source code to have, you probably would get a line somewhere really in the middle of circle and cross.",
            "But now that you change this, wrestle, and in this case we change the source code to 1 / 11, the line is somewhat pushed towards the negative case OK, because we want to be easily accepting in the supermarket case.",
            "So if you can get a good conditional probability estimator through logistic regression or whatever methods that you're interested in, what you can do really is to just change the stressful and done."
        ],
        [
            "OK, So what are the pros and cons of this simple approach?",
            "Well, one good thing as the Asian people would say is that it's optimal.",
            "If there's a good probability estimate based optimal solution, that's really theoretical, guaranteed.",
            "And if you are optimal or close to the optimal, you can get the strong theoretical guarantee that any patient setting can provide.",
            "I'm also it's simple.",
            "OK, basically, as we mentioned, the training part is almost unchanged and the prediction part only changes a little change the source code, nothing big.",
            "What are the disadvantages of this approach?",
            "Well, one thing is that often the good probability estimate, I mean, because you really need to get all those real numbers between 01 correct, are often harder to get.",
            "Then.",
            "Let's say you are getting a binary classifier.",
            "OK, so those are the debates from the discriminative and generative models and so on.",
            "I'm not saying which one is good or bad.",
            "But sometimes it's bad.",
            "Sometimes it's not easy to get a good generative model to get a good probability estimate.",
            "And also it's restricted OK.",
            "Remember in our setting what we need to do, we need to use the W plus and minus to calculate a new threshold.",
            "Now this is only available if you are in a metric setting.",
            "OK, if you are doing it vector really OK you vector setting.",
            "You only have a WN1 weight per example you don't have the other that we call the relative case to determine your threshold.",
            "So you really need to know the whole metrics in order to make the base optimal decision.",
            "You need the whole matrix OK to decide what to do and therefore, for example, you may not be able to couple this approach with weak learners in Adaboost, because in weak learners of edibles, what we essentially do is to provide each example of the different weights rather than in a in a class wise fashion.",
            "So you may ask, what are the other approaches for the example weighted set up rather than the class waited one that we can solve from the patient perspective?"
        ],
        [
            "OK, so now invasion perspective.",
            "Before we do that, I want to maybe stop for awhile and ask if there are any questions and so on questions.",
            "Confused.",
            "OK, if not, let's continue."
        ],
        [
            "OK, so the example weights OK that we use OK.",
            "Remember our goal is to pay a small cost for future unseen example that carries hidden weight.",
            "Summer.",
            "Now if we do this evaluation on one example.",
            "OK, So what XY?",
            "Then we need to charge the penalty by W, let's say W is 100, then OK, so this example run.",
            "We charge the penalty of 100.",
            "Let's say if we have 100 copies of the same example.",
            "So if there's one example, we charge 100.",
            "If I have 100 copies of the same example, what can we do?",
            "We can just charge each example by what equivalently, one example charge a hundred 100 examples charged by what?",
            "Now what do we mean by charge?",
            "Each error by what?",
            "Well?",
            "If you recall, that's just our old friend regular classification.",
            "So if we can manage to get W copies of the same example, then OK, we're simply in the training part.",
            "We charge all the penalty by one in the prediction part actually.",
            "Surely we charge the penalty by one, and so on, and we don't need to worry about those weights.",
            "That's the basic idea.",
            "OK, how do we do this copy?",
            "How do we virtually see that we have 100 copies of the same example while there are lots of ways?",
            "Of course, if you have integer weights, you can really just do copy.",
            "If you don't mind paying the computation.",
            "If you have weights that are real numbers, then you can probably do some oversampling according to the W that we want, and so on.",
            "And that's the method we're going to introduce next."
        ],
        [
            "OK, so that's our original problem.",
            "We have a bunch of examples.",
            "OK for some of them we want to basically consider a higher weight for some of them we want to consider a lower weight, OK, and so on.",
            "What we mean by example, copying or using oversampling or something is really to consider an equivalent problem like this.",
            "Now we're going to evaluate everything just by one that's regular classification.",
            "But in the data part, we do some copying, for example OK this one we are charging a cost of 10.",
            "We copy the data for 10 parts.",
            "For this one, we're charging for a cost of 100.",
            "We copy the data for 100 times.",
            "Get a bigger datasets and so are we send it to training.",
            "Get a classifier that classifier would do well in this regular classification problem.",
            "But this regular classification problem actually encodes all the information that we want in the cost sensitive classification problem that we have on the left hand side.",
            "And then how do you learn a good classifier for the right hand side?",
            "Well, again, you all know too well about binary classification.",
            "We have SVM.",
            "We have neural network, we have edibles, we have whatever your favorite binary classification algorithm that you want to use."
        ],
        [
            "OK, so basically what we need to do is to effectively transform our original datasets.",
            "Which contains the weight to a weightless or a regular classification data set that we see here.",
            "Hey Ann, I mean, we just show that you can do so by copying if you have integer weights and so on.",
            "But of course there are two approaches and the earliest approach.",
            "OK, we can do over or undersampling using some normalized W. So basically during your training you normalize those tablets so it's simulate a probability distribution and then you do the over or undersampling.",
            "Actually, some of you may often do so in edibles.",
            "Let's say with decision trees.",
            "An in some later works there are there proposed methods by that does undersampling by rejection.",
            "OK, so basically rejection based sampling rather than the basically the usual just wait based sampling.",
            "And then also you can also modify existing algorithms.",
            "Equivalently, in many of the machine learning algorithms, if you know the details of the optimal objective function that you're optimizing, you can plug in the weights to get an equivalent formulation.",
            "Those are possible ways to modify your original data to some equivalent data that's in a sense weightless.",
            "And then you can use your favorite algorithm to learn the classifier, and for this algorithm the prediction part is essentially unchanged.",
            "OK, previously the patient one prediction part change training part unchanged.",
            "Now we change the training part but not the prediction part.",
            "And it's simple in general.",
            "Many of you may have readily used it in one way or the other, and it's very popular for cost sensitive binary classification."
        ],
        [
            "Now let's look in more detail about what we mean by modification.",
            "Just to give everyone a sense of what I mean by modification, you can apply similar methods to other kinds of binary classification algorithms as well.",
            "If you do this modification, for example, let's say for SVM OK, so many of you know that in SVN basically you are solving a regularised hinge loss optimization problem.",
            "This is the.",
            "This is the regularization part and this is the hinge loss part.",
            "Now when your son is often urgent problem and you plug in the weights.",
            "OK, let's say you have an example that's twice the weight you want to charge it with twice the penalty.",
            "Then basically all need to do is OK. You have C for one of the example.",
            "You have C for the other copy.",
            "So then the penalty for actually for this example equivalently will be to see basically.",
            "What you do is to just change this C to C kinds.",
            "Other things unchanged.",
            "OK, so you can just do this.",
            "One line of modification an it's done.",
            "Of course this one line of modification depends on whether your solver supports that or or not, but there are like saying SVN there sleep SVN tools that actually supports this weighted formulation.",
            "You can feed the weights into that OK, but for formulation change is if you know the details of your underlying algorithm it's usually simple."
        ],
        [
            "So let's visualize the process of this modification on artificial data.",
            "What we do is starting from the original data.",
            "We copy the data for a few times over, simple or under simple or whatever to represent the different weights.",
            "Visually, what I do here is to basically show you some bigger circles.",
            "To mean that I'm putting a higher weight sound it OK, and then I send those information to a server that respects the size of those circles and crosses the weights of those and so on to get a classifier.",
            "You see here that OK?",
            "By considering this weights.",
            "OK, we get a classifier that basically is also pushed in this direction.",
            "So basically we have bigger circles and the classifier wants to avoid making those bigger circles wrong and therefore the boundary after training is effectively pushed towards the red direction.",
            "If you use a regular classifier, this is what we get.",
            "So you see that by using the weights properly during the training process, we can also get a reasonable cost sensitive binary classifier for the supermarket."
        ],
        [
            "An another approach, actually.",
            "This is pretty popular is basically doing this, but by rejection sampling, OK?",
            "The way we do in rejection sampling is basically just to get a random number for each example an if the random number is below the threshold that we set.",
            "In this case, the stress called will be just the normalized weight.",
            "Then we accept this example for training.",
            "If this random number is greater than this first, then we reject it.",
            "So effectively we would get a smaller datasets.",
            "Then we started for.",
            "They originally you have 1000 examples and a bunch of weight.",
            "So after rejection sampling you may just be getting 300 examples or something.",
            "Then after this rejection sampling you can learn a classifier based on those rejected examples and those examples would kind of come up with the distribution you are interested in that respects those weights or those costs.",
            "But of course you know the question you may have in mind is when you use rejection sampling, you have fewer examples and so on.",
            "Would you be getting worse classifiers and everything that's similarly happens to?",
            "For example, if you are backing and so on, getting a using fewer examples for classifiers, well, what you can do is to repeat the previous two steps multiple times.",
            "I mean subject to those randomness and so on.",
            "So you would get a bunch.",
            "Of different G an aggregate them very similar to what you would do in Bootstrap, every aggregation and so an for each new includes you can just predict with those aggregated G because each of them would be cost sensitive in some sense.",
            "Telling you the decision that respects those costs and weights.",
            "And when you aggregate then you can eliminate the variance in everything during your rejection sampling process.",
            "And this is commonly used if your favorite algorithm is a black box, OK, you don't know how to plug in the weights.",
            "You don't know how to do the modification rather than white box.",
            "OK, the SVN case we show you is a white box.",
            "Many of you here knows the details of the SVN.",
            "So basically we can do the modifications we want, But if you have an algorithm that you don't know how to modify or maybe people just provide the code to you so black box then you can couple.",
            "This algorithm, with the rejection sampling here an aggregation to get a cost sensitive binary classifier for the example weighted setting."
        ],
        [
            "OK, now in this very last part I'm going to give you my biased personal favorite.",
            "I mean, OK. Everyone has their bias favourites and so on, and people are biased by several things.",
            "OK, partly by their own work and so on, and partly by for example, the first method they started using everything.",
            "So I admit, OK for the personal favourites.",
            "I'm going to talk about here.",
            "It really doesn't cover all the approaches, include sensitive binary classification.",
            "There are many other interesting approaches and so on.",
            "Anne Anne, but I just want to share with you in case you are facing the axiom of choice.",
            "OK, don't know how to choose or or something.",
            "OK, so whenever possible I usually use so this reweighting OK.",
            "So cost proportional example relating by modification.",
            "So SVN neural network Adaboost OK for the algorithms I know to be a white box.",
            "I use modification simply be cause.",
            "Usually it gives you a somewhat more stable performance then rejection, sampling and aggregation.",
            "On the other hand, when would we use costing?",
            "Hosting is the last method that we introduced doing rejection sampling and then doing aggregation.",
            "Well, if I really only have the code on hand and I cannot afford to modify that, or if I need somewhat faster training because after rejection sampling I have fewer examples and so on, and for the many rounds of training I can send it to different machines and everything before aggregation.",
            "So if I need faster training, I will consider using rejection basically.",
            "And the performance is not that bad.",
            "OK, so if you aggregate enough classifiers, basically you get a relatively stable performance.",
            "Not even in the mood of patient.",
            "I mean, for some reason it seldom happens, but OK.",
            "If I have some particular applications that I really want to make everything patient could probability here and there, then I will consider using the approximate based optimal decision it simply because if I really have those probability then basically changing the final prediction rule will be just like.",
            "A very simple piece of cake.",
            "OK, that's my personal bias favorite.",
            "Of course, I believe if you have worked on weighted binary classification or sensitive finding classification, you may have your own favorite San.",
            "I mean, feel free to discuss with me if you have some thoughts in this part."
        ],
        [
            "OK, that's about end.",
            "The part I want to talk about in binary classification cost sensitive Byron classification.",
            "Are there any questions and anything?"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So today I'm going to talk about cost sensitive classification with the algorithms that I know and some of the advances that we see in the past few years.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Originally I prepare something to introduce myself, but listening has done a very good job and telling people that so we have so that the team, National University, and I'm honored to be part of them have warm some past KDD cops and also I'm doing research on many interesting new problems in machine learning including multi label classification, ranking and also active learning.",
                    "label": 1
                },
                {
                    "sent": "I've been doing cost sensitive classification research starting from 2007 that was back to the day when I was a grad student an.",
                    "label": 1
                },
                {
                    "sent": "Also I'm currently serving as the Secretary General of the Attorneys Association of Artificial Intelligence.",
                    "label": 0
                },
                {
                    "sent": "So if you happen to come to Taiwan and want to discuss with communist researchers in artificial intelligence or machine learning.",
                    "label": 0
                },
                {
                    "sent": "Please feel free to let me know an isolation mentioned.",
                    "label": 0
                },
                {
                    "sent": "I'm going to teach Mandarin based massive online courses in machine learning starting November 26, so that's the address that's going to be on Kucera.",
                    "label": 0
                },
                {
                    "sent": "So and he has collaborated with Kozyra in presenting measuring based machine learning courses, and I'm going to be one of the first professors to be teaching on that platform in.",
                    "label": 0
                },
                {
                    "sent": "Mandarin, so if you happen to be having students being measured, speaking, or yourself like to learn a little bit more Mandarin, please feel free to go through this course.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so for today I'm going to start from the easiest product.",
                    "label": 0
                },
                {
                    "sent": "So many of you may have known that we can do binary classification in the cost sensitive mode and we will need those for the US.",
                    "label": 0
                },
                {
                    "sent": "The basic tools for the multi class case.",
                    "label": 0
                },
                {
                    "sent": "So I'll start from the binary case, probably slightly faster and then go to the multiclass case which would be the core of the talk today.",
                    "label": 0
                },
                {
                    "sent": "And if you have any questions please feel free to just interrupt me.",
                    "label": 0
                },
                {
                    "sent": "And we can discuss on the way.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's start with a simple problem.",
                    "label": 0
                },
                {
                    "sent": "OK, is this your fingerprint?",
                    "label": 1
                },
                {
                    "sent": "So let's say we're going to build a fingerprint recognition system.",
                    "label": 0
                },
                {
                    "sent": "Let's say on your laptop or whatever place in which you need to be identified as the right user of the laptop before using it.",
                    "label": 0
                },
                {
                    "sent": "And there are two cases.",
                    "label": 0
                },
                {
                    "sent": "Of course one is U and the other is the intruder who tries to steal your data for anything bad.",
                    "label": 1
                },
                {
                    "sent": "And this is a binary classification problem, as many of you know.",
                    "label": 1
                },
                {
                    "sent": "And if you see this, you are probably wondering, this is a machine learning conference.",
                    "label": 0
                },
                {
                    "sent": "OK, you're talking about binary classification.",
                    "label": 0
                },
                {
                    "sent": "We all know too well about it.",
                    "label": 0
                },
                {
                    "sent": "Well, the reason I'm going to talk about this is just to set up some of the notations I'm going to use throughout the talk, so that will be on the same line.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we will do is basically in a supervised machine learning setting OK an because of the time constraints.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to touch other settings like unsupervised or reinforcement or some other settings in which actually you can use cost sensitive classification also there, but we're going to talk about the basic one, the supervised setting.",
                    "label": 0
                },
                {
                    "sent": "And as we know, supervised setting is much like teaching the student or teaching the kit.",
                    "label": 0
                },
                {
                    "sent": "OK, we present.",
                    "label": 0
                },
                {
                    "sent": "And the kid with a bunch of examples that contains labels or or the intended meanings.",
                    "label": 0
                },
                {
                    "sent": "And the kid tries to figure out from a bunch of possibilities the best hypothesis, the best function to be used for the future and in notation we're going to denote this.",
                    "label": 0
                },
                {
                    "sent": "True OK, the thing that we want the key to know by FOK.",
                    "label": 0
                },
                {
                    "sent": "So F would mean our target function and we're going to learn from a bunch of hypothesis that we call edge right away.",
                    "label": 0
                },
                {
                    "sent": "I can use this.",
                    "label": 0
                },
                {
                    "sent": "This works better, OK?",
                    "label": 0
                },
                {
                    "sent": "OK, we have edge here.",
                    "label": 0
                },
                {
                    "sent": "That's our learning model.",
                    "label": 0
                },
                {
                    "sent": "We're going to learn from a bunch of hypothesis.",
                    "label": 0
                },
                {
                    "sent": "And to get the final decision function that we call G. OK, so those are the notation that we use and for the examples we present to the learning algorithm, we're going to use X as the inputs and then the Y as the output part.",
                    "label": 0
                },
                {
                    "sent": "So that's the typical supervised learning setting.",
                    "label": 0
                },
                {
                    "sent": "And one key question, in this supervised setting is how we're going to evaluate whether the decision function that we get really corresponds or it really equals the target function that we want.",
                    "label": 1
                },
                {
                    "sent": "OK, this is the core question of supervised machine learning.",
                    "label": 0
                },
                {
                    "sent": "An will be the main topic we discussed today.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's look at this fingerprint verification problem OK. You have a target function.",
                    "label": 1
                },
                {
                    "sent": "It's going to say positive one it's you and it's going to say negative one.",
                    "label": 0
                },
                {
                    "sent": "It's the intruder.",
                    "label": 0
                },
                {
                    "sent": "OK, so we don't use positive for the good cases and negative for the bad cases.",
                    "label": 0
                },
                {
                    "sent": "And by the way, OK, this example is actually barreled from the textbooks that lacing just mentioned learning from data book.",
                    "label": 1
                },
                {
                    "sent": "It's on Amazon and I'm happy to be one of the Co authors of the book.",
                    "label": 0
                },
                {
                    "sent": "It's a very, very short book on machine learning.",
                    "label": 0
                },
                {
                    "sent": "It's actually only five chapters.",
                    "label": 0
                },
                {
                    "sent": "The story is that we started planning for much longer, but then realized that we only have the energy to finish a very very core part, but for that core part we think it's proper for people entering machine learning in all kinds of fields like biology or some other fields to understand machine learning, and that's how the book was out.",
                    "label": 0
                },
                {
                    "sent": "OK, and with so following this example you see that there are two types of error when we call the false accept an one calling the false reject.",
                    "label": 1
                },
                {
                    "sent": "So for first accept actually mean OK, we want.",
                    "label": 0
                },
                {
                    "sent": "OK, that that's you.",
                    "label": 0
                },
                {
                    "sent": "OK sorry, that's not you OK, but accidentally the machine says yes and accept the intruder from using your machine.",
                    "label": 0
                },
                {
                    "sent": "So that's false.",
                    "label": 0
                },
                {
                    "sent": "Accept the actual label.",
                    "label": 0
                },
                {
                    "sent": "It's not you, but the machine accepted that.",
                    "label": 0
                },
                {
                    "sent": "The other case is false.",
                    "label": 0
                },
                {
                    "sent": "Reject OK in which case, OK, that's you.",
                    "label": 0
                },
                {
                    "sent": "But when you put your fingerprints there, the machine somehow says no.",
                    "label": 0
                },
                {
                    "sent": "That's not you try again or something like that.",
                    "label": 0
                },
                {
                    "sent": "And simplest choice, as we all do in binary classification is to penalize these two kinds of errors equally OK, and something together.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's how we calculate the usual error rate when we calculate the user error rate.",
                    "label": 1
                },
                {
                    "sent": "OK, whether it's false positive or false, negative or false, accept false reject.",
                    "label": 0
                },
                {
                    "sent": "We calculated the same an average day.",
                    "label": 0
                },
                {
                    "sent": "That's the simple case in binary classification.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The case we are going to talk about goes beyond that.",
                    "label": 0
                },
                {
                    "sent": "Let's say we're going to use this fingerprint system for the supermarket OK for deciding whether we would give the same customers some discount.",
                    "label": 0
                },
                {
                    "sent": "If you are a frequent customer of the supermarkets, then you'll get some more discounts.",
                    "label": 0
                },
                {
                    "sent": "If you're not, then then the the supermarket would just give you a user price, so that's the use for fingerprint verification.",
                    "label": 1
                },
                {
                    "sent": "In the supermarket problem.",
                    "label": 0
                },
                {
                    "sent": "Then what happens between the two cases?",
                    "label": 0
                },
                {
                    "sent": "Well, let's say you are truly a loyal customer to the supermarket, but then you go to the system, put your fingerprints there, and the system says no reject OK, you are you are you are you are really not our loyal customer.",
                    "label": 0
                },
                {
                    "sent": "Blah blah blah.",
                    "label": 0
                },
                {
                    "sent": "You probably would feel very angry about it.",
                    "label": 0
                },
                {
                    "sent": "OK?",
                    "label": 0
                },
                {
                    "sent": "You probably would say wow, what a bad Superman.",
                    "label": 0
                },
                {
                    "sent": "OK, it's recording so I cannot say all the bad word and so on.",
                    "label": 0
                },
                {
                    "sent": "But let's say OK, you you?",
                    "label": 0
                },
                {
                    "sent": "Probably say OK, this is a bad system.",
                    "label": 0
                },
                {
                    "sent": "I wouldn't come to this supermarket again, blah blah blah.",
                    "label": 0
                },
                {
                    "sent": "You would be left with a really unhappy customer for the supermarket, possibly losing some future business with the customer.",
                    "label": 1
                },
                {
                    "sent": "So the first reject for the supermarket may be of high penalty.",
                    "label": 1
                },
                {
                    "sent": "On the other hand, let's say the system false accept someone.",
                    "label": 0
                },
                {
                    "sent": "OK, so basically that's not a loyal customer, but then the system just says yes, let's offer the customer some discount, and so on for the.",
                    "label": 0
                },
                {
                    "sent": "For the supermarket it's a relatively small loss, so maybe it's just one kind of loss or or something and kindly and after the discount, probably the supermarket still makes money, just less money.",
                    "label": 0
                },
                {
                    "sent": "OK, but it's it's.",
                    "label": 0
                },
                {
                    "sent": "Not really that serious and also if that's truly an intruder for the system and ensure the left the fingerprint on the machine well you may have some way to catch them or or something so relatively.",
                    "label": 0
                },
                {
                    "sent": "This is not a serious mistake.",
                    "label": 1
                },
                {
                    "sent": "So in the supermarket case we view the first reject.",
                    "label": 0
                },
                {
                    "sent": "Uh, this is serious mistake, but the false accept mass of Sears.",
                    "label": 1
                },
                {
                    "sent": "If we put them into numbers OK, I mean, those are just artificial numbers, but let's say OK, maybe we view the false reject could be 10 times more serious than the false accept in the supermarket case.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's look at another case.",
                    "label": 0
                },
                {
                    "sent": "Let's say we are using this fingerprint system for the CIA for something or confidential, and so on.",
                    "label": 0
                },
                {
                    "sent": "No, let's look at the CIA.",
                    "label": 0
                },
                {
                    "sent": "Use for the fingerprint application.",
                    "label": 0
                },
                {
                    "sent": "If our system gives a false accept, which means OK, really the person should not be authorized to to use these parts, but then your assistance is yes you can.",
                    "label": 0
                },
                {
                    "sent": "You can get in there, you can do whatever confidential seeing that you originally were not supposed to do.",
                    "label": 0
                },
                {
                    "sent": "That's a serious mistake.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if it's a false reject, so let's say, OK, I go there.",
                    "label": 0
                },
                {
                    "sent": "I'm a CIA worker.",
                    "label": 0
                },
                {
                    "sent": "I go there, I got rejected.",
                    "label": 0
                },
                {
                    "sent": "I just try again.",
                    "label": 0
                },
                {
                    "sent": "I work in CI, should accept this kind of mistakes and so on for security reasons.",
                    "label": 0
                },
                {
                    "sent": "So basically there will be unhappy employee from the CSA, not get it, but it's less serious then leaking out confidential information to people who should not access that.",
                    "label": 0
                },
                {
                    "sent": "So in the CIA case, probably we would evaluate the performance like this.",
                    "label": 0
                },
                {
                    "sent": "Basically a false reject.",
                    "label": 0
                },
                {
                    "sent": "Much less serious or false.",
                    "label": 0
                },
                {
                    "sent": "Accept very, very serious.",
                    "label": 0
                },
                {
                    "sent": "Let's denote it by 1000 or or.",
                    "label": 0
                },
                {
                    "sent": "Something like that.",
                    "label": 0
                },
                {
                    "sent": "So we see that even just for binary classification for the fingerprints verification case, if we put the system into different applications, we will have to consider different kinds of costs to satisfy the need of the system.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the original problem that the simplest one that we consider was binary classification, in which the regular one in which we just penalize all the errors equally.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have one here.",
                    "label": 0
                },
                {
                    "sent": "For the false reject, one here for the false accept equally.",
                    "label": 0
                },
                {
                    "sent": "And in that case, OK in our example.",
                    "label": 0
                },
                {
                    "sent": "In our training example, we will calculate our error.",
                    "label": 0
                },
                {
                    "sent": "We call it in, meaning the in sample error we calculate the error of the hypothesis by considering whether the label OK.",
                    "label": 0
                },
                {
                    "sent": "The desert label equals the prediction of the hypothesis and if it's not equal.",
                    "label": 0
                },
                {
                    "sent": "I'm using a Boolean operation.",
                    "label": 0
                },
                {
                    "sent": "Here we will cut count the error by one if it's not equal.",
                    "label": 0
                },
                {
                    "sent": "We count by the group, so that's how we calculate our in sample error.",
                    "label": 0
                },
                {
                    "sent": "And similarly we can count our out of sample error by picking an expectation over the distribution that our data is generated from.",
                    "label": 0
                },
                {
                    "sent": "That's the typical setting of statistical learning, and for this regular binary classification problem it's well studied.",
                    "label": 1
                },
                {
                    "sent": "You probably all know lots of algorithms about regular binary classification.",
                    "label": 1
                },
                {
                    "sent": "Yeah, you know.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we're going to talk about the cost sensitive one, much like the supermarket case and the first one.",
                    "label": 0
                },
                {
                    "sent": "We're going to talk about is called the class weighted case for the class weighted case.",
                    "label": 0
                },
                {
                    "sent": "We have OK cost metrics that we list here, let's see.",
                    "label": 0
                },
                {
                    "sent": "We have this matrix.",
                    "label": 0
                },
                {
                    "sent": "OK, for this cost metrics, it's encodes our intention OK, which kind of error should be penalized more?",
                    "label": 0
                },
                {
                    "sent": "Which should be penalized less, and so on.",
                    "label": 0
                },
                {
                    "sent": "And naturally we assume that the diagonal part of this cost metrics to be 0, meaning no cost is the correct prediction.",
                    "label": 0
                },
                {
                    "sent": "We want positive the system predicts positive.",
                    "label": 0
                },
                {
                    "sent": "We want negative, the system predicts negative, and for the other two cases we can fill in the number 2.",
                    "label": 0
                },
                {
                    "sent": "Encode our dimension of whether to charge this kind of cost more or less.",
                    "label": 0
                },
                {
                    "sent": "And in that case we're going to calculate the in sample errors differently.",
                    "label": 0
                },
                {
                    "sent": "OK, so you see that I list the equivalent formulas here.",
                    "label": 0
                },
                {
                    "sent": "There are basically 2 cases.",
                    "label": 0
                },
                {
                    "sent": "If.",
                    "label": 0
                },
                {
                    "sent": "There's something wrong.",
                    "label": 0
                },
                {
                    "sent": "Which is similar to the previous case of regular classification.",
                    "label": 0
                },
                {
                    "sent": "Originally, when there's something wrong, we basically add a penalty of what, now, when there's something wrong, we check the two different cases, one being a false reject case.",
                    "label": 0
                },
                {
                    "sent": "We see here.",
                    "label": 0
                },
                {
                    "sent": "Then basically we add a penalty of 10.",
                    "label": 0
                },
                {
                    "sent": "And then if it's false accept case, we add a penalty of what.",
                    "label": 0
                },
                {
                    "sent": "So for the two cases, we add different penalties.",
                    "label": 0
                },
                {
                    "sent": "Similarly, we can list the out of sample performance formula.",
                    "label": 0
                },
                {
                    "sent": "It's just the same.",
                    "label": 0
                },
                {
                    "sent": "OK, just replacing those incident called XY.",
                    "label": 0
                },
                {
                    "sent": "The data that we have on hand with the out of sample data that we get from the distribution of interest.",
                    "label": 0
                },
                {
                    "sent": "And for this setting it's usually called class weighted cost sensitive classification.",
                    "label": 0
                },
                {
                    "sent": "So class weighted meaning we really have different weights of our different penalties for examples from different classes.",
                    "label": 0
                },
                {
                    "sent": "If the example is from the positive class, the penalty is 10 of.",
                    "label": 0
                },
                {
                    "sent": "The example is from the negative class, the penalty is.",
                    "label": 0
                },
                {
                    "sent": "What this penalty corresponds to.",
                    "label": 0
                },
                {
                    "sent": "Wait an and there are two kinds of weighted binary classification problem that we will see.",
                    "label": 0
                },
                {
                    "sent": "And this is the simpler one.",
                    "label": 0
                },
                {
                    "sent": "Just wait by the class.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's look at the formal setting of this problem.",
                    "label": 0
                },
                {
                    "sent": "Basically, in this problem we're given a bunch of examples.",
                    "label": 0
                },
                {
                    "sent": "We're given a bunch of examples, and in addition to the examples we have, those cost information.",
                    "label": 0
                },
                {
                    "sent": "Basically, we have a whole cost table, but because of our assumption all we really care are the two entries that we called W plus and W minus, and that's representing the two entries of the cost metrics, so we can view the cost us an additional piece of.",
                    "label": 1
                },
                {
                    "sent": "Information to guide the machine on how we're going to evaluate.",
                    "label": 0
                },
                {
                    "sent": "If you're teaching this is much like telling your students OK which parts of your lecture would be counted more in the exam?",
                    "label": 0
                },
                {
                    "sent": "Which part of the lecture will be counted less in the exam so they can be prepared accordingly?",
                    "label": 0
                },
                {
                    "sent": "So we are giving the machine an additional piece of information.",
                    "label": 0
                },
                {
                    "sent": "And what we want?",
                    "label": 1
                },
                {
                    "sent": "Well, when the machine sees this information in addition to the data, it can use the information to guide the learning process to give us a decision function that pays a small cost.",
                    "label": 1
                },
                {
                    "sent": "So we use the same formula to do evaluation during the testing phase.",
                    "label": 0
                },
                {
                    "sent": "OK on future unseen example following our definition in this static statistical sense, it actually means we want the machine to reach a load out in this cost sensitive.",
                    "label": 0
                },
                {
                    "sent": "Or weighted definition.",
                    "label": 0
                },
                {
                    "sent": "And for regular classification you see that it's just a special case in which we have W plus and W minus both equals what?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's revisit the supermarket case.",
                    "label": 0
                },
                {
                    "sent": "When we have the false accept the false reject and so on.",
                    "label": 1
                },
                {
                    "sent": "Previously we say OK, we have two kinds of customers, one being the loyal customers we want to be careful not to reject those, one being the intruders, and basically it's OK if we occasionally accept them.",
                    "label": 0
                },
                {
                    "sent": "No, there may be some customers who are like super loyal customers or something, or we called in the big customers.",
                    "label": 1
                },
                {
                    "sent": "Maybe they buy a lot from the supermarket or something, and if you follow my argument previously you will see that we definitely don't want to reject those customers.",
                    "label": 0
                },
                {
                    "sent": "OK, we can reject the occasional ones.",
                    "label": 0
                },
                {
                    "sent": "OK, it doesn't matter, but if it's a big customer who comes to the supermarket once a day, buying lots of grocery or something.",
                    "label": 0
                },
                {
                    "sent": "The supermarket may say, OK, I really don't want to reject it, so there are different kinds of customers, big ones, or the usual ones.",
                    "label": 1
                },
                {
                    "sent": "And for those customers may be OK. Our desire to really accept it or reject it would be different.",
                    "label": 0
                },
                {
                    "sent": "In that case, maybe for the big customers, the penalty we should charge for rejecting them is higher then the usual customer.",
                    "label": 0
                },
                {
                    "sent": "OK, on the other hand, for the usual customer may be OK.",
                    "label": 0
                },
                {
                    "sent": "It's high but maybe not as high as the big one.",
                    "label": 0
                },
                {
                    "sent": "So you see that OK for different customers?",
                    "label": 0
                },
                {
                    "sent": "Although OK, they are both customers both for both of them.",
                    "label": 0
                },
                {
                    "sent": "We probably should accept them rather than reject him, but we really should charge the penalties differently.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so in that case we see that and we we have three kinds of costs, vectors and now we call the cost vectors which just corresponds to previously the roofs of the cost metrics.",
                    "label": 0
                },
                {
                    "sent": "We only need the rope to consider the importance of each customer.",
                    "label": 0
                },
                {
                    "sent": "And in that case that important.",
                    "label": 0
                },
                {
                    "sent": "I mean, we really only have 1 degree of freedom.",
                    "label": 0
                },
                {
                    "sent": "The authority under the correct case will always assume to be having 0 cost and for the other case we say OK, the importance will be called WN.",
                    "label": 0
                },
                {
                    "sent": "For the examples that we have on here.",
                    "label": 0
                },
                {
                    "sent": "OK, so if we know the importance of each customer that we have on hand and we will use similar importance to evaluate the future performance of the system.",
                    "label": 0
                },
                {
                    "sent": "How can we do well and that is usually called an example weighted cost sensitive classification.",
                    "label": 0
                },
                {
                    "sent": "OK, so for each example we have the weights OK for even for examples of the same class they can carry different weights and so on.",
                    "label": 0
                },
                {
                    "sent": "K so different W 4 different X&Y for people who have learned about Adaboost, you have probably seen this.",
                    "label": 0
                },
                {
                    "sent": "OK, this is actually those weights are exactly what we feed to the weak learners of the edibles.",
                    "label": 0
                },
                {
                    "sent": "To get diverse weak classifiers and so on for basically combining them in the future.",
                    "label": 0
                },
                {
                    "sent": "So if you have used edibles, you probably have used example weighted binary classification internally in your weekly.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's look at the setting here OK and example weighted setting for binary cost sensitive classification in addition to the data that we give to the machines, we have an additional part of the data that we call the weight.",
                    "label": 0
                },
                {
                    "sent": "OK, so for this weight.",
                    "label": 0
                },
                {
                    "sent": "So basically we actually intend to mean each example is of different importance and that important is used to calculate the in sample error.",
                    "label": 0
                },
                {
                    "sent": "The training area that we have on hand.",
                    "label": 0
                },
                {
                    "sent": "How about for the future?",
                    "label": 0
                },
                {
                    "sent": "For evaluating the test performance, we will use some weights to evaluate the performance of our decision function RG as well.",
                    "label": 0
                },
                {
                    "sent": "And for this unseen example we have the usual X.",
                    "label": 0
                },
                {
                    "sent": "That our decision function would know, but we assume that the label and the weights are hidden from the decision function.",
                    "label": 0
                },
                {
                    "sent": "So basically we generate examples.",
                    "label": 0
                },
                {
                    "sent": "Now with an additional entry called W. OK, but when making a decision we don't really give the digital functions adopted.",
                    "label": 0
                },
                {
                    "sent": "We just say OK, there's X, make your decision.",
                    "label": 0
                },
                {
                    "sent": "I only guarantee you that the W would be generated similarly to what you have seen in the training phase.",
                    "label": 0
                },
                {
                    "sent": "OK, so following those settings you probably see that we discuss about this.",
                    "label": 0
                },
                {
                    "sent": "The regular classification is a special case of the class waited setting by setting both ways to what and the class waited.",
                    "label": 0
                },
                {
                    "sent": "Setting is a special case of the example weighted settings example where this setting more general.",
                    "label": 0
                },
                {
                    "sent": "You can have different weights for different examples.",
                    "label": 0
                },
                {
                    "sent": "Class weighted setting.",
                    "label": 0
                },
                {
                    "sent": "You just set the positive examples to one particular wait.",
                    "label": 0
                },
                {
                    "sent": "The negative examples too.",
                    "label": 0
                },
                {
                    "sent": "Another part of it.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so how do we do binary cost sensitive classification and the lecture I'm going to show you follow some historical trace on the popular methods for binary weighted cost sensitive classification and the earliest one starts from the patient perspective.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the patient perspective, OK, we start asking ourselves the question.",
                    "label": 0
                },
                {
                    "sent": "We want a classifier that pays a small cost.",
                    "label": 1
                },
                {
                    "sent": "Now if we look at the patients and so everything should prove to be put under probability and so on, we see that the expected error that we are going to pay.",
                    "label": 0
                },
                {
                    "sent": "For predicting positive one on a particular example X.",
                    "label": 0
                },
                {
                    "sent": "So let's say we fix one particular X1 factor fingerprint and say OK, what's the expected error?",
                    "label": 0
                },
                {
                    "sent": "We see that the price we're going to pay is W -- * P of minus given X.",
                    "label": 0
                },
                {
                    "sent": "So if this particular example is labeled negative one by your target function plus the noise distribution, so you really get a label.",
                    "label": 0
                },
                {
                    "sent": "Of negative one.",
                    "label": 0
                },
                {
                    "sent": "And then you pretty positive.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's a false accept and in that force accept case, on average we pace this much.",
                    "label": 0
                },
                {
                    "sent": "The other case is if we insist on predicting negative if we predict negative OK, then we pay for the false reject cases and for the false reject case.",
                    "label": 0
                },
                {
                    "sent": "On average we pay this much.",
                    "label": 0
                },
                {
                    "sent": "OK, so you have two choices.",
                    "label": 0
                },
                {
                    "sent": "You are given one fingerprint.",
                    "label": 0
                },
                {
                    "sent": "You have two choices, positive one or negative one and the average error you receive are those two numbers.",
                    "label": 0
                },
                {
                    "sent": "So now.",
                    "label": 0
                },
                {
                    "sent": "Of course, in the general machine learning, we assume that those P OK the conditional probability are unknown.",
                    "label": 0
                },
                {
                    "sent": "But let's say, what if we know the conditional probability?",
                    "label": 0
                },
                {
                    "sent": "If you know the conditional probability, then you can make the base optimal decision by saying OK, let me look at the expected error for predicting negative one expected error for predicting positive one, choose the smaller one.",
                    "label": 1
                },
                {
                    "sent": "Right, OK, so you can only choose one and the other part of the error or the cost is unavoidable.",
                    "label": 0
                },
                {
                    "sent": "So basically choose the smaller one.",
                    "label": 0
                },
                {
                    "sent": "That's the equation that you get if OK, you know the true probability that's the base optimal decision.",
                    "label": 0
                },
                {
                    "sent": "But now we don't know about the the true probability and so how can we do well what we can do is to take another function to estimate the conditional probability.",
                    "label": 0
                },
                {
                    "sent": "So that's an estimate, and if our estimates is accurate to some sense, then and approximately good rule approximately good decision function would be just using this approximation.",
                    "label": 0
                },
                {
                    "sent": "To replace the true probability that we see here.",
                    "label": 0
                },
                {
                    "sent": "So how do we get a good conditional probability estimator?",
                    "label": 1
                },
                {
                    "sent": "Well, you guys know more than me.",
                    "label": 0
                },
                {
                    "sent": "Probably you can use logistic regression, naive Bayes, or whatever methods.",
                    "label": 0
                },
                {
                    "sent": "Let's say maximum likelihood, maximum posterior blah blah blah for estimating the conditional probability.",
                    "label": 0
                },
                {
                    "sent": "But I mean we have all the good tools.",
                    "label": 0
                },
                {
                    "sent": "If you are going in the probabilistic direction.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is 1 very simple algorithm I found in the literature that it doesn't really have a name, so I gave it a name approximate based optimal decision.",
                    "label": 0
                },
                {
                    "sent": "OK, the name is my OK not from any literature.",
                    "label": 0
                },
                {
                    "sent": "So basically what we want to do is to approximate a good solution using a good approximate approximator of the conditional probability.",
                    "label": 0
                },
                {
                    "sent": "So back to 2001.",
                    "label": 0
                },
                {
                    "sent": "That's from Malcolm and there's the derivation of the rule saying.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have this P. And basically if we follow this rule then we will predict a positive one.",
                    "label": 0
                },
                {
                    "sent": "If OK.",
                    "label": 0
                },
                {
                    "sent": "This inner part is positive.",
                    "label": 0
                },
                {
                    "sent": "If this inner part is positive, really OK if we just reorganized the terms and so on.",
                    "label": 0
                },
                {
                    "sent": "It corresponds to saying P of X.",
                    "label": 0
                },
                {
                    "sent": "Which is our estimate of the conditional probability is greater than a threshold that's computed from the blue plus and W minus.",
                    "label": 0
                },
                {
                    "sent": "It's a very simple calculation comes from switching the terms and so on, but I'm not going to the mathematical detail, but let's look at the numbers.",
                    "label": 0
                },
                {
                    "sent": "If I substitute the numbers that we have OK to this stressful, we see that it evaluates to 1 / 11.",
                    "label": 1
                },
                {
                    "sent": "For the supermarket case, which means we would easily accept OK if OK, the conditional probability is greater than 1 / 11 we would accept if the conditional probability is less than 1 / 11 we would reject, so we would easily accept.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, for the CIA case, we would easily reject OK, oh, so that's a typo here.",
                    "label": 0
                },
                {
                    "sent": "Actually, that should be 1000 / 1001.",
                    "label": 0
                },
                {
                    "sent": "OK, so we would easily reject only if we are highly confident that this is you.",
                    "label": 0
                },
                {
                    "sent": "We would accept otherwise we would just view it as a intruded.",
                    "label": 0
                },
                {
                    "sent": "So you see that the rule corresponds to our common sense of what we should do in the supermarket and the CIK supermarket case.",
                    "label": 0
                },
                {
                    "sent": "Accept more easily ancis case reject more easily.",
                    "label": 0
                },
                {
                    "sent": "Now, so for this approach you can just use your favourites algorithm to estimate this conditional probability and then for each new prediction you just basically form your prediction using a new threats code.",
                    "label": 1
                },
                {
                    "sent": "That we just calculated above.",
                    "label": 0
                },
                {
                    "sent": "So that's basically the approximate base optimal solution.",
                    "label": 0
                },
                {
                    "sent": "You see that for the training situation, basically nothing is changed.",
                    "label": 0
                },
                {
                    "sent": "OK if you are originally a big fan of estimating conditional probabilities, nothing is changed from the training part.",
                    "label": 0
                },
                {
                    "sent": "All you need to do is to defer all the cost sensitive decision-making to the prediction part, calculate a new threshold, and use that source code for your future prediction.",
                    "label": 1
                },
                {
                    "sent": "That's arguably one of the simplest approach of binary cost sensitive classification.",
                    "label": 1
                },
                {
                    "sent": "You do probability, estimate and then you do thresholds changing.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let me show you a picture of how things work in this case.",
                    "label": 0
                },
                {
                    "sent": "Actually I just formed in artificial data OK with all those examples we have the positive ones that I highlight in circle.",
                    "label": 1
                },
                {
                    "sent": "And the negative ones I highlight in cross.",
                    "label": 0
                },
                {
                    "sent": "So basically this is the two dimensional data we feed to the machine learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "If you want to use this approach for cost sensitive binary classification and the training part, I say OK, there's nothing you need to do.",
                    "label": 0
                },
                {
                    "sent": "You just run your favorite algorithm on conditional probability estimate.",
                    "label": 1
                },
                {
                    "sent": "Let's say logistic regression and then you get some some estimates like this.",
                    "label": 0
                },
                {
                    "sent": "So I'm using different colors.",
                    "label": 0
                },
                {
                    "sent": "Here, if things gets blue, it means higher probability of being positive.",
                    "label": 0
                },
                {
                    "sent": "If things get red, then it means more probability of getting negative.",
                    "label": 0
                },
                {
                    "sent": "So get this picture.",
                    "label": 0
                },
                {
                    "sent": "Now what you do during decision time is to set your decision boundary based on the cost metrics that you get.",
                    "label": 0
                },
                {
                    "sent": "OK, so in the supermarket case, OK, this is the cost metrics you get.",
                    "label": 0
                },
                {
                    "sent": "You use this cost metrics, there's W plus you're starting minus you do the calculation.",
                    "label": 0
                },
                {
                    "sent": "You get that the stress cold is 1 / 11.",
                    "label": 0
                },
                {
                    "sent": "You said the source code, two 1 / 11.",
                    "label": 0
                },
                {
                    "sent": "Then this part is where you claim to be positive.",
                    "label": 0
                },
                {
                    "sent": "This part is where you claim to be negative.",
                    "label": 0
                },
                {
                    "sent": "You see that this OK setting the new stress called give you a shifted boundary from your urgent why?",
                    "label": 0
                },
                {
                    "sent": "So this is what you do for regular classification.",
                    "label": 0
                },
                {
                    "sent": "For regular classification.",
                    "label": 0
                },
                {
                    "sent": "Remember we have W plus and minus both equal 1, so therefore the source code you are going to be setting in this case is 1/2.",
                    "label": 0
                },
                {
                    "sent": "So if you set the source code to have, you probably would get a line somewhere really in the middle of circle and cross.",
                    "label": 0
                },
                {
                    "sent": "But now that you change this, wrestle, and in this case we change the source code to 1 / 11, the line is somewhat pushed towards the negative case OK, because we want to be easily accepting in the supermarket case.",
                    "label": 0
                },
                {
                    "sent": "So if you can get a good conditional probability estimator through logistic regression or whatever methods that you're interested in, what you can do really is to just change the stressful and done.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what are the pros and cons of this simple approach?",
                    "label": 1
                },
                {
                    "sent": "Well, one good thing as the Asian people would say is that it's optimal.",
                    "label": 1
                },
                {
                    "sent": "If there's a good probability estimate based optimal solution, that's really theoretical, guaranteed.",
                    "label": 0
                },
                {
                    "sent": "And if you are optimal or close to the optimal, you can get the strong theoretical guarantee that any patient setting can provide.",
                    "label": 1
                },
                {
                    "sent": "I'm also it's simple.",
                    "label": 0
                },
                {
                    "sent": "OK, basically, as we mentioned, the training part is almost unchanged and the prediction part only changes a little change the source code, nothing big.",
                    "label": 0
                },
                {
                    "sent": "What are the disadvantages of this approach?",
                    "label": 0
                },
                {
                    "sent": "Well, one thing is that often the good probability estimate, I mean, because you really need to get all those real numbers between 01 correct, are often harder to get.",
                    "label": 0
                },
                {
                    "sent": "Then.",
                    "label": 0
                },
                {
                    "sent": "Let's say you are getting a binary classifier.",
                    "label": 0
                },
                {
                    "sent": "OK, so those are the debates from the discriminative and generative models and so on.",
                    "label": 0
                },
                {
                    "sent": "I'm not saying which one is good or bad.",
                    "label": 1
                },
                {
                    "sent": "But sometimes it's bad.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it's not easy to get a good generative model to get a good probability estimate.",
                    "label": 0
                },
                {
                    "sent": "And also it's restricted OK.",
                    "label": 0
                },
                {
                    "sent": "Remember in our setting what we need to do, we need to use the W plus and minus to calculate a new threshold.",
                    "label": 0
                },
                {
                    "sent": "Now this is only available if you are in a metric setting.",
                    "label": 0
                },
                {
                    "sent": "OK, if you are doing it vector really OK you vector setting.",
                    "label": 0
                },
                {
                    "sent": "You only have a WN1 weight per example you don't have the other that we call the relative case to determine your threshold.",
                    "label": 0
                },
                {
                    "sent": "So you really need to know the whole metrics in order to make the base optimal decision.",
                    "label": 0
                },
                {
                    "sent": "You need the whole matrix OK to decide what to do and therefore, for example, you may not be able to couple this approach with weak learners in Adaboost, because in weak learners of edibles, what we essentially do is to provide each example of the different weights rather than in a in a class wise fashion.",
                    "label": 0
                },
                {
                    "sent": "So you may ask, what are the other approaches for the example weighted set up rather than the class waited one that we can solve from the patient perspective?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now invasion perspective.",
                    "label": 0
                },
                {
                    "sent": "Before we do that, I want to maybe stop for awhile and ask if there are any questions and so on questions.",
                    "label": 0
                },
                {
                    "sent": "Confused.",
                    "label": 0
                },
                {
                    "sent": "OK, if not, let's continue.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the example weights OK that we use OK.",
                    "label": 0
                },
                {
                    "sent": "Remember our goal is to pay a small cost for future unseen example that carries hidden weight.",
                    "label": 1
                },
                {
                    "sent": "Summer.",
                    "label": 0
                },
                {
                    "sent": "Now if we do this evaluation on one example.",
                    "label": 0
                },
                {
                    "sent": "OK, So what XY?",
                    "label": 0
                },
                {
                    "sent": "Then we need to charge the penalty by W, let's say W is 100, then OK, so this example run.",
                    "label": 0
                },
                {
                    "sent": "We charge the penalty of 100.",
                    "label": 0
                },
                {
                    "sent": "Let's say if we have 100 copies of the same example.",
                    "label": 0
                },
                {
                    "sent": "So if there's one example, we charge 100.",
                    "label": 0
                },
                {
                    "sent": "If I have 100 copies of the same example, what can we do?",
                    "label": 0
                },
                {
                    "sent": "We can just charge each example by what equivalently, one example charge a hundred 100 examples charged by what?",
                    "label": 0
                },
                {
                    "sent": "Now what do we mean by charge?",
                    "label": 0
                },
                {
                    "sent": "Each error by what?",
                    "label": 0
                },
                {
                    "sent": "Well?",
                    "label": 0
                },
                {
                    "sent": "If you recall, that's just our old friend regular classification.",
                    "label": 1
                },
                {
                    "sent": "So if we can manage to get W copies of the same example, then OK, we're simply in the training part.",
                    "label": 0
                },
                {
                    "sent": "We charge all the penalty by one in the prediction part actually.",
                    "label": 0
                },
                {
                    "sent": "Surely we charge the penalty by one, and so on, and we don't need to worry about those weights.",
                    "label": 0
                },
                {
                    "sent": "That's the basic idea.",
                    "label": 0
                },
                {
                    "sent": "OK, how do we do this copy?",
                    "label": 0
                },
                {
                    "sent": "How do we virtually see that we have 100 copies of the same example while there are lots of ways?",
                    "label": 0
                },
                {
                    "sent": "Of course, if you have integer weights, you can really just do copy.",
                    "label": 0
                },
                {
                    "sent": "If you don't mind paying the computation.",
                    "label": 0
                },
                {
                    "sent": "If you have weights that are real numbers, then you can probably do some oversampling according to the W that we want, and so on.",
                    "label": 0
                },
                {
                    "sent": "And that's the method we're going to introduce next.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so that's our original problem.",
                    "label": 1
                },
                {
                    "sent": "We have a bunch of examples.",
                    "label": 0
                },
                {
                    "sent": "OK for some of them we want to basically consider a higher weight for some of them we want to consider a lower weight, OK, and so on.",
                    "label": 1
                },
                {
                    "sent": "What we mean by example, copying or using oversampling or something is really to consider an equivalent problem like this.",
                    "label": 0
                },
                {
                    "sent": "Now we're going to evaluate everything just by one that's regular classification.",
                    "label": 0
                },
                {
                    "sent": "But in the data part, we do some copying, for example OK this one we are charging a cost of 10.",
                    "label": 0
                },
                {
                    "sent": "We copy the data for 10 parts.",
                    "label": 0
                },
                {
                    "sent": "For this one, we're charging for a cost of 100.",
                    "label": 0
                },
                {
                    "sent": "We copy the data for 100 times.",
                    "label": 0
                },
                {
                    "sent": "Get a bigger datasets and so are we send it to training.",
                    "label": 0
                },
                {
                    "sent": "Get a classifier that classifier would do well in this regular classification problem.",
                    "label": 0
                },
                {
                    "sent": "But this regular classification problem actually encodes all the information that we want in the cost sensitive classification problem that we have on the left hand side.",
                    "label": 1
                },
                {
                    "sent": "And then how do you learn a good classifier for the right hand side?",
                    "label": 1
                },
                {
                    "sent": "Well, again, you all know too well about binary classification.",
                    "label": 0
                },
                {
                    "sent": "We have SVM.",
                    "label": 0
                },
                {
                    "sent": "We have neural network, we have edibles, we have whatever your favorite binary classification algorithm that you want to use.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so basically what we need to do is to effectively transform our original datasets.",
                    "label": 1
                },
                {
                    "sent": "Which contains the weight to a weightless or a regular classification data set that we see here.",
                    "label": 0
                },
                {
                    "sent": "Hey Ann, I mean, we just show that you can do so by copying if you have integer weights and so on.",
                    "label": 0
                },
                {
                    "sent": "But of course there are two approaches and the earliest approach.",
                    "label": 0
                },
                {
                    "sent": "OK, we can do over or undersampling using some normalized W. So basically during your training you normalize those tablets so it's simulate a probability distribution and then you do the over or undersampling.",
                    "label": 0
                },
                {
                    "sent": "Actually, some of you may often do so in edibles.",
                    "label": 0
                },
                {
                    "sent": "Let's say with decision trees.",
                    "label": 0
                },
                {
                    "sent": "An in some later works there are there proposed methods by that does undersampling by rejection.",
                    "label": 1
                },
                {
                    "sent": "OK, so basically rejection based sampling rather than the basically the usual just wait based sampling.",
                    "label": 0
                },
                {
                    "sent": "And then also you can also modify existing algorithms.",
                    "label": 1
                },
                {
                    "sent": "Equivalently, in many of the machine learning algorithms, if you know the details of the optimal objective function that you're optimizing, you can plug in the weights to get an equivalent formulation.",
                    "label": 0
                },
                {
                    "sent": "Those are possible ways to modify your original data to some equivalent data that's in a sense weightless.",
                    "label": 0
                },
                {
                    "sent": "And then you can use your favorite algorithm to learn the classifier, and for this algorithm the prediction part is essentially unchanged.",
                    "label": 1
                },
                {
                    "sent": "OK, previously the patient one prediction part change training part unchanged.",
                    "label": 0
                },
                {
                    "sent": "Now we change the training part but not the prediction part.",
                    "label": 0
                },
                {
                    "sent": "And it's simple in general.",
                    "label": 1
                },
                {
                    "sent": "Many of you may have readily used it in one way or the other, and it's very popular for cost sensitive binary classification.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's look in more detail about what we mean by modification.",
                    "label": 0
                },
                {
                    "sent": "Just to give everyone a sense of what I mean by modification, you can apply similar methods to other kinds of binary classification algorithms as well.",
                    "label": 0
                },
                {
                    "sent": "If you do this modification, for example, let's say for SVM OK, so many of you know that in SVN basically you are solving a regularised hinge loss optimization problem.",
                    "label": 0
                },
                {
                    "sent": "This is the.",
                    "label": 0
                },
                {
                    "sent": "This is the regularization part and this is the hinge loss part.",
                    "label": 0
                },
                {
                    "sent": "Now when your son is often urgent problem and you plug in the weights.",
                    "label": 0
                },
                {
                    "sent": "OK, let's say you have an example that's twice the weight you want to charge it with twice the penalty.",
                    "label": 0
                },
                {
                    "sent": "Then basically all need to do is OK. You have C for one of the example.",
                    "label": 0
                },
                {
                    "sent": "You have C for the other copy.",
                    "label": 0
                },
                {
                    "sent": "So then the penalty for actually for this example equivalently will be to see basically.",
                    "label": 0
                },
                {
                    "sent": "What you do is to just change this C to C kinds.",
                    "label": 0
                },
                {
                    "sent": "Other things unchanged.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can just do this.",
                    "label": 0
                },
                {
                    "sent": "One line of modification an it's done.",
                    "label": 0
                },
                {
                    "sent": "Of course this one line of modification depends on whether your solver supports that or or not, but there are like saying SVN there sleep SVN tools that actually supports this weighted formulation.",
                    "label": 0
                },
                {
                    "sent": "You can feed the weights into that OK, but for formulation change is if you know the details of your underlying algorithm it's usually simple.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's visualize the process of this modification on artificial data.",
                    "label": 1
                },
                {
                    "sent": "What we do is starting from the original data.",
                    "label": 0
                },
                {
                    "sent": "We copy the data for a few times over, simple or under simple or whatever to represent the different weights.",
                    "label": 0
                },
                {
                    "sent": "Visually, what I do here is to basically show you some bigger circles.",
                    "label": 0
                },
                {
                    "sent": "To mean that I'm putting a higher weight sound it OK, and then I send those information to a server that respects the size of those circles and crosses the weights of those and so on to get a classifier.",
                    "label": 0
                },
                {
                    "sent": "You see here that OK?",
                    "label": 0
                },
                {
                    "sent": "By considering this weights.",
                    "label": 0
                },
                {
                    "sent": "OK, we get a classifier that basically is also pushed in this direction.",
                    "label": 0
                },
                {
                    "sent": "So basically we have bigger circles and the classifier wants to avoid making those bigger circles wrong and therefore the boundary after training is effectively pushed towards the red direction.",
                    "label": 0
                },
                {
                    "sent": "If you use a regular classifier, this is what we get.",
                    "label": 0
                },
                {
                    "sent": "So you see that by using the weights properly during the training process, we can also get a reasonable cost sensitive binary classifier for the supermarket.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An another approach, actually.",
                    "label": 0
                },
                {
                    "sent": "This is pretty popular is basically doing this, but by rejection sampling, OK?",
                    "label": 0
                },
                {
                    "sent": "The way we do in rejection sampling is basically just to get a random number for each example an if the random number is below the threshold that we set.",
                    "label": 0
                },
                {
                    "sent": "In this case, the stress called will be just the normalized weight.",
                    "label": 0
                },
                {
                    "sent": "Then we accept this example for training.",
                    "label": 0
                },
                {
                    "sent": "If this random number is greater than this first, then we reject it.",
                    "label": 0
                },
                {
                    "sent": "So effectively we would get a smaller datasets.",
                    "label": 0
                },
                {
                    "sent": "Then we started for.",
                    "label": 0
                },
                {
                    "sent": "They originally you have 1000 examples and a bunch of weight.",
                    "label": 0
                },
                {
                    "sent": "So after rejection sampling you may just be getting 300 examples or something.",
                    "label": 0
                },
                {
                    "sent": "Then after this rejection sampling you can learn a classifier based on those rejected examples and those examples would kind of come up with the distribution you are interested in that respects those weights or those costs.",
                    "label": 0
                },
                {
                    "sent": "But of course you know the question you may have in mind is when you use rejection sampling, you have fewer examples and so on.",
                    "label": 0
                },
                {
                    "sent": "Would you be getting worse classifiers and everything that's similarly happens to?",
                    "label": 0
                },
                {
                    "sent": "For example, if you are backing and so on, getting a using fewer examples for classifiers, well, what you can do is to repeat the previous two steps multiple times.",
                    "label": 0
                },
                {
                    "sent": "I mean subject to those randomness and so on.",
                    "label": 0
                },
                {
                    "sent": "So you would get a bunch.",
                    "label": 0
                },
                {
                    "sent": "Of different G an aggregate them very similar to what you would do in Bootstrap, every aggregation and so an for each new includes you can just predict with those aggregated G because each of them would be cost sensitive in some sense.",
                    "label": 0
                },
                {
                    "sent": "Telling you the decision that respects those costs and weights.",
                    "label": 0
                },
                {
                    "sent": "And when you aggregate then you can eliminate the variance in everything during your rejection sampling process.",
                    "label": 0
                },
                {
                    "sent": "And this is commonly used if your favorite algorithm is a black box, OK, you don't know how to plug in the weights.",
                    "label": 1
                },
                {
                    "sent": "You don't know how to do the modification rather than white box.",
                    "label": 1
                },
                {
                    "sent": "OK, the SVN case we show you is a white box.",
                    "label": 0
                },
                {
                    "sent": "Many of you here knows the details of the SVN.",
                    "label": 0
                },
                {
                    "sent": "So basically we can do the modifications we want, But if you have an algorithm that you don't know how to modify or maybe people just provide the code to you so black box then you can couple.",
                    "label": 1
                },
                {
                    "sent": "This algorithm, with the rejection sampling here an aggregation to get a cost sensitive binary classifier for the example weighted setting.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now in this very last part I'm going to give you my biased personal favorite.",
                    "label": 0
                },
                {
                    "sent": "I mean, OK. Everyone has their bias favourites and so on, and people are biased by several things.",
                    "label": 0
                },
                {
                    "sent": "OK, partly by their own work and so on, and partly by for example, the first method they started using everything.",
                    "label": 0
                },
                {
                    "sent": "So I admit, OK for the personal favourites.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about here.",
                    "label": 0
                },
                {
                    "sent": "It really doesn't cover all the approaches, include sensitive binary classification.",
                    "label": 1
                },
                {
                    "sent": "There are many other interesting approaches and so on.",
                    "label": 0
                },
                {
                    "sent": "Anne Anne, but I just want to share with you in case you are facing the axiom of choice.",
                    "label": 0
                },
                {
                    "sent": "OK, don't know how to choose or or something.",
                    "label": 0
                },
                {
                    "sent": "OK, so whenever possible I usually use so this reweighting OK.",
                    "label": 0
                },
                {
                    "sent": "So cost proportional example relating by modification.",
                    "label": 1
                },
                {
                    "sent": "So SVN neural network Adaboost OK for the algorithms I know to be a white box.",
                    "label": 1
                },
                {
                    "sent": "I use modification simply be cause.",
                    "label": 0
                },
                {
                    "sent": "Usually it gives you a somewhat more stable performance then rejection, sampling and aggregation.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, when would we use costing?",
                    "label": 0
                },
                {
                    "sent": "Hosting is the last method that we introduced doing rejection sampling and then doing aggregation.",
                    "label": 0
                },
                {
                    "sent": "Well, if I really only have the code on hand and I cannot afford to modify that, or if I need somewhat faster training because after rejection sampling I have fewer examples and so on, and for the many rounds of training I can send it to different machines and everything before aggregation.",
                    "label": 0
                },
                {
                    "sent": "So if I need faster training, I will consider using rejection basically.",
                    "label": 0
                },
                {
                    "sent": "And the performance is not that bad.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you aggregate enough classifiers, basically you get a relatively stable performance.",
                    "label": 0
                },
                {
                    "sent": "Not even in the mood of patient.",
                    "label": 1
                },
                {
                    "sent": "I mean, for some reason it seldom happens, but OK.",
                    "label": 0
                },
                {
                    "sent": "If I have some particular applications that I really want to make everything patient could probability here and there, then I will consider using the approximate based optimal decision it simply because if I really have those probability then basically changing the final prediction rule will be just like.",
                    "label": 0
                },
                {
                    "sent": "A very simple piece of cake.",
                    "label": 0
                },
                {
                    "sent": "OK, that's my personal bias favorite.",
                    "label": 0
                },
                {
                    "sent": "Of course, I believe if you have worked on weighted binary classification or sensitive finding classification, you may have your own favorite San.",
                    "label": 0
                },
                {
                    "sent": "I mean, feel free to discuss with me if you have some thoughts in this part.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, that's about end.",
                    "label": 0
                },
                {
                    "sent": "The part I want to talk about in binary classification cost sensitive Byron classification.",
                    "label": 0
                },
                {
                    "sent": "Are there any questions and anything?",
                    "label": 0
                }
            ]
        }
    }
}