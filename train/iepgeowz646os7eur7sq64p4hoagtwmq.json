{
    "id": "iepgeowz646os7eur7sq64p4hoagtwmq",
    "title": "I-SEARCH multimodal search",
    "info": {
        "author": [
            "Dimitrios Tzovaras, ITI - Informatics and Telematics Institute, CERTH - Centre for Research and Technology Hellas",
            "Thomas Steiner, Google, Inc."
        ],
        "published": "Nov. 26, 2012",
        "recorded": "September 2012",
        "category": [
            "Top->Computer Science->Multimedia Search"
        ]
    },
    "url": "http://videolectures.net/searchcomputing2012_tzovaras_steiner_search/",
    "segmentation": [
        [
            "I will present the results on multimodal search of the strip project.",
            "I search that started in 2010 and it's about to finish in the next three months.",
            "My colleague Thomas Tyner from Google will present the interface and also give a short demo of our search engine and please do not forget that we have.",
            "You are able to go through our demo to play with our system in the room next to the coffee area, so please visit our stand because we didn't have that much visitors up to now.",
            "So."
        ],
        [
            "Will introduce I certainly was a project which had as a goal multimodal search, and for the in this respect we had to provide.",
            "Unified framework for multi modal context, indexing, sharing, search and retrieval and we wanted to handle any type of multimedia and multi Kendall multimodal content ranging from text to the images, videos, 3D objects and audio and combine it also with real world information like time and location and also emotional information so user related information in general.",
            "In this respect we are fully supporting user centric.",
            "Search engine and you can also see it in two of our use cases for music retrieval and Furthermore we worked a lot on the presentation of the results using an overlay visualization strategies and therefore we are supporting this in two different settings.",
            "Source of a mobile phone and high performance PC."
        ],
        [
            "So the three main objectives, first of all, is to define unified rich unified content description and I will give you more details on that.",
            "Then we call it recorded something like a new way of representing multimodal information.",
            "Then we worked a lot on the unintelligent coordinator actual mechanisms, and we will present today our interface and we are proud of this.",
            "It's a Google inspired and Google designed.",
            "And of course.",
            "However it isolates.",
            "It's not Google only and we also have various ways of presenting the results at shift using visual analytical technologies."
        ],
        [
            "We have worked on 7 use cases within the project.",
            "Two of them had to do with music retrieval using expressive embedded queries and also collaborative ways to extract.",
            "Used to select music using social related descriptors.",
            "We had a completely different since we are multi model we can support completely different use cases and we have the use case on search and retrieval of piece of furniture.",
            "And also sets of retrieval of multimedia content using just the smartphone and information that can be captured by the smartphone.",
            "So it could be an image could be another piece and this could be used this information to search.",
            "We had a personalized approach to cancel customer needs specifically.",
            "Search for a motorcycle of your own.",
            "This was the actual use case that we have research done and the last two use cases had to do with games.",
            "It's a typical one with the threaded cane component retrieval and also the retrieval of avatars for games."
        ],
        [
            "This three main layers in are also depicted in this architecture.",
            "You have the media layer where we have a network media, real world information and emotional expression and social information.",
            "We call it user related information and all of them are combined in the record.",
            "The rich unified code and description that we have and we have worked a lot on the interaction by getting individual and also collaborative relevance feedback.",
            "And recommendation schemes.",
            "And as I said, one of our targets was also visualization using visual analytic technology."
        ],
        [
            "So let's start from the rich unified corner description.",
            "So we wanted in order to be able to support multi model sets we had to put all the descriptors in one type of representation.",
            "That's why we introduced the Rocco does it called, which is just one file that we call a content object which contains all types of low level descriptors that could be text, image, video, audio, and three dimensional descriptors relevant to this specific content in the real world descriptions.",
            "It could be that I'm in the position the weather either RFID tags that could be associated with this content object and user related descriptors could be expressive, emotional or collaborative.",
            "In case that the scenario supports such."
        ],
        [
            "Let me now get into details of what we call a condo object.",
            "The cornered object for us.",
            "Is just a container of various multimedia objects that carry the same semantic information so.",
            "One content object may or may not contain all of these types of multimedia information, so it could be a complete contact object containing images text related to this images video related to them, and three dimensional representation, but it could be just the content object containing only one image without any other type of information."
        ],
        [
            "Our idea to perform multimodal searches to exploit various types of similarity, for example, that could be an image similarity between this component objects and three dimensional similarity between the others and utilizing this information you can have a multimodal distance and multimodal metric to measure similar."
        ],
        [
            "So the idea is to get the thread cone and objects to get the descriptors, to combine them to perform indexing in its type of this descriptors and then we are constructing an end to an adjacency matrix and extracting from them Laplacian Eigen Maps that we can then use as descriptors of the content objects that we have in the database.",
            "So finally."
        ],
        [
            "What we do is to create a multimodal feature space and compute the distance only in this feature space so that we can have a multimodal distance between this content objects."
        ],
        [
            "Let me go now to the presentation of the results.",
            "We have a typical way of presenting the results.",
            "We are giving information about the object we are providing thumbnail of use."
        ],
        [
            "Example we have thumbnails for the images we are supporting for the 3D render thumbnails of the three dimensional object and the slide source of the keyframes, and we can also have synthetic visual previews from the oral features, but we went a little bit the one step further on the presentation."
        ],
        [
            "For example.",
            "And the visualization of audio features we have tried to map the MFC spectral features first of all, after the dimensionality reduction we have tried to map them to a shape to an image with different colors and different save types, so they say, but the number of the petals, internal and external, and the colours internal and external depends on the specific descriptors.",
            "So we have a visual representation of the specific descriptor."
        ],
        [
            "In this way, just with a federal glance at the results, you can have an idea of the similarity in terms of color and also in terms of shapes of the petals."
        ],
        [
            "In terms of global visualization approaches, we have used the Classic One panel, one trim up in the hyperbolic tree one and Furthermore."
        ],
        [
            "We have worked on presenting the results in a grid and also in a smart way so that we bring similarly the results closer to the two dimensional space.",
            "In this case and also a similar approach which is a smart grid, a little bit a combination of both of them."
        ],
        [
            "For the real world information, we are providing information about location and time."
        ],
        [
            "Location in form of a map."
        ],
        [
            "And the time you see you can have an idea on when this documents that you have.",
            "Got this result when actually were actually created in the timeline from 2006, for example to 2009.",
            "In this case that I'm showing here."
        ],
        [
            "You can fully adopt later phase.",
            "You can also perform filtering in terms of modality in terms of tags that may be associated with the objects date, time and relevance to any type of content.",
            "And you can also."
        ],
        [
            "Perform also tag yourself.",
            "One of the results and also perform relevance.",
            "Assign relevant or irrelevant items in order for the relevance feedback algorithms to use this information to improve their results."
        ],
        [
            "Furthermore, we have advanced directive approaches for visualization.",
            "You see on the left there is a tree representation where you can just drag and drop drop.",
            "Each of these results and assign them to a different category and immediately the whole tree organizers and you get the different.",
            "Different structure of this tree corresponding to the relevance information that you have already provided.",
            "The same holds on the right side, but a little bit more structured in a grid like form.",
            "This is."
        ],
        [
            "All about the visualization of the presentation of the results, I will give the floor now to Thomas to present the interface of isets and also give a demo.",
            "Thank you for the first part of the presentation.",
            "So my name is Thomas Tyner.",
            "I work at Google Hamburg, but legally it's Google Dublin who is in project so.",
            "Yeah, I come from Hamburg.",
            "So OK when people think of search, think of search as a search box where they can put stuff.",
            "So we decided to take this paradigm also over to the world of multimodal search and now of course if you have a search box to type in something then you have letters.",
            "So the idea was to taking this idea of what you search appears in the search box.",
            "Also over to multimodal.",
            "So if you look at this example and I will show it later on.",
            "Life you can see a big search box here and you can see a couple of different modalities here.",
            "The first one is audio, the second one is an image.",
            "Then we have a sketch.",
            "We have emotion of location so you can see.",
            "Yep, it's search tokens that fill the search box.",
            "And finally you have the big search now button.",
            "So yeah, we really try to keep it simple even if a lot of the underlying technologies is quite complex or highly complex.",
            "But for the user it should be as simple as using the search engine of your choice today, so I will show."
        ],
        [
            "This in action now, and you're also invited as Demetrius set to come to our demo and see it for yourself.",
            "So yeah, this is the interface as it is now an below.",
            "You have the different input fields.",
            "So for example with a pen or with a pencil you can draw a sketch which is kind of hard to do, but.",
            "Let's just.",
            "Draw a lollipop here, done and you can see, but I've painted appears live in the search box so I could search with this now, but let's add something more.",
            "I don't know.",
            "Well, let's actually put this away because it's not really helpful, so let's use images.",
            "So in our interface we support drag and drop.",
            "So if you want to upload an image, you can just go like that, select the image.",
            "And then.",
            "Let's drag and drop this image over here and you can see it appears right away, and that's kind of trivial, Sir.",
            "So let's just see what it looks like.",
            "It takes a bit an it speaks with the back end server and hopefully if the Wi-Fi is working, yes, we get results back.",
            "So you can see now.",
            "I have searched for temple like buildings in our database, so let's start again from scratch.",
            "Anna add something more to it so.",
            "Let's use the temple.",
            "As before.",
            "An now, for example, let's say, well, we're interested in.",
            "Another modality which could be research for this temple like buildings in the UK.",
            "So we add.",
            "Our Geo location as an additional filter and then we can search again and the result should be different now.",
            "So this is just a combination of two modalities for different use cases.",
            "We can do more so now we can see it's less results because I filtered geographically of course.",
            "For other.",
            "For other modalities which is not visible in this user interface, but for example for the music use case, we can also search for emotion so it can maybe quickly show you the other latest interface which is.",
            "Available.",
            "Here is 1 second.",
            "But it can also have emotion.",
            "Well, you can simply use a slider interface to attach emotion to a certain, say music piece.",
            "So as I move around the slider, you can see the phase moves from happy to sad and the token gets updated in real time as as I search something and then another thing is searching for a rhythm.",
            "So we came up with.",
            "The natural way of inputing rhythm, of course, is just typing like that, so this is what we did.",
            "We made this very simple tapping interface where you can input a waltz rhythm like that 123123 and so on and then once it stands and how it runs for 10 seconds you can see the rhythm that I just input it by a tapping on the touchpad very naturally appears here.",
            "So now I could search for said waltz music.",
            "I don't know if this makes any sense, but just as an example.",
            "OK, and one last thing that I wanted to show is using 3D models so I have prepared some of them here.",
            "So this is using the open source colada format.",
            "I quickly open one so that you can get a feeling of it.",
            "This is just a preview so you can see the Volkswagen Beetle in 3D rotation view and if I take this.",
            "And I activate the 3D tab.",
            "And pull this thing here, it appears and then yeah, you could then search with 3D.",
            "So now if you use the 3D model of a car, let's add.",
            "Also photo of a car.",
            "So it was a beetle, so let's add another beetle.",
            "Rip well.",
            "Oh yeah, sure.",
            "So yeah, this is a good example.",
            "Actually offer interface iteration.",
            "So we kind of show an old interface which is the Safe Harbor because the latest latest iteration is not quite stable yet to show it on screen.",
            "But in the latest interface what we did was we enabled drag and dropping directly into the search box.",
            "So I kind of already got used to the new interface so I forgot that in the old interface we still had to switch toggles so.",
            "Let's see if I switch toggles to image.",
            "Now it should work.",
            "Pull it here.",
            "And then I can search for it and then we should hopefully get images of or representations of cars.",
            "Let's see if this works.",
            "3D is the most complex case, so it takes a little while before it can calculate 3D similarity, and then you can see at least some of the results.",
            "Are relevant more or less.",
            "So maybe this one and then what you can also do is if this is what you wanted to search for, you can then find.",
            "Similar things like that.",
            "So now on the fly.",
            "I've generated a new query and yeah, you can see a couple of other results here, so we're not claiming at this point that the results are perfect, but we are claiming is that yeah, we have tried to make multi model search as intuitive as possible with all those tokens.",
            "So of course you can also edit them interactively so you can say I remove this one and then you can.",
            "Of course, also simply add normal text.",
            "An answer for it.",
            "So yeah, this is our current interface iteration, one generation behind because the latest latest one, yeah, was not dimmable yet, but you're encouraged to come to the demo and then try to break it at our booth.",
            "So here you have seen the current iteration, so."
        ],
        [
            "Thank you and I think we can take some questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I will present the results on multimodal search of the strip project.",
                    "label": 0
                },
                {
                    "sent": "I search that started in 2010 and it's about to finish in the next three months.",
                    "label": 0
                },
                {
                    "sent": "My colleague Thomas Tyner from Google will present the interface and also give a short demo of our search engine and please do not forget that we have.",
                    "label": 0
                },
                {
                    "sent": "You are able to go through our demo to play with our system in the room next to the coffee area, so please visit our stand because we didn't have that much visitors up to now.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Will introduce I certainly was a project which had as a goal multimodal search, and for the in this respect we had to provide.",
                    "label": 0
                },
                {
                    "sent": "Unified framework for multi modal context, indexing, sharing, search and retrieval and we wanted to handle any type of multimedia and multi Kendall multimodal content ranging from text to the images, videos, 3D objects and audio and combine it also with real world information like time and location and also emotional information so user related information in general.",
                    "label": 1
                },
                {
                    "sent": "In this respect we are fully supporting user centric.",
                    "label": 0
                },
                {
                    "sent": "Search engine and you can also see it in two of our use cases for music retrieval and Furthermore we worked a lot on the presentation of the results using an overlay visualization strategies and therefore we are supporting this in two different settings.",
                    "label": 0
                },
                {
                    "sent": "Source of a mobile phone and high performance PC.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the three main objectives, first of all, is to define unified rich unified content description and I will give you more details on that.",
                    "label": 1
                },
                {
                    "sent": "Then we call it recorded something like a new way of representing multimodal information.",
                    "label": 0
                },
                {
                    "sent": "Then we worked a lot on the unintelligent coordinator actual mechanisms, and we will present today our interface and we are proud of this.",
                    "label": 0
                },
                {
                    "sent": "It's a Google inspired and Google designed.",
                    "label": 0
                },
                {
                    "sent": "And of course.",
                    "label": 0
                },
                {
                    "sent": "However it isolates.",
                    "label": 0
                },
                {
                    "sent": "It's not Google only and we also have various ways of presenting the results at shift using visual analytical technologies.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have worked on 7 use cases within the project.",
                    "label": 0
                },
                {
                    "sent": "Two of them had to do with music retrieval using expressive embedded queries and also collaborative ways to extract.",
                    "label": 0
                },
                {
                    "sent": "Used to select music using social related descriptors.",
                    "label": 0
                },
                {
                    "sent": "We had a completely different since we are multi model we can support completely different use cases and we have the use case on search and retrieval of piece of furniture.",
                    "label": 1
                },
                {
                    "sent": "And also sets of retrieval of multimedia content using just the smartphone and information that can be captured by the smartphone.",
                    "label": 0
                },
                {
                    "sent": "So it could be an image could be another piece and this could be used this information to search.",
                    "label": 1
                },
                {
                    "sent": "We had a personalized approach to cancel customer needs specifically.",
                    "label": 0
                },
                {
                    "sent": "Search for a motorcycle of your own.",
                    "label": 0
                },
                {
                    "sent": "This was the actual use case that we have research done and the last two use cases had to do with games.",
                    "label": 0
                },
                {
                    "sent": "It's a typical one with the threaded cane component retrieval and also the retrieval of avatars for games.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This three main layers in are also depicted in this architecture.",
                    "label": 0
                },
                {
                    "sent": "You have the media layer where we have a network media, real world information and emotional expression and social information.",
                    "label": 0
                },
                {
                    "sent": "We call it user related information and all of them are combined in the record.",
                    "label": 0
                },
                {
                    "sent": "The rich unified code and description that we have and we have worked a lot on the interaction by getting individual and also collaborative relevance feedback.",
                    "label": 0
                },
                {
                    "sent": "And recommendation schemes.",
                    "label": 0
                },
                {
                    "sent": "And as I said, one of our targets was also visualization using visual analytic technology.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's start from the rich unified corner description.",
                    "label": 0
                },
                {
                    "sent": "So we wanted in order to be able to support multi model sets we had to put all the descriptors in one type of representation.",
                    "label": 0
                },
                {
                    "sent": "That's why we introduced the Rocco does it called, which is just one file that we call a content object which contains all types of low level descriptors that could be text, image, video, audio, and three dimensional descriptors relevant to this specific content in the real world descriptions.",
                    "label": 0
                },
                {
                    "sent": "It could be that I'm in the position the weather either RFID tags that could be associated with this content object and user related descriptors could be expressive, emotional or collaborative.",
                    "label": 0
                },
                {
                    "sent": "In case that the scenario supports such.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me now get into details of what we call a condo object.",
                    "label": 0
                },
                {
                    "sent": "The cornered object for us.",
                    "label": 0
                },
                {
                    "sent": "Is just a container of various multimedia objects that carry the same semantic information so.",
                    "label": 1
                },
                {
                    "sent": "One content object may or may not contain all of these types of multimedia information, so it could be a complete contact object containing images text related to this images video related to them, and three dimensional representation, but it could be just the content object containing only one image without any other type of information.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our idea to perform multimodal searches to exploit various types of similarity, for example, that could be an image similarity between this component objects and three dimensional similarity between the others and utilizing this information you can have a multimodal distance and multimodal metric to measure similar.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the idea is to get the thread cone and objects to get the descriptors, to combine them to perform indexing in its type of this descriptors and then we are constructing an end to an adjacency matrix and extracting from them Laplacian Eigen Maps that we can then use as descriptors of the content objects that we have in the database.",
                    "label": 0
                },
                {
                    "sent": "So finally.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we do is to create a multimodal feature space and compute the distance only in this feature space so that we can have a multimodal distance between this content objects.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me go now to the presentation of the results.",
                    "label": 0
                },
                {
                    "sent": "We have a typical way of presenting the results.",
                    "label": 0
                },
                {
                    "sent": "We are giving information about the object we are providing thumbnail of use.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Example we have thumbnails for the images we are supporting for the 3D render thumbnails of the three dimensional object and the slide source of the keyframes, and we can also have synthetic visual previews from the oral features, but we went a little bit the one step further on the presentation.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For example.",
                    "label": 0
                },
                {
                    "sent": "And the visualization of audio features we have tried to map the MFC spectral features first of all, after the dimensionality reduction we have tried to map them to a shape to an image with different colors and different save types, so they say, but the number of the petals, internal and external, and the colours internal and external depends on the specific descriptors.",
                    "label": 1
                },
                {
                    "sent": "So we have a visual representation of the specific descriptor.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this way, just with a federal glance at the results, you can have an idea of the similarity in terms of color and also in terms of shapes of the petals.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In terms of global visualization approaches, we have used the Classic One panel, one trim up in the hyperbolic tree one and Furthermore.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have worked on presenting the results in a grid and also in a smart way so that we bring similarly the results closer to the two dimensional space.",
                    "label": 0
                },
                {
                    "sent": "In this case and also a similar approach which is a smart grid, a little bit a combination of both of them.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the real world information, we are providing information about location and time.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Location in form of a map.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the time you see you can have an idea on when this documents that you have.",
                    "label": 0
                },
                {
                    "sent": "Got this result when actually were actually created in the timeline from 2006, for example to 2009.",
                    "label": 0
                },
                {
                    "sent": "In this case that I'm showing here.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can fully adopt later phase.",
                    "label": 0
                },
                {
                    "sent": "You can also perform filtering in terms of modality in terms of tags that may be associated with the objects date, time and relevance to any type of content.",
                    "label": 0
                },
                {
                    "sent": "And you can also.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Perform also tag yourself.",
                    "label": 0
                },
                {
                    "sent": "One of the results and also perform relevance.",
                    "label": 0
                },
                {
                    "sent": "Assign relevant or irrelevant items in order for the relevance feedback algorithms to use this information to improve their results.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Furthermore, we have advanced directive approaches for visualization.",
                    "label": 0
                },
                {
                    "sent": "You see on the left there is a tree representation where you can just drag and drop drop.",
                    "label": 0
                },
                {
                    "sent": "Each of these results and assign them to a different category and immediately the whole tree organizers and you get the different.",
                    "label": 0
                },
                {
                    "sent": "Different structure of this tree corresponding to the relevance information that you have already provided.",
                    "label": 0
                },
                {
                    "sent": "The same holds on the right side, but a little bit more structured in a grid like form.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All about the visualization of the presentation of the results, I will give the floor now to Thomas to present the interface of isets and also give a demo.",
                    "label": 0
                },
                {
                    "sent": "Thank you for the first part of the presentation.",
                    "label": 0
                },
                {
                    "sent": "So my name is Thomas Tyner.",
                    "label": 0
                },
                {
                    "sent": "I work at Google Hamburg, but legally it's Google Dublin who is in project so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I come from Hamburg.",
                    "label": 0
                },
                {
                    "sent": "So OK when people think of search, think of search as a search box where they can put stuff.",
                    "label": 0
                },
                {
                    "sent": "So we decided to take this paradigm also over to the world of multimodal search and now of course if you have a search box to type in something then you have letters.",
                    "label": 0
                },
                {
                    "sent": "So the idea was to taking this idea of what you search appears in the search box.",
                    "label": 0
                },
                {
                    "sent": "Also over to multimodal.",
                    "label": 0
                },
                {
                    "sent": "So if you look at this example and I will show it later on.",
                    "label": 0
                },
                {
                    "sent": "Life you can see a big search box here and you can see a couple of different modalities here.",
                    "label": 0
                },
                {
                    "sent": "The first one is audio, the second one is an image.",
                    "label": 0
                },
                {
                    "sent": "Then we have a sketch.",
                    "label": 0
                },
                {
                    "sent": "We have emotion of location so you can see.",
                    "label": 0
                },
                {
                    "sent": "Yep, it's search tokens that fill the search box.",
                    "label": 0
                },
                {
                    "sent": "And finally you have the big search now button.",
                    "label": 0
                },
                {
                    "sent": "So yeah, we really try to keep it simple even if a lot of the underlying technologies is quite complex or highly complex.",
                    "label": 0
                },
                {
                    "sent": "But for the user it should be as simple as using the search engine of your choice today, so I will show.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This in action now, and you're also invited as Demetrius set to come to our demo and see it for yourself.",
                    "label": 0
                },
                {
                    "sent": "So yeah, this is the interface as it is now an below.",
                    "label": 0
                },
                {
                    "sent": "You have the different input fields.",
                    "label": 0
                },
                {
                    "sent": "So for example with a pen or with a pencil you can draw a sketch which is kind of hard to do, but.",
                    "label": 0
                },
                {
                    "sent": "Let's just.",
                    "label": 0
                },
                {
                    "sent": "Draw a lollipop here, done and you can see, but I've painted appears live in the search box so I could search with this now, but let's add something more.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "Well, let's actually put this away because it's not really helpful, so let's use images.",
                    "label": 0
                },
                {
                    "sent": "So in our interface we support drag and drop.",
                    "label": 0
                },
                {
                    "sent": "So if you want to upload an image, you can just go like that, select the image.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "Let's drag and drop this image over here and you can see it appears right away, and that's kind of trivial, Sir.",
                    "label": 0
                },
                {
                    "sent": "So let's just see what it looks like.",
                    "label": 0
                },
                {
                    "sent": "It takes a bit an it speaks with the back end server and hopefully if the Wi-Fi is working, yes, we get results back.",
                    "label": 0
                },
                {
                    "sent": "So you can see now.",
                    "label": 0
                },
                {
                    "sent": "I have searched for temple like buildings in our database, so let's start again from scratch.",
                    "label": 0
                },
                {
                    "sent": "Anna add something more to it so.",
                    "label": 0
                },
                {
                    "sent": "Let's use the temple.",
                    "label": 0
                },
                {
                    "sent": "As before.",
                    "label": 0
                },
                {
                    "sent": "An now, for example, let's say, well, we're interested in.",
                    "label": 0
                },
                {
                    "sent": "Another modality which could be research for this temple like buildings in the UK.",
                    "label": 0
                },
                {
                    "sent": "So we add.",
                    "label": 0
                },
                {
                    "sent": "Our Geo location as an additional filter and then we can search again and the result should be different now.",
                    "label": 0
                },
                {
                    "sent": "So this is just a combination of two modalities for different use cases.",
                    "label": 0
                },
                {
                    "sent": "We can do more so now we can see it's less results because I filtered geographically of course.",
                    "label": 0
                },
                {
                    "sent": "For other.",
                    "label": 0
                },
                {
                    "sent": "For other modalities which is not visible in this user interface, but for example for the music use case, we can also search for emotion so it can maybe quickly show you the other latest interface which is.",
                    "label": 0
                },
                {
                    "sent": "Available.",
                    "label": 0
                },
                {
                    "sent": "Here is 1 second.",
                    "label": 0
                },
                {
                    "sent": "But it can also have emotion.",
                    "label": 0
                },
                {
                    "sent": "Well, you can simply use a slider interface to attach emotion to a certain, say music piece.",
                    "label": 0
                },
                {
                    "sent": "So as I move around the slider, you can see the phase moves from happy to sad and the token gets updated in real time as as I search something and then another thing is searching for a rhythm.",
                    "label": 0
                },
                {
                    "sent": "So we came up with.",
                    "label": 0
                },
                {
                    "sent": "The natural way of inputing rhythm, of course, is just typing like that, so this is what we did.",
                    "label": 0
                },
                {
                    "sent": "We made this very simple tapping interface where you can input a waltz rhythm like that 123123 and so on and then once it stands and how it runs for 10 seconds you can see the rhythm that I just input it by a tapping on the touchpad very naturally appears here.",
                    "label": 0
                },
                {
                    "sent": "So now I could search for said waltz music.",
                    "label": 0
                },
                {
                    "sent": "I don't know if this makes any sense, but just as an example.",
                    "label": 0
                },
                {
                    "sent": "OK, and one last thing that I wanted to show is using 3D models so I have prepared some of them here.",
                    "label": 0
                },
                {
                    "sent": "So this is using the open source colada format.",
                    "label": 0
                },
                {
                    "sent": "I quickly open one so that you can get a feeling of it.",
                    "label": 0
                },
                {
                    "sent": "This is just a preview so you can see the Volkswagen Beetle in 3D rotation view and if I take this.",
                    "label": 0
                },
                {
                    "sent": "And I activate the 3D tab.",
                    "label": 0
                },
                {
                    "sent": "And pull this thing here, it appears and then yeah, you could then search with 3D.",
                    "label": 0
                },
                {
                    "sent": "So now if you use the 3D model of a car, let's add.",
                    "label": 0
                },
                {
                    "sent": "Also photo of a car.",
                    "label": 0
                },
                {
                    "sent": "So it was a beetle, so let's add another beetle.",
                    "label": 0
                },
                {
                    "sent": "Rip well.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, sure.",
                    "label": 0
                },
                {
                    "sent": "So yeah, this is a good example.",
                    "label": 0
                },
                {
                    "sent": "Actually offer interface iteration.",
                    "label": 0
                },
                {
                    "sent": "So we kind of show an old interface which is the Safe Harbor because the latest latest iteration is not quite stable yet to show it on screen.",
                    "label": 0
                },
                {
                    "sent": "But in the latest interface what we did was we enabled drag and dropping directly into the search box.",
                    "label": 0
                },
                {
                    "sent": "So I kind of already got used to the new interface so I forgot that in the old interface we still had to switch toggles so.",
                    "label": 0
                },
                {
                    "sent": "Let's see if I switch toggles to image.",
                    "label": 0
                },
                {
                    "sent": "Now it should work.",
                    "label": 0
                },
                {
                    "sent": "Pull it here.",
                    "label": 0
                },
                {
                    "sent": "And then I can search for it and then we should hopefully get images of or representations of cars.",
                    "label": 0
                },
                {
                    "sent": "Let's see if this works.",
                    "label": 0
                },
                {
                    "sent": "3D is the most complex case, so it takes a little while before it can calculate 3D similarity, and then you can see at least some of the results.",
                    "label": 0
                },
                {
                    "sent": "Are relevant more or less.",
                    "label": 0
                },
                {
                    "sent": "So maybe this one and then what you can also do is if this is what you wanted to search for, you can then find.",
                    "label": 0
                },
                {
                    "sent": "Similar things like that.",
                    "label": 0
                },
                {
                    "sent": "So now on the fly.",
                    "label": 0
                },
                {
                    "sent": "I've generated a new query and yeah, you can see a couple of other results here, so we're not claiming at this point that the results are perfect, but we are claiming is that yeah, we have tried to make multi model search as intuitive as possible with all those tokens.",
                    "label": 0
                },
                {
                    "sent": "So of course you can also edit them interactively so you can say I remove this one and then you can.",
                    "label": 0
                },
                {
                    "sent": "Of course, also simply add normal text.",
                    "label": 0
                },
                {
                    "sent": "An answer for it.",
                    "label": 0
                },
                {
                    "sent": "So yeah, this is our current interface iteration, one generation behind because the latest latest one, yeah, was not dimmable yet, but you're encouraged to come to the demo and then try to break it at our booth.",
                    "label": 0
                },
                {
                    "sent": "So here you have seen the current iteration, so.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you and I think we can take some questions.",
                    "label": 0
                }
            ]
        }
    }
}