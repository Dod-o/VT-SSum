{
    "id": "proavqawieyt45m6nfz2j6yumhi3qxmq",
    "title": "Feature selection, fundamentals and applications",
    "info": {
        "author": [
            "Isabelle Guyon, Clopinet"
        ],
        "published": "Dec. 3, 2007",
        "recorded": "September 2007",
        "category": [
            "Top->Computer Science->Data Mining"
        ]
    },
    "url": "http://videolectures.net/mmdss07_guyon_fsf/",
    "segmentation": [
        [
            "So this presentation is of a tutorial nature.",
            "I usually like to have people interrupt with questions, so please don't hesitate if there is something you would like to ask.",
            "And.",
            "This."
        ],
        [
            "Add.",
            "Book.",
            "So this is the way I define a feature selection.",
            "Imagine that you have a big data table.",
            "The lines are patterns and the columns are features.",
            "And you may have thousands to millions of low level features in a lot of recent applications.",
            "Day to present themselves in this way.",
            "And you would like to select the most relevant features in order to build better, faster or easier to understand learning machines, which in essence boils down to selecting a number of columns of your data matrix.",
            "So let me give you first a couple of examples of applications of this problem."
        ],
        [
            "I've been working a lot in medical data analysis and this first example is taken from a leukemia diagnosis problem.",
            "In which leukemia patients have been had had their blood drawn and from their serum.",
            "When has assessed the expression coefficients of their genes and and this is done with the so called DNA microarrays.",
            "So in this slide each little box here represents the.",
            "Level of expression of a given gene.",
            "If the gene is under expressed compared to normal, it's blue, and otherwise if it's red, it's over expressed them.",
            "And as I said before, each line represents a patient in each column represents a feature.",
            "In that case of Gina, there are some special columns that are to the right and the left, and those represent the target.",
            "There are two subpopulations in the data.",
            "The AML patients in the AML patients are two different kinds of leukemia.",
            "And those are represented by here.",
            "The red bar and here the blue bar.",
            "So one very simple way of selecting feature is to looking for those columns in the data set which are most correlated with the target or which are most anti correlated with the target.",
            "So here we have the opposite of the target.",
            "We usually use us target values plus or minus one, so we would have all plus ones for the LL an all minus ones for the AML.",
            "So we call.",
            "In that case the positive class DLL class.",
            "And so this is what people in this early paper in 1999 had done.",
            "The they looked for those jeans that were either correlated or uncorrelated with or anti correlated with the target.",
            "You don't need to limit yourself to two classification problems.",
            "You can do multi class.",
            "You can do regression and variety of other cases, But the problem always boils down to finding a subset of the original variables that are most predict."
        ],
        [
            "In this other example you see another representation of the data.",
            "Instead of showing the data matrix like in the previous slide, we show 3 features and we have a scatter plots of the examples in this 3 dimensional space.",
            "So imagine that you've run a feature selection algorithm and you found those three informative features an imagine that you had three classes.",
            "So in that case it's a prostate cancer problem and there are three populations of tissues.",
            "Tissues that correspond to be 9.",
            "Prostate hyperplasia, BPH and two different grades of cancer.",
            "Grade 3 and grade 4.",
            "And, um.",
            "Here are represented by these... We showed the three clusters forming by the three populations in these 3 dimensional space.",
            "So feature selection can be both used for improving prediction.",
            "Or for facilitating like here."
        ],
        [
            "Visualization of data.",
            "Now I'm going to show you a couple of other examples of feature selection to illustrate that feature selection does not always yield performance improvement.",
            "In this paper, the authors tried to perform cancer diagnosis also from DNA microarray data and they varied the number of genes and here they show accuracy of classification for a problem of classifying 14 trimmers.",
            "And then they tried a number of different classifiers without entering into details you have different curves here corresponding to different strategies that they adopted, and you see that in some cases there is an optimum so.",
            "Accuracy increases as you increase the number of features and then decreases again in other cases, even authentically.",
            "Basically not only increases so the more features the better.",
            "So as it turns out, nowadays there are some very some predictors that are very robust against overfitting.",
            "In the old days, people used to say that there is this curse of dimensionality and that it is extremely important to do space dimensionality reduction in order to get good performance.",
            "When you have a very few training examples.",
            "Nowadays this is no longer true.",
            "You can work in very very high dimensional spaces and if you have a highly regularised classifiers, you can get very good performance without performing first space, dimensionality reduction and this is illustrated by this curve here.",
            "The more features, the better the performance gets in a way if you throw away some features you throw away some information because you're not sure that you're going to be eliminating the bad ones."
        ],
        [
            "However, for some problems you do see a very clear optimum.",
            "So for some problems in which you have a large number of features that are irrelevant, then filtering them out.",
            "Does improve performance and as you can see here in this problem of drug screening in the in which there were.",
            "Around 2500 compounds that were tested for their ability to bind to a target site.",
            "The the purpose was to determine whether a molecule was going to be active or inactive and there were over 100,000 binary features describing the properties of the three dimensional molecule.",
            "Most of these features being completely useless and in fact as you can see as a function of success rate the optimal number of feature is between 20 and 40 among 100,000."
        ],
        [
            "Here's another example in text filtering, so feature selection has been used a lot in the biomedical domain, but also in text processing, and the authors of that paper used various known copper, including the Reuters.",
            "The 20 newsgroup webpage database, and for all of these they use a simple bag of word representation with about 100,000 features.",
            "And again we see on this particular data types of data we have an increase in performance as a number of features.",
            "So the best we can expect in terms of performance is to not degrade too much performance or not significantly degrade while using a very small number of features, and that alone can be a big win, both computationally and also from the point of view of data.",
            "Understanding in this case here I'm showing as an example for the 20 newsgroup, several examples of features that have been identified as very predictive of this particular newsgroup.",
            "Like for all the titanium.",
            "Not surprisingly, you have at Asian at least some morality.",
            "More Interestingly, perhaps for different newsgroups that deal with religion.",
            "If you don't necessarily have the same exact same keywords that are interesting.",
            "So you have got church and sing for subduct religion that Christian but you have Jesus God and Jehovah for Thunder tradition that makes so whatever the details are that can be of interest."
        ],
        [
            "So now other applications have been tried like face recognition.",
            "So in this application it is interesting to see that people have merely used the pixels.",
            "They haven't done any fancy feature extraction.",
            "And they've tried to find which part of the image is informative for classifying male versus female, and they tried two types of feature selection algorithm.",
            "One called relief.",
            "It's a filter that does a total ordering of the features and doesn't attempt to remove redundancy between features and the other ones seem bad that the device does take into account correlations between features and tries to find those features that are most complementary with one another.",
            "So here you see the result of using 100 features 500 and 1000 top ranking features.",
            "For the first algorithm that does not remove redundancy, you have kind of symmetry between the two sides of the feature, so features that are redundant because for example they represent the two eyes they are being selected, whereas with the second algorithm that removes redundancy, only half of the features corresponding on one side of the image are being selected."
        ],
        [
            "So I just presented, you know, to give you a rough idea, some examples of feature selection and before moving into the meat of the talk, I'd like to give you some nomenclature.",
            "I'll be talking about univariate methods as those methods that consider one variable or one feature at a time.",
            "By contrast, multivariate methods consider subsets of variables of our features that together are good for making predictions.",
            "Filter methods, rank features, or feature subsets independently of the predictor or the classifier.",
            "I've been talking a lot about classification, but I'd like to remind you that this also holds for regression problems.",
            "And the wrapper methods use rubber message user classifier to assess features or feature subsets and therefore determines usefulness of feature subsets with respect to a given."
        ],
        [
            "Prediction method.",
            "First, let me talk to you about the simplest methods.",
            "The univariate filter methods."
        ],
        [
            "The easiest is first to determine what feature irrelevances and by contrast will determine what feature relevance means.",
            "Feature irrelevances, as it turns out, is well defined mathematically.",
            "Imagine that we have a single variable XI.",
            "And that we have a two class classification problem with the positive Class Y = 1, An negative class Y = -- 1.",
            "An I'm representing here the density of the examples along these one axis XI.",
            "So if that particular feature is irrelevant, which we expect to see is that there is going to be 2 identical blobs for the two classes.",
            "You won't be able to distinguish the distribution of the examples of class one with respect to the example of class minus one.",
            "Mathematically, this can be formalized as the joint probability of Zion Y is equal to probability of exact probability, of why?",
            "So there is independence between zyan Y.",
            "Or using base rule you can also say that P of X are given y = P of XI.",
            "Or again you can say that P of XI given Y = 1 = P of XI give a Y = -- 1.",
            "So the distribution, the marginal distribution P of XI is the same thing as pure vexi condition on why call 1 Ann is also same thing as pure vexi condition on white called minus one.",
            "OK, so how do we define now relevance by compliance with irrelevance?",
            "What we need to do is that we need to measure some displacement between these two distributions."
        ],
        [
            "And this has done has been done in a wide variety of ways.",
            "There is only one way of defining irrelevance, but there are many, many ways of defining relevance.",
            "And one of the simplest way is to use this simple parametric method and the parameterized distribution with just two parameters.",
            "There are the mean and the standard deviation.",
            "And.",
            "Define a criterion of relevance as the displacement between the means of the two distributions, normalized somehow by the standard deviation.",
            "So, for example, normalized by the average of the standard deviations.",
            "So this criteria has been known as the signal to noise ratio criterion in the community of DNA microarray analysis.",
            "But, as it turns out, it's very similar to the person correlation coefficient.",
            "At least in the case where you have an equal number of examples of class one and Class 2.",
            "In this case, if the target is plus or minus one.",
            "If you do the dot product between variable XI and the target Y, you can easily see that when you multiply by the Y equal plus one of the elements of the variable and add them up, then you obtain something which is proportional to new plus.",
            "An when you multiply by minus one and sum up, you obtain minus mu minus right.",
            "So just doing this dot product is virtually identical.",
            "If you have a number of example which is the same for the two classes.",
            "By subtracting real plus from you minus.",
            "In practice, what people have been doing is exactly that they've been first normalizing their features by the standard deviation.",
            "And then computing the person correlation coefficient so they've been doing virtually the same thing as the people who do signal to noise ratio.",
            "So measuring displacement between the two distributions.",
            "So all of these methods I would qualify them as correlation methods and there are many different.",
            "You know correlation coefficients that can be defined in order to assess the correlation or the dependence between the variable Y and."
        ],
        [
            "Horrible XI.",
            "Now I just corrected myself when I said, you know, I said correlation by by assessing dependence.",
            "Because correlation in particular, the person correlation coefficient as this is linear dependency between Zion Y.",
            "So not all dependencies are linear and.",
            "One way of measuring and only their dependencies is to use the mutual information.",
            "So you as you remember, I defined independence using the simple equation that P of X&Y equal P of XPLY.",
            "And then in order to measure dependence, you could imagine to use some coefficient that would measure the discrepancy between these two distributions.",
            "And this is exactly what mutual information does.",
            "You first form the ratio of P of X&Y with respect to P of XP of Y.",
            "You take the log and then you take the expected value of RP of X&Y and this also is the kullback Leiber divergent between P of X&Y&P of XP of Y, so it really measures the discrepancy between these two.",
            "Distributions.",
            "So you could say that you know why don't we always use mutual information to perform feature selection?",
            "There are many reasons, and one of them is that mutual information is actually difficult to estimate.",
            "And for those of you who are familiar with the problem of overfitting.",
            "We can have a parallel here between the fact of using, you know simple linear model or training complex nonlinear model.",
            "If you use a simple linear model, you less at risk of overfitting than using a complex nonlinear model an in a way, even though we are not here talking about wrapper methods in which we have a classifier to select features.",
            "Filter methods use criteria that are similar to having using single variable as a predictor.",
            "And if you use a single variable as a predictor using a linear relationship, you'll less at risk overfitting that if you use a nonlinear relationship and not in aeration.",
            "Ships are harder to estimate from data.",
            "So.",
            "Simply so.",
            "This is a simple justification for not always using mutual information.",
            "As it turns out, the simple person correlation coefficient often gives best results."
        ],
        [
            "Well, there are many criteria I said for the for defining some kind of dependency between Zion Y and we have edited the book after our challenge we organized on feature selection in 2003.",
            "In chapter three of that book, you have a whole list of criteria and the choice of the method that you're going to be using to rank your features with those criteria will depend on the nature of the variables and the target, whether they are binary, categorical or continuous.",
            "The nature of the problem, whether you expect the dependencies between variable and target to be linear or nonlinear.",
            "And the availability of data.",
            "If you have smaller number of examples, you might want to use simpler criteria that are less at risk of overfitting.",
            "Also depends on the noise you have in data.",
            "And finally depends on the availability of tabulated statistics."
        ],
        [
            "I've presented in the previous slides some simple examples of criteria to determine the displacement of 1 distribution with respect to the other.",
            "No, you might have noticed that this signal to noise ratio that I presented is very similar to the T statistic.",
            "So why not use the T statistic in the 1st place?",
            "But it's a good question actually.",
            "You probably might want to use it because since it's tabulated you could also run a statistical test with it.",
            "The null hypothesis would be that the two distributions are identical, or more precisely in that case that there is no difference between the means of the two distributions.",
            "You first have to estimate to assume that the variance is known and that is the same, though that the variance even, even though it is unknown, is the same for the two hour classes.",
            "Then if the hypothesis is true, then the T statistics obeys the student distribution.",
            "And then after that you can use the P value as an estimation for the false positive rate.",
            "I'll go into more detail."
        ],
        [
            "So this in the next slide.",
            "So in chapter two of the book, you'll have a whole number of statistical tests that you may want to use.",
            "They all use the same principle.",
            "You have to assume another distribution.",
            "Which of this distribution of the garbage features those that are not relevant at all to your problem at hand?",
            "And there is a tail of the distribution of these garbage features.",
            "And if the features that you observe in your data are in the tail of the distribution of the garbage features there, you're going to, then you're going to say that with some confidence it's not possible that they belong to the null distribution, and therefore they should be relevant.",
            "It's kind of a indirect way of estimating relevance.",
            "So assume that you have defined the ranking criterion by which you are going to rank your features according to relevance, and I'm calling this criterion R for example.",
            "It's the person correlation coefficient.",
            "Or is that the statistics or whatever criterion you have defined.",
            "And then assume that for some reason you know the distribution of this of this criterion for the garbage features.",
            "For the ones that are not interesting, then you can set a threshold or zero that determines the tail of the distribution.",
            "And then the area under that curve here is the fraction of.",
            "Over garbage pictures that exceed that threshold.",
            "R 0 so for example, if you are against threshold, you might have just 1% of the garbage features exceed the threshold and therefore you may be satisfied with the fact that if your feature of interest has a value of our which exceeds our O may be satisfied with the fact that you know that only you know a very small fraction of the garbage features can exceed that value, so this gives you confident that this is not a garbage feature.",
            "I see some people found that clear enough.",
            "So the null hypothesis is that X&Y are independent and you define a relevance index that we're going to identify to a test statistic.",
            "Now the P value of these tests is going to be identified to a false positive rate that is the fraction of the number of false positive over the total number of irrelevant features.",
            "So this is the distribution of irrelevant features and the false positive rate is this area.",
            "Here is the fraction of features that I thought were relevant, but actually a garbage features.",
            "Now this this looks all nice, but in practice you often have hundreds of thousands of features that you are interested in testing, not just one.",
            "And then you have a problem of multiple testing.",
            "That is, you're repeating this test many, many many times.",
            "So then then the chances are that you're going to find quite a few features that fall in the in the tail of the distribution merely by the simple fact that you've repeated that test many time, and this multiple testing problem been known in statistics for a long time, and you need to correct for the P value in order to get some some estimate of the false positive rate.",
            "If we repeat the test many times and a simple way of doing that is just multiplying the original P value by North.",
            "So this corresponds either by moving R0 to keep your P value that you wanted at the beginning, or correcting the P value for a given value of R0 that you've chosen.",
            "Now, an additional difficulty is that most of the time you do not know the null distribution.",
            "Why should you know it in the 1st place, right?",
            "Of course, you could have postulated unknown distribution and say for example, you know it's the T distribution, which is the one that I want to look at, but you've been making some hypothesis in order to assume this distribution.",
            "Another way of doing that is to define an arbitrary criterion for relevant of relevance.",
            "That is, maybe application dependent that you think is reflects well the way you want to rank your features in order to relevant in order of relevance, and then instead of defining your distribution, you're going to inject into your problem garbage features that resemble as much as possible the real features that you have.",
            "So in order to do that, for example, one way of doing it is that you take your original data matrix and you take the contents of your original data matrix and you scramble the values.",
            "Now you get columns in your new data matrix with columns that are absolutely not correlated with your target data.",
            "They are not informative, but the distribution more or less looks like the original ones, and so you can rank those features together.",
            "With the real ones and see what fraction of the garbage features that.",
            "That you ranked end up to be in the top ranking ones.",
            "And that allows you to define a false discovery rate.",
            "Which is defined as the number of false positive over the number of selected candidates.",
            "And that you can bound by this quantity here.",
            "So by what we call the probe method, instead of calculating the P value using tabulated statistics, you estimate the false positive rate as the number of selected probes over the total number of probes.",
            "So remember, this was the distribution of the garbage features that we had postulated.",
            "Now we just inject garbage features that we have constructed by this randomization process.",
            "And what we do is that we look at what fraction of the garbage features that we have generated exceed this threshold are zero, and this gives us or estimate of the false positive rate and without you entering into details you can show that this can be plugged into this and you can also get an estimate of the false discovery rate in this way."
        ],
        [
            "So there's much you know that could still be said about univariate methods, but I guess the one of the reasons why they've been used so widely is because of their simplicity and speed.",
            "You can very quickly run a statistical tests on hundreds of thousands of features, rank, then estimate the false discovery rate, and then take, you know, the, say, the top.",
            "So and so many features that have a false discovery rate less than 1% or so, and then with some confidence you can say that there is only 1% of garbage features in the features you've selected, so that's been while they use now.",
            "This doesn't necessarily mean that when you've done that, you're going to get good performance on the predictor."
        ],
        [
            "And there are many reasons why there are limitations on the univariate methods and universe methods may fail.",
            "On this little scatter plots I'm illustrating.",
            "Why you might want to use multivariate methods?",
            "And I'm considering only two variables X1 and X2 for example.",
            "This is the age of the patient and this is the weight of a patient.",
            "And there are two 2 distributions of the right patients and the grid patients.",
            "And if you project on one of the features.",
            "You can see what is the univariant power of separation of that feature.",
            "So if you project on feature X one, you can see that the examples are reasonably well separated.",
            "At least you can separate with less than 50% error, whereas if you project on feature X2 you have almost, you know exact overlap between the two distributions and therefore feature X2 by itself is not predictive.",
            "So here is an example in which you have one feature feature X1, which is by itself predictive and one feature feature X2, which by itself is not predictive.",
            "So any univariate method would tell you to throw away feature X2.",
            "Yet you can see that in two dimensions you have an almost perfect separation of the two categories, whereas you cannot separate very well with this single feature X1.",
            "So this example illustrates that a feature, even if it's by itself completely irrelevant, it can help another feature become more relevant so.",
            "So in two dimensions, we might want to select both features in order to achieve better separation.",
            "In the second example, this is an example of a nonlinear.",
            "These were the classes are non linearly separable and it's well known in the machine learning that there is this nasty X or problem or chess board problem in which you can't find a linear separation and this is also a nasty problem for feature selection because when you project the examples and this time you can project them either on feature X one or and feature X2 you get no separation on either of the features.",
            "And so in that case, if you use univariate feature selection, it's completely hopeless.",
            "You need to look in two dimensions in order to get any kind of separation, but."
        ],
        [
            "Into categories.",
            "So how do you solve these problems?",
            "Um?",
            "Before in in when I talked about univariate methods I I mentioned in the title is univariate filter methods.",
            "Not all filter methods are univariate.",
            "There are some filter methods that take into account the context of other features.",
            "And they allow you eventually to do a total ranking of the features, but they inform you about the context of other features and will show you an example of that.",
            "Most commonly, people use reper methods for multivariate feature selection, and those consistent producing a large number of subsets of features, sending them to a predictor in order to access their prediction performance and then.",
            "Iterating, guiding the search in some way, producing more feature subsets and testing them again.",
            "There also so called embedded methods.",
            "In which simultaneously the algorithm produces an optimum feature subset."
        ],
        [
            "A predictor?",
            "So the first example I'm giving you is an example of.",
            "Ranking method.",
            "In which the ranking criterion uses the context of other features to produce a total ranking of the features.",
            "So assume that you have two features, again X1 and X2, and you recognize you know it's or chess board problem.",
            "So if you project in on any of the two dimensions X1 or X2, you don't get good separation and therefore it's very difficult to do feature to feature ranking in that case in a univariate way.",
            "But Karen Randall proposed a simple scheme that allows you to solve that problem, and it has the flavor of the nearest neighbor algorithm.",
            "For any different example.",
            "You're looking for its nearest hit and its nearest miss.",
            "The nearest hit is the nearest example of the same class and their nearest miss is the nearest example of the opposite class.",
            "And you do that this, you know, search for nearest hidden nearest Smiths in the origonal space.",
            "So taking into account all the features.",
            "So that's the trick.",
            "That's how you get information about the context.",
            "But next what you do is that you project.",
            "On the various features.",
            "So for example, you project on feature X1.",
            "And then you look at the distance to the nearest hit and the distance to the nearest miss in projection.",
            "On that particular feature.",
            "And in that projection, you compute the ratio of nearest MTR over nearest hit.",
            "So if you have, you know a large distance on average over all the examples between nearest messengers hit, then the feature is informative with respect to the separation between the two classes.",
            "So the way this this this works is in essence, it's like you have been, you know, slicing the space in a way right?",
            "And you're looking at a local neighborhood, and in a neighborhood of.",
            "Of some of the samples, if you see that there is a possible separation between the two classes, then the feature is informative.",
            "Even though you are projecting afterwards, so this also has the flavor of conditioning your conditioning on some neighborhood.",
            "By using this nearest neighbor trick.",
            "And if you know condition on some neighborhood of the examples you observe that there is a separation between the two classes, then this this particular feature is informative.",
            "So this criterion is largely heuristic, but it can be theoretically justified."
        ],
        [
            "But more commonly, people doing multivariate feature selection will not use filters.",
            "They will use wrappers.",
            "And for rappers, what you do is that we explore the space of all possible combinations of features, but that's a huge space because for any features there are two to the end possible feature subsets.",
            "So both computationally and statistically the problem is very complex.",
            "Which has led people to finding.",
            "Search strategies that are.",
            "Suboptimal in some ways, but efficient in other ways.",
            "So in this graph here, I'm representing all the possible.",
            "Feature subsets for only four features.",
            "And one means that the feature is selected and a 0 means the feature is not selected.",
            "And the errors here mean that you're walking through that space by either adding or removing one feature.",
            "So we'll see algorithms on how to move into that space."
        ],
        [
            "Efficiently.",
            "There are many such strategies that have been proposed, and some of them are listed in the chapter four of our book.",
            "They include exhaustive search, which essentially nobody recommends unless you have a large number of examples and few features.",
            "Other rather exhaustive methods include the simulated dynamic and genetic algorithm.",
            "Also very prone to overfitting and not recommendable unless you have a lot of data.",
            "Bing search methods, greedy search forward and backward selection, which I will go into more details later.",
            "And then some hybrid methods that allow you to move forward in this space and then a little bit backward, and then I will forward and those include these plus plus L. And take away our and their floating Sir."
        ],
        [
            "Ouch.",
            "So once you've selected so strategy, you also need to in order to have a complete algorithm to select a criterion by which you're going to decide whether your new state is better than the previous one, or better than another state that you've explored already.",
            "Typically what people do is that they use cross validation and most of you are familiar.",
            "Of course with cross validation there is the small twist here that we're going to split our data set in two or three subsets, one for training, one for validation and one for testing because we need to be careful not to mix up the feature selection process.",
            "With testing, we have in essence now like what we have when we do model selection multi level inference.",
            "Process which includes a for each feature subset, training and predictor on training data and then selecting the feature subset which performs best on validation data.",
            "Eventually repeating, you know to reduce.",
            "We just variance by performing cause validation on multiple splits between training and validation data.",
            "But finally only test on test data and that should happen only once you have finished completely the both the training and the feature selection process."
        ],
        [
            "I'm on this sub graph summarizing the three axis that those three ingredients of all feature selection methods.",
            "In which include defining a strategy defining criterion by which you're going to assess how good your features are.",
            "And choosing an assessment strategy so sometimes people are confused about why you know I'm separating these two axes.",
            "Defining the criterion doesn't tell you how you're going to estimate it statistically.",
            "And using cross validation is one way of assessing your criterion.",
            "But there are other ways, including performance bounds, and I've talked about, you know, statistical tests and in that space you can basically determine whether you have a filter or wrapper or an embedded method.",
            "So I've shaded here the area in which filters live an rappers live.",
            "You know, in that kind of area and embedded method in this way, so there is not a single way of categorizing or feature selection.",
            "Selection methods you can either.",
            "Categorise them in terms of you know universe versus multivariate or filter wrapper.",
            "Embedded or or categorize them by their source strategy or criterion or assessment method strategy.",
            "And now I'll be going through a couple of the most popular algorithms and also the most effective ones to give you a feel for what should be."
        ],
        [
            "Done.",
            "1st, I'll be talking about forward selection strategies in the wrapper setting.",
            "And it's also referred to as SFS or sequential forward selection.",
            "You know my little example in which you have only four features.",
            "We start with the state in which you have selected no features so 0000.",
            "And then we can move by adding a single feature to one of the next states.",
            "And we're going to be assessing how well we're doing into this next stage.",
            "For example by training a predictor an assessing its performance by cross validation, but not necessarily.",
            "You could also be having any criterion you want and just compute it on training data.",
            "You don't need to do cross validation if you don't want to, or you could have a performance plan or whatever you want.",
            "And then you decide which node performs best and you select that one and you move only from that node.",
            "So it's a greedy search in which you know you don't look back.",
            "You always move forward an at each step you consider all the possibilities, resulting for adding a single feature.",
            "And so you have N possibilities at the first step, N -- 1 at the 2nd and minus two, etc.",
            "I'm down."
        ],
        [
            "1.",
            "You could do the same thing in the embedded method flavor.",
            "In that case, your search will be even more narrow down now at each step, instead of considering all possible following features, you'll just consider only one, and the reason why you can afford this luxury is that your training algorithm is going to guide you through the search training algorithm is going to suggest to you which feature is going to be the good one to try next.",
            "And the difference between the two methods is that for the embedded method you had, you know, and then N -- 1, then minus two trials.",
            "Here you have only one at each step, and so total the total number of classifiers.",
            "And if you choose to train classifiers, total number of classifiers you're trying is only N instead of N -- 1 / 2.",
            "Sorry N + 1 / 2.",
            "So it's a small difference but.",
            "Computationally it can.",
            "It can be important, and also because you're trying fewer classifier.",
            "It simplifies your second level of insurance and I will see as we will see later."
        ],
        [
            "This is less prone to overfitting.",
            "OK, now that we've illustrated forward selection, here is an example of algorithm for selection.",
            "You can select the first feature DT.",
            "That is, the has the largest cosign with the target.",
            "And then proceed with the gram Schmidt orthogonal isation.",
            "Then for each of the remaining feature you can project.",
            "Project them on the null space of the features already selected, and then again compute the cosine in the.",
            "India projected.",
            "In the space of the projected features.",
            "And you if you repeat that, you know it's relatively.",
            "You get a subset of features that is optimum with respect to the least square predictor.",
            "So in that sense it's an embedded method for forward selection.",
            "It's a very effective one, and it tells a very compact."
        ],
        [
            "Subsets of feature.",
            "There are many other for selection methods and those of you who are familiar with the decision trees.",
            "Decision trees perform in a way, some embedded feature selection, because at each step in the decision tree process.",
            "D's three selects the algorithm, selects the feature that allows you to separate the data into subsets such that the purity of the subsets is enhanced, so there are more examples of class one in one of the subsets and more example of Class 2 in the other subset, and you repeat the process many times until you achieve maximum purity in the leaves.",
            "So there is an embedded way of selecting features and usually at each step you use entropy and in order to as a feature selection criterion.",
            "This is very similar.",
            "In fact, what people do when they use a mutual information for feature selection as entropy and mutual information are very."
        ],
        [
            "Related there is also another way of performing feature selection in a greedy manner, which is backward elimination.",
            "In that case, you start with all the features and you progressively remove features 1 by 1.",
            "And similarly as before, you can either perform that in in the wrapper way by trying all possible features that you would be removing."
        ],
        [
            "And the next step.",
            "Or you can try it in the embedded way.",
            "The difference being that the algorithm then it's going to guide your search is going to tell you which feature to try next."
        ],
        [
            "So one particular example of that is the backward elimination algorithm, RFE, that was designed for support vector machines, and it's very simple.",
            "You start with all features, then you train the learning machine on the current subsets of features by minimizing a given risk functional, and then for each of the remaining features you estimate the you select the feature that.",
            "It results in the least increase of the cost function or or the decrease of the construction.",
            "So either you least degrade or you improve your your performance as measured by your cost function.",
            "And then you remove that feature.",
            "That improves all these degrade J and you start over again.",
            "You train again with the remaining features and you iterate.",
            "So this kind of embedded methods work for SVM, for kernel methods and."
        ],
        [
            "Are all networks?",
            "No, instead of abruptly removing features in these you know greedies or or adding features.",
            "In this research methods.",
            "What you can do for embedded methods is to use scaling factors and this opens you the door to a wide variety of algorithm for feature selections that are combined with training.",
            "So before you know you know you know the space of feature subsets, we had a one for selecting the feature and a zero for not selecting it.",
            "This corresponds to this vector of sigmas.",
            "There is one for each feature.",
            "One represents presence of the feature and zero absence of the feature.",
            "So these are discrete indicators, so you can replace these discrete indicators by continuous scaling factors that are now between zero and one instead of belonging to just 01.",
            "Sorry there is a typo here, so this is meant to be between zero and one.",
            "So the advantage of doing that is that now we can perform gradient descent on the scaling factors.",
            "Do we take a break in the middle summer?",
            "So I think this would be a good time to break to make you reason and meditate on that and then the remaining part of the talk.",
            "I'll be showing you how you can design your own algorithm for feature selection with your own training method using the scaling factors.",
            "And then we'll be talking about causality.",
            "So in general, if you only care to make predictions, you don't care to know whether the variables cause the target or consequences of their targets, so.",
            "It's both predictive of disease.",
            "To find genes that are.",
            "That have mutated and are causing your disease, or to find the proteins that whose concentration varies in serum as a consequence of the disease.",
            "Like for example, an immune reaction.",
            "However, in some cases, as we will see when we care about performing actions on the system and seeing what is going to be the result of our actions, like for example delivering a drug to a patient, whether it's going to cure or not.",
            "In that case we care about causality and whether the features are not causally related to the target.",
            "So we'll talk to both, but both of these problems in the second part of the presentation, Now 5 minute break.",
            "It's a welcome back and thank you for coming back and after the test of the first hour.",
            "Um?",
            "First, I'd like you to take questions if you have some questions on the 1st part.",
            "If they can some questions offline, but welcome to you know.",
            "As someone here.",
            "You said that you think controlling the future is in taking into account also the multiplicity of the feature.",
            "So you correctly, properly.",
            "But then you say that you then estimate the discovery rate.",
            "So why don't you?",
            "We just optimize in thinking for coming to each day of the first discovery rate.",
            "Yes, that's correct.",
            "I didn't want to enter into details, but I briefly spoke about the problem of multiple testing and the boundary correction.",
            "But if you go the way of estimating the false discovery rate, this becomes irrelevant.",
            "You can only deal with the false discovery rate.",
            "I didn't also give you many details about this probe method and the difference with the using tabulated distribution.",
            "But there are some.",
            "Delicate aspects about this problem, and in some experiments with perform we do not get always the same results in estimating the false discovery rate with tabulated distribution.",
            "With the probe method.",
            "Because in a sense, the null distribution that you assume is rather different.",
            "In both cases.",
            "Um?",
            "So practically, a lot of people in the medical domain they're used to using P values and for discovery rate, so you need to give that to them.",
            "But practically I would trust most cause validation results.",
            "However, they give you slightly different information.",
            "Because, uh, the forces curve rate and P value give you some idea about the fraction of bad guys that you have in your selected features.",
            "Whereas your cross validation results give you more sense about how predictive your feature set is.",
            "An you may include a large fraction of garbage features in your feature set and yet get very good predictive values.",
            "So for example, if you use support vector machines on the entire feature set.",
            "You may get as good results as with your restricted feature set, or even better, results.",
            "Even though you have tons of garbage features.",
            "So in challenges that I've organized, I have introduced purposely some garbage features so I knew for a fact that they were garbage features and a lot of people have obtained.",
            "You know, top ranking performances without performing any feature selection.",
            "So you basically need both aspects depending what your emphasis is on right.",
            "If you care about.",
            "The features that you select, for example in the medical application, if you care about truly understanding which features are predictive.",
            "Knowing that you get good prediction performance in cross validation, maybe not enough, in which case you want you need to resort to computing file discover rate.",
            "I have one more question.",
            "Basically what he showed you should the.",
            "Picking up to come down, but I'm not please this piece of.",
            "But they also happen.",
            "Is that featured in one picture of those cities like important so important?",
            "If you have those two together, which you will not find out?",
            "That's correct.",
            "That's correct, so forward selection and backward elimination methods do not yield the same type of feature set set at all backward elimination.",
            "You may end up with a set of complementary features, and as you keep removing features then you have this catastrophic degradation at some point because at some point you will be removing one feature such that you know all the remaining features are not productive together anymore.",
            "You don't see that in forward selection because the first picture you select is predictive, but then you run at risk of selecting first feature that is predicted by itself, But then you miss eventually a pair of features that together would have been more productive than than that single feature that you selected first.",
            "In for selection, you tend to get very compact sets of subsets of features an which may be an advantage or disadvantage because sometimes redundancy helps you give you give get better performance on the test set so there is no you know one method that should be recommended depends on what you want to achieve.",
            "Sometimes it's important that you will be able to trade number of features for accuracy.",
            "This is an advantage in that case to do forward selection because you can smoothly move.",
            "You know your number of features without having this catastrophic degradation.",
            "And sometimes you really want to have an optimum feature subset of complementary features, in which case backward elimination may be an advantage.",
            "So I had drink the right two other questions.",
            "One of them was yes.",
            "Hi, in the reading of your presentation you show these cubes where.",
            "You asked that no feature selection was good for a certain classifier.",
            "Anne.",
            "Do you think that maybe that's a risky compression?",
            "Because that could be because we don't have a good feature selection method.",
            "I mean we the more, the more features we have, the better room is pacifying, but maybe that's good because our feature selection is not good so we don't have this maximum.",
            "And then with this, yes this is also a problem of the difficulty of finding good subsets of features when you don't have a lot of data.",
            "As it turns out, the feature selection problem is much harder than the problem of training the classifier.",
            "And so if I tell you which features are good, so for example, if I had some extra datasets on which I could be selecting features 1st and then using those to try, and you know on the data, then I might gain a lot of performance.",
            "Actually, these are experiments that we've done that we had some old data sets on which that we could use to select features and then we would use those features on some new data and then gain a lot in performance so.",
            "Feature selection is extremely data hungry and you have to do both feature selection and training your classifier on the same data.",
            "So if you don't have enough data, you may just be in better shape if you just choose all their audio features and train directly or classifier.",
            "If you have, you know some good regularization going on at the same time.",
            "I have a question about the context of time series problems.",
            "If you consider another regression.",
            "So then your data matrix becomes just values aspects of your output.",
            "Is there any fundamental difference in the techniques for showing in that respect?",
            "So I have no time for many experiments on that, so I'm not familiar with that problem and some other people have been asking me that question during the break.",
            "I suppose you know it would be a relatively simple extension of those algorithms to go to the online case in which you have continuous flux of data that eventually changes overtime.",
            "If you use these methods of scaling factors, you could keep adjusting your scaling factors overtime.",
            "There was another question also during direct regarding multi category classification or multi objective optimization in which you want to select a subset of features that simultaneously is good for many purposes.",
            "And obviously you can do that.",
            "Some people have been designing special purpose problems for that, in particular for multiclass SVM, but it has also been shown that you often get better performance if you select features.",
            "Separately, for each of the objectives and then.",
            "And then combine the results at the end.",
            "And again, it's very dependent upon the problem and also upon the number of examples that you have available.",
            "As always, if you have very, very few examples, you might benefit from constraining your problem more.",
            "So imposing that you have a single feature subset for all your multiple objectives, because this adds some regularization.",
            "If you have enough examples and you're probably better off having separate feature subsets for each of your objectives.",
            "Um, let's move on and."
        ],
        [
            "So one way as I said of doing embedded methods and it's very flexible is to have these scaling factors.",
            "So one way of viewing this is that you can think of your data matrix has, as you know, number of lines corresponding to examples and the number of columns corresponding to features.",
            "And why is you know an extra column for the target?",
            "And if you use a kernel method in which the parameters of your learning machines are the alphas, that way the examples that have been passed, for example some kernel function.",
            "Then you have the Alpha waves that monitor the lines of the matrix, and if you use some algorithm like SVM that select a certain number of support vectors that are examples in particular lines here you can see that you can see that you're going to.",
            "Reduce your data matrix in that direction here.",
            "Whereas if you introduce scaling factors, you do it in the other in the opposite direction.",
            "So in a way you have now two ways of compressing your data.",
            "One you know in the direction of the patterns and when in the action of the features.",
            "So a lot of people have been using scaling factors came from the kernel method community, and they found that this was a convenient way of.",
            "Introducing both, you know support vectors and kind of support features, right?",
            "Um?",
            "In the next few slides, I'm going to borrow some slides from on the cell phone."
        ],
        [
            "And talk about a little bit the formalism that goes behind these embedded methods.",
            "Many younger learning algorithms are casting to minimization of some regularised functional are of Alpha and Sigma.",
            "And the the minimum runs over the the weights of the learning algorithm, the alphas.",
            "They will not be always waits on the patterns, but you know, for all kernel methods that that's what they are.",
            "And this risk functional.",
            "Is the average of some loss function that you can represent as of.",
            "Measure of discrepancy between the function that you are.",
            "This is your model is your predictive model and that is itself a function of Alpha.",
            "And now of the scaling factors that each multiply.",
            "This is, you know, the the component why the componentwise multiplication, so each coming factor multiplies one input coefficient.",
            "And so we compute the discrepancy between this F of XK&YK.",
            "So you see, now that we have two sets of parameters in here.",
            "And we have, of course a regularizer on the alphas.",
            "Now what we can do is that we can once we have minimized over Alpha, define a function J of Sigma.",
            "Now function of the scaling factor."
        ],
        [
            "And has been shown by Andre and his collaborators.",
            "Under some conditions.",
            "The removal of 1 feature will induce a change in G proportional to this gradient here of F with respect to X I ^2.",
            "So this is like doing a sensitivity analysis.",
            "You are, you know, shaking a little bit.",
            "One of the inputs and you're looking at what is going to be the impact of this disturbance on F of X.",
            "And for example, if you apply this to the linear SVN, as it turns out, this DFF of RXI is nothing that is proportional to the Wii.",
            "So what you can do is that you can train an SVM.",
            "You can look at the simply the weights of the linear predictor an each take the square of these weights and rank the feature according to this query this ways.",
            "And this is basically what has been."
        ],
        [
            "Done with the recursive feature elimination algorithm.",
            "What you do is that you first take a certain set of features that is called F. Here in this notation.",
            "And then you compute the W star solution and an SVM on this data set restricted to the features in F. So this means that you are minimizing your risk functional with respect to Alpha.",
            "Then you select the top features ranked by the absolute value of WI star or the square of WI stars the same thing.",
            "And, um.",
            "And then you are going to basically remove the feature that has the smallest value of WI in absolute value.",
            "So this is equivalent to.",
            "Performing this trick of scaling factors because these WI absolute values are proportional to these scaling factors.",
            "And then back to Step 2, you each rate, you retrain your SVM.",
            "Then you find again those weights that are smallest etc.",
            "So it's nice to see that this very simple method is principle and it extends through the nonlinear case."
        ],
        [
            "To the nonlinear SVM.",
            "You can also perform gradient descent.",
            "So now in this case, instead of removing at each iteration, the weight that has the smallest absolute value.",
            "You can iterate and compute, recompute the scanning factors according to some gradient descent scheme.",
            "At in the end of this process, where you iterate minimizing over the alphas and minimizing over the sigmas, you obtain both a set of alphas and a set of sigmas.",
            "But this of course does not perform real feature selection, just rescales your inputs.",
            "So if you want to do real feature selection, then you will have to then remove the features that have the smallest sigmas.",
            "So it mixes well with, you know, any possible algorithm that allows us to do gradient descent, but it's."
        ],
        [
            "Saving computation and has local minima.",
            "Instead.",
            "People have been thinking of taking a different approach to feature selection.",
            "Which is that of.",
            "Minimizing the number of feature.",
            "And symbol Tanias Lee addressing the objective of obtaining best prediction performance.",
            "So instead of saying, you know, let's minimize or primary objective which is getting best performance and let's you know try to disturb it to this possible by doing some feature selection on this side, replace that by the problem of minimizing the number of feature an simultaneously addressing the objective of, for example, minimizing the number of errors of classification.",
            "So the problem with this is that this objective here is not very well behaved, because it's it's discrete.",
            "And so you don't.",
            "You can't perform gradient descent and all sorts of things on that, so people have been replacing this objective, but other objectives.",
            "For example, the sum of the absolute values of the Wis which people called the L1 norm, or some kind of differentiable function that uses that, performs basically the same thing.",
            "And so then you minimize jointly."
        ],
        [
            "The primary and the secondary secondary objective.",
            "One very popular algorithm that uses this principle is the so-called L1 SVM.",
            "And it's a version of the regularizer VM Ware, the minimization of the square norm has been replaced by the minimization of the one norm, so the sum of the Wii.",
            "And so it can be considered in some way, an embedded method, because it returns in the end.",
            "Both.",
            "A predictor that gives you no prediction on.",
            "On new examples, if you want an, it also returns a certain number of.",
            "A features that have not been discarded because some of the weights will be exactly 0.",
            "So the difference between the regular SVM and the other one is VM.",
            "Is that the algorithm drives some of the weights to 0.",
            "So how how?"
        ],
        [
            "This happened is not always very intuitive to understand, so I have a mechanical interpretation.",
            "Yes, thank you for clarifying that.",
            "It's true that this particular algorithm works for only linear SVM.",
            "On this figure I'm I'm illustrating the classical way in which we view these regularised optimizations.",
            "We think of them as minimizing a cost function, plus Lambda some regularizer.",
            "And in this figure here I'm showing two only two weights W1 and W2, and assume that you know the optimal solution without regularization is called W star.",
            "And assume that you have a quadratic.",
            "Objective like for instance what you would do in in a Ridge regression.",
            "In this case, as you go away from the optimal solution, you go up in tabloid.",
            "And the organizer can take can be thought of as a spring that pulls your solution to zero with the spring strength of Lambda.",
            "So this is the classical way people usually view regularization.",
            "Now you can.",
            "Can change the way you're thinking.",
            "Instead of thinking you know we want to minimize this primary objective, which is you know your origonal objective function, so you're only square function plus some regularizer.",
            "The primary objective is not the regularizer which is the in the case of the one norm, minimize the sum of the WS, or in the case of the so called 0 norm, when you minimize the number of features.",
            "This is not your primary objective plus one over Lambda.",
            "Your old primary objective, which is really your prediction error.",
            "You can represent it in the same way, except now the ball you know is the drives your weight to zero and what your spring is what?",
            "Pulls you to your solution.",
            "So why am I doing this crazy thing was just because it's easier than to understand what the one norm is.",
            "BM and the last, so does so the lasso is to the one or SVM what?",
            "Um?",
            "Basically, the lesson is the least square version of the one norm SVN.",
            "So the Ridge regression is the square version of the two norm SVN and the LASSO is the one norm SVN version.",
            "The one arm version of the regression case.",
            "I'm just showing that rather than the classification problem because for.",
            "The mechanical interpretations is easier to understand.",
            "So what you see in the case of the one norm is that again, these are two ways that we want and W2.",
            "And now.",
            "Or regularizer which was before 2 norm.",
            "So was kind of fabulously now it's a diamond.",
            "Because of the absolute value.",
            "And the the other part of the objective is the same as before.",
            "Is this spring that pulls you to the solution?",
            "So imagine now that you have you know the bead in a big box, right?",
            "And you have some force which is the spring here that pulls your bead.",
            "And if you you know you shake a little bit your box, what's going to happen is that your bid is going to roll somewhere to a solution pulled of course, by by your force.",
            "And where is it going to end up?",
            "Is it going to professional?",
            "You end up on on the face, or you know, on on an edge or in your corner where you easily see that easily slides into either an edge or into a corner.",
            "And this is what happens in this optimization with the one norm.",
            "What happens is that you end up in a solution that is on one of the axis.",
            "And most of our large fraction you know of the weights are going to be exactly 0 in that solution, so This is why when you perform this simple substitution of the two norm for the one norm, you're going to have some of the weights which are exactly 0, and this scheme performs feature selection indirectly and says been very popular recently.",
            "A lot of people just just choose that because.",
            "Yeah, I mean people like.",
            "Optimization problems that are, well formula."
        ],
        [
            "Um?",
            "No, let's go back to the 0 known problem minimizing the one or is kind of a surrogate to a problem.",
            "We see that because of this diamond shape objective, we are going to have some ways that are going to be pulled to 0, but our real objective was to actually minimize the number of non zero weights.",
            "And that's what's called the zero number.",
            "09 is nothing but the number of non zero weights.",
            "As we said before, it's a nasty objective to optimize so we can replace it by some more well behaved function.",
            "And Western collaborate have device to scheme to a minimize that objective.",
            "And they showed that it actually boils down to very simple algorithm.",
            "You start with some sigmas that are scaling factors and they are all equal to 1.",
            "Then you compute the solution of your SVN on the some data set.",
            "And then what you're going to do is, are you going to define scaling factors that are absolute values of your?",
            "Of your W star.",
            "And you re evaluate your scanning factor at each step by multiplying them by the W star.",
            "In absolute value.",
            "So it's very easy.",
            "What you do is that you're going to re scale your inputs at each iteration, but how important they are to the predictor using the absolute value of the weights.",
            "And you have some kind of an exponential decay of those least important features.",
            "What?",
            "Still in your eyes, yeah, but it can be generalized to the nonlinear is here.",
            "In a similar way than than what you do for recursive feature elimination.",
            "In that case, you don't choose the absolute value of the W stars, but you use scanning factors that correspond to the derivative of your cost function with respect to the variables.",
            "And this can be efficiently optimal, efficiently approximated if you assume that the support vectors don't change between two consecutive steps.",
            "That is, when you remove the feature that you're interested in.",
            "If you assume that this product is don't change, you can easily estimate that.",
            "But actually in the original papers, the authors only dealt with the.",
            "The linear SVM and this is, you know.",
            "Completely straightforward to implement and very efficient."
        ],
        [
            "Acid.",
            "So in summary, for embedded methods.",
            "I'm betting this is our good inspiration to design new feature selection algorithms for.",
            "You know your own algorithms.",
            "So for example, if you have you like we had in the audience some question, how can we do feature selection for nonstationary process or how can we do feature selection for this or that other algorithm?",
            "But the answer would be there you know try try these methods of scaling factors and try to optimize just getting factors in the process of learning this is this is 1 simple way of doing things.",
            "I'm better, misses are not too far from rapper techniques and they can be extended to multiclass introgression.",
            "But they have the advantage that they are faster than rapper techniques you don't have to go through as many trainings of classifiers or predictors."
        ],
        [
            "So now that we've talked in awhile, algorithms and a lot of technical things were going to go into more philosophical things.",
            "We're going to talk about the problem of causality."
        ],
        [
            "So far.",
            "I've just said that you know feature selection deals with removing features to improve all these degrade prediction of Y, regardless of how these features are wired."
        ],
        [
            "To the target.",
            "And I've given you these two examples as a justification why one should use multivariate feature selection and not just a univariate feature selection.",
            "Now I'm going to tell you that everything I told you so far was wrong.",
            "Um?",
            "And that actually is very dangerous to do multivariate feature selection.",
            "This is an example from real life data in which we had discovered one feature that was very.",
            "Discriminate by itself and then a second feature that helped the first feature, but by itself was not discriminate at all.",
            "This example is an example of mass spectrometry, in which all the points in the spectrum were used as a feature.",
            "And the the amplitude here is the value of that feature.",
            "So for example, at this position here we have overlaid all the Spectra of one class, which are the right Spectra and all the spectrum of the other class.",
            "The green Spectra.",
            "So you can see that the examples of the red class are well separated from the examples of the great class using this particular feature X one.",
            "This will be that feature in this scatter plot.",
            "And.",
            "There was another feature X2 that had been extracted by our feature selection algorithm corresponding to this value here and correspond to a feature which really does not separate well at all.",
            "The two classes, however, taken together, the two features had a much better power separation than X1 alone.",
            "Nothing a little bit deeper about what could be the meaning of these two features.",
            "We decided that only X one was really informative feature.",
            "Because this corresponds to the abundance of a protein in serum, and so there are some people who have, you know, more abundant protein than some other people.",
            "Whereas this position here doesn't really represent the abundance of a protein is just a value is just a protein that's equally non present in all the patients of the population.",
            "The only difference is that we observe here are just noise.",
            "And it just so happens that there must be a systematic source of noise that makes it that all the spectral simultaneously reason or goal will be down in this neighborhood here, and even though we had performed some baseline removal, there was some residual error.",
            "There was significant enough that it was useful to add this feature X2 that is a local estimation of the baseline here.",
            "So what I'm showing you actually here is a very very small piece of a very large.",
            "Spectrum."
        ],
        [
            "And so this is actually a close in the close neighborhood of this other feature.",
            "So from my point of view, having understood that feature X2 is not really a relevant feature, it is an indirect measure of some systematic error that we should have been removing by preprocessing.",
            "So I'm showing you that this is real that actually this was an example, but this is the real."
        ],
        [
            "The other way we had it.",
            "OK, so this is a similar problem.",
            "Can it shows up in the case of the ex or and we also have real data example in which we have situations like this in which in fact the the fact that we have separate clusters and this abrupt change.",
            "You know when one feature was selected separating the other one and then all of the sudden forgiven value of another feature it changes the other way around.",
            "Very often this is just the symptom of something that went wrong for example.",
            "There has been a.",
            "A shift in operator or or something bad happened to the process.",
            "There was a bad randomization of the samples and we're seeing some artifact here.",
            "So interpreted in terms of across all graph, this is what happens.",
            "WHI is the target in that case this would be or disease am an X one is the abundance of some you know component in blood.",
            "For example the abundance of a protein.",
            "So this is causes some changes in the abundant or protein.",
            "But simultaneously there is some.",
            "Some noise that also feeds into X1 some bad bad variable and so X one is the result of mixing these two elements here and so knowing X2 is actually useful to making better prediction on why then not knowing it.",
            "But this doesn't mean that X2 tells us something about.",
            "You know how, the, how.",
            "Why is related to X1.",
            "So for example in OK.",
            "So if you know of measuring instrument if I would change instrument if I would have now an instrument which is calibrating in a different way X one X2 will become a completely useless variable, whereas you know X1.",
            "If it has a reality in the system that we are interested in is the one that is relevant or important?",
            "What happens is that the reason why it's two becomes relevant is that for any particular value of X1, then there is a dependency which is induced by X1 between X2 and Y.",
            "So X2 is independent of Y, but given values of X1 it becomes dependent.",
            "The reason being that X2.",
            "Feeds information into X1.",
            "Is the same thing happens here in the case of the chess board problem for any particular value of X one, we see that there is a separation on the X2 axis between the two categories, so there is an induced dependency.",
            "In reality it's not really X2 that causes X one.",
            "It's probably more complicated than that.",
            "There may be a common cause, which is your systematic noise, so there is noise that causes change in the baseline.",
            "An result you know in some observed value of the spectrum that we call X1, and some observed value that we call X2.",
            "Practically, we can't differentiate between these two graphs.",
            "Just looking at the data, but it's important to keep that in mind in order to interpret what we were looking at.",
            "And it's important to understand that as soon as we have measurements.",
            "Measurements are always consequences of the phenomena that we want to observe.",
            "And we are not immune to measurement errors and we need to take that into account when we do feature selection."
        ],
        [
            "So I just if I somehow the.",
            "Discovery of causal relationships between XYXI&Y.",
            "So in essence, if you ignore them, you might make some bad decision about which features are important for your problem.",
            "And so."
        ],
        [
            "So in the remainder of the talk, I'm going to briefly tell you what are the tools to discover these causal relationships.",
            "Let me first define what I mean by causal feature relevance.",
            "Here is a rather complex graph that will that will not analyze completely, but I would like to give you a sense of all the things that can happen is relatively elaborate in some cases.",
            "Imagine that what you want to predict is lung cancer.",
            "And there are some variables that might be causing lung cancer, for example smoking or some genetic factor.",
            "And some variables that may be consequences of lung cancer like coughing or like having metastasis or some other symptoms that people sometimes call.",
            "You know, biomarkers.",
            "Now, there may be indirect causes like anxiety might be causing smoking and then ideal so consequences of causes that are called in the dragon.",
            "Confounding factors like other cancers.",
            "There may be hidden variables.",
            "For example, there may be variables like tar in lungs that are more direct causes than smoking to lung cancer, but unfortunately you don't have access to them.",
            "Then maybe also hidden confounders.",
            "So for example, imagine this being you know recurrent theme in the tobacco settlement problem in the United States.",
            "There has been by now you know a lot of health policies not to smoke in public places, and the restrictions on selling cigarettes and tobacco companies have been heavily complaining that there is no real evidence that smoking causes lung cancer that only correlation has been observed, and so far there hasn't been any real proof that there is no genetic factor that would both cause craving for smoking and lung cancer.",
            "So imagining that there is such a genetic factor, then this cool rule out the hypothesis that smoking causes lung cancer.",
            "Um?",
            "Then there are other interesting things happening.",
            "Imagine that you would like to predict lung cancer on the basis of coughing.",
            "Indeed, the coughing might be a predictor of lung cancer, but imagine that you have another cause of coughing.",
            "Like you know, allergy.",
            "If you don't know about allergy, then coughing becomes a very poor predictor of lung cancer.",
            "For example, if you are doing hay fever season and becomes virtually useless to a monitor, coughing to predict lung cancer.",
            "And you know, I've I've shown you other cases you know, is this story of the systematic noise.",
            "This is the example I gave you with the baseline problem.",
            "All in all, you have you know all these different cases here and people working on the causal feature discovery are playing this game of trying to find those variables that are closest to the target.",
            "In the sense that they shield the target from all the other variables and that's what people call the Markov blanket.",
            "The Markov blanket is the set of variables that are now in the blue shaded area.",
            "Such that when you consider these variables, then the variables outside the shaded area become independent of the target.",
            "Now, as you can see, this is not as simple as it may look over at least as the people working on Markov blanket may tell you, because if you have hidden variables or those may be the ones that are really in the Markov blanket.",
            "And in particular, in the case of systematic noise, the noise may be the thing that is really in the Markov blanket, but you don't know about it and you may be thinking that you know certain biomarker is relevant.",
            "For example, this certain position in my mass spectrum.",
            "You may think it's a it's relevant, but in reality it's not relevant.",
            "The noise was what's wrong with his wealth."
        ],
        [
            "Nevertheless, I think these tools, of course, on discovery, are very useful because they allow you to Orient some of the arcs instead of justice detecting correlations.",
            "You can detect causality.",
            "And that might become very important, particularly if you want to work.",
            "There are graphs between random variables where dependencies are represented by edges, and the graph allows you to compute various distributions between subsets of variables.",
            "What is given is.",
            "The probability of a given variable given its parents, and from that you can compute the joint probability of all the variables and all sorts of marginals that are, you know, using subsets of the variables.",
            "The edge direction in the original vision networks have no meaning, but in coastal vision networks they indicate."
        ],
        [
            "Causality.",
            "So here is an example.",
            "Of course all discovery algorithm, bias purpose and glymour I'm giving you this example, you know, not hoping that you will memorize it immediately, but to show you that it's not that complicated.",
            "Let's consider AB&C.",
            "Random variables that belong to a set X&VA subset of X.",
            "So initialize with a fully connected on oriented graph representing the causal relationships between the variables.",
            "Then find and oriented edges.",
            "By using a criterion that variable A shares a direct age with variable B.",
            "If and only if, no subsets of other variables, we can rental them conditionally independent, so this is what is written.",
            "A conditionally independent of B given V. So all these algorithms for detecting causal dependencies rely heavily upon tests of conditional independence between variables.",
            "And then Orient edges in circle colliders or triplets.",
            "So Collider is.",
            "Is it when two arrows?",
            "Points.",
            "To a given variable.",
            "So they collide onto C. So you are in ages in this Collider, using the criterion that if there is a direct edge between A&C.",
            "And between C&B, but not between A&B.",
            "Then you have this pattern of connection.",
            "If and only if there is no subset V containing C such that independent of the given given the OK.",
            "So just to say that you need to do another another test of conditional independence to determine this, and then there are some heuristic to further or."
        ],
        [
            "Edges.",
            "So.",
            "These algorithms are.",
            "Quite expensive computationally.",
            "An also statistically so they are very data hungry and take a lot of time, so usually people resort to simplifying them.",
            "And compromise, for example by.",
            "Abandoning the estimation of the full graph, but caring only about those links that are between the target variable and the other variables.",
            "Or abandoning the idea of fully oriented, orienting the graph, but orienting only a subset of the edges."
        ],
        [
            "So I will be talking to you about a prototypical algorithm for discovering the Markov blankets, which is the case in which you care only about a given target, and you want to find those variables that shield.",
            "The target from all the other variable."
        ],
        [
            "Is it?",
            "So what you do first is that you identify the parents and children."
        ],
        [
            "In the following way.",
            "That's the first iteration you add any given node, let's call it a second iteration.",
            "You add another node, let's call it B.",
            "And then at the third iteration you would be looking at conditional independence.",
            "So if a is conditionally independent of Y given B.",
            "Then remove a.",
            "Then you don't need A and keep only be."
        ],
        [
            "Cetera.",
            "So once you've done, you know identified parents and children in this way, you can identify parents of parents and children of children and parents of children.",
            "So the depth two relatives and the depth to relatives include the members of the Markov blanket and some additional variables that outside the Markov blanket.",
            "Perhaps a decision I should go back to this graph where I showed you the example first.",
            "To clarify why we need depth to relatives in the Markov blanket.",
            "Because I remember now that I didn't make that."
        ],
        [
            "So clear.",
            "If you remember this this graph here.",
            "Smoking would be apparent.",
            "Genetic factor one would be apparent.",
            "Coughing with your child mythicists would be a child, so these are direct relatives, parents and children.",
            "Now we have someone in direct relatives that are called spouses and those poses are important to make predictions because without them the children become much less productive like I explained.",
            "If you have only the variable coughing and you're doing hay fever season it becomes virtually useless because you can explain away.",
            "The cafe in by the allergy.",
            "So you need to know about allergy in order to have coughing be productive.",
            "So basically you need to know about the espouses in order for the children to inform you about the parents.",
            "In genetics, is a little bit more complicated than that, but a blue eyed children doesn't inform very well about the color of the eyes of his.",
            "You know, in that case it's more complicated than parents, but enhance sisters.",
            "You need to know something about the collateral relatives, so he's doing something about the family of the Father so that the eye color informs you about the family of the mother of the children.",
            "So this is the same idea.",
            "So we need the spouses in the Markov blanket.",
            "And you go back to where I was."
        ],
        [
            "So you're left with the problem of just eliminating the."
        ],
        [
            "Their relatives.",
            "And you do that again with some conditional independence criteria that I don't want to go into detail about.",
            "But basically it's a fairly simple process."
        ],
        [
            "Now before I conclude, people often ask me, well, we have now old.",
            "You know, these feature selection algorithm.",
            "So which one should I use, right?",
            "How should I proceed to select what's going to work best on my method?",
            "A lot of it has to do with the complexity and how much data you have in your data set and how many features you have."
        ],
        [
            "So I'd like to emphasize that.",
            "Feature selection is, as I said before, A2 level inference process.",
            "And if we simplify things, instead of doing a cross validation, we assume we have just a single validation set.",
            "Just to make things simpler to explain, we have now in these two level of inference and the first level of influence dealing with the amount of training data we have in order to prevent overfitting at the first level and the second level of inference we have to deal with the amount of validation data we have to prevent overfitting.",
            "Of the feature selection in that case.",
            "So this is something that perhaps is not emphasized enough in the literature of machine learning.",
            "You really need to care about both the size of the training set and the size of the validation set.",
            "In the algorithms that presented you, I've shown you that not algorithms are equal in terms of how many classifiers you are training each time you're training a classifier and testing, testing it using the validation set to assess its performance.",
            "You're using your validation set.",
            "As you may know when you do.",
            "When you're trying to infer the generalization performance on new unknown test data, if you are in the case where you select from a finite number of classifiers.",
            "The complexity of this process is proportional somehow to the log of the number of the number of the classifiers that you're testing.",
            "So it's either the log of the number of classifier texting or the square root of the log, depending on whether or not you have zero training error, but it's not really important.",
            "The important thing is that things scale with the logarithm of the number of things that you've tried with the number of classifiers that you've tried.",
            "And we send message, you try fewer classifiers than others.",
            "So if you try a exhaustive search wrapper.",
            "You tried two to the N classifiers and every time you use your validation set right to assess its performance.",
            "So you incur complexity of N and being the number of features.",
            "Now, if you do another method like a greedy method.",
            "So you remember that you do for selection of or backward elimination in the represents first time you have any features to select from.",
            "The second time you have N -- 1 to select from etc.",
            "Then in total you have N + 1 / 2 classifiers that your training and testing in order to assess which one is best.",
            "And you also may remember that I said, well, we can do also the embedded version of forward selection and backward elimination, in which case we are guided by the search and each time we try, we add or remove a feature.",
            "We don't try many, we just try one.",
            "So in that case we test only North.",
            "Classifiers in fact, you know from the point of your complexity, it doesn't make a lot of sense, but it doesn't make a lot of difference because the computational complexity, the statistical complexity you incur, is of the same order of magnitude.",
            "But from you know the computational point of view.",
            "This might be an advantage, and still you know it's maybe a little bit of an advantage from the statistical point of view.",
            "Also.",
            "So all we know.",
            "OK, this people know about this, you know, tradeoff between.",
            "Perhaps I should tell you that anyways.",
            "This access this is the number of feature on this.",
            "I said this is the error rate.",
            "As you increase the number of features, the training error usually decreases monotonically.",
            "But because of this complexity term here.",
            "You have a bound which is similar to the bounds that you have usually.",
            "No, the VC bounds another balance, except that because we are now looking at the second level of influence, not just training the classifier but selecting the features.",
            "Here.",
            "What goes into the bound now is the validation error, not the training error plus some complexity term.",
            "Which is simple.",
            "It doesn't involve you know VC dimension or anything, it just involves the log of the number of classifiers that you've trained.",
            "And so the complexity term if you add it to the validation error, you get this bound on the generalization error.",
            "And now you see why in some cases you will have a curve that will decrease and not really go through an optimum, but smoothly, you know attend the centre or in that in some cases you might get a sharp optimum in your generalization error.",
            "It all depends, you know, on the complexity of your second level of inference and what you're going to get here into that bound here, which I'm representing as a simple line.",
            "And here I'm representing as a curve in another case."
        ],
        [
            "So what happens is that.",
            "You you can get, you know.",
            "A whole range of difficulty in the methods.",
            "On one axis here in this table I'm representing the complexity of the first level of inference.",
            "And that is when you move from a linear predictor to a non linear predictor.",
            "Then people know that you increase.",
            "You know the complexity or the capacity of your classifier.",
            "And in order to avoid overfitting, you should just increase the size of your of your training set.",
            "So in order to keep things under control at the first level of inference, you should make sure that your number of training examples is commensurate to your complexity of your first level of inference, but not from the point of view of feature selection.",
            "You also have the second level of inference in which you need to know.",
            "In which need to be aware of the fact that your number of validation examples is going to be critical to avoid overfitting the second level of inference, and you can put you in this table various.",
            "Feature selection strategies.",
            "Arranging from using inner upper way linear or nonlinear methods.",
            "And in this other direction, you could be univariate or multivariate, so increasing the complexity of the feature selection process.",
            "So I went through the exercise of putting some message in either each of these squares here, but this just gives you intuitively the idea of that you can.",
            "Increase the complexity of your feature selection process.",
            "If you're rich in data, but you need to be very careful if you have little data to train."
        ],
        [
            "Waste.",
            "Um?",
            "Now we have put together a package that's called the change learning object package that's based on Matlab, and he's built on top of the spiders package.",
            "It has some very simple abstractions.",
            "Daytime model object.",
            "From which you know you can build more complex objects and it includes many feature selection algorithms.",
            "So among the ones that have been most successful in challenges that we've organized, so you can."
        ],
        [
            "Download that package and use that and it's been very successful in reproducing the results of the NIPS 2003 feature selection Challenge.",
            "I've used it in the class that I taught, and the students could.",
            "Either, you know, outperform more closely match the performance of the challenge.",
            "I give them some baseline methods that are the black, you know methods.",
            "Here you have the balanced error rate they obtain on the five datasets of the challenge.",
            "And this is the challenge best.",
            "And here you have the error bar and so the student best results always.",
            "He thought you know better or not significantly worse than the best results in the challenge.",
            "So we have this resource for doing feature selection.",
            "An interesting Lee, the methods that work best are all very simple filter methods here, so this is encouraging for practitioners and may be disappointing.",
            "For theoreticians that still have some more work to do to make complex methods work better.",
            "And one of the directions of research I think, which is going to be very promising and important, is particularly these causal feature discovery.",
            "Finding to what extent we can alleviate the problem of multivariate feature selection, which tends to select not only good features but also."
        ],
        [
            "So artifacts.",
            "And so in conclusion, feature selection focuses on uncovering subsets of variables that are predictive of the target and multi very feature selections.",
            "In principle more powerful than universe feature selection.",
            "And there are some good, you know theoretical arguments why it should be more powerful, but it's not always more powerful in practice, both for reasons of statistical complexity like I've pointed out, we have now a second level of inference we have to pay the price for getting good subsets of features, and this surprising number of examples in our validation set.",
            "And of course, the more examples in a validation set, the fewer examples in training set.",
            "So do we want to pay that price rather than just using all our training data to train our classifiers and not performing feature selection?",
            "Taking a closer look at the type of dependencies in terms of causal relationship may help us refine the notion of variable relevance and distinguish between artifacts and truly relevant."
        ],
        [
            "Rebels and here you have two resources you can use in your research or in your applications.",
            "One of them is this book on feature extraction where we compiled all the results of the feature selection challenge and it has a number of tutorial chapters explaining notions of statistical testing applied to feature selection and filter wrappers and embedded methods, and a chapter that we recently wrote on the causal feature selection.",
            "Thank you very much for your patience or your attention.",
            "I'm not an expert in machine learning, so this question is not only for you, but also for the audience.",
            "Anne.",
            "When using this SVM linear multivariate method for filtering features, I wonder whether we could use for somebody in a multi class classification problem.",
            "Take all the all the weights of the fear of the different classified of the infamous VM machines and maybe perform some spectral analysis to to get which are the best features for the whole of the classes.",
            "Yes, this has been done the.",
            "Message data mentioned have been generalized to the multi class SVM.",
            "You can look at papers by Western and collaborator.",
            "Thank you M. Can you tell me a few words how is feature selection in your view related to PCA principal component analysis?",
            "It is very much related.",
            "You can, you know.",
            "If you remember the.",
            "A prototype of the mentioning of minimizing the two norm SVM.",
            "Then you can move to minimizing the one norm SVM, then to the zero norm SVN.",
            "Um?",
            "You have a continuum between between the problems.",
            "It can be shown that Ridge regression, which is minimizing the.",
            "The square loss with the regularizer, which is the two norm.",
            "Is in fact doing something very close to a principal component analysis.",
            "It is doing a soft selection of the directions which are correspond to the largest eigenvalues.",
            "And so you can move continuously from from that by changing the regularizer to the problem of feature selection, in which you're eliminating features along the axis rather than along the principle directions.",
            "Wait, my question is about the coastal feature selection.",
            "Hi, I'm not an expert but I I had come to the I. I know the problem of Markov blanket.",
            "I think when we built the Markov blanket we use a lot of domain specific knowledge.",
            "In order to check whether two variables are conditionally independent.",
            "With that I can I see that when we do the causal feature selection, we just put all these domain specific knowledge into the feature selection.",
            "Is there a question or inventor or not?",
            "They also automized.",
            "Automate automized this process.",
            "I mean we can always build this domain specific knowledge into the feature selection process, but is it really possible to?",
            "I don't have an answer to this question and actually we are presently designing a challenge of causal feature selection, then will hopefully start in the next few months and I would like to be able to answer that question.",
            "So we're formulating a number of problems that we will submit to people.",
            "And hopefully they will have the answer to that after the challenge and how people addressed this and whether.",
            "I have another question.",
            "You are in the opposite sense.",
            "Do you know any work using feature selection techniques to build a Markov blanket?",
            "Well, there's the work of Constantin Aliferis, and he's been constantly crossing the border between the two domains, and he has devised techniques that benefit from feature selection and vice versa.",
            "Personally, until you know I see the results of the challenge, I won't be convinced that the one thing works better than another, but I can see from the you know what I've been discussing in this presentation that if we can do it then it will be beneficial, but it's unclear whether whether this can be done effectively, you know, and for how much and how costly this will be in terms of number of examples as we've seen.",
            "Before in the first feature selection challenge, it is possible to get very good performance without feature selection.",
            "So now, how much do we gain, you know?",
            "And what do we gain with causal discovery is something we'd like to to answer.",
            "Is there a relation between causal feature selection and constructability of analysis of George Clear?",
            "London.",
            "I'm not familiar with that.",
            "I think we need to talk about it offline.",
            "How can you comment something about the uniqueness of your final set of features?",
            "Sometimes I've been using, for example, a member of backward selection of formal selection, and at any moment when you have to select your features, your criterion can give exactly the same value.",
            "Very similar.",
            "So can you.",
            "Can you comment on that?",
            "In a multivariate feature selection, usually there is no uniqueness of the solution.",
            "In fact, in some cases when there is a lot of redundancy in data, the there are many many feature subsets that give you equivalent.",
            "Results.",
            "If you go now to the Markov blanket, specially so Marco Market will claim that there is a unique Markov blanket under some conditions.",
            "Unfortunately, the conditions are usually not met and so there's no real uniqueness, but practically people have been doing some bootstraps and data in order to assess the stability of the features, and as it turns out practically this.",
            "So this gives good results even though you know then when you do bootstraps and you select those features that are most stable, they usually are not anymore part of 1 complementary subset of features.",
            "I would consider that only a Horace tick, but it's it's a good one."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this presentation is of a tutorial nature.",
                    "label": 0
                },
                {
                    "sent": "I usually like to have people interrupt with questions, so please don't hesitate if there is something you would like to ask.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Add.",
                    "label": 0
                },
                {
                    "sent": "Book.",
                    "label": 0
                },
                {
                    "sent": "So this is the way I define a feature selection.",
                    "label": 0
                },
                {
                    "sent": "Imagine that you have a big data table.",
                    "label": 0
                },
                {
                    "sent": "The lines are patterns and the columns are features.",
                    "label": 0
                },
                {
                    "sent": "And you may have thousands to millions of low level features in a lot of recent applications.",
                    "label": 1
                },
                {
                    "sent": "Day to present themselves in this way.",
                    "label": 0
                },
                {
                    "sent": "And you would like to select the most relevant features in order to build better, faster or easier to understand learning machines, which in essence boils down to selecting a number of columns of your data matrix.",
                    "label": 1
                },
                {
                    "sent": "So let me give you first a couple of examples of applications of this problem.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I've been working a lot in medical data analysis and this first example is taken from a leukemia diagnosis problem.",
                    "label": 1
                },
                {
                    "sent": "In which leukemia patients have been had had their blood drawn and from their serum.",
                    "label": 0
                },
                {
                    "sent": "When has assessed the expression coefficients of their genes and and this is done with the so called DNA microarrays.",
                    "label": 0
                },
                {
                    "sent": "So in this slide each little box here represents the.",
                    "label": 0
                },
                {
                    "sent": "Level of expression of a given gene.",
                    "label": 0
                },
                {
                    "sent": "If the gene is under expressed compared to normal, it's blue, and otherwise if it's red, it's over expressed them.",
                    "label": 0
                },
                {
                    "sent": "And as I said before, each line represents a patient in each column represents a feature.",
                    "label": 0
                },
                {
                    "sent": "In that case of Gina, there are some special columns that are to the right and the left, and those represent the target.",
                    "label": 0
                },
                {
                    "sent": "There are two subpopulations in the data.",
                    "label": 0
                },
                {
                    "sent": "The AML patients in the AML patients are two different kinds of leukemia.",
                    "label": 0
                },
                {
                    "sent": "And those are represented by here.",
                    "label": 0
                },
                {
                    "sent": "The red bar and here the blue bar.",
                    "label": 0
                },
                {
                    "sent": "So one very simple way of selecting feature is to looking for those columns in the data set which are most correlated with the target or which are most anti correlated with the target.",
                    "label": 0
                },
                {
                    "sent": "So here we have the opposite of the target.",
                    "label": 0
                },
                {
                    "sent": "We usually use us target values plus or minus one, so we would have all plus ones for the LL an all minus ones for the AML.",
                    "label": 0
                },
                {
                    "sent": "So we call.",
                    "label": 0
                },
                {
                    "sent": "In that case the positive class DLL class.",
                    "label": 0
                },
                {
                    "sent": "And so this is what people in this early paper in 1999 had done.",
                    "label": 0
                },
                {
                    "sent": "The they looked for those jeans that were either correlated or uncorrelated with or anti correlated with the target.",
                    "label": 0
                },
                {
                    "sent": "You don't need to limit yourself to two classification problems.",
                    "label": 0
                },
                {
                    "sent": "You can do multi class.",
                    "label": 0
                },
                {
                    "sent": "You can do regression and variety of other cases, But the problem always boils down to finding a subset of the original variables that are most predict.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In this other example you see another representation of the data.",
                    "label": 0
                },
                {
                    "sent": "Instead of showing the data matrix like in the previous slide, we show 3 features and we have a scatter plots of the examples in this 3 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So imagine that you've run a feature selection algorithm and you found those three informative features an imagine that you had three classes.",
                    "label": 0
                },
                {
                    "sent": "So in that case it's a prostate cancer problem and there are three populations of tissues.",
                    "label": 1
                },
                {
                    "sent": "Tissues that correspond to be 9.",
                    "label": 0
                },
                {
                    "sent": "Prostate hyperplasia, BPH and two different grades of cancer.",
                    "label": 0
                },
                {
                    "sent": "Grade 3 and grade 4.",
                    "label": 0
                },
                {
                    "sent": "And, um.",
                    "label": 0
                },
                {
                    "sent": "Here are represented by these... We showed the three clusters forming by the three populations in these 3 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So feature selection can be both used for improving prediction.",
                    "label": 0
                },
                {
                    "sent": "Or for facilitating like here.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Visualization of data.",
                    "label": 0
                },
                {
                    "sent": "Now I'm going to show you a couple of other examples of feature selection to illustrate that feature selection does not always yield performance improvement.",
                    "label": 0
                },
                {
                    "sent": "In this paper, the authors tried to perform cancer diagnosis also from DNA microarray data and they varied the number of genes and here they show accuracy of classification for a problem of classifying 14 trimmers.",
                    "label": 0
                },
                {
                    "sent": "And then they tried a number of different classifiers without entering into details you have different curves here corresponding to different strategies that they adopted, and you see that in some cases there is an optimum so.",
                    "label": 0
                },
                {
                    "sent": "Accuracy increases as you increase the number of features and then decreases again in other cases, even authentically.",
                    "label": 0
                },
                {
                    "sent": "Basically not only increases so the more features the better.",
                    "label": 0
                },
                {
                    "sent": "So as it turns out, nowadays there are some very some predictors that are very robust against overfitting.",
                    "label": 0
                },
                {
                    "sent": "In the old days, people used to say that there is this curse of dimensionality and that it is extremely important to do space dimensionality reduction in order to get good performance.",
                    "label": 0
                },
                {
                    "sent": "When you have a very few training examples.",
                    "label": 0
                },
                {
                    "sent": "Nowadays this is no longer true.",
                    "label": 0
                },
                {
                    "sent": "You can work in very very high dimensional spaces and if you have a highly regularised classifiers, you can get very good performance without performing first space, dimensionality reduction and this is illustrated by this curve here.",
                    "label": 0
                },
                {
                    "sent": "The more features, the better the performance gets in a way if you throw away some features you throw away some information because you're not sure that you're going to be eliminating the bad ones.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "However, for some problems you do see a very clear optimum.",
                    "label": 0
                },
                {
                    "sent": "So for some problems in which you have a large number of features that are irrelevant, then filtering them out.",
                    "label": 0
                },
                {
                    "sent": "Does improve performance and as you can see here in this problem of drug screening in the in which there were.",
                    "label": 0
                },
                {
                    "sent": "Around 2500 compounds that were tested for their ability to bind to a target site.",
                    "label": 1
                },
                {
                    "sent": "The the purpose was to determine whether a molecule was going to be active or inactive and there were over 100,000 binary features describing the properties of the three dimensional molecule.",
                    "label": 0
                },
                {
                    "sent": "Most of these features being completely useless and in fact as you can see as a function of success rate the optimal number of feature is between 20 and 40 among 100,000.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's another example in text filtering, so feature selection has been used a lot in the biomedical domain, but also in text processing, and the authors of that paper used various known copper, including the Reuters.",
                    "label": 0
                },
                {
                    "sent": "The 20 newsgroup webpage database, and for all of these they use a simple bag of word representation with about 100,000 features.",
                    "label": 0
                },
                {
                    "sent": "And again we see on this particular data types of data we have an increase in performance as a number of features.",
                    "label": 0
                },
                {
                    "sent": "So the best we can expect in terms of performance is to not degrade too much performance or not significantly degrade while using a very small number of features, and that alone can be a big win, both computationally and also from the point of view of data.",
                    "label": 0
                },
                {
                    "sent": "Understanding in this case here I'm showing as an example for the 20 newsgroup, several examples of features that have been identified as very predictive of this particular newsgroup.",
                    "label": 0
                },
                {
                    "sent": "Like for all the titanium.",
                    "label": 0
                },
                {
                    "sent": "Not surprisingly, you have at Asian at least some morality.",
                    "label": 0
                },
                {
                    "sent": "More Interestingly, perhaps for different newsgroups that deal with religion.",
                    "label": 0
                },
                {
                    "sent": "If you don't necessarily have the same exact same keywords that are interesting.",
                    "label": 0
                },
                {
                    "sent": "So you have got church and sing for subduct religion that Christian but you have Jesus God and Jehovah for Thunder tradition that makes so whatever the details are that can be of interest.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now other applications have been tried like face recognition.",
                    "label": 1
                },
                {
                    "sent": "So in this application it is interesting to see that people have merely used the pixels.",
                    "label": 0
                },
                {
                    "sent": "They haven't done any fancy feature extraction.",
                    "label": 0
                },
                {
                    "sent": "And they've tried to find which part of the image is informative for classifying male versus female, and they tried two types of feature selection algorithm.",
                    "label": 0
                },
                {
                    "sent": "One called relief.",
                    "label": 0
                },
                {
                    "sent": "It's a filter that does a total ordering of the features and doesn't attempt to remove redundancy between features and the other ones seem bad that the device does take into account correlations between features and tries to find those features that are most complementary with one another.",
                    "label": 0
                },
                {
                    "sent": "So here you see the result of using 100 features 500 and 1000 top ranking features.",
                    "label": 0
                },
                {
                    "sent": "For the first algorithm that does not remove redundancy, you have kind of symmetry between the two sides of the feature, so features that are redundant because for example they represent the two eyes they are being selected, whereas with the second algorithm that removes redundancy, only half of the features corresponding on one side of the image are being selected.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I just presented, you know, to give you a rough idea, some examples of feature selection and before moving into the meat of the talk, I'd like to give you some nomenclature.",
                    "label": 0
                },
                {
                    "sent": "I'll be talking about univariate methods as those methods that consider one variable or one feature at a time.",
                    "label": 1
                },
                {
                    "sent": "By contrast, multivariate methods consider subsets of variables of our features that together are good for making predictions.",
                    "label": 0
                },
                {
                    "sent": "Filter methods, rank features, or feature subsets independently of the predictor or the classifier.",
                    "label": 1
                },
                {
                    "sent": "I've been talking a lot about classification, but I'd like to remind you that this also holds for regression problems.",
                    "label": 1
                },
                {
                    "sent": "And the wrapper methods use rubber message user classifier to assess features or feature subsets and therefore determines usefulness of feature subsets with respect to a given.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Prediction method.",
                    "label": 0
                },
                {
                    "sent": "First, let me talk to you about the simplest methods.",
                    "label": 0
                },
                {
                    "sent": "The univariate filter methods.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The easiest is first to determine what feature irrelevances and by contrast will determine what feature relevance means.",
                    "label": 0
                },
                {
                    "sent": "Feature irrelevances, as it turns out, is well defined mathematically.",
                    "label": 0
                },
                {
                    "sent": "Imagine that we have a single variable XI.",
                    "label": 0
                },
                {
                    "sent": "And that we have a two class classification problem with the positive Class Y = 1, An negative class Y = -- 1.",
                    "label": 0
                },
                {
                    "sent": "An I'm representing here the density of the examples along these one axis XI.",
                    "label": 0
                },
                {
                    "sent": "So if that particular feature is irrelevant, which we expect to see is that there is going to be 2 identical blobs for the two classes.",
                    "label": 0
                },
                {
                    "sent": "You won't be able to distinguish the distribution of the examples of class one with respect to the example of class minus one.",
                    "label": 0
                },
                {
                    "sent": "Mathematically, this can be formalized as the joint probability of Zion Y is equal to probability of exact probability, of why?",
                    "label": 0
                },
                {
                    "sent": "So there is independence between zyan Y.",
                    "label": 0
                },
                {
                    "sent": "Or using base rule you can also say that P of X are given y = P of XI.",
                    "label": 0
                },
                {
                    "sent": "Or again you can say that P of XI given Y = 1 = P of XI give a Y = -- 1.",
                    "label": 0
                },
                {
                    "sent": "So the distribution, the marginal distribution P of XI is the same thing as pure vexi condition on why call 1 Ann is also same thing as pure vexi condition on white called minus one.",
                    "label": 0
                },
                {
                    "sent": "OK, so how do we define now relevance by compliance with irrelevance?",
                    "label": 0
                },
                {
                    "sent": "What we need to do is that we need to measure some displacement between these two distributions.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this has done has been done in a wide variety of ways.",
                    "label": 0
                },
                {
                    "sent": "There is only one way of defining irrelevance, but there are many, many ways of defining relevance.",
                    "label": 0
                },
                {
                    "sent": "And one of the simplest way is to use this simple parametric method and the parameterized distribution with just two parameters.",
                    "label": 0
                },
                {
                    "sent": "There are the mean and the standard deviation.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Define a criterion of relevance as the displacement between the means of the two distributions, normalized somehow by the standard deviation.",
                    "label": 0
                },
                {
                    "sent": "So, for example, normalized by the average of the standard deviations.",
                    "label": 0
                },
                {
                    "sent": "So this criteria has been known as the signal to noise ratio criterion in the community of DNA microarray analysis.",
                    "label": 0
                },
                {
                    "sent": "But, as it turns out, it's very similar to the person correlation coefficient.",
                    "label": 0
                },
                {
                    "sent": "At least in the case where you have an equal number of examples of class one and Class 2.",
                    "label": 0
                },
                {
                    "sent": "In this case, if the target is plus or minus one.",
                    "label": 0
                },
                {
                    "sent": "If you do the dot product between variable XI and the target Y, you can easily see that when you multiply by the Y equal plus one of the elements of the variable and add them up, then you obtain something which is proportional to new plus.",
                    "label": 0
                },
                {
                    "sent": "An when you multiply by minus one and sum up, you obtain minus mu minus right.",
                    "label": 0
                },
                {
                    "sent": "So just doing this dot product is virtually identical.",
                    "label": 0
                },
                {
                    "sent": "If you have a number of example which is the same for the two classes.",
                    "label": 0
                },
                {
                    "sent": "By subtracting real plus from you minus.",
                    "label": 0
                },
                {
                    "sent": "In practice, what people have been doing is exactly that they've been first normalizing their features by the standard deviation.",
                    "label": 0
                },
                {
                    "sent": "And then computing the person correlation coefficient so they've been doing virtually the same thing as the people who do signal to noise ratio.",
                    "label": 0
                },
                {
                    "sent": "So measuring displacement between the two distributions.",
                    "label": 0
                },
                {
                    "sent": "So all of these methods I would qualify them as correlation methods and there are many different.",
                    "label": 0
                },
                {
                    "sent": "You know correlation coefficients that can be defined in order to assess the correlation or the dependence between the variable Y and.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Horrible XI.",
                    "label": 0
                },
                {
                    "sent": "Now I just corrected myself when I said, you know, I said correlation by by assessing dependence.",
                    "label": 0
                },
                {
                    "sent": "Because correlation in particular, the person correlation coefficient as this is linear dependency between Zion Y.",
                    "label": 0
                },
                {
                    "sent": "So not all dependencies are linear and.",
                    "label": 0
                },
                {
                    "sent": "One way of measuring and only their dependencies is to use the mutual information.",
                    "label": 0
                },
                {
                    "sent": "So you as you remember, I defined independence using the simple equation that P of X&Y equal P of XPLY.",
                    "label": 0
                },
                {
                    "sent": "And then in order to measure dependence, you could imagine to use some coefficient that would measure the discrepancy between these two distributions.",
                    "label": 0
                },
                {
                    "sent": "And this is exactly what mutual information does.",
                    "label": 0
                },
                {
                    "sent": "You first form the ratio of P of X&Y with respect to P of XP of Y.",
                    "label": 0
                },
                {
                    "sent": "You take the log and then you take the expected value of RP of X&Y and this also is the kullback Leiber divergent between P of X&Y&P of XP of Y, so it really measures the discrepancy between these two.",
                    "label": 0
                },
                {
                    "sent": "Distributions.",
                    "label": 0
                },
                {
                    "sent": "So you could say that you know why don't we always use mutual information to perform feature selection?",
                    "label": 0
                },
                {
                    "sent": "There are many reasons, and one of them is that mutual information is actually difficult to estimate.",
                    "label": 0
                },
                {
                    "sent": "And for those of you who are familiar with the problem of overfitting.",
                    "label": 0
                },
                {
                    "sent": "We can have a parallel here between the fact of using, you know simple linear model or training complex nonlinear model.",
                    "label": 0
                },
                {
                    "sent": "If you use a simple linear model, you less at risk of overfitting than using a complex nonlinear model an in a way, even though we are not here talking about wrapper methods in which we have a classifier to select features.",
                    "label": 0
                },
                {
                    "sent": "Filter methods use criteria that are similar to having using single variable as a predictor.",
                    "label": 0
                },
                {
                    "sent": "And if you use a single variable as a predictor using a linear relationship, you'll less at risk overfitting that if you use a nonlinear relationship and not in aeration.",
                    "label": 0
                },
                {
                    "sent": "Ships are harder to estimate from data.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Simply so.",
                    "label": 0
                },
                {
                    "sent": "This is a simple justification for not always using mutual information.",
                    "label": 0
                },
                {
                    "sent": "As it turns out, the simple person correlation coefficient often gives best results.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, there are many criteria I said for the for defining some kind of dependency between Zion Y and we have edited the book after our challenge we organized on feature selection in 2003.",
                    "label": 0
                },
                {
                    "sent": "In chapter three of that book, you have a whole list of criteria and the choice of the method that you're going to be using to rank your features with those criteria will depend on the nature of the variables and the target, whether they are binary, categorical or continuous.",
                    "label": 1
                },
                {
                    "sent": "The nature of the problem, whether you expect the dependencies between variable and target to be linear or nonlinear.",
                    "label": 0
                },
                {
                    "sent": "And the availability of data.",
                    "label": 0
                },
                {
                    "sent": "If you have smaller number of examples, you might want to use simpler criteria that are less at risk of overfitting.",
                    "label": 0
                },
                {
                    "sent": "Also depends on the noise you have in data.",
                    "label": 0
                },
                {
                    "sent": "And finally depends on the availability of tabulated statistics.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I've presented in the previous slides some simple examples of criteria to determine the displacement of 1 distribution with respect to the other.",
                    "label": 0
                },
                {
                    "sent": "No, you might have noticed that this signal to noise ratio that I presented is very similar to the T statistic.",
                    "label": 0
                },
                {
                    "sent": "So why not use the T statistic in the 1st place?",
                    "label": 0
                },
                {
                    "sent": "But it's a good question actually.",
                    "label": 0
                },
                {
                    "sent": "You probably might want to use it because since it's tabulated you could also run a statistical test with it.",
                    "label": 0
                },
                {
                    "sent": "The null hypothesis would be that the two distributions are identical, or more precisely in that case that there is no difference between the means of the two distributions.",
                    "label": 0
                },
                {
                    "sent": "You first have to estimate to assume that the variance is known and that is the same, though that the variance even, even though it is unknown, is the same for the two hour classes.",
                    "label": 0
                },
                {
                    "sent": "Then if the hypothesis is true, then the T statistics obeys the student distribution.",
                    "label": 0
                },
                {
                    "sent": "And then after that you can use the P value as an estimation for the false positive rate.",
                    "label": 0
                },
                {
                    "sent": "I'll go into more detail.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this in the next slide.",
                    "label": 0
                },
                {
                    "sent": "So in chapter two of the book, you'll have a whole number of statistical tests that you may want to use.",
                    "label": 0
                },
                {
                    "sent": "They all use the same principle.",
                    "label": 0
                },
                {
                    "sent": "You have to assume another distribution.",
                    "label": 0
                },
                {
                    "sent": "Which of this distribution of the garbage features those that are not relevant at all to your problem at hand?",
                    "label": 0
                },
                {
                    "sent": "And there is a tail of the distribution of these garbage features.",
                    "label": 0
                },
                {
                    "sent": "And if the features that you observe in your data are in the tail of the distribution of the garbage features there, you're going to, then you're going to say that with some confidence it's not possible that they belong to the null distribution, and therefore they should be relevant.",
                    "label": 0
                },
                {
                    "sent": "It's kind of a indirect way of estimating relevance.",
                    "label": 0
                },
                {
                    "sent": "So assume that you have defined the ranking criterion by which you are going to rank your features according to relevance, and I'm calling this criterion R for example.",
                    "label": 0
                },
                {
                    "sent": "It's the person correlation coefficient.",
                    "label": 0
                },
                {
                    "sent": "Or is that the statistics or whatever criterion you have defined.",
                    "label": 0
                },
                {
                    "sent": "And then assume that for some reason you know the distribution of this of this criterion for the garbage features.",
                    "label": 0
                },
                {
                    "sent": "For the ones that are not interesting, then you can set a threshold or zero that determines the tail of the distribution.",
                    "label": 0
                },
                {
                    "sent": "And then the area under that curve here is the fraction of.",
                    "label": 0
                },
                {
                    "sent": "Over garbage pictures that exceed that threshold.",
                    "label": 0
                },
                {
                    "sent": "R 0 so for example, if you are against threshold, you might have just 1% of the garbage features exceed the threshold and therefore you may be satisfied with the fact that if your feature of interest has a value of our which exceeds our O may be satisfied with the fact that you know that only you know a very small fraction of the garbage features can exceed that value, so this gives you confident that this is not a garbage feature.",
                    "label": 0
                },
                {
                    "sent": "I see some people found that clear enough.",
                    "label": 0
                },
                {
                    "sent": "So the null hypothesis is that X&Y are independent and you define a relevance index that we're going to identify to a test statistic.",
                    "label": 1
                },
                {
                    "sent": "Now the P value of these tests is going to be identified to a false positive rate that is the fraction of the number of false positive over the total number of irrelevant features.",
                    "label": 1
                },
                {
                    "sent": "So this is the distribution of irrelevant features and the false positive rate is this area.",
                    "label": 0
                },
                {
                    "sent": "Here is the fraction of features that I thought were relevant, but actually a garbage features.",
                    "label": 1
                },
                {
                    "sent": "Now this this looks all nice, but in practice you often have hundreds of thousands of features that you are interested in testing, not just one.",
                    "label": 0
                },
                {
                    "sent": "And then you have a problem of multiple testing.",
                    "label": 0
                },
                {
                    "sent": "That is, you're repeating this test many, many many times.",
                    "label": 0
                },
                {
                    "sent": "So then then the chances are that you're going to find quite a few features that fall in the in the tail of the distribution merely by the simple fact that you've repeated that test many time, and this multiple testing problem been known in statistics for a long time, and you need to correct for the P value in order to get some some estimate of the false positive rate.",
                    "label": 0
                },
                {
                    "sent": "If we repeat the test many times and a simple way of doing that is just multiplying the original P value by North.",
                    "label": 0
                },
                {
                    "sent": "So this corresponds either by moving R0 to keep your P value that you wanted at the beginning, or correcting the P value for a given value of R0 that you've chosen.",
                    "label": 0
                },
                {
                    "sent": "Now, an additional difficulty is that most of the time you do not know the null distribution.",
                    "label": 0
                },
                {
                    "sent": "Why should you know it in the 1st place, right?",
                    "label": 0
                },
                {
                    "sent": "Of course, you could have postulated unknown distribution and say for example, you know it's the T distribution, which is the one that I want to look at, but you've been making some hypothesis in order to assume this distribution.",
                    "label": 0
                },
                {
                    "sent": "Another way of doing that is to define an arbitrary criterion for relevant of relevance.",
                    "label": 0
                },
                {
                    "sent": "That is, maybe application dependent that you think is reflects well the way you want to rank your features in order to relevant in order of relevance, and then instead of defining your distribution, you're going to inject into your problem garbage features that resemble as much as possible the real features that you have.",
                    "label": 0
                },
                {
                    "sent": "So in order to do that, for example, one way of doing it is that you take your original data matrix and you take the contents of your original data matrix and you scramble the values.",
                    "label": 0
                },
                {
                    "sent": "Now you get columns in your new data matrix with columns that are absolutely not correlated with your target data.",
                    "label": 0
                },
                {
                    "sent": "They are not informative, but the distribution more or less looks like the original ones, and so you can rank those features together.",
                    "label": 0
                },
                {
                    "sent": "With the real ones and see what fraction of the garbage features that.",
                    "label": 0
                },
                {
                    "sent": "That you ranked end up to be in the top ranking ones.",
                    "label": 1
                },
                {
                    "sent": "And that allows you to define a false discovery rate.",
                    "label": 0
                },
                {
                    "sent": "Which is defined as the number of false positive over the number of selected candidates.",
                    "label": 0
                },
                {
                    "sent": "And that you can bound by this quantity here.",
                    "label": 0
                },
                {
                    "sent": "So by what we call the probe method, instead of calculating the P value using tabulated statistics, you estimate the false positive rate as the number of selected probes over the total number of probes.",
                    "label": 0
                },
                {
                    "sent": "So remember, this was the distribution of the garbage features that we had postulated.",
                    "label": 0
                },
                {
                    "sent": "Now we just inject garbage features that we have constructed by this randomization process.",
                    "label": 0
                },
                {
                    "sent": "And what we do is that we look at what fraction of the garbage features that we have generated exceed this threshold are zero, and this gives us or estimate of the false positive rate and without you entering into details you can show that this can be plugged into this and you can also get an estimate of the false discovery rate in this way.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there's much you know that could still be said about univariate methods, but I guess the one of the reasons why they've been used so widely is because of their simplicity and speed.",
                    "label": 0
                },
                {
                    "sent": "You can very quickly run a statistical tests on hundreds of thousands of features, rank, then estimate the false discovery rate, and then take, you know, the, say, the top.",
                    "label": 0
                },
                {
                    "sent": "So and so many features that have a false discovery rate less than 1% or so, and then with some confidence you can say that there is only 1% of garbage features in the features you've selected, so that's been while they use now.",
                    "label": 0
                },
                {
                    "sent": "This doesn't necessarily mean that when you've done that, you're going to get good performance on the predictor.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there are many reasons why there are limitations on the univariate methods and universe methods may fail.",
                    "label": 1
                },
                {
                    "sent": "On this little scatter plots I'm illustrating.",
                    "label": 0
                },
                {
                    "sent": "Why you might want to use multivariate methods?",
                    "label": 0
                },
                {
                    "sent": "And I'm considering only two variables X1 and X2 for example.",
                    "label": 0
                },
                {
                    "sent": "This is the age of the patient and this is the weight of a patient.",
                    "label": 0
                },
                {
                    "sent": "And there are two 2 distributions of the right patients and the grid patients.",
                    "label": 0
                },
                {
                    "sent": "And if you project on one of the features.",
                    "label": 0
                },
                {
                    "sent": "You can see what is the univariant power of separation of that feature.",
                    "label": 0
                },
                {
                    "sent": "So if you project on feature X one, you can see that the examples are reasonably well separated.",
                    "label": 0
                },
                {
                    "sent": "At least you can separate with less than 50% error, whereas if you project on feature X2 you have almost, you know exact overlap between the two distributions and therefore feature X2 by itself is not predictive.",
                    "label": 0
                },
                {
                    "sent": "So here is an example in which you have one feature feature X1, which is by itself predictive and one feature feature X2, which by itself is not predictive.",
                    "label": 0
                },
                {
                    "sent": "So any univariate method would tell you to throw away feature X2.",
                    "label": 0
                },
                {
                    "sent": "Yet you can see that in two dimensions you have an almost perfect separation of the two categories, whereas you cannot separate very well with this single feature X1.",
                    "label": 0
                },
                {
                    "sent": "So this example illustrates that a feature, even if it's by itself completely irrelevant, it can help another feature become more relevant so.",
                    "label": 0
                },
                {
                    "sent": "So in two dimensions, we might want to select both features in order to achieve better separation.",
                    "label": 0
                },
                {
                    "sent": "In the second example, this is an example of a nonlinear.",
                    "label": 0
                },
                {
                    "sent": "These were the classes are non linearly separable and it's well known in the machine learning that there is this nasty X or problem or chess board problem in which you can't find a linear separation and this is also a nasty problem for feature selection because when you project the examples and this time you can project them either on feature X one or and feature X2 you get no separation on either of the features.",
                    "label": 0
                },
                {
                    "sent": "And so in that case, if you use univariate feature selection, it's completely hopeless.",
                    "label": 0
                },
                {
                    "sent": "You need to look in two dimensions in order to get any kind of separation, but.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Into categories.",
                    "label": 0
                },
                {
                    "sent": "So how do you solve these problems?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Before in in when I talked about univariate methods I I mentioned in the title is univariate filter methods.",
                    "label": 0
                },
                {
                    "sent": "Not all filter methods are univariate.",
                    "label": 0
                },
                {
                    "sent": "There are some filter methods that take into account the context of other features.",
                    "label": 0
                },
                {
                    "sent": "And they allow you eventually to do a total ranking of the features, but they inform you about the context of other features and will show you an example of that.",
                    "label": 0
                },
                {
                    "sent": "Most commonly, people use reper methods for multivariate feature selection, and those consistent producing a large number of subsets of features, sending them to a predictor in order to access their prediction performance and then.",
                    "label": 0
                },
                {
                    "sent": "Iterating, guiding the search in some way, producing more feature subsets and testing them again.",
                    "label": 1
                },
                {
                    "sent": "There also so called embedded methods.",
                    "label": 1
                },
                {
                    "sent": "In which simultaneously the algorithm produces an optimum feature subset.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A predictor?",
                    "label": 0
                },
                {
                    "sent": "So the first example I'm giving you is an example of.",
                    "label": 0
                },
                {
                    "sent": "Ranking method.",
                    "label": 0
                },
                {
                    "sent": "In which the ranking criterion uses the context of other features to produce a total ranking of the features.",
                    "label": 0
                },
                {
                    "sent": "So assume that you have two features, again X1 and X2, and you recognize you know it's or chess board problem.",
                    "label": 0
                },
                {
                    "sent": "So if you project in on any of the two dimensions X1 or X2, you don't get good separation and therefore it's very difficult to do feature to feature ranking in that case in a univariate way.",
                    "label": 0
                },
                {
                    "sent": "But Karen Randall proposed a simple scheme that allows you to solve that problem, and it has the flavor of the nearest neighbor algorithm.",
                    "label": 0
                },
                {
                    "sent": "For any different example.",
                    "label": 0
                },
                {
                    "sent": "You're looking for its nearest hit and its nearest miss.",
                    "label": 1
                },
                {
                    "sent": "The nearest hit is the nearest example of the same class and their nearest miss is the nearest example of the opposite class.",
                    "label": 0
                },
                {
                    "sent": "And you do that this, you know, search for nearest hidden nearest Smiths in the origonal space.",
                    "label": 0
                },
                {
                    "sent": "So taking into account all the features.",
                    "label": 0
                },
                {
                    "sent": "So that's the trick.",
                    "label": 0
                },
                {
                    "sent": "That's how you get information about the context.",
                    "label": 0
                },
                {
                    "sent": "But next what you do is that you project.",
                    "label": 0
                },
                {
                    "sent": "On the various features.",
                    "label": 0
                },
                {
                    "sent": "So for example, you project on feature X1.",
                    "label": 0
                },
                {
                    "sent": "And then you look at the distance to the nearest hit and the distance to the nearest miss in projection.",
                    "label": 0
                },
                {
                    "sent": "On that particular feature.",
                    "label": 0
                },
                {
                    "sent": "And in that projection, you compute the ratio of nearest MTR over nearest hit.",
                    "label": 0
                },
                {
                    "sent": "So if you have, you know a large distance on average over all the examples between nearest messengers hit, then the feature is informative with respect to the separation between the two classes.",
                    "label": 0
                },
                {
                    "sent": "So the way this this this works is in essence, it's like you have been, you know, slicing the space in a way right?",
                    "label": 0
                },
                {
                    "sent": "And you're looking at a local neighborhood, and in a neighborhood of.",
                    "label": 0
                },
                {
                    "sent": "Of some of the samples, if you see that there is a possible separation between the two classes, then the feature is informative.",
                    "label": 0
                },
                {
                    "sent": "Even though you are projecting afterwards, so this also has the flavor of conditioning your conditioning on some neighborhood.",
                    "label": 0
                },
                {
                    "sent": "By using this nearest neighbor trick.",
                    "label": 0
                },
                {
                    "sent": "And if you know condition on some neighborhood of the examples you observe that there is a separation between the two classes, then this this particular feature is informative.",
                    "label": 0
                },
                {
                    "sent": "So this criterion is largely heuristic, but it can be theoretically justified.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But more commonly, people doing multivariate feature selection will not use filters.",
                    "label": 0
                },
                {
                    "sent": "They will use wrappers.",
                    "label": 0
                },
                {
                    "sent": "And for rappers, what you do is that we explore the space of all possible combinations of features, but that's a huge space because for any features there are two to the end possible feature subsets.",
                    "label": 0
                },
                {
                    "sent": "So both computationally and statistically the problem is very complex.",
                    "label": 0
                },
                {
                    "sent": "Which has led people to finding.",
                    "label": 0
                },
                {
                    "sent": "Search strategies that are.",
                    "label": 0
                },
                {
                    "sent": "Suboptimal in some ways, but efficient in other ways.",
                    "label": 0
                },
                {
                    "sent": "So in this graph here, I'm representing all the possible.",
                    "label": 0
                },
                {
                    "sent": "Feature subsets for only four features.",
                    "label": 1
                },
                {
                    "sent": "And one means that the feature is selected and a 0 means the feature is not selected.",
                    "label": 0
                },
                {
                    "sent": "And the errors here mean that you're walking through that space by either adding or removing one feature.",
                    "label": 0
                },
                {
                    "sent": "So we'll see algorithms on how to move into that space.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Efficiently.",
                    "label": 0
                },
                {
                    "sent": "There are many such strategies that have been proposed, and some of them are listed in the chapter four of our book.",
                    "label": 0
                },
                {
                    "sent": "They include exhaustive search, which essentially nobody recommends unless you have a large number of examples and few features.",
                    "label": 0
                },
                {
                    "sent": "Other rather exhaustive methods include the simulated dynamic and genetic algorithm.",
                    "label": 0
                },
                {
                    "sent": "Also very prone to overfitting and not recommendable unless you have a lot of data.",
                    "label": 0
                },
                {
                    "sent": "Bing search methods, greedy search forward and backward selection, which I will go into more details later.",
                    "label": 1
                },
                {
                    "sent": "And then some hybrid methods that allow you to move forward in this space and then a little bit backward, and then I will forward and those include these plus plus L. And take away our and their floating Sir.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ouch.",
                    "label": 0
                },
                {
                    "sent": "So once you've selected so strategy, you also need to in order to have a complete algorithm to select a criterion by which you're going to decide whether your new state is better than the previous one, or better than another state that you've explored already.",
                    "label": 0
                },
                {
                    "sent": "Typically what people do is that they use cross validation and most of you are familiar.",
                    "label": 0
                },
                {
                    "sent": "Of course with cross validation there is the small twist here that we're going to split our data set in two or three subsets, one for training, one for validation and one for testing because we need to be careful not to mix up the feature selection process.",
                    "label": 0
                },
                {
                    "sent": "With testing, we have in essence now like what we have when we do model selection multi level inference.",
                    "label": 0
                },
                {
                    "sent": "Process which includes a for each feature subset, training and predictor on training data and then selecting the feature subset which performs best on validation data.",
                    "label": 1
                },
                {
                    "sent": "Eventually repeating, you know to reduce.",
                    "label": 0
                },
                {
                    "sent": "We just variance by performing cause validation on multiple splits between training and validation data.",
                    "label": 0
                },
                {
                    "sent": "But finally only test on test data and that should happen only once you have finished completely the both the training and the feature selection process.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm on this sub graph summarizing the three axis that those three ingredients of all feature selection methods.",
                    "label": 0
                },
                {
                    "sent": "In which include defining a strategy defining criterion by which you're going to assess how good your features are.",
                    "label": 0
                },
                {
                    "sent": "And choosing an assessment strategy so sometimes people are confused about why you know I'm separating these two axes.",
                    "label": 0
                },
                {
                    "sent": "Defining the criterion doesn't tell you how you're going to estimate it statistically.",
                    "label": 0
                },
                {
                    "sent": "And using cross validation is one way of assessing your criterion.",
                    "label": 0
                },
                {
                    "sent": "But there are other ways, including performance bounds, and I've talked about, you know, statistical tests and in that space you can basically determine whether you have a filter or wrapper or an embedded method.",
                    "label": 0
                },
                {
                    "sent": "So I've shaded here the area in which filters live an rappers live.",
                    "label": 0
                },
                {
                    "sent": "You know, in that kind of area and embedded method in this way, so there is not a single way of categorizing or feature selection.",
                    "label": 0
                },
                {
                    "sent": "Selection methods you can either.",
                    "label": 0
                },
                {
                    "sent": "Categorise them in terms of you know universe versus multivariate or filter wrapper.",
                    "label": 0
                },
                {
                    "sent": "Embedded or or categorize them by their source strategy or criterion or assessment method strategy.",
                    "label": 0
                },
                {
                    "sent": "And now I'll be going through a couple of the most popular algorithms and also the most effective ones to give you a feel for what should be.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Done.",
                    "label": 0
                },
                {
                    "sent": "1st, I'll be talking about forward selection strategies in the wrapper setting.",
                    "label": 0
                },
                {
                    "sent": "And it's also referred to as SFS or sequential forward selection.",
                    "label": 1
                },
                {
                    "sent": "You know my little example in which you have only four features.",
                    "label": 0
                },
                {
                    "sent": "We start with the state in which you have selected no features so 0000.",
                    "label": 0
                },
                {
                    "sent": "And then we can move by adding a single feature to one of the next states.",
                    "label": 0
                },
                {
                    "sent": "And we're going to be assessing how well we're doing into this next stage.",
                    "label": 0
                },
                {
                    "sent": "For example by training a predictor an assessing its performance by cross validation, but not necessarily.",
                    "label": 0
                },
                {
                    "sent": "You could also be having any criterion you want and just compute it on training data.",
                    "label": 0
                },
                {
                    "sent": "You don't need to do cross validation if you don't want to, or you could have a performance plan or whatever you want.",
                    "label": 0
                },
                {
                    "sent": "And then you decide which node performs best and you select that one and you move only from that node.",
                    "label": 0
                },
                {
                    "sent": "So it's a greedy search in which you know you don't look back.",
                    "label": 0
                },
                {
                    "sent": "You always move forward an at each step you consider all the possibilities, resulting for adding a single feature.",
                    "label": 0
                },
                {
                    "sent": "And so you have N possibilities at the first step, N -- 1 at the 2nd and minus two, etc.",
                    "label": 0
                },
                {
                    "sent": "I'm down.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "1.",
                    "label": 0
                },
                {
                    "sent": "You could do the same thing in the embedded method flavor.",
                    "label": 0
                },
                {
                    "sent": "In that case, your search will be even more narrow down now at each step, instead of considering all possible following features, you'll just consider only one, and the reason why you can afford this luxury is that your training algorithm is going to guide you through the search training algorithm is going to suggest to you which feature is going to be the good one to try next.",
                    "label": 0
                },
                {
                    "sent": "And the difference between the two methods is that for the embedded method you had, you know, and then N -- 1, then minus two trials.",
                    "label": 0
                },
                {
                    "sent": "Here you have only one at each step, and so total the total number of classifiers.",
                    "label": 0
                },
                {
                    "sent": "And if you choose to train classifiers, total number of classifiers you're trying is only N instead of N -- 1 / 2.",
                    "label": 0
                },
                {
                    "sent": "Sorry N + 1 / 2.",
                    "label": 0
                },
                {
                    "sent": "So it's a small difference but.",
                    "label": 0
                },
                {
                    "sent": "Computationally it can.",
                    "label": 0
                },
                {
                    "sent": "It can be important, and also because you're trying fewer classifier.",
                    "label": 0
                },
                {
                    "sent": "It simplifies your second level of insurance and I will see as we will see later.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is less prone to overfitting.",
                    "label": 0
                },
                {
                    "sent": "OK, now that we've illustrated forward selection, here is an example of algorithm for selection.",
                    "label": 0
                },
                {
                    "sent": "You can select the first feature DT.",
                    "label": 1
                },
                {
                    "sent": "That is, the has the largest cosign with the target.",
                    "label": 0
                },
                {
                    "sent": "And then proceed with the gram Schmidt orthogonal isation.",
                    "label": 0
                },
                {
                    "sent": "Then for each of the remaining feature you can project.",
                    "label": 1
                },
                {
                    "sent": "Project them on the null space of the features already selected, and then again compute the cosine in the.",
                    "label": 1
                },
                {
                    "sent": "India projected.",
                    "label": 0
                },
                {
                    "sent": "In the space of the projected features.",
                    "label": 0
                },
                {
                    "sent": "And you if you repeat that, you know it's relatively.",
                    "label": 0
                },
                {
                    "sent": "You get a subset of features that is optimum with respect to the least square predictor.",
                    "label": 1
                },
                {
                    "sent": "So in that sense it's an embedded method for forward selection.",
                    "label": 0
                },
                {
                    "sent": "It's a very effective one, and it tells a very compact.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Subsets of feature.",
                    "label": 0
                },
                {
                    "sent": "There are many other for selection methods and those of you who are familiar with the decision trees.",
                    "label": 0
                },
                {
                    "sent": "Decision trees perform in a way, some embedded feature selection, because at each step in the decision tree process.",
                    "label": 1
                },
                {
                    "sent": "D's three selects the algorithm, selects the feature that allows you to separate the data into subsets such that the purity of the subsets is enhanced, so there are more examples of class one in one of the subsets and more example of Class 2 in the other subset, and you repeat the process many times until you achieve maximum purity in the leaves.",
                    "label": 0
                },
                {
                    "sent": "So there is an embedded way of selecting features and usually at each step you use entropy and in order to as a feature selection criterion.",
                    "label": 0
                },
                {
                    "sent": "This is very similar.",
                    "label": 0
                },
                {
                    "sent": "In fact, what people do when they use a mutual information for feature selection as entropy and mutual information are very.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Related there is also another way of performing feature selection in a greedy manner, which is backward elimination.",
                    "label": 1
                },
                {
                    "sent": "In that case, you start with all the features and you progressively remove features 1 by 1.",
                    "label": 0
                },
                {
                    "sent": "And similarly as before, you can either perform that in in the wrapper way by trying all possible features that you would be removing.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the next step.",
                    "label": 0
                },
                {
                    "sent": "Or you can try it in the embedded way.",
                    "label": 0
                },
                {
                    "sent": "The difference being that the algorithm then it's going to guide your search is going to tell you which feature to try next.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one particular example of that is the backward elimination algorithm, RFE, that was designed for support vector machines, and it's very simple.",
                    "label": 0
                },
                {
                    "sent": "You start with all features, then you train the learning machine on the current subsets of features by minimizing a given risk functional, and then for each of the remaining features you estimate the you select the feature that.",
                    "label": 1
                },
                {
                    "sent": "It results in the least increase of the cost function or or the decrease of the construction.",
                    "label": 0
                },
                {
                    "sent": "So either you least degrade or you improve your your performance as measured by your cost function.",
                    "label": 0
                },
                {
                    "sent": "And then you remove that feature.",
                    "label": 0
                },
                {
                    "sent": "That improves all these degrade J and you start over again.",
                    "label": 0
                },
                {
                    "sent": "You train again with the remaining features and you iterate.",
                    "label": 1
                },
                {
                    "sent": "So this kind of embedded methods work for SVM, for kernel methods and.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Are all networks?",
                    "label": 0
                },
                {
                    "sent": "No, instead of abruptly removing features in these you know greedies or or adding features.",
                    "label": 0
                },
                {
                    "sent": "In this research methods.",
                    "label": 0
                },
                {
                    "sent": "What you can do for embedded methods is to use scaling factors and this opens you the door to a wide variety of algorithm for feature selections that are combined with training.",
                    "label": 0
                },
                {
                    "sent": "So before you know you know you know the space of feature subsets, we had a one for selecting the feature and a zero for not selecting it.",
                    "label": 0
                },
                {
                    "sent": "This corresponds to this vector of sigmas.",
                    "label": 0
                },
                {
                    "sent": "There is one for each feature.",
                    "label": 0
                },
                {
                    "sent": "One represents presence of the feature and zero absence of the feature.",
                    "label": 0
                },
                {
                    "sent": "So these are discrete indicators, so you can replace these discrete indicators by continuous scaling factors that are now between zero and one instead of belonging to just 01.",
                    "label": 1
                },
                {
                    "sent": "Sorry there is a typo here, so this is meant to be between zero and one.",
                    "label": 0
                },
                {
                    "sent": "So the advantage of doing that is that now we can perform gradient descent on the scaling factors.",
                    "label": 1
                },
                {
                    "sent": "Do we take a break in the middle summer?",
                    "label": 0
                },
                {
                    "sent": "So I think this would be a good time to break to make you reason and meditate on that and then the remaining part of the talk.",
                    "label": 0
                },
                {
                    "sent": "I'll be showing you how you can design your own algorithm for feature selection with your own training method using the scaling factors.",
                    "label": 0
                },
                {
                    "sent": "And then we'll be talking about causality.",
                    "label": 0
                },
                {
                    "sent": "So in general, if you only care to make predictions, you don't care to know whether the variables cause the target or consequences of their targets, so.",
                    "label": 0
                },
                {
                    "sent": "It's both predictive of disease.",
                    "label": 0
                },
                {
                    "sent": "To find genes that are.",
                    "label": 0
                },
                {
                    "sent": "That have mutated and are causing your disease, or to find the proteins that whose concentration varies in serum as a consequence of the disease.",
                    "label": 0
                },
                {
                    "sent": "Like for example, an immune reaction.",
                    "label": 0
                },
                {
                    "sent": "However, in some cases, as we will see when we care about performing actions on the system and seeing what is going to be the result of our actions, like for example delivering a drug to a patient, whether it's going to cure or not.",
                    "label": 0
                },
                {
                    "sent": "In that case we care about causality and whether the features are not causally related to the target.",
                    "label": 0
                },
                {
                    "sent": "So we'll talk to both, but both of these problems in the second part of the presentation, Now 5 minute break.",
                    "label": 0
                },
                {
                    "sent": "It's a welcome back and thank you for coming back and after the test of the first hour.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "First, I'd like you to take questions if you have some questions on the 1st part.",
                    "label": 0
                },
                {
                    "sent": "If they can some questions offline, but welcome to you know.",
                    "label": 0
                },
                {
                    "sent": "As someone here.",
                    "label": 0
                },
                {
                    "sent": "You said that you think controlling the future is in taking into account also the multiplicity of the feature.",
                    "label": 0
                },
                {
                    "sent": "So you correctly, properly.",
                    "label": 0
                },
                {
                    "sent": "But then you say that you then estimate the discovery rate.",
                    "label": 0
                },
                {
                    "sent": "So why don't you?",
                    "label": 0
                },
                {
                    "sent": "We just optimize in thinking for coming to each day of the first discovery rate.",
                    "label": 0
                },
                {
                    "sent": "Yes, that's correct.",
                    "label": 0
                },
                {
                    "sent": "I didn't want to enter into details, but I briefly spoke about the problem of multiple testing and the boundary correction.",
                    "label": 0
                },
                {
                    "sent": "But if you go the way of estimating the false discovery rate, this becomes irrelevant.",
                    "label": 0
                },
                {
                    "sent": "You can only deal with the false discovery rate.",
                    "label": 0
                },
                {
                    "sent": "I didn't also give you many details about this probe method and the difference with the using tabulated distribution.",
                    "label": 0
                },
                {
                    "sent": "But there are some.",
                    "label": 0
                },
                {
                    "sent": "Delicate aspects about this problem, and in some experiments with perform we do not get always the same results in estimating the false discovery rate with tabulated distribution.",
                    "label": 0
                },
                {
                    "sent": "With the probe method.",
                    "label": 0
                },
                {
                    "sent": "Because in a sense, the null distribution that you assume is rather different.",
                    "label": 0
                },
                {
                    "sent": "In both cases.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So practically, a lot of people in the medical domain they're used to using P values and for discovery rate, so you need to give that to them.",
                    "label": 0
                },
                {
                    "sent": "But practically I would trust most cause validation results.",
                    "label": 0
                },
                {
                    "sent": "However, they give you slightly different information.",
                    "label": 0
                },
                {
                    "sent": "Because, uh, the forces curve rate and P value give you some idea about the fraction of bad guys that you have in your selected features.",
                    "label": 0
                },
                {
                    "sent": "Whereas your cross validation results give you more sense about how predictive your feature set is.",
                    "label": 0
                },
                {
                    "sent": "An you may include a large fraction of garbage features in your feature set and yet get very good predictive values.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you use support vector machines on the entire feature set.",
                    "label": 0
                },
                {
                    "sent": "You may get as good results as with your restricted feature set, or even better, results.",
                    "label": 0
                },
                {
                    "sent": "Even though you have tons of garbage features.",
                    "label": 0
                },
                {
                    "sent": "So in challenges that I've organized, I have introduced purposely some garbage features so I knew for a fact that they were garbage features and a lot of people have obtained.",
                    "label": 0
                },
                {
                    "sent": "You know, top ranking performances without performing any feature selection.",
                    "label": 0
                },
                {
                    "sent": "So you basically need both aspects depending what your emphasis is on right.",
                    "label": 0
                },
                {
                    "sent": "If you care about.",
                    "label": 0
                },
                {
                    "sent": "The features that you select, for example in the medical application, if you care about truly understanding which features are predictive.",
                    "label": 0
                },
                {
                    "sent": "Knowing that you get good prediction performance in cross validation, maybe not enough, in which case you want you need to resort to computing file discover rate.",
                    "label": 0
                },
                {
                    "sent": "I have one more question.",
                    "label": 0
                },
                {
                    "sent": "Basically what he showed you should the.",
                    "label": 0
                },
                {
                    "sent": "Picking up to come down, but I'm not please this piece of.",
                    "label": 0
                },
                {
                    "sent": "But they also happen.",
                    "label": 0
                },
                {
                    "sent": "Is that featured in one picture of those cities like important so important?",
                    "label": 0
                },
                {
                    "sent": "If you have those two together, which you will not find out?",
                    "label": 0
                },
                {
                    "sent": "That's correct.",
                    "label": 0
                },
                {
                    "sent": "That's correct, so forward selection and backward elimination methods do not yield the same type of feature set set at all backward elimination.",
                    "label": 0
                },
                {
                    "sent": "You may end up with a set of complementary features, and as you keep removing features then you have this catastrophic degradation at some point because at some point you will be removing one feature such that you know all the remaining features are not productive together anymore.",
                    "label": 0
                },
                {
                    "sent": "You don't see that in forward selection because the first picture you select is predictive, but then you run at risk of selecting first feature that is predicted by itself, But then you miss eventually a pair of features that together would have been more productive than than that single feature that you selected first.",
                    "label": 0
                },
                {
                    "sent": "In for selection, you tend to get very compact sets of subsets of features an which may be an advantage or disadvantage because sometimes redundancy helps you give you give get better performance on the test set so there is no you know one method that should be recommended depends on what you want to achieve.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it's important that you will be able to trade number of features for accuracy.",
                    "label": 0
                },
                {
                    "sent": "This is an advantage in that case to do forward selection because you can smoothly move.",
                    "label": 0
                },
                {
                    "sent": "You know your number of features without having this catastrophic degradation.",
                    "label": 0
                },
                {
                    "sent": "And sometimes you really want to have an optimum feature subset of complementary features, in which case backward elimination may be an advantage.",
                    "label": 0
                },
                {
                    "sent": "So I had drink the right two other questions.",
                    "label": 0
                },
                {
                    "sent": "One of them was yes.",
                    "label": 0
                },
                {
                    "sent": "Hi, in the reading of your presentation you show these cubes where.",
                    "label": 0
                },
                {
                    "sent": "You asked that no feature selection was good for a certain classifier.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Do you think that maybe that's a risky compression?",
                    "label": 0
                },
                {
                    "sent": "Because that could be because we don't have a good feature selection method.",
                    "label": 0
                },
                {
                    "sent": "I mean we the more, the more features we have, the better room is pacifying, but maybe that's good because our feature selection is not good so we don't have this maximum.",
                    "label": 0
                },
                {
                    "sent": "And then with this, yes this is also a problem of the difficulty of finding good subsets of features when you don't have a lot of data.",
                    "label": 0
                },
                {
                    "sent": "As it turns out, the feature selection problem is much harder than the problem of training the classifier.",
                    "label": 0
                },
                {
                    "sent": "And so if I tell you which features are good, so for example, if I had some extra datasets on which I could be selecting features 1st and then using those to try, and you know on the data, then I might gain a lot of performance.",
                    "label": 0
                },
                {
                    "sent": "Actually, these are experiments that we've done that we had some old data sets on which that we could use to select features and then we would use those features on some new data and then gain a lot in performance so.",
                    "label": 0
                },
                {
                    "sent": "Feature selection is extremely data hungry and you have to do both feature selection and training your classifier on the same data.",
                    "label": 0
                },
                {
                    "sent": "So if you don't have enough data, you may just be in better shape if you just choose all their audio features and train directly or classifier.",
                    "label": 0
                },
                {
                    "sent": "If you have, you know some good regularization going on at the same time.",
                    "label": 0
                },
                {
                    "sent": "I have a question about the context of time series problems.",
                    "label": 0
                },
                {
                    "sent": "If you consider another regression.",
                    "label": 0
                },
                {
                    "sent": "So then your data matrix becomes just values aspects of your output.",
                    "label": 0
                },
                {
                    "sent": "Is there any fundamental difference in the techniques for showing in that respect?",
                    "label": 0
                },
                {
                    "sent": "So I have no time for many experiments on that, so I'm not familiar with that problem and some other people have been asking me that question during the break.",
                    "label": 0
                },
                {
                    "sent": "I suppose you know it would be a relatively simple extension of those algorithms to go to the online case in which you have continuous flux of data that eventually changes overtime.",
                    "label": 0
                },
                {
                    "sent": "If you use these methods of scaling factors, you could keep adjusting your scaling factors overtime.",
                    "label": 0
                },
                {
                    "sent": "There was another question also during direct regarding multi category classification or multi objective optimization in which you want to select a subset of features that simultaneously is good for many purposes.",
                    "label": 1
                },
                {
                    "sent": "And obviously you can do that.",
                    "label": 0
                },
                {
                    "sent": "Some people have been designing special purpose problems for that, in particular for multiclass SVM, but it has also been shown that you often get better performance if you select features.",
                    "label": 0
                },
                {
                    "sent": "Separately, for each of the objectives and then.",
                    "label": 0
                },
                {
                    "sent": "And then combine the results at the end.",
                    "label": 0
                },
                {
                    "sent": "And again, it's very dependent upon the problem and also upon the number of examples that you have available.",
                    "label": 0
                },
                {
                    "sent": "As always, if you have very, very few examples, you might benefit from constraining your problem more.",
                    "label": 0
                },
                {
                    "sent": "So imposing that you have a single feature subset for all your multiple objectives, because this adds some regularization.",
                    "label": 0
                },
                {
                    "sent": "If you have enough examples and you're probably better off having separate feature subsets for each of your objectives.",
                    "label": 0
                },
                {
                    "sent": "Um, let's move on and.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one way as I said of doing embedded methods and it's very flexible is to have these scaling factors.",
                    "label": 0
                },
                {
                    "sent": "So one way of viewing this is that you can think of your data matrix has, as you know, number of lines corresponding to examples and the number of columns corresponding to features.",
                    "label": 0
                },
                {
                    "sent": "And why is you know an extra column for the target?",
                    "label": 0
                },
                {
                    "sent": "And if you use a kernel method in which the parameters of your learning machines are the alphas, that way the examples that have been passed, for example some kernel function.",
                    "label": 0
                },
                {
                    "sent": "Then you have the Alpha waves that monitor the lines of the matrix, and if you use some algorithm like SVM that select a certain number of support vectors that are examples in particular lines here you can see that you can see that you're going to.",
                    "label": 0
                },
                {
                    "sent": "Reduce your data matrix in that direction here.",
                    "label": 0
                },
                {
                    "sent": "Whereas if you introduce scaling factors, you do it in the other in the opposite direction.",
                    "label": 1
                },
                {
                    "sent": "So in a way you have now two ways of compressing your data.",
                    "label": 0
                },
                {
                    "sent": "One you know in the direction of the patterns and when in the action of the features.",
                    "label": 0
                },
                {
                    "sent": "So a lot of people have been using scaling factors came from the kernel method community, and they found that this was a convenient way of.",
                    "label": 0
                },
                {
                    "sent": "Introducing both, you know support vectors and kind of support features, right?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "In the next few slides, I'm going to borrow some slides from on the cell phone.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And talk about a little bit the formalism that goes behind these embedded methods.",
                    "label": 0
                },
                {
                    "sent": "Many younger learning algorithms are casting to minimization of some regularised functional are of Alpha and Sigma.",
                    "label": 1
                },
                {
                    "sent": "And the the minimum runs over the the weights of the learning algorithm, the alphas.",
                    "label": 0
                },
                {
                    "sent": "They will not be always waits on the patterns, but you know, for all kernel methods that that's what they are.",
                    "label": 0
                },
                {
                    "sent": "And this risk functional.",
                    "label": 0
                },
                {
                    "sent": "Is the average of some loss function that you can represent as of.",
                    "label": 0
                },
                {
                    "sent": "Measure of discrepancy between the function that you are.",
                    "label": 0
                },
                {
                    "sent": "This is your model is your predictive model and that is itself a function of Alpha.",
                    "label": 0
                },
                {
                    "sent": "And now of the scaling factors that each multiply.",
                    "label": 0
                },
                {
                    "sent": "This is, you know, the the component why the componentwise multiplication, so each coming factor multiplies one input coefficient.",
                    "label": 0
                },
                {
                    "sent": "And so we compute the discrepancy between this F of XK&YK.",
                    "label": 0
                },
                {
                    "sent": "So you see, now that we have two sets of parameters in here.",
                    "label": 0
                },
                {
                    "sent": "And we have, of course a regularizer on the alphas.",
                    "label": 0
                },
                {
                    "sent": "Now what we can do is that we can once we have minimized over Alpha, define a function J of Sigma.",
                    "label": 0
                },
                {
                    "sent": "Now function of the scaling factor.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And has been shown by Andre and his collaborators.",
                    "label": 0
                },
                {
                    "sent": "Under some conditions.",
                    "label": 0
                },
                {
                    "sent": "The removal of 1 feature will induce a change in G proportional to this gradient here of F with respect to X I ^2.",
                    "label": 1
                },
                {
                    "sent": "So this is like doing a sensitivity analysis.",
                    "label": 0
                },
                {
                    "sent": "You are, you know, shaking a little bit.",
                    "label": 0
                },
                {
                    "sent": "One of the inputs and you're looking at what is going to be the impact of this disturbance on F of X.",
                    "label": 0
                },
                {
                    "sent": "And for example, if you apply this to the linear SVN, as it turns out, this DFF of RXI is nothing that is proportional to the Wii.",
                    "label": 0
                },
                {
                    "sent": "So what you can do is that you can train an SVM.",
                    "label": 0
                },
                {
                    "sent": "You can look at the simply the weights of the linear predictor an each take the square of these weights and rank the feature according to this query this ways.",
                    "label": 0
                },
                {
                    "sent": "And this is basically what has been.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Done with the recursive feature elimination algorithm.",
                    "label": 1
                },
                {
                    "sent": "What you do is that you first take a certain set of features that is called F. Here in this notation.",
                    "label": 0
                },
                {
                    "sent": "And then you compute the W star solution and an SVM on this data set restricted to the features in F. So this means that you are minimizing your risk functional with respect to Alpha.",
                    "label": 0
                },
                {
                    "sent": "Then you select the top features ranked by the absolute value of WI star or the square of WI stars the same thing.",
                    "label": 0
                },
                {
                    "sent": "And, um.",
                    "label": 0
                },
                {
                    "sent": "And then you are going to basically remove the feature that has the smallest value of WI in absolute value.",
                    "label": 0
                },
                {
                    "sent": "So this is equivalent to.",
                    "label": 0
                },
                {
                    "sent": "Performing this trick of scaling factors because these WI absolute values are proportional to these scaling factors.",
                    "label": 0
                },
                {
                    "sent": "And then back to Step 2, you each rate, you retrain your SVM.",
                    "label": 0
                },
                {
                    "sent": "Then you find again those weights that are smallest etc.",
                    "label": 0
                },
                {
                    "sent": "So it's nice to see that this very simple method is principle and it extends through the nonlinear case.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To the nonlinear SVM.",
                    "label": 0
                },
                {
                    "sent": "You can also perform gradient descent.",
                    "label": 0
                },
                {
                    "sent": "So now in this case, instead of removing at each iteration, the weight that has the smallest absolute value.",
                    "label": 0
                },
                {
                    "sent": "You can iterate and compute, recompute the scanning factors according to some gradient descent scheme.",
                    "label": 0
                },
                {
                    "sent": "At in the end of this process, where you iterate minimizing over the alphas and minimizing over the sigmas, you obtain both a set of alphas and a set of sigmas.",
                    "label": 0
                },
                {
                    "sent": "But this of course does not perform real feature selection, just rescales your inputs.",
                    "label": 0
                },
                {
                    "sent": "So if you want to do real feature selection, then you will have to then remove the features that have the smallest sigmas.",
                    "label": 0
                },
                {
                    "sent": "So it mixes well with, you know, any possible algorithm that allows us to do gradient descent, but it's.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Saving computation and has local minima.",
                    "label": 0
                },
                {
                    "sent": "Instead.",
                    "label": 0
                },
                {
                    "sent": "People have been thinking of taking a different approach to feature selection.",
                    "label": 0
                },
                {
                    "sent": "Which is that of.",
                    "label": 0
                },
                {
                    "sent": "Minimizing the number of feature.",
                    "label": 1
                },
                {
                    "sent": "And symbol Tanias Lee addressing the objective of obtaining best prediction performance.",
                    "label": 0
                },
                {
                    "sent": "So instead of saying, you know, let's minimize or primary objective which is getting best performance and let's you know try to disturb it to this possible by doing some feature selection on this side, replace that by the problem of minimizing the number of feature an simultaneously addressing the objective of, for example, minimizing the number of errors of classification.",
                    "label": 0
                },
                {
                    "sent": "So the problem with this is that this objective here is not very well behaved, because it's it's discrete.",
                    "label": 0
                },
                {
                    "sent": "And so you don't.",
                    "label": 0
                },
                {
                    "sent": "You can't perform gradient descent and all sorts of things on that, so people have been replacing this objective, but other objectives.",
                    "label": 1
                },
                {
                    "sent": "For example, the sum of the absolute values of the Wis which people called the L1 norm, or some kind of differentiable function that uses that, performs basically the same thing.",
                    "label": 0
                },
                {
                    "sent": "And so then you minimize jointly.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The primary and the secondary secondary objective.",
                    "label": 0
                },
                {
                    "sent": "One very popular algorithm that uses this principle is the so-called L1 SVM.",
                    "label": 0
                },
                {
                    "sent": "And it's a version of the regularizer VM Ware, the minimization of the square norm has been replaced by the minimization of the one norm, so the sum of the Wii.",
                    "label": 0
                },
                {
                    "sent": "And so it can be considered in some way, an embedded method, because it returns in the end.",
                    "label": 1
                },
                {
                    "sent": "Both.",
                    "label": 0
                },
                {
                    "sent": "A predictor that gives you no prediction on.",
                    "label": 0
                },
                {
                    "sent": "On new examples, if you want an, it also returns a certain number of.",
                    "label": 1
                },
                {
                    "sent": "A features that have not been discarded because some of the weights will be exactly 0.",
                    "label": 0
                },
                {
                    "sent": "So the difference between the regular SVM and the other one is VM.",
                    "label": 0
                },
                {
                    "sent": "Is that the algorithm drives some of the weights to 0.",
                    "label": 0
                },
                {
                    "sent": "So how how?",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This happened is not always very intuitive to understand, so I have a mechanical interpretation.",
                    "label": 0
                },
                {
                    "sent": "Yes, thank you for clarifying that.",
                    "label": 0
                },
                {
                    "sent": "It's true that this particular algorithm works for only linear SVM.",
                    "label": 0
                },
                {
                    "sent": "On this figure I'm I'm illustrating the classical way in which we view these regularised optimizations.",
                    "label": 0
                },
                {
                    "sent": "We think of them as minimizing a cost function, plus Lambda some regularizer.",
                    "label": 0
                },
                {
                    "sent": "And in this figure here I'm showing two only two weights W1 and W2, and assume that you know the optimal solution without regularization is called W star.",
                    "label": 0
                },
                {
                    "sent": "And assume that you have a quadratic.",
                    "label": 0
                },
                {
                    "sent": "Objective like for instance what you would do in in a Ridge regression.",
                    "label": 0
                },
                {
                    "sent": "In this case, as you go away from the optimal solution, you go up in tabloid.",
                    "label": 0
                },
                {
                    "sent": "And the organizer can take can be thought of as a spring that pulls your solution to zero with the spring strength of Lambda.",
                    "label": 0
                },
                {
                    "sent": "So this is the classical way people usually view regularization.",
                    "label": 0
                },
                {
                    "sent": "Now you can.",
                    "label": 0
                },
                {
                    "sent": "Can change the way you're thinking.",
                    "label": 0
                },
                {
                    "sent": "Instead of thinking you know we want to minimize this primary objective, which is you know your origonal objective function, so you're only square function plus some regularizer.",
                    "label": 0
                },
                {
                    "sent": "The primary objective is not the regularizer which is the in the case of the one norm, minimize the sum of the WS, or in the case of the so called 0 norm, when you minimize the number of features.",
                    "label": 0
                },
                {
                    "sent": "This is not your primary objective plus one over Lambda.",
                    "label": 0
                },
                {
                    "sent": "Your old primary objective, which is really your prediction error.",
                    "label": 0
                },
                {
                    "sent": "You can represent it in the same way, except now the ball you know is the drives your weight to zero and what your spring is what?",
                    "label": 0
                },
                {
                    "sent": "Pulls you to your solution.",
                    "label": 0
                },
                {
                    "sent": "So why am I doing this crazy thing was just because it's easier than to understand what the one norm is.",
                    "label": 0
                },
                {
                    "sent": "BM and the last, so does so the lasso is to the one or SVM what?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Basically, the lesson is the least square version of the one norm SVN.",
                    "label": 0
                },
                {
                    "sent": "So the Ridge regression is the square version of the two norm SVN and the LASSO is the one norm SVN version.",
                    "label": 0
                },
                {
                    "sent": "The one arm version of the regression case.",
                    "label": 0
                },
                {
                    "sent": "I'm just showing that rather than the classification problem because for.",
                    "label": 0
                },
                {
                    "sent": "The mechanical interpretations is easier to understand.",
                    "label": 0
                },
                {
                    "sent": "So what you see in the case of the one norm is that again, these are two ways that we want and W2.",
                    "label": 0
                },
                {
                    "sent": "And now.",
                    "label": 0
                },
                {
                    "sent": "Or regularizer which was before 2 norm.",
                    "label": 0
                },
                {
                    "sent": "So was kind of fabulously now it's a diamond.",
                    "label": 0
                },
                {
                    "sent": "Because of the absolute value.",
                    "label": 0
                },
                {
                    "sent": "And the the other part of the objective is the same as before.",
                    "label": 0
                },
                {
                    "sent": "Is this spring that pulls you to the solution?",
                    "label": 0
                },
                {
                    "sent": "So imagine now that you have you know the bead in a big box, right?",
                    "label": 0
                },
                {
                    "sent": "And you have some force which is the spring here that pulls your bead.",
                    "label": 0
                },
                {
                    "sent": "And if you you know you shake a little bit your box, what's going to happen is that your bid is going to roll somewhere to a solution pulled of course, by by your force.",
                    "label": 0
                },
                {
                    "sent": "And where is it going to end up?",
                    "label": 0
                },
                {
                    "sent": "Is it going to professional?",
                    "label": 0
                },
                {
                    "sent": "You end up on on the face, or you know, on on an edge or in your corner where you easily see that easily slides into either an edge or into a corner.",
                    "label": 0
                },
                {
                    "sent": "And this is what happens in this optimization with the one norm.",
                    "label": 0
                },
                {
                    "sent": "What happens is that you end up in a solution that is on one of the axis.",
                    "label": 0
                },
                {
                    "sent": "And most of our large fraction you know of the weights are going to be exactly 0 in that solution, so This is why when you perform this simple substitution of the two norm for the one norm, you're going to have some of the weights which are exactly 0, and this scheme performs feature selection indirectly and says been very popular recently.",
                    "label": 0
                },
                {
                    "sent": "A lot of people just just choose that because.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean people like.",
                    "label": 0
                },
                {
                    "sent": "Optimization problems that are, well formula.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "No, let's go back to the 0 known problem minimizing the one or is kind of a surrogate to a problem.",
                    "label": 0
                },
                {
                    "sent": "We see that because of this diamond shape objective, we are going to have some ways that are going to be pulled to 0, but our real objective was to actually minimize the number of non zero weights.",
                    "label": 0
                },
                {
                    "sent": "And that's what's called the zero number.",
                    "label": 0
                },
                {
                    "sent": "09 is nothing but the number of non zero weights.",
                    "label": 0
                },
                {
                    "sent": "As we said before, it's a nasty objective to optimize so we can replace it by some more well behaved function.",
                    "label": 0
                },
                {
                    "sent": "And Western collaborate have device to scheme to a minimize that objective.",
                    "label": 0
                },
                {
                    "sent": "And they showed that it actually boils down to very simple algorithm.",
                    "label": 1
                },
                {
                    "sent": "You start with some sigmas that are scaling factors and they are all equal to 1.",
                    "label": 0
                },
                {
                    "sent": "Then you compute the solution of your SVN on the some data set.",
                    "label": 0
                },
                {
                    "sent": "And then what you're going to do is, are you going to define scaling factors that are absolute values of your?",
                    "label": 0
                },
                {
                    "sent": "Of your W star.",
                    "label": 0
                },
                {
                    "sent": "And you re evaluate your scanning factor at each step by multiplying them by the W star.",
                    "label": 0
                },
                {
                    "sent": "In absolute value.",
                    "label": 0
                },
                {
                    "sent": "So it's very easy.",
                    "label": 0
                },
                {
                    "sent": "What you do is that you're going to re scale your inputs at each iteration, but how important they are to the predictor using the absolute value of the weights.",
                    "label": 0
                },
                {
                    "sent": "And you have some kind of an exponential decay of those least important features.",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "Still in your eyes, yeah, but it can be generalized to the nonlinear is here.",
                    "label": 0
                },
                {
                    "sent": "In a similar way than than what you do for recursive feature elimination.",
                    "label": 0
                },
                {
                    "sent": "In that case, you don't choose the absolute value of the W stars, but you use scanning factors that correspond to the derivative of your cost function with respect to the variables.",
                    "label": 0
                },
                {
                    "sent": "And this can be efficiently optimal, efficiently approximated if you assume that the support vectors don't change between two consecutive steps.",
                    "label": 0
                },
                {
                    "sent": "That is, when you remove the feature that you're interested in.",
                    "label": 0
                },
                {
                    "sent": "If you assume that this product is don't change, you can easily estimate that.",
                    "label": 0
                },
                {
                    "sent": "But actually in the original papers, the authors only dealt with the.",
                    "label": 1
                },
                {
                    "sent": "The linear SVM and this is, you know.",
                    "label": 0
                },
                {
                    "sent": "Completely straightforward to implement and very efficient.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Acid.",
                    "label": 0
                },
                {
                    "sent": "So in summary, for embedded methods.",
                    "label": 1
                },
                {
                    "sent": "I'm betting this is our good inspiration to design new feature selection algorithms for.",
                    "label": 1
                },
                {
                    "sent": "You know your own algorithms.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you have you like we had in the audience some question, how can we do feature selection for nonstationary process or how can we do feature selection for this or that other algorithm?",
                    "label": 0
                },
                {
                    "sent": "But the answer would be there you know try try these methods of scaling factors and try to optimize just getting factors in the process of learning this is this is 1 simple way of doing things.",
                    "label": 0
                },
                {
                    "sent": "I'm better, misses are not too far from rapper techniques and they can be extended to multiclass introgression.",
                    "label": 1
                },
                {
                    "sent": "But they have the advantage that they are faster than rapper techniques you don't have to go through as many trainings of classifiers or predictors.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now that we've talked in awhile, algorithms and a lot of technical things were going to go into more philosophical things.",
                    "label": 0
                },
                {
                    "sent": "We're going to talk about the problem of causality.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So far.",
                    "label": 0
                },
                {
                    "sent": "I've just said that you know feature selection deals with removing features to improve all these degrade prediction of Y, regardless of how these features are wired.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To the target.",
                    "label": 0
                },
                {
                    "sent": "And I've given you these two examples as a justification why one should use multivariate feature selection and not just a univariate feature selection.",
                    "label": 0
                },
                {
                    "sent": "Now I'm going to tell you that everything I told you so far was wrong.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And that actually is very dangerous to do multivariate feature selection.",
                    "label": 0
                },
                {
                    "sent": "This is an example from real life data in which we had discovered one feature that was very.",
                    "label": 0
                },
                {
                    "sent": "Discriminate by itself and then a second feature that helped the first feature, but by itself was not discriminate at all.",
                    "label": 0
                },
                {
                    "sent": "This example is an example of mass spectrometry, in which all the points in the spectrum were used as a feature.",
                    "label": 0
                },
                {
                    "sent": "And the the amplitude here is the value of that feature.",
                    "label": 0
                },
                {
                    "sent": "So for example, at this position here we have overlaid all the Spectra of one class, which are the right Spectra and all the spectrum of the other class.",
                    "label": 0
                },
                {
                    "sent": "The green Spectra.",
                    "label": 0
                },
                {
                    "sent": "So you can see that the examples of the red class are well separated from the examples of the great class using this particular feature X one.",
                    "label": 0
                },
                {
                    "sent": "This will be that feature in this scatter plot.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "There was another feature X2 that had been extracted by our feature selection algorithm corresponding to this value here and correspond to a feature which really does not separate well at all.",
                    "label": 0
                },
                {
                    "sent": "The two classes, however, taken together, the two features had a much better power separation than X1 alone.",
                    "label": 0
                },
                {
                    "sent": "Nothing a little bit deeper about what could be the meaning of these two features.",
                    "label": 0
                },
                {
                    "sent": "We decided that only X one was really informative feature.",
                    "label": 0
                },
                {
                    "sent": "Because this corresponds to the abundance of a protein in serum, and so there are some people who have, you know, more abundant protein than some other people.",
                    "label": 0
                },
                {
                    "sent": "Whereas this position here doesn't really represent the abundance of a protein is just a value is just a protein that's equally non present in all the patients of the population.",
                    "label": 0
                },
                {
                    "sent": "The only difference is that we observe here are just noise.",
                    "label": 0
                },
                {
                    "sent": "And it just so happens that there must be a systematic source of noise that makes it that all the spectral simultaneously reason or goal will be down in this neighborhood here, and even though we had performed some baseline removal, there was some residual error.",
                    "label": 0
                },
                {
                    "sent": "There was significant enough that it was useful to add this feature X2 that is a local estimation of the baseline here.",
                    "label": 0
                },
                {
                    "sent": "So what I'm showing you actually here is a very very small piece of a very large.",
                    "label": 0
                },
                {
                    "sent": "Spectrum.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so this is actually a close in the close neighborhood of this other feature.",
                    "label": 0
                },
                {
                    "sent": "So from my point of view, having understood that feature X2 is not really a relevant feature, it is an indirect measure of some systematic error that we should have been removing by preprocessing.",
                    "label": 0
                },
                {
                    "sent": "So I'm showing you that this is real that actually this was an example, but this is the real.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The other way we had it.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a similar problem.",
                    "label": 0
                },
                {
                    "sent": "Can it shows up in the case of the ex or and we also have real data example in which we have situations like this in which in fact the the fact that we have separate clusters and this abrupt change.",
                    "label": 0
                },
                {
                    "sent": "You know when one feature was selected separating the other one and then all of the sudden forgiven value of another feature it changes the other way around.",
                    "label": 0
                },
                {
                    "sent": "Very often this is just the symptom of something that went wrong for example.",
                    "label": 0
                },
                {
                    "sent": "There has been a.",
                    "label": 0
                },
                {
                    "sent": "A shift in operator or or something bad happened to the process.",
                    "label": 0
                },
                {
                    "sent": "There was a bad randomization of the samples and we're seeing some artifact here.",
                    "label": 0
                },
                {
                    "sent": "So interpreted in terms of across all graph, this is what happens.",
                    "label": 0
                },
                {
                    "sent": "WHI is the target in that case this would be or disease am an X one is the abundance of some you know component in blood.",
                    "label": 0
                },
                {
                    "sent": "For example the abundance of a protein.",
                    "label": 0
                },
                {
                    "sent": "So this is causes some changes in the abundant or protein.",
                    "label": 0
                },
                {
                    "sent": "But simultaneously there is some.",
                    "label": 0
                },
                {
                    "sent": "Some noise that also feeds into X1 some bad bad variable and so X one is the result of mixing these two elements here and so knowing X2 is actually useful to making better prediction on why then not knowing it.",
                    "label": 0
                },
                {
                    "sent": "But this doesn't mean that X2 tells us something about.",
                    "label": 0
                },
                {
                    "sent": "You know how, the, how.",
                    "label": 0
                },
                {
                    "sent": "Why is related to X1.",
                    "label": 0
                },
                {
                    "sent": "So for example in OK.",
                    "label": 0
                },
                {
                    "sent": "So if you know of measuring instrument if I would change instrument if I would have now an instrument which is calibrating in a different way X one X2 will become a completely useless variable, whereas you know X1.",
                    "label": 0
                },
                {
                    "sent": "If it has a reality in the system that we are interested in is the one that is relevant or important?",
                    "label": 0
                },
                {
                    "sent": "What happens is that the reason why it's two becomes relevant is that for any particular value of X1, then there is a dependency which is induced by X1 between X2 and Y.",
                    "label": 0
                },
                {
                    "sent": "So X2 is independent of Y, but given values of X1 it becomes dependent.",
                    "label": 0
                },
                {
                    "sent": "The reason being that X2.",
                    "label": 0
                },
                {
                    "sent": "Feeds information into X1.",
                    "label": 0
                },
                {
                    "sent": "Is the same thing happens here in the case of the chess board problem for any particular value of X one, we see that there is a separation on the X2 axis between the two categories, so there is an induced dependency.",
                    "label": 0
                },
                {
                    "sent": "In reality it's not really X2 that causes X one.",
                    "label": 0
                },
                {
                    "sent": "It's probably more complicated than that.",
                    "label": 0
                },
                {
                    "sent": "There may be a common cause, which is your systematic noise, so there is noise that causes change in the baseline.",
                    "label": 0
                },
                {
                    "sent": "An result you know in some observed value of the spectrum that we call X1, and some observed value that we call X2.",
                    "label": 0
                },
                {
                    "sent": "Practically, we can't differentiate between these two graphs.",
                    "label": 0
                },
                {
                    "sent": "Just looking at the data, but it's important to keep that in mind in order to interpret what we were looking at.",
                    "label": 0
                },
                {
                    "sent": "And it's important to understand that as soon as we have measurements.",
                    "label": 0
                },
                {
                    "sent": "Measurements are always consequences of the phenomena that we want to observe.",
                    "label": 0
                },
                {
                    "sent": "And we are not immune to measurement errors and we need to take that into account when we do feature selection.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I just if I somehow the.",
                    "label": 0
                },
                {
                    "sent": "Discovery of causal relationships between XYXI&Y.",
                    "label": 1
                },
                {
                    "sent": "So in essence, if you ignore them, you might make some bad decision about which features are important for your problem.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in the remainder of the talk, I'm going to briefly tell you what are the tools to discover these causal relationships.",
                    "label": 0
                },
                {
                    "sent": "Let me first define what I mean by causal feature relevance.",
                    "label": 1
                },
                {
                    "sent": "Here is a rather complex graph that will that will not analyze completely, but I would like to give you a sense of all the things that can happen is relatively elaborate in some cases.",
                    "label": 0
                },
                {
                    "sent": "Imagine that what you want to predict is lung cancer.",
                    "label": 0
                },
                {
                    "sent": "And there are some variables that might be causing lung cancer, for example smoking or some genetic factor.",
                    "label": 0
                },
                {
                    "sent": "And some variables that may be consequences of lung cancer like coughing or like having metastasis or some other symptoms that people sometimes call.",
                    "label": 0
                },
                {
                    "sent": "You know, biomarkers.",
                    "label": 0
                },
                {
                    "sent": "Now, there may be indirect causes like anxiety might be causing smoking and then ideal so consequences of causes that are called in the dragon.",
                    "label": 1
                },
                {
                    "sent": "Confounding factors like other cancers.",
                    "label": 0
                },
                {
                    "sent": "There may be hidden variables.",
                    "label": 0
                },
                {
                    "sent": "For example, there may be variables like tar in lungs that are more direct causes than smoking to lung cancer, but unfortunately you don't have access to them.",
                    "label": 1
                },
                {
                    "sent": "Then maybe also hidden confounders.",
                    "label": 0
                },
                {
                    "sent": "So for example, imagine this being you know recurrent theme in the tobacco settlement problem in the United States.",
                    "label": 0
                },
                {
                    "sent": "There has been by now you know a lot of health policies not to smoke in public places, and the restrictions on selling cigarettes and tobacco companies have been heavily complaining that there is no real evidence that smoking causes lung cancer that only correlation has been observed, and so far there hasn't been any real proof that there is no genetic factor that would both cause craving for smoking and lung cancer.",
                    "label": 0
                },
                {
                    "sent": "So imagining that there is such a genetic factor, then this cool rule out the hypothesis that smoking causes lung cancer.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Then there are other interesting things happening.",
                    "label": 0
                },
                {
                    "sent": "Imagine that you would like to predict lung cancer on the basis of coughing.",
                    "label": 0
                },
                {
                    "sent": "Indeed, the coughing might be a predictor of lung cancer, but imagine that you have another cause of coughing.",
                    "label": 0
                },
                {
                    "sent": "Like you know, allergy.",
                    "label": 0
                },
                {
                    "sent": "If you don't know about allergy, then coughing becomes a very poor predictor of lung cancer.",
                    "label": 0
                },
                {
                    "sent": "For example, if you are doing hay fever season and becomes virtually useless to a monitor, coughing to predict lung cancer.",
                    "label": 0
                },
                {
                    "sent": "And you know, I've I've shown you other cases you know, is this story of the systematic noise.",
                    "label": 0
                },
                {
                    "sent": "This is the example I gave you with the baseline problem.",
                    "label": 0
                },
                {
                    "sent": "All in all, you have you know all these different cases here and people working on the causal feature discovery are playing this game of trying to find those variables that are closest to the target.",
                    "label": 0
                },
                {
                    "sent": "In the sense that they shield the target from all the other variables and that's what people call the Markov blanket.",
                    "label": 0
                },
                {
                    "sent": "The Markov blanket is the set of variables that are now in the blue shaded area.",
                    "label": 0
                },
                {
                    "sent": "Such that when you consider these variables, then the variables outside the shaded area become independent of the target.",
                    "label": 0
                },
                {
                    "sent": "Now, as you can see, this is not as simple as it may look over at least as the people working on Markov blanket may tell you, because if you have hidden variables or those may be the ones that are really in the Markov blanket.",
                    "label": 0
                },
                {
                    "sent": "And in particular, in the case of systematic noise, the noise may be the thing that is really in the Markov blanket, but you don't know about it and you may be thinking that you know certain biomarker is relevant.",
                    "label": 0
                },
                {
                    "sent": "For example, this certain position in my mass spectrum.",
                    "label": 0
                },
                {
                    "sent": "You may think it's a it's relevant, but in reality it's not relevant.",
                    "label": 0
                },
                {
                    "sent": "The noise was what's wrong with his wealth.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nevertheless, I think these tools, of course, on discovery, are very useful because they allow you to Orient some of the arcs instead of justice detecting correlations.",
                    "label": 0
                },
                {
                    "sent": "You can detect causality.",
                    "label": 0
                },
                {
                    "sent": "And that might become very important, particularly if you want to work.",
                    "label": 0
                },
                {
                    "sent": "There are graphs between random variables where dependencies are represented by edges, and the graph allows you to compute various distributions between subsets of variables.",
                    "label": 1
                },
                {
                    "sent": "What is given is.",
                    "label": 0
                },
                {
                    "sent": "The probability of a given variable given its parents, and from that you can compute the joint probability of all the variables and all sorts of marginals that are, you know, using subsets of the variables.",
                    "label": 1
                },
                {
                    "sent": "The edge direction in the original vision networks have no meaning, but in coastal vision networks they indicate.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Causality.",
                    "label": 0
                },
                {
                    "sent": "So here is an example.",
                    "label": 0
                },
                {
                    "sent": "Of course all discovery algorithm, bias purpose and glymour I'm giving you this example, you know, not hoping that you will memorize it immediately, but to show you that it's not that complicated.",
                    "label": 0
                },
                {
                    "sent": "Let's consider AB&C.",
                    "label": 0
                },
                {
                    "sent": "Random variables that belong to a set X&VA subset of X.",
                    "label": 0
                },
                {
                    "sent": "So initialize with a fully connected on oriented graph representing the causal relationships between the variables.",
                    "label": 1
                },
                {
                    "sent": "Then find and oriented edges.",
                    "label": 1
                },
                {
                    "sent": "By using a criterion that variable A shares a direct age with variable B.",
                    "label": 1
                },
                {
                    "sent": "If and only if, no subsets of other variables, we can rental them conditionally independent, so this is what is written.",
                    "label": 0
                },
                {
                    "sent": "A conditionally independent of B given V. So all these algorithms for detecting causal dependencies rely heavily upon tests of conditional independence between variables.",
                    "label": 0
                },
                {
                    "sent": "And then Orient edges in circle colliders or triplets.",
                    "label": 0
                },
                {
                    "sent": "So Collider is.",
                    "label": 0
                },
                {
                    "sent": "Is it when two arrows?",
                    "label": 0
                },
                {
                    "sent": "Points.",
                    "label": 1
                },
                {
                    "sent": "To a given variable.",
                    "label": 1
                },
                {
                    "sent": "So they collide onto C. So you are in ages in this Collider, using the criterion that if there is a direct edge between A&C.",
                    "label": 0
                },
                {
                    "sent": "And between C&B, but not between A&B.",
                    "label": 0
                },
                {
                    "sent": "Then you have this pattern of connection.",
                    "label": 0
                },
                {
                    "sent": "If and only if there is no subset V containing C such that independent of the given given the OK.",
                    "label": 1
                },
                {
                    "sent": "So just to say that you need to do another another test of conditional independence to determine this, and then there are some heuristic to further or.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Edges.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "These algorithms are.",
                    "label": 0
                },
                {
                    "sent": "Quite expensive computationally.",
                    "label": 0
                },
                {
                    "sent": "An also statistically so they are very data hungry and take a lot of time, so usually people resort to simplifying them.",
                    "label": 0
                },
                {
                    "sent": "And compromise, for example by.",
                    "label": 0
                },
                {
                    "sent": "Abandoning the estimation of the full graph, but caring only about those links that are between the target variable and the other variables.",
                    "label": 1
                },
                {
                    "sent": "Or abandoning the idea of fully oriented, orienting the graph, but orienting only a subset of the edges.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I will be talking to you about a prototypical algorithm for discovering the Markov blankets, which is the case in which you care only about a given target, and you want to find those variables that shield.",
                    "label": 0
                },
                {
                    "sent": "The target from all the other variable.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is it?",
                    "label": 0
                },
                {
                    "sent": "So what you do first is that you identify the parents and children.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the following way.",
                    "label": 0
                },
                {
                    "sent": "That's the first iteration you add any given node, let's call it a second iteration.",
                    "label": 0
                },
                {
                    "sent": "You add another node, let's call it B.",
                    "label": 0
                },
                {
                    "sent": "And then at the third iteration you would be looking at conditional independence.",
                    "label": 0
                },
                {
                    "sent": "So if a is conditionally independent of Y given B.",
                    "label": 0
                },
                {
                    "sent": "Then remove a.",
                    "label": 0
                },
                {
                    "sent": "Then you don't need A and keep only be.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Cetera.",
                    "label": 0
                },
                {
                    "sent": "So once you've done, you know identified parents and children in this way, you can identify parents of parents and children of children and parents of children.",
                    "label": 1
                },
                {
                    "sent": "So the depth two relatives and the depth to relatives include the members of the Markov blanket and some additional variables that outside the Markov blanket.",
                    "label": 0
                },
                {
                    "sent": "Perhaps a decision I should go back to this graph where I showed you the example first.",
                    "label": 0
                },
                {
                    "sent": "To clarify why we need depth to relatives in the Markov blanket.",
                    "label": 0
                },
                {
                    "sent": "Because I remember now that I didn't make that.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So clear.",
                    "label": 0
                },
                {
                    "sent": "If you remember this this graph here.",
                    "label": 0
                },
                {
                    "sent": "Smoking would be apparent.",
                    "label": 0
                },
                {
                    "sent": "Genetic factor one would be apparent.",
                    "label": 0
                },
                {
                    "sent": "Coughing with your child mythicists would be a child, so these are direct relatives, parents and children.",
                    "label": 0
                },
                {
                    "sent": "Now we have someone in direct relatives that are called spouses and those poses are important to make predictions because without them the children become much less productive like I explained.",
                    "label": 0
                },
                {
                    "sent": "If you have only the variable coughing and you're doing hay fever season it becomes virtually useless because you can explain away.",
                    "label": 0
                },
                {
                    "sent": "The cafe in by the allergy.",
                    "label": 0
                },
                {
                    "sent": "So you need to know about allergy in order to have coughing be productive.",
                    "label": 0
                },
                {
                    "sent": "So basically you need to know about the espouses in order for the children to inform you about the parents.",
                    "label": 0
                },
                {
                    "sent": "In genetics, is a little bit more complicated than that, but a blue eyed children doesn't inform very well about the color of the eyes of his.",
                    "label": 0
                },
                {
                    "sent": "You know, in that case it's more complicated than parents, but enhance sisters.",
                    "label": 0
                },
                {
                    "sent": "You need to know something about the collateral relatives, so he's doing something about the family of the Father so that the eye color informs you about the family of the mother of the children.",
                    "label": 0
                },
                {
                    "sent": "So this is the same idea.",
                    "label": 0
                },
                {
                    "sent": "So we need the spouses in the Markov blanket.",
                    "label": 0
                },
                {
                    "sent": "And you go back to where I was.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you're left with the problem of just eliminating the.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Their relatives.",
                    "label": 0
                },
                {
                    "sent": "And you do that again with some conditional independence criteria that I don't want to go into detail about.",
                    "label": 0
                },
                {
                    "sent": "But basically it's a fairly simple process.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now before I conclude, people often ask me, well, we have now old.",
                    "label": 0
                },
                {
                    "sent": "You know, these feature selection algorithm.",
                    "label": 0
                },
                {
                    "sent": "So which one should I use, right?",
                    "label": 0
                },
                {
                    "sent": "How should I proceed to select what's going to work best on my method?",
                    "label": 0
                },
                {
                    "sent": "A lot of it has to do with the complexity and how much data you have in your data set and how many features you have.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'd like to emphasize that.",
                    "label": 0
                },
                {
                    "sent": "Feature selection is, as I said before, A2 level inference process.",
                    "label": 0
                },
                {
                    "sent": "And if we simplify things, instead of doing a cross validation, we assume we have just a single validation set.",
                    "label": 0
                },
                {
                    "sent": "Just to make things simpler to explain, we have now in these two level of inference and the first level of influence dealing with the amount of training data we have in order to prevent overfitting at the first level and the second level of inference we have to deal with the amount of validation data we have to prevent overfitting.",
                    "label": 0
                },
                {
                    "sent": "Of the feature selection in that case.",
                    "label": 1
                },
                {
                    "sent": "So this is something that perhaps is not emphasized enough in the literature of machine learning.",
                    "label": 0
                },
                {
                    "sent": "You really need to care about both the size of the training set and the size of the validation set.",
                    "label": 0
                },
                {
                    "sent": "In the algorithms that presented you, I've shown you that not algorithms are equal in terms of how many classifiers you are training each time you're training a classifier and testing, testing it using the validation set to assess its performance.",
                    "label": 0
                },
                {
                    "sent": "You're using your validation set.",
                    "label": 0
                },
                {
                    "sent": "As you may know when you do.",
                    "label": 0
                },
                {
                    "sent": "When you're trying to infer the generalization performance on new unknown test data, if you are in the case where you select from a finite number of classifiers.",
                    "label": 0
                },
                {
                    "sent": "The complexity of this process is proportional somehow to the log of the number of the number of the classifiers that you're testing.",
                    "label": 1
                },
                {
                    "sent": "So it's either the log of the number of classifier texting or the square root of the log, depending on whether or not you have zero training error, but it's not really important.",
                    "label": 0
                },
                {
                    "sent": "The important thing is that things scale with the logarithm of the number of things that you've tried with the number of classifiers that you've tried.",
                    "label": 0
                },
                {
                    "sent": "And we send message, you try fewer classifiers than others.",
                    "label": 1
                },
                {
                    "sent": "So if you try a exhaustive search wrapper.",
                    "label": 0
                },
                {
                    "sent": "You tried two to the N classifiers and every time you use your validation set right to assess its performance.",
                    "label": 0
                },
                {
                    "sent": "So you incur complexity of N and being the number of features.",
                    "label": 0
                },
                {
                    "sent": "Now, if you do another method like a greedy method.",
                    "label": 0
                },
                {
                    "sent": "So you remember that you do for selection of or backward elimination in the represents first time you have any features to select from.",
                    "label": 0
                },
                {
                    "sent": "The second time you have N -- 1 to select from etc.",
                    "label": 0
                },
                {
                    "sent": "Then in total you have N + 1 / 2 classifiers that your training and testing in order to assess which one is best.",
                    "label": 0
                },
                {
                    "sent": "And you also may remember that I said, well, we can do also the embedded version of forward selection and backward elimination, in which case we are guided by the search and each time we try, we add or remove a feature.",
                    "label": 0
                },
                {
                    "sent": "We don't try many, we just try one.",
                    "label": 0
                },
                {
                    "sent": "So in that case we test only North.",
                    "label": 0
                },
                {
                    "sent": "Classifiers in fact, you know from the point of your complexity, it doesn't make a lot of sense, but it doesn't make a lot of difference because the computational complexity, the statistical complexity you incur, is of the same order of magnitude.",
                    "label": 0
                },
                {
                    "sent": "But from you know the computational point of view.",
                    "label": 0
                },
                {
                    "sent": "This might be an advantage, and still you know it's maybe a little bit of an advantage from the statistical point of view.",
                    "label": 0
                },
                {
                    "sent": "Also.",
                    "label": 0
                },
                {
                    "sent": "So all we know.",
                    "label": 0
                },
                {
                    "sent": "OK, this people know about this, you know, tradeoff between.",
                    "label": 1
                },
                {
                    "sent": "Perhaps I should tell you that anyways.",
                    "label": 0
                },
                {
                    "sent": "This access this is the number of feature on this.",
                    "label": 0
                },
                {
                    "sent": "I said this is the error rate.",
                    "label": 0
                },
                {
                    "sent": "As you increase the number of features, the training error usually decreases monotonically.",
                    "label": 0
                },
                {
                    "sent": "But because of this complexity term here.",
                    "label": 0
                },
                {
                    "sent": "You have a bound which is similar to the bounds that you have usually.",
                    "label": 0
                },
                {
                    "sent": "No, the VC bounds another balance, except that because we are now looking at the second level of influence, not just training the classifier but selecting the features.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "What goes into the bound now is the validation error, not the training error plus some complexity term.",
                    "label": 0
                },
                {
                    "sent": "Which is simple.",
                    "label": 0
                },
                {
                    "sent": "It doesn't involve you know VC dimension or anything, it just involves the log of the number of classifiers that you've trained.",
                    "label": 0
                },
                {
                    "sent": "And so the complexity term if you add it to the validation error, you get this bound on the generalization error.",
                    "label": 0
                },
                {
                    "sent": "And now you see why in some cases you will have a curve that will decrease and not really go through an optimum, but smoothly, you know attend the centre or in that in some cases you might get a sharp optimum in your generalization error.",
                    "label": 0
                },
                {
                    "sent": "It all depends, you know, on the complexity of your second level of inference and what you're going to get here into that bound here, which I'm representing as a simple line.",
                    "label": 0
                },
                {
                    "sent": "And here I'm representing as a curve in another case.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what happens is that.",
                    "label": 0
                },
                {
                    "sent": "You you can get, you know.",
                    "label": 0
                },
                {
                    "sent": "A whole range of difficulty in the methods.",
                    "label": 0
                },
                {
                    "sent": "On one axis here in this table I'm representing the complexity of the first level of inference.",
                    "label": 0
                },
                {
                    "sent": "And that is when you move from a linear predictor to a non linear predictor.",
                    "label": 0
                },
                {
                    "sent": "Then people know that you increase.",
                    "label": 0
                },
                {
                    "sent": "You know the complexity or the capacity of your classifier.",
                    "label": 0
                },
                {
                    "sent": "And in order to avoid overfitting, you should just increase the size of your of your training set.",
                    "label": 0
                },
                {
                    "sent": "So in order to keep things under control at the first level of inference, you should make sure that your number of training examples is commensurate to your complexity of your first level of inference, but not from the point of view of feature selection.",
                    "label": 0
                },
                {
                    "sent": "You also have the second level of inference in which you need to know.",
                    "label": 0
                },
                {
                    "sent": "In which need to be aware of the fact that your number of validation examples is going to be critical to avoid overfitting the second level of inference, and you can put you in this table various.",
                    "label": 0
                },
                {
                    "sent": "Feature selection strategies.",
                    "label": 0
                },
                {
                    "sent": "Arranging from using inner upper way linear or nonlinear methods.",
                    "label": 0
                },
                {
                    "sent": "And in this other direction, you could be univariate or multivariate, so increasing the complexity of the feature selection process.",
                    "label": 0
                },
                {
                    "sent": "So I went through the exercise of putting some message in either each of these squares here, but this just gives you intuitively the idea of that you can.",
                    "label": 0
                },
                {
                    "sent": "Increase the complexity of your feature selection process.",
                    "label": 0
                },
                {
                    "sent": "If you're rich in data, but you need to be very careful if you have little data to train.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Waste.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Now we have put together a package that's called the change learning object package that's based on Matlab, and he's built on top of the spiders package.",
                    "label": 1
                },
                {
                    "sent": "It has some very simple abstractions.",
                    "label": 0
                },
                {
                    "sent": "Daytime model object.",
                    "label": 0
                },
                {
                    "sent": "From which you know you can build more complex objects and it includes many feature selection algorithms.",
                    "label": 0
                },
                {
                    "sent": "So among the ones that have been most successful in challenges that we've organized, so you can.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Download that package and use that and it's been very successful in reproducing the results of the NIPS 2003 feature selection Challenge.",
                    "label": 1
                },
                {
                    "sent": "I've used it in the class that I taught, and the students could.",
                    "label": 0
                },
                {
                    "sent": "Either, you know, outperform more closely match the performance of the challenge.",
                    "label": 0
                },
                {
                    "sent": "I give them some baseline methods that are the black, you know methods.",
                    "label": 0
                },
                {
                    "sent": "Here you have the balanced error rate they obtain on the five datasets of the challenge.",
                    "label": 0
                },
                {
                    "sent": "And this is the challenge best.",
                    "label": 0
                },
                {
                    "sent": "And here you have the error bar and so the student best results always.",
                    "label": 0
                },
                {
                    "sent": "He thought you know better or not significantly worse than the best results in the challenge.",
                    "label": 0
                },
                {
                    "sent": "So we have this resource for doing feature selection.",
                    "label": 0
                },
                {
                    "sent": "An interesting Lee, the methods that work best are all very simple filter methods here, so this is encouraging for practitioners and may be disappointing.",
                    "label": 0
                },
                {
                    "sent": "For theoreticians that still have some more work to do to make complex methods work better.",
                    "label": 0
                },
                {
                    "sent": "And one of the directions of research I think, which is going to be very promising and important, is particularly these causal feature discovery.",
                    "label": 0
                },
                {
                    "sent": "Finding to what extent we can alleviate the problem of multivariate feature selection, which tends to select not only good features but also.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So artifacts.",
                    "label": 0
                },
                {
                    "sent": "And so in conclusion, feature selection focuses on uncovering subsets of variables that are predictive of the target and multi very feature selections.",
                    "label": 1
                },
                {
                    "sent": "In principle more powerful than universe feature selection.",
                    "label": 0
                },
                {
                    "sent": "And there are some good, you know theoretical arguments why it should be more powerful, but it's not always more powerful in practice, both for reasons of statistical complexity like I've pointed out, we have now a second level of inference we have to pay the price for getting good subsets of features, and this surprising number of examples in our validation set.",
                    "label": 0
                },
                {
                    "sent": "And of course, the more examples in a validation set, the fewer examples in training set.",
                    "label": 0
                },
                {
                    "sent": "So do we want to pay that price rather than just using all our training data to train our classifiers and not performing feature selection?",
                    "label": 0
                },
                {
                    "sent": "Taking a closer look at the type of dependencies in terms of causal relationship may help us refine the notion of variable relevance and distinguish between artifacts and truly relevant.",
                    "label": 1
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Rebels and here you have two resources you can use in your research or in your applications.",
                    "label": 0
                },
                {
                    "sent": "One of them is this book on feature extraction where we compiled all the results of the feature selection challenge and it has a number of tutorial chapters explaining notions of statistical testing applied to feature selection and filter wrappers and embedded methods, and a chapter that we recently wrote on the causal feature selection.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much for your patience or your attention.",
                    "label": 0
                },
                {
                    "sent": "I'm not an expert in machine learning, so this question is not only for you, but also for the audience.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "When using this SVM linear multivariate method for filtering features, I wonder whether we could use for somebody in a multi class classification problem.",
                    "label": 0
                },
                {
                    "sent": "Take all the all the weights of the fear of the different classified of the infamous VM machines and maybe perform some spectral analysis to to get which are the best features for the whole of the classes.",
                    "label": 0
                },
                {
                    "sent": "Yes, this has been done the.",
                    "label": 0
                },
                {
                    "sent": "Message data mentioned have been generalized to the multi class SVM.",
                    "label": 0
                },
                {
                    "sent": "You can look at papers by Western and collaborator.",
                    "label": 0
                },
                {
                    "sent": "Thank you M. Can you tell me a few words how is feature selection in your view related to PCA principal component analysis?",
                    "label": 0
                },
                {
                    "sent": "It is very much related.",
                    "label": 0
                },
                {
                    "sent": "You can, you know.",
                    "label": 0
                },
                {
                    "sent": "If you remember the.",
                    "label": 0
                },
                {
                    "sent": "A prototype of the mentioning of minimizing the two norm SVM.",
                    "label": 0
                },
                {
                    "sent": "Then you can move to minimizing the one norm SVM, then to the zero norm SVN.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "You have a continuum between between the problems.",
                    "label": 0
                },
                {
                    "sent": "It can be shown that Ridge regression, which is minimizing the.",
                    "label": 0
                },
                {
                    "sent": "The square loss with the regularizer, which is the two norm.",
                    "label": 0
                },
                {
                    "sent": "Is in fact doing something very close to a principal component analysis.",
                    "label": 0
                },
                {
                    "sent": "It is doing a soft selection of the directions which are correspond to the largest eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "And so you can move continuously from from that by changing the regularizer to the problem of feature selection, in which you're eliminating features along the axis rather than along the principle directions.",
                    "label": 0
                },
                {
                    "sent": "Wait, my question is about the coastal feature selection.",
                    "label": 0
                },
                {
                    "sent": "Hi, I'm not an expert but I I had come to the I. I know the problem of Markov blanket.",
                    "label": 0
                },
                {
                    "sent": "I think when we built the Markov blanket we use a lot of domain specific knowledge.",
                    "label": 0
                },
                {
                    "sent": "In order to check whether two variables are conditionally independent.",
                    "label": 0
                },
                {
                    "sent": "With that I can I see that when we do the causal feature selection, we just put all these domain specific knowledge into the feature selection.",
                    "label": 1
                },
                {
                    "sent": "Is there a question or inventor or not?",
                    "label": 0
                },
                {
                    "sent": "They also automized.",
                    "label": 0
                },
                {
                    "sent": "Automate automized this process.",
                    "label": 0
                },
                {
                    "sent": "I mean we can always build this domain specific knowledge into the feature selection process, but is it really possible to?",
                    "label": 0
                },
                {
                    "sent": "I don't have an answer to this question and actually we are presently designing a challenge of causal feature selection, then will hopefully start in the next few months and I would like to be able to answer that question.",
                    "label": 0
                },
                {
                    "sent": "So we're formulating a number of problems that we will submit to people.",
                    "label": 0
                },
                {
                    "sent": "And hopefully they will have the answer to that after the challenge and how people addressed this and whether.",
                    "label": 0
                },
                {
                    "sent": "I have another question.",
                    "label": 0
                },
                {
                    "sent": "You are in the opposite sense.",
                    "label": 0
                },
                {
                    "sent": "Do you know any work using feature selection techniques to build a Markov blanket?",
                    "label": 0
                },
                {
                    "sent": "Well, there's the work of Constantin Aliferis, and he's been constantly crossing the border between the two domains, and he has devised techniques that benefit from feature selection and vice versa.",
                    "label": 0
                },
                {
                    "sent": "Personally, until you know I see the results of the challenge, I won't be convinced that the one thing works better than another, but I can see from the you know what I've been discussing in this presentation that if we can do it then it will be beneficial, but it's unclear whether whether this can be done effectively, you know, and for how much and how costly this will be in terms of number of examples as we've seen.",
                    "label": 0
                },
                {
                    "sent": "Before in the first feature selection challenge, it is possible to get very good performance without feature selection.",
                    "label": 0
                },
                {
                    "sent": "So now, how much do we gain, you know?",
                    "label": 0
                },
                {
                    "sent": "And what do we gain with causal discovery is something we'd like to to answer.",
                    "label": 0
                },
                {
                    "sent": "Is there a relation between causal feature selection and constructability of analysis of George Clear?",
                    "label": 0
                },
                {
                    "sent": "London.",
                    "label": 0
                },
                {
                    "sent": "I'm not familiar with that.",
                    "label": 1
                },
                {
                    "sent": "I think we need to talk about it offline.",
                    "label": 0
                },
                {
                    "sent": "How can you comment something about the uniqueness of your final set of features?",
                    "label": 0
                },
                {
                    "sent": "Sometimes I've been using, for example, a member of backward selection of formal selection, and at any moment when you have to select your features, your criterion can give exactly the same value.",
                    "label": 0
                },
                {
                    "sent": "Very similar.",
                    "label": 0
                },
                {
                    "sent": "So can you.",
                    "label": 0
                },
                {
                    "sent": "Can you comment on that?",
                    "label": 0
                },
                {
                    "sent": "In a multivariate feature selection, usually there is no uniqueness of the solution.",
                    "label": 0
                },
                {
                    "sent": "In fact, in some cases when there is a lot of redundancy in data, the there are many many feature subsets that give you equivalent.",
                    "label": 0
                },
                {
                    "sent": "Results.",
                    "label": 0
                },
                {
                    "sent": "If you go now to the Markov blanket, specially so Marco Market will claim that there is a unique Markov blanket under some conditions.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, the conditions are usually not met and so there's no real uniqueness, but practically people have been doing some bootstraps and data in order to assess the stability of the features, and as it turns out practically this.",
                    "label": 0
                },
                {
                    "sent": "So this gives good results even though you know then when you do bootstraps and you select those features that are most stable, they usually are not anymore part of 1 complementary subset of features.",
                    "label": 0
                },
                {
                    "sent": "I would consider that only a Horace tick, but it's it's a good one.",
                    "label": 0
                }
            ]
        }
    }
}