{
    "id": "c7l4hvhj5wkzxbxd7nvynoptraayiet4",
    "title": "Polyhedral Classifier for Target Detection A Case Study",
    "info": {
        "author": [
            "Vikas Raykar, Department of Computer Science, University of Maryland"
        ],
        "published": "Aug. 5, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml08_raykar_pcf/",
    "segmentation": [
        [
            "With colon, cancer is a main application.",
            "In this talk, I'll propose a new classifier called polyhedral classifier, designed especially for this application and this is joint work with Marat Matias, Aaron Marcos, and me.",
            "And we are all scientists at Siemens Medical Solutions and more author supposed to be here to present the talk, and you couldn't make it."
        ],
        [
            "OK.",
            "So.",
            "So colon cancer is supposed to be the second cause of all cancer related death in the US.",
            "So in the colon there is something called polyps, and this polyps are known to be precursors of cancer.",
            "So if you can find this polyps quite earlier, then we can prevent the cancer from progressing.",
            "So what a computer aided diagnosis system takes is you take a CT scan of: and you want to identify what are the different.",
            "Can we identify the polyps?",
            "So what I showed you on the right hand side is a typical CT scan and the right the yellow thing you see on here is a typical Poly which is marked by a radiologist.",
            "So generally we give this CD scans radiologist and he marks the location of polyps and based on such multiple CT scan, we want to learn a classifier which, given a region in a CT scan, can predict whether it is polypore not apology.",
            "So the algorithms typically proceed in three stages.",
            "In the first stage, we identify all certain suspicious regions.",
            "This is essentially an output of image processing algorithm which says these could be suspicious and this could be suspicious polyps.",
            "So the task of a classifier is to take these regions and extract features from each of these candidates and then predict whether it is a polyp or a non polyp.",
            "For example, this is the only polyp in this CT scan."
        ],
        [
            "So the main motivation for designing the polyhedral classifier is the data encounter in computer diagnosis is typically multi mode, so the only ground truth we have is what is the location of the polyp as marked by the radiologist.",
            "So that is our positive class.",
            "So all candidates which come close to the location of the Poly or positives but we don't have what is negative the rest of the world is a negative class.",
            "So as a result of this, there can be huge clusters of different negatives, so the variation among negatives can be quite large."
        ],
        [
            "For example, to give you an example on the left hand side are the two polyps.",
            "This is silent and this is what we want to distinguish and the false positives could be huge.",
            "They could be stool, it could be a noise sample, it could be a rectal tube, it could be some fold.",
            "So a priority.",
            "We know that there are these multiple clusters of negatives, and typically when we design A classifier, what we do is we pull all these negatives as one negative class and all these positives is another positive class and find a simple linear hyperplane which can find these two classifiers.",
            "But so the motivation here comes from if you know that there are these subclasses, can we do much more better?"
        ],
        [
            "So there are so in order to deal with this multi mode data some of."
        ],
        [
            "The state of the art methods are like.",
            "Mixture models which try to model the class distribution for each subclass.",
            "But the problem here is that we in our example, we are too few positives and too many features and robust estimation of this model parameters is."
        ],
        [
            "Quite difficult.",
            "The second we could always use any nonlinear classifier.",
            "For example, you could use the state of the art, SVM or RVM with kernel and get a complete nonlinear boundary.",
            "But we never got anything successful results with this nonlinear classifier result is we have too many few features in a very high dimensional space and it's very easily observed that the data can overfit.",
            "Can be offered by a nonlinear."
        ],
        [
            "Sapphire.",
            "And this is another class of methods called one class classifier, which omits the negative class an models only the positive class and by Colonel Ising.",
            "This with this technique you can also get the nonlinear boundaries."
        ],
        [
            "So in summary, what I want to say is that we like linear classifiers.",
            "Linear classifiers are less prone to overfitting, but they don't have that much capacity to fit to the multimodal nature of the data when there are many subclasses of negatives.",
            "Well, the nonlinear classifiers are good at doing this task.",
            "They can lead to overfitting.",
            "So we want to come something in between where it should be."
        ],
        [
            "OK, let me.",
            "So this comes to our proposed method.",
            "Instead of designing and only a classifier, let us see if you know there are case subclasses of negatives, we design A series of linear classifier, one for each subclass of the negatives.",
            "So this has essentially more capacity than linear classifier, but it is less.",
            "Prone to overfitting then and nonlinear classifier.",
            "And a new example would be classified as positive if all the K classifiers say it is positive."
        ],
        [
            "So.",
            "If you want just to train K linear classifiers, you could just take all the negatives from one class and the positives design one linear classifier, take the negatives from the 2nd subclass and the positives, and define a second classifier and in such a way we can design K linear classifiers.",
            "But what we're trying to achieve here is trying to design all of them jointly.",
            "So if you design it separately, it's a bit inefficient 'cause we're excessively penalising every misclassified positive sample, whereas we are seeing an example is positive if all of them say it is positive."
        ],
        [
            "We just so the proposed approach.",
            "What it does is optimizes this K classifiers jointly.",
            "So we have one classifier for each subclass of the negative data.",
            "As a result of which the objective function is penalized only once for a misclassified positive example.",
            "And since we said that an ex positive sample is classified right, if all K linear classifiers classified correctly, then resulting decision boundary isn't exactly a pool."
        ],
        [
            "Children.",
            "So just to give you a toy example, the center one or all are positive data and we know that there are K negative subclasses, and we're designing one, and we want to design 8 linear classifiers in a joint fashion.",
            "So this is the decision boundary as given by the algorithm.",
            "So we have something like a polyhedrin.",
            "So we design algorithm in the framework from an SVM framework."
        ],
        [
            "So typically we want to design A hyperplane SVM.",
            "You have a hyperplane classifier with a certain hinge loss.",
            "So your Alpha is the parameter which characterizes the hyperplane.",
            "You want to find Alpha and the loss for any example is the distance from the hyperplane.",
            "So the loss for the example is this.",
            "The hinge loss, which is the maximum of 0 and 1 -- Y Alpha transpose."
        ],
        [
            "I.",
            "Zero in the example is correctly classified.",
            "If the hinge loss is greater than zero, the example is misclassified OK.",
            "So now we are moving from one classifier to K classifiers.",
            "And I define epsilon, CIK to be the hinge loss.",
            "On the example induced by the classifier K. So we set a positive example is classified correctly if all the classifiers say it is positive.",
            "That means the hinge loss should be equal to 0 by all the classifiers.",
            "So how do we make the hinge loss loss to be exactly 0 to all the classifiers biamax operation?",
            "So if the Max is 0.",
            "That means all of them will be 0.",
            "So we want to.",
            "So now instead of.",
            "This is essential.",
            "Anything since we said that a negative is negative if it is.",
            "Hyperplane."
        ],
        [
            "Work for our objective function becomes like this, so there are K classifiers, Alpha and A2 to Alpha K which we want to optimize.",
            "So this is the standard SVM error on the negative examples.",
            "It remains the same.",
            "On positive examples, instead of having just a hinge loss, now we have the Max of the hinge loss of all the other example from all the classifiers.",
            "And this is an overall regularization term.",
            "Penalty.",
            "And this is a convex problem and we can be solved by another optimization routines.",
            "What we have now designed this.",
            "We're designed a classifier K classifiers.",
            "And.",
            "In give your design K linear classifiers for separating out the positive from case different negative subclasses in.",
            "Essentially, we don't.",
            "'cause it belongs to.",
            "Separate the positives from the negatives."
        ],
        [
            "Well, so the one problem here is that.",
            "So in this and framework.",
            "We assume that for every negative subclass, every negative example, we knew what subclass it belonged to, but this is not a realistic scenario.",
            "For example, in our CAD application we have around like few 100 positives and the number of negatives is of the order of 500,000.",
            "So a radiologist cannot typically mark all the negatives.",
            "But what we can do is we can take a subset of these negatives.",
            "And ask him to mark what kind of different negatives are those.",
            "So there are three types of examples in our training data.",
            "We have the positives which we know sure.",
            "And we know negatives with known subclass membership.",
            "We said what kind of negatives it was so we can have case such different negatives.",
            "And then there is a third class of negatives which we do not know the subclass membership.",
            "So the and algorithm solved the problem only when we knew.",
            "The.",
            "Membership for each.",
            "Then we knew the exact membership for each negative example.",
            "Now let's say, how do we account the examples for which we do not know the subclass membership?",
            "And this is exactly where."
        ],
        [
            "Our operation comes into picture.",
            "So we now have designed a classifier in something called and or framework.",
            "So this is essentially the same as the previous objective function.",
            "Every example which we know the subclass information has to be classified as negative by that particular hyperplane.",
            "The positive new term which we enter here is.",
            "No, the label what we say is.",
            "If for a negative example, we do not know which subclass it belongs to, that is which of the K classifiers should correctly classify you say a negative sample is classified correctly if at least one of them says it is correct.",
            "So if you have K of them, if at least one set it is 0.",
            "So the notion that the error on by from at least one has to be minimum can be captured exactly by the product here.",
            "So this is the product of the hinge losses.",
            "So this is exactly the operation.",
            "So if a product of terms is equal to if at least one of them is 0, then the product will be 0.",
            "The and operation is captured by the Max operation.",
            "The Max of the different errors, hinge losses, and the OR operation is just the product.",
            "So classic.",
            "Positives and some negatives which don't have any.",
            "Will this position problem because of the product term is not convex?",
            "So the solution?"
        ],
        [
            "We have this.",
            "And art.",
            "At each iteration, since there are K classifiers, we fix the rest of the classifiers and optimize only one of them.",
            "So once we've fixed all the other classifiers.",
            "Once we fix all the other classifiers, the problem, the subproblem becomes convex.",
            "So.",
            "So at each iteration, as I said, it's an alternating optimization algorithm.",
            "So at each iteration we have K steps and each step optimizes.",
            "So OK, so this is our final algorithm.",
            "We fixed all the classifiers, but the classifier K and minimized it and keep repeating until we converge.",
            "OK.",
            "So our goal here was to design K linear classifiers."
        ],
        [
            "And this is the.",
            "This is the functionality optimized.",
            "Actually an interesting.",
            "Advantage of using this kind of classifier is so the final rule is once your design all the K classifier you classify an example as positive if all of them say it is positive.",
            "So we can evaluate each of the K classifiers classifier 1234 till K and say whether all of them positive.",
            "So the important the order in which you do it doesn't matter.",
            "Here you can evaluate the classifier in any order you want.",
            "But let me let us think that each of the classifiers used only a subset of the features.",
            "In such a scenario, the ordering actually makes a huge difference in terms of your execution time.",
            "So most of the CAD applications we have.",
            "The features are not available.",
            "We have to query for features so you have an image.",
            "There is a candidate mark and if the classifier needs some features, the algorithm computes it and this the whole system has to run online in real time.",
            "So feature computation is extremely expensive or typical formulation.",
            "People uses a cascade of classifiers.",
            "So the first stage of the Cascade will use only the simplest features.",
            "The second stage will use more expensive features.",
            "The third will use the most expensive one.",
            "So by putting the."
        ],
        [
            "Regularization term to control the penalty here, if you use an L1 penalty."
        ],
        [
            "If you set P of Alpha, Kate is going to L1 norm instead of will have K sparse classifiers.",
            "Each with different nonzero coefficients.",
            "So if you do, the ordering doesn't matter for the accuracy.",
            "So if we implement it in a certain order, say start with the classifier that has the least number of non zero coefficients.",
            "And if you apply the classifier on that, if it is negative, we reject that classifier, and if it is positive with send it to the next stage.",
            "And in this way implement all the K classes."
        ],
        [
            "It is.",
            "But this is how it looks like, so we had K classifiers.",
            "So if we implement it in a cascade framework, we have a huge improvement in our runtime of the algorithm."
        ],
        [
            "So here I will describe some results.",
            "So we had a 316 patients for training, and we sequestered us another 385 patients for testing.",
            "And this is kind of an unbalanced problem.",
            "We are like 226 polyps and lot of negative candidates.",
            "And for each of the candidate's, there are a bunch of 98 numerical features.",
            "So let's say there are 1250 negatives, of which only 177 were actually annotated by the radiologist.",
            "So he told us what kind of negative it is.",
            "And the rest of the all the negatives.",
            "We just notice a negative.",
            "We do not know what subclass it belongs to.",
            "And totally we identified 9 subclasses."
        ],
        [
            "And here is just the.",
            "Here are the arosi plots.",
            "So this is plotting the specificity versus sensitivity is essentially a typical auto Seaport, but in auto see you put 1 minus specificity on the X axis.",
            "So the black dotted line is the proposed paliative classifier.",
            "And the green one is a simple SVM with users and RBF kernel.",
            "It's the nonlinear classifier.",
            "And the blue dot is in one class, SVM is called SVD.",
            "And the red one is a simple linear classifier.",
            "We have shown that by using this multiple class information we can design.",
            "Classifieds which are more active."
        ],
        [
            "And in terms of runtime, we have roughly around 25% gain in execution time over other methods, because a byproduct of our algorithm was that if implemented in a casket framework, it was extremely fast."
        ],
        [
            "So to conclude.",
            "We have shown how to design A polyhedral classifier for multimode data.",
            "So the end framework can be used when subclass information is fully available.",
            "The and or framework can incorporate the scenario when the subclass information is partially available.",
            "And because of this algorithm, the Cascade Design is a byproduct speeds up online execution of the algorithm.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With colon, cancer is a main application.",
                    "label": 0
                },
                {
                    "sent": "In this talk, I'll propose a new classifier called polyhedral classifier, designed especially for this application and this is joint work with Marat Matias, Aaron Marcos, and me.",
                    "label": 0
                },
                {
                    "sent": "And we are all scientists at Siemens Medical Solutions and more author supposed to be here to present the talk, and you couldn't make it.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So colon cancer is supposed to be the second cause of all cancer related death in the US.",
                    "label": 1
                },
                {
                    "sent": "So in the colon there is something called polyps, and this polyps are known to be precursors of cancer.",
                    "label": 0
                },
                {
                    "sent": "So if you can find this polyps quite earlier, then we can prevent the cancer from progressing.",
                    "label": 0
                },
                {
                    "sent": "So what a computer aided diagnosis system takes is you take a CT scan of: and you want to identify what are the different.",
                    "label": 1
                },
                {
                    "sent": "Can we identify the polyps?",
                    "label": 0
                },
                {
                    "sent": "So what I showed you on the right hand side is a typical CT scan and the right the yellow thing you see on here is a typical Poly which is marked by a radiologist.",
                    "label": 0
                },
                {
                    "sent": "So generally we give this CD scans radiologist and he marks the location of polyps and based on such multiple CT scan, we want to learn a classifier which, given a region in a CT scan, can predict whether it is polypore not apology.",
                    "label": 0
                },
                {
                    "sent": "So the algorithms typically proceed in three stages.",
                    "label": 1
                },
                {
                    "sent": "In the first stage, we identify all certain suspicious regions.",
                    "label": 0
                },
                {
                    "sent": "This is essentially an output of image processing algorithm which says these could be suspicious and this could be suspicious polyps.",
                    "label": 0
                },
                {
                    "sent": "So the task of a classifier is to take these regions and extract features from each of these candidates and then predict whether it is a polyp or a non polyp.",
                    "label": 1
                },
                {
                    "sent": "For example, this is the only polyp in this CT scan.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the main motivation for designing the polyhedral classifier is the data encounter in computer diagnosis is typically multi mode, so the only ground truth we have is what is the location of the polyp as marked by the radiologist.",
                    "label": 1
                },
                {
                    "sent": "So that is our positive class.",
                    "label": 0
                },
                {
                    "sent": "So all candidates which come close to the location of the Poly or positives but we don't have what is negative the rest of the world is a negative class.",
                    "label": 1
                },
                {
                    "sent": "So as a result of this, there can be huge clusters of different negatives, so the variation among negatives can be quite large.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For example, to give you an example on the left hand side are the two polyps.",
                    "label": 0
                },
                {
                    "sent": "This is silent and this is what we want to distinguish and the false positives could be huge.",
                    "label": 1
                },
                {
                    "sent": "They could be stool, it could be a noise sample, it could be a rectal tube, it could be some fold.",
                    "label": 0
                },
                {
                    "sent": "So a priority.",
                    "label": 0
                },
                {
                    "sent": "We know that there are these multiple clusters of negatives, and typically when we design A classifier, what we do is we pull all these negatives as one negative class and all these positives is another positive class and find a simple linear hyperplane which can find these two classifiers.",
                    "label": 0
                },
                {
                    "sent": "But so the motivation here comes from if you know that there are these subclasses, can we do much more better?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there are so in order to deal with this multi mode data some of.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The state of the art methods are like.",
                    "label": 0
                },
                {
                    "sent": "Mixture models which try to model the class distribution for each subclass.",
                    "label": 1
                },
                {
                    "sent": "But the problem here is that we in our example, we are too few positives and too many features and robust estimation of this model parameters is.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Quite difficult.",
                    "label": 0
                },
                {
                    "sent": "The second we could always use any nonlinear classifier.",
                    "label": 0
                },
                {
                    "sent": "For example, you could use the state of the art, SVM or RVM with kernel and get a complete nonlinear boundary.",
                    "label": 0
                },
                {
                    "sent": "But we never got anything successful results with this nonlinear classifier result is we have too many few features in a very high dimensional space and it's very easily observed that the data can overfit.",
                    "label": 1
                },
                {
                    "sent": "Can be offered by a nonlinear.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sapphire.",
                    "label": 0
                },
                {
                    "sent": "And this is another class of methods called one class classifier, which omits the negative class an models only the positive class and by Colonel Ising.",
                    "label": 1
                },
                {
                    "sent": "This with this technique you can also get the nonlinear boundaries.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in summary, what I want to say is that we like linear classifiers.",
                    "label": 0
                },
                {
                    "sent": "Linear classifiers are less prone to overfitting, but they don't have that much capacity to fit to the multimodal nature of the data when there are many subclasses of negatives.",
                    "label": 1
                },
                {
                    "sent": "Well, the nonlinear classifiers are good at doing this task.",
                    "label": 0
                },
                {
                    "sent": "They can lead to overfitting.",
                    "label": 0
                },
                {
                    "sent": "So we want to come something in between where it should be.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, let me.",
                    "label": 0
                },
                {
                    "sent": "So this comes to our proposed method.",
                    "label": 0
                },
                {
                    "sent": "Instead of designing and only a classifier, let us see if you know there are case subclasses of negatives, we design A series of linear classifier, one for each subclass of the negatives.",
                    "label": 1
                },
                {
                    "sent": "So this has essentially more capacity than linear classifier, but it is less.",
                    "label": 1
                },
                {
                    "sent": "Prone to overfitting then and nonlinear classifier.",
                    "label": 1
                },
                {
                    "sent": "And a new example would be classified as positive if all the K classifiers say it is positive.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If you want just to train K linear classifiers, you could just take all the negatives from one class and the positives design one linear classifier, take the negatives from the 2nd subclass and the positives, and define a second classifier and in such a way we can design K linear classifiers.",
                    "label": 0
                },
                {
                    "sent": "But what we're trying to achieve here is trying to design all of them jointly.",
                    "label": 0
                },
                {
                    "sent": "So if you design it separately, it's a bit inefficient 'cause we're excessively penalising every misclassified positive sample, whereas we are seeing an example is positive if all of them say it is positive.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We just so the proposed approach.",
                    "label": 1
                },
                {
                    "sent": "What it does is optimizes this K classifiers jointly.",
                    "label": 0
                },
                {
                    "sent": "So we have one classifier for each subclass of the negative data.",
                    "label": 1
                },
                {
                    "sent": "As a result of which the objective function is penalized only once for a misclassified positive example.",
                    "label": 0
                },
                {
                    "sent": "And since we said that an ex positive sample is classified right, if all K linear classifiers classified correctly, then resulting decision boundary isn't exactly a pool.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Children.",
                    "label": 0
                },
                {
                    "sent": "So just to give you a toy example, the center one or all are positive data and we know that there are K negative subclasses, and we're designing one, and we want to design 8 linear classifiers in a joint fashion.",
                    "label": 1
                },
                {
                    "sent": "So this is the decision boundary as given by the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So we have something like a polyhedrin.",
                    "label": 0
                },
                {
                    "sent": "So we design algorithm in the framework from an SVM framework.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So typically we want to design A hyperplane SVM.",
                    "label": 0
                },
                {
                    "sent": "You have a hyperplane classifier with a certain hinge loss.",
                    "label": 0
                },
                {
                    "sent": "So your Alpha is the parameter which characterizes the hyperplane.",
                    "label": 0
                },
                {
                    "sent": "You want to find Alpha and the loss for any example is the distance from the hyperplane.",
                    "label": 0
                },
                {
                    "sent": "So the loss for the example is this.",
                    "label": 0
                },
                {
                    "sent": "The hinge loss, which is the maximum of 0 and 1 -- Y Alpha transpose.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "Zero in the example is correctly classified.",
                    "label": 1
                },
                {
                    "sent": "If the hinge loss is greater than zero, the example is misclassified OK.",
                    "label": 1
                },
                {
                    "sent": "So now we are moving from one classifier to K classifiers.",
                    "label": 1
                },
                {
                    "sent": "And I define epsilon, CIK to be the hinge loss.",
                    "label": 1
                },
                {
                    "sent": "On the example induced by the classifier K. So we set a positive example is classified correctly if all the classifiers say it is positive.",
                    "label": 1
                },
                {
                    "sent": "That means the hinge loss should be equal to 0 by all the classifiers.",
                    "label": 0
                },
                {
                    "sent": "So how do we make the hinge loss loss to be exactly 0 to all the classifiers biamax operation?",
                    "label": 0
                },
                {
                    "sent": "So if the Max is 0.",
                    "label": 0
                },
                {
                    "sent": "That means all of them will be 0.",
                    "label": 0
                },
                {
                    "sent": "So we want to.",
                    "label": 0
                },
                {
                    "sent": "So now instead of.",
                    "label": 0
                },
                {
                    "sent": "This is essential.",
                    "label": 0
                },
                {
                    "sent": "Anything since we said that a negative is negative if it is.",
                    "label": 0
                },
                {
                    "sent": "Hyperplane.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Work for our objective function becomes like this, so there are K classifiers, Alpha and A2 to Alpha K which we want to optimize.",
                    "label": 0
                },
                {
                    "sent": "So this is the standard SVM error on the negative examples.",
                    "label": 1
                },
                {
                    "sent": "It remains the same.",
                    "label": 1
                },
                {
                    "sent": "On positive examples, instead of having just a hinge loss, now we have the Max of the hinge loss of all the other example from all the classifiers.",
                    "label": 0
                },
                {
                    "sent": "And this is an overall regularization term.",
                    "label": 0
                },
                {
                    "sent": "Penalty.",
                    "label": 0
                },
                {
                    "sent": "And this is a convex problem and we can be solved by another optimization routines.",
                    "label": 0
                },
                {
                    "sent": "What we have now designed this.",
                    "label": 0
                },
                {
                    "sent": "We're designed a classifier K classifiers.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "In give your design K linear classifiers for separating out the positive from case different negative subclasses in.",
                    "label": 0
                },
                {
                    "sent": "Essentially, we don't.",
                    "label": 0
                },
                {
                    "sent": "'cause it belongs to.",
                    "label": 0
                },
                {
                    "sent": "Separate the positives from the negatives.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, so the one problem here is that.",
                    "label": 0
                },
                {
                    "sent": "So in this and framework.",
                    "label": 0
                },
                {
                    "sent": "We assume that for every negative subclass, every negative example, we knew what subclass it belonged to, but this is not a realistic scenario.",
                    "label": 0
                },
                {
                    "sent": "For example, in our CAD application we have around like few 100 positives and the number of negatives is of the order of 500,000.",
                    "label": 0
                },
                {
                    "sent": "So a radiologist cannot typically mark all the negatives.",
                    "label": 1
                },
                {
                    "sent": "But what we can do is we can take a subset of these negatives.",
                    "label": 0
                },
                {
                    "sent": "And ask him to mark what kind of different negatives are those.",
                    "label": 0
                },
                {
                    "sent": "So there are three types of examples in our training data.",
                    "label": 1
                },
                {
                    "sent": "We have the positives which we know sure.",
                    "label": 1
                },
                {
                    "sent": "And we know negatives with known subclass membership.",
                    "label": 1
                },
                {
                    "sent": "We said what kind of negatives it was so we can have case such different negatives.",
                    "label": 0
                },
                {
                    "sent": "And then there is a third class of negatives which we do not know the subclass membership.",
                    "label": 1
                },
                {
                    "sent": "So the and algorithm solved the problem only when we knew.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Membership for each.",
                    "label": 0
                },
                {
                    "sent": "Then we knew the exact membership for each negative example.",
                    "label": 0
                },
                {
                    "sent": "Now let's say, how do we account the examples for which we do not know the subclass membership?",
                    "label": 0
                },
                {
                    "sent": "And this is exactly where.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our operation comes into picture.",
                    "label": 0
                },
                {
                    "sent": "So we now have designed a classifier in something called and or framework.",
                    "label": 0
                },
                {
                    "sent": "So this is essentially the same as the previous objective function.",
                    "label": 1
                },
                {
                    "sent": "Every example which we know the subclass information has to be classified as negative by that particular hyperplane.",
                    "label": 0
                },
                {
                    "sent": "The positive new term which we enter here is.",
                    "label": 0
                },
                {
                    "sent": "No, the label what we say is.",
                    "label": 0
                },
                {
                    "sent": "If for a negative example, we do not know which subclass it belongs to, that is which of the K classifiers should correctly classify you say a negative sample is classified correctly if at least one of them says it is correct.",
                    "label": 0
                },
                {
                    "sent": "So if you have K of them, if at least one set it is 0.",
                    "label": 0
                },
                {
                    "sent": "So the notion that the error on by from at least one has to be minimum can be captured exactly by the product here.",
                    "label": 0
                },
                {
                    "sent": "So this is the product of the hinge losses.",
                    "label": 0
                },
                {
                    "sent": "So this is exactly the operation.",
                    "label": 0
                },
                {
                    "sent": "So if a product of terms is equal to if at least one of them is 0, then the product will be 0.",
                    "label": 1
                },
                {
                    "sent": "The and operation is captured by the Max operation.",
                    "label": 1
                },
                {
                    "sent": "The Max of the different errors, hinge losses, and the OR operation is just the product.",
                    "label": 0
                },
                {
                    "sent": "So classic.",
                    "label": 1
                },
                {
                    "sent": "Positives and some negatives which don't have any.",
                    "label": 0
                },
                {
                    "sent": "Will this position problem because of the product term is not convex?",
                    "label": 0
                },
                {
                    "sent": "So the solution?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have this.",
                    "label": 0
                },
                {
                    "sent": "And art.",
                    "label": 0
                },
                {
                    "sent": "At each iteration, since there are K classifiers, we fix the rest of the classifiers and optimize only one of them.",
                    "label": 0
                },
                {
                    "sent": "So once we've fixed all the other classifiers.",
                    "label": 0
                },
                {
                    "sent": "Once we fix all the other classifiers, the problem, the subproblem becomes convex.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So at each iteration, as I said, it's an alternating optimization algorithm.",
                    "label": 0
                },
                {
                    "sent": "So at each iteration we have K steps and each step optimizes.",
                    "label": 1
                },
                {
                    "sent": "So OK, so this is our final algorithm.",
                    "label": 1
                },
                {
                    "sent": "We fixed all the classifiers, but the classifier K and minimized it and keep repeating until we converge.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So our goal here was to design K linear classifiers.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is the.",
                    "label": 0
                },
                {
                    "sent": "This is the functionality optimized.",
                    "label": 0
                },
                {
                    "sent": "Actually an interesting.",
                    "label": 0
                },
                {
                    "sent": "Advantage of using this kind of classifier is so the final rule is once your design all the K classifier you classify an example as positive if all of them say it is positive.",
                    "label": 0
                },
                {
                    "sent": "So we can evaluate each of the K classifiers classifier 1234 till K and say whether all of them positive.",
                    "label": 0
                },
                {
                    "sent": "So the important the order in which you do it doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "Here you can evaluate the classifier in any order you want.",
                    "label": 0
                },
                {
                    "sent": "But let me let us think that each of the classifiers used only a subset of the features.",
                    "label": 0
                },
                {
                    "sent": "In such a scenario, the ordering actually makes a huge difference in terms of your execution time.",
                    "label": 0
                },
                {
                    "sent": "So most of the CAD applications we have.",
                    "label": 0
                },
                {
                    "sent": "The features are not available.",
                    "label": 0
                },
                {
                    "sent": "We have to query for features so you have an image.",
                    "label": 0
                },
                {
                    "sent": "There is a candidate mark and if the classifier needs some features, the algorithm computes it and this the whole system has to run online in real time.",
                    "label": 0
                },
                {
                    "sent": "So feature computation is extremely expensive or typical formulation.",
                    "label": 0
                },
                {
                    "sent": "People uses a cascade of classifiers.",
                    "label": 0
                },
                {
                    "sent": "So the first stage of the Cascade will use only the simplest features.",
                    "label": 0
                },
                {
                    "sent": "The second stage will use more expensive features.",
                    "label": 0
                },
                {
                    "sent": "The third will use the most expensive one.",
                    "label": 0
                },
                {
                    "sent": "So by putting the.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Regularization term to control the penalty here, if you use an L1 penalty.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you set P of Alpha, Kate is going to L1 norm instead of will have K sparse classifiers.",
                    "label": 0
                },
                {
                    "sent": "Each with different nonzero coefficients.",
                    "label": 0
                },
                {
                    "sent": "So if you do, the ordering doesn't matter for the accuracy.",
                    "label": 0
                },
                {
                    "sent": "So if we implement it in a certain order, say start with the classifier that has the least number of non zero coefficients.",
                    "label": 0
                },
                {
                    "sent": "And if you apply the classifier on that, if it is negative, we reject that classifier, and if it is positive with send it to the next stage.",
                    "label": 0
                },
                {
                    "sent": "And in this way implement all the K classes.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is.",
                    "label": 0
                },
                {
                    "sent": "But this is how it looks like, so we had K classifiers.",
                    "label": 0
                },
                {
                    "sent": "So if we implement it in a cascade framework, we have a huge improvement in our runtime of the algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here I will describe some results.",
                    "label": 0
                },
                {
                    "sent": "So we had a 316 patients for training, and we sequestered us another 385 patients for testing.",
                    "label": 0
                },
                {
                    "sent": "And this is kind of an unbalanced problem.",
                    "label": 0
                },
                {
                    "sent": "We are like 226 polyps and lot of negative candidates.",
                    "label": 1
                },
                {
                    "sent": "And for each of the candidate's, there are a bunch of 98 numerical features.",
                    "label": 0
                },
                {
                    "sent": "So let's say there are 1250 negatives, of which only 177 were actually annotated by the radiologist.",
                    "label": 0
                },
                {
                    "sent": "So he told us what kind of negative it is.",
                    "label": 0
                },
                {
                    "sent": "And the rest of the all the negatives.",
                    "label": 0
                },
                {
                    "sent": "We just notice a negative.",
                    "label": 0
                },
                {
                    "sent": "We do not know what subclass it belongs to.",
                    "label": 0
                },
                {
                    "sent": "And totally we identified 9 subclasses.",
                    "label": 1
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here is just the.",
                    "label": 0
                },
                {
                    "sent": "Here are the arosi plots.",
                    "label": 0
                },
                {
                    "sent": "So this is plotting the specificity versus sensitivity is essentially a typical auto Seaport, but in auto see you put 1 minus specificity on the X axis.",
                    "label": 0
                },
                {
                    "sent": "So the black dotted line is the proposed paliative classifier.",
                    "label": 0
                },
                {
                    "sent": "And the green one is a simple SVM with users and RBF kernel.",
                    "label": 0
                },
                {
                    "sent": "It's the nonlinear classifier.",
                    "label": 0
                },
                {
                    "sent": "And the blue dot is in one class, SVM is called SVD.",
                    "label": 0
                },
                {
                    "sent": "And the red one is a simple linear classifier.",
                    "label": 0
                },
                {
                    "sent": "We have shown that by using this multiple class information we can design.",
                    "label": 0
                },
                {
                    "sent": "Classifieds which are more active.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in terms of runtime, we have roughly around 25% gain in execution time over other methods, because a byproduct of our algorithm was that if implemented in a casket framework, it was extremely fast.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to conclude.",
                    "label": 0
                },
                {
                    "sent": "We have shown how to design A polyhedral classifier for multimode data.",
                    "label": 1
                },
                {
                    "sent": "So the end framework can be used when subclass information is fully available.",
                    "label": 1
                },
                {
                    "sent": "The and or framework can incorporate the scenario when the subclass information is partially available.",
                    "label": 0
                },
                {
                    "sent": "And because of this algorithm, the Cascade Design is a byproduct speeds up online execution of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}