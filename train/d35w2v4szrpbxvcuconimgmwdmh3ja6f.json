{
    "id": "d35w2v4szrpbxvcuconimgmwdmh3ja6f",
    "title": "TweetsKB- A Public and Large-Scale RDF Corpus of Annotated Tweets",
    "info": {
        "author": [
            "Vasileios Iosifidis, Leibniz University of Hannover"
        ],
        "published": "July 10, 2018",
        "recorded": "June 2018",
        "category": [
            "Top->Computer Science->Big Data",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/eswc2018_iosifidis_annotated_tweets/",
    "segmentation": [
        [
            "Hello everyone, my name is Russell.",
            "This is Phyllis.",
            "I'm from labels, University of Hannover.",
            "And I'm here to present to each KB public and large scale RDF corpus of annotated tweets.",
            "So."
        ],
        [
            "Microblogging services are part of our daily life, and I bet almost everyone here has used one of them.",
            "Microblogging services combine blogging and miss on messaging, which allows users to interact with each other and this interaction.",
            "Generate shoots amount of data."
        ],
        [
            "So one of the most famous platforms is Twitter of course, and it has been the generated data from Twitter have been used in many applications and research fields.",
            "From data science, sociology, and psychology, and so on.",
            "Researchers.",
            "Governments.",
            "Companies want to take advantage of this data.",
            "Up to today, more than 6 million.",
            "Articles, papers, and books refer to Twitter.",
            "And why is that?"
        ],
        [
            "Each becausw of the generated context.",
            "The amount of data produced per day.",
            "Before 2010, less than 50,000,000 to each word generated by the users per month.",
            "And we see that these numbers are increasing.",
            "Every year and we're talking about more than 300 million tweets per month these days.",
            "So."
        ],
        [
            "How can we use this data?",
            "What if we want to do if we want to extract entity relations from this data, how can we do it for example?",
            "Given the entity of Barack Obama how can we find with which entities he's related in specific time periods?",
            "Or"
        ],
        [
            "How can we track down the popularity of specific entities such as Trump, let's say?",
            "That we want to track down the popularity during he, the presidential elections.",
            "Or if we want to compare the popularity.",
            "His popularity and healers popularity.",
            "Right?",
            "So."
        ],
        [
            "There is an analysis conducted by two journalists in the end of 2016.",
            "In which they wanted to find which places or persons were insulted by Trump from Twitter.",
            "So what they have done is that they downloaded 40.",
            "Yeah.",
            "14,000 tweets of his account.",
            "And they have manually labeled.",
            "Each and everyone of his tweets.",
            "And they have done this manually.",
            "And.",
            "As we can all assume, this is a very costly process.",
            "It's very very expensive in.",
            "Terms of resource and time.",
            "So this is prohibited when we're having all these cool machine learning algorithms for this stuff."
        ],
        [
            "So as we see, there is a lack of large scale public social archives which facilitate annotated data.",
            "For example.",
            "Twitter."
        ],
        [
            "In our case, so that's why we generated to each KB, which is a publicly available RDF corpus of anonymized.",
            "It's of course because it's illegal.",
            "So it has more than 1 1/2 billion tweets which form the period of January 2013 up to November 2017.",
            "And it contains entities and sentiment annotations.",
            "And of course you will ask why do I want to use such a corpus?",
            "So.",
            "You can perform time Aware entity centric queries to this corpus.",
            "You can generate time series of different.",
            "Of opinionated streams time series, sorry.",
            "You can also integrate.",
            "The corpus, with existing knowledge basis such as DB pedia."
        ],
        [
            "And now let's see how we generated this corpus.",
            "First of all, in my research are there we have cluster and we store the tweets of every every day too.",
            "It's from the publicly available stream that Twitter provides this 1% so from 2013.",
            "Up to November 2017, there were six billion tweets.",
            "And from this we are removed.",
            "We have removed the non English tweets.",
            "Very tweets.",
            "And the spam tweets.",
            "Force detecting spam spam twitch.",
            "We have used the eight spam data set.",
            "Which is.",
            "From the period of 2013.",
            "It contains also tweets and it has this binary.",
            "Binary binary label.",
            "If it is spam or not.",
            "So we have trained them multinomial naive by yes.",
            "On this data set and we have applied it to our corpus and we removed.",
            "All these pumps, so we ended up with 1 1/2 billion tweets English which."
        ],
        [
            "Now I said that we our corpus contains entities.",
            "And sentimental notations.",
            "For extracting, extracting the entities we have used yahooz fell too.",
            "Which is specifically designed for short text.",
            "And for for short text.",
            "This tool produced 1.4 million distinct entities from the overall corpus.",
            "And we also evaluated this to see how good it is using the nil data set ground truth, which is.",
            "Also from Twitter and it has 9000 English tweets from 2000, eleven, 1314 and 15.",
            "So this ground truth has the text and the manually annotated entities.",
            "So the precision was 85% with recall 39%.",
            "For the Yahoo."
        ],
        [
            "Funding for sentiment annotation.",
            "We have used Sandy strength.",
            "And these two also has been specifically designed for certex.",
            "It generates 2 scores given a text, so goes for the positive ones from 5 up to from one up to five and for the negative one negative ones from minus one to minus five.",
            "Based on this sentimentality, which means how strong is the sentiment inside the.",
            "Sentence.",
            "And we've also evaluated this two using this sentiment.",
            "15 We took the ground truth.",
            "Which is again from Twitter and has 2.5 million English myths.",
            "And we show that its accuracy is 91%."
        ],
        [
            "And the key.",
            "Let's see some.",
            "Overall statistics, we have uploaded the data set.",
            "As in three files to another repository.",
            "And we've also registered it at Datahub.",
            "And we see here the statistics of our corpus we are talking about.",
            "1.5 billion tweets.",
            "The distinct users inside this corpus is 125 millions, but this week has tags which have been used by the users.",
            "And yeah, it's 40 millions, almost 41.",
            "The distinct user mentioned.",
            "Inside the text you mentioned, users from Twitter users actually is NT 1,000,000.",
            "Then you think entities that they fail to produce these 1.4.",
            "And the sentiment that which which contains sentiment is almost 800 millions.",
            "And what do we mean?",
            "They contain sentiment.",
            "Is that both of the scores, the positive and the negative?",
            "Their difference was not zero because if we have.",
            "Right, which has one positive score and minus one positive score so negative score so it's 0 does not convince neutral.",
            "So the amount of generated triplets.",
            "Of these statistics here, of these numbers is 48 billion triplets in the end, so the overall.",
            "Corpus contains 48 billion triplets."
        ],
        [
            "Now about the computational cost.",
            "We've done this.",
            "Distributed becauses we're talking about huge amount of data and the overall process is very expensive.",
            "So we have a cluster with 500 CPU's, 7 terabytes of RAM.",
            "And now for.",
            "Sentimental imitation.",
            "The throughput from the Hadoop cluster was 6 million tweets per minute and for the file two was almost 5 million tweets per minute.",
            "And the different triplication was the easy part.",
            "14 million tweets."
        ],
        [
            "And now let's see our schema here.",
            "We have used already existed.",
            "Vocabularies such as ciok scheme, organ tonics.",
            "We use this because we didn't want to violate schema schema.",
            "The yeah we want to avoid schema violations.",
            "Another way we've done this is because we want it differentiable your eyes and also the ability of of extensibility.",
            "Can extend so."
        ],
        [
            "Let's see.",
            "Which meta data we have used.",
            "We have used the tweet ID.",
            "The username.",
            "For which we have encrypted the username.",
            "Because we want to avoid any privacy issue may arise even though Twitter states about the context, it's prohibited to share.",
            "So in any case we encrypted this as well.",
            "For the entity extracted by fell, we're storing three things.",
            "We are storing the surface form the one which is found in the text.",
            "The UI that corresponds to this entity and the confidence score of the filter so.",
            "Someone wants to go for high precision.",
            "They can adjust this threshold and they can obtain different results.",
            "We also store the mentions of the text again encrypted the hashtags.",
            "The sentiment scores, positive and negative.",
            "And some interesting interaction statistics, such as the favorite count of the tweet or the retweet count of the tweet.",
            "And these numbers correspond to the.",
            "Actually two when we download.",
            "The tweets every day.",
            "These numbers will be the ones that are provided by Twitter at that point in the next day.",
            "So it could be retweeted more times, but we don't have the ones that are given at that point from Twitter.",
            "Same for the favorite color."
        ],
        [
            "But now here is an example of a tweet.",
            "We have the tweet, tweet, tweet ID 121.",
            "We have the sentiment provided by Sandy strength and we split.",
            "We have the positive emotion and the negative emotion.",
            "And let's say that there are three entities which appear inside this.",
            "Tweet one could be Roger Federer.",
            "Which had been located as federal inside the text with minus 1.5 threshold.",
            "Answer."
        ],
        [
            "Show will show.",
            "Provide some analysis for the top 100,000 entities and hastags the most frequent ones in our corpus.",
            "And we show that for the end of this, the first 10,000 have also more than appear more than 10,000 inside the corpus.",
            "And for the hashtags, the first 5000 appear more than 10,000 times in the corpus."
        ],
        [
            "And also we check the type of the entities we fabricated the again the 1st.",
            "100,000.",
            "The most frequent and we show that 21% corresponds to persons 15% to organizations 8 two locations, and so on."
        ],
        [
            "OK, how can we use now this corpus?",
            "Right?",
            "We can construct sparkuhl queries.",
            "In which we can also integrate information from existing knowledge basis.",
            "Here we are quitting.",
            "DB Pedia and we want to find.",
            "German politicians.",
            "In the year of 2016.",
            "Which have negative sentiment above this number.",
            "So at the query level at the Curie time we can integrate other.",
            "Knowledge basis."
        ],
        [
            "What does we can do?",
            "Another interesting case study is we can track down the sentiment.",
            "Overtime.",
            "For example, for fixed granularity of time, let's say monthly, we can find for the period of 2015 the.",
            "Popularity of our latest repairs.",
            "Alex Cypress Creek Prime Minister in the year of 2015.",
            "Or we can do it weekly or daily so we can also generate.",
            "And data streams.",
            "By this I mean we can generate time series with this.",
            "Option for the specific case and then.",
            "I possible.",
            "Case would be to fit this time series to a machine learning algorithm and do predictions overtime.",
            "Many options."
        ],
        [
            "In addition, another interesting case would be entity recommendation.",
            "So given an entity, find the.",
            "Call cured other entities for the specific time time span that you're setting.",
            "Again, we have an extra bus here.",
            "And we've.",
            "What we have done is that we aggregated.",
            "Then the coral the Co cured entities and we created the graph.",
            "And overtime graph.",
            "We set the granularity to three months, so four from April until June.",
            "We found the top 10.",
            "We found the top 10.",
            "They found the top ten correlated entities and we see that it's Greece, Athens, Reuters.",
            "And at that.",
            "We see that there was the.",
            "The referendum that took place and the possibility of Brexit.",
            "And whatever.",
            "The next three months we see that these entities here change, and the other three months.",
            "They also change some remain.",
            "Some are persisting somewhere, some other entities are more ephemeral, so.",
            "It could be also applied for in the recommendations."
        ],
        [
            "As a case study.",
            "Now what I said.",
            "We have deposed this corpus designado, and we have another also.",
            "And register it at the hub.",
            "We have integrated this also to our projects we have in our Department.",
            "The after project in the Alexandria project.",
            "And in addition, we are.",
            "Providing an end point.",
            "Which currently facilitates 5% of the overall corpus.",
            "Here.",
            "And the five percent is aggregated monthly, so every month for every month we took 5% of its content.",
            "And via.",
            "We will.",
            "Update this corpus every six months.",
            "Or four depends of the.",
            "Time here and so we want to keep an updated corpus.",
            "That's why we will try to update every six months.",
            "And in addition, we there were some comments.",
            "And we will try now to compress.",
            "Our corpus with some specific structures to facilitate 100% of the corpus.",
            "With our endpoint from our endpoint."
        ],
        [
            "So.",
            "We saw.",
            "To escape, be one of the largest, if not the largest, annotating 30th corpus.",
            "We can.",
            "Create with Sparkle queries advanced and it is a decant I'm aware.",
            "Analysis We can you can do.",
            "Time aware and it is centric analysis.",
            "We can integrate.",
            "Different knowledge basis with our corpus and it has, I believe, many applications in research area so.",
            "Industry.",
            "Anything you can think.",
            "So."
        ],
        [
            "Thanks for your time."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hello everyone, my name is Russell.",
                    "label": 0
                },
                {
                    "sent": "This is Phyllis.",
                    "label": 0
                },
                {
                    "sent": "I'm from labels, University of Hannover.",
                    "label": 0
                },
                {
                    "sent": "And I'm here to present to each KB public and large scale RDF corpus of annotated tweets.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Microblogging services are part of our daily life, and I bet almost everyone here has used one of them.",
                    "label": 1
                },
                {
                    "sent": "Microblogging services combine blogging and miss on messaging, which allows users to interact with each other and this interaction.",
                    "label": 0
                },
                {
                    "sent": "Generate shoots amount of data.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one of the most famous platforms is Twitter of course, and it has been the generated data from Twitter have been used in many applications and research fields.",
                    "label": 1
                },
                {
                    "sent": "From data science, sociology, and psychology, and so on.",
                    "label": 1
                },
                {
                    "sent": "Researchers.",
                    "label": 0
                },
                {
                    "sent": "Governments.",
                    "label": 0
                },
                {
                    "sent": "Companies want to take advantage of this data.",
                    "label": 0
                },
                {
                    "sent": "Up to today, more than 6 million.",
                    "label": 0
                },
                {
                    "sent": "Articles, papers, and books refer to Twitter.",
                    "label": 0
                },
                {
                    "sent": "And why is that?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Each becausw of the generated context.",
                    "label": 0
                },
                {
                    "sent": "The amount of data produced per day.",
                    "label": 0
                },
                {
                    "sent": "Before 2010, less than 50,000,000 to each word generated by the users per month.",
                    "label": 0
                },
                {
                    "sent": "And we see that these numbers are increasing.",
                    "label": 0
                },
                {
                    "sent": "Every year and we're talking about more than 300 million tweets per month these days.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How can we use this data?",
                    "label": 0
                },
                {
                    "sent": "What if we want to do if we want to extract entity relations from this data, how can we do it for example?",
                    "label": 0
                },
                {
                    "sent": "Given the entity of Barack Obama how can we find with which entities he's related in specific time periods?",
                    "label": 0
                },
                {
                    "sent": "Or",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How can we track down the popularity of specific entities such as Trump, let's say?",
                    "label": 0
                },
                {
                    "sent": "That we want to track down the popularity during he, the presidential elections.",
                    "label": 0
                },
                {
                    "sent": "Or if we want to compare the popularity.",
                    "label": 0
                },
                {
                    "sent": "His popularity and healers popularity.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is an analysis conducted by two journalists in the end of 2016.",
                    "label": 0
                },
                {
                    "sent": "In which they wanted to find which places or persons were insulted by Trump from Twitter.",
                    "label": 0
                },
                {
                    "sent": "So what they have done is that they downloaded 40.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "14,000 tweets of his account.",
                    "label": 0
                },
                {
                    "sent": "And they have manually labeled.",
                    "label": 0
                },
                {
                    "sent": "Each and everyone of his tweets.",
                    "label": 0
                },
                {
                    "sent": "And they have done this manually.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "As we can all assume, this is a very costly process.",
                    "label": 0
                },
                {
                    "sent": "It's very very expensive in.",
                    "label": 0
                },
                {
                    "sent": "Terms of resource and time.",
                    "label": 0
                },
                {
                    "sent": "So this is prohibited when we're having all these cool machine learning algorithms for this stuff.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as we see, there is a lack of large scale public social archives which facilitate annotated data.",
                    "label": 1
                },
                {
                    "sent": "For example.",
                    "label": 0
                },
                {
                    "sent": "Twitter.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In our case, so that's why we generated to each KB, which is a publicly available RDF corpus of anonymized.",
                    "label": 1
                },
                {
                    "sent": "It's of course because it's illegal.",
                    "label": 0
                },
                {
                    "sent": "So it has more than 1 1/2 billion tweets which form the period of January 2013 up to November 2017.",
                    "label": 0
                },
                {
                    "sent": "And it contains entities and sentiment annotations.",
                    "label": 1
                },
                {
                    "sent": "And of course you will ask why do I want to use such a corpus?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "You can perform time Aware entity centric queries to this corpus.",
                    "label": 0
                },
                {
                    "sent": "You can generate time series of different.",
                    "label": 0
                },
                {
                    "sent": "Of opinionated streams time series, sorry.",
                    "label": 0
                },
                {
                    "sent": "You can also integrate.",
                    "label": 0
                },
                {
                    "sent": "The corpus, with existing knowledge basis such as DB pedia.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now let's see how we generated this corpus.",
                    "label": 0
                },
                {
                    "sent": "First of all, in my research are there we have cluster and we store the tweets of every every day too.",
                    "label": 0
                },
                {
                    "sent": "It's from the publicly available stream that Twitter provides this 1% so from 2013.",
                    "label": 0
                },
                {
                    "sent": "Up to November 2017, there were six billion tweets.",
                    "label": 0
                },
                {
                    "sent": "And from this we are removed.",
                    "label": 0
                },
                {
                    "sent": "We have removed the non English tweets.",
                    "label": 0
                },
                {
                    "sent": "Very tweets.",
                    "label": 0
                },
                {
                    "sent": "And the spam tweets.",
                    "label": 0
                },
                {
                    "sent": "Force detecting spam spam twitch.",
                    "label": 0
                },
                {
                    "sent": "We have used the eight spam data set.",
                    "label": 0
                },
                {
                    "sent": "Which is.",
                    "label": 0
                },
                {
                    "sent": "From the period of 2013.",
                    "label": 0
                },
                {
                    "sent": "It contains also tweets and it has this binary.",
                    "label": 0
                },
                {
                    "sent": "Binary binary label.",
                    "label": 0
                },
                {
                    "sent": "If it is spam or not.",
                    "label": 0
                },
                {
                    "sent": "So we have trained them multinomial naive by yes.",
                    "label": 0
                },
                {
                    "sent": "On this data set and we have applied it to our corpus and we removed.",
                    "label": 0
                },
                {
                    "sent": "All these pumps, so we ended up with 1 1/2 billion tweets English which.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now I said that we our corpus contains entities.",
                    "label": 0
                },
                {
                    "sent": "And sentimental notations.",
                    "label": 0
                },
                {
                    "sent": "For extracting, extracting the entities we have used yahooz fell too.",
                    "label": 0
                },
                {
                    "sent": "Which is specifically designed for short text.",
                    "label": 1
                },
                {
                    "sent": "And for for short text.",
                    "label": 1
                },
                {
                    "sent": "This tool produced 1.4 million distinct entities from the overall corpus.",
                    "label": 1
                },
                {
                    "sent": "And we also evaluated this to see how good it is using the nil data set ground truth, which is.",
                    "label": 0
                },
                {
                    "sent": "Also from Twitter and it has 9000 English tweets from 2000, eleven, 1314 and 15.",
                    "label": 0
                },
                {
                    "sent": "So this ground truth has the text and the manually annotated entities.",
                    "label": 0
                },
                {
                    "sent": "So the precision was 85% with recall 39%.",
                    "label": 0
                },
                {
                    "sent": "For the Yahoo.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Funding for sentiment annotation.",
                    "label": 0
                },
                {
                    "sent": "We have used Sandy strength.",
                    "label": 1
                },
                {
                    "sent": "And these two also has been specifically designed for certex.",
                    "label": 0
                },
                {
                    "sent": "It generates 2 scores given a text, so goes for the positive ones from 5 up to from one up to five and for the negative one negative ones from minus one to minus five.",
                    "label": 0
                },
                {
                    "sent": "Based on this sentimentality, which means how strong is the sentiment inside the.",
                    "label": 0
                },
                {
                    "sent": "Sentence.",
                    "label": 0
                },
                {
                    "sent": "And we've also evaluated this two using this sentiment.",
                    "label": 0
                },
                {
                    "sent": "15 We took the ground truth.",
                    "label": 1
                },
                {
                    "sent": "Which is again from Twitter and has 2.5 million English myths.",
                    "label": 0
                },
                {
                    "sent": "And we show that its accuracy is 91%.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the key.",
                    "label": 0
                },
                {
                    "sent": "Let's see some.",
                    "label": 0
                },
                {
                    "sent": "Overall statistics, we have uploaded the data set.",
                    "label": 0
                },
                {
                    "sent": "As in three files to another repository.",
                    "label": 0
                },
                {
                    "sent": "And we've also registered it at Datahub.",
                    "label": 0
                },
                {
                    "sent": "And we see here the statistics of our corpus we are talking about.",
                    "label": 0
                },
                {
                    "sent": "1.5 billion tweets.",
                    "label": 0
                },
                {
                    "sent": "The distinct users inside this corpus is 125 millions, but this week has tags which have been used by the users.",
                    "label": 0
                },
                {
                    "sent": "And yeah, it's 40 millions, almost 41.",
                    "label": 0
                },
                {
                    "sent": "The distinct user mentioned.",
                    "label": 0
                },
                {
                    "sent": "Inside the text you mentioned, users from Twitter users actually is NT 1,000,000.",
                    "label": 0
                },
                {
                    "sent": "Then you think entities that they fail to produce these 1.4.",
                    "label": 0
                },
                {
                    "sent": "And the sentiment that which which contains sentiment is almost 800 millions.",
                    "label": 0
                },
                {
                    "sent": "And what do we mean?",
                    "label": 0
                },
                {
                    "sent": "They contain sentiment.",
                    "label": 0
                },
                {
                    "sent": "Is that both of the scores, the positive and the negative?",
                    "label": 0
                },
                {
                    "sent": "Their difference was not zero because if we have.",
                    "label": 0
                },
                {
                    "sent": "Right, which has one positive score and minus one positive score so negative score so it's 0 does not convince neutral.",
                    "label": 0
                },
                {
                    "sent": "So the amount of generated triplets.",
                    "label": 0
                },
                {
                    "sent": "Of these statistics here, of these numbers is 48 billion triplets in the end, so the overall.",
                    "label": 0
                },
                {
                    "sent": "Corpus contains 48 billion triplets.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now about the computational cost.",
                    "label": 1
                },
                {
                    "sent": "We've done this.",
                    "label": 0
                },
                {
                    "sent": "Distributed becauses we're talking about huge amount of data and the overall process is very expensive.",
                    "label": 0
                },
                {
                    "sent": "So we have a cluster with 500 CPU's, 7 terabytes of RAM.",
                    "label": 0
                },
                {
                    "sent": "And now for.",
                    "label": 0
                },
                {
                    "sent": "Sentimental imitation.",
                    "label": 0
                },
                {
                    "sent": "The throughput from the Hadoop cluster was 6 million tweets per minute and for the file two was almost 5 million tweets per minute.",
                    "label": 1
                },
                {
                    "sent": "And the different triplication was the easy part.",
                    "label": 0
                },
                {
                    "sent": "14 million tweets.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now let's see our schema here.",
                    "label": 0
                },
                {
                    "sent": "We have used already existed.",
                    "label": 0
                },
                {
                    "sent": "Vocabularies such as ciok scheme, organ tonics.",
                    "label": 0
                },
                {
                    "sent": "We use this because we didn't want to violate schema schema.",
                    "label": 0
                },
                {
                    "sent": "The yeah we want to avoid schema violations.",
                    "label": 0
                },
                {
                    "sent": "Another way we've done this is because we want it differentiable your eyes and also the ability of of extensibility.",
                    "label": 0
                },
                {
                    "sent": "Can extend so.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's see.",
                    "label": 0
                },
                {
                    "sent": "Which meta data we have used.",
                    "label": 0
                },
                {
                    "sent": "We have used the tweet ID.",
                    "label": 0
                },
                {
                    "sent": "The username.",
                    "label": 0
                },
                {
                    "sent": "For which we have encrypted the username.",
                    "label": 0
                },
                {
                    "sent": "Because we want to avoid any privacy issue may arise even though Twitter states about the context, it's prohibited to share.",
                    "label": 0
                },
                {
                    "sent": "So in any case we encrypted this as well.",
                    "label": 0
                },
                {
                    "sent": "For the entity extracted by fell, we're storing three things.",
                    "label": 0
                },
                {
                    "sent": "We are storing the surface form the one which is found in the text.",
                    "label": 1
                },
                {
                    "sent": "The UI that corresponds to this entity and the confidence score of the filter so.",
                    "label": 0
                },
                {
                    "sent": "Someone wants to go for high precision.",
                    "label": 0
                },
                {
                    "sent": "They can adjust this threshold and they can obtain different results.",
                    "label": 0
                },
                {
                    "sent": "We also store the mentions of the text again encrypted the hashtags.",
                    "label": 0
                },
                {
                    "sent": "The sentiment scores, positive and negative.",
                    "label": 1
                },
                {
                    "sent": "And some interesting interaction statistics, such as the favorite count of the tweet or the retweet count of the tweet.",
                    "label": 1
                },
                {
                    "sent": "And these numbers correspond to the.",
                    "label": 0
                },
                {
                    "sent": "Actually two when we download.",
                    "label": 0
                },
                {
                    "sent": "The tweets every day.",
                    "label": 0
                },
                {
                    "sent": "These numbers will be the ones that are provided by Twitter at that point in the next day.",
                    "label": 0
                },
                {
                    "sent": "So it could be retweeted more times, but we don't have the ones that are given at that point from Twitter.",
                    "label": 0
                },
                {
                    "sent": "Same for the favorite color.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But now here is an example of a tweet.",
                    "label": 0
                },
                {
                    "sent": "We have the tweet, tweet, tweet ID 121.",
                    "label": 0
                },
                {
                    "sent": "We have the sentiment provided by Sandy strength and we split.",
                    "label": 0
                },
                {
                    "sent": "We have the positive emotion and the negative emotion.",
                    "label": 0
                },
                {
                    "sent": "And let's say that there are three entities which appear inside this.",
                    "label": 0
                },
                {
                    "sent": "Tweet one could be Roger Federer.",
                    "label": 0
                },
                {
                    "sent": "Which had been located as federal inside the text with minus 1.5 threshold.",
                    "label": 0
                },
                {
                    "sent": "Answer.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Show will show.",
                    "label": 0
                },
                {
                    "sent": "Provide some analysis for the top 100,000 entities and hastags the most frequent ones in our corpus.",
                    "label": 1
                },
                {
                    "sent": "And we show that for the end of this, the first 10,000 have also more than appear more than 10,000 inside the corpus.",
                    "label": 0
                },
                {
                    "sent": "And for the hashtags, the first 5000 appear more than 10,000 times in the corpus.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And also we check the type of the entities we fabricated the again the 1st.",
                    "label": 0
                },
                {
                    "sent": "100,000.",
                    "label": 0
                },
                {
                    "sent": "The most frequent and we show that 21% corresponds to persons 15% to organizations 8 two locations, and so on.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, how can we use now this corpus?",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "We can construct sparkuhl queries.",
                    "label": 0
                },
                {
                    "sent": "In which we can also integrate information from existing knowledge basis.",
                    "label": 0
                },
                {
                    "sent": "Here we are quitting.",
                    "label": 0
                },
                {
                    "sent": "DB Pedia and we want to find.",
                    "label": 0
                },
                {
                    "sent": "German politicians.",
                    "label": 0
                },
                {
                    "sent": "In the year of 2016.",
                    "label": 0
                },
                {
                    "sent": "Which have negative sentiment above this number.",
                    "label": 0
                },
                {
                    "sent": "So at the query level at the Curie time we can integrate other.",
                    "label": 0
                },
                {
                    "sent": "Knowledge basis.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What does we can do?",
                    "label": 0
                },
                {
                    "sent": "Another interesting case study is we can track down the sentiment.",
                    "label": 0
                },
                {
                    "sent": "Overtime.",
                    "label": 0
                },
                {
                    "sent": "For example, for fixed granularity of time, let's say monthly, we can find for the period of 2015 the.",
                    "label": 0
                },
                {
                    "sent": "Popularity of our latest repairs.",
                    "label": 0
                },
                {
                    "sent": "Alex Cypress Creek Prime Minister in the year of 2015.",
                    "label": 0
                },
                {
                    "sent": "Or we can do it weekly or daily so we can also generate.",
                    "label": 0
                },
                {
                    "sent": "And data streams.",
                    "label": 0
                },
                {
                    "sent": "By this I mean we can generate time series with this.",
                    "label": 0
                },
                {
                    "sent": "Option for the specific case and then.",
                    "label": 0
                },
                {
                    "sent": "I possible.",
                    "label": 0
                },
                {
                    "sent": "Case would be to fit this time series to a machine learning algorithm and do predictions overtime.",
                    "label": 0
                },
                {
                    "sent": "Many options.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In addition, another interesting case would be entity recommendation.",
                    "label": 0
                },
                {
                    "sent": "So given an entity, find the.",
                    "label": 0
                },
                {
                    "sent": "Call cured other entities for the specific time time span that you're setting.",
                    "label": 0
                },
                {
                    "sent": "Again, we have an extra bus here.",
                    "label": 0
                },
                {
                    "sent": "And we've.",
                    "label": 0
                },
                {
                    "sent": "What we have done is that we aggregated.",
                    "label": 0
                },
                {
                    "sent": "Then the coral the Co cured entities and we created the graph.",
                    "label": 0
                },
                {
                    "sent": "And overtime graph.",
                    "label": 0
                },
                {
                    "sent": "We set the granularity to three months, so four from April until June.",
                    "label": 0
                },
                {
                    "sent": "We found the top 10.",
                    "label": 0
                },
                {
                    "sent": "We found the top 10.",
                    "label": 0
                },
                {
                    "sent": "They found the top ten correlated entities and we see that it's Greece, Athens, Reuters.",
                    "label": 0
                },
                {
                    "sent": "And at that.",
                    "label": 0
                },
                {
                    "sent": "We see that there was the.",
                    "label": 0
                },
                {
                    "sent": "The referendum that took place and the possibility of Brexit.",
                    "label": 0
                },
                {
                    "sent": "And whatever.",
                    "label": 0
                },
                {
                    "sent": "The next three months we see that these entities here change, and the other three months.",
                    "label": 0
                },
                {
                    "sent": "They also change some remain.",
                    "label": 0
                },
                {
                    "sent": "Some are persisting somewhere, some other entities are more ephemeral, so.",
                    "label": 0
                },
                {
                    "sent": "It could be also applied for in the recommendations.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As a case study.",
                    "label": 0
                },
                {
                    "sent": "Now what I said.",
                    "label": 0
                },
                {
                    "sent": "We have deposed this corpus designado, and we have another also.",
                    "label": 0
                },
                {
                    "sent": "And register it at the hub.",
                    "label": 0
                },
                {
                    "sent": "We have integrated this also to our projects we have in our Department.",
                    "label": 0
                },
                {
                    "sent": "The after project in the Alexandria project.",
                    "label": 0
                },
                {
                    "sent": "And in addition, we are.",
                    "label": 0
                },
                {
                    "sent": "Providing an end point.",
                    "label": 0
                },
                {
                    "sent": "Which currently facilitates 5% of the overall corpus.",
                    "label": 1
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "And the five percent is aggregated monthly, so every month for every month we took 5% of its content.",
                    "label": 0
                },
                {
                    "sent": "And via.",
                    "label": 0
                },
                {
                    "sent": "We will.",
                    "label": 0
                },
                {
                    "sent": "Update this corpus every six months.",
                    "label": 0
                },
                {
                    "sent": "Or four depends of the.",
                    "label": 0
                },
                {
                    "sent": "Time here and so we want to keep an updated corpus.",
                    "label": 0
                },
                {
                    "sent": "That's why we will try to update every six months.",
                    "label": 0
                },
                {
                    "sent": "And in addition, we there were some comments.",
                    "label": 0
                },
                {
                    "sent": "And we will try now to compress.",
                    "label": 0
                },
                {
                    "sent": "Our corpus with some specific structures to facilitate 100% of the corpus.",
                    "label": 0
                },
                {
                    "sent": "With our endpoint from our endpoint.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We saw.",
                    "label": 0
                },
                {
                    "sent": "To escape, be one of the largest, if not the largest, annotating 30th corpus.",
                    "label": 0
                },
                {
                    "sent": "We can.",
                    "label": 0
                },
                {
                    "sent": "Create with Sparkle queries advanced and it is a decant I'm aware.",
                    "label": 0
                },
                {
                    "sent": "Analysis We can you can do.",
                    "label": 0
                },
                {
                    "sent": "Time aware and it is centric analysis.",
                    "label": 0
                },
                {
                    "sent": "We can integrate.",
                    "label": 0
                },
                {
                    "sent": "Different knowledge basis with our corpus and it has, I believe, many applications in research area so.",
                    "label": 0
                },
                {
                    "sent": "Industry.",
                    "label": 0
                },
                {
                    "sent": "Anything you can think.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks for your time.",
                    "label": 0
                }
            ]
        }
    }
}