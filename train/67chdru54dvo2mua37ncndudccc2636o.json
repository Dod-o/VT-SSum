{
    "id": "67chdru54dvo2mua37ncndudccc2636o",
    "title": "Bayesian Neural Nets",
    "info": {
        "author": [
            "Andrew Gordon Wilson, Department of Computer Science, Cornell University"
        ],
        "published": "Oct. 11, 2018",
        "recorded": "July 2018",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/DLRLsummerschool2018_wilson_bayesian_neural_nets/",
    "segmentation": [
        [
            "Thank you, it's really great to be here like many of the speakers, it seems I really started my PhD at a machine learning summer school, in this case at Cambridge in the UK, and so you know, it really helped me think about research directions for the next sort of five or six years.",
            "Now I'd like to ask if you've ever wished you could go back in time and tell yourself something.",
            "Don't do it.",
            "I've certainly had that feeling many times, and part of this talk will actually be what you know.",
            "I would say if I could go back to that summer school in 2009 and just say a few things to myself and so some of that will be a bit tutorial in flavor, some of it will be very high level considerations about model construction and model development.",
            "How do we think about building models that generalize from first principles perspective and some of it will be about some of our latest research in the space of Bayesian deep learning.",
            "So I do."
        ],
        [
            "To start with the question, let's suppose we have this observed data set of airline passenger numbers indexed by time.",
            "Since the time series of Dots are observed data points and we'd like to fit this data so that we can make a good extrapolation.",
            "Say we want to predict airline passenger numbers in 1961, and we're going to consider three choices, choice, one linear Model choice, 2A cubic polynomial Choice 3 or 10 thousandth order polynomial.",
            "So I'd like to show of hands.",
            "How many want to go with choice one?",
            "Choice 2.",
            "OK choice 3.",
            "Interesting.",
            "OK, so I would say about 70% choice, 110% choice 2.",
            "10% choice 310% shy.",
            "OK, very interesting so.",
            "You know those who go with choice one I think are worried about what's called overfitting.",
            "They want something that will generalize reasonably well.",
            "They understand they're probably going to some structure in the data, but there's a trade off those who go with choice two, you know typically don't want to miss out on some of the interesting structure there.",
            "We see quasi periodic trends, etc.",
            "We're going to miss those if we use a linear function, so we'll go with this too.",
            "And people with go with choice three are very worried about sort of really missing structure.",
            "I don't think their choice.",
            "Is very popular typically, but in this talk I'm going to argue that that's the choice we want to make an.",
            "In fact, if we could, we would want to go with an infinite order polynomial, and you know, if we could, we would want something even more expressive than that.",
            "And the key to unlocking this seeming contradiction about how we can go with an expressive model and not overfit really rests with trying to separately understand model specification and how we do learning with those models.",
            "And so my sort of rationalization for wanting to go with the 10,000.",
            "Order polynomial or a very expressive.",
            "Model classes because we believe that actually no matter which choice we make, the truth is probably outside of the model class, but with a very expressive model will be able to get closer to the truth than we could with a simpler model.",
            "And in fact choices one and two here are special cases of the 10,000 TH order polynomial.",
            "There's some setting of the parameters of that big polynomial such that we can exactly recover the smaller polynomials, and so it just rests in how we're trying to sort of find those parameters and.",
            "Additionally, you know often we can do a little bit better if we add a bit of extra structure to our to our models, so Radford Neal has a beautiful passage of his 1996 thesis on Bayesian neural networks where he says that you know, no matter how well we're doing, for example, in performing character recognition, we could do a bit better if we just accounted for something extra.",
            "You know, irregular writing styles, weird inkblots, whatever it might be.",
            "There's always something we can add to our model to get better performance.",
            "OK, so I'll come back to that shortly."
        ],
        [
            "So here I have a cartoon from ex KCD sort of in a very broad stroke way, trying to show a difference between the classical in a Bayesian approach.",
            "So you've got two guys talking.",
            "The first guy says the neutrino detector measures whether the sun is gone Nova.",
            "The other guy says then it rolls two dice.",
            "If they both come up six, it lies to us, otherwise it tells the truth.",
            "So the other guy says let's try it.",
            "Detector is the sun going Nova?",
            "We hear some dice rolling.",
            "It says yes.",
            "The frequentist says, well, the probability of this result happening by random chance is 1 / 36.",
            "16 squared, so the P value is less than .05.",
            "I conclude that the sun is exploded.",
            "The other guy says, you know that you $50.",
            "It hasn't now different people in different areas have different reactions to this cartoon.",
            "If you ask, a game theorist will say, well, you know anything, right?",
            "Like what more could I lose if the sun is exploded?",
            "Now I actually don't think that sometimes you know if we think if we're being very thoughtful, there's necessarily a profound difference between the Bayesian classical approaches.",
            "Both are actually using priors, just in somewhat different different ways here.",
            "The intuition, I suppose in this case, is that our prior even just based on, you know, vague daily experiences that it's very unlikely that the sun is just going to spontaneously go Nova, but we probably had a lot of experience.",
            "For example, playing board games and throwing 2 sixes, and so we just feel you know intuitively that that it's much more likely that this is just happened by random chance, then the sun has has gone Nova and we can actually make that intuition very precise, and we can do hypothesis testing, and we can compare the posterior probabilities of those two hypothesis.",
            "Under very reasonable assumptions, for example, using domain expertise in physics about Suns, stars going Nova, etc and compute that one is billions of times more likely than the other.",
            "OK."
        ],
        [
            "So a bit of motivation.",
            "You know why do we care about Bayesian deep learning?",
            "And why do we care about Bayesian machine learning?",
            "I guess so a lot of the same points would apply, so it's a powerful and coherent framework for model construction and understanding generalization.",
            "We've had a number of talks showing all the great things we can do with neural networks, but also a lot of it has been mysterious.",
            "You know why is it that models with more parameters than data points can generalize, etc?",
            "And we can actually.",
            "Make those results a bit less mysterious if we view things from a probabilistic Bayesian perspective.",
            "Sometimes I'll use the word probabilistic and Bayesian interchangeably.",
            "It depends on the context uncertainty representation I think is a well understood advantage of a Bayesian approach.",
            "It can be achieved through other means as well, but this is something that's important if we're building models to make decisions, and one might argue from a practical perspective that everything in the end is ultimately to make some kind of decision.",
            "If we're not making a decision with the model, really.",
            "What's the point?",
            "And if we're going to make a decision, we really want some knowledge of uncertainty and even better, we want a full predictive distribution, because then we can wait the different predictions by the loss associated with those predictions.",
            "For example, if we're building an autonomous vehicle, we want to avoid really rare but costly mistakes.",
            "Perhaps a less well recognized benefit, especially in the context of neural networks, is also better point estimates.",
            "You will actually make different predictions.",
            "It's not just that you're getting error bars on your predictions, it's a different mechanism for arriving at.",
            "Your point estimates involving something called a Bayesian model average, and actually I think in some cases.",
            "For example, if you have a lot of data in a fairly simple model, perhaps you won't see much of a difference between the point estimates of a Bayesian and classical approach.",
            "But if you have a very rich neural network and you're doing proper Bayesian marginalization, I think you can see a really massive difference actually in the types of predictions that are being made, so I think there's really a lot of promises, specially in neural networks research.",
            "To try to develop good Bayesian inference procedures.",
            "Another reason that this is interesting is because it has worked really well before.",
            "So in what sometimes called the 2nd wave of neural networks at the very end, Radford Neal developed these incredible Bayesian neural networks that Toronto that we're winning all the competitions and so on.",
            "And it was through these these very principled probabilistic MCMC approaches, and the reason we don't use those now is because they are computationally intractable for the types of models that were interested in now.",
            "But this is sort of innocence.",
            "Let us know that there's a lot of gold buried in this direction, and we just need the right tools to be able to access that gold.",
            "So that's also good motivation for PhD research.",
            "We want to build these tools together.",
            "OK, why not?",
            "So I've already sort of said it.",
            "You know, these methods can be very computationally intractable.",
            "They don't necessarily have to be.",
            "And they can involve a lot of moving parts, so sometimes our algorithms can start to look quite complicated if we're trying to be probabilistic about everything we do.",
            "But as some of the speakers have mentioned, we can even start from that place and then at least we know what we want to do in principle, and we can derive then approximations that might sort of get us what I want, what we want in sort of a reasonable amount of time, and I would say there's been really a lot of progress in the last two years.",
            "Addressing these limitations, it now is possible to do Bayesian inference in state of the art neural networks using things like stochastic gradient.",
            "Hamiltonian Monte Carlo and certain variational methods without a lot of additional computational overhead, I think there's still a massive way to go, but we've been making incredible progress.",
            "OK, so."
        ],
        [
            "Let's return to this airline passenger number example.",
            "We use this to set up a bit of notation as well, so we have this basic regression problem.",
            "We have our observations which were noting by RY values.",
            "Here.",
            "They are indexed by our inputs X one up to XN and our goal is to predict output at some test input X star and I'd like to ask you, you know, let's suppose we're just going right back to high school.",
            "How might you approach this problem?",
            "What would be your algorithm for trying to solve this problem?"
        ],
        [
            "So probably you would start by thinking about function classes that you're familiar with, so we considered this type of function here, sometimes called a linear basis function model nonlinear in the inputs, linear in the parameters W. So that would be like a polynomial.",
            "For example, we could have a linear model, or we could even have a really flexible model which is nonlinear in both the parameters in the inputs, like a neural network.",
            "So we would basically guess a function we would look at the data points, try to guess something that looks sort of reasonable.",
            "We might see some trends fail.",
            "We might want to.",
            "A trigonometric function in there somewhere, etc.",
            "And then we would go about trying to learn the parameters of this function, and probably the simplest thing that you might do is to define some kind of error measure on the training sets.",
            "They will let's, let's set those parameters so as to minimize the distance between the outputs of our function and the values of those data points where they've been observed and squared error is a popular choice, so we could differentiate that and use conjugant gradients or something like this to try to find some values of the parameters."
        ],
        [
            "An alternative approach would be to say that our data is generated by some noise free function polynomial, whatever it might be, plus some noise donated to note it.",
            "Here is epsilon effects.",
            "This could be a Gaussian distribution or something else.",
            "That noise will have noise variance.",
            "In this case Sigma squared.",
            "So Sigma is very large.",
            "That means there's a lot of noise, and then from that we can form what's called a likelihood.",
            "So the probability of our of our data given our parameters.",
            "And then we can maximize that with respect to our parameters so that we want the probability of having observed that data to be as large as possible for the parameters that we want to use.",
            "So that's often called maximum likelihood if we make certain choices like we have here like that, we have Gaussian noise.",
            "For instance, we can see by just taking the log of the likelihood and maximizing that with respect to W that will actually get exactly the same answer that we would get if we were to minimize squared error.",
            "And so you know, in a sense this you know at first glance might seem a bit deflationary.",
            "Well, what's the point?",
            "If we're making the same predictions in this case, while the point here is that we actually now can interpret that error measure, we know that these assumptions lead us to this squared error metric, whereas before we might not have a good reason to prefer squared error over absolute error over any any of the other possible choices.",
            "And so this provides us with sort of a formula for model construction.",
            "This is something to think about, actually, when we're doing deep learning quite often now we just default to things like cross entropy loss if we're doing classification L2, regularised mean squared error if we're doing regression, etc.",
            "And we could actually very easily think of a lot of other reasonable loss functions starting from this probabilistic perspective.",
            "Another thing that."
        ],
        [
            "Often done is.",
            "Regularization, so you might want to add a penalty here to prevent, for example, your weights from becoming very large.",
            "This has an issue of having to decide again what kind of penalty you want to add, how much you want to penalize this complexity, how we've been going to sort of reason and define complexity.",
            "Is this going to be parameterisation invariants, etc.",
            "And you know that could involve a lot of sort of hand tuning and an intervention.",
            "Again, we can actually perceive.",
            "These types of penalties is putting a prior on parameters and maximizing a posterior with respect to those parameters, sometimes called map optimization maximum posteriori.",
            "This is not a Bayesian thing to do, though this is still, you know, optimization.",
            "Bayesians want to do marginalization, we'll get to that shortly, but this does provide a probabilistic perspective, so we could basically see our prior in this case as a regularizer, and we could use that to try and decide how we're going to construct useful priors."
        ],
        [
            "So when we're trying to follow a Bayesian approach to inference, there's very little we really need to know.",
            "It's basically just the sum in the product rules that I've given here in equation 7 and eight.",
            "If we want the marginal distribution for some variable X, we can sum out all the other variables from a joint distribution.",
            "And this product rule P of X&Y, the joint distribution of X&Y is equal to the conditional distribution of X given y *, P Y, or equivalently, the conditional distribution of Y given X * P X, and we can use those rules to derive Bayes rule etc."
        ],
        [
            "OK, so this is the equation that we really want to evaluate if performing.",
            "If we're following a fully Bayesian approach, here we have our predictive distribution given our parameters W and we're waiting those parameters by the posterior probabilities.",
            "This could basically correspond to saying, OK, well we want to actually use in the continuous setting an uncountably infinite space of models corresponding to every possible setting of W. And if we're using a neural network, then this is going to be a very rich class of functions.",
            "You could actually sort of see each different setting of the weights is very different function.",
            "We're going to wait each of those functions by their posterior probabilities.",
            "You know, if we follow this approach and we try to honestly represent our beliefs in the model, we typically will generalize.",
            "We won't need to worry about things like overfitting and we will get a predictive distribution.",
            "The difficulty with this approach is often this integral is very hard to evaluate, so often we don't have an analytic expression for this integral.",
            "The common approaches to dealing with this is our include Markov chain Monte Carlo and deterministic approximations.",
            "Very recently, variational approximations, which are a specific type of deterministic approximation.",
            "Gained a lot of popularity, so to be clear, a deterministic approximation would say let's approximate our posterior P of W given Y by some other density for which this integral does become tractable, and a Markov joint Monte Carlo approach would involve sampling from this posterior distribution.",
            "I'm.",
            "Now we can also view the classical approach is a special case of this Bayesian model average, where our posterior P of W given Y is just a Delta function, it's collapsed on the maximum likelihood setting of the parameters.",
            "So that means there's only going to be one term that really counts in this integral, and that will recover us that answer.",
            "And that's actually a useful way of thinking about how these two approaches compare and when and why they might make different predictions."
        ],
        [
            "OK, so sort of just to wrap up the tutorial part of the talk will finish with an example that's really made a deep impression.",
            "It seems on students in my class at Cornell on Bayesian machine learning, so later in the I sort of have a number of different examples of Bayesian inference in this course and later on in tests and exams when I'm asking sort of for an explanation of how Bayesian inference works, it's been surprising to me how often this particular example has been referred to, so let's go through it briefly.",
            "So let's suppose we have.",
            "A bent coin, sometimes called a biased coin with probability Lambda of landing tails.",
            "We want to know what's the likelihood of a set of data X1 to XM, so that would give.",
            "That would be the result of each flip.",
            "That we want to use that to find the maximum likelihood solution for Lambda.",
            "The probability that we're going to get tails on any individual flip, and then we want to think about what our prediction would be if we observed to date 2 two flips and they both came out tails when we're trying to think about, probably the next clips of Tails would be, so that would basically be a special case of this question.",
            "One where we have X1 and X2.",
            "They're both pills.",
            "We go in computer maximum likelihood solution, so maybe just take.",
            "A minute and just think about that.",
            "Maybe write down a few equations.",
            "Think about how you would approach that problem."
        ],
        [
            "OK, so we can write down our likelihood.",
            "So this is just a binomial distribution.",
            "We have N flips.",
            "We want the likelihood of getting em tails.",
            "The probability of getting tail is Lambda here.",
            "So this is something that we can maximize with respect to Lambda.",
            "We can take the log, differentiate with respect to Lambda, double check that we're actually getting the maximum of this distribution, and so on.",
            "And if we do that, we'll find the solution for Lambda will be M over an, so M is the number of tails an is the total number of flips.",
            "And so you know, if we were trying to decide, you know what the probability for the next flip would be.",
            "We would just substitute in those values for M&N and so if we see two tails then that means M / N is going to be 1.",
            "And that means our prediction is that the next flip will be tails with 100% probability.",
            "So do we believe this estimate?",
            "All you know is that there's a bias point.",
            "I flip it twice.",
            "I see two tails.",
            "My prediction is that it's always good to flip tails from that point forward.",
            "No, that's not intuitive to us because you know it doesn't involve sort of a prior that would really make sense.",
            "So let's think about this from a Bayesian perspective."
        ],
        [
            "Let's choose a prior that has this functional form so that the posterior has the same form.",
            "So when we multiply this thing together with the likelihood, we get a posterior.",
            "Then when we do that, we'll see that our posterior is beta distributed and we can write down analytically the mean and the variance for Lambda here.",
            "And we can go with quite a variety in this case of beta priors.",
            "This isn't the only prior we could choose.",
            "There are many possibilities, but this is nice because we can write down in closed form what the poster looks like.",
            "So if we believe."
        ],
        [
            "The bias was anything between zero and one with equal probability.",
            "That's sort of like saying, you know I know nothing.",
            "We just representing that that sort of belief into the model.",
            "We can do that by just setting a = 1 B equals one with that prior.",
            "If we believe that the bias was maybe concentrated around 50%, we could also incorporate that into our prior.",
            "I think that might actually be more reasonable than saying I know nothing but saying I know nothing is still going to give us a very different answer than the maximum likelihood solution.",
            "Yep.",
            "So that's what I'm doing here.",
            "So conjugate meaning that the posterior has the same form as the prior, the same functional form.",
            "So I've done that.",
            "You don't have to do that, I would say generally try to choose the prior that you believe in.",
            "There is a bit of nuance associated with that, but.",
            "That's what I would go with.",
            "I'm OK so."
        ],
        [
            "So when we choose this beta prior, we can just leave.",
            "A&B is variables for now.",
            "And we look at the posterior mean for the parameter Lambda.",
            "We can see that.",
            "A&B these sort of parameters of the prior actually act as pseudo observations.",
            "Oh, and this actually is a very nice interpretation, because we can then view posterior inference is kind of an online procedure where we we have start with their prior with some setting of A&B we observe a certain number of flips.",
            "We compute our posterior and then our posterior becomes kind of our old prior.",
            "That's the number of observations we have.",
            "We see some more flips.",
            "We can update our post here.",
            "So that will give us the same answer that if as if we basically just did everything in batch and whenever we are computing these estimates it's good to sort of test the limits so.",
            "You know what happens if we make A&B really, really large?",
            "So if we make a really large, this means that we're going to be very biased in our post here.",
            "Just saying the next flip is Tails.",
            "That makes sense, right?",
            "Like we want to kind of understand the effect of prior is having on our post here.",
            "So if we're super confident in our prior work, our job is done.",
            "We don't even really need to do inference like who cares about the data.",
            "Now, if he is very large then you know.",
            "Similarly, the posterior will say OK, well doesn't really matter.",
            "You know what happened with the flips and if we have an infinite amount of data, then both M&N are going to be very very large and so those terms will completely dominate in the prior won't matter anymore and so this is another feature of Bayesian inference.",
            "Once you have enough data, the likelihood dominates and you do get the same answer.",
            "OK."
        ],
        [
            "So now I'm going to talk about.",
            "Kind of thinking about prior selection, so this is a big question.",
            "How do you choose your prior?",
            "I would say this is very related to how do you choose your model?",
            "You know there are both innocence priors.",
            "We both know that you know the explicit distribution over parameters and the functional form of the model that we choose will be misspecified.",
            "To some extent.",
            "All we can do is try to honestly represent our beliefs, and in a way these two things aren't really separate.",
            "What really matters is not the prior over the parameters, it's how that distribution over the parameters interacts with the functional form of the model to induce a distribution over functions.",
            "And so this is a perspective that has really been championed in Gaussian process modeling.",
            "But it applies also quite generally.",
            "So this kind of approach was actually taken somewhat by Radford Neal in 1996 in his PhD thesis, when he showed that some.",
            "Uh.",
            "But we can sort of think about prior selection over parameters of neural networks by first sort of sampling from our distribution parameters.",
            "Conditioning on those and drawing the corresponding functions and seeing what kinds of properties they had because we wanted a prior that was intuitive in function space."
        ],
        [
            "And so here's the sort of formal definition of a Gaussian process.",
            "Don't worry if you don't sort of get everything.",
            "The main thing here is this sort of function space view of modeling.",
            "So a Gaussian process is defined as a collection of random variables.",
            "Any finite number of which have a joint Gaussian distribution.",
            "And this notation here at this distributed is a GP means that any collection of function values evaluated at any collection of.",
            "Of inputs, X has a joint multivariate Gaussian distribution with a mean vector muina covariance matrix K, defined by what's called the mean function of the GP and the kernel or covariance function of the GP little K. And so on.",
            "The left panel.",
            "Here we have sample prior functions from this model, so this basically is telling us what types of solutions we think are a priority likely.",
            "And on the right we've observed some data.",
            "And then we've we've inferred a posterior distribution over functions that can fit our data.",
            "So we see in black, purple, and Green 3 sample functions from this posterior over functions.",
            "And in blue we see the posterior mean.",
            "So this is kind of the expectation of our post year over functions given data in shade.",
            "We also so 95% of the predictive density.",
            "So 95% of these functions will be contained within that shade.",
            "So that gives us a sense of uncertainty as well, and so I'm going to argue in this talk it's going to be very natural to think in function space when we're trying to do Bayesian deep learning, because often it is very hard to represent our beliefs over the parameters.",
            "Of internal network.",
            "The weights because this is a very complicated kind of object, but it's not so hard necessarily to represent our belief over functions.",
            "If we think our functions are smooth, periodic, finitely differentiable, very over some kind of scale.",
            "These are all intuitions we can incorporate into our prior.",
            "OK, so in fact a lot of models are examples of Gaussian processes, and there's a bit of a joke in machine learning that you know everything, in a sense is a Gaussian process with a special type of kernel function.",
            "And so let's start with some very simple examples so."
        ],
        [
            "We could consider a linear model F of X = a non plus A1 X.",
            "Everything is just one dimensional here and let's put just a standard normal distribution over a, not in a one.",
            "Now we can get a sense of the induced distribution over functions by sampling values of a naughtone one from the standard normal conditioning on those samples and then drawing straight lines with different slopes and intercepts.",
            "And so we can see that, for example, had we instead of choosing a standard normal distribution, chosen something with a variance gamma, and then wanted me to be very small that these lines would for instance, become horizontal, that their slope would be very close to zero, and so we can control this induced distribution over functions through the distribution on the parameters here.",
            "And we."
        ],
        [
            "Then go ahead and show that any collection of function values evaluated at any set of inputs X one up to XN will have a joint Gaussian distribution.",
            "And we."
        ],
        [
            "Can derive what the covariance function would be so we can just basically use the definition of covariance.",
            "The expectation of F of X I * F of XJ minus the product of the expectations over F of XINF of XJ.",
            "And we can workout.",
            "This is the kernel, and so we can now go ahead and use our Gaussian process model in function space instead of worrying explicitly about these parameters a not in a one, which is sort of what I was doing at the beginning of this talk when I presented the Bayesian model averaging over parameters.",
            "We can also do this for more general classes of models, so any model actually which can be expressed as an inner product of a vector of parameters times some vector of basis functions.",
            "This could be polynomials, Fourier series, whatever else can be shown to be a Gaussian process with a particular type of kernel function.",
            "Really."
        ],
        [
            "Popular kernel function is called the RBF kernel.",
            "Some people call this the King of kernels.",
            "I don't really like that description because there are some some issues, yeah?",
            "Yeah, so in a very intuitive sense, a kernel tells us the similarity between a pair of data points and so in the case of Gaussian process, if we're considering two outputs of our function, it says how similar are those outputs if we know, for instance, the distance between the inputs and so this Colonel here right here, which is saying what's the covariance between our random function evaluated at a point X and appoint ex prime depends on the Euclidean distance between X&X.",
            "Prime and this parameter Elvis like scale parameter and so this is a very intuitive kernel because what it's saying is that function values that are close together in input space.",
            "So airline passenger numbers that are close together in time are going to be more correlated than airline passenger numbers that are far away in time.",
            "So this is an example of what's called an inductive bias.",
            "So this is a very popular kernel function, partly because it's very intuitive.",
            "It represents an inductive bias that probably generalizes to a large number of problems.",
            "We have these parameters that we can learn the signal variance A that's kind of how much the functions are oscillating vertically, and the lengthscale L which sort of says you know how we clear the functions.",
            "And they have very nice theoretical properties.",
            "So in the appendix of this talk I actually have a derivation from the RBF kernel starting from basically this standard basis function approach where we work with a model that has some parameters in some inputs, and we want to drive what the kernel function is and what's quite incredible about the RBF kernel is that it corresponds to an infinite basis expansion.",
            "So basically this function space representation where we're working with kernels instead of with, say weights, allows us to work with models that have.",
            "An infinite number of weights and an infinite number of basis functions.",
            "So I started at the beginning with the 10,000 TH order polynomial.",
            "It basically lets us use the infinite order polynomial and that's really beautiful.",
            "It says we can use these infinite models.",
            "Nonparametric models with a finite amount of computational power.",
            "If we move to this representation, and you can imagine this would have incredible theoretical properties like universal approximation etc that within our prior we could contain for instance any continuous function.",
            "So to get a bit of intuition about this particular kernel will start by just saying we want to query our random function at a set of points, and that's really what I did.",
            "You know here the black dots here represent a set of inputs X that I chose.",
            "I constructed my multivariate Gaussian distribution and then I sample from that distribution to get these different functions for the solid curves.",
            "Here I just joined the dots together, but of course we can only query this function at a finite number of points in memory.",
            "And so."
        ],
        [
            "Here in Matlab code I've.",
            "I've chosen a set of points from minus 10 to 10 space .2 apart.",
            "I create my covariance matrix by evaluating this RBF kernel at all possible pairs of points in this input set.",
            "So this gives me sort of an N by N matrix, where N is the number of elements in X.",
            "And now I just want to sample from a multivariate Gaussian distribution that has that covariance matrix.",
            "And so this is just some some math that allows us to do that.",
            "We can take what's called the Cholesky decomposition and multiply that against just a vector of points sampled from a standard normal distribution.",
            "And when we do that, we can get these."
        ],
        [
            "So basically, each of these three curves corresponds to sampling points from exactly the same multivariate normal distribution with the same covariance matrix three different times.",
            "And that allows us to sample from our distribution over functions exactly in the same way we were sampling from our distribution over functions when we considered the straight lines.",
            "Remember, we just sample the parameters.",
            "In that case conditioned on the parameters and then drew the corresponding function.",
            "Here we're just doing it more directly or directly sampling from this induced distribution over functions, and we could have done that in the linear case as well.",
            "We could have just said, well, this is the kernel for that model will evaluate that kernel at all possible pairs of inputs in that space, and then we'll create our covariance matrix.",
            "And will sample from that multivariate normal distribution.",
            "OK, so."
        ],
        [
            "Here we have a visualization of the RBF covariance matrix that we get in this particular case where we have just ordered 1 dimensional inputs and we evaluate our kernel, all possible pairs.",
            "This is basically showing us that the entries are the largest on the diagonal and then they start to get smaller and smaller and this makes sense.",
            "What that means is that function values that are close together are more correlated than function values that are far apart in the input space.",
            "The entries on the diagonal are actually.",
            "Function values queried at the same inputs so nothing could really be more correlated than the function value with itself."
        ],
        [
            "Alright, so in practice when people are working with these Gaussian process models, they follow a two step procedure.",
            "They first use what's called a marginal likelihood.",
            "So this is an objective function.",
            "It's computed by basically doing that integral that I showed earlier, integrating over the Gaussian process itself, so we can actually marginal marginalized away this whole distribution over functions, and this leaves us with a distribution that only depends on data which are the hyper parameters of the model.",
            "Like that link scale parameter.",
            "That I mentioned in the RBF kernel, and So what we do is we optimize that objective.",
            "We condition on the parameters that we get when we do that and then we form our predictors distribution and this allows us to make predictions with that model.",
            "I don't expect you to parse all of this sort of notation in real time, but what you should know is basically often practitioners do follow this two step procedure.",
            "They have an objective for learning any parameters that are leftover in their kernel function and they have an objective for making predictions conditioned on those parameters.",
            "And the data why and the key bottleneck here is solving a linear system involving an N by N matrix are covariance matrix and computing a log determinant over that matrix which naively incurs cubic computations with a number of training points that we have.",
            "So what this means is that naively, we can only apply these models to problems with.",
            "2000 data points at most.",
            "No, that's not true anymore.",
            "We can actually apply these models to problems with millions of points, but we have to do a bit extra."
        ],
        [
            "So I'll give an example briefly just doing inference with RBF kernel, so we specify our distribution over functions.",
            "We choose an RBF kernel, we choose some values for the signal variance in the light scale we have our distribution over functions in our prior on the left panel here and then we have our distribution or functions in the posterior conditioned on some observations in the right, and I want you to tell me if anything looks kind of funny about these functions.",
            "Would we be happy with these predictions given by the solid blue curve on the right?",
            "And would we be happy with these sample poster year functions?",
            "Ha.",
            "So are we happy with this fit?",
            "So if we observe these data points in the right panel and we were using this solid blue curve to extrapolate between the points, would we be happy with that?",
            "Like does that seem like a good predictor?",
            "And if not, what is wrong with it?",
            "Little louder.",
            "Not I.",
            "Right, yeah, so it's basically just saying that the points aren't correlated like that like it sees this point over around, you know, minus four or so, and then it shoots back down to zero very quickly.",
            "And so in a sense, if we can see this, perhaps even more clearly if we look at the sample, posterior functions are very wiggly compared to what seems to be the scale of the data.",
            "It's really not modeling the correlations between the points very, very effectively, and in this case this is because we've chosen a too small length scale and we could take that to its extreme.",
            "Like if we chose the smallest possible like scale than our prior would be that our data is white noise.",
            "And of course that's a terrible prior, and we're not going to learn very much."
        ],
        [
            "Now we can increase the length scale a lot and observe the opposite effect.",
            "So here's our new induced distribution over functions in the prior on the left.",
            "So basically the functions that are a priori likely are very slowly very very simple.",
            "We observe some data, we can see that the fit is probably simpler than we would like.",
            "We're missing a lot of structure in the data points near the right side of this right panel.",
            "An uncertainty estimates also look a bit off.",
            "Um?",
            "And so it's very important to think about how we can set these hyperparameters.",
            "They really control things like the complexity of our induced distribution over functions.",
            "And so I."
        ],
        [
            "Mentioned briefly, that we often use this objective called the marginal likelihood.",
            "So I'll motivate, motivate that a little bit more, so the marginal likelihood corresponds to what we get in this case, if we were to integrate over our Gaussian process model in words, it's the probability that we would sample our data set if we were to randomly sample from the parameters of our model in a prior.",
            "And so on.",
            "The left panel here I have this visualization that David Mackay had in his PhD thesis where he was considering deterministic approximations for Bayesian neural Nets on the horizontal axis.",
            "We have all possible datasets.",
            "On the vertical axis we have the marginal likelihood for each of these datasets.",
            "So the probability of generating that data set if we were to randomly sample from the parameters of the model.",
            "So in this illustration, a simple model is only going to be able to generate a very small range of datasets, so let's go back to that linear model.",
            "We had a not plus a 1X and some distribution over a knot in a one.",
            "Every time we re sample a knot in a one from the prior, we get a different straight line with a different slope and intercept.",
            "But of course that's not going to describe very many datasets, and so it has absolutely no support for most of the datasets in this diagram.",
            "At the same time, because this is a proper probability density, it has to normalize etc.",
            "It's going to give a lot of density to those datasets that it can generate, so that's why the curve is actually quite high here.",
            "Conversely, we could choose an extremely flexible model, and we could have a very diffuse distribution over its parameters in the prior, and in that case every time we re sample the parameters and draw the corresponding outputs, we get a completely different data set.",
            "And similarly because this is sort of a normalizable distribution and we're not concentrating our support in any particular space, this is going to have relatively low probability for any given data set.",
            "So from this perspective, if we condition on a particular data set, the evidence is going to favor a model of appropriate complexity.",
            "And when we do that in the case of Gaussian processes, and we use this marginal likelihood to learn the kernel hyperparameters, we get this fit in green here.",
            "In red and pink I showed the two previous bids that we had on it the last slides.",
            "We can see intuitively that this fit is a lot nicer.",
            "It seems to have a more appropriate level of complexity."
        ],
        [
            "So this is the objective that we get when we do this with Gaussian processes.",
            "We can see that it compartmentalizes into two key terms here, one associated with model fit, one associated with complexity, which is a log determinant over this covariance matrix that we have, and that could be interpreted roughly as sort of the volume of solutions that we can express in our prior."
        ],
        [
            "You can also use this diagram quite separately from Gaussian processes to reason about generalization, so I said earlier that we really want to use the model with.",
            "You know, a lot of flexibility, the 10 thousandth order polynomial, something even more flexible if we could, because we believe that probably the generative mechanism for our observations is extremely sophisticated, and we want to honestly represent our beliefs in the modeling process.",
            "But we also don't want to overfit and from this perspective, generalization is really a 2 dimensional concept.",
            "It's not strictly related to which solutions we can represent, it's also related to which solutions are a priority, likely.",
            "So from this perspective, I would argue that we want our support which solutions we can represent to be as large as possible, as long as we actually believe that there's some probability that that could be the solution we want to represent it with their model, because that just is honestly representing our beliefs.",
            "But we also want to distribute that support very carefully.",
            "So not all solutions.",
            "For instance, we would actually believe are a priority, likely when we're reasoning about things in this function space.",
            "And so you could actually use this to start to understand things like the difference between a fully connected neural network and a convolutional neural network.",
            "So a fully connected neural network is more flexible than a convolutional neural network, yet it does better for a lot of problems in computer vision.",
            "The way that that works from this perspective is by creating a restriction bias saying OK, well, if we don't have these kinds of invariances then we don't want to represent those solutions.",
            "And so that basically crunches the support of this model and concentrates it around the types of datasets that are interesting in that application domain.",
            "And that's why we see better generalization now if we could actually take this a step further, instead of having a restriction bias, we could have a more general inductive bias.",
            "If we actually believe that other types of solutions were possible that couldn't be represented by the convolutional neural network, then we might start with something like a fully connected neural network, have a distribution over the parameters so that it's concentrated around the types of solutions that are favored by a convolutional neural network, but don't exclude other solutions that we believe to be a priori possible.",
            "OK, so."
        ],
        [
            "I briefly mentioned that that.",
            "You know, Radford Neal triggered a lot of interest in Gaussian processes during his PhD thesis, so this was kind of in the mid late 90s.",
            "He was showing that you could use Markov chain Monte Carlo algorithms in conjunction with distributions over neural network parameters to achieve very good results.",
            "He was also arguing that we do want to have very flexible models and along that direction he showed that if you have a neural network with a distribution over the parameters and an infinite number of hidden units.",
            "That actually converge to a Gaussian process with a very particular type of kernel function, sometimes called the neural network kernel function.",
            "And this was very exciting because it meant we could actually relatively easily use these incredibly flexible models that have really nice properties, universal approximation, etc.",
            "But David Mackay actually wrote an interesting article in a collected edition about neural networks edited by Christopher Bishop, where he was, you know, generally providing a tutorial on Gaussian processes, but he had this very interesting comment.",
            "He said that neural networks were envisioned to sort of become eventually intelligent agents which could discover very interesting hidden representations in data.",
            "But Gaussian processes, though they had all these nice properties and they were getting very good performance at the time.",
            "We're basically just smoothing devices with the popular kernel functions that we were using.",
            "They weren't discovering such sort of exciting hidden representations that we might associate with intelligent agents, and so he asked in that case, whether, in treating Gaussian processes, is perhaps a principled replacement for neural networks.",
            "Whether we're throwing out the baby with the bathwater.",
            "And so the answer to this question, I believe, is to develop more expressive kernel functions which can discover interesting hidden representations in data.",
            "And we can do that with the help of neural networks.",
            "I think quite often neural networks and kernel methods are perceived as competing approaches, when in fact they have very complementary statistical properties that can be combined to great effect.",
            "Kernel methods.",
            "I think, as I sort of alluded to earlier, provide a very elegant way of working with nonparametric models using a finite amount of computational power.",
            "And Gaussian processes also provide this very natural kind of function space approach to regression.",
            "It's not specific to using a kernel method, but it's something that's often associated with GPS, whereas neural networks provide really powerful inductive biases for learning in all sorts of interesting problems they provide adaptive basis functions that are incredibly well motivated and very useful for a range of problems and."
        ],
        [
            "You know the idea of deep kernel learning is basically to use.",
            "An infinite number of basis functions, but have them be adaptive and have them be informed by neural networks.",
            "And so that way we can achieve some of the best of both worlds.",
            "And we can also get predictive uncertainties etc.",
            "And so basically in this diagram here, what we have is our input layer.",
            "So these could be pixels in an image.",
            "They could be different times in some kind of time series extrapolation problem, different spatial locations, whatever you like, we have the neural network operating on those inputs.",
            "And then we have a layer where a Gaussian process is applied to those outputs and then we have another layer which kind of mixes those Gaussian processes together linearly to form outputs.",
            "Very crucially here though, the neural network is not treated as just some kind of preprocessing to the data, it's all learn jointly through the marginal likelihood.",
            "So this is a different objective for training the neural network parameters and this can allow us to actually write this model in what's called the dual space as an infinite expansion that has these adaptive basis functions.",
            "And."
        ],
        [
            "So you know, in equations we basically start with some base kernel.",
            "We could be an RBF kernel, spectral mixture, whatever we like.",
            "We transform the inputs of that kernel is some function that has its own parameters and then our goal is to learn those parameters, say the weights of general network in conjunction with the base kernel hyperparameters through the marginal likelihood of the Gaussian process.",
            "So I can't really emphasize this enough.",
            "That's very important if we just pre trained the neural network and apply a Gaussian process, we have a very different model, very different predictions, very different statistical properties.",
            "So."
        ],
        [
            "The issue then is scalability, so I mentioned that Gaussian processes naively scale cubically with the number of training points.",
            "And once you've done that.",
            "It'll take quadratic complexity for each prediction that you make.",
            "We've been working very hard at developing new approaches in numerical linear algebra and scientific computing, which make these computations very scalable without actually sacrificing a lot of the interesting model structure.",
            "So some of you may have heard of the K fact model.",
            "That sort of employs a similar sort of idea to neural networks, where you have a chronic or factorization of the Fisher information matrix.",
            "Here, we're developing approximations to kernel matrices.",
            "Which allow us to preserve the representation but achieve linear time scaling in training and constant time scaling in testing.",
            "And you can show actually that these approximations can be accurate to within numerical precision.",
            "And so this is all implemented in this new library.",
            "G Py torch with a lot of examples.",
            "So pretty much every example that I have in this talk corresponds to a notebook in this G Pytorch library.",
            "And these approaches, also, I think, harmonize very nicely with GPU acceleration, so I won't go too much into the details here.",
            "But it's all basically driven by creating an approximation to a matrix of interest, which will admit fast matrix vector multiplications.",
            "Once we have that, we can use things like linear conjugate gradients to solve linear systems efficiently, only involving matrix vector multiplications, and we can use things like stochastic Lanczos expansions to compute things like log determinants and their derivatives efficiently.",
            "Again only involving matrix vector multiplications and GPU's are very good at accelerating those kinds of operations, so not only is the complexity quite nice, asymptotically GPU's are making these computations several orders of magnitude faster.",
            "And I would say this is an approach that can also be employed farv on Gaussian processes.",
            "I've mentioned Capac, I think that's a very nice approach.",
            "There's a lot we can do, for example to come up with structured representations, for instance of neural network weights, which will speed up computation without necessarily sacrificing what we like about those models.",
            "Another example of that are tensor train neural networks, which have been developed by several Russian groups.",
            "So."
        ],
        [
            "Um?",
            "You probably can't see everything on this table, but the point that's being made here is that when we do deploy this model with basically training a Gaussian process with this deep kernel through the marginal likelihood and applying it to every regression problem, in this case on UCI, we see that the point predictions are a bit better than what you get through a standalone deep neural network.",
            "The runtime isn't significantly more.",
            "You pay about 10% runtime overhead in exchange for predictive distributions, and somewhat better predictions.",
            "These datasets all have very different properties, so different numbers of data points and different numbers of dimensions for the inputs."
        ],
        [
            "Here we applied this problem.",
            "So this is this, this sort of one of these papers is joint work with Russ Salakhutdinov and he looked at this already faces example and a few papers.",
            "So basically here we have faces with different orientation angles were trying to predict these angles.",
            "This is a regression problem but basically we have images are inputs.",
            "We use a convolutional kind of based architecture for the deep kernel and when we train everything an project into 2 dimensions where each line segment corresponds to a phase, we see that.",
            "Faces with similar orientation angles kind of clustered together.",
            "We see line segments with similar slopes kind of clustering together.",
            "So basically we're learning a non Euclidean metric to try to solve this problem.",
            "This is very crucial, so another."
        ],
        [
            "Stration of this.",
            "Involves ordering all the faces by orientation angles and evaluating our deep kernel at every possible pair of faces in the left two panels here, and we see a very pronounced diagonal band.",
            "Basically these two panels correspond to different base base kernels here, but what it's learning is that faces with similar orientation angles are more correlated, and that's a very non Euclidean metric learning problem, and so that's something you can't do with pretty much all of the popular kernels that people use.",
            "And in fact in the far right panel we have the RBF kernel.",
            "You know trained to this problem, we can see that the entries are quite diffuse no matter what we do with that kernel.",
            "We're not actually learning the metric, that's going to help us solve this problem effectively, so we can see the neural network in this case is doing a kind of non Euclidean metric learning.",
            "In this."
        ],
        [
            "Problem we have data sample from a step function shown in black and we have fits using Gaussian processes with various different kernels.",
            "So we have the RBF kernel in blue.",
            "Basically this kind of data is such an unlikely draw from our prior over functions.",
            "Using an RBF kernel that it can only be explained from that perspective by saying that there must be a lot of noise and so it basically chooses a very simple fit.",
            "Says there's a lot of noise and has very high uncertainty and red.",
            "We have a different kind of kernel.",
            "Call the spectral mixture kernel.",
            "It doesn't.",
            "You know a better job of capturing the data, but it's still over smoothing.",
            "Here we have to fit using a Gaussian process with a deep kernel and green and we can see that it captures these sharp discontinuity's quite nicely and so we made this demonstration because you know, often Gaussian processes with standard kernels are accused of over smoothing the data and this is not the case if we actually are helped a bit by neural networks.",
            "We can also see that these uncertainty estimates corresponding to 95% of our predictive distribution do capture the data very confident.",
            "We've."
        ],
        [
            "Develop these approaches also for RNS and LST emsan.",
            "We've applied them to autonomous driving, in particular, where we might want things like uncertainties on estimates of Lane boundaries and in the top."
        ],
        [
            "Panel you have the predictions using a standard LTM in the bottom panel you have basically the whole predictive distribution that you get out of this model.",
            "So both the point predictions and our uncertainties and a lot of additional information that we can use in decision making.",
            "So that's one approach to being, you know, Bayesian about neural networks.",
            "We can build things like Gaussian processes.",
            "We can develop deep kernels which have been inspired by neural networks.",
            "We can use these Bayesian objectives for training.",
            "The models will get different predictions will also get uncertainty estimates, and we can make these things scalable by developing new approaches in numerical linear algebra and scientific computing.",
            "As an aside, a lot of students ask me, you know what kinds of courses should I take if I'm interested in machine learning and AI very consistently recommend scientific computing.",
            "Feel like it's a very foundational skill and it will help you in almost anything that you do.",
            "Another"
        ],
        [
            "Coach, that's perhaps the more conventional approach to being Bayesian neural networks is trying to be explicitly Bayesian about the weights about the parameters of the neural network.",
            "So put a distribution over those parameters and then do Bayesian marginalization.",
            "So this is something we did in a paper called the Bayesian Gam where we worked out a way to do posterior inference in response to adversarial feedback over the generator.",
            "In the discriminator parameters here, I'm focusing on the generator parameters just illustrate a particular concept.",
            "So what we've done in this top panel here is after we have our posterior over generator parameters.",
            "We sample from that posterior 6 times and that gives us basically six different generators sampled from our posterior, just in the same way we were thinking about Gaussian processes.",
            "We were thinking about sampling posterior functions.",
            "Here we're basically sampling post to your generators and we're storing sort of an uncountably infinite collection of possible generators that could have created our data.",
            "You could view.",
            "Each generator is basically a different hypothesis for our data.",
            "And so we can see that the images produced by each of these six generators are qualitatively quite different, so some are thicker summer, thinner.",
            "Some might have different writing styles, some might correspond to different writing implements.",
            "And we're looking at the likelihood surface or the posterior over the generator parameters.",
            "We can see that you know some of these points might correspond, so might might might be very likely explanations of the data, and some might be very unlikely explanations of the data, so it could be that we sampled some parameter in detail here that actually is a very unlikely hypothesis for the data, and maybe it creates very weird looking images.",
            "But if we believe it's possible, then maybe we ought to represent it, and this turned out to be quite helpful for semi supervised learning in the bottom panel we trained.",
            "It's called the DC again 6 times and then just generated images from these DC games.",
            "They actually don't look bad.",
            "In fact, you might argue that they look better than than some of the images from these panels, but you would also argue that that might be expected because.",
            "But the classical approach here in the DC again is essentially choosing one of the modes of this posterior and using dot mode as it's one generator.",
            "So if you had to bet everything on one model, maybe you would want to bet on the model at the most posterior probability.",
            "But if you want to represent the distribution, that's a different kind of thing, and it has different types of advantages.",
            "We can see that the images look good, but they're fairly homogeneous.",
            "So this sort of."
        ],
        [
            "Just in the next part of the talk were reasoning about the geometry of these likelihood functions.",
            "These kinds of loss functions that we used to train these models, so I think this is really an important thing to do if we want to have successful Bayesian marginalization of neural network parameters.",
            "This will help us inform our parameters so our priors it will help us inform what kinds of inference procedures we want to use, whether we want to focus on a single mode, whether we want to try to really explore the whole distribution, etc so poorly knows it all visited Cornell couple years ago and.",
            "Presented this figure and I found it incredibly inspiring from a Bayesian perspective.",
            "So what he was saying was related to optimization.",
            "He was saying that small batch SGD tended to converge too.",
            "Broader local Optima.",
            "And that could explain why those solutions generalize better than if you use like a large batch method which might converge to a sharper local Optima.",
            "And this diagram is showing the training loss in black and the testing loss in red.",
            "So the intuition here is that if we choose a point on this flat Optima and we translate it vertically up onto the red curve, it still doesn't have terrible loss.",
            "But if we do that with a sharp Optima, the loss becomes very bad and so I feel like that's fairly intuitive.",
            "You know, if we're near a sharp Optima, we have to be very precise, and it's quite sort of intuitive that we would see this shift because we wouldn't really have enough information to be necessarily too confident about what the parameters should be.",
            "However, from a Bayesian perspective, this meant that if we if there truly were these broad local Optima and they did truly correspond to, say, a diverse collection of models.",
            "Then there's going to be a very big difference between Bayesian integration, Bayesian marginalization, and optimization in terms of the point predictions.",
            "So if the curve for the likelihood or the loss really was sharp like this, then perhaps we could successfully represent it with a single point, and the predictions wouldn't be too different than if we were to try to integrate over that region.",
            "But if these flat regions of the likelihood surface really existed and they really meant something.",
            "Then that was great motivation for trying to do this integral for trying to get different types of predictions.",
            "And so I feel like the potential for Bayesian methods to really make a difference is greater in deep learning that it is in many other types of models.",
            "I think the challenges are also a lot greater."
        ],
        [
            "So in a number of recent papers we've been exploring, the geometry of these loss functions in a sense, ultimately motivated to do Bayesian marginalization over neural network parameters, but initially considering properties of the law surface that will help us in just having better approaches to training these models from a conventional perspective, and some of the conclusions we had where the local Optima in these really high dimensional spaces, rather than being isolated, were actually connected by very simple curves.",
            "And I'll go into that shortly.",
            "Another conclusion was that mini batch SGD doesn't actually converge to broad Optima in many directions, and there are good ways of finding those directions.",
            "That averaging weights along SGD trajectory's with high constant or cyclical learning rates can lead to much faster convergence and solutions that generalize better.",
            "And there is a relationship between that procedure.",
            "An finding flatter regions of the training loss.",
            "And we can try to approximate ensembles and Bayesian model averages with a single model.",
            "I feel like this is actually a good good compromise.",
            "Sometimes if you're trying to follow a Bayesian approach because we're working in an exceptionally high dimensional spaces and you know, I think that perhaps will never be able to fully perform Bayesian integration over these kinds of likelihood.",
            "Surface is, it's just such an enormous challenge, but we can restrict ourselves in various ways and still see improvements.",
            "We can still try to do something we can try to go halfway an I think.",
            "You know one thing that we can do that's actually very practical is reason about what a single model might look like, but a single setting of the parameters might look like they could approximate something like that Bayesian model average.",
            "Just going back to this figure quickly.",
            "One other point, I suppose, is that if we are doing integration versus optimization, then a lot of the mass will be contained in that broad local Optima.",
            "So that also explains why a Bayesian approach might actually be quite robust, and we're thinking about generalization, so that will kind of happen automatically.",
            "So this is."
        ],
        [
            "Visualization of this first point that I mentioned in the top figure.",
            "Here we have three local Optima that have been discovered by basically running SGD as one would typically run SGD on a resonant 110 cifar 100 using different initializations.",
            "So this is corresponding to the standard intuition where these very high dimensional Optima are quite separated.",
            "This is formed by sort of an affine composition of all possible settings.",
            "Of these three weight vectors.",
            "In the bottom left we have a special plane.",
            "In this high dimensional space along which we can walk from one of these lower Optima to the other and have essentially constant training loss and testing loss.",
            "In the bottom right panel we have another special plane, basically a slice through this really high dimensional space along which we can walk on a bezier curve quadratic bezier curve from one of the optimal to the other and have essentially constant training and test."
        ],
        [
            "So to make this a little bit more concrete, in the left panel we have a polygonal chain with a single bend W. One hat is a setting of weights that have been discovered using SGD with mini batches, data basically corresponds to what we want to learn here.",
            "That's the bending point where we need to turn when we're walking from one optimal to another W2 is the other setting of the weights, so we have a curve which has been anchored by two settings of weights discovered by SGD.",
            "In the bottom right we have sort of the equation here for the bezier curve, so it's just another parametric function.",
            "So the way that we discovered these paths."
        ],
        [
            "Was to choose this curve, which would have as its endpoints local Optima discovered by SGD and then minimize our standard loss function.",
            "For example, L2 regularizer cross entropy uniformly in expectation over the curve.",
            "So what this sort of looks like is a line integral over the parameterisation of this curve, normalized by arclength.",
            "This is something that was actually fairly computationally tractable to evaluate, so we could sort of view this integral.",
            "They're interested.",
            "It is an expectation over a variable uniform standard uniform variable of the loss.",
            "We can sort of sample from that and take gradient steps and then discover these curves."
        ],
        [
            "And you know when we walk along these curves, we have near constant training and test loss.",
            "We can actually find quite a rich variety of parameterisations, the simplest that we found was the polygonal chain with a single bend.",
            "We never found that we could actually take a direct linear path from one Optima to another.",
            "I think there might be some Optima that are linearly connected to others, but there's so many that it's very unlikely that we would find those just by running SGD.",
            "But we could very consistently find a Polygon.",
            "Change between any two Optima so the red curve basically shows the error as we walk from one Optima to the other on a straight line path.",
            "The green and blue curves show the error that's incurred as we walk from one point to the other on these other parameterized curves that have been discovered.",
            "Now when we first found this, I was a little bit scared that this could just mean that we found redundancy in the parameterization of the neural network, that these are all basically the same model.",
            "I mean still kind of cool, but you know, maybe there's not much you can do with that practically, but it turned out that actually, although the models were having the same train loss, roughly the same test loss they were making different predictions.",
            "So they were making different types of errors, which meant that we could actually ensemble them for much better performance.",
            "You know very easily and this started us thinking more about loss values rather than single points that could actually optimize our objective.",
            "And you can also use this kind of reasoning to try to derive new types of loss functions which might favor things like brought Optima.",
            "So here we started by anchoring curves by two endpoints discovered by SGD.",
            "But instead you might say, well, I want to minimize.",
            "Surface integral over cross entropy subject is some kind of constraint which we might enforce, for example through a LaGrange multiplier.",
            "Say, OK, we want the value to be at least this large, and if we think that that's associated with generalization and that could be quite interesting to explore, what happens if we do that and?",
            "You know this would also innocence calibrate our loss functions so that points kind of in the center of brought Optima wouldn't necessarily generalize better than points on the periphery, because now we've accounted for the fact that central points kind of been doing this integral are better that they might generalize better.",
            "So I think that this result this kind of early result that flatter, optimum might generalize better also suggests that the loss function that we're using itself might be a bit miscalibrated.",
            "We wouldn't necessarily have that for a well calibrated loss, and this can help us inspire better types of loss functions.",
            "So."
        ],
        [
            "This kind of discovery lead us to think more and more about ensemble ING and how we could actually take fairly small steps and wait space and actually still see a lot of diversity and models.",
            "So we explored using a cyclical learning rate and basically capturing models at the bottom of each cycle.",
            "So just doing one single training run of SGD with the cyclical rate capturing models at the minimum of each cycles and then using them as part of an ensemble.",
            "So this would require the same computational budget for training, but you would get an ensemble and the predictions were really good.",
            "You did improve a lot and you did better if you kind of fix the computational budget than you would if you were just to run SGD a bunch of times with random initializations and then ensemble those together.",
            "And in the process we started thinking more and more about visual."
        ],
        [
            "Using the trajectory of SGD in these planes so.",
            "We know there's something a bit funny, so these are basically three weights that have been sampled along the trajectory of SGD with this cyclical learning rate schedule on the test loss surface.",
            "So this this is, this plane is formed by all affine combinations of these three weight vectors.",
            "And."
        ],
        [
            "I thought, well, why are we actually?"
        ],
        [
            "Averaging the outputs of these models when averaging the parameters themselves seems like potentially an interesting thing to do.",
            "You know, and when we did that?"
        ],
        [
            "We found that these points actually generalize really well, so in the bottom right panel here we have what happens if we run.",
            "A resonant 110 on Cifar 100 using a prescribed learning rate schedule with SGD up to epoch 125.",
            "And then for stochastic weight averaging, what we're doing, we're maintaining a running average of these weights.",
            "We change the schedule so that we're using a cyclical rate, and then we just store the running average and then we use that average at the end.",
            "And then for SGD we just continue running until convergence and we see that the cast equate averaging moves to sort of the periphery of a local Optima.",
            "It's not the best solution and train whatsoever.",
            "SGD does converge to the local Optima, but once we move into test, everything shifts and the SWA solution generalizes quite a bit better than the SSD solution."
        ],
        [
            "And here we have basically projections of the trajectories of SGD onto these loss surface.",
            "Is using different types of learning rate schedule.",
            "So in the top two panels we have cyclical learning rate schedules.",
            "In the bottom we have very high constant learning rate schedules and the main difference is that with the high constant learning rate schedules, the points traverse by SGD or a lot worse.",
            "But they still seem to be almost symmetrically aligned around points of good generalization and test."
        ],
        [
            "So to try to understand why these solutions might generalize better, we started thinking about this with hypothesis.",
            "So in this figure here we sampled direction vector uniformly on the unit sphere and then we stepped away from both the stochastic weight averaging solution.",
            "In this SSD solution a bunch of times.",
            "So each curve corresponds to a different direction.",
            "So we could see that we actually had to move a much further distance to increase error by the same amount using SWA.",
            "Then if we were to start from the SSD solution.",
            "So this."
        ],
        [
            "It made us wonder what it would look like if we were to actually walk directly from the SWA solution to the SGD solutions that corresponds to the origin.",
            "In this figure, this is test error.",
            "Um?",
            "No, this is as we have the train loss in green and we have test error in blue.",
            "We can see when we do that that SGD actually converges to appoint with better test loss it's a bit lower than the SWA solution, But the SWA.",
            "Arthur Train, yes Thanks so better training loss.",
            "But Swa converges to a much flatter region.",
            "Of the training loss and then in test SWA is is generalizing better it has.",
            "It has lower test area and it seems to be almost at the minimum of this curve whereas.",
            "SGD is not generalizing as well and we can see here that SGD is actually near the periphery of very sharp ascent in this particular direction.",
            "So this actually gives us a mechanism to find a direction where SGD is not converging to kind of a flat local optimum we also consider."
        ],
        [
            "The relationship to this other ensemble in approach that we were using earlier where we were taking steps and then kind of averaging the outputs of our models using this cyclical learning rate and by design.",
            "These steps were actually very small.",
            "We found that we could still explore a pretty diverse collection of models by taking small steps in weight space.",
            "So we did a linearisation analysis to show that this sick astic weight averaging solution would approximate.",
            "Actually, the answers that were getting using this this ensemble in that we were doing before called Fge.",
            "And we run it."
        ],
        [
            "Using the best models on a lot of these vision benchmarks, so Imagenet Cifar 10 Cifar 100 and so on, we find that stochastic weight averaging performs competitively, sometimes even better than the ensemble, and pretty reliably better than SGD.",
            "You could also run this weight averaging procedure for more budgets and continue to improve performance because in this case SSD isn't really converging to anything.",
            "It's just exploring the loss surface, and in fact there have been some interesting recent papers saying that if you have a high constant learning rate, SGD is approximately sampling from at least locally, approximately sampling from a Gaussian distribution centered at one of the local Optima.",
            "So we found that we could run this up to two or three budgets and still see improvements in performance.",
            "I'm."
        ],
        [
            "This sort of some results on image NET.",
            "In practice, I would say that you can train the model end to end.",
            "In this way use the high constant learning rate.",
            "Are cyclical rate making running average.",
            "But you can also just start from a pre trained model that's been trained with SGD and then start running SWA for a few epoch Cincy, improve performance and that's very practical for things like image net we're training actually can be quite expensive so it didn't really take us a lot of resource is to start with the pre trained model on image net and run for five or ten epoxy.",
            "So."
        ],
        [
            "This is going back to this Gaussian intuition, so if it's the case that SGD with a high constant learning rate is approximately sampling from a high dimensional Gaussian distribution, that means that it's traversing basically the surface of a hypersphere.",
            "So in high dimensions, Gaussian distributions have most of their mass in a very thin shell, and we can understand this by thinking about mass as density times volume.",
            "So although the density is the greatest.",
            "The center.",
            "So if we have a very high dimensional standard normal distribution, zero has the highest density.",
            "Volume increases very quickly as we increase radius and these two things sort of tradeoff.",
            "Then we get a key distribution for the mass, and it's it's point of highest density.",
            "Is it root?",
            "D Anet concentrates very quickly, so you can sort of try this at home in Matlab or Python or whatever.",
            "See what happens if you sample from a standard normal distribution with maybe 100,000 dimensions and then histogram of the norms of those samples.",
            "And you'll see that they're very concentrated around Ruthie.",
            "And this is much less the case for lower dimensional Gaussian.",
            "So this means that by basically averaging the points on this shell, we're able to move to a point of higher density very quickly.",
            "So this sort of helps us understand things like convergence as well."
        ],
        [
            "In this experiment here we're running SGD with a high constant learning rate in green on Cifar 100.",
            "With Resnet 110 I think are wide Resnet, so on this particular problem 20% error is reasonably good.",
            "We can see that it's hovering around 35% error in blue we have the normal prescribed learning rate schedule for this model.",
            "In red we have what happens if we start averaging the the solutions traverse by STD with the high constant learning rate at epoch at about 1:45 so we can see after just about five iterations.",
            "Here the error plunges down to value.",
            "That's lower than what SGD would converge do.",
            "Um?",
            "So you can see that you know there can be quite dramatic improvements in convergence.",
            "I think there's a natural question of when do you start averaging.",
            "I think that question is actually very similar to the question of when do you start decreasing the learning rate in a regular STD schedule?",
            "So this is something that is actually difficult to know precisely.",
            "There are some good sort of heuristics based on theory, but you know what you can do at least to start with an STD plane train model and."
        ],
        [
            "Averaging or you could start averaging when you would normally start decreasing the learning rate for an STD model, and that will tend to work well."
        ],
        [
            "So in conclusion, you know if we're following a Bayesian approach, we can often achieve better predictions than we would if we were representing the posterior with just a single point, which is what we're doing in a classical approach.",
            "We can also get uncertainty estimates, which are crucial for decision-making.",
            "Computation is a key challenge, and this challenge certainly hasn't been fully addressed.",
            "We've been thinking about this issue mostly from the perspective of developing techniques in numerical linear algebra, that Gaussian process is really scalable, and.",
            "Use very deep kernels.",
            "I think there's a lot of promise actually to deploy similar kinds of techniques directly to exploiting structure and weight matrices in neural networks, so that's done with a paper called tense arising neural networks.",
            "It's also done by K fact.",
            "I think there's really a lot of room to develop better inference procedures as well, so using the HMC methods that Radford Neal used so successfully in the 90s just wouldn't be possible for the types of neural networks that we're considering now, these are just.",
            "2 high dimensional parameter spaces.",
            "We also really need to do some kind of like stochastic sampling.",
            "We don't want to be using full batches.",
            "We've discovered that that's incredibly important for being blue train.",
            "These models in the last year or two.",
            "There have been some very exciting algorithms that have emerged like stochastic gradient Hamiltonian Monte Carlo, which algorithmically are actually very similar to SGD.",
            "They look a bit like SGD plus Gaussian noise with variance that depends on learning rate, momentum, and a few other terms and so.",
            "This is very exciting in the sense that we can apply those algorithms without a lot of additional computational overhead.",
            "If you're using SGD to train a neural network, you can also use DHMC to do sampling.",
            "However you know, I think there's still a lot more work to do exploring these very rich likelihood surface is going to be very, very difficult, and it's certainly not fully achieved by stochastic MCMC algorithms that we have now variational approximations.",
            "Similarly often concentrate most of their efforts on a single mode.",
            "I feel like with neural networks in particular, the different local Optima well they might perform similarly well often correspond to interesting Lee different representations, so this was hidden sort of hinted out of it when we were talking about Bayesian Ganz.",
            "The different modes might actually correspond in that case to different generators with interesting different statistical properties, and so really we don't want to throw away all of that information if we're developing a deterministic approximation, it would be very nice to try to.",
            "Consider more than just a single mode, and this is also partly why I'm really excited about MCMC.",
            "This is something that happens, perhaps a little bit more naturally with with MCMC.",
            "I think from the perspective of creating deterministic approximations, we also want to think beyond variational methods, so the variational method at a high level.",
            "Starts by, you know, is every other deterministic method.",
            "Having us choose some distribution that we can use for our posterior which will give us analytic tractability.",
            "And then we learn the parameters of this approximate distribution by minimizing the KL divergent between that distribution and sort of our true objective distribution.",
            "When we do that, we have a bias towards very compact representations of the data, so the KL divergent when optimized that way round will really want us to avoid having our approximation place any mass where the true distribution doesn't have mass.",
            "And for that reason we can actually miss a lot of interesting properties of our likelihood surface.",
            "So this is something that's actually been discussed a bit in the context of Gans when people think about you, know Jensen, Shannon Divergent says versus KL divergences versus Wasserstein divergences, etc.",
            "And I think this is something that we also ought to think about when we're building deterministic approximate inference algorithms, and I think we can probably import a lot of what we've learned in thinking about gains towards building better approximate inference strategies that don't necessarily just rely on variational approximations.",
            "So as a concrete example, we could consider things like Alpha divergences which actually have KL divergences as a special case, and we could think about how we might want to learn the parameters of those more general families of divergences.",
            "We also I think you know, as usual, want to think very carefully about the particular application that we're considering.",
            "In some cases that compactness might not be a problem.",
            "They could even be a feature.",
            "So I think there's really a lot of room to build on these methods, particularly for more scalable approaches, but also to try to ensure that we're actually.",
            "Exploring this rich distribution, which corresponds to many different types of interesting representations for the data.",
            "I also think there's a lot of promise in taking a function space approach to machine learning in general, so this isn't necessarily just something we need to do.",
            "Is Bayesians, it's something we can do when we're thinking about regularizers, for example.",
            "So I said, you know, we could do something like L2 regularization, something I mentioned very early on, where I said we might not want the weights to be too large.",
            "I think I also remarked, but it really depends on how we parameterise our model.",
            "We really want to think about how our weights actually interact with the functional form of the model.",
            "So whether we want the weights to be bigger small depends very much on the function.",
            "We can do things to the function to make big weights good or small way.",
            "It's good, and so in taking this function space approach where operating directly on the outputs of the model, rather than reasoning about the parameters of the model, I think we can construct very intuitive regularizers and we can also construct very intuitive priors that we can use for Bayesian inference, and this is something that's really been championed in the Gaussian process Community, but I think it applies.",
            "Very generally, I think there's a lot of exciting work starting to emerge in this space.",
            "And I think you know, even if we're not following a Bayesian approach directly, we can take inspiration from what would be a Bayesian approach to develop, you know, interesting procedures.",
            "So actually in the original dropout paper, Bayesian marginalization was mentioned as inspiration for a lot of the algorithmic ideas.",
            "So all the code available for you know stochastic weight averaging, scalable GPS etc is available at this URL and so thank you."
        ],
        [
            "This one, that one that one Yep Yep.",
            "Could you explain the various axises?",
            "Where's data changing?",
            "Where's the model changing?",
            "OK, so I mean this is just directly from from this paper by Kessler at all.",
            "The idea is that we have our loss function in black and.",
            "A solution that that is good X axis.",
            "So the X axis would be different weights the parameters.",
            "So we remove.",
            "This way is moving the weights.",
            "Yeah yeah, so the vertical axis would be the loss and the horizontal axis would be the parameters.",
            "So I have a question about this one.",
            "You know the work of laundon and repair metrization and flat minimum.",
            "Yeah.",
            "So how do you reconcile this notion that you can re parameterized in parameter space?",
            "And this go from the flat minimum to shock one or vice versa.",
            "But you know, in functions based nothings changed.",
            "Yes seems to be the contradiction here.",
            "I agree, yeah, so I think we really need to to to create better definitions of width.",
            "Alot of the common definitions won't necessarily accommodate pathological solutions like Rayleigh rescaling and so on, where we might get the appearance of width, but it's not.",
            "With that will actually help for generalizations, so in this sort of visualization there sort of implicitly.",
            "There's this idea that a central point is desirable because it will be more robust to these kinds of shifts, but of course if the actual functions are the same.",
            "Then there's no difference between a central point and a peripheral point.",
            "So I think 1 one way to reason about that is to try to build better definitions of the width that are parameterization invariant, perhaps thinking about the Fisher information matrix, I think.",
            "Ideally we would almost want sort of some functions based definition of width, and you know that's something that I'm sort of very excited to think about.",
            "I think, perhaps empirically speaking, if we just sample random directions and we see how much the error is increasing as we move along those directions, it's possible that we've sampled a pathological direction, but I think it will give us some sort of useful intuition about what so connected with this in in the later part of your presentation when you talked about the averaging in parameter space.",
            "Couldn't you get so?",
            "There are two effects here, right?",
            "One?",
            "And this shell that you talk about in the Gaussian shell?",
            "So we could just make the Gaussian shell smaller.",
            "By I mean closer to the center.",
            "If we reduce the learning rate.",
            "But if you need the learning rate then you just make everything smaller.",
            "Anne, Anne, Anne and so you have the choice now between averaging or beginning the learning rate.",
            "But you also.",
            "Show that you get better generalization, not just a better training loss, so can you.",
            "Explain what is going on here.",
            "Yeah, it's a very good question.",
            "I'm so I would say mostly we've observed better generalization and this is actually a bit surprising, so the idea of averaging sometimes appears in convex optimization.",
            "Often it's employed as an exponential moving average with the decaying rate rather than an equally weighted average.",
            "Would say Acyclical or a high concentrate, but there in convex optimization the benefits of averaging have focused around improve convergence, not improve generalization, whereas here what we found when we average with these high concentrates, cyclical rates and so on is that.",
            "It's mostly the generalization that improves a lot.",
            "We're starting to work on improved conversions, but this isn't something that we have noticed as much yet in terms of convergence.",
            "I would say yes, if you could sort.",
            "If you decrease the learning rate than you would decrease the size of this shell and you would move towards the center, but you could still perhaps see convergence a lot faster by having a high high learning rate and kind of bouncing around the shell and averaging versus sort of slowly decaying the rate.",
            "I think there's also issues of like when you make the learning rate really small and so on.",
            "You could get sort of stock, maybe in an undesirable solution.",
            "One kind of interesting thing that we've actually started looking at as well as not just take the first moment basically of the.",
            "Trajectory of SGD but actually consider higher order moments as well.",
            "So if it's true that basically these points with these special learning rate schedules or sampling from some Gaussian distribution, we can use this to create an approximate distribution in weight space and then some sort of forward sample from that through our predictive distribution to get uncertainty estimates and do model averaging in that way.",
            "And so we've actually explored this a little bit, and it workshop a new AI coming up where we take the second moment of this trajectory, and.",
            "Use that to get predictive uncertainties as well, roughly at the same cost as you know, just training a single net.",
            "Hey, thanks for the great talk.",
            "So I had one question regarding your evasion Gan setup and it was essentially.",
            "When and when you typically train again and this is and then just the theoretical part of it, and you want the generator to win, the generator captures entire data distribution.",
            "So in your case, because you're sampling multiple generators, shouldn't that be the same generator?",
            "If it training has succeeded, you can because you don't have that same generator.",
            "Is that evidence that training is not succeeding?",
            "So in the limit of infinite data, I think you would want this distribution to collapse, but given a finite amount of information, there is uncertainty about what's the right generator.",
            "And so basically this just represents that uncertainty.",
            "In a way, rather than having a single generator competing against a single discriminator, you have a whole distribution over generators, and it's competing against the distribution over discriminators, and so if we think about this purely from the generators perspective, we're getting a bunch of adverse aerial feedback.",
            "So let's say we sample from our distribution over parameters that gives us a particular generator that generator is used to produce some samples.",
            "Let's say there's no, that's those samples are terrible, right?",
            "Then we want to basically use that as a likelihood signal in this case, to say we really need to.",
            "Update our posterior over these parameters and find a region that's actually more reasonable, but there's still going to be uncertainty about what's best.",
            "Last question.",
            "I want to ask you about Bayesian optimization and the use of Gaussian processes for hyperparameter optimization.",
            "I think it was very popular in a few few years ago, but it doesn't scale much for a large number of parameters, so I wanted to ask you like do you think it has a future?",
            "Like for helping optimizing hyperparameters for deep neural Nets or?",
            "What's your view about that?",
            "That's a really great question, and it's something I've honestly wondered myself.",
            "I have some thoughts about it though, so six years ago it was showing that you could use Bayesian optimization to automatically at least relatively speaking achieve state of the art results on Cifar 10, and that was very exciting because, you know, we didn't want to spend a lot of time hand tuning these hyperparameters.",
            "This led to, you know, really great interest in Bayesian optimization for hyperparameter learning in machine learning.",
            "Bayesian optimization is a very general sort of thing.",
            "We can use it for all sorts of other sort of applications like AB testing, etc where it does work very well.",
            "Recently, people haven't been using it.",
            "I think very much in practice to achieve a great performance on these vision benchmarks, and I think for a couple of reasons.",
            "I mean, one reason is we've had these benchmarks for awhile and we figured out learning rates that work really well and so there might not actually be a lot to be gained by doing some kind of extensive search.",
            "For those particular applications, however, if you're building a new continent and you're applying it to an entirely new problem, then you need to start from scratch, and so there isn't some good learning rate that someone can give you necessarily.",
            "For instance, and in that case I know certainly companies are extensively using Bayesian optimization to try to tune hyperparameters.",
            "I think that there's still a long way to go, though, like the key issues in Bayesian optimization, our scalability kind of per iteration and.",
            "Applicability that to high dimensional problems, and I think that the latter question is also related to this.",
            "This bigger question of kernel selection.",
            "So actually when I started my PhD, perhaps you know deep learning wasn't as popular as it is now, and one of the criticisms was there are so many design decisions to make you know what architecture, how many hidden layers, hidden units, activation functions, optimization procedure, etc.",
            "And perhaps a lack of a principle framework to make those decisions, whereas.",
            "Things like Gaussian processes.",
            "It felt like you didn't really need to decide a lot and everyone could press the same button and get the same answer everywhere, but hidden beneath that was this very crucial decision of which kernel function to use.",
            "And a lot of people are defaulting to the RBF kernel, which I think is very similar to what would happen if you were to say I want to use the same exact neural architecture for every problem, which would really make sense.",
            "So I think kernel learning actually in a way is a lot like architecture learning, and if we want to have Bayesian optimization methods that work really well in high dimensional spaces, we need kernels that encode very reasonable inductive biases in those spaces.",
            "And we want gradient information as well, and that's related to scalability, so I think it's a very promising research direction.",
            "I know that all the big companies are using Bayesian optimization for a number of different tasks, including hyperparameter learning.",
            "It hasn't been used as much in machine learning research in vision, but that could change as we sort of have fresh problems and fresh models to consider.",
            "Alright, thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you, it's really great to be here like many of the speakers, it seems I really started my PhD at a machine learning summer school, in this case at Cambridge in the UK, and so you know, it really helped me think about research directions for the next sort of five or six years.",
                    "label": 0
                },
                {
                    "sent": "Now I'd like to ask if you've ever wished you could go back in time and tell yourself something.",
                    "label": 0
                },
                {
                    "sent": "Don't do it.",
                    "label": 0
                },
                {
                    "sent": "I've certainly had that feeling many times, and part of this talk will actually be what you know.",
                    "label": 0
                },
                {
                    "sent": "I would say if I could go back to that summer school in 2009 and just say a few things to myself and so some of that will be a bit tutorial in flavor, some of it will be very high level considerations about model construction and model development.",
                    "label": 0
                },
                {
                    "sent": "How do we think about building models that generalize from first principles perspective and some of it will be about some of our latest research in the space of Bayesian deep learning.",
                    "label": 0
                },
                {
                    "sent": "So I do.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To start with the question, let's suppose we have this observed data set of airline passenger numbers indexed by time.",
                    "label": 0
                },
                {
                    "sent": "Since the time series of Dots are observed data points and we'd like to fit this data so that we can make a good extrapolation.",
                    "label": 0
                },
                {
                    "sent": "Say we want to predict airline passenger numbers in 1961, and we're going to consider three choices, choice, one linear Model choice, 2A cubic polynomial Choice 3 or 10 thousandth order polynomial.",
                    "label": 0
                },
                {
                    "sent": "So I'd like to show of hands.",
                    "label": 0
                },
                {
                    "sent": "How many want to go with choice one?",
                    "label": 0
                },
                {
                    "sent": "Choice 2.",
                    "label": 0
                },
                {
                    "sent": "OK choice 3.",
                    "label": 0
                },
                {
                    "sent": "Interesting.",
                    "label": 0
                },
                {
                    "sent": "OK, so I would say about 70% choice, 110% choice 2.",
                    "label": 0
                },
                {
                    "sent": "10% choice 310% shy.",
                    "label": 0
                },
                {
                    "sent": "OK, very interesting so.",
                    "label": 0
                },
                {
                    "sent": "You know those who go with choice one I think are worried about what's called overfitting.",
                    "label": 0
                },
                {
                    "sent": "They want something that will generalize reasonably well.",
                    "label": 0
                },
                {
                    "sent": "They understand they're probably going to some structure in the data, but there's a trade off those who go with choice two, you know typically don't want to miss out on some of the interesting structure there.",
                    "label": 0
                },
                {
                    "sent": "We see quasi periodic trends, etc.",
                    "label": 0
                },
                {
                    "sent": "We're going to miss those if we use a linear function, so we'll go with this too.",
                    "label": 0
                },
                {
                    "sent": "And people with go with choice three are very worried about sort of really missing structure.",
                    "label": 0
                },
                {
                    "sent": "I don't think their choice.",
                    "label": 0
                },
                {
                    "sent": "Is very popular typically, but in this talk I'm going to argue that that's the choice we want to make an.",
                    "label": 0
                },
                {
                    "sent": "In fact, if we could, we would want to go with an infinite order polynomial, and you know, if we could, we would want something even more expressive than that.",
                    "label": 0
                },
                {
                    "sent": "And the key to unlocking this seeming contradiction about how we can go with an expressive model and not overfit really rests with trying to separately understand model specification and how we do learning with those models.",
                    "label": 0
                },
                {
                    "sent": "And so my sort of rationalization for wanting to go with the 10,000.",
                    "label": 0
                },
                {
                    "sent": "Order polynomial or a very expressive.",
                    "label": 0
                },
                {
                    "sent": "Model classes because we believe that actually no matter which choice we make, the truth is probably outside of the model class, but with a very expressive model will be able to get closer to the truth than we could with a simpler model.",
                    "label": 0
                },
                {
                    "sent": "And in fact choices one and two here are special cases of the 10,000 TH order polynomial.",
                    "label": 0
                },
                {
                    "sent": "There's some setting of the parameters of that big polynomial such that we can exactly recover the smaller polynomials, and so it just rests in how we're trying to sort of find those parameters and.",
                    "label": 0
                },
                {
                    "sent": "Additionally, you know often we can do a little bit better if we add a bit of extra structure to our to our models, so Radford Neal has a beautiful passage of his 1996 thesis on Bayesian neural networks where he says that you know, no matter how well we're doing, for example, in performing character recognition, we could do a bit better if we just accounted for something extra.",
                    "label": 0
                },
                {
                    "sent": "You know, irregular writing styles, weird inkblots, whatever it might be.",
                    "label": 0
                },
                {
                    "sent": "There's always something we can add to our model to get better performance.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'll come back to that shortly.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here I have a cartoon from ex KCD sort of in a very broad stroke way, trying to show a difference between the classical in a Bayesian approach.",
                    "label": 0
                },
                {
                    "sent": "So you've got two guys talking.",
                    "label": 0
                },
                {
                    "sent": "The first guy says the neutrino detector measures whether the sun is gone Nova.",
                    "label": 0
                },
                {
                    "sent": "The other guy says then it rolls two dice.",
                    "label": 0
                },
                {
                    "sent": "If they both come up six, it lies to us, otherwise it tells the truth.",
                    "label": 0
                },
                {
                    "sent": "So the other guy says let's try it.",
                    "label": 0
                },
                {
                    "sent": "Detector is the sun going Nova?",
                    "label": 0
                },
                {
                    "sent": "We hear some dice rolling.",
                    "label": 0
                },
                {
                    "sent": "It says yes.",
                    "label": 0
                },
                {
                    "sent": "The frequentist says, well, the probability of this result happening by random chance is 1 / 36.",
                    "label": 0
                },
                {
                    "sent": "16 squared, so the P value is less than .05.",
                    "label": 0
                },
                {
                    "sent": "I conclude that the sun is exploded.",
                    "label": 0
                },
                {
                    "sent": "The other guy says, you know that you $50.",
                    "label": 0
                },
                {
                    "sent": "It hasn't now different people in different areas have different reactions to this cartoon.",
                    "label": 0
                },
                {
                    "sent": "If you ask, a game theorist will say, well, you know anything, right?",
                    "label": 0
                },
                {
                    "sent": "Like what more could I lose if the sun is exploded?",
                    "label": 0
                },
                {
                    "sent": "Now I actually don't think that sometimes you know if we think if we're being very thoughtful, there's necessarily a profound difference between the Bayesian classical approaches.",
                    "label": 0
                },
                {
                    "sent": "Both are actually using priors, just in somewhat different different ways here.",
                    "label": 0
                },
                {
                    "sent": "The intuition, I suppose in this case, is that our prior even just based on, you know, vague daily experiences that it's very unlikely that the sun is just going to spontaneously go Nova, but we probably had a lot of experience.",
                    "label": 0
                },
                {
                    "sent": "For example, playing board games and throwing 2 sixes, and so we just feel you know intuitively that that it's much more likely that this is just happened by random chance, then the sun has has gone Nova and we can actually make that intuition very precise, and we can do hypothesis testing, and we can compare the posterior probabilities of those two hypothesis.",
                    "label": 0
                },
                {
                    "sent": "Under very reasonable assumptions, for example, using domain expertise in physics about Suns, stars going Nova, etc and compute that one is billions of times more likely than the other.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So a bit of motivation.",
                    "label": 0
                },
                {
                    "sent": "You know why do we care about Bayesian deep learning?",
                    "label": 1
                },
                {
                    "sent": "And why do we care about Bayesian machine learning?",
                    "label": 0
                },
                {
                    "sent": "I guess so a lot of the same points would apply, so it's a powerful and coherent framework for model construction and understanding generalization.",
                    "label": 1
                },
                {
                    "sent": "We've had a number of talks showing all the great things we can do with neural networks, but also a lot of it has been mysterious.",
                    "label": 0
                },
                {
                    "sent": "You know why is it that models with more parameters than data points can generalize, etc?",
                    "label": 0
                },
                {
                    "sent": "And we can actually.",
                    "label": 0
                },
                {
                    "sent": "Make those results a bit less mysterious if we view things from a probabilistic Bayesian perspective.",
                    "label": 0
                },
                {
                    "sent": "Sometimes I'll use the word probabilistic and Bayesian interchangeably.",
                    "label": 0
                },
                {
                    "sent": "It depends on the context uncertainty representation I think is a well understood advantage of a Bayesian approach.",
                    "label": 0
                },
                {
                    "sent": "It can be achieved through other means as well, but this is something that's important if we're building models to make decisions, and one might argue from a practical perspective that everything in the end is ultimately to make some kind of decision.",
                    "label": 0
                },
                {
                    "sent": "If we're not making a decision with the model, really.",
                    "label": 0
                },
                {
                    "sent": "What's the point?",
                    "label": 0
                },
                {
                    "sent": "And if we're going to make a decision, we really want some knowledge of uncertainty and even better, we want a full predictive distribution, because then we can wait the different predictions by the loss associated with those predictions.",
                    "label": 0
                },
                {
                    "sent": "For example, if we're building an autonomous vehicle, we want to avoid really rare but costly mistakes.",
                    "label": 1
                },
                {
                    "sent": "Perhaps a less well recognized benefit, especially in the context of neural networks, is also better point estimates.",
                    "label": 0
                },
                {
                    "sent": "You will actually make different predictions.",
                    "label": 0
                },
                {
                    "sent": "It's not just that you're getting error bars on your predictions, it's a different mechanism for arriving at.",
                    "label": 0
                },
                {
                    "sent": "Your point estimates involving something called a Bayesian model average, and actually I think in some cases.",
                    "label": 0
                },
                {
                    "sent": "For example, if you have a lot of data in a fairly simple model, perhaps you won't see much of a difference between the point estimates of a Bayesian and classical approach.",
                    "label": 0
                },
                {
                    "sent": "But if you have a very rich neural network and you're doing proper Bayesian marginalization, I think you can see a really massive difference actually in the types of predictions that are being made, so I think there's really a lot of promises, specially in neural networks research.",
                    "label": 0
                },
                {
                    "sent": "To try to develop good Bayesian inference procedures.",
                    "label": 0
                },
                {
                    "sent": "Another reason that this is interesting is because it has worked really well before.",
                    "label": 0
                },
                {
                    "sent": "So in what sometimes called the 2nd wave of neural networks at the very end, Radford Neal developed these incredible Bayesian neural networks that Toronto that we're winning all the competitions and so on.",
                    "label": 0
                },
                {
                    "sent": "And it was through these these very principled probabilistic MCMC approaches, and the reason we don't use those now is because they are computationally intractable for the types of models that were interested in now.",
                    "label": 0
                },
                {
                    "sent": "But this is sort of innocence.",
                    "label": 0
                },
                {
                    "sent": "Let us know that there's a lot of gold buried in this direction, and we just need the right tools to be able to access that gold.",
                    "label": 0
                },
                {
                    "sent": "So that's also good motivation for PhD research.",
                    "label": 0
                },
                {
                    "sent": "We want to build these tools together.",
                    "label": 0
                },
                {
                    "sent": "OK, why not?",
                    "label": 1
                },
                {
                    "sent": "So I've already sort of said it.",
                    "label": 0
                },
                {
                    "sent": "You know, these methods can be very computationally intractable.",
                    "label": 1
                },
                {
                    "sent": "They don't necessarily have to be.",
                    "label": 0
                },
                {
                    "sent": "And they can involve a lot of moving parts, so sometimes our algorithms can start to look quite complicated if we're trying to be probabilistic about everything we do.",
                    "label": 0
                },
                {
                    "sent": "But as some of the speakers have mentioned, we can even start from that place and then at least we know what we want to do in principle, and we can derive then approximations that might sort of get us what I want, what we want in sort of a reasonable amount of time, and I would say there's been really a lot of progress in the last two years.",
                    "label": 0
                },
                {
                    "sent": "Addressing these limitations, it now is possible to do Bayesian inference in state of the art neural networks using things like stochastic gradient.",
                    "label": 0
                },
                {
                    "sent": "Hamiltonian Monte Carlo and certain variational methods without a lot of additional computational overhead, I think there's still a massive way to go, but we've been making incredible progress.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's return to this airline passenger number example.",
                    "label": 0
                },
                {
                    "sent": "We use this to set up a bit of notation as well, so we have this basic regression problem.",
                    "label": 0
                },
                {
                    "sent": "We have our observations which were noting by RY values.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "They are indexed by our inputs X one up to XN and our goal is to predict output at some test input X star and I'd like to ask you, you know, let's suppose we're just going right back to high school.",
                    "label": 1
                },
                {
                    "sent": "How might you approach this problem?",
                    "label": 0
                },
                {
                    "sent": "What would be your algorithm for trying to solve this problem?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So probably you would start by thinking about function classes that you're familiar with, so we considered this type of function here, sometimes called a linear basis function model nonlinear in the inputs, linear in the parameters W. So that would be like a polynomial.",
                    "label": 1
                },
                {
                    "sent": "For example, we could have a linear model, or we could even have a really flexible model which is nonlinear in both the parameters in the inputs, like a neural network.",
                    "label": 1
                },
                {
                    "sent": "So we would basically guess a function we would look at the data points, try to guess something that looks sort of reasonable.",
                    "label": 0
                },
                {
                    "sent": "We might see some trends fail.",
                    "label": 0
                },
                {
                    "sent": "We might want to.",
                    "label": 0
                },
                {
                    "sent": "A trigonometric function in there somewhere, etc.",
                    "label": 0
                },
                {
                    "sent": "And then we would go about trying to learn the parameters of this function, and probably the simplest thing that you might do is to define some kind of error measure on the training sets.",
                    "label": 0
                },
                {
                    "sent": "They will let's, let's set those parameters so as to minimize the distance between the outputs of our function and the values of those data points where they've been observed and squared error is a popular choice, so we could differentiate that and use conjugant gradients or something like this to try to find some values of the parameters.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An alternative approach would be to say that our data is generated by some noise free function polynomial, whatever it might be, plus some noise donated to note it.",
                    "label": 0
                },
                {
                    "sent": "Here is epsilon effects.",
                    "label": 0
                },
                {
                    "sent": "This could be a Gaussian distribution or something else.",
                    "label": 1
                },
                {
                    "sent": "That noise will have noise variance.",
                    "label": 0
                },
                {
                    "sent": "In this case Sigma squared.",
                    "label": 0
                },
                {
                    "sent": "So Sigma is very large.",
                    "label": 0
                },
                {
                    "sent": "That means there's a lot of noise, and then from that we can form what's called a likelihood.",
                    "label": 0
                },
                {
                    "sent": "So the probability of our of our data given our parameters.",
                    "label": 0
                },
                {
                    "sent": "And then we can maximize that with respect to our parameters so that we want the probability of having observed that data to be as large as possible for the parameters that we want to use.",
                    "label": 0
                },
                {
                    "sent": "So that's often called maximum likelihood if we make certain choices like we have here like that, we have Gaussian noise.",
                    "label": 0
                },
                {
                    "sent": "For instance, we can see by just taking the log of the likelihood and maximizing that with respect to W that will actually get exactly the same answer that we would get if we were to minimize squared error.",
                    "label": 1
                },
                {
                    "sent": "And so you know, in a sense this you know at first glance might seem a bit deflationary.",
                    "label": 0
                },
                {
                    "sent": "Well, what's the point?",
                    "label": 0
                },
                {
                    "sent": "If we're making the same predictions in this case, while the point here is that we actually now can interpret that error measure, we know that these assumptions lead us to this squared error metric, whereas before we might not have a good reason to prefer squared error over absolute error over any any of the other possible choices.",
                    "label": 0
                },
                {
                    "sent": "And so this provides us with sort of a formula for model construction.",
                    "label": 0
                },
                {
                    "sent": "This is something to think about, actually, when we're doing deep learning quite often now we just default to things like cross entropy loss if we're doing classification L2, regularised mean squared error if we're doing regression, etc.",
                    "label": 0
                },
                {
                    "sent": "And we could actually very easily think of a lot of other reasonable loss functions starting from this probabilistic perspective.",
                    "label": 0
                },
                {
                    "sent": "Another thing that.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Often done is.",
                    "label": 0
                },
                {
                    "sent": "Regularization, so you might want to add a penalty here to prevent, for example, your weights from becoming very large.",
                    "label": 0
                },
                {
                    "sent": "This has an issue of having to decide again what kind of penalty you want to add, how much you want to penalize this complexity, how we've been going to sort of reason and define complexity.",
                    "label": 0
                },
                {
                    "sent": "Is this going to be parameterisation invariants, etc.",
                    "label": 0
                },
                {
                    "sent": "And you know that could involve a lot of sort of hand tuning and an intervention.",
                    "label": 0
                },
                {
                    "sent": "Again, we can actually perceive.",
                    "label": 0
                },
                {
                    "sent": "These types of penalties is putting a prior on parameters and maximizing a posterior with respect to those parameters, sometimes called map optimization maximum posteriori.",
                    "label": 1
                },
                {
                    "sent": "This is not a Bayesian thing to do, though this is still, you know, optimization.",
                    "label": 1
                },
                {
                    "sent": "Bayesians want to do marginalization, we'll get to that shortly, but this does provide a probabilistic perspective, so we could basically see our prior in this case as a regularizer, and we could use that to try and decide how we're going to construct useful priors.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So when we're trying to follow a Bayesian approach to inference, there's very little we really need to know.",
                    "label": 0
                },
                {
                    "sent": "It's basically just the sum in the product rules that I've given here in equation 7 and eight.",
                    "label": 0
                },
                {
                    "sent": "If we want the marginal distribution for some variable X, we can sum out all the other variables from a joint distribution.",
                    "label": 0
                },
                {
                    "sent": "And this product rule P of X&Y, the joint distribution of X&Y is equal to the conditional distribution of X given y *, P Y, or equivalently, the conditional distribution of Y given X * P X, and we can use those rules to derive Bayes rule etc.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is the equation that we really want to evaluate if performing.",
                    "label": 0
                },
                {
                    "sent": "If we're following a fully Bayesian approach, here we have our predictive distribution given our parameters W and we're waiting those parameters by the posterior probabilities.",
                    "label": 1
                },
                {
                    "sent": "This could basically correspond to saying, OK, well we want to actually use in the continuous setting an uncountably infinite space of models corresponding to every possible setting of W. And if we're using a neural network, then this is going to be a very rich class of functions.",
                    "label": 0
                },
                {
                    "sent": "You could actually sort of see each different setting of the weights is very different function.",
                    "label": 0
                },
                {
                    "sent": "We're going to wait each of those functions by their posterior probabilities.",
                    "label": 1
                },
                {
                    "sent": "You know, if we follow this approach and we try to honestly represent our beliefs in the model, we typically will generalize.",
                    "label": 0
                },
                {
                    "sent": "We won't need to worry about things like overfitting and we will get a predictive distribution.",
                    "label": 0
                },
                {
                    "sent": "The difficulty with this approach is often this integral is very hard to evaluate, so often we don't have an analytic expression for this integral.",
                    "label": 0
                },
                {
                    "sent": "The common approaches to dealing with this is our include Markov chain Monte Carlo and deterministic approximations.",
                    "label": 0
                },
                {
                    "sent": "Very recently, variational approximations, which are a specific type of deterministic approximation.",
                    "label": 0
                },
                {
                    "sent": "Gained a lot of popularity, so to be clear, a deterministic approximation would say let's approximate our posterior P of W given Y by some other density for which this integral does become tractable, and a Markov joint Monte Carlo approach would involve sampling from this posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "Now we can also view the classical approach is a special case of this Bayesian model average, where our posterior P of W given Y is just a Delta function, it's collapsed on the maximum likelihood setting of the parameters.",
                    "label": 1
                },
                {
                    "sent": "So that means there's only going to be one term that really counts in this integral, and that will recover us that answer.",
                    "label": 0
                },
                {
                    "sent": "And that's actually a useful way of thinking about how these two approaches compare and when and why they might make different predictions.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so sort of just to wrap up the tutorial part of the talk will finish with an example that's really made a deep impression.",
                    "label": 0
                },
                {
                    "sent": "It seems on students in my class at Cornell on Bayesian machine learning, so later in the I sort of have a number of different examples of Bayesian inference in this course and later on in tests and exams when I'm asking sort of for an explanation of how Bayesian inference works, it's been surprising to me how often this particular example has been referred to, so let's go through it briefly.",
                    "label": 0
                },
                {
                    "sent": "So let's suppose we have.",
                    "label": 1
                },
                {
                    "sent": "A bent coin, sometimes called a biased coin with probability Lambda of landing tails.",
                    "label": 1
                },
                {
                    "sent": "We want to know what's the likelihood of a set of data X1 to XM, so that would give.",
                    "label": 1
                },
                {
                    "sent": "That would be the result of each flip.",
                    "label": 0
                },
                {
                    "sent": "That we want to use that to find the maximum likelihood solution for Lambda.",
                    "label": 0
                },
                {
                    "sent": "The probability that we're going to get tails on any individual flip, and then we want to think about what our prediction would be if we observed to date 2 two flips and they both came out tails when we're trying to think about, probably the next clips of Tails would be, so that would basically be a special case of this question.",
                    "label": 0
                },
                {
                    "sent": "One where we have X1 and X2.",
                    "label": 0
                },
                {
                    "sent": "They're both pills.",
                    "label": 0
                },
                {
                    "sent": "We go in computer maximum likelihood solution, so maybe just take.",
                    "label": 0
                },
                {
                    "sent": "A minute and just think about that.",
                    "label": 0
                },
                {
                    "sent": "Maybe write down a few equations.",
                    "label": 0
                },
                {
                    "sent": "Think about how you would approach that problem.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we can write down our likelihood.",
                    "label": 0
                },
                {
                    "sent": "So this is just a binomial distribution.",
                    "label": 0
                },
                {
                    "sent": "We have N flips.",
                    "label": 0
                },
                {
                    "sent": "We want the likelihood of getting em tails.",
                    "label": 1
                },
                {
                    "sent": "The probability of getting tail is Lambda here.",
                    "label": 0
                },
                {
                    "sent": "So this is something that we can maximize with respect to Lambda.",
                    "label": 0
                },
                {
                    "sent": "We can take the log, differentiate with respect to Lambda, double check that we're actually getting the maximum of this distribution, and so on.",
                    "label": 0
                },
                {
                    "sent": "And if we do that, we'll find the solution for Lambda will be M over an, so M is the number of tails an is the total number of flips.",
                    "label": 1
                },
                {
                    "sent": "And so you know, if we were trying to decide, you know what the probability for the next flip would be.",
                    "label": 0
                },
                {
                    "sent": "We would just substitute in those values for M&N and so if we see two tails then that means M / N is going to be 1.",
                    "label": 0
                },
                {
                    "sent": "And that means our prediction is that the next flip will be tails with 100% probability.",
                    "label": 0
                },
                {
                    "sent": "So do we believe this estimate?",
                    "label": 0
                },
                {
                    "sent": "All you know is that there's a bias point.",
                    "label": 0
                },
                {
                    "sent": "I flip it twice.",
                    "label": 0
                },
                {
                    "sent": "I see two tails.",
                    "label": 1
                },
                {
                    "sent": "My prediction is that it's always good to flip tails from that point forward.",
                    "label": 0
                },
                {
                    "sent": "No, that's not intuitive to us because you know it doesn't involve sort of a prior that would really make sense.",
                    "label": 0
                },
                {
                    "sent": "So let's think about this from a Bayesian perspective.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's choose a prior that has this functional form so that the posterior has the same form.",
                    "label": 1
                },
                {
                    "sent": "So when we multiply this thing together with the likelihood, we get a posterior.",
                    "label": 0
                },
                {
                    "sent": "Then when we do that, we'll see that our posterior is beta distributed and we can write down analytically the mean and the variance for Lambda here.",
                    "label": 1
                },
                {
                    "sent": "And we can go with quite a variety in this case of beta priors.",
                    "label": 1
                },
                {
                    "sent": "This isn't the only prior we could choose.",
                    "label": 0
                },
                {
                    "sent": "There are many possibilities, but this is nice because we can write down in closed form what the poster looks like.",
                    "label": 0
                },
                {
                    "sent": "So if we believe.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The bias was anything between zero and one with equal probability.",
                    "label": 0
                },
                {
                    "sent": "That's sort of like saying, you know I know nothing.",
                    "label": 0
                },
                {
                    "sent": "We just representing that that sort of belief into the model.",
                    "label": 0
                },
                {
                    "sent": "We can do that by just setting a = 1 B equals one with that prior.",
                    "label": 0
                },
                {
                    "sent": "If we believe that the bias was maybe concentrated around 50%, we could also incorporate that into our prior.",
                    "label": 0
                },
                {
                    "sent": "I think that might actually be more reasonable than saying I know nothing but saying I know nothing is still going to give us a very different answer than the maximum likelihood solution.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "So that's what I'm doing here.",
                    "label": 0
                },
                {
                    "sent": "So conjugate meaning that the posterior has the same form as the prior, the same functional form.",
                    "label": 0
                },
                {
                    "sent": "So I've done that.",
                    "label": 0
                },
                {
                    "sent": "You don't have to do that, I would say generally try to choose the prior that you believe in.",
                    "label": 0
                },
                {
                    "sent": "There is a bit of nuance associated with that, but.",
                    "label": 0
                },
                {
                    "sent": "That's what I would go with.",
                    "label": 0
                },
                {
                    "sent": "I'm OK so.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So when we choose this beta prior, we can just leave.",
                    "label": 0
                },
                {
                    "sent": "A&B is variables for now.",
                    "label": 0
                },
                {
                    "sent": "And we look at the posterior mean for the parameter Lambda.",
                    "label": 0
                },
                {
                    "sent": "We can see that.",
                    "label": 0
                },
                {
                    "sent": "A&B these sort of parameters of the prior actually act as pseudo observations.",
                    "label": 0
                },
                {
                    "sent": "Oh, and this actually is a very nice interpretation, because we can then view posterior inference is kind of an online procedure where we we have start with their prior with some setting of A&B we observe a certain number of flips.",
                    "label": 0
                },
                {
                    "sent": "We compute our posterior and then our posterior becomes kind of our old prior.",
                    "label": 0
                },
                {
                    "sent": "That's the number of observations we have.",
                    "label": 0
                },
                {
                    "sent": "We see some more flips.",
                    "label": 0
                },
                {
                    "sent": "We can update our post here.",
                    "label": 0
                },
                {
                    "sent": "So that will give us the same answer that if as if we basically just did everything in batch and whenever we are computing these estimates it's good to sort of test the limits so.",
                    "label": 0
                },
                {
                    "sent": "You know what happens if we make A&B really, really large?",
                    "label": 0
                },
                {
                    "sent": "So if we make a really large, this means that we're going to be very biased in our post here.",
                    "label": 0
                },
                {
                    "sent": "Just saying the next flip is Tails.",
                    "label": 1
                },
                {
                    "sent": "That makes sense, right?",
                    "label": 0
                },
                {
                    "sent": "Like we want to kind of understand the effect of prior is having on our post here.",
                    "label": 0
                },
                {
                    "sent": "So if we're super confident in our prior work, our job is done.",
                    "label": 0
                },
                {
                    "sent": "We don't even really need to do inference like who cares about the data.",
                    "label": 0
                },
                {
                    "sent": "Now, if he is very large then you know.",
                    "label": 0
                },
                {
                    "sent": "Similarly, the posterior will say OK, well doesn't really matter.",
                    "label": 0
                },
                {
                    "sent": "You know what happened with the flips and if we have an infinite amount of data, then both M&N are going to be very very large and so those terms will completely dominate in the prior won't matter anymore and so this is another feature of Bayesian inference.",
                    "label": 0
                },
                {
                    "sent": "Once you have enough data, the likelihood dominates and you do get the same answer.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now I'm going to talk about.",
                    "label": 0
                },
                {
                    "sent": "Kind of thinking about prior selection, so this is a big question.",
                    "label": 0
                },
                {
                    "sent": "How do you choose your prior?",
                    "label": 0
                },
                {
                    "sent": "I would say this is very related to how do you choose your model?",
                    "label": 0
                },
                {
                    "sent": "You know there are both innocence priors.",
                    "label": 0
                },
                {
                    "sent": "We both know that you know the explicit distribution over parameters and the functional form of the model that we choose will be misspecified.",
                    "label": 0
                },
                {
                    "sent": "To some extent.",
                    "label": 0
                },
                {
                    "sent": "All we can do is try to honestly represent our beliefs, and in a way these two things aren't really separate.",
                    "label": 0
                },
                {
                    "sent": "What really matters is not the prior over the parameters, it's how that distribution over the parameters interacts with the functional form of the model to induce a distribution over functions.",
                    "label": 0
                },
                {
                    "sent": "And so this is a perspective that has really been championed in Gaussian process modeling.",
                    "label": 1
                },
                {
                    "sent": "But it applies also quite generally.",
                    "label": 1
                },
                {
                    "sent": "So this kind of approach was actually taken somewhat by Radford Neal in 1996 in his PhD thesis, when he showed that some.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "But we can sort of think about prior selection over parameters of neural networks by first sort of sampling from our distribution parameters.",
                    "label": 0
                },
                {
                    "sent": "Conditioning on those and drawing the corresponding functions and seeing what kinds of properties they had because we wanted a prior that was intuitive in function space.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so here's the sort of formal definition of a Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "Don't worry if you don't sort of get everything.",
                    "label": 0
                },
                {
                    "sent": "The main thing here is this sort of function space view of modeling.",
                    "label": 0
                },
                {
                    "sent": "So a Gaussian process is defined as a collection of random variables.",
                    "label": 1
                },
                {
                    "sent": "Any finite number of which have a joint Gaussian distribution.",
                    "label": 1
                },
                {
                    "sent": "And this notation here at this distributed is a GP means that any collection of function values evaluated at any collection of.",
                    "label": 0
                },
                {
                    "sent": "Of inputs, X has a joint multivariate Gaussian distribution with a mean vector muina covariance matrix K, defined by what's called the mean function of the GP and the kernel or covariance function of the GP little K. And so on.",
                    "label": 0
                },
                {
                    "sent": "The left panel.",
                    "label": 0
                },
                {
                    "sent": "Here we have sample prior functions from this model, so this basically is telling us what types of solutions we think are a priority likely.",
                    "label": 0
                },
                {
                    "sent": "And on the right we've observed some data.",
                    "label": 0
                },
                {
                    "sent": "And then we've we've inferred a posterior distribution over functions that can fit our data.",
                    "label": 0
                },
                {
                    "sent": "So we see in black, purple, and Green 3 sample functions from this posterior over functions.",
                    "label": 0
                },
                {
                    "sent": "And in blue we see the posterior mean.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of the expectation of our post year over functions given data in shade.",
                    "label": 0
                },
                {
                    "sent": "We also so 95% of the predictive density.",
                    "label": 0
                },
                {
                    "sent": "So 95% of these functions will be contained within that shade.",
                    "label": 0
                },
                {
                    "sent": "So that gives us a sense of uncertainty as well, and so I'm going to argue in this talk it's going to be very natural to think in function space when we're trying to do Bayesian deep learning, because often it is very hard to represent our beliefs over the parameters.",
                    "label": 0
                },
                {
                    "sent": "Of internal network.",
                    "label": 0
                },
                {
                    "sent": "The weights because this is a very complicated kind of object, but it's not so hard necessarily to represent our belief over functions.",
                    "label": 0
                },
                {
                    "sent": "If we think our functions are smooth, periodic, finitely differentiable, very over some kind of scale.",
                    "label": 0
                },
                {
                    "sent": "These are all intuitions we can incorporate into our prior.",
                    "label": 0
                },
                {
                    "sent": "OK, so in fact a lot of models are examples of Gaussian processes, and there's a bit of a joke in machine learning that you know everything, in a sense is a Gaussian process with a special type of kernel function.",
                    "label": 0
                },
                {
                    "sent": "And so let's start with some very simple examples so.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We could consider a linear model F of X = a non plus A1 X.",
                    "label": 0
                },
                {
                    "sent": "Everything is just one dimensional here and let's put just a standard normal distribution over a, not in a one.",
                    "label": 0
                },
                {
                    "sent": "Now we can get a sense of the induced distribution over functions by sampling values of a naughtone one from the standard normal conditioning on those samples and then drawing straight lines with different slopes and intercepts.",
                    "label": 0
                },
                {
                    "sent": "And so we can see that, for example, had we instead of choosing a standard normal distribution, chosen something with a variance gamma, and then wanted me to be very small that these lines would for instance, become horizontal, that their slope would be very close to zero, and so we can control this induced distribution over functions through the distribution on the parameters here.",
                    "label": 0
                },
                {
                    "sent": "And we.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then go ahead and show that any collection of function values evaluated at any set of inputs X one up to XN will have a joint Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "And we.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can derive what the covariance function would be so we can just basically use the definition of covariance.",
                    "label": 0
                },
                {
                    "sent": "The expectation of F of X I * F of XJ minus the product of the expectations over F of XINF of XJ.",
                    "label": 0
                },
                {
                    "sent": "And we can workout.",
                    "label": 0
                },
                {
                    "sent": "This is the kernel, and so we can now go ahead and use our Gaussian process model in function space instead of worrying explicitly about these parameters a not in a one, which is sort of what I was doing at the beginning of this talk when I presented the Bayesian model averaging over parameters.",
                    "label": 0
                },
                {
                    "sent": "We can also do this for more general classes of models, so any model actually which can be expressed as an inner product of a vector of parameters times some vector of basis functions.",
                    "label": 0
                },
                {
                    "sent": "This could be polynomials, Fourier series, whatever else can be shown to be a Gaussian process with a particular type of kernel function.",
                    "label": 0
                },
                {
                    "sent": "Really.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Popular kernel function is called the RBF kernel.",
                    "label": 1
                },
                {
                    "sent": "Some people call this the King of kernels.",
                    "label": 0
                },
                {
                    "sent": "I don't really like that description because there are some some issues, yeah?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so in a very intuitive sense, a kernel tells us the similarity between a pair of data points and so in the case of Gaussian process, if we're considering two outputs of our function, it says how similar are those outputs if we know, for instance, the distance between the inputs and so this Colonel here right here, which is saying what's the covariance between our random function evaluated at a point X and appoint ex prime depends on the Euclidean distance between X&X.",
                    "label": 0
                },
                {
                    "sent": "Prime and this parameter Elvis like scale parameter and so this is a very intuitive kernel because what it's saying is that function values that are close together in input space.",
                    "label": 0
                },
                {
                    "sent": "So airline passenger numbers that are close together in time are going to be more correlated than airline passenger numbers that are far away in time.",
                    "label": 1
                },
                {
                    "sent": "So this is an example of what's called an inductive bias.",
                    "label": 0
                },
                {
                    "sent": "So this is a very popular kernel function, partly because it's very intuitive.",
                    "label": 0
                },
                {
                    "sent": "It represents an inductive bias that probably generalizes to a large number of problems.",
                    "label": 0
                },
                {
                    "sent": "We have these parameters that we can learn the signal variance A that's kind of how much the functions are oscillating vertically, and the lengthscale L which sort of says you know how we clear the functions.",
                    "label": 0
                },
                {
                    "sent": "And they have very nice theoretical properties.",
                    "label": 0
                },
                {
                    "sent": "So in the appendix of this talk I actually have a derivation from the RBF kernel starting from basically this standard basis function approach where we work with a model that has some parameters in some inputs, and we want to drive what the kernel function is and what's quite incredible about the RBF kernel is that it corresponds to an infinite basis expansion.",
                    "label": 0
                },
                {
                    "sent": "So basically this function space representation where we're working with kernels instead of with, say weights, allows us to work with models that have.",
                    "label": 0
                },
                {
                    "sent": "An infinite number of weights and an infinite number of basis functions.",
                    "label": 0
                },
                {
                    "sent": "So I started at the beginning with the 10,000 TH order polynomial.",
                    "label": 0
                },
                {
                    "sent": "It basically lets us use the infinite order polynomial and that's really beautiful.",
                    "label": 0
                },
                {
                    "sent": "It says we can use these infinite models.",
                    "label": 0
                },
                {
                    "sent": "Nonparametric models with a finite amount of computational power.",
                    "label": 0
                },
                {
                    "sent": "If we move to this representation, and you can imagine this would have incredible theoretical properties like universal approximation etc that within our prior we could contain for instance any continuous function.",
                    "label": 0
                },
                {
                    "sent": "So to get a bit of intuition about this particular kernel will start by just saying we want to query our random function at a set of points, and that's really what I did.",
                    "label": 0
                },
                {
                    "sent": "You know here the black dots here represent a set of inputs X that I chose.",
                    "label": 0
                },
                {
                    "sent": "I constructed my multivariate Gaussian distribution and then I sample from that distribution to get these different functions for the solid curves.",
                    "label": 0
                },
                {
                    "sent": "Here I just joined the dots together, but of course we can only query this function at a finite number of points in memory.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here in Matlab code I've.",
                    "label": 1
                },
                {
                    "sent": "I've chosen a set of points from minus 10 to 10 space .2 apart.",
                    "label": 0
                },
                {
                    "sent": "I create my covariance matrix by evaluating this RBF kernel at all possible pairs of points in this input set.",
                    "label": 0
                },
                {
                    "sent": "So this gives me sort of an N by N matrix, where N is the number of elements in X.",
                    "label": 0
                },
                {
                    "sent": "And now I just want to sample from a multivariate Gaussian distribution that has that covariance matrix.",
                    "label": 1
                },
                {
                    "sent": "And so this is just some some math that allows us to do that.",
                    "label": 0
                },
                {
                    "sent": "We can take what's called the Cholesky decomposition and multiply that against just a vector of points sampled from a standard normal distribution.",
                    "label": 0
                },
                {
                    "sent": "And when we do that, we can get these.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So basically, each of these three curves corresponds to sampling points from exactly the same multivariate normal distribution with the same covariance matrix three different times.",
                    "label": 0
                },
                {
                    "sent": "And that allows us to sample from our distribution over functions exactly in the same way we were sampling from our distribution over functions when we considered the straight lines.",
                    "label": 0
                },
                {
                    "sent": "Remember, we just sample the parameters.",
                    "label": 0
                },
                {
                    "sent": "In that case conditioned on the parameters and then drew the corresponding function.",
                    "label": 0
                },
                {
                    "sent": "Here we're just doing it more directly or directly sampling from this induced distribution over functions, and we could have done that in the linear case as well.",
                    "label": 0
                },
                {
                    "sent": "We could have just said, well, this is the kernel for that model will evaluate that kernel at all possible pairs of inputs in that space, and then we'll create our covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "And will sample from that multivariate normal distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here we have a visualization of the RBF covariance matrix that we get in this particular case where we have just ordered 1 dimensional inputs and we evaluate our kernel, all possible pairs.",
                    "label": 0
                },
                {
                    "sent": "This is basically showing us that the entries are the largest on the diagonal and then they start to get smaller and smaller and this makes sense.",
                    "label": 0
                },
                {
                    "sent": "What that means is that function values that are close together are more correlated than function values that are far apart in the input space.",
                    "label": 0
                },
                {
                    "sent": "The entries on the diagonal are actually.",
                    "label": 0
                },
                {
                    "sent": "Function values queried at the same inputs so nothing could really be more correlated than the function value with itself.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so in practice when people are working with these Gaussian process models, they follow a two step procedure.",
                    "label": 0
                },
                {
                    "sent": "They first use what's called a marginal likelihood.",
                    "label": 0
                },
                {
                    "sent": "So this is an objective function.",
                    "label": 0
                },
                {
                    "sent": "It's computed by basically doing that integral that I showed earlier, integrating over the Gaussian process itself, so we can actually marginal marginalized away this whole distribution over functions, and this leaves us with a distribution that only depends on data which are the hyper parameters of the model.",
                    "label": 0
                },
                {
                    "sent": "Like that link scale parameter.",
                    "label": 0
                },
                {
                    "sent": "That I mentioned in the RBF kernel, and So what we do is we optimize that objective.",
                    "label": 0
                },
                {
                    "sent": "We condition on the parameters that we get when we do that and then we form our predictors distribution and this allows us to make predictions with that model.",
                    "label": 1
                },
                {
                    "sent": "I don't expect you to parse all of this sort of notation in real time, but what you should know is basically often practitioners do follow this two step procedure.",
                    "label": 0
                },
                {
                    "sent": "They have an objective for learning any parameters that are leftover in their kernel function and they have an objective for making predictions conditioned on those parameters.",
                    "label": 0
                },
                {
                    "sent": "And the data why and the key bottleneck here is solving a linear system involving an N by N matrix are covariance matrix and computing a log determinant over that matrix which naively incurs cubic computations with a number of training points that we have.",
                    "label": 0
                },
                {
                    "sent": "So what this means is that naively, we can only apply these models to problems with.",
                    "label": 0
                },
                {
                    "sent": "2000 data points at most.",
                    "label": 0
                },
                {
                    "sent": "No, that's not true anymore.",
                    "label": 0
                },
                {
                    "sent": "We can actually apply these models to problems with millions of points, but we have to do a bit extra.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'll give an example briefly just doing inference with RBF kernel, so we specify our distribution over functions.",
                    "label": 0
                },
                {
                    "sent": "We choose an RBF kernel, we choose some values for the signal variance in the light scale we have our distribution over functions in our prior on the left panel here and then we have our distribution or functions in the posterior conditioned on some observations in the right, and I want you to tell me if anything looks kind of funny about these functions.",
                    "label": 1
                },
                {
                    "sent": "Would we be happy with these predictions given by the solid blue curve on the right?",
                    "label": 0
                },
                {
                    "sent": "And would we be happy with these sample poster year functions?",
                    "label": 0
                },
                {
                    "sent": "Ha.",
                    "label": 0
                },
                {
                    "sent": "So are we happy with this fit?",
                    "label": 0
                },
                {
                    "sent": "So if we observe these data points in the right panel and we were using this solid blue curve to extrapolate between the points, would we be happy with that?",
                    "label": 0
                },
                {
                    "sent": "Like does that seem like a good predictor?",
                    "label": 0
                },
                {
                    "sent": "And if not, what is wrong with it?",
                    "label": 0
                },
                {
                    "sent": "Little louder.",
                    "label": 0
                },
                {
                    "sent": "Not I.",
                    "label": 0
                },
                {
                    "sent": "Right, yeah, so it's basically just saying that the points aren't correlated like that like it sees this point over around, you know, minus four or so, and then it shoots back down to zero very quickly.",
                    "label": 0
                },
                {
                    "sent": "And so in a sense, if we can see this, perhaps even more clearly if we look at the sample, posterior functions are very wiggly compared to what seems to be the scale of the data.",
                    "label": 0
                },
                {
                    "sent": "It's really not modeling the correlations between the points very, very effectively, and in this case this is because we've chosen a too small length scale and we could take that to its extreme.",
                    "label": 0
                },
                {
                    "sent": "Like if we chose the smallest possible like scale than our prior would be that our data is white noise.",
                    "label": 0
                },
                {
                    "sent": "And of course that's a terrible prior, and we're not going to learn very much.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we can increase the length scale a lot and observe the opposite effect.",
                    "label": 0
                },
                {
                    "sent": "So here's our new induced distribution over functions in the prior on the left.",
                    "label": 0
                },
                {
                    "sent": "So basically the functions that are a priori likely are very slowly very very simple.",
                    "label": 0
                },
                {
                    "sent": "We observe some data, we can see that the fit is probably simpler than we would like.",
                    "label": 0
                },
                {
                    "sent": "We're missing a lot of structure in the data points near the right side of this right panel.",
                    "label": 0
                },
                {
                    "sent": "An uncertainty estimates also look a bit off.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And so it's very important to think about how we can set these hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "They really control things like the complexity of our induced distribution over functions.",
                    "label": 0
                },
                {
                    "sent": "And so I.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mentioned briefly, that we often use this objective called the marginal likelihood.",
                    "label": 0
                },
                {
                    "sent": "So I'll motivate, motivate that a little bit more, so the marginal likelihood corresponds to what we get in this case, if we were to integrate over our Gaussian process model in words, it's the probability that we would sample our data set if we were to randomly sample from the parameters of our model in a prior.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "The left panel here I have this visualization that David Mackay had in his PhD thesis where he was considering deterministic approximations for Bayesian neural Nets on the horizontal axis.",
                    "label": 0
                },
                {
                    "sent": "We have all possible datasets.",
                    "label": 1
                },
                {
                    "sent": "On the vertical axis we have the marginal likelihood for each of these datasets.",
                    "label": 1
                },
                {
                    "sent": "So the probability of generating that data set if we were to randomly sample from the parameters of the model.",
                    "label": 0
                },
                {
                    "sent": "So in this illustration, a simple model is only going to be able to generate a very small range of datasets, so let's go back to that linear model.",
                    "label": 0
                },
                {
                    "sent": "We had a not plus a 1X and some distribution over a knot in a one.",
                    "label": 0
                },
                {
                    "sent": "Every time we re sample a knot in a one from the prior, we get a different straight line with a different slope and intercept.",
                    "label": 0
                },
                {
                    "sent": "But of course that's not going to describe very many datasets, and so it has absolutely no support for most of the datasets in this diagram.",
                    "label": 0
                },
                {
                    "sent": "At the same time, because this is a proper probability density, it has to normalize etc.",
                    "label": 0
                },
                {
                    "sent": "It's going to give a lot of density to those datasets that it can generate, so that's why the curve is actually quite high here.",
                    "label": 0
                },
                {
                    "sent": "Conversely, we could choose an extremely flexible model, and we could have a very diffuse distribution over its parameters in the prior, and in that case every time we re sample the parameters and draw the corresponding outputs, we get a completely different data set.",
                    "label": 0
                },
                {
                    "sent": "And similarly because this is sort of a normalizable distribution and we're not concentrating our support in any particular space, this is going to have relatively low probability for any given data set.",
                    "label": 0
                },
                {
                    "sent": "So from this perspective, if we condition on a particular data set, the evidence is going to favor a model of appropriate complexity.",
                    "label": 0
                },
                {
                    "sent": "And when we do that in the case of Gaussian processes, and we use this marginal likelihood to learn the kernel hyperparameters, we get this fit in green here.",
                    "label": 1
                },
                {
                    "sent": "In red and pink I showed the two previous bids that we had on it the last slides.",
                    "label": 0
                },
                {
                    "sent": "We can see intuitively that this fit is a lot nicer.",
                    "label": 0
                },
                {
                    "sent": "It seems to have a more appropriate level of complexity.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the objective that we get when we do this with Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "We can see that it compartmentalizes into two key terms here, one associated with model fit, one associated with complexity, which is a log determinant over this covariance matrix that we have, and that could be interpreted roughly as sort of the volume of solutions that we can express in our prior.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You can also use this diagram quite separately from Gaussian processes to reason about generalization, so I said earlier that we really want to use the model with.",
                    "label": 0
                },
                {
                    "sent": "You know, a lot of flexibility, the 10 thousandth order polynomial, something even more flexible if we could, because we believe that probably the generative mechanism for our observations is extremely sophisticated, and we want to honestly represent our beliefs in the modeling process.",
                    "label": 0
                },
                {
                    "sent": "But we also don't want to overfit and from this perspective, generalization is really a 2 dimensional concept.",
                    "label": 0
                },
                {
                    "sent": "It's not strictly related to which solutions we can represent, it's also related to which solutions are a priority, likely.",
                    "label": 1
                },
                {
                    "sent": "So from this perspective, I would argue that we want our support which solutions we can represent to be as large as possible, as long as we actually believe that there's some probability that that could be the solution we want to represent it with their model, because that just is honestly representing our beliefs.",
                    "label": 1
                },
                {
                    "sent": "But we also want to distribute that support very carefully.",
                    "label": 0
                },
                {
                    "sent": "So not all solutions.",
                    "label": 1
                },
                {
                    "sent": "For instance, we would actually believe are a priority, likely when we're reasoning about things in this function space.",
                    "label": 0
                },
                {
                    "sent": "And so you could actually use this to start to understand things like the difference between a fully connected neural network and a convolutional neural network.",
                    "label": 0
                },
                {
                    "sent": "So a fully connected neural network is more flexible than a convolutional neural network, yet it does better for a lot of problems in computer vision.",
                    "label": 0
                },
                {
                    "sent": "The way that that works from this perspective is by creating a restriction bias saying OK, well, if we don't have these kinds of invariances then we don't want to represent those solutions.",
                    "label": 1
                },
                {
                    "sent": "And so that basically crunches the support of this model and concentrates it around the types of datasets that are interesting in that application domain.",
                    "label": 0
                },
                {
                    "sent": "And that's why we see better generalization now if we could actually take this a step further, instead of having a restriction bias, we could have a more general inductive bias.",
                    "label": 0
                },
                {
                    "sent": "If we actually believe that other types of solutions were possible that couldn't be represented by the convolutional neural network, then we might start with something like a fully connected neural network, have a distribution over the parameters so that it's concentrated around the types of solutions that are favored by a convolutional neural network, but don't exclude other solutions that we believe to be a priori possible.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I briefly mentioned that that.",
                    "label": 0
                },
                {
                    "sent": "You know, Radford Neal triggered a lot of interest in Gaussian processes during his PhD thesis, so this was kind of in the mid late 90s.",
                    "label": 0
                },
                {
                    "sent": "He was showing that you could use Markov chain Monte Carlo algorithms in conjunction with distributions over neural network parameters to achieve very good results.",
                    "label": 0
                },
                {
                    "sent": "He was also arguing that we do want to have very flexible models and along that direction he showed that if you have a neural network with a distribution over the parameters and an infinite number of hidden units.",
                    "label": 0
                },
                {
                    "sent": "That actually converge to a Gaussian process with a very particular type of kernel function, sometimes called the neural network kernel function.",
                    "label": 0
                },
                {
                    "sent": "And this was very exciting because it meant we could actually relatively easily use these incredibly flexible models that have really nice properties, universal approximation, etc.",
                    "label": 0
                },
                {
                    "sent": "But David Mackay actually wrote an interesting article in a collected edition about neural networks edited by Christopher Bishop, where he was, you know, generally providing a tutorial on Gaussian processes, but he had this very interesting comment.",
                    "label": 0
                },
                {
                    "sent": "He said that neural networks were envisioned to sort of become eventually intelligent agents which could discover very interesting hidden representations in data.",
                    "label": 0
                },
                {
                    "sent": "But Gaussian processes, though they had all these nice properties and they were getting very good performance at the time.",
                    "label": 0
                },
                {
                    "sent": "We're basically just smoothing devices with the popular kernel functions that we were using.",
                    "label": 0
                },
                {
                    "sent": "They weren't discovering such sort of exciting hidden representations that we might associate with intelligent agents, and so he asked in that case, whether, in treating Gaussian processes, is perhaps a principled replacement for neural networks.",
                    "label": 0
                },
                {
                    "sent": "Whether we're throwing out the baby with the bathwater.",
                    "label": 1
                },
                {
                    "sent": "And so the answer to this question, I believe, is to develop more expressive kernel functions which can discover interesting hidden representations in data.",
                    "label": 0
                },
                {
                    "sent": "And we can do that with the help of neural networks.",
                    "label": 0
                },
                {
                    "sent": "I think quite often neural networks and kernel methods are perceived as competing approaches, when in fact they have very complementary statistical properties that can be combined to great effect.",
                    "label": 0
                },
                {
                    "sent": "Kernel methods.",
                    "label": 0
                },
                {
                    "sent": "I think, as I sort of alluded to earlier, provide a very elegant way of working with nonparametric models using a finite amount of computational power.",
                    "label": 0
                },
                {
                    "sent": "And Gaussian processes also provide this very natural kind of function space approach to regression.",
                    "label": 0
                },
                {
                    "sent": "It's not specific to using a kernel method, but it's something that's often associated with GPS, whereas neural networks provide really powerful inductive biases for learning in all sorts of interesting problems they provide adaptive basis functions that are incredibly well motivated and very useful for a range of problems and.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You know the idea of deep kernel learning is basically to use.",
                    "label": 1
                },
                {
                    "sent": "An infinite number of basis functions, but have them be adaptive and have them be informed by neural networks.",
                    "label": 0
                },
                {
                    "sent": "And so that way we can achieve some of the best of both worlds.",
                    "label": 0
                },
                {
                    "sent": "And we can also get predictive uncertainties etc.",
                    "label": 1
                },
                {
                    "sent": "And so basically in this diagram here, what we have is our input layer.",
                    "label": 0
                },
                {
                    "sent": "So these could be pixels in an image.",
                    "label": 0
                },
                {
                    "sent": "They could be different times in some kind of time series extrapolation problem, different spatial locations, whatever you like, we have the neural network operating on those inputs.",
                    "label": 0
                },
                {
                    "sent": "And then we have a layer where a Gaussian process is applied to those outputs and then we have another layer which kind of mixes those Gaussian processes together linearly to form outputs.",
                    "label": 0
                },
                {
                    "sent": "Very crucially here though, the neural network is not treated as just some kind of preprocessing to the data, it's all learn jointly through the marginal likelihood.",
                    "label": 0
                },
                {
                    "sent": "So this is a different objective for training the neural network parameters and this can allow us to actually write this model in what's called the dual space as an infinite expansion that has these adaptive basis functions.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you know, in equations we basically start with some base kernel.",
                    "label": 0
                },
                {
                    "sent": "We could be an RBF kernel, spectral mixture, whatever we like.",
                    "label": 0
                },
                {
                    "sent": "We transform the inputs of that kernel is some function that has its own parameters and then our goal is to learn those parameters, say the weights of general network in conjunction with the base kernel hyperparameters through the marginal likelihood of the Gaussian process.",
                    "label": 1
                },
                {
                    "sent": "So I can't really emphasize this enough.",
                    "label": 0
                },
                {
                    "sent": "That's very important if we just pre trained the neural network and apply a Gaussian process, we have a very different model, very different predictions, very different statistical properties.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The issue then is scalability, so I mentioned that Gaussian processes naively scale cubically with the number of training points.",
                    "label": 0
                },
                {
                    "sent": "And once you've done that.",
                    "label": 0
                },
                {
                    "sent": "It'll take quadratic complexity for each prediction that you make.",
                    "label": 0
                },
                {
                    "sent": "We've been working very hard at developing new approaches in numerical linear algebra and scientific computing, which make these computations very scalable without actually sacrificing a lot of the interesting model structure.",
                    "label": 0
                },
                {
                    "sent": "So some of you may have heard of the K fact model.",
                    "label": 0
                },
                {
                    "sent": "That sort of employs a similar sort of idea to neural networks, where you have a chronic or factorization of the Fisher information matrix.",
                    "label": 0
                },
                {
                    "sent": "Here, we're developing approximations to kernel matrices.",
                    "label": 0
                },
                {
                    "sent": "Which allow us to preserve the representation but achieve linear time scaling in training and constant time scaling in testing.",
                    "label": 0
                },
                {
                    "sent": "And you can show actually that these approximations can be accurate to within numerical precision.",
                    "label": 0
                },
                {
                    "sent": "And so this is all implemented in this new library.",
                    "label": 1
                },
                {
                    "sent": "G Py torch with a lot of examples.",
                    "label": 0
                },
                {
                    "sent": "So pretty much every example that I have in this talk corresponds to a notebook in this G Pytorch library.",
                    "label": 1
                },
                {
                    "sent": "And these approaches, also, I think, harmonize very nicely with GPU acceleration, so I won't go too much into the details here.",
                    "label": 0
                },
                {
                    "sent": "But it's all basically driven by creating an approximation to a matrix of interest, which will admit fast matrix vector multiplications.",
                    "label": 1
                },
                {
                    "sent": "Once we have that, we can use things like linear conjugate gradients to solve linear systems efficiently, only involving matrix vector multiplications, and we can use things like stochastic Lanczos expansions to compute things like log determinants and their derivatives efficiently.",
                    "label": 0
                },
                {
                    "sent": "Again only involving matrix vector multiplications and GPU's are very good at accelerating those kinds of operations, so not only is the complexity quite nice, asymptotically GPU's are making these computations several orders of magnitude faster.",
                    "label": 0
                },
                {
                    "sent": "And I would say this is an approach that can also be employed farv on Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "I've mentioned Capac, I think that's a very nice approach.",
                    "label": 0
                },
                {
                    "sent": "There's a lot we can do, for example to come up with structured representations, for instance of neural network weights, which will speed up computation without necessarily sacrificing what we like about those models.",
                    "label": 0
                },
                {
                    "sent": "Another example of that are tensor train neural networks, which have been developed by several Russian groups.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "You probably can't see everything on this table, but the point that's being made here is that when we do deploy this model with basically training a Gaussian process with this deep kernel through the marginal likelihood and applying it to every regression problem, in this case on UCI, we see that the point predictions are a bit better than what you get through a standalone deep neural network.",
                    "label": 0
                },
                {
                    "sent": "The runtime isn't significantly more.",
                    "label": 0
                },
                {
                    "sent": "You pay about 10% runtime overhead in exchange for predictive distributions, and somewhat better predictions.",
                    "label": 0
                },
                {
                    "sent": "These datasets all have very different properties, so different numbers of data points and different numbers of dimensions for the inputs.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here we applied this problem.",
                    "label": 0
                },
                {
                    "sent": "So this is this, this sort of one of these papers is joint work with Russ Salakhutdinov and he looked at this already faces example and a few papers.",
                    "label": 0
                },
                {
                    "sent": "So basically here we have faces with different orientation angles were trying to predict these angles.",
                    "label": 0
                },
                {
                    "sent": "This is a regression problem but basically we have images are inputs.",
                    "label": 0
                },
                {
                    "sent": "We use a convolutional kind of based architecture for the deep kernel and when we train everything an project into 2 dimensions where each line segment corresponds to a phase, we see that.",
                    "label": 0
                },
                {
                    "sent": "Faces with similar orientation angles kind of clustered together.",
                    "label": 0
                },
                {
                    "sent": "We see line segments with similar slopes kind of clustering together.",
                    "label": 0
                },
                {
                    "sent": "So basically we're learning a non Euclidean metric to try to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "This is very crucial, so another.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stration of this.",
                    "label": 0
                },
                {
                    "sent": "Involves ordering all the faces by orientation angles and evaluating our deep kernel at every possible pair of faces in the left two panels here, and we see a very pronounced diagonal band.",
                    "label": 0
                },
                {
                    "sent": "Basically these two panels correspond to different base base kernels here, but what it's learning is that faces with similar orientation angles are more correlated, and that's a very non Euclidean metric learning problem, and so that's something you can't do with pretty much all of the popular kernels that people use.",
                    "label": 0
                },
                {
                    "sent": "And in fact in the far right panel we have the RBF kernel.",
                    "label": 0
                },
                {
                    "sent": "You know trained to this problem, we can see that the entries are quite diffuse no matter what we do with that kernel.",
                    "label": 0
                },
                {
                    "sent": "We're not actually learning the metric, that's going to help us solve this problem effectively, so we can see the neural network in this case is doing a kind of non Euclidean metric learning.",
                    "label": 0
                },
                {
                    "sent": "In this.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Problem we have data sample from a step function shown in black and we have fits using Gaussian processes with various different kernels.",
                    "label": 1
                },
                {
                    "sent": "So we have the RBF kernel in blue.",
                    "label": 0
                },
                {
                    "sent": "Basically this kind of data is such an unlikely draw from our prior over functions.",
                    "label": 0
                },
                {
                    "sent": "Using an RBF kernel that it can only be explained from that perspective by saying that there must be a lot of noise and so it basically chooses a very simple fit.",
                    "label": 0
                },
                {
                    "sent": "Says there's a lot of noise and has very high uncertainty and red.",
                    "label": 0
                },
                {
                    "sent": "We have a different kind of kernel.",
                    "label": 0
                },
                {
                    "sent": "Call the spectral mixture kernel.",
                    "label": 0
                },
                {
                    "sent": "It doesn't.",
                    "label": 0
                },
                {
                    "sent": "You know a better job of capturing the data, but it's still over smoothing.",
                    "label": 0
                },
                {
                    "sent": "Here we have to fit using a Gaussian process with a deep kernel and green and we can see that it captures these sharp discontinuity's quite nicely and so we made this demonstration because you know, often Gaussian processes with standard kernels are accused of over smoothing the data and this is not the case if we actually are helped a bit by neural networks.",
                    "label": 1
                },
                {
                    "sent": "We can also see that these uncertainty estimates corresponding to 95% of our predictive distribution do capture the data very confident.",
                    "label": 0
                },
                {
                    "sent": "We've.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Develop these approaches also for RNS and LST emsan.",
                    "label": 0
                },
                {
                    "sent": "We've applied them to autonomous driving, in particular, where we might want things like uncertainties on estimates of Lane boundaries and in the top.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Panel you have the predictions using a standard LTM in the bottom panel you have basically the whole predictive distribution that you get out of this model.",
                    "label": 0
                },
                {
                    "sent": "So both the point predictions and our uncertainties and a lot of additional information that we can use in decision making.",
                    "label": 0
                },
                {
                    "sent": "So that's one approach to being, you know, Bayesian about neural networks.",
                    "label": 0
                },
                {
                    "sent": "We can build things like Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "We can develop deep kernels which have been inspired by neural networks.",
                    "label": 0
                },
                {
                    "sent": "We can use these Bayesian objectives for training.",
                    "label": 0
                },
                {
                    "sent": "The models will get different predictions will also get uncertainty estimates, and we can make these things scalable by developing new approaches in numerical linear algebra and scientific computing.",
                    "label": 0
                },
                {
                    "sent": "As an aside, a lot of students ask me, you know what kinds of courses should I take if I'm interested in machine learning and AI very consistently recommend scientific computing.",
                    "label": 0
                },
                {
                    "sent": "Feel like it's a very foundational skill and it will help you in almost anything that you do.",
                    "label": 0
                },
                {
                    "sent": "Another",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Coach, that's perhaps the more conventional approach to being Bayesian neural networks is trying to be explicitly Bayesian about the weights about the parameters of the neural network.",
                    "label": 0
                },
                {
                    "sent": "So put a distribution over those parameters and then do Bayesian marginalization.",
                    "label": 0
                },
                {
                    "sent": "So this is something we did in a paper called the Bayesian Gam where we worked out a way to do posterior inference in response to adversarial feedback over the generator.",
                    "label": 0
                },
                {
                    "sent": "In the discriminator parameters here, I'm focusing on the generator parameters just illustrate a particular concept.",
                    "label": 0
                },
                {
                    "sent": "So what we've done in this top panel here is after we have our posterior over generator parameters.",
                    "label": 0
                },
                {
                    "sent": "We sample from that posterior 6 times and that gives us basically six different generators sampled from our posterior, just in the same way we were thinking about Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "We were thinking about sampling posterior functions.",
                    "label": 0
                },
                {
                    "sent": "Here we're basically sampling post to your generators and we're storing sort of an uncountably infinite collection of possible generators that could have created our data.",
                    "label": 0
                },
                {
                    "sent": "You could view.",
                    "label": 0
                },
                {
                    "sent": "Each generator is basically a different hypothesis for our data.",
                    "label": 0
                },
                {
                    "sent": "And so we can see that the images produced by each of these six generators are qualitatively quite different, so some are thicker summer, thinner.",
                    "label": 0
                },
                {
                    "sent": "Some might have different writing styles, some might correspond to different writing implements.",
                    "label": 0
                },
                {
                    "sent": "And we're looking at the likelihood surface or the posterior over the generator parameters.",
                    "label": 0
                },
                {
                    "sent": "We can see that you know some of these points might correspond, so might might might be very likely explanations of the data, and some might be very unlikely explanations of the data, so it could be that we sampled some parameter in detail here that actually is a very unlikely hypothesis for the data, and maybe it creates very weird looking images.",
                    "label": 0
                },
                {
                    "sent": "But if we believe it's possible, then maybe we ought to represent it, and this turned out to be quite helpful for semi supervised learning in the bottom panel we trained.",
                    "label": 0
                },
                {
                    "sent": "It's called the DC again 6 times and then just generated images from these DC games.",
                    "label": 0
                },
                {
                    "sent": "They actually don't look bad.",
                    "label": 0
                },
                {
                    "sent": "In fact, you might argue that they look better than than some of the images from these panels, but you would also argue that that might be expected because.",
                    "label": 0
                },
                {
                    "sent": "But the classical approach here in the DC again is essentially choosing one of the modes of this posterior and using dot mode as it's one generator.",
                    "label": 0
                },
                {
                    "sent": "So if you had to bet everything on one model, maybe you would want to bet on the model at the most posterior probability.",
                    "label": 0
                },
                {
                    "sent": "But if you want to represent the distribution, that's a different kind of thing, and it has different types of advantages.",
                    "label": 0
                },
                {
                    "sent": "We can see that the images look good, but they're fairly homogeneous.",
                    "label": 0
                },
                {
                    "sent": "So this sort of.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just in the next part of the talk were reasoning about the geometry of these likelihood functions.",
                    "label": 0
                },
                {
                    "sent": "These kinds of loss functions that we used to train these models, so I think this is really an important thing to do if we want to have successful Bayesian marginalization of neural network parameters.",
                    "label": 0
                },
                {
                    "sent": "This will help us inform our parameters so our priors it will help us inform what kinds of inference procedures we want to use, whether we want to focus on a single mode, whether we want to try to really explore the whole distribution, etc so poorly knows it all visited Cornell couple years ago and.",
                    "label": 0
                },
                {
                    "sent": "Presented this figure and I found it incredibly inspiring from a Bayesian perspective.",
                    "label": 0
                },
                {
                    "sent": "So what he was saying was related to optimization.",
                    "label": 0
                },
                {
                    "sent": "He was saying that small batch SGD tended to converge too.",
                    "label": 0
                },
                {
                    "sent": "Broader local Optima.",
                    "label": 0
                },
                {
                    "sent": "And that could explain why those solutions generalize better than if you use like a large batch method which might converge to a sharper local Optima.",
                    "label": 0
                },
                {
                    "sent": "And this diagram is showing the training loss in black and the testing loss in red.",
                    "label": 0
                },
                {
                    "sent": "So the intuition here is that if we choose a point on this flat Optima and we translate it vertically up onto the red curve, it still doesn't have terrible loss.",
                    "label": 0
                },
                {
                    "sent": "But if we do that with a sharp Optima, the loss becomes very bad and so I feel like that's fairly intuitive.",
                    "label": 0
                },
                {
                    "sent": "You know, if we're near a sharp Optima, we have to be very precise, and it's quite sort of intuitive that we would see this shift because we wouldn't really have enough information to be necessarily too confident about what the parameters should be.",
                    "label": 0
                },
                {
                    "sent": "However, from a Bayesian perspective, this meant that if we if there truly were these broad local Optima and they did truly correspond to, say, a diverse collection of models.",
                    "label": 0
                },
                {
                    "sent": "Then there's going to be a very big difference between Bayesian integration, Bayesian marginalization, and optimization in terms of the point predictions.",
                    "label": 1
                },
                {
                    "sent": "So if the curve for the likelihood or the loss really was sharp like this, then perhaps we could successfully represent it with a single point, and the predictions wouldn't be too different than if we were to try to integrate over that region.",
                    "label": 0
                },
                {
                    "sent": "But if these flat regions of the likelihood surface really existed and they really meant something.",
                    "label": 0
                },
                {
                    "sent": "Then that was great motivation for trying to do this integral for trying to get different types of predictions.",
                    "label": 0
                },
                {
                    "sent": "And so I feel like the potential for Bayesian methods to really make a difference is greater in deep learning that it is in many other types of models.",
                    "label": 1
                },
                {
                    "sent": "I think the challenges are also a lot greater.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in a number of recent papers we've been exploring, the geometry of these loss functions in a sense, ultimately motivated to do Bayesian marginalization over neural network parameters, but initially considering properties of the law surface that will help us in just having better approaches to training these models from a conventional perspective, and some of the conclusions we had where the local Optima in these really high dimensional spaces, rather than being isolated, were actually connected by very simple curves.",
                    "label": 0
                },
                {
                    "sent": "And I'll go into that shortly.",
                    "label": 0
                },
                {
                    "sent": "Another conclusion was that mini batch SGD doesn't actually converge to broad Optima in many directions, and there are good ways of finding those directions.",
                    "label": 0
                },
                {
                    "sent": "That averaging weights along SGD trajectory's with high constant or cyclical learning rates can lead to much faster convergence and solutions that generalize better.",
                    "label": 1
                },
                {
                    "sent": "And there is a relationship between that procedure.",
                    "label": 0
                },
                {
                    "sent": "An finding flatter regions of the training loss.",
                    "label": 1
                },
                {
                    "sent": "And we can try to approximate ensembles and Bayesian model averages with a single model.",
                    "label": 0
                },
                {
                    "sent": "I feel like this is actually a good good compromise.",
                    "label": 0
                },
                {
                    "sent": "Sometimes if you're trying to follow a Bayesian approach because we're working in an exceptionally high dimensional spaces and you know, I think that perhaps will never be able to fully perform Bayesian integration over these kinds of likelihood.",
                    "label": 0
                },
                {
                    "sent": "Surface is, it's just such an enormous challenge, but we can restrict ourselves in various ways and still see improvements.",
                    "label": 0
                },
                {
                    "sent": "We can still try to do something we can try to go halfway an I think.",
                    "label": 0
                },
                {
                    "sent": "You know one thing that we can do that's actually very practical is reason about what a single model might look like, but a single setting of the parameters might look like they could approximate something like that Bayesian model average.",
                    "label": 0
                },
                {
                    "sent": "Just going back to this figure quickly.",
                    "label": 0
                },
                {
                    "sent": "One other point, I suppose, is that if we are doing integration versus optimization, then a lot of the mass will be contained in that broad local Optima.",
                    "label": 0
                },
                {
                    "sent": "So that also explains why a Bayesian approach might actually be quite robust, and we're thinking about generalization, so that will kind of happen automatically.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Visualization of this first point that I mentioned in the top figure.",
                    "label": 0
                },
                {
                    "sent": "Here we have three local Optima that have been discovered by basically running SGD as one would typically run SGD on a resonant 110 cifar 100 using different initializations.",
                    "label": 0
                },
                {
                    "sent": "So this is corresponding to the standard intuition where these very high dimensional Optima are quite separated.",
                    "label": 0
                },
                {
                    "sent": "This is formed by sort of an affine composition of all possible settings.",
                    "label": 0
                },
                {
                    "sent": "Of these three weight vectors.",
                    "label": 0
                },
                {
                    "sent": "In the bottom left we have a special plane.",
                    "label": 0
                },
                {
                    "sent": "In this high dimensional space along which we can walk from one of these lower Optima to the other and have essentially constant training loss and testing loss.",
                    "label": 0
                },
                {
                    "sent": "In the bottom right panel we have another special plane, basically a slice through this really high dimensional space along which we can walk on a bezier curve quadratic bezier curve from one of the optimal to the other and have essentially constant training and test.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to make this a little bit more concrete, in the left panel we have a polygonal chain with a single bend W. One hat is a setting of weights that have been discovered using SGD with mini batches, data basically corresponds to what we want to learn here.",
                    "label": 0
                },
                {
                    "sent": "That's the bending point where we need to turn when we're walking from one optimal to another W2 is the other setting of the weights, so we have a curve which has been anchored by two settings of weights discovered by SGD.",
                    "label": 0
                },
                {
                    "sent": "In the bottom right we have sort of the equation here for the bezier curve, so it's just another parametric function.",
                    "label": 0
                },
                {
                    "sent": "So the way that we discovered these paths.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Was to choose this curve, which would have as its endpoints local Optima discovered by SGD and then minimize our standard loss function.",
                    "label": 0
                },
                {
                    "sent": "For example, L2 regularizer cross entropy uniformly in expectation over the curve.",
                    "label": 0
                },
                {
                    "sent": "So what this sort of looks like is a line integral over the parameterisation of this curve, normalized by arclength.",
                    "label": 0
                },
                {
                    "sent": "This is something that was actually fairly computationally tractable to evaluate, so we could sort of view this integral.",
                    "label": 0
                },
                {
                    "sent": "They're interested.",
                    "label": 0
                },
                {
                    "sent": "It is an expectation over a variable uniform standard uniform variable of the loss.",
                    "label": 0
                },
                {
                    "sent": "We can sort of sample from that and take gradient steps and then discover these curves.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you know when we walk along these curves, we have near constant training and test loss.",
                    "label": 0
                },
                {
                    "sent": "We can actually find quite a rich variety of parameterisations, the simplest that we found was the polygonal chain with a single bend.",
                    "label": 0
                },
                {
                    "sent": "We never found that we could actually take a direct linear path from one Optima to another.",
                    "label": 0
                },
                {
                    "sent": "I think there might be some Optima that are linearly connected to others, but there's so many that it's very unlikely that we would find those just by running SGD.",
                    "label": 0
                },
                {
                    "sent": "But we could very consistently find a Polygon.",
                    "label": 0
                },
                {
                    "sent": "Change between any two Optima so the red curve basically shows the error as we walk from one Optima to the other on a straight line path.",
                    "label": 0
                },
                {
                    "sent": "The green and blue curves show the error that's incurred as we walk from one point to the other on these other parameterized curves that have been discovered.",
                    "label": 0
                },
                {
                    "sent": "Now when we first found this, I was a little bit scared that this could just mean that we found redundancy in the parameterization of the neural network, that these are all basically the same model.",
                    "label": 0
                },
                {
                    "sent": "I mean still kind of cool, but you know, maybe there's not much you can do with that practically, but it turned out that actually, although the models were having the same train loss, roughly the same test loss they were making different predictions.",
                    "label": 0
                },
                {
                    "sent": "So they were making different types of errors, which meant that we could actually ensemble them for much better performance.",
                    "label": 0
                },
                {
                    "sent": "You know very easily and this started us thinking more about loss values rather than single points that could actually optimize our objective.",
                    "label": 0
                },
                {
                    "sent": "And you can also use this kind of reasoning to try to derive new types of loss functions which might favor things like brought Optima.",
                    "label": 0
                },
                {
                    "sent": "So here we started by anchoring curves by two endpoints discovered by SGD.",
                    "label": 0
                },
                {
                    "sent": "But instead you might say, well, I want to minimize.",
                    "label": 0
                },
                {
                    "sent": "Surface integral over cross entropy subject is some kind of constraint which we might enforce, for example through a LaGrange multiplier.",
                    "label": 0
                },
                {
                    "sent": "Say, OK, we want the value to be at least this large, and if we think that that's associated with generalization and that could be quite interesting to explore, what happens if we do that and?",
                    "label": 0
                },
                {
                    "sent": "You know this would also innocence calibrate our loss functions so that points kind of in the center of brought Optima wouldn't necessarily generalize better than points on the periphery, because now we've accounted for the fact that central points kind of been doing this integral are better that they might generalize better.",
                    "label": 0
                },
                {
                    "sent": "So I think that this result this kind of early result that flatter, optimum might generalize better also suggests that the loss function that we're using itself might be a bit miscalibrated.",
                    "label": 0
                },
                {
                    "sent": "We wouldn't necessarily have that for a well calibrated loss, and this can help us inspire better types of loss functions.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This kind of discovery lead us to think more and more about ensemble ING and how we could actually take fairly small steps and wait space and actually still see a lot of diversity and models.",
                    "label": 0
                },
                {
                    "sent": "So we explored using a cyclical learning rate and basically capturing models at the bottom of each cycle.",
                    "label": 0
                },
                {
                    "sent": "So just doing one single training run of SGD with the cyclical rate capturing models at the minimum of each cycles and then using them as part of an ensemble.",
                    "label": 0
                },
                {
                    "sent": "So this would require the same computational budget for training, but you would get an ensemble and the predictions were really good.",
                    "label": 0
                },
                {
                    "sent": "You did improve a lot and you did better if you kind of fix the computational budget than you would if you were just to run SGD a bunch of times with random initializations and then ensemble those together.",
                    "label": 0
                },
                {
                    "sent": "And in the process we started thinking more and more about visual.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Using the trajectory of SGD in these planes so.",
                    "label": 1
                },
                {
                    "sent": "We know there's something a bit funny, so these are basically three weights that have been sampled along the trajectory of SGD with this cyclical learning rate schedule on the test loss surface.",
                    "label": 0
                },
                {
                    "sent": "So this this is, this plane is formed by all affine combinations of these three weight vectors.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I thought, well, why are we actually?",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Averaging the outputs of these models when averaging the parameters themselves seems like potentially an interesting thing to do.",
                    "label": 0
                },
                {
                    "sent": "You know, and when we did that?",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We found that these points actually generalize really well, so in the bottom right panel here we have what happens if we run.",
                    "label": 0
                },
                {
                    "sent": "A resonant 110 on Cifar 100 using a prescribed learning rate schedule with SGD up to epoch 125.",
                    "label": 0
                },
                {
                    "sent": "And then for stochastic weight averaging, what we're doing, we're maintaining a running average of these weights.",
                    "label": 0
                },
                {
                    "sent": "We change the schedule so that we're using a cyclical rate, and then we just store the running average and then we use that average at the end.",
                    "label": 0
                },
                {
                    "sent": "And then for SGD we just continue running until convergence and we see that the cast equate averaging moves to sort of the periphery of a local Optima.",
                    "label": 0
                },
                {
                    "sent": "It's not the best solution and train whatsoever.",
                    "label": 0
                },
                {
                    "sent": "SGD does converge to the local Optima, but once we move into test, everything shifts and the SWA solution generalizes quite a bit better than the SSD solution.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here we have basically projections of the trajectories of SGD onto these loss surface.",
                    "label": 0
                },
                {
                    "sent": "Is using different types of learning rate schedule.",
                    "label": 0
                },
                {
                    "sent": "So in the top two panels we have cyclical learning rate schedules.",
                    "label": 0
                },
                {
                    "sent": "In the bottom we have very high constant learning rate schedules and the main difference is that with the high constant learning rate schedules, the points traverse by SGD or a lot worse.",
                    "label": 0
                },
                {
                    "sent": "But they still seem to be almost symmetrically aligned around points of good generalization and test.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to try to understand why these solutions might generalize better, we started thinking about this with hypothesis.",
                    "label": 0
                },
                {
                    "sent": "So in this figure here we sampled direction vector uniformly on the unit sphere and then we stepped away from both the stochastic weight averaging solution.",
                    "label": 0
                },
                {
                    "sent": "In this SSD solution a bunch of times.",
                    "label": 0
                },
                {
                    "sent": "So each curve corresponds to a different direction.",
                    "label": 0
                },
                {
                    "sent": "So we could see that we actually had to move a much further distance to increase error by the same amount using SWA.",
                    "label": 0
                },
                {
                    "sent": "Then if we were to start from the SSD solution.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It made us wonder what it would look like if we were to actually walk directly from the SWA solution to the SGD solutions that corresponds to the origin.",
                    "label": 0
                },
                {
                    "sent": "In this figure, this is test error.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "No, this is as we have the train loss in green and we have test error in blue.",
                    "label": 1
                },
                {
                    "sent": "We can see when we do that that SGD actually converges to appoint with better test loss it's a bit lower than the SWA solution, But the SWA.",
                    "label": 0
                },
                {
                    "sent": "Arthur Train, yes Thanks so better training loss.",
                    "label": 0
                },
                {
                    "sent": "But Swa converges to a much flatter region.",
                    "label": 0
                },
                {
                    "sent": "Of the training loss and then in test SWA is is generalizing better it has.",
                    "label": 0
                },
                {
                    "sent": "It has lower test area and it seems to be almost at the minimum of this curve whereas.",
                    "label": 0
                },
                {
                    "sent": "SGD is not generalizing as well and we can see here that SGD is actually near the periphery of very sharp ascent in this particular direction.",
                    "label": 0
                },
                {
                    "sent": "So this actually gives us a mechanism to find a direction where SGD is not converging to kind of a flat local optimum we also consider.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The relationship to this other ensemble in approach that we were using earlier where we were taking steps and then kind of averaging the outputs of our models using this cyclical learning rate and by design.",
                    "label": 0
                },
                {
                    "sent": "These steps were actually very small.",
                    "label": 0
                },
                {
                    "sent": "We found that we could still explore a pretty diverse collection of models by taking small steps in weight space.",
                    "label": 1
                },
                {
                    "sent": "So we did a linearisation analysis to show that this sick astic weight averaging solution would approximate.",
                    "label": 0
                },
                {
                    "sent": "Actually, the answers that were getting using this this ensemble in that we were doing before called Fge.",
                    "label": 0
                },
                {
                    "sent": "And we run it.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using the best models on a lot of these vision benchmarks, so Imagenet Cifar 10 Cifar 100 and so on, we find that stochastic weight averaging performs competitively, sometimes even better than the ensemble, and pretty reliably better than SGD.",
                    "label": 0
                },
                {
                    "sent": "You could also run this weight averaging procedure for more budgets and continue to improve performance because in this case SSD isn't really converging to anything.",
                    "label": 0
                },
                {
                    "sent": "It's just exploring the loss surface, and in fact there have been some interesting recent papers saying that if you have a high constant learning rate, SGD is approximately sampling from at least locally, approximately sampling from a Gaussian distribution centered at one of the local Optima.",
                    "label": 0
                },
                {
                    "sent": "So we found that we could run this up to two or three budgets and still see improvements in performance.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This sort of some results on image NET.",
                    "label": 0
                },
                {
                    "sent": "In practice, I would say that you can train the model end to end.",
                    "label": 0
                },
                {
                    "sent": "In this way use the high constant learning rate.",
                    "label": 0
                },
                {
                    "sent": "Are cyclical rate making running average.",
                    "label": 0
                },
                {
                    "sent": "But you can also just start from a pre trained model that's been trained with SGD and then start running SWA for a few epoch Cincy, improve performance and that's very practical for things like image net we're training actually can be quite expensive so it didn't really take us a lot of resource is to start with the pre trained model on image net and run for five or ten epoxy.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is going back to this Gaussian intuition, so if it's the case that SGD with a high constant learning rate is approximately sampling from a high dimensional Gaussian distribution, that means that it's traversing basically the surface of a hypersphere.",
                    "label": 1
                },
                {
                    "sent": "So in high dimensions, Gaussian distributions have most of their mass in a very thin shell, and we can understand this by thinking about mass as density times volume.",
                    "label": 0
                },
                {
                    "sent": "So although the density is the greatest.",
                    "label": 0
                },
                {
                    "sent": "The center.",
                    "label": 0
                },
                {
                    "sent": "So if we have a very high dimensional standard normal distribution, zero has the highest density.",
                    "label": 0
                },
                {
                    "sent": "Volume increases very quickly as we increase radius and these two things sort of tradeoff.",
                    "label": 0
                },
                {
                    "sent": "Then we get a key distribution for the mass, and it's it's point of highest density.",
                    "label": 0
                },
                {
                    "sent": "Is it root?",
                    "label": 0
                },
                {
                    "sent": "D Anet concentrates very quickly, so you can sort of try this at home in Matlab or Python or whatever.",
                    "label": 0
                },
                {
                    "sent": "See what happens if you sample from a standard normal distribution with maybe 100,000 dimensions and then histogram of the norms of those samples.",
                    "label": 0
                },
                {
                    "sent": "And you'll see that they're very concentrated around Ruthie.",
                    "label": 0
                },
                {
                    "sent": "And this is much less the case for lower dimensional Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So this means that by basically averaging the points on this shell, we're able to move to a point of higher density very quickly.",
                    "label": 0
                },
                {
                    "sent": "So this sort of helps us understand things like convergence as well.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this experiment here we're running SGD with a high constant learning rate in green on Cifar 100.",
                    "label": 0
                },
                {
                    "sent": "With Resnet 110 I think are wide Resnet, so on this particular problem 20% error is reasonably good.",
                    "label": 0
                },
                {
                    "sent": "We can see that it's hovering around 35% error in blue we have the normal prescribed learning rate schedule for this model.",
                    "label": 0
                },
                {
                    "sent": "In red we have what happens if we start averaging the the solutions traverse by STD with the high constant learning rate at epoch at about 1:45 so we can see after just about five iterations.",
                    "label": 0
                },
                {
                    "sent": "Here the error plunges down to value.",
                    "label": 0
                },
                {
                    "sent": "That's lower than what SGD would converge do.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So you can see that you know there can be quite dramatic improvements in convergence.",
                    "label": 0
                },
                {
                    "sent": "I think there's a natural question of when do you start averaging.",
                    "label": 0
                },
                {
                    "sent": "I think that question is actually very similar to the question of when do you start decreasing the learning rate in a regular STD schedule?",
                    "label": 0
                },
                {
                    "sent": "So this is something that is actually difficult to know precisely.",
                    "label": 0
                },
                {
                    "sent": "There are some good sort of heuristics based on theory, but you know what you can do at least to start with an STD plane train model and.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Averaging or you could start averaging when you would normally start decreasing the learning rate for an STD model, and that will tend to work well.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in conclusion, you know if we're following a Bayesian approach, we can often achieve better predictions than we would if we were representing the posterior with just a single point, which is what we're doing in a classical approach.",
                    "label": 0
                },
                {
                    "sent": "We can also get uncertainty estimates, which are crucial for decision-making.",
                    "label": 1
                },
                {
                    "sent": "Computation is a key challenge, and this challenge certainly hasn't been fully addressed.",
                    "label": 1
                },
                {
                    "sent": "We've been thinking about this issue mostly from the perspective of developing techniques in numerical linear algebra, that Gaussian process is really scalable, and.",
                    "label": 0
                },
                {
                    "sent": "Use very deep kernels.",
                    "label": 0
                },
                {
                    "sent": "I think there's a lot of promise actually to deploy similar kinds of techniques directly to exploiting structure and weight matrices in neural networks, so that's done with a paper called tense arising neural networks.",
                    "label": 0
                },
                {
                    "sent": "It's also done by K fact.",
                    "label": 0
                },
                {
                    "sent": "I think there's really a lot of room to develop better inference procedures as well, so using the HMC methods that Radford Neal used so successfully in the 90s just wouldn't be possible for the types of neural networks that we're considering now, these are just.",
                    "label": 0
                },
                {
                    "sent": "2 high dimensional parameter spaces.",
                    "label": 0
                },
                {
                    "sent": "We also really need to do some kind of like stochastic sampling.",
                    "label": 0
                },
                {
                    "sent": "We don't want to be using full batches.",
                    "label": 0
                },
                {
                    "sent": "We've discovered that that's incredibly important for being blue train.",
                    "label": 0
                },
                {
                    "sent": "These models in the last year or two.",
                    "label": 0
                },
                {
                    "sent": "There have been some very exciting algorithms that have emerged like stochastic gradient Hamiltonian Monte Carlo, which algorithmically are actually very similar to SGD.",
                    "label": 0
                },
                {
                    "sent": "They look a bit like SGD plus Gaussian noise with variance that depends on learning rate, momentum, and a few other terms and so.",
                    "label": 0
                },
                {
                    "sent": "This is very exciting in the sense that we can apply those algorithms without a lot of additional computational overhead.",
                    "label": 0
                },
                {
                    "sent": "If you're using SGD to train a neural network, you can also use DHMC to do sampling.",
                    "label": 0
                },
                {
                    "sent": "However you know, I think there's still a lot more work to do exploring these very rich likelihood surface is going to be very, very difficult, and it's certainly not fully achieved by stochastic MCMC algorithms that we have now variational approximations.",
                    "label": 0
                },
                {
                    "sent": "Similarly often concentrate most of their efforts on a single mode.",
                    "label": 0
                },
                {
                    "sent": "I feel like with neural networks in particular, the different local Optima well they might perform similarly well often correspond to interesting Lee different representations, so this was hidden sort of hinted out of it when we were talking about Bayesian Ganz.",
                    "label": 0
                },
                {
                    "sent": "The different modes might actually correspond in that case to different generators with interesting different statistical properties, and so really we don't want to throw away all of that information if we're developing a deterministic approximation, it would be very nice to try to.",
                    "label": 0
                },
                {
                    "sent": "Consider more than just a single mode, and this is also partly why I'm really excited about MCMC.",
                    "label": 0
                },
                {
                    "sent": "This is something that happens, perhaps a little bit more naturally with with MCMC.",
                    "label": 0
                },
                {
                    "sent": "I think from the perspective of creating deterministic approximations, we also want to think beyond variational methods, so the variational method at a high level.",
                    "label": 0
                },
                {
                    "sent": "Starts by, you know, is every other deterministic method.",
                    "label": 0
                },
                {
                    "sent": "Having us choose some distribution that we can use for our posterior which will give us analytic tractability.",
                    "label": 0
                },
                {
                    "sent": "And then we learn the parameters of this approximate distribution by minimizing the KL divergent between that distribution and sort of our true objective distribution.",
                    "label": 0
                },
                {
                    "sent": "When we do that, we have a bias towards very compact representations of the data, so the KL divergent when optimized that way round will really want us to avoid having our approximation place any mass where the true distribution doesn't have mass.",
                    "label": 0
                },
                {
                    "sent": "And for that reason we can actually miss a lot of interesting properties of our likelihood surface.",
                    "label": 0
                },
                {
                    "sent": "So this is something that's actually been discussed a bit in the context of Gans when people think about you, know Jensen, Shannon Divergent says versus KL divergences versus Wasserstein divergences, etc.",
                    "label": 0
                },
                {
                    "sent": "And I think this is something that we also ought to think about when we're building deterministic approximate inference algorithms, and I think we can probably import a lot of what we've learned in thinking about gains towards building better approximate inference strategies that don't necessarily just rely on variational approximations.",
                    "label": 0
                },
                {
                    "sent": "So as a concrete example, we could consider things like Alpha divergences which actually have KL divergences as a special case, and we could think about how we might want to learn the parameters of those more general families of divergences.",
                    "label": 0
                },
                {
                    "sent": "We also I think you know, as usual, want to think very carefully about the particular application that we're considering.",
                    "label": 0
                },
                {
                    "sent": "In some cases that compactness might not be a problem.",
                    "label": 0
                },
                {
                    "sent": "They could even be a feature.",
                    "label": 0
                },
                {
                    "sent": "So I think there's really a lot of room to build on these methods, particularly for more scalable approaches, but also to try to ensure that we're actually.",
                    "label": 0
                },
                {
                    "sent": "Exploring this rich distribution, which corresponds to many different types of interesting representations for the data.",
                    "label": 1
                },
                {
                    "sent": "I also think there's a lot of promise in taking a function space approach to machine learning in general, so this isn't necessarily just something we need to do.",
                    "label": 0
                },
                {
                    "sent": "Is Bayesians, it's something we can do when we're thinking about regularizers, for example.",
                    "label": 0
                },
                {
                    "sent": "So I said, you know, we could do something like L2 regularization, something I mentioned very early on, where I said we might not want the weights to be too large.",
                    "label": 0
                },
                {
                    "sent": "I think I also remarked, but it really depends on how we parameterise our model.",
                    "label": 0
                },
                {
                    "sent": "We really want to think about how our weights actually interact with the functional form of the model.",
                    "label": 0
                },
                {
                    "sent": "So whether we want the weights to be bigger small depends very much on the function.",
                    "label": 0
                },
                {
                    "sent": "We can do things to the function to make big weights good or small way.",
                    "label": 0
                },
                {
                    "sent": "It's good, and so in taking this function space approach where operating directly on the outputs of the model, rather than reasoning about the parameters of the model, I think we can construct very intuitive regularizers and we can also construct very intuitive priors that we can use for Bayesian inference, and this is something that's really been championed in the Gaussian process Community, but I think it applies.",
                    "label": 0
                },
                {
                    "sent": "Very generally, I think there's a lot of exciting work starting to emerge in this space.",
                    "label": 0
                },
                {
                    "sent": "And I think you know, even if we're not following a Bayesian approach directly, we can take inspiration from what would be a Bayesian approach to develop, you know, interesting procedures.",
                    "label": 0
                },
                {
                    "sent": "So actually in the original dropout paper, Bayesian marginalization was mentioned as inspiration for a lot of the algorithmic ideas.",
                    "label": 0
                },
                {
                    "sent": "So all the code available for you know stochastic weight averaging, scalable GPS etc is available at this URL and so thank you.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This one, that one that one Yep Yep.",
                    "label": 0
                },
                {
                    "sent": "Could you explain the various axises?",
                    "label": 0
                },
                {
                    "sent": "Where's data changing?",
                    "label": 0
                },
                {
                    "sent": "Where's the model changing?",
                    "label": 0
                },
                {
                    "sent": "OK, so I mean this is just directly from from this paper by Kessler at all.",
                    "label": 0
                },
                {
                    "sent": "The idea is that we have our loss function in black and.",
                    "label": 0
                },
                {
                    "sent": "A solution that that is good X axis.",
                    "label": 0
                },
                {
                    "sent": "So the X axis would be different weights the parameters.",
                    "label": 0
                },
                {
                    "sent": "So we remove.",
                    "label": 0
                },
                {
                    "sent": "This way is moving the weights.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, so the vertical axis would be the loss and the horizontal axis would be the parameters.",
                    "label": 0
                },
                {
                    "sent": "So I have a question about this one.",
                    "label": 0
                },
                {
                    "sent": "You know the work of laundon and repair metrization and flat minimum.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So how do you reconcile this notion that you can re parameterized in parameter space?",
                    "label": 0
                },
                {
                    "sent": "And this go from the flat minimum to shock one or vice versa.",
                    "label": 0
                },
                {
                    "sent": "But you know, in functions based nothings changed.",
                    "label": 0
                },
                {
                    "sent": "Yes seems to be the contradiction here.",
                    "label": 0
                },
                {
                    "sent": "I agree, yeah, so I think we really need to to to create better definitions of width.",
                    "label": 0
                },
                {
                    "sent": "Alot of the common definitions won't necessarily accommodate pathological solutions like Rayleigh rescaling and so on, where we might get the appearance of width, but it's not.",
                    "label": 0
                },
                {
                    "sent": "With that will actually help for generalizations, so in this sort of visualization there sort of implicitly.",
                    "label": 0
                },
                {
                    "sent": "There's this idea that a central point is desirable because it will be more robust to these kinds of shifts, but of course if the actual functions are the same.",
                    "label": 0
                },
                {
                    "sent": "Then there's no difference between a central point and a peripheral point.",
                    "label": 0
                },
                {
                    "sent": "So I think 1 one way to reason about that is to try to build better definitions of the width that are parameterization invariant, perhaps thinking about the Fisher information matrix, I think.",
                    "label": 0
                },
                {
                    "sent": "Ideally we would almost want sort of some functions based definition of width, and you know that's something that I'm sort of very excited to think about.",
                    "label": 0
                },
                {
                    "sent": "I think, perhaps empirically speaking, if we just sample random directions and we see how much the error is increasing as we move along those directions, it's possible that we've sampled a pathological direction, but I think it will give us some sort of useful intuition about what so connected with this in in the later part of your presentation when you talked about the averaging in parameter space.",
                    "label": 0
                },
                {
                    "sent": "Couldn't you get so?",
                    "label": 0
                },
                {
                    "sent": "There are two effects here, right?",
                    "label": 0
                },
                {
                    "sent": "One?",
                    "label": 0
                },
                {
                    "sent": "And this shell that you talk about in the Gaussian shell?",
                    "label": 0
                },
                {
                    "sent": "So we could just make the Gaussian shell smaller.",
                    "label": 0
                },
                {
                    "sent": "By I mean closer to the center.",
                    "label": 0
                },
                {
                    "sent": "If we reduce the learning rate.",
                    "label": 0
                },
                {
                    "sent": "But if you need the learning rate then you just make everything smaller.",
                    "label": 0
                },
                {
                    "sent": "Anne, Anne, Anne and so you have the choice now between averaging or beginning the learning rate.",
                    "label": 0
                },
                {
                    "sent": "But you also.",
                    "label": 0
                },
                {
                    "sent": "Show that you get better generalization, not just a better training loss, so can you.",
                    "label": 0
                },
                {
                    "sent": "Explain what is going on here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's a very good question.",
                    "label": 0
                },
                {
                    "sent": "I'm so I would say mostly we've observed better generalization and this is actually a bit surprising, so the idea of averaging sometimes appears in convex optimization.",
                    "label": 0
                },
                {
                    "sent": "Often it's employed as an exponential moving average with the decaying rate rather than an equally weighted average.",
                    "label": 0
                },
                {
                    "sent": "Would say Acyclical or a high concentrate, but there in convex optimization the benefits of averaging have focused around improve convergence, not improve generalization, whereas here what we found when we average with these high concentrates, cyclical rates and so on is that.",
                    "label": 0
                },
                {
                    "sent": "It's mostly the generalization that improves a lot.",
                    "label": 0
                },
                {
                    "sent": "We're starting to work on improved conversions, but this isn't something that we have noticed as much yet in terms of convergence.",
                    "label": 0
                },
                {
                    "sent": "I would say yes, if you could sort.",
                    "label": 0
                },
                {
                    "sent": "If you decrease the learning rate than you would decrease the size of this shell and you would move towards the center, but you could still perhaps see convergence a lot faster by having a high high learning rate and kind of bouncing around the shell and averaging versus sort of slowly decaying the rate.",
                    "label": 0
                },
                {
                    "sent": "I think there's also issues of like when you make the learning rate really small and so on.",
                    "label": 0
                },
                {
                    "sent": "You could get sort of stock, maybe in an undesirable solution.",
                    "label": 0
                },
                {
                    "sent": "One kind of interesting thing that we've actually started looking at as well as not just take the first moment basically of the.",
                    "label": 0
                },
                {
                    "sent": "Trajectory of SGD but actually consider higher order moments as well.",
                    "label": 0
                },
                {
                    "sent": "So if it's true that basically these points with these special learning rate schedules or sampling from some Gaussian distribution, we can use this to create an approximate distribution in weight space and then some sort of forward sample from that through our predictive distribution to get uncertainty estimates and do model averaging in that way.",
                    "label": 0
                },
                {
                    "sent": "And so we've actually explored this a little bit, and it workshop a new AI coming up where we take the second moment of this trajectory, and.",
                    "label": 0
                },
                {
                    "sent": "Use that to get predictive uncertainties as well, roughly at the same cost as you know, just training a single net.",
                    "label": 0
                },
                {
                    "sent": "Hey, thanks for the great talk.",
                    "label": 0
                },
                {
                    "sent": "So I had one question regarding your evasion Gan setup and it was essentially.",
                    "label": 0
                },
                {
                    "sent": "When and when you typically train again and this is and then just the theoretical part of it, and you want the generator to win, the generator captures entire data distribution.",
                    "label": 0
                },
                {
                    "sent": "So in your case, because you're sampling multiple generators, shouldn't that be the same generator?",
                    "label": 0
                },
                {
                    "sent": "If it training has succeeded, you can because you don't have that same generator.",
                    "label": 0
                },
                {
                    "sent": "Is that evidence that training is not succeeding?",
                    "label": 0
                },
                {
                    "sent": "So in the limit of infinite data, I think you would want this distribution to collapse, but given a finite amount of information, there is uncertainty about what's the right generator.",
                    "label": 0
                },
                {
                    "sent": "And so basically this just represents that uncertainty.",
                    "label": 0
                },
                {
                    "sent": "In a way, rather than having a single generator competing against a single discriminator, you have a whole distribution over generators, and it's competing against the distribution over discriminators, and so if we think about this purely from the generators perspective, we're getting a bunch of adverse aerial feedback.",
                    "label": 0
                },
                {
                    "sent": "So let's say we sample from our distribution over parameters that gives us a particular generator that generator is used to produce some samples.",
                    "label": 0
                },
                {
                    "sent": "Let's say there's no, that's those samples are terrible, right?",
                    "label": 0
                },
                {
                    "sent": "Then we want to basically use that as a likelihood signal in this case, to say we really need to.",
                    "label": 0
                },
                {
                    "sent": "Update our posterior over these parameters and find a region that's actually more reasonable, but there's still going to be uncertainty about what's best.",
                    "label": 0
                },
                {
                    "sent": "Last question.",
                    "label": 0
                },
                {
                    "sent": "I want to ask you about Bayesian optimization and the use of Gaussian processes for hyperparameter optimization.",
                    "label": 0
                },
                {
                    "sent": "I think it was very popular in a few few years ago, but it doesn't scale much for a large number of parameters, so I wanted to ask you like do you think it has a future?",
                    "label": 0
                },
                {
                    "sent": "Like for helping optimizing hyperparameters for deep neural Nets or?",
                    "label": 0
                },
                {
                    "sent": "What's your view about that?",
                    "label": 0
                },
                {
                    "sent": "That's a really great question, and it's something I've honestly wondered myself.",
                    "label": 0
                },
                {
                    "sent": "I have some thoughts about it though, so six years ago it was showing that you could use Bayesian optimization to automatically at least relatively speaking achieve state of the art results on Cifar 10, and that was very exciting because, you know, we didn't want to spend a lot of time hand tuning these hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "This led to, you know, really great interest in Bayesian optimization for hyperparameter learning in machine learning.",
                    "label": 0
                },
                {
                    "sent": "Bayesian optimization is a very general sort of thing.",
                    "label": 0
                },
                {
                    "sent": "We can use it for all sorts of other sort of applications like AB testing, etc where it does work very well.",
                    "label": 0
                },
                {
                    "sent": "Recently, people haven't been using it.",
                    "label": 0
                },
                {
                    "sent": "I think very much in practice to achieve a great performance on these vision benchmarks, and I think for a couple of reasons.",
                    "label": 0
                },
                {
                    "sent": "I mean, one reason is we've had these benchmarks for awhile and we figured out learning rates that work really well and so there might not actually be a lot to be gained by doing some kind of extensive search.",
                    "label": 0
                },
                {
                    "sent": "For those particular applications, however, if you're building a new continent and you're applying it to an entirely new problem, then you need to start from scratch, and so there isn't some good learning rate that someone can give you necessarily.",
                    "label": 0
                },
                {
                    "sent": "For instance, and in that case I know certainly companies are extensively using Bayesian optimization to try to tune hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "I think that there's still a long way to go, though, like the key issues in Bayesian optimization, our scalability kind of per iteration and.",
                    "label": 0
                },
                {
                    "sent": "Applicability that to high dimensional problems, and I think that the latter question is also related to this.",
                    "label": 0
                },
                {
                    "sent": "This bigger question of kernel selection.",
                    "label": 0
                },
                {
                    "sent": "So actually when I started my PhD, perhaps you know deep learning wasn't as popular as it is now, and one of the criticisms was there are so many design decisions to make you know what architecture, how many hidden layers, hidden units, activation functions, optimization procedure, etc.",
                    "label": 0
                },
                {
                    "sent": "And perhaps a lack of a principle framework to make those decisions, whereas.",
                    "label": 0
                },
                {
                    "sent": "Things like Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "It felt like you didn't really need to decide a lot and everyone could press the same button and get the same answer everywhere, but hidden beneath that was this very crucial decision of which kernel function to use.",
                    "label": 0
                },
                {
                    "sent": "And a lot of people are defaulting to the RBF kernel, which I think is very similar to what would happen if you were to say I want to use the same exact neural architecture for every problem, which would really make sense.",
                    "label": 0
                },
                {
                    "sent": "So I think kernel learning actually in a way is a lot like architecture learning, and if we want to have Bayesian optimization methods that work really well in high dimensional spaces, we need kernels that encode very reasonable inductive biases in those spaces.",
                    "label": 0
                },
                {
                    "sent": "And we want gradient information as well, and that's related to scalability, so I think it's a very promising research direction.",
                    "label": 0
                },
                {
                    "sent": "I know that all the big companies are using Bayesian optimization for a number of different tasks, including hyperparameter learning.",
                    "label": 0
                },
                {
                    "sent": "It hasn't been used as much in machine learning research in vision, but that could change as we sort of have fresh problems and fresh models to consider.",
                    "label": 0
                },
                {
                    "sent": "Alright, thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}