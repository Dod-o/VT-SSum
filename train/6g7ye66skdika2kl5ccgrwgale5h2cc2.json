{
    "id": "6g7ye66skdika2kl5ccgrwgale5h2cc2",
    "title": "The Projectron: a Bounded Kernel-Based Perceptron",
    "info": {
        "author": [
            "Francesco Orabona, Toyota Technological Institute at Chicago"
        ],
        "published": "Aug. 4, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/icml08_orabona_pbk/",
    "segmentation": [
        [
            "OK, so good morning everybody in Francesca Ramona and this is a joint work with Joseph and Barbara Caputo."
        ],
        [
            "So this is the outline of the presentation.",
            "Officially introduce you the problem of bounded online learning with kernels and I'll talk about some previous works.",
            "And then I'll talk about the main idea of mine algorithm.",
            "That is the projection step.",
            "I'll introduce the algorithm itself and its analysis in terms of mistake bounds, and at last I will show you some expn."
        ],
        [
            "Mental results.",
            "So what is an online algorithm I not going to introduce again?",
            "What is an online algorithm?",
            "But basically we can say that online algorithm observe example in a sequence of round at the construct, the classification function incrementally.",
            "And why we should use an online algorithm?",
            "Well, basically because kernel basic discriminative online algorithms perform pretty well on binary classification task, especially when coupled with batch to online to batch conversions.",
            "And they're very easy to implement.",
            "They have good bound.",
            "How do they work?",
            "Well, they basically they keep a subset of distances that is called super set and the classification function is defined as a kernel dependent weighted combination of the storage samples.",
            "So it seems that everything is fine, but there is something."
        ],
        [
            "Sometimes nobody says that each time in instances misclassified, it will be added to the Super set."
        ],
        [
            "So why this is a problem?",
            "Cause if the data is not linearly separable.",
            "The algorithms will never stop updating the classification function.",
            "What does it mean that if I am using a kernel, it means that the size of the solution will grow forever and sooner or later the classifier will be too slow to be used and instead it would be better to have to impose a maximum size of the solution or at least to have some guarantees that the solution will be bounded."
        ],
        [
            "So the first algorithm to overcome this unlimited growth, the Super set, was proposed by Krammer ET al.",
            "And the basic idea is to fix and, uh, uh, priority budget.",
            "Let's say something like 100 and once I have reached that point, every time the algorithm must insert a new sample in the Super set, it just discard the previous one.",
            "And the algorithm was then refined about Western at all, and similar strategies are also used in the algorithm.",
            "Norman silk.",
            "Now."
        ],
        [
            "Now the problem with this previous algorithm is that they don't try to quantify the damage done by removal of a simple.",
            "And and the standard first algorithm.",
            "They fixed budget and at the same time a mistake bound are the forgotten by Decatur at Al and the random budget perceptron by Chase Bank.",
            "It'll.",
            "And in both this algorithm works in the same way as the previous one.",
            "They discard super vectors from samples from the Super set to keep the solution of a fixed size."
        ],
        [
            "Now it's really discarding the only way to to keep the solution bounded, and there is also another problem is that of course we are discarding some kind of information from our classifier.",
            "So the best thing that we can hope is just too.",
            "To to try to reduce this damage that we're inflicting the solution, but maybe there is another way to solve this problem and at the same time we could obtain even better performance than the perceptron.",
            "That is the basis for all these algorithms.",
            "So let's start from the perception."
        ],
        [
            "Let's review a bit how it works and let's see if it is possible reasoning on it to find another way.",
            "So the perceptron at given moment as a current hypothesis, that is W our hyperplane.",
            "And receive a new instance X and classify trying to classify it with the with W and then we receive the true label.",
            "If there is a mistake X of T * Y OT is just added to W and X30 is added to the Super set.",
            "Now for this simple algorithm, it is possible to prove a lot of things, and among them it is possible to prove that the maximum number of mistakes it's it's bounded and in relation to the performance of the best classifier that we can find in at either side, let's call it U.",
            "And in particular, is 2 times the inch loss of U on the on the sequence of training samples plus a complexity term that it's the squared normal view.",
            "So now what are we doing each time there is a mistake, we are just adding a vector to W where W is a particular vector becausw it leaves in the span of the of the Super set because it is just a linear combination of of the samples in the Super set."
        ],
        [
            "So suppose for a moment that.",
            "The span the space is bounded by the samples in the Super set is this plane here coloring plane and W is just a vector that is living inside there.",
            "Now we have X of T and we must we should update W with the XLT, but maybe we can do another thing.",
            "We can take X or T we can project it in the in the space spanned by the by the samples in the Super set and we can use this vector per week so T to update.",
            "WY you should do this becausw.",
            "PR week so T in can be expressed as a linear combination of the samples in the Super set.",
            "So when we use it to update the, we don't need to add X of T to the Super set.",
            "That is, we are not increasing the size of the solution, but we will just up the coefficient of the old samples.",
            "And Moreover, if the.",
            "If the if the Super set is spanning the entire space and this space is finite dimensional, the error in the projection will be always 0, so we are recovering exactly the perceptron algorithm."
        ],
        [
            "So this is exactly our idea.",
            "Instead of discarding every time, all the samples from the Super set, we will try to project every time we can and we will see what does it mean the new ones into the space spanning previous ones.",
            "And we hope that in this way we can control the size of the solution and at the same time we can never mistake."
        ],
        [
            "So this is.",
            "The projects are an algorithm that is just a modification of the perceptor, so again we have W that is our April plane.",
            "We're assuming new instance.",
            "We predict it.",
            "We received it through label and if there is a mistake, we create two different possible updates.",
            "The first one is just the update, then with XT.",
            "So it's just the standard perceptron update and the other one is using the projection of XLT.",
            "Now if the difference between these two in Norm is less than a threshold eater that it's a parameter of the algorithm, we just use the second one.",
            "That is, we're updating the solution with the projection without increasing its size.",
            "If this.",
            "Big dentist and we just update resolution in the stampers perception way.",
            "And we had the exact date of the Super set.",
            "Now notice that everything in the algorithm can be expressed with scalar products, so everything here can be all the algorithm can be implemented with kernels without problems."
        ],
        [
            "So now we have an algorithm and we have a parameter that is E to that.",
            "Is this threshold here for our check?",
            "And we want to understand what is the behavior of the algorithm in relation to it.",
            "So if it is equal to zero, we recover exactly the perceptron algorithm.",
            "But still we could obtain special solution.",
            "Because suppose, for example, that we receive two times the same sample.",
            "But in the general case with it are greater than zero.",
            "The projection will introduce an error, but at the same time will give us as partial solution.",
            "So there is a tradeoff between these two things, so let's analyze first.",
            "What is the behavior of the sites of the?"
        ],
        [
            "For setting relation to it and we have a very nice results that the size of the supersets is always bounded regardless of the kernel, regardless of the number of training samples that I use.",
            "If ITA is greater than 0.",
            "So.",
            "Note that here the philosophy compared to the budget algorithm, it's a little bit different because in the budget algorithm, like for getting a random budget perceptron, you impose a fixed apriori.",
            "Sites of the solution.",
            "And while here you just fix it A and this will result in the maximum size of the Super sets, but you cannot predict.",
            "How big it will be.",
            "You just know that it will be bounded."
        ],
        [
            "And then let's see the influence of it on the performance of the algorithm, and it is possible to prove this mistake bound.",
            "And here if we set it to equal to zero, we recover exactly the perceptron algorithm instead.",
            "If ITA is small compared to the norm of G, that is the best classifier 90 side.",
            "We expect that we will not lose too much accuracy compared to the processor.",
            "Now it is possible, given that it's A is relation to the norm of the competitor.",
            "It is possible even to change the parameter of the."
        ],
        [
            "Instead of using it as a parameter, we can use the maximum norm of the competitor as a parameter and we will set it to in each round in relation to this storm, and in this case the mistake bound is exactly 2 times the bound of the perceptron, and this is interesting because this is exactly the same bound over the forgotten and this will allows allows us to compare the two algorithms using somehow a fair comparison using the bounds."
        ],
        [
            "So what we have done until now is just to try to track as close as possible the performance of the perceptron, not to lose accuracy compared to him.",
            "But maybe we can do even better.",
            "We could obtain an algorithm that is better than Perceptron, but at the same time is bounded.",
            "And how to do it?",
            "We can just update.",
            "On our rounds in which there is not a mistake, but still, the confidence on the prediction is too small.",
            "And in that rounds we update only if it is possible to do a projection.",
            "That is, we analyze the mistake bound and we see if the update would improve or not.",
            "The mistake bound.",
            "If yes, we update the solution, otherwise we don't do anything like in the perception.",
            "And this is something like adding a margin to the perceptron algorithm, and so this would improve the performance, but only some updates.",
            "At the same time, we are not having the disadvantage of adding a merging, so having a lot of super set a lot of samples continuously added to super set like in the passive aggressive."
        ],
        [
            "So let's let's see some results.",
            "We compared our large grid that is projector in the Projector Pro plus that is this second variant with the perception the passive aggressive that will give us somehow an upper bound to the performance of the projector and plus plus the forget run the random budget perception.",
            "Another language that I could stop throwing that it's really simple and.",
            "One it reaches the maximum imposed solution, it just stops updating the classifier just to see if all these strategies that we are developing are useful or not compared to the real stupid one.",
            "And to have somehow a fair comparison because the algorithm are implicitly different, we have set U that is the mask maximum norm of the competitor in projector and projector and Pro plus and forget run to be exactly the same.",
            "So again this will result in a certain fixed budget for the forgotten.",
            "But this will result in an unpredictable sides of the solution for projector and projector plus plus.",
            "But we will see.",
            "In the results that our solution is always smaller."
        ],
        [
            "So this is the an experiment on the advertising data set with the Gaussian kernel and it's the size of the solution in relation to the number of training samples, and you see that with the perceptron that is the red line.",
            "The solution is basically growing linearly with number of training samples and with passive aggressive it's even worse.",
            "And it will grow forever.",
            "It will never stop.",
            "And here you see, instead of these two lines, superimposed are the behavior for the forgotten and random budget perception.",
            "So basically they start growing, then they reached the maximum sites and it remains constant for for the rest of the life of the algorithm and the other two lines here and for the projector and projector and plus plus.",
            "And you see that they are slowly growing and we know that they will reach a limit.",
            "There is an assistant and they cannot go.",
            "Beyond that point in from this graph, you can start seeing more or less where where their sympathies.",
            "So let's now see."
        ],
        [
            "The performance.",
            "This is the average number of mistakes in relation to the number of training samples.",
            "And the perception is the red line.",
            "And it's almost superimposed with the blue line that is the projector.",
            "That is where we have obtained an algorithm that is obtaining exactly the same performance, or we are losing very very few compared to the perceptron.",
            "Bye."
        ],
        [
            "But the number of the sites of the solution here at the end of the training is around 7000, and in our algorithm is around 1:00."
        ],
        [
            "And these two lines here.",
            "Instead RARD behavior for the random budget perception in the forget run, so you can see that until this point they are just standard perceptron there close to the perception.",
            "But from this point on this card discarding samples from the Super set and so somehow they stop improving and they go even a little worse from this point on.",
            "And instead of this black line, here is the projector and plus plus that it's better than perceptor, and it's worse than the passive aggressive."
        ],
        [
            "But the number of the sites of the solution is even smaller than the projectile itself."
        ],
        [
            "Let's see some other numerical results.",
            "This is another data set, quite big again negation kernel.",
            "And you can see we have used two different budget sites.",
            "You see that the projector and projector and plus plus always outperform the forgotten and random budget perceptor.",
            "And and again, the projector on plus plus it's always better.",
            "Then the perceptor.",
            "And the sites of the solution here.",
            "It was impossible to 4000 imposing the norm of the maximum competitor.",
            "And it turns out that it's smaller for the projector in projection plus plus, and even for this other setting of the budget size.",
            "And it is interesting to note that the performance of the stopped run that in average are not that bad compared to the others, but the variance is huge, so it means that in any case, these algorithms are doing something something useful."
        ],
        [
            "OK, so.",
            "This is the summary.",
            "I presented two different version of a bounded online algorithm and they depend on the parameter that allows you to trade accuracy for sparseness of the solution.",
            "And compared to the other budget algorithm, they have the advantage that they we have a bounded support set size, but we don't discard the instances and it's exactly this that keeps the performance side.",
            "However, even if we know that the size of the solution is bounded, we cannot determine these sites in advance.",
            "And as a future work we want to mix the budget strategy with the projection strategy, because we know that we are some our talk.",
            "We have a."
        ],
        [
            "That's all, thanks.",
            "So I just thought about it was interesting you had this bound.",
            "That was something like it was a regular bound divided by 1 -- 8 times the norm of the solution.",
            "It was, it was much earlier on."
        ],
        [
            "This one yeah, so.",
            "I mean, that's actually a nice bounded is 8 are there.",
            "So is is this?",
            "I mean is this the case where you have a perfect classifier?",
            "I didn't.",
            "I guess I didn't get a chance to read you.",
            "Have you have is G have to make?",
            "Does it mean there exists a perfect classifier?",
            "There exists a perfect G. Sorry, does that mean that there is there exist?",
            "Does this guy have to be?",
            "Uh, uh, noise trees, Jesus.",
            "The best classifier you can find the ninth site on the sequence of training samples.",
            "OK, so it exists, you know that it exists, OK, but it doesn't have to have zero mistakes on the no.",
            "Is this Aidan?",
            "Maybe to get Lucia finite budget, will they'd have to depend on TSS?",
            "Nothing else occurs because no no.",
            "It it depends on Tytus.",
            "No, it is just a constant.",
            "OK, OK?",
            "How?",
            "Who found some problem of overfitting using your algorithm, 'cause you?",
            "From the first point it is.",
            "More.",
            "It is not OK. Yeah, have you?",
            "Well, basically no, because in the end.",
            "The first version is just like a presenter.",
            "I am each time in projecting in the in the sub in a certain subspace the solution.",
            "But this cannot increase the amount of information, so I cannot really overfit my data.",
            "I am decreasing the amount of information that I have served."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so good morning everybody in Francesca Ramona and this is a joint work with Joseph and Barbara Caputo.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the outline of the presentation.",
                    "label": 0
                },
                {
                    "sent": "Officially introduce you the problem of bounded online learning with kernels and I'll talk about some previous works.",
                    "label": 1
                },
                {
                    "sent": "And then I'll talk about the main idea of mine algorithm.",
                    "label": 1
                },
                {
                    "sent": "That is the projection step.",
                    "label": 0
                },
                {
                    "sent": "I'll introduce the algorithm itself and its analysis in terms of mistake bounds, and at last I will show you some expn.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mental results.",
                    "label": 0
                },
                {
                    "sent": "So what is an online algorithm I not going to introduce again?",
                    "label": 0
                },
                {
                    "sent": "What is an online algorithm?",
                    "label": 0
                },
                {
                    "sent": "But basically we can say that online algorithm observe example in a sequence of round at the construct, the classification function incrementally.",
                    "label": 1
                },
                {
                    "sent": "And why we should use an online algorithm?",
                    "label": 1
                },
                {
                    "sent": "Well, basically because kernel basic discriminative online algorithms perform pretty well on binary classification task, especially when coupled with batch to online to batch conversions.",
                    "label": 0
                },
                {
                    "sent": "And they're very easy to implement.",
                    "label": 0
                },
                {
                    "sent": "They have good bound.",
                    "label": 0
                },
                {
                    "sent": "How do they work?",
                    "label": 0
                },
                {
                    "sent": "Well, they basically they keep a subset of distances that is called super set and the classification function is defined as a kernel dependent weighted combination of the storage samples.",
                    "label": 1
                },
                {
                    "sent": "So it seems that everything is fine, but there is something.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sometimes nobody says that each time in instances misclassified, it will be added to the Super set.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So why this is a problem?",
                    "label": 0
                },
                {
                    "sent": "Cause if the data is not linearly separable.",
                    "label": 1
                },
                {
                    "sent": "The algorithms will never stop updating the classification function.",
                    "label": 1
                },
                {
                    "sent": "What does it mean that if I am using a kernel, it means that the size of the solution will grow forever and sooner or later the classifier will be too slow to be used and instead it would be better to have to impose a maximum size of the solution or at least to have some guarantees that the solution will be bounded.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the first algorithm to overcome this unlimited growth, the Super set, was proposed by Krammer ET al.",
                    "label": 1
                },
                {
                    "sent": "And the basic idea is to fix and, uh, uh, priority budget.",
                    "label": 1
                },
                {
                    "sent": "Let's say something like 100 and once I have reached that point, every time the algorithm must insert a new sample in the Super set, it just discard the previous one.",
                    "label": 0
                },
                {
                    "sent": "And the algorithm was then refined about Western at all, and similar strategies are also used in the algorithm.",
                    "label": 0
                },
                {
                    "sent": "Norman silk.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the problem with this previous algorithm is that they don't try to quantify the damage done by removal of a simple.",
                    "label": 1
                },
                {
                    "sent": "And and the standard first algorithm.",
                    "label": 1
                },
                {
                    "sent": "They fixed budget and at the same time a mistake bound are the forgotten by Decatur at Al and the random budget perceptron by Chase Bank.",
                    "label": 0
                },
                {
                    "sent": "It'll.",
                    "label": 0
                },
                {
                    "sent": "And in both this algorithm works in the same way as the previous one.",
                    "label": 0
                },
                {
                    "sent": "They discard super vectors from samples from the Super set to keep the solution of a fixed size.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now it's really discarding the only way to to keep the solution bounded, and there is also another problem is that of course we are discarding some kind of information from our classifier.",
                    "label": 0
                },
                {
                    "sent": "So the best thing that we can hope is just too.",
                    "label": 1
                },
                {
                    "sent": "To to try to reduce this damage that we're inflicting the solution, but maybe there is another way to solve this problem and at the same time we could obtain even better performance than the perceptron.",
                    "label": 1
                },
                {
                    "sent": "That is the basis for all these algorithms.",
                    "label": 1
                },
                {
                    "sent": "So let's start from the perception.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's review a bit how it works and let's see if it is possible reasoning on it to find another way.",
                    "label": 0
                },
                {
                    "sent": "So the perceptron at given moment as a current hypothesis, that is W our hyperplane.",
                    "label": 1
                },
                {
                    "sent": "And receive a new instance X and classify trying to classify it with the with W and then we receive the true label.",
                    "label": 1
                },
                {
                    "sent": "If there is a mistake X of T * Y OT is just added to W and X30 is added to the Super set.",
                    "label": 0
                },
                {
                    "sent": "Now for this simple algorithm, it is possible to prove a lot of things, and among them it is possible to prove that the maximum number of mistakes it's it's bounded and in relation to the performance of the best classifier that we can find in at either side, let's call it U.",
                    "label": 1
                },
                {
                    "sent": "And in particular, is 2 times the inch loss of U on the on the sequence of training samples plus a complexity term that it's the squared normal view.",
                    "label": 0
                },
                {
                    "sent": "So now what are we doing each time there is a mistake, we are just adding a vector to W where W is a particular vector becausw it leaves in the span of the of the Super set because it is just a linear combination of of the samples in the Super set.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So suppose for a moment that.",
                    "label": 0
                },
                {
                    "sent": "The span the space is bounded by the samples in the Super set is this plane here coloring plane and W is just a vector that is living inside there.",
                    "label": 0
                },
                {
                    "sent": "Now we have X of T and we must we should update W with the XLT, but maybe we can do another thing.",
                    "label": 0
                },
                {
                    "sent": "We can take X or T we can project it in the in the space spanned by the by the samples in the Super set and we can use this vector per week so T to update.",
                    "label": 0
                },
                {
                    "sent": "WY you should do this becausw.",
                    "label": 0
                },
                {
                    "sent": "PR week so T in can be expressed as a linear combination of the samples in the Super set.",
                    "label": 1
                },
                {
                    "sent": "So when we use it to update the, we don't need to add X of T to the Super set.",
                    "label": 0
                },
                {
                    "sent": "That is, we are not increasing the size of the solution, but we will just up the coefficient of the old samples.",
                    "label": 0
                },
                {
                    "sent": "And Moreover, if the.",
                    "label": 0
                },
                {
                    "sent": "If the if the Super set is spanning the entire space and this space is finite dimensional, the error in the projection will be always 0, so we are recovering exactly the perceptron algorithm.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is exactly our idea.",
                    "label": 0
                },
                {
                    "sent": "Instead of discarding every time, all the samples from the Super set, we will try to project every time we can and we will see what does it mean the new ones into the space spanning previous ones.",
                    "label": 1
                },
                {
                    "sent": "And we hope that in this way we can control the size of the solution and at the same time we can never mistake.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "The projects are an algorithm that is just a modification of the perceptor, so again we have W that is our April plane.",
                    "label": 0
                },
                {
                    "sent": "We're assuming new instance.",
                    "label": 0
                },
                {
                    "sent": "We predict it.",
                    "label": 0
                },
                {
                    "sent": "We received it through label and if there is a mistake, we create two different possible updates.",
                    "label": 0
                },
                {
                    "sent": "The first one is just the update, then with XT.",
                    "label": 0
                },
                {
                    "sent": "So it's just the standard perceptron update and the other one is using the projection of XLT.",
                    "label": 0
                },
                {
                    "sent": "Now if the difference between these two in Norm is less than a threshold eater that it's a parameter of the algorithm, we just use the second one.",
                    "label": 0
                },
                {
                    "sent": "That is, we're updating the solution with the projection without increasing its size.",
                    "label": 0
                },
                {
                    "sent": "If this.",
                    "label": 0
                },
                {
                    "sent": "Big dentist and we just update resolution in the stampers perception way.",
                    "label": 0
                },
                {
                    "sent": "And we had the exact date of the Super set.",
                    "label": 0
                },
                {
                    "sent": "Now notice that everything in the algorithm can be expressed with scalar products, so everything here can be all the algorithm can be implemented with kernels without problems.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we have an algorithm and we have a parameter that is E to that.",
                    "label": 0
                },
                {
                    "sent": "Is this threshold here for our check?",
                    "label": 0
                },
                {
                    "sent": "And we want to understand what is the behavior of the algorithm in relation to it.",
                    "label": 0
                },
                {
                    "sent": "So if it is equal to zero, we recover exactly the perceptron algorithm.",
                    "label": 0
                },
                {
                    "sent": "But still we could obtain special solution.",
                    "label": 0
                },
                {
                    "sent": "Because suppose, for example, that we receive two times the same sample.",
                    "label": 0
                },
                {
                    "sent": "But in the general case with it are greater than zero.",
                    "label": 0
                },
                {
                    "sent": "The projection will introduce an error, but at the same time will give us as partial solution.",
                    "label": 0
                },
                {
                    "sent": "So there is a tradeoff between these two things, so let's analyze first.",
                    "label": 0
                },
                {
                    "sent": "What is the behavior of the sites of the?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For setting relation to it and we have a very nice results that the size of the supersets is always bounded regardless of the kernel, regardless of the number of training samples that I use.",
                    "label": 1
                },
                {
                    "sent": "If ITA is greater than 0.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Note that here the philosophy compared to the budget algorithm, it's a little bit different because in the budget algorithm, like for getting a random budget perceptron, you impose a fixed apriori.",
                    "label": 0
                },
                {
                    "sent": "Sites of the solution.",
                    "label": 1
                },
                {
                    "sent": "And while here you just fix it A and this will result in the maximum size of the Super sets, but you cannot predict.",
                    "label": 0
                },
                {
                    "sent": "How big it will be.",
                    "label": 0
                },
                {
                    "sent": "You just know that it will be bounded.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then let's see the influence of it on the performance of the algorithm, and it is possible to prove this mistake bound.",
                    "label": 1
                },
                {
                    "sent": "And here if we set it to equal to zero, we recover exactly the perceptron algorithm instead.",
                    "label": 0
                },
                {
                    "sent": "If ITA is small compared to the norm of G, that is the best classifier 90 side.",
                    "label": 0
                },
                {
                    "sent": "We expect that we will not lose too much accuracy compared to the processor.",
                    "label": 0
                },
                {
                    "sent": "Now it is possible, given that it's A is relation to the norm of the competitor.",
                    "label": 0
                },
                {
                    "sent": "It is possible even to change the parameter of the.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Instead of using it as a parameter, we can use the maximum norm of the competitor as a parameter and we will set it to in each round in relation to this storm, and in this case the mistake bound is exactly 2 times the bound of the perceptron, and this is interesting because this is exactly the same bound over the forgotten and this will allows allows us to compare the two algorithms using somehow a fair comparison using the bounds.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we have done until now is just to try to track as close as possible the performance of the perceptron, not to lose accuracy compared to him.",
                    "label": 0
                },
                {
                    "sent": "But maybe we can do even better.",
                    "label": 1
                },
                {
                    "sent": "We could obtain an algorithm that is better than Perceptron, but at the same time is bounded.",
                    "label": 0
                },
                {
                    "sent": "And how to do it?",
                    "label": 0
                },
                {
                    "sent": "We can just update.",
                    "label": 0
                },
                {
                    "sent": "On our rounds in which there is not a mistake, but still, the confidence on the prediction is too small.",
                    "label": 0
                },
                {
                    "sent": "And in that rounds we update only if it is possible to do a projection.",
                    "label": 1
                },
                {
                    "sent": "That is, we analyze the mistake bound and we see if the update would improve or not.",
                    "label": 1
                },
                {
                    "sent": "The mistake bound.",
                    "label": 0
                },
                {
                    "sent": "If yes, we update the solution, otherwise we don't do anything like in the perception.",
                    "label": 0
                },
                {
                    "sent": "And this is something like adding a margin to the perceptron algorithm, and so this would improve the performance, but only some updates.",
                    "label": 1
                },
                {
                    "sent": "At the same time, we are not having the disadvantage of adding a merging, so having a lot of super set a lot of samples continuously added to super set like in the passive aggressive.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's let's see some results.",
                    "label": 0
                },
                {
                    "sent": "We compared our large grid that is projector in the Projector Pro plus that is this second variant with the perception the passive aggressive that will give us somehow an upper bound to the performance of the projector and plus plus the forget run the random budget perception.",
                    "label": 0
                },
                {
                    "sent": "Another language that I could stop throwing that it's really simple and.",
                    "label": 0
                },
                {
                    "sent": "One it reaches the maximum imposed solution, it just stops updating the classifier just to see if all these strategies that we are developing are useful or not compared to the real stupid one.",
                    "label": 0
                },
                {
                    "sent": "And to have somehow a fair comparison because the algorithm are implicitly different, we have set U that is the mask maximum norm of the competitor in projector and projector and Pro plus and forget run to be exactly the same.",
                    "label": 0
                },
                {
                    "sent": "So again this will result in a certain fixed budget for the forgotten.",
                    "label": 1
                },
                {
                    "sent": "But this will result in an unpredictable sides of the solution for projector and projector plus plus.",
                    "label": 1
                },
                {
                    "sent": "But we will see.",
                    "label": 0
                },
                {
                    "sent": "In the results that our solution is always smaller.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the an experiment on the advertising data set with the Gaussian kernel and it's the size of the solution in relation to the number of training samples, and you see that with the perceptron that is the red line.",
                    "label": 1
                },
                {
                    "sent": "The solution is basically growing linearly with number of training samples and with passive aggressive it's even worse.",
                    "label": 0
                },
                {
                    "sent": "And it will grow forever.",
                    "label": 0
                },
                {
                    "sent": "It will never stop.",
                    "label": 0
                },
                {
                    "sent": "And here you see, instead of these two lines, superimposed are the behavior for the forgotten and random budget perception.",
                    "label": 0
                },
                {
                    "sent": "So basically they start growing, then they reached the maximum sites and it remains constant for for the rest of the life of the algorithm and the other two lines here and for the projector and projector and plus plus.",
                    "label": 0
                },
                {
                    "sent": "And you see that they are slowly growing and we know that they will reach a limit.",
                    "label": 0
                },
                {
                    "sent": "There is an assistant and they cannot go.",
                    "label": 0
                },
                {
                    "sent": "Beyond that point in from this graph, you can start seeing more or less where where their sympathies.",
                    "label": 0
                },
                {
                    "sent": "So let's now see.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The performance.",
                    "label": 0
                },
                {
                    "sent": "This is the average number of mistakes in relation to the number of training samples.",
                    "label": 1
                },
                {
                    "sent": "And the perception is the red line.",
                    "label": 0
                },
                {
                    "sent": "And it's almost superimposed with the blue line that is the projector.",
                    "label": 0
                },
                {
                    "sent": "That is where we have obtained an algorithm that is obtaining exactly the same performance, or we are losing very very few compared to the perceptron.",
                    "label": 0
                },
                {
                    "sent": "Bye.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But the number of the sites of the solution here at the end of the training is around 7000, and in our algorithm is around 1:00.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And these two lines here.",
                    "label": 0
                },
                {
                    "sent": "Instead RARD behavior for the random budget perception in the forget run, so you can see that until this point they are just standard perceptron there close to the perception.",
                    "label": 0
                },
                {
                    "sent": "But from this point on this card discarding samples from the Super set and so somehow they stop improving and they go even a little worse from this point on.",
                    "label": 0
                },
                {
                    "sent": "And instead of this black line, here is the projector and plus plus that it's better than perceptor, and it's worse than the passive aggressive.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But the number of the sites of the solution is even smaller than the projectile itself.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's see some other numerical results.",
                    "label": 0
                },
                {
                    "sent": "This is another data set, quite big again negation kernel.",
                    "label": 0
                },
                {
                    "sent": "And you can see we have used two different budget sites.",
                    "label": 0
                },
                {
                    "sent": "You see that the projector and projector and plus plus always outperform the forgotten and random budget perceptor.",
                    "label": 0
                },
                {
                    "sent": "And and again, the projector on plus plus it's always better.",
                    "label": 0
                },
                {
                    "sent": "Then the perceptor.",
                    "label": 0
                },
                {
                    "sent": "And the sites of the solution here.",
                    "label": 0
                },
                {
                    "sent": "It was impossible to 4000 imposing the norm of the maximum competitor.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that it's smaller for the projector in projection plus plus, and even for this other setting of the budget size.",
                    "label": 0
                },
                {
                    "sent": "And it is interesting to note that the performance of the stopped run that in average are not that bad compared to the others, but the variance is huge, so it means that in any case, these algorithms are doing something something useful.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "This is the summary.",
                    "label": 0
                },
                {
                    "sent": "I presented two different version of a bounded online algorithm and they depend on the parameter that allows you to trade accuracy for sparseness of the solution.",
                    "label": 1
                },
                {
                    "sent": "And compared to the other budget algorithm, they have the advantage that they we have a bounded support set size, but we don't discard the instances and it's exactly this that keeps the performance side.",
                    "label": 0
                },
                {
                    "sent": "However, even if we know that the size of the solution is bounded, we cannot determine these sites in advance.",
                    "label": 0
                },
                {
                    "sent": "And as a future work we want to mix the budget strategy with the projection strategy, because we know that we are some our talk.",
                    "label": 0
                },
                {
                    "sent": "We have a.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's all, thanks.",
                    "label": 0
                },
                {
                    "sent": "So I just thought about it was interesting you had this bound.",
                    "label": 0
                },
                {
                    "sent": "That was something like it was a regular bound divided by 1 -- 8 times the norm of the solution.",
                    "label": 0
                },
                {
                    "sent": "It was, it was much earlier on.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This one yeah, so.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's actually a nice bounded is 8 are there.",
                    "label": 0
                },
                {
                    "sent": "So is is this?",
                    "label": 0
                },
                {
                    "sent": "I mean is this the case where you have a perfect classifier?",
                    "label": 0
                },
                {
                    "sent": "I didn't.",
                    "label": 0
                },
                {
                    "sent": "I guess I didn't get a chance to read you.",
                    "label": 0
                },
                {
                    "sent": "Have you have is G have to make?",
                    "label": 0
                },
                {
                    "sent": "Does it mean there exists a perfect classifier?",
                    "label": 0
                },
                {
                    "sent": "There exists a perfect G. Sorry, does that mean that there is there exist?",
                    "label": 0
                },
                {
                    "sent": "Does this guy have to be?",
                    "label": 0
                },
                {
                    "sent": "Uh, uh, noise trees, Jesus.",
                    "label": 0
                },
                {
                    "sent": "The best classifier you can find the ninth site on the sequence of training samples.",
                    "label": 1
                },
                {
                    "sent": "OK, so it exists, you know that it exists, OK, but it doesn't have to have zero mistakes on the no.",
                    "label": 0
                },
                {
                    "sent": "Is this Aidan?",
                    "label": 0
                },
                {
                    "sent": "Maybe to get Lucia finite budget, will they'd have to depend on TSS?",
                    "label": 0
                },
                {
                    "sent": "Nothing else occurs because no no.",
                    "label": 0
                },
                {
                    "sent": "It it depends on Tytus.",
                    "label": 0
                },
                {
                    "sent": "No, it is just a constant.",
                    "label": 0
                },
                {
                    "sent": "OK, OK?",
                    "label": 0
                },
                {
                    "sent": "How?",
                    "label": 0
                },
                {
                    "sent": "Who found some problem of overfitting using your algorithm, 'cause you?",
                    "label": 0
                },
                {
                    "sent": "From the first point it is.",
                    "label": 0
                },
                {
                    "sent": "More.",
                    "label": 0
                },
                {
                    "sent": "It is not OK. Yeah, have you?",
                    "label": 0
                },
                {
                    "sent": "Well, basically no, because in the end.",
                    "label": 0
                },
                {
                    "sent": "The first version is just like a presenter.",
                    "label": 0
                },
                {
                    "sent": "I am each time in projecting in the in the sub in a certain subspace the solution.",
                    "label": 0
                },
                {
                    "sent": "But this cannot increase the amount of information, so I cannot really overfit my data.",
                    "label": 0
                },
                {
                    "sent": "I am decreasing the amount of information that I have served.",
                    "label": 0
                }
            ]
        }
    }
}