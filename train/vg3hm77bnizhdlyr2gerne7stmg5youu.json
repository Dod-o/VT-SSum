{
    "id": "vg3hm77bnizhdlyr2gerne7stmg5youu",
    "title": "Token and Type Constraints for Cross-Lingual Part-of-Speech Tagging",
    "info": {
        "author": [
            "Oscar T\u00e4ckstr\u00f6m, SICS - Swedish Institute of Computer Science"
        ],
        "published": "Oct. 2, 2013",
        "recorded": "August 2013",
        "category": [
            "Top->Computer Science->Computational Linguistics"
        ]
    },
    "url": "http://videolectures.net/acl2013_tackstrom_token/",
    "segmentation": [
        [
            "So thank you for coming.",
            "I'm going to talk about.",
            "Talk in a tight constraints for crosslinking parts.",
            "Be studying so."
        ],
        [
            "The scenario we consider is.",
            "OK, speak louder so this now we consider is.",
            "First base tagging where we don't have.",
            "OK, better now.",
            "OK, so we consider a scenario where we don't have any annotated treebank data available in the target language is.",
            "So this is true of most of the world's languages, so it's an important scenario to consider.",
            "So most previous work in this area have looked at two different kinds of supervision.",
            "So on one hand people have used type constraints that is.",
            "List of words and the potential part of speech tags with those words in some context and reducing nose to constraint generative models.",
            "And the second type of constraint is token level constraint, particularly constraints projected using word aligned by text.",
            "And.",
            "Previous work I've considered using either type constraints or token constraints.",
            "There's been a little bit of debate of which kind of supervision is the most useful, and in this work will show that actually combining these two kinds of constraints is going to be beneficial, and in particular we show that.",
            "Rather than.",
            "Training generative models.",
            "When we use these couple constraints, we can actually move to discriminative models and particularly latent variables CRF models.",
            "And we will show that this will give us substantial improvements over previous state of the art."
        ],
        [
            "So we're considering part of speech tagging.",
            "Given a sentence, we want to predict the part of speech tag of each word in the sentence.",
            "We want to do this not only for English.",
            "We want to do this for all the world's languages.",
            "So one way to do this is to sit down and annotate each language.",
            "And train a supervised Tiger and this will give us about 95% accuracy on in domain data and about 90% accuracy will be low on our domain data and the method that I will show that will not require any explicit treatment annotation in the target language is will actually come close to this 90%."
        ],
        [
            "So throughout the talk, I would consider not to fine grained language specific part of speech.",
            "Sites are available free banks and I would consider coarse grained universal tags.",
            "In particular, we used to speech tags, mapping apart speech tags from fine grained to coarse grained given by petrol in 2012."
        ],
        [
            "So this gives us a consistent annotation of 12 different parts of speech across languages."
        ],
        [
            "This is a destructive process, so we lose some information by collapsing fine grained tags, intercourse, Contacts."
        ],
        [
            "At the same time.",
            "By using this consistent notation across languages, we can use cross lingual projection methods, and it's also been shown that if you have this universal tags available, we can use them to train downstream cross lingual tasks such as dependency parsers."
        ],
        [
            "So over the next slide, so we consider three types of constraints.",
            "1st, I'm going to look at to constraints.",
            "Impact, particularly, is projected to level constraints.",
            "Then I'm going to add type constraints.",
            "And finally, I'm going to look at how to combine these two types of constraints."
        ],
        [
            "And I will look at two different statistical models.",
            "The first model is feature based or log linear hidden Markov models.",
            "And we will use this when training type constraint models in line with previous work.",
            "So this is similar to standard here hidden Markov model.",
            "But instead of multinomial transition and emission potentials we will look at log linear parametrisations of these and this allows us to incorporate richer features compared to a standard.",
            "Categorial hidden Markov model.",
            "And then we will see that when we incorporate some token constraints, we can actually train a discriminative model.",
            "So we will look at partial observed or latent variables RF models and this allows us to incorporate even richer feature set compared to the standard market model."
        ],
        [
            "So to set the base here, this is the standard supervised setting where we assume that we have complete token level supervision.",
            "So we have sentences here annotate where each word is annotated with the correct or part of speech.",
            "The option of this is that it provides us with complete disintegration in context, so here we have the word vulgar in Swedish, which can mean to be in the Websense or commodity in the nonsense, and with this token level invitation we completely disambiguate these cases."
        ],
        [
            "So as I said, it's expensive to achieve obtain this kind of annotation, so instead we're going to look at the projection methods and we follow work by Ruskin Guy from 2001 where we simply use word aligned by text to project part of speech tags.",
            "So we assume that you have some resource rich language.",
            "We will use English result.",
            "Every word, a line.",
            "We aligned words in these two languages and then we part of speech tag source side.",
            "In this case English, and then we project the tags using this.",
            "Alignment links.",
            "So this is good because it's cheap to obtain bilingual by text data in a lot of languages.",
            "And the problem is that it only gives us an incomplete.",
            "Talking constraints, so in this case we have the World Health care which due to non literal translation doesn't have any word alignment and therefore doesn't receive A tag from the source side.",
            "So that tag is now completely unknown to us.",
            "And also this projection methods will often give us systematic errors.",
            "So in this case for some reason agricultural is here are aligned with product owner, which should be a noun.",
            "And Jeffrey this word receives the incorrect addictiv tag and due to systematic divergences between languages this type of noise tends to be systematic."
        ],
        [
            "OK.",
            "So assuming we have these partial token level constraints, we can use them to train.",
            "Latent variable CRF model.",
            "So in this visualization, which I will use over the next couple of slides and this blue path here denote the inference or space of the Tiger or the partition function in CRF model."
        ],
        [
            "And we can optimize the marginal conditional likelihood of the data by summing over all of these purple paths that represent observations.",
            "So here we have this word.",
            "Hilton didn't receive A tag, so we have to sum over all possible taggings of that word during training.",
            "And I should say also that these token level constraints are only needed during training, so we don't need any parallel data for a test time.",
            "Then we only need.",
            "They are only needed during training.",
            "OK yeah, so we sum over these possible paths."
        ],
        [
            "So I was talking to strange.",
            "Let's look at type constraints.",
            "So the idea here is that we have A tag dictionary which lists for each word in the lexicon the set of tags that that word can obtain in some context.",
            "So there's been a different recent work has shown that you can derive such secretaries in different ways.",
            "For example, you can use cross lingual projection similar to the token level projections.",
            "You can project an aggregated attack dictionary and the work of doesn't petrol until 11.",
            "Show that you can also use label propagation to expand this.",
            "Dictionaries.",
            "Another recent work by Lidl in 2012 show that can use Wiktionary are freely available crowd source, lexecon to construct this tag dictionaries.",
            "And finally, recent work by Greta Del showed that if you haven't limited time for annotation, it might be better to spend that annotating types.",
            "Rather than tokens.",
            "The problem of course with this is that it only provides us with the Portuguese immigration, so in this case here we have to say again they were involved and the only thing we know about this is that it can be either a noun and verb.",
            "We don't know in a specific context which one should be.",
            "OK, but still."
        ],
        [
            "You can use this to train generative model.",
            "So the idea here is that we use these tag dictionary as type constraints to prune the search space of the Tiger.",
            "So what we do here is we optimize."
        ],
        [
            "Then we maximize the likelihood of surge words while marginalizing over this restricted inference based."
        ],
        [
            "OK, so the problem with using either token or tight constraints is that.",
            "As I said, I constraints only provides a partial integration, whereas this projected token level constraints might be noisy.",
            "But the idea of this method approach here is to.",
            "Couple these types of constraints so that is that.",
            "The systematic noise that is present in the projected token level constraints is a different type of noise compared to the bias inherent in the tag dictionary.",
            "So we would show here in the next example that you can use to tag dictionary to actually filter the projected to constraints to give a better get a better tagging so we could do that simple three step process.",
            "First we're going to just take the type constraints again.",
            "We're going to prune the inference base with those constraints like this.",
            "Then"
        ],
        [
            "We're going to superimpose the projected token level tags.",
            "We're going to simply use these type constraints to filter out the token level constraints that are inconsistent with them.",
            "So in this case we will filter out."
        ],
        [
            "Incorrectly predicted adjective tag and force it to be the correct down tag because.",
            "The noun tag was listed in the tag dictionary, whereas Adjective Tag was not.",
            "And also we will prune."
        ],
        [
            "Otis.",
            "Disambiguous tags.",
            "That were inconsistent with the tag dictionary, and this gives us."
        ],
        [
            "Very restricted learning problem, which is much simpler to handle, so again we can train a model with a CRF model by simply."
        ],
        [
            "Redistributing probability mass in this blue space towards this purple path.",
            "So again we just marginalized over these incomplete observationes, while restricting the search space with the time constraints."
        ],
        [
            "OK, so.",
            "Let's look at some results of these different types of constraints.",
            "So starting with the type constraints and simple 1st order feature based in Markov model, we're going to consider three different types of sources of dictionaries.",
            "First, going to look at Wiktionary, so this is similar to the work of little.",
            "2012 and this gives us an accuracy of about 76%.",
            "So all these type constraints model will be generative models.",
            "We're going to look at projected constraints using the method of inducing Petro, but without their additional labor of application step because preliminary experiments showed that for our datasets it actually hurt to use label propagation.",
            "So we just take projected tags and aggregate them into attack dictionary using their method.",
            "And this gives us about 79% accuracy and this shows are averaged over 50 languages.",
            "Most of them are in European languages, but they also include languages such as Chinese, Turkish and Japanese.",
            "So.",
            "The Wiktionary dictionary and projected dictionary have a slightly different coverage, so way to improve results with this type level constraints is to simply take the union of these dictionaries to simply broader coverage.",
            "And if we do this and train as type constraint model, we get about 80% accuracy.",
            "OK.",
            "So to raise the baseline a bit further before we consider the token and type constraint models, we also add word clusters.",
            "So we run a word clustering algorithm.",
            "Delegated muskrats advance on monolingual target language data, and we use those clusters as features, object type, constraint model.",
            "And this raises the based on significantly, substantially up to about 83%.",
            "OK, so then when we consider combining token and type constraints, we're using the projected total level constraints.",
            "We are cutting them with type constraint based on dictionary, not the Union dictionary because we're going to take the Union dictionary.",
            "That will contain the same systematic noise that was already present in the projected data, so we won't use that for filtering.",
            "And then we also had this word cluster features like we used in the previous baseline.",
            "This gives us about 84.5% accuracy, so that's already a significant further significant improvement.",
            "OK."
        ],
        [
            "So comparing with the previous state of the art, we look at the work with Dawson Petrofac, 11 which use.",
            "This aggregation of our projected dictionary, together with label propagation.",
            "And these experiments are only run or averages over 8 languages because the other previous work only used 18 European languages.",
            "So doesn't Petrov.",
            "Got about 83% accuracy on this task, then lead.",
            "I'll in 2012 used the Wiktionary.",
            "Together with the 2nd order hidden Markov model, so doesn't petrol use the 1st order hidden Markov model late.",
            "I'll use the 2nd order model and they showed that you can improve ourselves up to about 85% accuracy on average.",
            "This languages.",
            "And then looking at our new work on token and type constraints, we get almost 89% accuracy.",
            "So that's like 25% relative error reduction compared to the previous data dot."
        ],
        [
            "So now I'm going to look at.",
            "Two different types of errors that are token in type constraint system can make the first type of errors projection errors, which is the case when you have projected tag that is incorrectly projected.",
            "And and you fail to filter it out using the type constraints.",
            "OK, so in this case we had."
        ],
        [
            "This objective tag that was incorrectly projected.",
            "When we had this word predictor in the tech dictionary, we could use that."
        ],
        [
            "To constrain the space to the correct Mount tag."
        ],
        [
            "But if we didn't have this word in the dictionary."
        ],
        [
            "We would still force the Tigers to make this prediction.",
            "OK, so this is.",
            "Not the worst kind of error, because in another context the same word might have the correct tag projected.",
            "And if we have the correct tag projected in the majority of cases, we could still.",
            "Probably learn to do direct acting.",
            "The more problematic kind of error is what it called prune."
        ],
        [
            "Errors, which is when a word is present in the dictionary, but for some reason it fails to list one or more of the correct tags.",
            "So in this case we correct adverbial form here was listed in the dictionary."
        ],
        [
            "If the tag dictionary only listed objective form.",
            "We would forced to always make this error in this word.",
            "Of course, in some contexts the objective would be the correct tag, but we would always force it, so that's amore.",
            "Important kind of error to address.",
            "OK, so we."
        ],
        [
            "Have some experiments to see how.",
            "How much will impact is pruning errors are because the projection errors are difficult to measure since we don't have any token level annotation on the projected by only by text.",
            "So we can't really measure the projection errors directly.",
            "But this pruning errors we can measure by taking the dictionary using them as tight constraints on the test set, and then see how often we actually filter away the correct tag by mistake and using the current version of dictionary we see that in about almost 6% of words we actually prune away the correct tag, so that provides us with an upper bound of about 94% accuracy.",
            "So that's the best we can achieve, and as you saw, we achieved about 85% accuracy over the 15 languages.",
            "So having a 94% upper bound might really affect the system.",
            "The good thing about this, or the positive side of this is that it turns out that we can.",
            "That we can fix these errors quite easily by simply correcting the most common words in dictionary.",
            "So if we take the 50 most common words in the dictionary and we make sure that.",
            "Their correct tags are listed.",
            "We reduce the errors almost in half and then if we do 100 most common words in the 500 most common words, we quickly improved upper bound of the Tiger up to almost 99%.",
            "So this suggests tying back to work with credibility."
        ],
        [
            "Suggest that maybe we should spend 2 hours of dictionary editing to really raise this baseline or raised upper bound."
        ],
        [
            "So in conclusion.",
            "The main take home message of this talk is that even though we looked at part of speech tagging pacifically, it shows that you can really use this discriminative models even though when you don't have full supervision.",
            "In particular, I show that if you use these type constraint and talking constraint in couple of them, you can get a significant improvement compared to using only one of the type constraints and generative model.",
            "And we saw more than 25% relative error reduction.",
            "And also I should stress that previous work such as work doesn't.",
            "Petrol and Gretel have used things like labor propagation and model minimization and different touristics?",
            "As preprocessing steps in a cutter complicated pipeline, whereas in this work we only.",
            "The only heuristic we use this when we use the type constraints to filter the inference base.",
            "After that we generally have training data that we just plug into a standard partially observed theoretic model.",
            "Also, I think there might be interesting extension in this work.",
            "For example, you can imagine doing named entity recognition frame semantics.",
            "Virtually any task where you can imagine coming up with some type constraints as well as cross lingual projected token level constraints.",
            "OK."
        ],
        [
            "So thank you listening.",
            "Thank you very interesting work.",
            "It might be in the paper, but I don't have it here.",
            "Do you have the result for the type token combination without the clusters?",
            "We don't have them in the paper because of too many results, but.",
            "The relative difference is about the same.",
            "So yeah, we don't have them in the paper, but you see the token and type constraints work better even when you don't have the clusters.",
            "OK, so you don't remember the number.",
            "I don't remember the number.",
            "Thank you.",
            "2 questions.",
            "One is perhaps a clarification question, so when you talk it seemed like like like really the type constraints helped so we can sort of recover from errors in the token based alignment.",
            "Do you have examples where it's the other way around where the token sort of helps you recover from the type?",
            "I think, yeah, I think that's not really the token level, but the projected constraints.",
            "When we create A tag dictionary from them that gives and we take the union of that with dictionary, that gives us an expanded dictionary, but it does so that can fix this error when you incorrectly pruned type of mistake.",
            "But since we use the tight constraints as like the primary filter, we only consider to consumers that are consistent with those.",
            "We can't use the direct talking constraints to fix the.",
            "In this dictionary based approach, how well does this work for highly inflected languages?",
            "So.",
            "It will work worse, definitely.",
            "I mean so in this case with project from English and in result we see that for example Turkish, you get substantially lower sales compared to the in European languages.",
            "I think it's around 70% something, but you still see an improvement from combining these two types of constraints.",
            "Don't know about Japanese and Chinese, but you also see significant improvement for those languages.",
            "But yeah, it's further from English.",
            "You go the worse it gets.",
            "Thanks for the talk, I just wondering how to you optimize the new object function menu apps on constraints.",
            "How do you optimize the new objective function you propose over optimize it?",
            "Yeah, Oh yeah.",
            "So we.",
            "Basically, in terms of using gradient based optimization, So what you get is basically.",
            "You move away from feature expectations according to the model in.",
            "This holds inference pace towards feature expectations in the model in this.",
            "Partial observation space.",
            "Yeah, maybe we can take it offline to discuss it, but it's just some overall the possible paths in observation that is, rather than having a single path in the changes that you made to Wiktionary with a percentage of them were closed class changes versus open class words, and what would the modification, what percentage were adjectives versus nouns in that type of thing?",
            "Oh, I'm not sure there is some results.",
            "We have some visualizations in the paper where we look at pruning errors.",
            "Per tag and that varies across languages quite substantially.",
            "Some tax in some languages see a lot of errors, but I think that might be because of partial.",
            "It might be because from the conversion from fine grained to coarse grained tags because there is also this X tag which contains the tags that couldn't be mapped to another category, so there might be different there.",
            "But we didn't look specifically at open and closed.",
            "That's where you could imagine having separate filtering steps for open class versus closed last words or handled differently in some way, but we didn't do that in this world.",
            "Thank you.",
            "Hi, I'm long from everything um.",
            "Do you consider like menu to one mapping?",
            "Many to many mapping when you project that app from so language to target language and the single question, did you consider using different language other than English?",
            "So what's the second question?",
            "If we projected from another language language?",
            "So no, we didn't consider any other language than English because we have most parallel data from English.",
            "But you could.",
            "It's not necessarily tide to English, but if you have so whatever parallel data you have, you can use it.",
            "Anne was the first.",
            "The first question is when you project.",
            "It seems that you only exploit the one to one mapping, right?",
            "Yeah, we only have about many to one in before, so bidirectional mapping, so this also makes the token constraints much sparser than they could have been if we consider many to one mappings.",
            "In particular, we see problems with compound words, for example.",
            "And so I think.",
            "DEP after the coverage of the token constraints is about 50%, so 50% of the tokens have a projected tag, 50% have don't have created tag in terms of type constraints.",
            "On average.",
            "I think it's about 6070% rewards types that are covered in some way, but at which an airy.",
            "So by combining these two constraints, you kind of.",
            "Compressed air, thank you very much.",
            "OK, well let's thank our speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So thank you for coming.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about.",
                    "label": 0
                },
                {
                    "sent": "Talk in a tight constraints for crosslinking parts.",
                    "label": 1
                },
                {
                    "sent": "Be studying so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The scenario we consider is.",
                    "label": 0
                },
                {
                    "sent": "OK, speak louder so this now we consider is.",
                    "label": 0
                },
                {
                    "sent": "First base tagging where we don't have.",
                    "label": 0
                },
                {
                    "sent": "OK, better now.",
                    "label": 0
                },
                {
                    "sent": "OK, so we consider a scenario where we don't have any annotated treebank data available in the target language is.",
                    "label": 0
                },
                {
                    "sent": "So this is true of most of the world's languages, so it's an important scenario to consider.",
                    "label": 0
                },
                {
                    "sent": "So most previous work in this area have looked at two different kinds of supervision.",
                    "label": 0
                },
                {
                    "sent": "So on one hand people have used type constraints that is.",
                    "label": 0
                },
                {
                    "sent": "List of words and the potential part of speech tags with those words in some context and reducing nose to constraint generative models.",
                    "label": 0
                },
                {
                    "sent": "And the second type of constraint is token level constraint, particularly constraints projected using word aligned by text.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Previous work I've considered using either type constraints or token constraints.",
                    "label": 0
                },
                {
                    "sent": "There's been a little bit of debate of which kind of supervision is the most useful, and in this work will show that actually combining these two kinds of constraints is going to be beneficial, and in particular we show that.",
                    "label": 0
                },
                {
                    "sent": "Rather than.",
                    "label": 0
                },
                {
                    "sent": "Training generative models.",
                    "label": 0
                },
                {
                    "sent": "When we use these couple constraints, we can actually move to discriminative models and particularly latent variables CRF models.",
                    "label": 0
                },
                {
                    "sent": "And we will show that this will give us substantial improvements over previous state of the art.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we're considering part of speech tagging.",
                    "label": 0
                },
                {
                    "sent": "Given a sentence, we want to predict the part of speech tag of each word in the sentence.",
                    "label": 0
                },
                {
                    "sent": "We want to do this not only for English.",
                    "label": 0
                },
                {
                    "sent": "We want to do this for all the world's languages.",
                    "label": 0
                },
                {
                    "sent": "So one way to do this is to sit down and annotate each language.",
                    "label": 0
                },
                {
                    "sent": "And train a supervised Tiger and this will give us about 95% accuracy on in domain data and about 90% accuracy will be low on our domain data and the method that I will show that will not require any explicit treatment annotation in the target language is will actually come close to this 90%.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So throughout the talk, I would consider not to fine grained language specific part of speech.",
                    "label": 0
                },
                {
                    "sent": "Sites are available free banks and I would consider coarse grained universal tags.",
                    "label": 0
                },
                {
                    "sent": "In particular, we used to speech tags, mapping apart speech tags from fine grained to coarse grained given by petrol in 2012.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this gives us a consistent annotation of 12 different parts of speech across languages.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is a destructive process, so we lose some information by collapsing fine grained tags, intercourse, Contacts.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At the same time.",
                    "label": 0
                },
                {
                    "sent": "By using this consistent notation across languages, we can use cross lingual projection methods, and it's also been shown that if you have this universal tags available, we can use them to train downstream cross lingual tasks such as dependency parsers.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So over the next slide, so we consider three types of constraints.",
                    "label": 0
                },
                {
                    "sent": "1st, I'm going to look at to constraints.",
                    "label": 0
                },
                {
                    "sent": "Impact, particularly, is projected to level constraints.",
                    "label": 0
                },
                {
                    "sent": "Then I'm going to add type constraints.",
                    "label": 1
                },
                {
                    "sent": "And finally, I'm going to look at how to combine these two types of constraints.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I will look at two different statistical models.",
                    "label": 0
                },
                {
                    "sent": "The first model is feature based or log linear hidden Markov models.",
                    "label": 0
                },
                {
                    "sent": "And we will use this when training type constraint models in line with previous work.",
                    "label": 0
                },
                {
                    "sent": "So this is similar to standard here hidden Markov model.",
                    "label": 0
                },
                {
                    "sent": "But instead of multinomial transition and emission potentials we will look at log linear parametrisations of these and this allows us to incorporate richer features compared to a standard.",
                    "label": 0
                },
                {
                    "sent": "Categorial hidden Markov model.",
                    "label": 0
                },
                {
                    "sent": "And then we will see that when we incorporate some token constraints, we can actually train a discriminative model.",
                    "label": 0
                },
                {
                    "sent": "So we will look at partial observed or latent variables RF models and this allows us to incorporate even richer feature set compared to the standard market model.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to set the base here, this is the standard supervised setting where we assume that we have complete token level supervision.",
                    "label": 0
                },
                {
                    "sent": "So we have sentences here annotate where each word is annotated with the correct or part of speech.",
                    "label": 0
                },
                {
                    "sent": "The option of this is that it provides us with complete disintegration in context, so here we have the word vulgar in Swedish, which can mean to be in the Websense or commodity in the nonsense, and with this token level invitation we completely disambiguate these cases.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as I said, it's expensive to achieve obtain this kind of annotation, so instead we're going to look at the projection methods and we follow work by Ruskin Guy from 2001 where we simply use word aligned by text to project part of speech tags.",
                    "label": 0
                },
                {
                    "sent": "So we assume that you have some resource rich language.",
                    "label": 0
                },
                {
                    "sent": "We will use English result.",
                    "label": 0
                },
                {
                    "sent": "Every word, a line.",
                    "label": 0
                },
                {
                    "sent": "We aligned words in these two languages and then we part of speech tag source side.",
                    "label": 0
                },
                {
                    "sent": "In this case English, and then we project the tags using this.",
                    "label": 0
                },
                {
                    "sent": "Alignment links.",
                    "label": 0
                },
                {
                    "sent": "So this is good because it's cheap to obtain bilingual by text data in a lot of languages.",
                    "label": 0
                },
                {
                    "sent": "And the problem is that it only gives us an incomplete.",
                    "label": 0
                },
                {
                    "sent": "Talking constraints, so in this case we have the World Health care which due to non literal translation doesn't have any word alignment and therefore doesn't receive A tag from the source side.",
                    "label": 0
                },
                {
                    "sent": "So that tag is now completely unknown to us.",
                    "label": 0
                },
                {
                    "sent": "And also this projection methods will often give us systematic errors.",
                    "label": 0
                },
                {
                    "sent": "So in this case for some reason agricultural is here are aligned with product owner, which should be a noun.",
                    "label": 0
                },
                {
                    "sent": "And Jeffrey this word receives the incorrect addictiv tag and due to systematic divergences between languages this type of noise tends to be systematic.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So assuming we have these partial token level constraints, we can use them to train.",
                    "label": 0
                },
                {
                    "sent": "Latent variable CRF model.",
                    "label": 0
                },
                {
                    "sent": "So in this visualization, which I will use over the next couple of slides and this blue path here denote the inference or space of the Tiger or the partition function in CRF model.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we can optimize the marginal conditional likelihood of the data by summing over all of these purple paths that represent observations.",
                    "label": 1
                },
                {
                    "sent": "So here we have this word.",
                    "label": 0
                },
                {
                    "sent": "Hilton didn't receive A tag, so we have to sum over all possible taggings of that word during training.",
                    "label": 0
                },
                {
                    "sent": "And I should say also that these token level constraints are only needed during training, so we don't need any parallel data for a test time.",
                    "label": 0
                },
                {
                    "sent": "Then we only need.",
                    "label": 0
                },
                {
                    "sent": "They are only needed during training.",
                    "label": 0
                },
                {
                    "sent": "OK yeah, so we sum over these possible paths.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I was talking to strange.",
                    "label": 0
                },
                {
                    "sent": "Let's look at type constraints.",
                    "label": 1
                },
                {
                    "sent": "So the idea here is that we have A tag dictionary which lists for each word in the lexicon the set of tags that that word can obtain in some context.",
                    "label": 0
                },
                {
                    "sent": "So there's been a different recent work has shown that you can derive such secretaries in different ways.",
                    "label": 0
                },
                {
                    "sent": "For example, you can use cross lingual projection similar to the token level projections.",
                    "label": 0
                },
                {
                    "sent": "You can project an aggregated attack dictionary and the work of doesn't petrol until 11.",
                    "label": 0
                },
                {
                    "sent": "Show that you can also use label propagation to expand this.",
                    "label": 0
                },
                {
                    "sent": "Dictionaries.",
                    "label": 0
                },
                {
                    "sent": "Another recent work by Lidl in 2012 show that can use Wiktionary are freely available crowd source, lexecon to construct this tag dictionaries.",
                    "label": 0
                },
                {
                    "sent": "And finally, recent work by Greta Del showed that if you haven't limited time for annotation, it might be better to spend that annotating types.",
                    "label": 0
                },
                {
                    "sent": "Rather than tokens.",
                    "label": 0
                },
                {
                    "sent": "The problem of course with this is that it only provides us with the Portuguese immigration, so in this case here we have to say again they were involved and the only thing we know about this is that it can be either a noun and verb.",
                    "label": 0
                },
                {
                    "sent": "We don't know in a specific context which one should be.",
                    "label": 0
                },
                {
                    "sent": "OK, but still.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You can use this to train generative model.",
                    "label": 0
                },
                {
                    "sent": "So the idea here is that we use these tag dictionary as type constraints to prune the search space of the Tiger.",
                    "label": 1
                },
                {
                    "sent": "So what we do here is we optimize.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we maximize the likelihood of surge words while marginalizing over this restricted inference based.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the problem with using either token or tight constraints is that.",
                    "label": 0
                },
                {
                    "sent": "As I said, I constraints only provides a partial integration, whereas this projected token level constraints might be noisy.",
                    "label": 0
                },
                {
                    "sent": "But the idea of this method approach here is to.",
                    "label": 0
                },
                {
                    "sent": "Couple these types of constraints so that is that.",
                    "label": 0
                },
                {
                    "sent": "The systematic noise that is present in the projected token level constraints is a different type of noise compared to the bias inherent in the tag dictionary.",
                    "label": 0
                },
                {
                    "sent": "So we would show here in the next example that you can use to tag dictionary to actually filter the projected to constraints to give a better get a better tagging so we could do that simple three step process.",
                    "label": 0
                },
                {
                    "sent": "First we're going to just take the type constraints again.",
                    "label": 1
                },
                {
                    "sent": "We're going to prune the inference base with those constraints like this.",
                    "label": 0
                },
                {
                    "sent": "Then",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We're going to superimpose the projected token level tags.",
                    "label": 0
                },
                {
                    "sent": "We're going to simply use these type constraints to filter out the token level constraints that are inconsistent with them.",
                    "label": 1
                },
                {
                    "sent": "So in this case we will filter out.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Incorrectly predicted adjective tag and force it to be the correct down tag because.",
                    "label": 0
                },
                {
                    "sent": "The noun tag was listed in the tag dictionary, whereas Adjective Tag was not.",
                    "label": 0
                },
                {
                    "sent": "And also we will prune.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Otis.",
                    "label": 0
                },
                {
                    "sent": "Disambiguous tags.",
                    "label": 0
                },
                {
                    "sent": "That were inconsistent with the tag dictionary, and this gives us.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very restricted learning problem, which is much simpler to handle, so again we can train a model with a CRF model by simply.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Redistributing probability mass in this blue space towards this purple path.",
                    "label": 0
                },
                {
                    "sent": "So again we just marginalized over these incomplete observationes, while restricting the search space with the time constraints.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Let's look at some results of these different types of constraints.",
                    "label": 0
                },
                {
                    "sent": "So starting with the type constraints and simple 1st order feature based in Markov model, we're going to consider three different types of sources of dictionaries.",
                    "label": 0
                },
                {
                    "sent": "First, going to look at Wiktionary, so this is similar to the work of little.",
                    "label": 0
                },
                {
                    "sent": "2012 and this gives us an accuracy of about 76%.",
                    "label": 0
                },
                {
                    "sent": "So all these type constraints model will be generative models.",
                    "label": 0
                },
                {
                    "sent": "We're going to look at projected constraints using the method of inducing Petro, but without their additional labor of application step because preliminary experiments showed that for our datasets it actually hurt to use label propagation.",
                    "label": 0
                },
                {
                    "sent": "So we just take projected tags and aggregate them into attack dictionary using their method.",
                    "label": 0
                },
                {
                    "sent": "And this gives us about 79% accuracy and this shows are averaged over 50 languages.",
                    "label": 0
                },
                {
                    "sent": "Most of them are in European languages, but they also include languages such as Chinese, Turkish and Japanese.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The Wiktionary dictionary and projected dictionary have a slightly different coverage, so way to improve results with this type level constraints is to simply take the union of these dictionaries to simply broader coverage.",
                    "label": 0
                },
                {
                    "sent": "And if we do this and train as type constraint model, we get about 80% accuracy.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So to raise the baseline a bit further before we consider the token and type constraint models, we also add word clusters.",
                    "label": 0
                },
                {
                    "sent": "So we run a word clustering algorithm.",
                    "label": 0
                },
                {
                    "sent": "Delegated muskrats advance on monolingual target language data, and we use those clusters as features, object type, constraint model.",
                    "label": 0
                },
                {
                    "sent": "And this raises the based on significantly, substantially up to about 83%.",
                    "label": 0
                },
                {
                    "sent": "OK, so then when we consider combining token and type constraints, we're using the projected total level constraints.",
                    "label": 1
                },
                {
                    "sent": "We are cutting them with type constraint based on dictionary, not the Union dictionary because we're going to take the Union dictionary.",
                    "label": 0
                },
                {
                    "sent": "That will contain the same systematic noise that was already present in the projected data, so we won't use that for filtering.",
                    "label": 0
                },
                {
                    "sent": "And then we also had this word cluster features like we used in the previous baseline.",
                    "label": 0
                },
                {
                    "sent": "This gives us about 84.5% accuracy, so that's already a significant further significant improvement.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So comparing with the previous state of the art, we look at the work with Dawson Petrofac, 11 which use.",
                    "label": 1
                },
                {
                    "sent": "This aggregation of our projected dictionary, together with label propagation.",
                    "label": 0
                },
                {
                    "sent": "And these experiments are only run or averages over 8 languages because the other previous work only used 18 European languages.",
                    "label": 0
                },
                {
                    "sent": "So doesn't Petrov.",
                    "label": 0
                },
                {
                    "sent": "Got about 83% accuracy on this task, then lead.",
                    "label": 0
                },
                {
                    "sent": "I'll in 2012 used the Wiktionary.",
                    "label": 0
                },
                {
                    "sent": "Together with the 2nd order hidden Markov model, so doesn't petrol use the 1st order hidden Markov model late.",
                    "label": 0
                },
                {
                    "sent": "I'll use the 2nd order model and they showed that you can improve ourselves up to about 85% accuracy on average.",
                    "label": 0
                },
                {
                    "sent": "This languages.",
                    "label": 0
                },
                {
                    "sent": "And then looking at our new work on token and type constraints, we get almost 89% accuracy.",
                    "label": 0
                },
                {
                    "sent": "So that's like 25% relative error reduction compared to the previous data dot.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now I'm going to look at.",
                    "label": 0
                },
                {
                    "sent": "Two different types of errors that are token in type constraint system can make the first type of errors projection errors, which is the case when you have projected tag that is incorrectly projected.",
                    "label": 1
                },
                {
                    "sent": "And and you fail to filter it out using the type constraints.",
                    "label": 0
                },
                {
                    "sent": "OK, so in this case we had.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This objective tag that was incorrectly projected.",
                    "label": 0
                },
                {
                    "sent": "When we had this word predictor in the tech dictionary, we could use that.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To constrain the space to the correct Mount tag.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But if we didn't have this word in the dictionary.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We would still force the Tigers to make this prediction.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is.",
                    "label": 0
                },
                {
                    "sent": "Not the worst kind of error, because in another context the same word might have the correct tag projected.",
                    "label": 0
                },
                {
                    "sent": "And if we have the correct tag projected in the majority of cases, we could still.",
                    "label": 0
                },
                {
                    "sent": "Probably learn to do direct acting.",
                    "label": 0
                },
                {
                    "sent": "The more problematic kind of error is what it called prune.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Errors, which is when a word is present in the dictionary, but for some reason it fails to list one or more of the correct tags.",
                    "label": 0
                },
                {
                    "sent": "So in this case we correct adverbial form here was listed in the dictionary.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If the tag dictionary only listed objective form.",
                    "label": 0
                },
                {
                    "sent": "We would forced to always make this error in this word.",
                    "label": 0
                },
                {
                    "sent": "Of course, in some contexts the objective would be the correct tag, but we would always force it, so that's amore.",
                    "label": 0
                },
                {
                    "sent": "Important kind of error to address.",
                    "label": 0
                },
                {
                    "sent": "OK, so we.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Have some experiments to see how.",
                    "label": 0
                },
                {
                    "sent": "How much will impact is pruning errors are because the projection errors are difficult to measure since we don't have any token level annotation on the projected by only by text.",
                    "label": 0
                },
                {
                    "sent": "So we can't really measure the projection errors directly.",
                    "label": 0
                },
                {
                    "sent": "But this pruning errors we can measure by taking the dictionary using them as tight constraints on the test set, and then see how often we actually filter away the correct tag by mistake and using the current version of dictionary we see that in about almost 6% of words we actually prune away the correct tag, so that provides us with an upper bound of about 94% accuracy.",
                    "label": 0
                },
                {
                    "sent": "So that's the best we can achieve, and as you saw, we achieved about 85% accuracy over the 15 languages.",
                    "label": 0
                },
                {
                    "sent": "So having a 94% upper bound might really affect the system.",
                    "label": 0
                },
                {
                    "sent": "The good thing about this, or the positive side of this is that it turns out that we can.",
                    "label": 0
                },
                {
                    "sent": "That we can fix these errors quite easily by simply correcting the most common words in dictionary.",
                    "label": 0
                },
                {
                    "sent": "So if we take the 50 most common words in the dictionary and we make sure that.",
                    "label": 0
                },
                {
                    "sent": "Their correct tags are listed.",
                    "label": 0
                },
                {
                    "sent": "We reduce the errors almost in half and then if we do 100 most common words in the 500 most common words, we quickly improved upper bound of the Tiger up to almost 99%.",
                    "label": 0
                },
                {
                    "sent": "So this suggests tying back to work with credibility.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Suggest that maybe we should spend 2 hours of dictionary editing to really raise this baseline or raised upper bound.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in conclusion.",
                    "label": 0
                },
                {
                    "sent": "The main take home message of this talk is that even though we looked at part of speech tagging pacifically, it shows that you can really use this discriminative models even though when you don't have full supervision.",
                    "label": 0
                },
                {
                    "sent": "In particular, I show that if you use these type constraint and talking constraint in couple of them, you can get a significant improvement compared to using only one of the type constraints and generative model.",
                    "label": 0
                },
                {
                    "sent": "And we saw more than 25% relative error reduction.",
                    "label": 1
                },
                {
                    "sent": "And also I should stress that previous work such as work doesn't.",
                    "label": 1
                },
                {
                    "sent": "Petrol and Gretel have used things like labor propagation and model minimization and different touristics?",
                    "label": 0
                },
                {
                    "sent": "As preprocessing steps in a cutter complicated pipeline, whereas in this work we only.",
                    "label": 0
                },
                {
                    "sent": "The only heuristic we use this when we use the type constraints to filter the inference base.",
                    "label": 0
                },
                {
                    "sent": "After that we generally have training data that we just plug into a standard partially observed theoretic model.",
                    "label": 0
                },
                {
                    "sent": "Also, I think there might be interesting extension in this work.",
                    "label": 1
                },
                {
                    "sent": "For example, you can imagine doing named entity recognition frame semantics.",
                    "label": 0
                },
                {
                    "sent": "Virtually any task where you can imagine coming up with some type constraints as well as cross lingual projected token level constraints.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So thank you listening.",
                    "label": 0
                },
                {
                    "sent": "Thank you very interesting work.",
                    "label": 0
                },
                {
                    "sent": "It might be in the paper, but I don't have it here.",
                    "label": 0
                },
                {
                    "sent": "Do you have the result for the type token combination without the clusters?",
                    "label": 0
                },
                {
                    "sent": "We don't have them in the paper because of too many results, but.",
                    "label": 0
                },
                {
                    "sent": "The relative difference is about the same.",
                    "label": 0
                },
                {
                    "sent": "So yeah, we don't have them in the paper, but you see the token and type constraints work better even when you don't have the clusters.",
                    "label": 0
                },
                {
                    "sent": "OK, so you don't remember the number.",
                    "label": 0
                },
                {
                    "sent": "I don't remember the number.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "2 questions.",
                    "label": 0
                },
                {
                    "sent": "One is perhaps a clarification question, so when you talk it seemed like like like really the type constraints helped so we can sort of recover from errors in the token based alignment.",
                    "label": 0
                },
                {
                    "sent": "Do you have examples where it's the other way around where the token sort of helps you recover from the type?",
                    "label": 0
                },
                {
                    "sent": "I think, yeah, I think that's not really the token level, but the projected constraints.",
                    "label": 0
                },
                {
                    "sent": "When we create A tag dictionary from them that gives and we take the union of that with dictionary, that gives us an expanded dictionary, but it does so that can fix this error when you incorrectly pruned type of mistake.",
                    "label": 0
                },
                {
                    "sent": "But since we use the tight constraints as like the primary filter, we only consider to consumers that are consistent with those.",
                    "label": 0
                },
                {
                    "sent": "We can't use the direct talking constraints to fix the.",
                    "label": 0
                },
                {
                    "sent": "In this dictionary based approach, how well does this work for highly inflected languages?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "It will work worse, definitely.",
                    "label": 0
                },
                {
                    "sent": "I mean so in this case with project from English and in result we see that for example Turkish, you get substantially lower sales compared to the in European languages.",
                    "label": 0
                },
                {
                    "sent": "I think it's around 70% something, but you still see an improvement from combining these two types of constraints.",
                    "label": 0
                },
                {
                    "sent": "Don't know about Japanese and Chinese, but you also see significant improvement for those languages.",
                    "label": 0
                },
                {
                    "sent": "But yeah, it's further from English.",
                    "label": 0
                },
                {
                    "sent": "You go the worse it gets.",
                    "label": 0
                },
                {
                    "sent": "Thanks for the talk, I just wondering how to you optimize the new object function menu apps on constraints.",
                    "label": 0
                },
                {
                    "sent": "How do you optimize the new objective function you propose over optimize it?",
                    "label": 0
                },
                {
                    "sent": "Yeah, Oh yeah.",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 0
                },
                {
                    "sent": "Basically, in terms of using gradient based optimization, So what you get is basically.",
                    "label": 0
                },
                {
                    "sent": "You move away from feature expectations according to the model in.",
                    "label": 0
                },
                {
                    "sent": "This holds inference pace towards feature expectations in the model in this.",
                    "label": 0
                },
                {
                    "sent": "Partial observation space.",
                    "label": 0
                },
                {
                    "sent": "Yeah, maybe we can take it offline to discuss it, but it's just some overall the possible paths in observation that is, rather than having a single path in the changes that you made to Wiktionary with a percentage of them were closed class changes versus open class words, and what would the modification, what percentage were adjectives versus nouns in that type of thing?",
                    "label": 0
                },
                {
                    "sent": "Oh, I'm not sure there is some results.",
                    "label": 0
                },
                {
                    "sent": "We have some visualizations in the paper where we look at pruning errors.",
                    "label": 0
                },
                {
                    "sent": "Per tag and that varies across languages quite substantially.",
                    "label": 0
                },
                {
                    "sent": "Some tax in some languages see a lot of errors, but I think that might be because of partial.",
                    "label": 0
                },
                {
                    "sent": "It might be because from the conversion from fine grained to coarse grained tags because there is also this X tag which contains the tags that couldn't be mapped to another category, so there might be different there.",
                    "label": 0
                },
                {
                    "sent": "But we didn't look specifically at open and closed.",
                    "label": 0
                },
                {
                    "sent": "That's where you could imagine having separate filtering steps for open class versus closed last words or handled differently in some way, but we didn't do that in this world.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Hi, I'm long from everything um.",
                    "label": 0
                },
                {
                    "sent": "Do you consider like menu to one mapping?",
                    "label": 0
                },
                {
                    "sent": "Many to many mapping when you project that app from so language to target language and the single question, did you consider using different language other than English?",
                    "label": 0
                },
                {
                    "sent": "So what's the second question?",
                    "label": 0
                },
                {
                    "sent": "If we projected from another language language?",
                    "label": 0
                },
                {
                    "sent": "So no, we didn't consider any other language than English because we have most parallel data from English.",
                    "label": 0
                },
                {
                    "sent": "But you could.",
                    "label": 0
                },
                {
                    "sent": "It's not necessarily tide to English, but if you have so whatever parallel data you have, you can use it.",
                    "label": 0
                },
                {
                    "sent": "Anne was the first.",
                    "label": 0
                },
                {
                    "sent": "The first question is when you project.",
                    "label": 0
                },
                {
                    "sent": "It seems that you only exploit the one to one mapping, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, we only have about many to one in before, so bidirectional mapping, so this also makes the token constraints much sparser than they could have been if we consider many to one mappings.",
                    "label": 0
                },
                {
                    "sent": "In particular, we see problems with compound words, for example.",
                    "label": 0
                },
                {
                    "sent": "And so I think.",
                    "label": 0
                },
                {
                    "sent": "DEP after the coverage of the token constraints is about 50%, so 50% of the tokens have a projected tag, 50% have don't have created tag in terms of type constraints.",
                    "label": 0
                },
                {
                    "sent": "On average.",
                    "label": 0
                },
                {
                    "sent": "I think it's about 6070% rewards types that are covered in some way, but at which an airy.",
                    "label": 0
                },
                {
                    "sent": "So by combining these two constraints, you kind of.",
                    "label": 0
                },
                {
                    "sent": "Compressed air, thank you very much.",
                    "label": 0
                },
                {
                    "sent": "OK, well let's thank our speaker again.",
                    "label": 0
                }
            ]
        }
    }
}