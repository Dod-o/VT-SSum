{
    "id": "a3ljlyiy4bbo22kvajicaozjg2h2u5iq",
    "title": "Event-Enhanced Learning for Knowledge Graph Completion",
    "info": {
        "author": [
            "Martin Ringsquandl, Ludwig-Maximilians Universit\u00e4t"
        ],
        "published": "July 10, 2018",
        "recorded": "June 2018",
        "category": [
            "Top->Computer Science->Big Data",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/eswc2018_ringsquandl_graph_completion/",
    "segmentation": [
        [
            "OK, hello and welcome to my talk about events, enhance learning for knowledge graph completion.",
            "I was hoping for a bigger screen so you might not be able to see or everything back in there in the back row.",
            "It's another talk from Siemens.",
            "I did this work in collaboration with Siemens when I did my PhD at LMU University and this is actually a research talk, but we have in UC kind of flavor.",
            "I have to speak up.",
            "Apparently the mic is not working.",
            "It doesn't do anything, it's just for the camera.",
            "OK."
        ],
        [
            "So I will introduce the special kind of knowledge graphs that we will be that we are using in this case study.",
            "That also motivates the approach and then we will go into the more into the machine learning models."
        ],
        [
            "So to start off the Knowledge Graph that we look at are essentially.",
            "Conceptual models of manufacturing systems or this is the use case that I will be talking about and we call them this is not working.",
            "You can see it on the right inside here or from your perspective.",
            "The left hand side.",
            "This is the digital twin model is just a conceptual model of the manufacturing system.",
            "Here you can see a car seat production that is manufacturing Cara seeds that have based parts and for example the headrests they are put into assembly processes and we.",
            "We also depict the production equipment as part as entities in our Knowledge graph.",
            "And the the special thing about these."
        ],
        [
            "If we model these kind of knowledge graphs, is that we also observations coming from the physical systems, and these observations are most importantly events time serious.",
            "So for example here the robot entity might emit a set of events that tell about an axis crash, that the robot that grabs the the headrest of the car seat had a crash event that happened after some energy anomalies."
        ],
        [
            "So what is the use case here?"
        ],
        [
            "Imagine that we have this kind of knowledge craft.",
            "They remodeled the assembly line.",
            "There's a process here that there's the preparation of the car seeds.",
            "This process is followed by a finishing process.",
            "It also talks about how the equipments are connected.",
            "So the locking robot of the headrest is connected to the finishing station and now we have an engineer.",
            "The production engineer comes and two things.",
            "OK, let's make this process more efficient.",
            "What do I do?",
            "I introduce a new process entity and I introduce a new robot to these this assembly line and he uses his tooling to modify this within the automation engineering tools.",
            "He inputs this information, but unfortunately he also has to model other kinds of links or relations in the knowledge graph, such as OK, the new introduced assembly robot is also part of the assembly line, and the new assembly process is now is followed by the finishing process.",
            "Why does he have to input also this kind of links?",
            "So the red dash links here on the left hand side are missing.",
            "Essentially, if he doesn't put them in.",
            "And he wonders, can we not infer these links automatically by using some?",
            "So we could use rules like domain knowledge for example, that always when some equipments are connected then the process is also followed by each other.",
            "There could be a domain rule, but we can also imagine that we can sort of learn this automatically."
        ],
        [
            "And how this is typically done is nowadays.",
            "So called yeah, we call it statistical relational learning or also a knowledge graph representation, learning whatever community you're coming from."
        ],
        [
            "And I like to introduce these approaches by thinking about the Knowledge Graph as a tensor.",
            "In the first case or the tensor that you see here on the left hand side is just a 3 dimensional area, and we say that we 2 dimensions have the have the entities, one dimension is denoting the number of relations they are in.",
            "And so one entry in this 10.",
            "So let's say for the identity and the Jason Entity in the R in the case relation.",
            "This can be either one or zero is 1.",
            "If this triple or fact is true in our knowledge graph and it's 0 otherwise.",
            "What this notion directly brings is that we can now interpret each of these triples as a binary or Bernoulli random variable in a statistical sense.",
            "And this is modeled with in this.",
            "So each triple is distributed with the Bernoulli distribution and a parameter P Ain the probability P is modeled as some, or is modeled proportionately to some kind of scoring function over latent features of entities and relations.",
            "So we denote here P is proportional to F over latent features of EIRK and EJ.",
            "And then in the representation, learning sense, so latent features means we learn representations is that we optimize F by learning these latent features.",
            "By trying to learn these latent features."
        ],
        [
            "And just to give you an example, the very simple model is that Renzi model.",
            "I think you've heard it several times.",
            "If you have been in the deep Learning Workshop yesterday, so each entity here is model is a D dimensional vector.",
            "Each relation is also modeled as a D dimensional vector and the transition model just enforces that when we do the vector addition from the from the left entity with the relation, we should end up close to the right entity."
        ],
        [
            "And we can also visualize this in a sort of in a 2 dimensional space.",
            "Imagine that we have a bunch of person entities randomly distributed in this 2 dimensional space."
        ],
        [
            "And we have also a bunch of country entities randomly distributed in this space.",
            "Then what the transition model tries to learn or do is basically a clustering from one point cloud to the other.",
            "So the born in relation would sort of enforce the person entities to form nationality.",
            "Clusters of German persons, Greek persons and so on."
        ],
        [
            "What's an important part of learning these representations is that they can also be enhanced with different background knowledge.",
            "The most importantly text enhanced models have been used.",
            "There are several approaches that take, for example, the Wikipedia entries.",
            "Of the entities in the Knowledge Graph, they scan these, they learn representations and they merge these together.",
            "This specially is helpful when you have sparse, sparsely connected entities you don't know much about them in the Knowledge Graph, but you have a Wikipedia entry that helps."
        ],
        [
            "There are also rules have been incorporated, for example, transitivity.",
            "So we want.",
            "Also.",
            "The embeddings should also follow or try to incorporate these rules.",
            "And also type constraints have been considered.",
            "For example, we know that the domain of the born in relation is person, so we can enforce this in our in our embedding or latent feature representation learning algorithm and what we have in this case are we talk about events and events are a special kind of background knowledge and they are sort of fundamentally different from all the others."
        ],
        [
            "So what's the spec?"
        ],
        [
            "The part about events in this case so.",
            "In our in our knowledge graphs, the events are represented as they have a unique representation.",
            "So for example, the low energy event is represented is represented as one entity in the Knowledge Graph without a timestamp.",
            "It's a unique entity and it has connections to its sources.",
            "So for example it has sourced some kind of robot and it effects some kind of process and it also has some typing or grouping information.",
            "It belongs to the Group of energy events for example.",
            "But on the other hand, we also have the ordered.",
            "We have ordered sets of these event tokens and their event entities can be contained multiple times or they can occur multiple times.",
            "And the exact order here is important, and the other caveat is that the events are only affecting a small part of the Knowledge Graph.",
            "So in comparison to text enhanced models where they assume that each entity has actually a text attached to it, in our case we have only exactly 1 type of entities that have background information.",
            "These are the event entities.",
            "And but we will still want to try to learn a combined vector representation, and I will introduce this later on.",
            "But we also another pitfall of this is if we model events like this then we can we also have missing links for event entities.",
            "So we want also want to recover if the source of an event is missing.",
            "So we also want to recover these missing links."
        ],
        [
            "And now through our approach."
        ],
        [
            "How do we actually do this?",
            "This is the first very first model that we come up with, so let's say.",
            "We have now these two notions of the event entities.",
            "They are contained in the Knowledge Graph and they are contained in these event sequences.",
            "What do we do?",
            "OK, why can we not just jointly learn these representations, right?",
            "So here on the left hand side you see the LS is the objective function for event for the event sequences and on the right inside LK is the objective function for the knowledge graph triples and what we do on the left hand side we just take the ordered set of tokens from the event stream.",
            "We get their representations.",
            "We look up the vector representations, then we feed them into a.",
            "In this case a negative sampled binary classification.",
            "So we try to learn to distinguish the actual center event of given its context, like predecessors and followers by a negative sampled out of context event.",
            "And on the right hand side we feed.",
            "We simultaneously feed triples from the knowledge graph into the sort of standard representation learning framework.",
            "This is here.",
            "We use the objective function like Max margin, objective function actually.",
            "And the F here is generic enough to be.",
            "You can plug in essentially any of the representation learning models such as transier or factorization based ones.",
            "And we also trained them by a negative sampling.",
            "So.",
            "Yeah, and another thing is hyperparameter tuning, obviously, so you have to do.",
            "We do grid search based on the dimensionality of the vectors, the window sizes of the event, Windows, the learning rate, and so on.",
            "And we train this for maximum epoques of 100 with early stopping and Bernoulli based negative sampling."
        ],
        [
            "But maybe there's a better way to do this, so the first idea was just we use a shared representation of event entities in the sequences and in the Knowledge Graph.",
            "This might not be the best thing to do, so we thought about more advanced architecture.",
            "That sort of learns a combination weighted combination of these two.",
            "We still have the joint objective function.",
            "LKL joined this LK plus some weighting factor of LS, but now if we treat the representations differently, so on the left hand side, still we take the tokens from the event stream, we look up their vector representations.",
            "And then we feed them into the negative classified binary classification with the negative event.",
            "But on the other hand, we also for the triple based one we have a different representation for each entity, so a different set of parameters.",
            "And then we try to average the event entity parameters and the of the sequence embeddings and event entity parameters of the triple or the knowledge graph embeddings.",
            "And we do this by using the Hadamard product with relation specific waiting.",
            "So in this case the left hand side we would take E12 and E-11 and we would wait them by two weighting vectors of a relation specific weighting vectors AR&BR.",
            "And these weighting vectors are also trained and by stochastic gradient descent and optimize."
        ],
        [
            "So this would be rather trivial if the event context could be.",
            "Easily embedded or we could use word embeddings like word two VEC directly on the events.",
            "But this is not the case as we will see in the experiment.",
            "So we had to think of different context models on how to actually learn the representations of the events.",
            "And I will show you two of them on the left hand side is the ekl full model that uses the full context of the event streams.",
            "So we say in a sliding window fashion we take the middle event.",
            "Look up its representation.",
            "And then take from the context, we just concatenate all its predecessors and its followers into a another vector and we try to distinguish this with this context center event from a negative sampling event.",
            "In the other case, on the right hand side we have a model that's called ekl cause, and here we just look about.",
            "We look at the predecessors of this of the event entity that we want to predict.",
            "So this is sort of only trying to use all the causes that led up to a certain event.",
            "We concatenate these two and score and still try to predict the sent or the actually causing event from a negative sample event."
        ],
        [
            "And some other notable mention.",
            "So we also tried recurrent neural networks here like LST, EMS sequence encoders and the Skip Gram Co occurrence models.",
            "But they didn't work as well and they try to overfit in some cases."
        ],
        [
            "So let's look at the results.",
            "We evaluated this approach on two datasets, so the first one was a manufacturing data set that's taken from a Siemens factory.",
            "We modeled this as Knowledge Graph that has about 7000 triples.",
            "And 10 relations.",
            "And the second data set is the traffic data set.",
            "That's sort of a little bigger knowledge graph.",
            "This is.",
            "This models a set of traffic events that are happening inside a city over a certain span of time.",
            "And also the traffic knowledge graph is bigger, it has less relation, so we only have five types of relations in this one.",
            "And what we can see in the evaluation is that for the manufacturing data set, Interestingly, the shared architecture performed best in terms of mean rank and hits top 10 and hits top three.",
            "So here essentially the ekl cause model had the lowest mean rank overall.",
            "And also, Interestingly, the Ekl autoencoder model had the best position, or like hits the hits one, so the best hit was classified best in the ekl autoencoder model and for the traffic data set is was actually the other way around.",
            "So in the traffic data set we always found that using the combined architecture works better than the shared one, and this might be due to several things.",
            "So first of all.",
            "The number of relations is less, so you can also see that trancy performs really, really worse on this traffic data set because it has only five relations, and transit typically works only if you have lots of them.",
            "And we also see that the tiki model, which is one of the text enhanced embedding models, is outperformed by our approaches in all of these settings we have done some other experiments.",
            "You can look them up in our big data paper last year."
        ],
        [
            "Another thing I want to show is also the scenario of zero shot learning.",
            "So if we take event entities out of the Knowledge graph, we remove essentially all of the links from the event entities.",
            "And we still want to predict where they end up in the Knowledge Graph or we want to predict these missing links that we took out of the event entities.",
            "And for this we conducted different experiments by removing a certain proportion of these event entities from the Knowledge Graph.",
            "So we have 10 percent, 30% and 50% scenarios where we removed these proportions from the Knowledge Graph.",
            "So this is the proportion of 0 short event entities.",
            "And on the X axis you can see the which proportion of the events is contained in the sequence that we observe.",
            "So 100% means that each event entity has been at least observed once in our sequence data set.",
            "And as we move along the X axis, you can see that we also outperform the TK model consistently, which is the black line here and the in this case the Ekl full model performs best in most of these zero shot learning cases."
        ],
        [
            "So too."
        ],
        [
            "Come to my conclusion.",
            "We saw the use case of synchronizing digital Twins as part of the manufacturing systems and we can model inference of missing links as knowledge graph completion.",
            "We saw that events as a background knowledge and or enhancement need a specialized treatment and these event embeddings can help to improve link prediction for some parts of the Knowledge Graph.",
            "Also for Future Challenge we see that you might want to detect and introduce these missing entities if they are not modeled.",
            "How can we see if actually an entity is missing in the Knowledge Graph?",
            "But this might be a hard nut to crack.",
            "And we have implemented all of these models on guitar.",
            "Please check them out.",
            "There's translation based ones.",
            "We have factorization based ones.",
            "We use auto encoders and these concatenation models.",
            "Yeah.",
            "And might also have unhappy engineer at the end, so.",
            "OK, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, hello and welcome to my talk about events, enhance learning for knowledge graph completion.",
                    "label": 1
                },
                {
                    "sent": "I was hoping for a bigger screen so you might not be able to see or everything back in there in the back row.",
                    "label": 0
                },
                {
                    "sent": "It's another talk from Siemens.",
                    "label": 0
                },
                {
                    "sent": "I did this work in collaboration with Siemens when I did my PhD at LMU University and this is actually a research talk, but we have in UC kind of flavor.",
                    "label": 0
                },
                {
                    "sent": "I have to speak up.",
                    "label": 0
                },
                {
                    "sent": "Apparently the mic is not working.",
                    "label": 0
                },
                {
                    "sent": "It doesn't do anything, it's just for the camera.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I will introduce the special kind of knowledge graphs that we will be that we are using in this case study.",
                    "label": 0
                },
                {
                    "sent": "That also motivates the approach and then we will go into the more into the machine learning models.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to start off the Knowledge Graph that we look at are essentially.",
                    "label": 1
                },
                {
                    "sent": "Conceptual models of manufacturing systems or this is the use case that I will be talking about and we call them this is not working.",
                    "label": 1
                },
                {
                    "sent": "You can see it on the right inside here or from your perspective.",
                    "label": 0
                },
                {
                    "sent": "The left hand side.",
                    "label": 0
                },
                {
                    "sent": "This is the digital twin model is just a conceptual model of the manufacturing system.",
                    "label": 0
                },
                {
                    "sent": "Here you can see a car seat production that is manufacturing Cara seeds that have based parts and for example the headrests they are put into assembly processes and we.",
                    "label": 0
                },
                {
                    "sent": "We also depict the production equipment as part as entities in our Knowledge graph.",
                    "label": 0
                },
                {
                    "sent": "And the the special thing about these.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we model these kind of knowledge graphs, is that we also observations coming from the physical systems, and these observations are most importantly events time serious.",
                    "label": 0
                },
                {
                    "sent": "So for example here the robot entity might emit a set of events that tell about an axis crash, that the robot that grabs the the headrest of the car seat had a crash event that happened after some energy anomalies.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what is the use case here?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Imagine that we have this kind of knowledge craft.",
                    "label": 0
                },
                {
                    "sent": "They remodeled the assembly line.",
                    "label": 0
                },
                {
                    "sent": "There's a process here that there's the preparation of the car seeds.",
                    "label": 0
                },
                {
                    "sent": "This process is followed by a finishing process.",
                    "label": 0
                },
                {
                    "sent": "It also talks about how the equipments are connected.",
                    "label": 0
                },
                {
                    "sent": "So the locking robot of the headrest is connected to the finishing station and now we have an engineer.",
                    "label": 0
                },
                {
                    "sent": "The production engineer comes and two things.",
                    "label": 0
                },
                {
                    "sent": "OK, let's make this process more efficient.",
                    "label": 0
                },
                {
                    "sent": "What do I do?",
                    "label": 0
                },
                {
                    "sent": "I introduce a new process entity and I introduce a new robot to these this assembly line and he uses his tooling to modify this within the automation engineering tools.",
                    "label": 0
                },
                {
                    "sent": "He inputs this information, but unfortunately he also has to model other kinds of links or relations in the knowledge graph, such as OK, the new introduced assembly robot is also part of the assembly line, and the new assembly process is now is followed by the finishing process.",
                    "label": 0
                },
                {
                    "sent": "Why does he have to input also this kind of links?",
                    "label": 0
                },
                {
                    "sent": "So the red dash links here on the left hand side are missing.",
                    "label": 0
                },
                {
                    "sent": "Essentially, if he doesn't put them in.",
                    "label": 0
                },
                {
                    "sent": "And he wonders, can we not infer these links automatically by using some?",
                    "label": 0
                },
                {
                    "sent": "So we could use rules like domain knowledge for example, that always when some equipments are connected then the process is also followed by each other.",
                    "label": 0
                },
                {
                    "sent": "There could be a domain rule, but we can also imagine that we can sort of learn this automatically.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And how this is typically done is nowadays.",
                    "label": 0
                },
                {
                    "sent": "So called yeah, we call it statistical relational learning or also a knowledge graph representation, learning whatever community you're coming from.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I like to introduce these approaches by thinking about the Knowledge Graph as a tensor.",
                    "label": 0
                },
                {
                    "sent": "In the first case or the tensor that you see here on the left hand side is just a 3 dimensional area, and we say that we 2 dimensions have the have the entities, one dimension is denoting the number of relations they are in.",
                    "label": 0
                },
                {
                    "sent": "And so one entry in this 10.",
                    "label": 0
                },
                {
                    "sent": "So let's say for the identity and the Jason Entity in the R in the case relation.",
                    "label": 0
                },
                {
                    "sent": "This can be either one or zero is 1.",
                    "label": 0
                },
                {
                    "sent": "If this triple or fact is true in our knowledge graph and it's 0 otherwise.",
                    "label": 0
                },
                {
                    "sent": "What this notion directly brings is that we can now interpret each of these triples as a binary or Bernoulli random variable in a statistical sense.",
                    "label": 0
                },
                {
                    "sent": "And this is modeled with in this.",
                    "label": 0
                },
                {
                    "sent": "So each triple is distributed with the Bernoulli distribution and a parameter P Ain the probability P is modeled as some, or is modeled proportionately to some kind of scoring function over latent features of entities and relations.",
                    "label": 0
                },
                {
                    "sent": "So we denote here P is proportional to F over latent features of EIRK and EJ.",
                    "label": 0
                },
                {
                    "sent": "And then in the representation, learning sense, so latent features means we learn representations is that we optimize F by learning these latent features.",
                    "label": 0
                },
                {
                    "sent": "By trying to learn these latent features.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And just to give you an example, the very simple model is that Renzi model.",
                    "label": 0
                },
                {
                    "sent": "I think you've heard it several times.",
                    "label": 0
                },
                {
                    "sent": "If you have been in the deep Learning Workshop yesterday, so each entity here is model is a D dimensional vector.",
                    "label": 0
                },
                {
                    "sent": "Each relation is also modeled as a D dimensional vector and the transition model just enforces that when we do the vector addition from the from the left entity with the relation, we should end up close to the right entity.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we can also visualize this in a sort of in a 2 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "Imagine that we have a bunch of person entities randomly distributed in this 2 dimensional space.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we have also a bunch of country entities randomly distributed in this space.",
                    "label": 0
                },
                {
                    "sent": "Then what the transition model tries to learn or do is basically a clustering from one point cloud to the other.",
                    "label": 0
                },
                {
                    "sent": "So the born in relation would sort of enforce the person entities to form nationality.",
                    "label": 0
                },
                {
                    "sent": "Clusters of German persons, Greek persons and so on.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What's an important part of learning these representations is that they can also be enhanced with different background knowledge.",
                    "label": 0
                },
                {
                    "sent": "The most importantly text enhanced models have been used.",
                    "label": 0
                },
                {
                    "sent": "There are several approaches that take, for example, the Wikipedia entries.",
                    "label": 0
                },
                {
                    "sent": "Of the entities in the Knowledge Graph, they scan these, they learn representations and they merge these together.",
                    "label": 0
                },
                {
                    "sent": "This specially is helpful when you have sparse, sparsely connected entities you don't know much about them in the Knowledge Graph, but you have a Wikipedia entry that helps.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are also rules have been incorporated, for example, transitivity.",
                    "label": 0
                },
                {
                    "sent": "So we want.",
                    "label": 0
                },
                {
                    "sent": "Also.",
                    "label": 0
                },
                {
                    "sent": "The embeddings should also follow or try to incorporate these rules.",
                    "label": 0
                },
                {
                    "sent": "And also type constraints have been considered.",
                    "label": 0
                },
                {
                    "sent": "For example, we know that the domain of the born in relation is person, so we can enforce this in our in our embedding or latent feature representation learning algorithm and what we have in this case are we talk about events and events are a special kind of background knowledge and they are sort of fundamentally different from all the others.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what's the spec?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The part about events in this case so.",
                    "label": 0
                },
                {
                    "sent": "In our in our knowledge graphs, the events are represented as they have a unique representation.",
                    "label": 0
                },
                {
                    "sent": "So for example, the low energy event is represented is represented as one entity in the Knowledge Graph without a timestamp.",
                    "label": 0
                },
                {
                    "sent": "It's a unique entity and it has connections to its sources.",
                    "label": 0
                },
                {
                    "sent": "So for example it has sourced some kind of robot and it effects some kind of process and it also has some typing or grouping information.",
                    "label": 0
                },
                {
                    "sent": "It belongs to the Group of energy events for example.",
                    "label": 0
                },
                {
                    "sent": "But on the other hand, we also have the ordered.",
                    "label": 0
                },
                {
                    "sent": "We have ordered sets of these event tokens and their event entities can be contained multiple times or they can occur multiple times.",
                    "label": 0
                },
                {
                    "sent": "And the exact order here is important, and the other caveat is that the events are only affecting a small part of the Knowledge Graph.",
                    "label": 0
                },
                {
                    "sent": "So in comparison to text enhanced models where they assume that each entity has actually a text attached to it, in our case we have only exactly 1 type of entities that have background information.",
                    "label": 0
                },
                {
                    "sent": "These are the event entities.",
                    "label": 0
                },
                {
                    "sent": "And but we will still want to try to learn a combined vector representation, and I will introduce this later on.",
                    "label": 0
                },
                {
                    "sent": "But we also another pitfall of this is if we model events like this then we can we also have missing links for event entities.",
                    "label": 0
                },
                {
                    "sent": "So we want also want to recover if the source of an event is missing.",
                    "label": 0
                },
                {
                    "sent": "So we also want to recover these missing links.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now through our approach.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How do we actually do this?",
                    "label": 0
                },
                {
                    "sent": "This is the first very first model that we come up with, so let's say.",
                    "label": 0
                },
                {
                    "sent": "We have now these two notions of the event entities.",
                    "label": 0
                },
                {
                    "sent": "They are contained in the Knowledge Graph and they are contained in these event sequences.",
                    "label": 0
                },
                {
                    "sent": "What do we do?",
                    "label": 0
                },
                {
                    "sent": "OK, why can we not just jointly learn these representations, right?",
                    "label": 0
                },
                {
                    "sent": "So here on the left hand side you see the LS is the objective function for event for the event sequences and on the right inside LK is the objective function for the knowledge graph triples and what we do on the left hand side we just take the ordered set of tokens from the event stream.",
                    "label": 0
                },
                {
                    "sent": "We get their representations.",
                    "label": 0
                },
                {
                    "sent": "We look up the vector representations, then we feed them into a.",
                    "label": 0
                },
                {
                    "sent": "In this case a negative sampled binary classification.",
                    "label": 0
                },
                {
                    "sent": "So we try to learn to distinguish the actual center event of given its context, like predecessors and followers by a negative sampled out of context event.",
                    "label": 0
                },
                {
                    "sent": "And on the right hand side we feed.",
                    "label": 0
                },
                {
                    "sent": "We simultaneously feed triples from the knowledge graph into the sort of standard representation learning framework.",
                    "label": 0
                },
                {
                    "sent": "This is here.",
                    "label": 0
                },
                {
                    "sent": "We use the objective function like Max margin, objective function actually.",
                    "label": 0
                },
                {
                    "sent": "And the F here is generic enough to be.",
                    "label": 0
                },
                {
                    "sent": "You can plug in essentially any of the representation learning models such as transier or factorization based ones.",
                    "label": 0
                },
                {
                    "sent": "And we also trained them by a negative sampling.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and another thing is hyperparameter tuning, obviously, so you have to do.",
                    "label": 0
                },
                {
                    "sent": "We do grid search based on the dimensionality of the vectors, the window sizes of the event, Windows, the learning rate, and so on.",
                    "label": 0
                },
                {
                    "sent": "And we train this for maximum epoques of 100 with early stopping and Bernoulli based negative sampling.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But maybe there's a better way to do this, so the first idea was just we use a shared representation of event entities in the sequences and in the Knowledge Graph.",
                    "label": 0
                },
                {
                    "sent": "This might not be the best thing to do, so we thought about more advanced architecture.",
                    "label": 0
                },
                {
                    "sent": "That sort of learns a combination weighted combination of these two.",
                    "label": 0
                },
                {
                    "sent": "We still have the joint objective function.",
                    "label": 0
                },
                {
                    "sent": "LKL joined this LK plus some weighting factor of LS, but now if we treat the representations differently, so on the left hand side, still we take the tokens from the event stream, we look up their vector representations.",
                    "label": 0
                },
                {
                    "sent": "And then we feed them into the negative classified binary classification with the negative event.",
                    "label": 0
                },
                {
                    "sent": "But on the other hand, we also for the triple based one we have a different representation for each entity, so a different set of parameters.",
                    "label": 0
                },
                {
                    "sent": "And then we try to average the event entity parameters and the of the sequence embeddings and event entity parameters of the triple or the knowledge graph embeddings.",
                    "label": 0
                },
                {
                    "sent": "And we do this by using the Hadamard product with relation specific waiting.",
                    "label": 0
                },
                {
                    "sent": "So in this case the left hand side we would take E12 and E-11 and we would wait them by two weighting vectors of a relation specific weighting vectors AR&BR.",
                    "label": 0
                },
                {
                    "sent": "And these weighting vectors are also trained and by stochastic gradient descent and optimize.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this would be rather trivial if the event context could be.",
                    "label": 0
                },
                {
                    "sent": "Easily embedded or we could use word embeddings like word two VEC directly on the events.",
                    "label": 0
                },
                {
                    "sent": "But this is not the case as we will see in the experiment.",
                    "label": 0
                },
                {
                    "sent": "So we had to think of different context models on how to actually learn the representations of the events.",
                    "label": 0
                },
                {
                    "sent": "And I will show you two of them on the left hand side is the ekl full model that uses the full context of the event streams.",
                    "label": 0
                },
                {
                    "sent": "So we say in a sliding window fashion we take the middle event.",
                    "label": 0
                },
                {
                    "sent": "Look up its representation.",
                    "label": 0
                },
                {
                    "sent": "And then take from the context, we just concatenate all its predecessors and its followers into a another vector and we try to distinguish this with this context center event from a negative sampling event.",
                    "label": 0
                },
                {
                    "sent": "In the other case, on the right hand side we have a model that's called ekl cause, and here we just look about.",
                    "label": 0
                },
                {
                    "sent": "We look at the predecessors of this of the event entity that we want to predict.",
                    "label": 0
                },
                {
                    "sent": "So this is sort of only trying to use all the causes that led up to a certain event.",
                    "label": 0
                },
                {
                    "sent": "We concatenate these two and score and still try to predict the sent or the actually causing event from a negative sample event.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And some other notable mention.",
                    "label": 0
                },
                {
                    "sent": "So we also tried recurrent neural networks here like LST, EMS sequence encoders and the Skip Gram Co occurrence models.",
                    "label": 0
                },
                {
                    "sent": "But they didn't work as well and they try to overfit in some cases.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's look at the results.",
                    "label": 0
                },
                {
                    "sent": "We evaluated this approach on two datasets, so the first one was a manufacturing data set that's taken from a Siemens factory.",
                    "label": 0
                },
                {
                    "sent": "We modeled this as Knowledge Graph that has about 7000 triples.",
                    "label": 0
                },
                {
                    "sent": "And 10 relations.",
                    "label": 0
                },
                {
                    "sent": "And the second data set is the traffic data set.",
                    "label": 0
                },
                {
                    "sent": "That's sort of a little bigger knowledge graph.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "This models a set of traffic events that are happening inside a city over a certain span of time.",
                    "label": 0
                },
                {
                    "sent": "And also the traffic knowledge graph is bigger, it has less relation, so we only have five types of relations in this one.",
                    "label": 0
                },
                {
                    "sent": "And what we can see in the evaluation is that for the manufacturing data set, Interestingly, the shared architecture performed best in terms of mean rank and hits top 10 and hits top three.",
                    "label": 0
                },
                {
                    "sent": "So here essentially the ekl cause model had the lowest mean rank overall.",
                    "label": 0
                },
                {
                    "sent": "And also, Interestingly, the Ekl autoencoder model had the best position, or like hits the hits one, so the best hit was classified best in the ekl autoencoder model and for the traffic data set is was actually the other way around.",
                    "label": 0
                },
                {
                    "sent": "So in the traffic data set we always found that using the combined architecture works better than the shared one, and this might be due to several things.",
                    "label": 0
                },
                {
                    "sent": "So first of all.",
                    "label": 0
                },
                {
                    "sent": "The number of relations is less, so you can also see that trancy performs really, really worse on this traffic data set because it has only five relations, and transit typically works only if you have lots of them.",
                    "label": 0
                },
                {
                    "sent": "And we also see that the tiki model, which is one of the text enhanced embedding models, is outperformed by our approaches in all of these settings we have done some other experiments.",
                    "label": 0
                },
                {
                    "sent": "You can look them up in our big data paper last year.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another thing I want to show is also the scenario of zero shot learning.",
                    "label": 0
                },
                {
                    "sent": "So if we take event entities out of the Knowledge graph, we remove essentially all of the links from the event entities.",
                    "label": 0
                },
                {
                    "sent": "And we still want to predict where they end up in the Knowledge Graph or we want to predict these missing links that we took out of the event entities.",
                    "label": 0
                },
                {
                    "sent": "And for this we conducted different experiments by removing a certain proportion of these event entities from the Knowledge Graph.",
                    "label": 0
                },
                {
                    "sent": "So we have 10 percent, 30% and 50% scenarios where we removed these proportions from the Knowledge Graph.",
                    "label": 0
                },
                {
                    "sent": "So this is the proportion of 0 short event entities.",
                    "label": 0
                },
                {
                    "sent": "And on the X axis you can see the which proportion of the events is contained in the sequence that we observe.",
                    "label": 0
                },
                {
                    "sent": "So 100% means that each event entity has been at least observed once in our sequence data set.",
                    "label": 0
                },
                {
                    "sent": "And as we move along the X axis, you can see that we also outperform the TK model consistently, which is the black line here and the in this case the Ekl full model performs best in most of these zero shot learning cases.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So too.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Come to my conclusion.",
                    "label": 0
                },
                {
                    "sent": "We saw the use case of synchronizing digital Twins as part of the manufacturing systems and we can model inference of missing links as knowledge graph completion.",
                    "label": 0
                },
                {
                    "sent": "We saw that events as a background knowledge and or enhancement need a specialized treatment and these event embeddings can help to improve link prediction for some parts of the Knowledge Graph.",
                    "label": 0
                },
                {
                    "sent": "Also for Future Challenge we see that you might want to detect and introduce these missing entities if they are not modeled.",
                    "label": 0
                },
                {
                    "sent": "How can we see if actually an entity is missing in the Knowledge Graph?",
                    "label": 0
                },
                {
                    "sent": "But this might be a hard nut to crack.",
                    "label": 0
                },
                {
                    "sent": "And we have implemented all of these models on guitar.",
                    "label": 0
                },
                {
                    "sent": "Please check them out.",
                    "label": 0
                },
                {
                    "sent": "There's translation based ones.",
                    "label": 0
                },
                {
                    "sent": "We have factorization based ones.",
                    "label": 0
                },
                {
                    "sent": "We use auto encoders and these concatenation models.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "And might also have unhappy engineer at the end, so.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                }
            ]
        }
    }
}