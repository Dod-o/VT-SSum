{
    "id": "kl3dkypr6a67fuhfgnvgulrkdx3wgxys",
    "title": "Hierarchical sampling for active learning",
    "info": {
        "author": [
            "Daniel Hsu, Microsoft Research New England, Microsoft Research"
        ],
        "published": "July 28, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning"
        ]
    },
    "url": "http://videolectures.net/icml08_hsu_hsa/",
    "segmentation": [
        [
            "Alright, so this talk is on, yeah, hierarchical sampling for active learning.",
            "This is joint work.",
            "My advisor shandra desk."
        ],
        [
            "Alright, so active learning is a problem where it tries to address this dichotomy in the availability of data or different types of data.",
            "One on one hand you have unlabeled data.",
            "This is just like the raw signal that you get from.",
            "You can collect from and usually this is very cheap and plentiful.",
            "For example if you want to get text data, you just mind the web for speech data.",
            "You can just turn on microphone like I have on right now and images can get those on Flickr.",
            "Things that we want to predict.",
            "These labels.",
            "They are usually hard to come by because these are usually.",
            "Usually need some human effort to to get these, so you might have to read an article to figure out the topic of the article.",
            "So the goal in active learning is if you're given a bunch of unlabeled data and access to a human labeler, someone who can actually look at the data and tell you what the label is.",
            "The goal would be to learn, say, like a classifier using as few requests to this human labeler as possible as possible."
        ],
        [
            "So there are sort of two different strategies that people try to employ with an active learning.",
            "One of what we called the efficient search through hypothesis space so.",
            "As you get more and more labels to the set of kinda hypothesis that you might consider sort of shrinks down as which ones the hypothesis that seem likely will shrink, I should look at more and more labels.",
            "So this is sort of been the main effort in active learning, at least in recent years.",
            "This is things like query by committee or region of disagreement.",
            "Turn another perspective on.",
            "Active learning is trying to exploit cluster structure in the data, and So what typically happens with data is that they sort of common.",
            "These little small clusters of data in terms of some sort of distance measure.",
            "And if you could just you know the most optimistic thing you could do is just to look at one point in each cluster askers label and then just assign that label to the entire cluster and that would be.",
            "And then the idea.",
            "Then, if you can, just you have this fully labeled data set and you can run any sort of supervised learning on that at the end.",
            "So there's been some work in this area.",
            "One notable ones is Bayesian method.",
            "You choose regard Ghahramani and laugh."
        ],
        [
            "Pretty.",
            "But that's what this."
        ],
        [
            "Will be focusing on.",
            "So there are a lot of them actively encourage sticks that people have employed in practice here.",
            "Sort of a typical one.",
            "You start with a pool of unlabeled data and then you ask for the label of a few of the points.",
            "And then you have repeating this process.",
            "You train a classifier on the current set of labeled data, and now you want to decide which point the label next.",
            "So we might do is just choose the point that's closest to the decision boundary, and there's a lot of variations on this.",
            "Like you choose the most points, most uncertain about one with the smallest margin according to some sort of linear classifier.",
            "The problem with this is that there's this issue of sampling bias that you quickly run."
        ],
        [
            "Active.",
            "So here's an example of what might happen.",
            "So this is a data set that one dimension along the line an this is the data distribution of the unlabeled part."
        ],
        [
            "So let's say that you draw a few points and you guys for the labels.",
            "So you have a bunch of blue points on the left and red points on the right.",
            "Since you have these two clusters and their majority clusters, those are more likely to be the ones that come up first.",
            "So the first hypothesis that you output is the one that's right in the middle.",
            "That's the most sensible one.",
            "Now the points that are closest to it are just all the points in that small cluster in the middle, so you just."
        ],
        [
            "Ask for more points in there and you know maybe it turns out that all the points in the middle or just come up red and blue with equal probability.",
            "So all you really do is doing is just refining where that separator is in the middle, so you're never going to ask for label queries elsewhere."
        ],
        [
            "The problem with this is that if the label distribution is actually quite different from here from this.",
            "The.",
            "If you had this cluster of red points over there, the left one with 5%.",
            "Optimal hypothesis is quite different from the hypothesis that you eventually return.",
            "So what actually happens is that you never were so confident about this Miss cluster that you never asked for court."
        ],
        [
            "Is there?",
            "And so this is the known issue of sampling bias, because you didn't really get a representative set of labeled points from from your standpoint, and so here you see that the best hypothesis, 2.5% error, and the one that you return is always at least 5% here.",
            "So this is an issue of sampling bias that comes up with active learning, and so the goal of sound active learning strategies is somehow to properly manage this bias."
        ],
        [
            "OK, so this is sort of a consistency type guarantee that you'd like to have with active learning strategies.",
            "If the goal is maybe is goal is that you should never do worse than just randomly picking points to label.",
            "This is just equivalent to what would be a passive supervised learning.",
            "So our general methodology and a lot of methods is to somehow balances random sampling with something a little bit more aggressive, aggressive, selective sampling and you want to balance the bias that gets introduced.",
            "So there are a lot of tricks that.",
            "Can be used to implement this.",
            "Some of the previous works have used things like rejection sampling and confidence interval."
        ],
        [
            "So there are the method we propose is the kind of cluster adaptive sampling, and the goal is system.",
            "As I said before, it just somehow just sample the query for points in the query for labels in the data and somehow just label every data point in the data set and will do this by just assigning the majority label within each cluster to all of his constituents.",
            "So that in the end you have a fully labeled data set and the goal we will show that will have most of the labels will be correct and now you can run any supervised learner on this data set and to get a classifier out."
        ],
        [
            "So here's an example of what a run of this algorithm might like, so you have some initial pull data."
        ],
        [
            "Unlabeled data and so.",
            "Let's say somehow you cluster data somehow by some distance metric, or."
        ],
        [
            "It will ask for a few labels within each cluster, and so they, unless the sort of turned up red and blue and on the right they all."
        ],
        [
            "Turned up red.",
            "So there's sort of a problem that you run into and this is what you seem to be stuck, so alright, that's OK, but on the left you don't know whether to give all the points.",
            "All the points in Blue Label or Red label."
        ],
        [
            "So our records this is somehow to use a hierarchical clustering of the data instead of just a fixed flat clustering.",
            "So on it.",
            "So I've numbered the clusters in this tree.",
            "And so they correspond to the different clusters you see here.",
            "So on the right the cluster two is relatively pure, so we can basically stop sampling from there for the time being, and that focus our efforts of sampling on the 1st cluster."
        ],
        [
            "So indeed we sample some more and then we see that clusters three and four.",
            "Doing these seem relatively pure."
        ],
        [
            "And So what you might do is just select a new different pruning of the tree.",
            "So now you have clusters 2, three and four.",
            "And they all seem relatively pure."
        ],
        [
            "So maybe you could stop there.",
            "The main idea is just to somehow choose a pruning of the tree that consists mostly of just pure nodes.",
            "And like here I mean pure and label the labels the kernel.",
            "So our algorithm.",
            "Will just maintain a pruning of the tree and will opt opportunistically.",
            "Choose which cluster into pruning to sample from.",
            "And then when we choose a once we've chosen, the cluster will rank, draw random point in that cluster to ask for label.",
            "So since we have all these random.",
            "Since we're just randomly sampling from each from the clusters, we can maintain these sort of empirical counts and those will correspond to empirical probabilities.",
            "Will update these statistics.",
            "We can also sign confidence intervals with with all these quantities, and then after we've done these sampling for update, our choice of the printing of the tree.",
            "So what happens is so.",
            "For example, if we chose cluster to the sample from and we get a random leaf, leaf court leave corresponds to data points here.",
            "If it's also containing cluster 13 in Cluster 6, and it's as if we're sort of also sampling from these clusters cluster 1613 so we can update the statistics for the clusters as well."
        ],
        [
            "Here's the algorithm in detail.",
            "The input is just a hierarchical clustering of the data and will initialize with an arbitrary with pruning just consisting of the root node and some labeling of the nodes.",
            "And then we'll just iteratively proceed by selecting a node in the pruning and choosing a random point from the from the cluster to to label.",
            "And then we'll update all of the empirical counts in terms of the label frequencies that we've seen so far.",
            "And then the 4th.",
            "The last step here is to choose a pruning and labeling.",
            "The current node that we sampled from an update the printing accordingly, so I'll show you how to do that in a minute, but.",
            "So after you say we run out of queries or labels then what I said before.",
            "If you just assign the majority label of each node in your pruning to two of his constituents and you got a fully labeled data set."
        ],
        [
            "So the things that need to be specified here are how the hierarchically clustered data, how to choose which note the sample from.",
            "And how to choose a good printing and labeling?"
        ],
        [
            "The first one addresses how you build a hierarchical clustering, so there are a lot of methods.",
            "And really I'm just going to put on this point.",
            "They're always methods that people will come up with an.",
            "The point is that we just want to be able to cluster data well enough so that at some level relatively shallow level of the tree there is some pure pruning.",
            "That exists in that tree, and the goal is of our methods just to discover a printing of comparable quality."
        ],
        [
            "OK. Another detail is how do you actually choose the printing of the tree once you have certain statistics on all the clusters and this turned out just to be a dynamic program.",
            "So we can estimate the error of assigning a particular label to a cluster by 1 minus the empirical fraction of that label within that cluster.",
            "So that's just 1 -- P hat of node B with label L. So this is a rough estimate of what the error would be if you assign that label to the cluster, and then the dynamic program is somehow follows somewhat naturally so.",
            "The point is just you can either assign the label to the node and incur that error, or you can look at the 2 two children of the particular node and see if if you sign different labels to them that you get better pruning.",
            "So the issue here is that you know sometimes your estimates of the fraction label fraction proportions aren't going to be well estimated enough, so you have to take some care and to tell you that.",
            "But relatively simple and you can compute this thing in a single pass through the tree."
        ],
        [
            "And then the main issue here now is how do you actually select a node that sample from?",
            "Rock variation there are possible so, and this is essentially just random sampling 'cause all your doing is splitting the sampling process into 2 steps.",
            "You first pick a node portion of his weight and then you pick a random point within that node.",
            "So that's just random sampling.",
            "The second one I showed here is this sort of more aggressive sampling method and have to choose a node proportional to its weight, but also time the error you expected to incur.",
            "So I showed it showed here by 1 -- P hat sort of over lower bound on that.",
            "So this would be an upper bound on the error that you expect.",
            "So this is a more aggressive sampling strategy where you sort of try to focus your sampling on notes that are you focus your same sampling away from notes that look pure and focus it on knows that seem little lesson here in hopes of discovering a more pure printing of the tree.",
            "The thing is, a general ideas.",
            "You can combine this with a lot of different sampling rules or no selection rules.",
            "They can use these sort of backpack based off style priors where you can combine it with a margin based rule.",
            "So there are a lot of variance possible, will just show that the second one in particular does."
        ],
        [
            "Very well.",
            "So there are some consistency guarantees associated with this method, so if you just look at the random sampling rule, the one where you just choose a node proportional to its weight.",
            "Then if there's a pruning the tree that relatively small printing save the K clusters and encourage relatively low ireda.",
            "An algorithm will discover a comperable pruning of comparable quality of order ETA error after only K over ETA label queries, so this is sort of what you might expect if you just did this with select, supervised or semi supervised learning rate, considered all possible Cape earnings of the tree and you selected the best one.",
            "But here we don't have to do that.",
            "We can just sort of discover it on on the fly.",
            "And with the active sampling rule, we have sort of a fall back guarantee that says that you never do much worse than if you just pick the notes to sample from randomly.",
            "There's never much worse than just random sampling, but you can often do a lot better because you sort of have a more aggressive, no selection rule."
        ],
        [
            "So this method is quite flexible, so there are a lot of immediate extensions that you get out, so one for instance multi class is very very easy to handle, you just have several tracked.",
            "Several different empirical counts.",
            "And then you can use different sort of confidence intervals, say like with the multinomial instead of the binomial pounds.",
            "Is batch mode is really easy extend to all you have to do is repeatedly call this select node procedure and then sample query a point within each of the clusters that it returns.",
            "There's another variant of active learning called rare category detection, and the goal here Sis somehow discover rare classes within the data, and this has been applied to things like fraud detection where you want uncovered new patterns or fraud.",
            "So the active sampling rule?",
            "Well, sort of try to maintain balance.",
            "The coverage of the data for you.",
            "Try to direct the sampling away from pure nodes of the majority class and try to focus your efforts of sampling where you might be more likely to find.",
            "A rare class."
        ],
        [
            "It's a.",
            "We ran some experience with this method and compared it to.",
            "Some other common methods, so we used the active sampling role with our cluster adaptive standpoint and we use logistic regression to train a linear model.",
            "After we've propagated the labels of each cluster.",
            "And we compare this to random sampling.",
            "This is just the same as passive learning.",
            "We also tried it with compared to margin based standpoint.",
            "This is the method I described at the very beginning where you just choose the point.",
            "The sample that you choose, the point closest to the current decision boundary to sample from.",
            "And then we use both logistic regression for random sampling and margin based sampling."
        ],
        [
            "So here, just some of the experimental results.",
            "This is a plot of the test error of the methods.",
            "On the left we used newsgroup data set with text.",
            "This is sort of the bag of feature representation of the text.",
            "There are two.",
            "Lines for the cluster adaptive sampling.",
            "That's because we use different ways of building the hierarchical clustering.",
            "So the first one was when you just represent each document as a TF IDF vector and a second one.",
            "When you use average linkage to build a clustering.",
            "The second one we represented, or you will learn the topic model using light and Air show allocation.",
            "And then we built the tree clustering of that using a variation of average linkage.",
            "So you see that they both.",
            "All the active learning rules do a lot better than random sampling, and both costs, both versions of cluster adaptive sampling are pretty comfortable with margin based sampling.",
            "The margin based sampling has some.",
            "There's some weird issue with it, where is somehow gets a lower error rate with fewer labels.",
            "If you don't look at all of the labels, so that's sort of a side issue about margin based sampling.",
            "On the right we also ran it on a multi class.",
            "Data set with emnace OCR digits.",
            "And you see that the cluster method does really well at the beginning was just very relatively very few late label queries, and it's definitely significantly better than random sampling.",
            "And at the beginning it's a lot better than the margin based sampling method also."
        ],
        [
            "So some of the future work that we want to.",
            "Continue on with this.",
            "Somehow characterize the sample complexity of this method.",
            "Main issues here.",
            "What sort of the optimal sampling rule?",
            "I suggested a few random random sampling role was just sort of baseline and active sampling role, but you can imagine there would be more more aggressive roles that somehow adapted to date a little more quickly.",
            "In a lot of cases them exponential savings seem like they might be possible, so here be interesting to see if that's the case.",
            "Also, in general, the general methodology here some how to use unlike unsupervised learning and combine it with some sort of sampling method.",
            "We'd like to use generalize our method to this to other structures that you might learn with unsupervised learning, and somehow turn that into an active learning rule."
        ],
        [
            "So in summary.",
            "We presented a cluster adaptive sampling method for active learning.",
            "So the main goal here was just to somehow discover viable clusterings in the data if they exist in the hierarchical clustering.",
            "So we manage the sampling bias by combining these confidence intervals.",
            "And also we we we still have sort of a fall back guarantee that it never does much worse than random sampling and see.",
            "It's an experiment that does a lot better than random sampling, and it's been actually competitive with random sampling.",
            "Sorry, competitive with these unsafe heuristics like margin based sampling."
        ],
        [
            "Thanks.",
            "Worried if this clustering assumption doesn't workout.",
            "So say at a particular level industry you start labeling and all and none of the cluster underneath that node.",
            "Look sure at all.",
            "It seems like it be good at that point of time clusters using semi supervised clustering using supervision that you've already gathered at that point and at that point we cluster and go from there.",
            "I'm just concerned it's like just assuming that your hierarchical clustering.",
            "Perfect out of the box is not not using the later super addition to maybe refinement, right?",
            "So yeah, so definitely would be better to just somehow use the labels in building the clustering as well.",
            "So that is 1 approach that you could fall back on if you start seeing you can actually see with this method which nodes start steam pure and you can sort of ignore those for now and then continue later on the notes that seem impure but the goal here is actually just show that this simple rule would be able to discover printing if it did exist in your tree.",
            "But as you said, if it doesn't exist in the tree, then yeah, there definitely should do something better.",
            "Have you thought away?",
            "Define.",
            "Modern day standard for sample paper festival please.",
            "Yeah, so there are some rules that we actually showed some in the paper where you might want to just before you decide whether which node the sample from you would just train a classifier on your current data and look for the clusters that are sort of near the boundary of that cluster of the near their current role and then choose a random point from those clusters.",
            "So that would be another selection rule that would be possible.",
            "No, we haven't tried that yet.",
            "The knowledge from the current hypothesis media president.",
            "I mean if you have many features.",
            "Switch out your relevant.",
            "Daniel.",
            "I'm sorry, no question.",
            "Then you can trust your judgement, right?",
            "I see.",
            "So basically the clustering.",
            "Are you suggesting that the clustering assumption might be too strong?",
            "Right?",
            "So the question is whether you could use the current hypothesis.",
            "Find the.",
            "I see that might be.",
            "Yeah, so you might yeah somehow.",
            "Oh, I guess but.",
            "Right, yeah, so there it could be that you know the distance function that you used to build the clustering was not very good, or it's only good in certain regions of the data.",
            "So in those cases you might think about just choosing a new clustering now based on your labels you've seen so far this thing is there are some sort of this is the call complications that again associated with that when you?",
            "Seeing some of the labels already, now you want to start doing sampling and you or both clustering based on the standpoint that label she's seen so far.",
            "But yeah, that was sort of the.",
            "Somewhere find that method that you could do.",
            "What's the best food fight all the pizza?",
            "If you could actually estimate the frac empirical fractions exactly, the label fractions exactly, then it would just be.",
            "The.",
            "Right so yeah.",
            "So in general you want to go as far down the tree as you can, as long as you can estimate the majority labels accurately.",
            "But if you can't do that then that will slow down how far you go down the tree.",
            "Thanks.",
            "Jump."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so this talk is on, yeah, hierarchical sampling for active learning.",
                    "label": 1
                },
                {
                    "sent": "This is joint work.",
                    "label": 0
                },
                {
                    "sent": "My advisor shandra desk.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so active learning is a problem where it tries to address this dichotomy in the availability of data or different types of data.",
                    "label": 0
                },
                {
                    "sent": "One on one hand you have unlabeled data.",
                    "label": 1
                },
                {
                    "sent": "This is just like the raw signal that you get from.",
                    "label": 0
                },
                {
                    "sent": "You can collect from and usually this is very cheap and plentiful.",
                    "label": 0
                },
                {
                    "sent": "For example if you want to get text data, you just mind the web for speech data.",
                    "label": 0
                },
                {
                    "sent": "You can just turn on microphone like I have on right now and images can get those on Flickr.",
                    "label": 0
                },
                {
                    "sent": "Things that we want to predict.",
                    "label": 1
                },
                {
                    "sent": "These labels.",
                    "label": 0
                },
                {
                    "sent": "They are usually hard to come by because these are usually.",
                    "label": 1
                },
                {
                    "sent": "Usually need some human effort to to get these, so you might have to read an article to figure out the topic of the article.",
                    "label": 0
                },
                {
                    "sent": "So the goal in active learning is if you're given a bunch of unlabeled data and access to a human labeler, someone who can actually look at the data and tell you what the label is.",
                    "label": 1
                },
                {
                    "sent": "The goal would be to learn, say, like a classifier using as few requests to this human labeler as possible as possible.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there are sort of two different strategies that people try to employ with an active learning.",
                    "label": 0
                },
                {
                    "sent": "One of what we called the efficient search through hypothesis space so.",
                    "label": 1
                },
                {
                    "sent": "As you get more and more labels to the set of kinda hypothesis that you might consider sort of shrinks down as which ones the hypothesis that seem likely will shrink, I should look at more and more labels.",
                    "label": 0
                },
                {
                    "sent": "So this is sort of been the main effort in active learning, at least in recent years.",
                    "label": 0
                },
                {
                    "sent": "This is things like query by committee or region of disagreement.",
                    "label": 0
                },
                {
                    "sent": "Turn another perspective on.",
                    "label": 0
                },
                {
                    "sent": "Active learning is trying to exploit cluster structure in the data, and So what typically happens with data is that they sort of common.",
                    "label": 1
                },
                {
                    "sent": "These little small clusters of data in terms of some sort of distance measure.",
                    "label": 0
                },
                {
                    "sent": "And if you could just you know the most optimistic thing you could do is just to look at one point in each cluster askers label and then just assign that label to the entire cluster and that would be.",
                    "label": 0
                },
                {
                    "sent": "And then the idea.",
                    "label": 0
                },
                {
                    "sent": "Then, if you can, just you have this fully labeled data set and you can run any sort of supervised learning on that at the end.",
                    "label": 1
                },
                {
                    "sent": "So there's been some work in this area.",
                    "label": 0
                },
                {
                    "sent": "One notable ones is Bayesian method.",
                    "label": 0
                },
                {
                    "sent": "You choose regard Ghahramani and laugh.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pretty.",
                    "label": 0
                },
                {
                    "sent": "But that's what this.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Will be focusing on.",
                    "label": 0
                },
                {
                    "sent": "So there are a lot of them actively encourage sticks that people have employed in practice here.",
                    "label": 0
                },
                {
                    "sent": "Sort of a typical one.",
                    "label": 0
                },
                {
                    "sent": "You start with a pool of unlabeled data and then you ask for the label of a few of the points.",
                    "label": 1
                },
                {
                    "sent": "And then you have repeating this process.",
                    "label": 1
                },
                {
                    "sent": "You train a classifier on the current set of labeled data, and now you want to decide which point the label next.",
                    "label": 0
                },
                {
                    "sent": "So we might do is just choose the point that's closest to the decision boundary, and there's a lot of variations on this.",
                    "label": 0
                },
                {
                    "sent": "Like you choose the most points, most uncertain about one with the smallest margin according to some sort of linear classifier.",
                    "label": 0
                },
                {
                    "sent": "The problem with this is that there's this issue of sampling bias that you quickly run.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Active.",
                    "label": 0
                },
                {
                    "sent": "So here's an example of what might happen.",
                    "label": 0
                },
                {
                    "sent": "So this is a data set that one dimension along the line an this is the data distribution of the unlabeled part.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's say that you draw a few points and you guys for the labels.",
                    "label": 1
                },
                {
                    "sent": "So you have a bunch of blue points on the left and red points on the right.",
                    "label": 0
                },
                {
                    "sent": "Since you have these two clusters and their majority clusters, those are more likely to be the ones that come up first.",
                    "label": 0
                },
                {
                    "sent": "So the first hypothesis that you output is the one that's right in the middle.",
                    "label": 1
                },
                {
                    "sent": "That's the most sensible one.",
                    "label": 0
                },
                {
                    "sent": "Now the points that are closest to it are just all the points in that small cluster in the middle, so you just.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ask for more points in there and you know maybe it turns out that all the points in the middle or just come up red and blue with equal probability.",
                    "label": 0
                },
                {
                    "sent": "So all you really do is doing is just refining where that separator is in the middle, so you're never going to ask for label queries elsewhere.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The problem with this is that if the label distribution is actually quite different from here from this.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "If you had this cluster of red points over there, the left one with 5%.",
                    "label": 0
                },
                {
                    "sent": "Optimal hypothesis is quite different from the hypothesis that you eventually return.",
                    "label": 0
                },
                {
                    "sent": "So what actually happens is that you never were so confident about this Miss cluster that you never asked for court.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is there?",
                    "label": 0
                },
                {
                    "sent": "And so this is the known issue of sampling bias, because you didn't really get a representative set of labeled points from from your standpoint, and so here you see that the best hypothesis, 2.5% error, and the one that you return is always at least 5% here.",
                    "label": 0
                },
                {
                    "sent": "So this is an issue of sampling bias that comes up with active learning, and so the goal of sound active learning strategies is somehow to properly manage this bias.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is sort of a consistency type guarantee that you'd like to have with active learning strategies.",
                    "label": 1
                },
                {
                    "sent": "If the goal is maybe is goal is that you should never do worse than just randomly picking points to label.",
                    "label": 1
                },
                {
                    "sent": "This is just equivalent to what would be a passive supervised learning.",
                    "label": 0
                },
                {
                    "sent": "So our general methodology and a lot of methods is to somehow balances random sampling with something a little bit more aggressive, aggressive, selective sampling and you want to balance the bias that gets introduced.",
                    "label": 0
                },
                {
                    "sent": "So there are a lot of tricks that.",
                    "label": 1
                },
                {
                    "sent": "Can be used to implement this.",
                    "label": 0
                },
                {
                    "sent": "Some of the previous works have used things like rejection sampling and confidence interval.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there are the method we propose is the kind of cluster adaptive sampling, and the goal is system.",
                    "label": 0
                },
                {
                    "sent": "As I said before, it just somehow just sample the query for points in the query for labels in the data and somehow just label every data point in the data set and will do this by just assigning the majority label within each cluster to all of his constituents.",
                    "label": 1
                },
                {
                    "sent": "So that in the end you have a fully labeled data set and the goal we will show that will have most of the labels will be correct and now you can run any supervised learner on this data set and to get a classifier out.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's an example of what a run of this algorithm might like, so you have some initial pull data.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Unlabeled data and so.",
                    "label": 0
                },
                {
                    "sent": "Let's say somehow you cluster data somehow by some distance metric, or.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It will ask for a few labels within each cluster, and so they, unless the sort of turned up red and blue and on the right they all.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Turned up red.",
                    "label": 0
                },
                {
                    "sent": "So there's sort of a problem that you run into and this is what you seem to be stuck, so alright, that's OK, but on the left you don't know whether to give all the points.",
                    "label": 1
                },
                {
                    "sent": "All the points in Blue Label or Red label.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our records this is somehow to use a hierarchical clustering of the data instead of just a fixed flat clustering.",
                    "label": 1
                },
                {
                    "sent": "So on it.",
                    "label": 0
                },
                {
                    "sent": "So I've numbered the clusters in this tree.",
                    "label": 0
                },
                {
                    "sent": "And so they correspond to the different clusters you see here.",
                    "label": 1
                },
                {
                    "sent": "So on the right the cluster two is relatively pure, so we can basically stop sampling from there for the time being, and that focus our efforts of sampling on the 1st cluster.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So indeed we sample some more and then we see that clusters three and four.",
                    "label": 0
                },
                {
                    "sent": "Doing these seem relatively pure.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And So what you might do is just select a new different pruning of the tree.",
                    "label": 0
                },
                {
                    "sent": "So now you have clusters 2, three and four.",
                    "label": 0
                },
                {
                    "sent": "And they all seem relatively pure.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So maybe you could stop there.",
                    "label": 0
                },
                {
                    "sent": "The main idea is just to somehow choose a pruning of the tree that consists mostly of just pure nodes.",
                    "label": 1
                },
                {
                    "sent": "And like here I mean pure and label the labels the kernel.",
                    "label": 0
                },
                {
                    "sent": "So our algorithm.",
                    "label": 1
                },
                {
                    "sent": "Will just maintain a pruning of the tree and will opt opportunistically.",
                    "label": 0
                },
                {
                    "sent": "Choose which cluster into pruning to sample from.",
                    "label": 0
                },
                {
                    "sent": "And then when we choose a once we've chosen, the cluster will rank, draw random point in that cluster to ask for label.",
                    "label": 0
                },
                {
                    "sent": "So since we have all these random.",
                    "label": 0
                },
                {
                    "sent": "Since we're just randomly sampling from each from the clusters, we can maintain these sort of empirical counts and those will correspond to empirical probabilities.",
                    "label": 0
                },
                {
                    "sent": "Will update these statistics.",
                    "label": 0
                },
                {
                    "sent": "We can also sign confidence intervals with with all these quantities, and then after we've done these sampling for update, our choice of the printing of the tree.",
                    "label": 0
                },
                {
                    "sent": "So what happens is so.",
                    "label": 0
                },
                {
                    "sent": "For example, if we chose cluster to the sample from and we get a random leaf, leaf court leave corresponds to data points here.",
                    "label": 0
                },
                {
                    "sent": "If it's also containing cluster 13 in Cluster 6, and it's as if we're sort of also sampling from these clusters cluster 1613 so we can update the statistics for the clusters as well.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's the algorithm in detail.",
                    "label": 0
                },
                {
                    "sent": "The input is just a hierarchical clustering of the data and will initialize with an arbitrary with pruning just consisting of the root node and some labeling of the nodes.",
                    "label": 0
                },
                {
                    "sent": "And then we'll just iteratively proceed by selecting a node in the pruning and choosing a random point from the from the cluster to to label.",
                    "label": 1
                },
                {
                    "sent": "And then we'll update all of the empirical counts in terms of the label frequencies that we've seen so far.",
                    "label": 1
                },
                {
                    "sent": "And then the 4th.",
                    "label": 0
                },
                {
                    "sent": "The last step here is to choose a pruning and labeling.",
                    "label": 0
                },
                {
                    "sent": "The current node that we sampled from an update the printing accordingly, so I'll show you how to do that in a minute, but.",
                    "label": 0
                },
                {
                    "sent": "So after you say we run out of queries or labels then what I said before.",
                    "label": 0
                },
                {
                    "sent": "If you just assign the majority label of each node in your pruning to two of his constituents and you got a fully labeled data set.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the things that need to be specified here are how the hierarchically clustered data, how to choose which note the sample from.",
                    "label": 0
                },
                {
                    "sent": "And how to choose a good printing and labeling?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The first one addresses how you build a hierarchical clustering, so there are a lot of methods.",
                    "label": 1
                },
                {
                    "sent": "And really I'm just going to put on this point.",
                    "label": 0
                },
                {
                    "sent": "They're always methods that people will come up with an.",
                    "label": 0
                },
                {
                    "sent": "The point is that we just want to be able to cluster data well enough so that at some level relatively shallow level of the tree there is some pure pruning.",
                    "label": 1
                },
                {
                    "sent": "That exists in that tree, and the goal is of our methods just to discover a printing of comparable quality.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. Another detail is how do you actually choose the printing of the tree once you have certain statistics on all the clusters and this turned out just to be a dynamic program.",
                    "label": 0
                },
                {
                    "sent": "So we can estimate the error of assigning a particular label to a cluster by 1 minus the empirical fraction of that label within that cluster.",
                    "label": 0
                },
                {
                    "sent": "So that's just 1 -- P hat of node B with label L. So this is a rough estimate of what the error would be if you assign that label to the cluster, and then the dynamic program is somehow follows somewhat naturally so.",
                    "label": 0
                },
                {
                    "sent": "The point is just you can either assign the label to the node and incur that error, or you can look at the 2 two children of the particular node and see if if you sign different labels to them that you get better pruning.",
                    "label": 0
                },
                {
                    "sent": "So the issue here is that you know sometimes your estimates of the fraction label fraction proportions aren't going to be well estimated enough, so you have to take some care and to tell you that.",
                    "label": 0
                },
                {
                    "sent": "But relatively simple and you can compute this thing in a single pass through the tree.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then the main issue here now is how do you actually select a node that sample from?",
                    "label": 1
                },
                {
                    "sent": "Rock variation there are possible so, and this is essentially just random sampling 'cause all your doing is splitting the sampling process into 2 steps.",
                    "label": 0
                },
                {
                    "sent": "You first pick a node portion of his weight and then you pick a random point within that node.",
                    "label": 1
                },
                {
                    "sent": "So that's just random sampling.",
                    "label": 0
                },
                {
                    "sent": "The second one I showed here is this sort of more aggressive sampling method and have to choose a node proportional to its weight, but also time the error you expected to incur.",
                    "label": 0
                },
                {
                    "sent": "So I showed it showed here by 1 -- P hat sort of over lower bound on that.",
                    "label": 0
                },
                {
                    "sent": "So this would be an upper bound on the error that you expect.",
                    "label": 0
                },
                {
                    "sent": "So this is a more aggressive sampling strategy where you sort of try to focus your sampling on notes that are you focus your same sampling away from notes that look pure and focus it on knows that seem little lesson here in hopes of discovering a more pure printing of the tree.",
                    "label": 1
                },
                {
                    "sent": "The thing is, a general ideas.",
                    "label": 0
                },
                {
                    "sent": "You can combine this with a lot of different sampling rules or no selection rules.",
                    "label": 0
                },
                {
                    "sent": "They can use these sort of backpack based off style priors where you can combine it with a margin based rule.",
                    "label": 0
                },
                {
                    "sent": "So there are a lot of variance possible, will just show that the second one in particular does.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very well.",
                    "label": 0
                },
                {
                    "sent": "So there are some consistency guarantees associated with this method, so if you just look at the random sampling rule, the one where you just choose a node proportional to its weight.",
                    "label": 1
                },
                {
                    "sent": "Then if there's a pruning the tree that relatively small printing save the K clusters and encourage relatively low ireda.",
                    "label": 1
                },
                {
                    "sent": "An algorithm will discover a comperable pruning of comparable quality of order ETA error after only K over ETA label queries, so this is sort of what you might expect if you just did this with select, supervised or semi supervised learning rate, considered all possible Cape earnings of the tree and you selected the best one.",
                    "label": 0
                },
                {
                    "sent": "But here we don't have to do that.",
                    "label": 1
                },
                {
                    "sent": "We can just sort of discover it on on the fly.",
                    "label": 0
                },
                {
                    "sent": "And with the active sampling rule, we have sort of a fall back guarantee that says that you never do much worse than if you just pick the notes to sample from randomly.",
                    "label": 0
                },
                {
                    "sent": "There's never much worse than just random sampling, but you can often do a lot better because you sort of have a more aggressive, no selection rule.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this method is quite flexible, so there are a lot of immediate extensions that you get out, so one for instance multi class is very very easy to handle, you just have several tracked.",
                    "label": 0
                },
                {
                    "sent": "Several different empirical counts.",
                    "label": 0
                },
                {
                    "sent": "And then you can use different sort of confidence intervals, say like with the multinomial instead of the binomial pounds.",
                    "label": 0
                },
                {
                    "sent": "Is batch mode is really easy extend to all you have to do is repeatedly call this select node procedure and then sample query a point within each of the clusters that it returns.",
                    "label": 0
                },
                {
                    "sent": "There's another variant of active learning called rare category detection, and the goal here Sis somehow discover rare classes within the data, and this has been applied to things like fraud detection where you want uncovered new patterns or fraud.",
                    "label": 0
                },
                {
                    "sent": "So the active sampling rule?",
                    "label": 1
                },
                {
                    "sent": "Well, sort of try to maintain balance.",
                    "label": 1
                },
                {
                    "sent": "The coverage of the data for you.",
                    "label": 0
                },
                {
                    "sent": "Try to direct the sampling away from pure nodes of the majority class and try to focus your efforts of sampling where you might be more likely to find.",
                    "label": 1
                },
                {
                    "sent": "A rare class.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's a.",
                    "label": 0
                },
                {
                    "sent": "We ran some experience with this method and compared it to.",
                    "label": 0
                },
                {
                    "sent": "Some other common methods, so we used the active sampling role with our cluster adaptive standpoint and we use logistic regression to train a linear model.",
                    "label": 1
                },
                {
                    "sent": "After we've propagated the labels of each cluster.",
                    "label": 1
                },
                {
                    "sent": "And we compare this to random sampling.",
                    "label": 0
                },
                {
                    "sent": "This is just the same as passive learning.",
                    "label": 0
                },
                {
                    "sent": "We also tried it with compared to margin based standpoint.",
                    "label": 0
                },
                {
                    "sent": "This is the method I described at the very beginning where you just choose the point.",
                    "label": 0
                },
                {
                    "sent": "The sample that you choose, the point closest to the current decision boundary to sample from.",
                    "label": 0
                },
                {
                    "sent": "And then we use both logistic regression for random sampling and margin based sampling.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here, just some of the experimental results.",
                    "label": 0
                },
                {
                    "sent": "This is a plot of the test error of the methods.",
                    "label": 0
                },
                {
                    "sent": "On the left we used newsgroup data set with text.",
                    "label": 0
                },
                {
                    "sent": "This is sort of the bag of feature representation of the text.",
                    "label": 0
                },
                {
                    "sent": "There are two.",
                    "label": 0
                },
                {
                    "sent": "Lines for the cluster adaptive sampling.",
                    "label": 0
                },
                {
                    "sent": "That's because we use different ways of building the hierarchical clustering.",
                    "label": 0
                },
                {
                    "sent": "So the first one was when you just represent each document as a TF IDF vector and a second one.",
                    "label": 0
                },
                {
                    "sent": "When you use average linkage to build a clustering.",
                    "label": 0
                },
                {
                    "sent": "The second one we represented, or you will learn the topic model using light and Air show allocation.",
                    "label": 0
                },
                {
                    "sent": "And then we built the tree clustering of that using a variation of average linkage.",
                    "label": 0
                },
                {
                    "sent": "So you see that they both.",
                    "label": 0
                },
                {
                    "sent": "All the active learning rules do a lot better than random sampling, and both costs, both versions of cluster adaptive sampling are pretty comfortable with margin based sampling.",
                    "label": 0
                },
                {
                    "sent": "The margin based sampling has some.",
                    "label": 0
                },
                {
                    "sent": "There's some weird issue with it, where is somehow gets a lower error rate with fewer labels.",
                    "label": 0
                },
                {
                    "sent": "If you don't look at all of the labels, so that's sort of a side issue about margin based sampling.",
                    "label": 0
                },
                {
                    "sent": "On the right we also ran it on a multi class.",
                    "label": 0
                },
                {
                    "sent": "Data set with emnace OCR digits.",
                    "label": 0
                },
                {
                    "sent": "And you see that the cluster method does really well at the beginning was just very relatively very few late label queries, and it's definitely significantly better than random sampling.",
                    "label": 0
                },
                {
                    "sent": "And at the beginning it's a lot better than the margin based sampling method also.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So some of the future work that we want to.",
                    "label": 0
                },
                {
                    "sent": "Continue on with this.",
                    "label": 0
                },
                {
                    "sent": "Somehow characterize the sample complexity of this method.",
                    "label": 0
                },
                {
                    "sent": "Main issues here.",
                    "label": 0
                },
                {
                    "sent": "What sort of the optimal sampling rule?",
                    "label": 1
                },
                {
                    "sent": "I suggested a few random random sampling role was just sort of baseline and active sampling role, but you can imagine there would be more more aggressive roles that somehow adapted to date a little more quickly.",
                    "label": 0
                },
                {
                    "sent": "In a lot of cases them exponential savings seem like they might be possible, so here be interesting to see if that's the case.",
                    "label": 0
                },
                {
                    "sent": "Also, in general, the general methodology here some how to use unlike unsupervised learning and combine it with some sort of sampling method.",
                    "label": 0
                },
                {
                    "sent": "We'd like to use generalize our method to this to other structures that you might learn with unsupervised learning, and somehow turn that into an active learning rule.",
                    "label": 1
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in summary.",
                    "label": 0
                },
                {
                    "sent": "We presented a cluster adaptive sampling method for active learning.",
                    "label": 1
                },
                {
                    "sent": "So the main goal here was just to somehow discover viable clusterings in the data if they exist in the hierarchical clustering.",
                    "label": 1
                },
                {
                    "sent": "So we manage the sampling bias by combining these confidence intervals.",
                    "label": 0
                },
                {
                    "sent": "And also we we we still have sort of a fall back guarantee that it never does much worse than random sampling and see.",
                    "label": 1
                },
                {
                    "sent": "It's an experiment that does a lot better than random sampling, and it's been actually competitive with random sampling.",
                    "label": 0
                },
                {
                    "sent": "Sorry, competitive with these unsafe heuristics like margin based sampling.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "Worried if this clustering assumption doesn't workout.",
                    "label": 0
                },
                {
                    "sent": "So say at a particular level industry you start labeling and all and none of the cluster underneath that node.",
                    "label": 0
                },
                {
                    "sent": "Look sure at all.",
                    "label": 0
                },
                {
                    "sent": "It seems like it be good at that point of time clusters using semi supervised clustering using supervision that you've already gathered at that point and at that point we cluster and go from there.",
                    "label": 0
                },
                {
                    "sent": "I'm just concerned it's like just assuming that your hierarchical clustering.",
                    "label": 0
                },
                {
                    "sent": "Perfect out of the box is not not using the later super addition to maybe refinement, right?",
                    "label": 0
                },
                {
                    "sent": "So yeah, so definitely would be better to just somehow use the labels in building the clustering as well.",
                    "label": 0
                },
                {
                    "sent": "So that is 1 approach that you could fall back on if you start seeing you can actually see with this method which nodes start steam pure and you can sort of ignore those for now and then continue later on the notes that seem impure but the goal here is actually just show that this simple rule would be able to discover printing if it did exist in your tree.",
                    "label": 0
                },
                {
                    "sent": "But as you said, if it doesn't exist in the tree, then yeah, there definitely should do something better.",
                    "label": 0
                },
                {
                    "sent": "Have you thought away?",
                    "label": 0
                },
                {
                    "sent": "Define.",
                    "label": 0
                },
                {
                    "sent": "Modern day standard for sample paper festival please.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so there are some rules that we actually showed some in the paper where you might want to just before you decide whether which node the sample from you would just train a classifier on your current data and look for the clusters that are sort of near the boundary of that cluster of the near their current role and then choose a random point from those clusters.",
                    "label": 0
                },
                {
                    "sent": "So that would be another selection rule that would be possible.",
                    "label": 0
                },
                {
                    "sent": "No, we haven't tried that yet.",
                    "label": 0
                },
                {
                    "sent": "The knowledge from the current hypothesis media president.",
                    "label": 0
                },
                {
                    "sent": "I mean if you have many features.",
                    "label": 0
                },
                {
                    "sent": "Switch out your relevant.",
                    "label": 0
                },
                {
                    "sent": "Daniel.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry, no question.",
                    "label": 0
                },
                {
                    "sent": "Then you can trust your judgement, right?",
                    "label": 0
                },
                {
                    "sent": "I see.",
                    "label": 0
                },
                {
                    "sent": "So basically the clustering.",
                    "label": 0
                },
                {
                    "sent": "Are you suggesting that the clustering assumption might be too strong?",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So the question is whether you could use the current hypothesis.",
                    "label": 0
                },
                {
                    "sent": "Find the.",
                    "label": 0
                },
                {
                    "sent": "I see that might be.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so you might yeah somehow.",
                    "label": 0
                },
                {
                    "sent": "Oh, I guess but.",
                    "label": 0
                },
                {
                    "sent": "Right, yeah, so there it could be that you know the distance function that you used to build the clustering was not very good, or it's only good in certain regions of the data.",
                    "label": 0
                },
                {
                    "sent": "So in those cases you might think about just choosing a new clustering now based on your labels you've seen so far this thing is there are some sort of this is the call complications that again associated with that when you?",
                    "label": 0
                },
                {
                    "sent": "Seeing some of the labels already, now you want to start doing sampling and you or both clustering based on the standpoint that label she's seen so far.",
                    "label": 0
                },
                {
                    "sent": "But yeah, that was sort of the.",
                    "label": 0
                },
                {
                    "sent": "Somewhere find that method that you could do.",
                    "label": 0
                },
                {
                    "sent": "What's the best food fight all the pizza?",
                    "label": 0
                },
                {
                    "sent": "If you could actually estimate the frac empirical fractions exactly, the label fractions exactly, then it would just be.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Right so yeah.",
                    "label": 0
                },
                {
                    "sent": "So in general you want to go as far down the tree as you can, as long as you can estimate the majority labels accurately.",
                    "label": 0
                },
                {
                    "sent": "But if you can't do that then that will slow down how far you go down the tree.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "Jump.",
                    "label": 0
                }
            ]
        }
    }
}