{
    "id": "of2vqrjaedi7c642hkcyscmrtmcducsm",
    "title": "Structured Prediction Problems in Natural Language Processing",
    "info": {
        "author": [
            "Michael Collins, Computer Science and Artificial Intelligence Laboratory (CSAIL), Massachusetts Institute of Technology, MIT"
        ],
        "introducer": [
            "William Cohen, Carnegie Mellon University"
        ],
        "published": "July 24, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Natural Language Processing"
        ]
    },
    "url": "http://videolectures.net/icml08_collins_spp/",
    "segmentation": [
        [
            "OK, I'd like to ask people to come on in and sit down.",
            "And if there's an empty seat next to, you might want to slide over and kind of cuddle up with the person nearby you so you can make space for for other people.",
            "So for those of you that don't know me, I'm William Cohen and the general chair for this conference this year, and general chair.",
            "Of course they get to delegate almost every responsibility to either.",
            "The local organizers have done a fantastic job or to the program chair.",
            "Scientific element Amara way, so unfortunately couldn't be here today.",
            "And they've done a great job and I get to just sort of reserve a few of the most prestigious and enjoyable activities.",
            "So I decided to introduce Michael.",
            "So I've known Michael, or at least I've known of Michael for probably 10 or 12 years, so I remember when Michael was a graduate student having a conversation with a bunch of my colleagues at AT&T Labs.",
            "Asking them what was interesting and exciting in sort of the recent conferences they've been too, and everybody mentioned this fantastic results from the computational linguistics community.",
            "So apparently some new graduate student that no one had really heard of had gone and built this brand new statistical parser that was very elegant and did better on the Penn treebank data than anybody else had ever done in the history of the world.",
            "So they were very impressed at this.",
            "We're really trying hard to get that graduate student up for an internship, so they were very excited about that possibility, which.",
            "It's not sort of a usual thing.",
            "You're not usually excited about.",
            "Graduate student.",
            "Interns are sort of a burden more than anything else.",
            "Sorry, graduate students.",
            "But so we were actually very lucky.",
            "I don't remember whether he came as an intern, but a few years later Michael graduated from the University of Pennsylvania.",
            "His advisor, I think was Marcus and he came to AT&T and I had the pleasure of working with him for a couple of years and he immediately jumped in, and although he came out of the computational linguistics community, there's clearly immediately contact, contributing in all sorts of interesting ways to the core of machine learning.",
            "And he's been working in this intersection of natural language processing and the core of machine learning.",
            "Basically, ever since then, and he's had a very illustrious career.",
            "So I.",
            "At AT&T he was part of were fairly large group.",
            "I would say there's probably 60 people, 80 people in the lab, probably 25 or so.",
            "We're doing machine learning, and Michael was honored in 2002 by 1 being one of I think, three people in machine learning that were not fired.",
            "Biotin.",
            "He went on to MIT and just to give you a slight sense of his career since then, in 2002 he had got a best paper award at the conference Empirical Methods on Natural Language Processing, which is the I guess actually.",
            "The main computation linguistics conferences have all become very empirical.",
            "MLP was the first one to be exclusively empirical or or.",
            "Especially your purple, and he got the best paper award there in 2002.",
            "He also got the best paper award in 2004.",
            "I know you're wondering the same thing.",
            "What about 2003 and 2003?",
            "He was that he was the program Chair for Human LP, so of course he couldn't give the paper or best paper work for himself, but he did get 2 paper additional best paper awards in 2004 and 2005 at UI.",
            "So he's had a fantastic career.",
            "He's done a lot of really interesting work in an area that I'm very interested in structured prediction.",
            "I'm just going to turn things over now to my palms lightly, thanks.",
            "So pretty much all the work I'm going to talk about today is joint work with this group chavier Carreras America, listen, Terry Koo.",
            "So I just wanted to acknowledge them first."
        ],
        [
            "So structured prediction problems.",
            "So this is going to be the topic of today's talk, so I should explain what these are.",
            "So we're going to be considering a supervised learning setting setting.",
            "So as usual, I'll goal is to learn a function from some set X to some set Y, and we're assuming that we have a set of labeled examples of that desired mapping.",
            "OK, very well understood problem or very well studied problem, at least is a binary classification where the set why is simply a set of two possible labels.",
            "That's called minus 1 + 1 generalization of binary classification is multiclass classification.",
            "In this case the set.",
            "Is if K labels where K is generally a fairly small number.",
            "So in recent years, structured prediction problems have shown quite a bit of work in the machine learning community.",
            "These have the following characteristics, so the set Y of possible labels for any input is typically a very large set.",
            "So, more often than not, it will grow exponentially fast with the size of the input X, so it's a very large set.",
            "In addition, each member of this set has internal structure and we can leverage this internal structure in a couple of ways.",
            "One in terms of how we represent these objects or labels, and Secondly in the algorithms we use in formulating these prediction problems.",
            "So let me."
        ],
        [
            "A little more concrete, let me give you some examples, so structured prediction problems arise in many fields.",
            "Here's some examples.",
            "One is speech recognition.",
            "So here the problem is to map an acoustic input to a sentence.",
            "OK, so each label innocence is an entire sentence.",
            "In computer vision.",
            "We might want to map images to underlying segmentations, for example using a Markov random field.",
            "Again, the label in this case is a segmentation, again, a complex structured object, and these kind of problems arise very frequently in natural language processing so.",
            "NLP focus focuses a great deal of energy on these types of problems.",
            "Now I'm going to be talking about 2:00 today.",
            "The first actually most of the talk will be about the problem of natural language parsing, so this is the problem of mapping strings to underlying parse trees.",
            "In addition, on the way I'll talk a little bit about working machine translation, so here the problem is to match strings in one language, two strings in another language, and I'll talk at least briefly about how syntax or these parse trees can play a role within machine translation systems."
        ],
        [
            "OK, so let's talk a little bit about the pausing problem.",
            "So as I said, the problem here is to take an underlying string and to map this to an underlying syntactic structure a little later in the talk, I'll talk about exactly what these structures represent.",
            "OK, but just abstractly this looks like a labeled tree, so we have some hierarchical decomposition of a sentence, and in the form of a tree, and we have labels at these nodes.",
            "For example, SNP, NVP, various linguistic categories.",
            "So necessarily, most of the examples I'll show in this talk a fairly simple examples of this."
        ],
        [
            "One, but he's a real pause tree from our datasets.",
            "This is a 30 word sentence.",
            "Here's the past tree, which I don't expect you to read.",
            "Of course, the main point being that these are pretty intricate structures.",
            "There's quite a lot of structure here.",
            "There's a lot of decisions on the way to building this full parse tree, and this is really quite a challenging problem.",
            "One of the main challenges is actually ambiguity that there might be many possible parse trees for a particular input sentence.",
            "We somehow have to sift through those past reason, select the most likely or most probable one.",
            "So the main focus of this talk is going to be the following so."
        ],
        [
            "Conditional random fields and related discriminative models.",
            "For example, Max margin Markov networks have been shown to be a powerful alternative to Hmm's, and there actually are perhaps the most commonly used approach for structured prediction, and they have some particular advantages over Hmm's.",
            "The main one, as I see it, is that they allow quite flexibel representations of structured objects.",
            "A secondary advantage is that they combine those representations with discriminative learning methods.",
            "So the main question is talk is going to be about how we can generalize these CRF style models to pausing to recovering these structures back here.",
            "Now see reps have been around since the early 2006 since 2001, I think, but it's only in the last, maybe two or three years that we've already gotten these to scale to the full parsing problem.",
            "Now talk about some problems and some solutions that we've gone through to get to this point.",
            "So the first part of the talk."
        ],
        [
            "Local focus on the choice of model structure or how we parameterized these pausing models.",
            "How we generalize CRF's from, say, sequence labeling to recovering these recursive syntactic tree like structures, and so this will be primarily about different grammars and how they can be embedded within.",
            "See refs."
        ],
        [
            "The second part of the talk will be an inference, so a major bottleneck in these models is inference.",
            "Finding the most likely parse tree, for example, under a particular model, and this can be extremely expensive in the case of pausing exact inference, methods can be extremely expensive, and so I'll talk about some work on approximate inference that shows to be quite effective in this kind of scenario."
        ],
        [
            "The third part of the talk will talk about parameter estimation for these models, and again one of the key issues here is efficiency.",
            "With like parameter estimation methods, which for example take relatively few passes over the training set before they converge to a good parameter setting.",
            "So in one sense, this talk will be a case study on this particular problem, natural language parsing, but I hope also that the lessons and the techniques that we see in each of these areas would carry across too many other structured prediction problems.",
            "So there will hopefully be some general lessons there and I'll come back to these at the end of the talk."
        ],
        [
            "OK, so here's an overview of the talk.",
            "As I said, the main part of the talk, we models inference and learning for this problem.",
            "In the middle I have an interlude where we talk about machine translation for a little while and I'll stop."
        ],
        [
            "The background.",
            "OK, So what are these piles?",
            "Trees so?",
            "For this let's go back to the very start.",
            "So perhaps the most well known syntactic models for language for natural languages, context free grammars.",
            "So many of you will be familiar with these.",
            "They go back at least to Chomsky's PhD thesis over 50 years ago in their application to language.",
            "So the basic element in a CFG is a set of rules.",
            "OK, so the grammar is defined by a set of rules.",
            "These look like rewrite rules, for example, saying that an S rewrites and NP followed by VP.",
            "To paraphrase that, this is essentially saying that sentence can be decomposed as something called a noun phrase followed by verb phrase.",
            "We then have additional rules for these other categories.",
            "NP and VP.",
            "OK, now given a set of rules like this, we have a set of well defined structures under the grammar.",
            "And in particular, for a given sentence, for example, John saw Mary, we have a set of possible parse trees for that sentence.",
            "There's just one past three in this case.",
            "So Path tree looks like one of these labeled trees.",
            "I showed you, but where each level in the tree corresponds to one of these rules.",
            "So for example, we have the S goes to NP VP rule at the very top of the tree.",
            "OK, so this is perhaps the simplest model for language.",
            "You can simplest model for syntax, we can write context free grammars of this form and then get back a set of possible pause trees for a sentence."
        ],
        [
            "So what are these trees useful for?",
            "So try in a nutshell to give one of the main motivations for covering these trees.",
            "Why these syntactic structures are useful in a quite wide variety of natural language applications.",
            "So let's take a simple sentence like John saw Mary in Helsinki.",
            "So basic level of analysis we'd like to do on the sentence is to recover the grammatical relations that we see in this sentence.",
            "So, for example, one grammatical relation is between Soren John, more specifically that John is the subject of the verb sore.",
            "Another grammatical relation is that Mary is the object of sore, and so on.",
            "So these are the basic units of who did what to whom in the sentence.",
            "For example, the verb and its different arguments.",
            "OK, and we'd like to be able to recover this information.",
            "So one property of the."
        ],
        [
            "Syntactic structures.",
            "These parse trees is that they fairly directly encode exactly these grammatical relations.",
            "So once we've correctly identified a parse tree for a particular input, we can directly recover the grammatical relations of this form, and then that may be useful and quite a number of applications.",
            "So here's one example.",
            "Whenever we see this kind of tree for configuration, so it's SNPVP and then avoid below, we can essentially read off the grammatical relation between a subject in its verb.",
            "So this template innocence fits in this position in the tree.",
            "OK, so the tree in some sense is made.",
            "This grammatical relation transparent.",
            "You can just read it off the tree.",
            "Now of course, again, I emphasize, these are simple examples, so this might look like an easy job for this particular example.",
            "For the more complex sentences.",
            "For longer sentences, recovering these grammatical relations is far from trivial."
        ],
        [
            "Here's another example is a verb object example.",
            "So here we have another syntactic configuration shown in red.",
            "Here a VP dominating verb followed by a noun phrase, and again we can just read off a verb object relation in this case."
        ],
        [
            "OK, and there are others that are."
        ],
        [
            "There are other relations within these trees.",
            "So that's perhaps one of the primary motivations this idea of grammatical relations.",
            "Just to finish this section on background, let me first talk about one of the first models applied to machine learning for syntax, and perhaps one of the simplest models.",
            "Many or all of you will be familiar with these.",
            "This is the idea of probabilistic context free grammars.",
            "OK so in PCF geez, we generalize CFG's by adding a probability to each rule production.",
            "And here's one probability, one term.",
            "This looks like the conditional probability given that we're expanding the symbol S of seeing these particular nonterminals below that NP 4 by VP.",
            "OK, so this essentially looks like a probability associated with each rule in the grammar.",
            "Given these probabilities, we can take an entire parse tree and define the probability of that tree as a product of these rule probabilities.",
            "So that's a very simple direct model where we start to have probabilities over these structures, and we can use this to actually learn mappings from strings to these structures.",
            "There's a problem in that PC FGS.",
            "In this simple form as I've shown you here where we have a very small set of non terminals are actually extremely poor models of language.",
            "In a couple of senses.",
            "Firstly they perform very badly at ranking alternate parse trees for a particular input in terms of probability and Secondly they're very poor in terms of assigning probability distributions to strings.",
            "And the main reason for that is that these nonterminals encode very little information about the surrounding context in the tree and that means they lead to quite poor models.",
            "So a little later in the talk will come back to PC FGS.",
            "And we'll see some tricks whereby you can considerably improve their performance, essentially by exploding the number of non terminals in the grammar.",
            "That's one approach that's been used quite fruitfully."
        ],
        [
            "OK, so now let's talk about."
        ],
        [
            "The models I'll be using, so I guess the key points here are as follows.",
            "Firstly, this is general point of how we can use probabilistic awaited grammars and machine learning, and more specifically I'll be describing how CRF style models can be generalized to learning with these weighted grammars.",
            "The second main point in this section is to introduce tree adjoining grammars as an alternative grammar formalism to CFG's.",
            "So true adjoining grammars are fairly well known in the natural language processing community, but probably much less familiar to the community here, and their efforts will advantages over context free grammars in terms of how they parameterized the parsing problem, so I'll try to describe the advantages these grammars grammars bring.",
            "OK."
        ],
        [
            "So we're going to be talking about CRF, so let me just briefly describe conditional random fields.",
            "I guess pretty much all of you would have seen these, but at least to define notation so, and let's talk about conditional random fields.",
            "OK, so simplest problem.",
            "You can apply conditional random field.",
            "Two is sequence labeling.",
            "So in this problem, our input X is a sequence of tokens.",
            "For example sequence of words in a sentence and the output Y is a sequence of underlying states.",
            "OK, so for example the output why might be a sequence of part of speech tags?",
            "One part of speech for each word in the sentence?"
        ],
        [
            "So here's a particular example.",
            "We might have this input sentence, and this might be one candidate output sequence.",
            "We have a noun, verb, determiner, preposition, and so on.",
            "So a crucial idea in conditional random fields.",
            "Really the building block for conditional random fields is that of feature vectors which are used to represent these sequence pairs.",
            "So we have a feature vector mapping of the following form, so F. Um?",
            "It's a feature vector and it takes as input an entire sentence XA position in that sentence.",
            "For example, maybe we're looking at the fourth position and a state transition.",
            "So a pair of tags which are adjacent in the sentence.",
            "OK, now typically these feature vector representations will look at all kinds of features of the input sentence conjoined with these two tags.",
            "So we might for example, be sensitive to the fact that cake is being assigned the tag noun, or maybe the tag noun is here and we have some surrounding context in the string, and so on and so on.",
            "And really, one of the nice things about CRF's is the freedom with which you can define these feature vectors."
        ],
        [
            "So the model form for CRF looks like the following, so the optimal state sequence.",
            "Why star for a particular input X is the way that maximizes the sum of terms.",
            "So here I have a sum over the end positions in the sentence, so I'm summing over position by position and at each position I essentially have a state transition.",
            "OK, so a pair of labels just I showed you, and each of those state transitions gets a score that's an inner product between the parameter vector W and the feature vector F. So you can think of these inner products as being a measure of the plausibility or probability of this particular state transition at this particular position in the sentence.",
            "OK, we can find the highest scoring state sequence using the Viterbi algorithm, so because the last decomposes in this way, we can use dynamic programming to search through the space of possible structures efficiently."
        ],
        [
            "So that's a CRF.",
            "So the big question is, how do we generalize these serious style models to pausing?",
            "So there are a couple of options, one which has been explored is to think about context free grammars.",
            "The representations that context free grammars induce, and how these can be leveraged within CRF's.",
            "The alternative, which I'll talk about next, is based on alternative tree adjoining grammars due to work by Arvind Joshi and many of his colleagues."
        ],
        [
            "OK so tags motor tree adjoining grammars.",
            "So there's really a rich literature on these grammars like I can't do anything close to justice for it.",
            "Instead, I'm going to describe a fairly simple tag like formalism that we've applied recently to this particular problem.",
            "OK, so in context free grammars, the basic elements in the grammar, a set of these rewrite rules.",
            "Entry adjoining grammars.",
            "In contrast, the grammar is essentially defined by a set of water called elementary trees, so these look like tree fragments.",
            "And in this particular case that we are using, our elementary trees are these simple spine like structures.",
            "So the structures in our grammar look like a wooden lexical item, which is often called the anchor of the tree with some small sub structure above it.",
            "Some small fragment of tree above it.",
            "So for example, Mary has a couple of levels going up to an NP.",
            "Each has a few more levels and so on.",
            "So these are the basic units in tree adjoining grammars.",
            "So the next question is how we build."
        ],
        [
            "Build larger pieces of syntactic structure.",
            "And so in general there are basic operations which can be used to combine these small tree fragments into larger pieces of structure.",
            "And the one operation that we use today is something called Sister Adjunction.",
            "So sister Junction basically takes two of these spines that I've just shown you and joins them together or combine them to form a larger piece of the syntactic structure.",
            "In particular, this disjunction operation takes one spine called modifier, which is the Mary Spine.",
            "Here another spine, which is called the head.",
            "That's the spot in here.",
            "In addition, it takes a position at which the modifier spine attaches underneath the head spine, so underneath this S position here."
        ],
        [
            "OK, so this is a primitive primitive operation that combines these two trees and essentially forms a rule.",
            "S goes to N PvP.",
            "At this point in the tree."
        ],
        [
            "So we can keep going, and actually you can imagine a full parse tree for this full sentence.",
            "Ascentia Lee being formed by a series of these kind of junction operations.",
            "So later we'll see that the constraints on these junctions, operations or other costs so in regular grammars you might have some constraints on how these things can combine exactly where different elementary trees can adjoin.",
            "In our case, we will simply use costs.",
            "Essentially, will be learned in a CRF style model."
        ],
        [
            "So intuitively, we can think about a full tree here when now decomposing this into a set of junction operations.",
            "So pausing in A tag intuitively looks like a bottom up process where you first for each word in the input, choose one of these elementary trees.",
            "So you choose a piece of projected structure, and then Secondly you choose how these different pieces of structure fit together to form a full parse tree."
        ],
        [
            "OK, So what I might be why we be interested in these kind of grammars as an alternative, say to CFG.",
            "So wanted to highlight a couple of advantages.",
            "The first is the following.",
            "If you think about the lexical entries in the grammar.",
            "These quite naturally capture constraints associated with lexical items, so it's basically a constraint at least once or as a verb, but we see this piece of tree fragment above it, so this is quite a natural way to capture the influence of individual words on higher structure in the tree.",
            "Perhaps more importantly, we can start to associate probabilities or costs with these combination operations.",
            "I just spoke about.",
            "So for example, one operation might be to combine these fragments associated with word eats and cake to form this structure with one of the sister junction operations I just showed you, and this can now have a probability or cost.",
            "Now essentially what we're doing here when we form this structure is with choosing the word cake as the direct object of eats.",
            "And by allowing parameters which can have access to this information, actually these kind of parameters have been shown time and time again to be very useful in disambiguating different power structures with choosing between different power structures."
        ],
        [
            "So you know to contrast with PCF.",
            "Jeez, like I said in tags we associate costs or probabilities with these junction operations.",
            "This contrasts with CFG's, where we associate probabilities or costs with rules.",
            "So we have quite different where parameterising or thinking about the problem in these two cases."
        ],
        [
            "So the next trick is going to be to generalize ideas about feature vectors in conditional random fields to these adjunction operations.",
            "So the models we've been using essentially map these junction operations to feature vectors.",
            "So now we have a feature vector mapping which looks as follows.",
            "We have an input X, which is a string.",
            "And we have information which basically specifies everything I've shown you in red here.",
            "So we have the indices of the head warden, the modifier word participating in this relationship.",
            "So three and five.",
            "In this case, the third word, and the fifth word.",
            "We have the identity of those two spines.",
            "Those two elementary trees that have been chosen.",
            "And we have the position of where this junction operation happens, so we know that for example is NP is moving under this PP right here.",
            "OK.",
            "So the feature vectors could potentially track any subset of the information shown here on this slide.",
            "In practice, we look at features."
        ],
        [
            "The following the two words involved in this junction."
        ],
        [
            "Relation, maybe information about the surrounding world."
        ],
        [
            "This.",
            "Maybe information about the parts of speech of that over words, verb and noun and."
        ],
        [
            "Case."
        ],
        [
            "And in particular, subsets of the junction operation.",
            "So these three non terminals are actually very useful in describing the grammatical relation.",
            "In this case, a verb object relation in this particular junction operation.",
            "So the feature vectors again, we can leverage the flexibility of these definitions and including quite broad features which look at all kinds of surface features of the string, as in regular CRF's combined with these junction operations."
        ],
        [
            "So that's."
        ],
        [
            "Well, that's it for the model.",
            "So just to sort of wrap this up, the goal as I said, is to map an input sentence X2, apostrophe, Y.",
            "The model form looks rather similar to CRF's, so the optimal path tree for an input is now going to pause tree, which maximizes again some of schools.",
            "But now, rather than associating schools with state transitions, we associate schools with these adjunction operations.",
            "So we have a sum over the Czars, where each R is a tuple specifying one of these adjunction operations that's being used to build the tree.",
            "So a tree is available, evaluated by the plausibility or score of the different junction operations which form that tree.",
            "And you can compare this to model form for CRF so you can see it's fairly similar."
        ],
        [
            "OK, so how well does this model work?",
            "Let me talk about some experiments, so inference and parameter estimation are things I'll come to next for training the model, we use the average Perceptron algorithm for inference.",
            "We use forensic dynamic programming, which I'll talk about.",
            "In terms of data, I'll talk about results on the Penn Wall Street Journal treebank.",
            "This is a pretty standard valuation method metric.",
            "And in terms of the evaluation metric that's being used, we can look at precision and recall in terms of recovering fragments of these piles.",
            "Trees, in particular in recovering constituents within these past trees or rules within these parse trees so we can look at precision recall and F1 score in recovering these structures.",
            "The main comparisons I'll make our two PCF based models.",
            "So I've spoken a bit about these and let me talk."
        ],
        [
            "About methods which are based on PCF just briefly so we can do the comparisons.",
            "So like I said, PC FGS associate probabilities with rules in the tree and they have this problem in that in these kind of vanilla PC FGS weather very small number of nonterminals.",
            "They really fail to capture a lot of the contextual dependencies you see in language.",
            "And so this is.",
            "This is a big problem.",
            "So."
        ],
        [
            "The main method to deal with this has been to expand the set of non terminals in the grammar in various different ways and so here's one way, which is perhaps the simplest which is to add an annotation to each non terminal keeping track of its parent non terminal.",
            "So call these parent annotations.",
            "So for example.",
            "Here we now have A tag which keeps track of the fact that this NP has an S right above it and similarly keep track of the fact that NP has a VP right above it.",
            "So we've essentially added one level of context to the tree, and we now have these exploded nonterminals and we just estimate a regular PC FG using this method.",
            "So this is perhaps the simplest method."
        ],
        [
            "Why we do this?",
            "Another option which is very well known is to use water called lexicalized PCF cheese and in some senses these are somewhat similar to the trade writing grammars I just showed you.",
            "So in this case we add information to nonterminals, again by propagating words up through these trees.",
            "OK, so for example, we propagate the word John or sore marry in various different ways and we now have additional information on these nonterminals specifying these headwords.",
            "We end up with a regular PC FG again.",
            "And of course we need quite a bit of care in estimating these rule production probabilities 'cause we have a very large number of nonterminals, very large number of set of possible rules and so estimating these parameters.",
            "Take some care, but it can be done OK."
        ],
        [
            "The final method of compared to, which is some really very interesting recent work, is due to Petra from client last year.",
            "So here the idea is to gain allow the non terminals in the grammar to split in some sense.",
            "Let's be refined, but to learn those splits automatically using M. So now imagine I take each non terminal, say an S non terminal and I say I can split this into 128 different possibilities.",
            "Some number of possibilities.",
            "Now these splits are latent there not observed in the training data.",
            "And so we can use them essentially to recover these latent annotations.",
            "And to really make this work, I think this is one major insider petrol from Klein.",
            "You need ways of allowing different nonterminals to split to different degrees.",
            "You need some kind of model selection and this is quite quite effective."
        ],
        [
            "So let me give you some results and some main messages of these results, so parent annotation method certainly works better than a ropy CFG, but still has about 20% error in terms of recovering these constituents.",
            "So that's evidence that you know even the simple well, even this method for refining on terminals doesn't work too well."
        ],
        [
            "We compared to Lexicalized PC FGS.",
            "We can see that there's about a 25% reduction in error if we look at the model, so the tag based model is down here.",
            "And this is probably primarily due to the features which we can include in this model, which are really quite difficult to include.",
            "And let's close PCF.",
            "Geez, so we really leverage this idea of CLF style features to include Surface features and include various overlapping features which are very difficult to get into PC fegs."
        ],
        [
            "Finally, if we compared to petrol incline, we still see some improvements over their work, although the results are closer.",
            "I think an interesting area for future work is to try to combine these two methods, for example using state splitting approaches for the tag based grammars."
        ],
        [
            "OK, so.",
            "I'll get an interlude.",
            "I wanted to talk a little bit about machine translation as one application of these kind of models.",
            "An interesting thing here is that some of the techniques I've shown you in pausing and now being carried across to."
        ],
        [
            "Machine learning for machine translation.",
            "So in statistical approaches to translation again, we think of this as a structured prediction problem where we're mapping strings and, say, German to strings in English, and we have potentially a very large set of example translations.",
            "Typically you might have a few 100,000 examples of translations between the two languages.",
            "Now you could think of two possible approaches.",
            "Roughly speaking, one an approach where you try to learn this mapping directly and you don't make so much use of latent structure.",
            "A second possible approaches where you might use these syntactic structures.",
            "Maybe these grammatical relations as a level of intermediate structure in performing this mapping.",
            "So let's talk."
        ],
        [
            "These two approaches, so the first is phrase based translation, so this was a major breakthrough in statistical machine translation, and it's really impressive how well these models have worked in phrase based models.",
            "The basic component or the basic entries in the model is a lexecon of so-called phrase pairs.",
            "So each entry in this lexecon looks like a German substring, for example in Denigan are paired with an English substring over here.",
            "And we might potentially induce a very large lexecon of entries of this form from a set of example translations.",
            "Now, given this lexecon, translation proceeds."
        ],
        [
            "The following 2 steps.",
            "So firstly we segment the input German string into a series of German segments and for each of those segments we choose an English translation.",
            "So I've shown you this step.",
            "Here we have a bunch of we have a segmentation and now we have a bunch of English fragments down here.",
            "The translations withdrawn from this lexecon."
        ],
        [
            "In the second step, we choose some ordering of the resulting English phrases, so we might move things around a little bit.",
            "So in particular, the subject here elections have come after the verb take in German.",
            "German has quite flexible word order, so you often see this kind of thing was the subject always has to appear before the verb in English, and so we sort of scrambled these phrases, rearrange them to produce this new translation.",
            "Now these two steps, the choice of the phrases and then the choice of the ordering of course had probabilities or associated costs which are somehow estimated from a from a corpus.",
            "In particular, in the reordering step you make heavy use of a language model of prior model over strings in the language.",
            "For example, the trigram language model.",
            "So these models have gotten along way in translation, but they really don't make any direct use of syntactic information.",
            "You might think, for example, that knowing where the different argument.",
            "Arguments, whether different grammatical relations are on the German side might be useful in predicting the structure of the English tree."
        ],
        [
            "So recent work for example at ISI and DBN 2 examples I've shown you here has started to frame the translation process essentially as opposing task.",
            "It's really really very interesting take, I think on translation and have shown very really quite impressive results, particularly in Chinese, which is language which has very different word order from English.",
            "So in this idea the phrases are augmented to carry with them pieces of syntactic structure.",
            "So now a phrase entry in R model might pair this German string with an English string which has a piece of tree fragment above it.",
            "So maybe we have paused the English corpus and annotated our English strings with these paltry fragments.",
            "OK, so now we have this additional information."
        ],
        [
            "And now we gain have a two step process in translating a sentence.",
            "So the first step is to segment the German sentence and again to choose a set of phrasal entries.",
            "But now we have a set of these or sequence of these syntactic fragments.",
            "OK, so we have a sequence of these tree fragments."
        ],
        [
            "The second step is to somehow assemble these tree fragments into a full parse tree.",
            "So that essentially involves something very similar to the adjunction operations I showed you earlier in the talk.",
            "When we start to fit these tree structures together with some costs and recover a in English pause tree.",
            "And some reordering is allowed.",
            "So we are actually allowed to move around the order."
        ],
        [
            "With these different fragments.",
            "So finally we get a translation.",
            "The output of the system is this English poetry."
        ],
        [
            "So that's sort of a sketch of this model.",
            "A couple of things about this.",
            "So what is that the search pro?"
        ],
        [
            "So again, we're going to have some process that first searches for these fragments and then."
        ],
        [
            "Looks at how to combine them.",
            "This can be implemented using essentially extensions of algorithms used for regular parsing, and so this is what I mean by translation as a pausing problem.",
            "We essentially.",
            "Our formulate the translation problem is decoding under one of these pausing models.",
            "And there's some potential advantages which have been shown in this work.",
            "One is that you build an English pause tree and thereby have some control over dramatic aliti of that tree.",
            "So you have some control over the fact that the verb needs a subject and an object, and all these different things.",
            "The second thing is that in capturing the difference in Word order between 2 languages, for example, if we go back here, the fact that the subject comes after the verb in German before it in English, we can capture those kind of effects by reordering on these piles, trees which is kind of a natural way to capture world radio.",
            "Reordering in the two languages."
        ],
        [
            "So that's translation.",
            "Let me now go to."
        ],
        [
            "So inference under these models.",
            "So one question I lead left open was how we find the most likely parse tree under the model I showed you.",
            "So here again I've shown you the model form where a pause tree is scored by the sum of scores of these different junction operations."
        ],
        [
            "So some main points of this part of the talk.",
            "So dynamic programming algorithms exist for tank grammars, which actually or at least for the tank grammars I've shown.",
            "You have very similar runtime to grammar algorithms for PCF Geez.",
            "So for example cubic time runtime, where in the length of the sentence?",
            "So that's somewhat promising.",
            "Unfortunately, exact inference under these models is still very expensive.",
            "Actually, prohibitively expensive.",
            "You can easily spend minutes pausing a sentence.",
            "An inference is a bottleneck both in applying the model to new sentences and also during training of the model.",
            "OK, because the training algorithms will see for conditional random fields generally require decoding the training examples multiple times, and so inference is a bottleneck in training.",
            "So one solution which has been found effective many times in the pausing literature is to use an idea called course.",
            "Define dynamic programming.",
            "For example, Czarniak has been using this for a long time and pausing models the basic idea here is to use a first pass model, which is a simple computationally cheap model to restrict the search space of the full parsing model and this can actually be remarkably effective in the models.",
            "Will see.",
            "So let me describe this idea.",
            "So to understand."
        ],
        [
            "The algorithms will actually go to another syntactic formalism.",
            "We've seen context free grammars.",
            "We've seen tags.",
            "Now I'm going to talk a little bit about dependency grammars or dependency representations.",
            "So the top of this slide is a dependency structure, which can be thought of as an alternative representation to context free trees.",
            "So it basically is formed by a directed graph which forms a tree with this special node called star, the root node.",
            "Sorry Malta star at the root of this tree.",
            "And we have arcs between basically heads in their modifyers.",
            "So I have a dependency for example between John's or basically corresponding to a subject verb dependency.",
            "And the simplest form of these dependency structures there, just as I've shown you there.",
            "In a slightly more refined form, you might have labels on these arcs, so you might actually have a label saying subject or label.",
            "Saying movie is the object of sore and so on and so on.",
            "So this can be thought of as an alternative representation.",
            "It's again quite widely used in natural natural language community.",
            "Another way of representing syntactic structures.",
            "So a few years ago, Ryan McDonald's, an collaborators, came up with CRF style models for dependency parsing.",
            "So now we can score these structures using again a CRF style model.",
            "So the score of any structure is going to be a sum over the dependencies within that structure, and then each dependency gets a school, which is just another inner product.",
            "Features now track dependencies and potentially any information about the input sentence."
        ],
        [
            "I'm.",
            "So one thing which is remarkable remarkable about these dependency structures is that they have very efficient decoding algorithms.",
            "Juza Jason Eisner.",
            "So for the structures I've just shown you, which have no labels, the most probable or lowest cost dependency structure under this kind of model can be found in cubic time in the length of the sentence.",
            "With very low constants.",
            "And the algorithms and it came up with a really quite beautiful algorithms that dynamic programming algorithms and they decompose dependency structures into subparts and quite ingenious way and a kind of dynamic programming bottom up search so it looks rather different from, say, passing the context free grammar, but the complexity is actually very similar to CFG's, which are also cubic in the length of the sentence.",
            "McDonald's our leverage this to great effect in training up discriminative models.",
            "Based on these dependency representations.",
            "In fact, these representations are efficient enough that exhaustive pausing is really quite quite plausable, quite feasible."
        ],
        [
            "So what does this fit with the tank structures that I've shown you?",
            "So if you think about these tag structures, they look very much like dependency structures with some additional information.",
            "So essentially dependencies structures augmented with these spines and also invented with these positions of these junction points in these spines.",
            "So for example, there is essentially a dependency between the words with and almonds, but now it's mediated by these two spines.",
            "In this junction position.",
            "So you can think of tank structures as being more elaborate dependency structures.",
            "Essentially, there's a close connection between the two."
        ],
        [
            "And that actually means that aizen's algorithms can also be applied to this formalism.",
            "So most probable lowest cost pause under these tag grammars can again be found in cubic time in the length of the sentence, again in time, which is essentially the same as pausing a context free grammar.",
            "We now have a grammar constant, so this constant G is actually a function of various things, mainly the number of possible spines for any potential word, and the maximum height of any spine.",
            "So G can be thought of as a measure of the size of the grammar.",
            "And as an additional constant in this runtime."
        ],
        [
            "So that's extremely useful, but it doesn't quite get us there.",
            "So the problem is that this grammar constant is prohibitive.",
            "We can easily have situations where G is 1000 or even 10,000.",
            "Potentially more so the grammar constant is really a cure in decoding with these models.",
            "So the solution is as I said, to try to use a simpler model.",
            "In our case, one of McDonald's dependency parsing models to prune the space of the full parser and the simple model has a much lower constant, say roughly equal to the number of non terminals in the grammar and is thereby efficient.",
            "It can be applied efficiently."
        ],
        [
            "So let me just elaborate on this idea a little bit.",
            "So if we think of any adjunction operation in the textile grammar.",
            "That essentially implies a dependency between 2 words.",
            "So here again I have the example eaten cake modifying each other.",
            "And we can see that essentially this implies a dependency between eaten cake.",
            "Now it's a label dependency.",
            "What I've done here is I pulled out these three labels that are involved here.",
            "OK, so I now have a label doc.",
            "And actually we build 3 different dependency parsing models similar to McDonald's, one for each of these three labels.",
            "And these simpler models can essentially estimate the posterior probability of dependencies with these labels.",
            "These these three different labels.",
            "So now we have a way of scoring potential dependencies using a model with a much lower grammar constant.",
            "And we can basically discard dependencies which have very low probability.",
            "So the basic idea is to only allow junctions in the textile model which correspond to dependencies that have reasonably high probability under the similar models.",
            "So let me show you some results for this.",
            "For this kind of approach."
        ],
        [
            "So the road to look at is the last row.",
            "This is the beam size we used.",
            "So we can basically throw away all but about .3% of these potential junction points.",
            "And still end up in a situation where an Oracle parser so a search process that sort of found the best tree possible.",
            "Under that constraint recovered about 98 1/2% of the constituents.",
            "OK, so we've vastly reduce the search space while still enabling a positive.",
            "Potentially get very close to 100% accuracy.",
            "So the main message here, I guess, is that these kind of course defined approach is a very effective in dynamic programming problems and have been found to be very effective in these."
        ],
        [
            "Causing problems.",
            "OK, so the last part of the talk I want to talk about optimization."
        ],
        [
            "Learning.",
            "So the question is, how do we learn these parameter vectors W. Ann?",
            "For simplicity, I'll go back to basic conditional random fields where feature vectors are associated with state transitions.",
            "Just because I think people are more familiar with this.",
            "So how do we?"
        ],
        [
            "In the parameters W. So gain efficiency is a bottleneck, so pausing and other problems in LP we often have many training samples, and in addition we have these structured problems where we'll have to perform inference again and again on these training examples.",
            "So our experience is being very similar to a lot of experience machine learning, going back to stochastic gradient descent, training of neural networks, namely that online algorithms are often much more efficient than batch gradient methods so online at style algorithms which visit a single training example at a time and do an update on just that training example.",
            "So I actually described two online algorithms that we use.",
            "One is the perceptron or an average variant of it, and the 2nd is a method for training CRF's based on exponentiated gradient algorithm."
        ],
        [
            "OK, so here's the perceptron.",
            "So this is a straightforward generalization of the regular perceptron algorithm as devised for classification to these structured problems.",
            "So the method looks like as follows.",
            "We initialize our parameters to 0.",
            "And then we make some number of passes over the training set.",
            "So we make big T passes over the training set.",
            "T is typically fairly small, say 510 or 20.",
            "So in each pass over the training set, we iterate over the training examples.",
            "So we go over the training samples one by one.",
            "Each training example is an XY pair, where X is a sentence.",
            "And why is a sequence of labels?",
            "Remember, we're doing a sequence labeling problem here.",
            "OK.",
            "So the first thing we do is we decode under the current model parameters.",
            "So we choose Z to be state sequence that is most likely under our current parameters W. Now see is correct.",
            "If it matches why we are conservative and we do know update to the parameters we leave them untouched.",
            "On the other hand, if we make a mistake, if Z is not equal to why we do the following update?",
            "So we say the new parameters W of the old parameters and then we have two terms.",
            "The first term just looks like a summer feature.",
            "Vectors in the truth, the target label Y and the second term looks like a summer feature vectors for all the state transitions in the incorrectly proposed state sequence said so intuitively, we're going to raise the weight of any parameters seen in the truth the target.",
            "Decrease the weight in any parameters seen in this competitive sequence that we've just erroneously produced.",
            "And then we just return W. So."
        ],
        [
            "That's the.",
            "The structured perceptron.",
            "A very important modification to it is averaging, so this was first introduced by Foreigner Japeri, essentially as a heuristic approximation to voting and voting has formal guarantees.",
            "Averaging, I think, is much less well understood.",
            "So and averaging we do the following in addition to the regular weight vector I showed you, we keep around the second vector of parameters, called the average parameters, and these are again additional set initially to 0.",
            "And then we basically use these parameters to keep it running average.",
            "So at every point we take average parameters and add in W. So we add in the parameter vector after the training example, whether we've done an update or not.",
            "And finally we re scale the average parameters.",
            "So we really have an average.",
            "The rescaling step really isn't important, but will include it here.",
            "OK, so this is the average variance of the set."
        ],
        [
            "So a few notes on this, so in terms of theoretical guarantees, if there exists some parameter vector which correctly classifieds all the data, then you can show the perception will converge parameter values which which achieved that goal.",
            "In addition, the number of errors it makes a number of updates IT users before it reaches a separating hyperplane is based on a margin, just as it's based on the margin.",
            "In regular classification problems we can also relate these margin based properties to generalization properties.",
            "Price, most importantly, let's talk about a few practical findings with the perceptron.",
            "One thing we found time and time again is that averaging improves performance a lot.",
            "Without averaging, the perceptron is a pretty crummy algorithm on these problems.",
            "With averaging, it's very competitive.",
            "In fact, it often performs nearly as well as full CRF training or Max margin networks, or often performs as well as these other training methods.",
            "It typically reaches a good solution after only a few, say 5 iterations, which is very handy in these large scale problems.",
            "Something else which is maybe underappreciated is that it returns relatively sparse solutions, so because these updates only depend on two sequences at every point, and because we only make a relatively small number of passes over the training set, it actually leaves a lot of the parameters equal to 0, so it returns pretty sparse solutions.",
            "Again, a very useful property when you have high dimensional feature spaces."
        ],
        [
            "So here is convergence actually on the pausing test just to illustrate the kind of speed of convergence you see with the Perceptron, here's the iteration versus accuracy.",
            "You can see after five or six iterations it's almost there and you know sort of gets a bit further if we go beyond 10 iterations.",
            "But the main point here is that it can often converge.",
            "You often get to a very decent solution and barely five or six iterations.",
            "This is test data.",
            "This is valid.",
            "Yeah, this is validation data.",
            "Yep."
        ],
        [
            "So let me conclude talk about another training algorithm, which also has an online flavor, but which it looks rather different from the perceptron.",
            "So say we'd like to train a regular regularised log likelihood objective, which is commonly used in training conditional random fields.",
            "Let me just show you how to define this.",
            "So another piece of notation will say F of X&Y is a feature vector associated with an entire sequence.",
            "It's just a sum over the individual components.",
            "Individual straight state transitions in that sequence.",
            "Given this, we can define a conditional distribution over structures Y, which just looks like a regular Gibbs style distribution.",
            "So we have an inner product in the exponent, then a partition function.",
            "So the objective will talk about minimizing.",
            "Looks like a combination of the negative log likelihood under this kind of model and two norm penalty or regularizer.",
            "OK often you'd have some constant here dictating the level of regularization in the model.",
            "For simplicity we'll just take this to be half in this part of the talk.",
            "So how do we minimize this?"
        ],
        [
            "Function.",
            "So I'm going to talk about an algorithm that makes use of a Joule fulmination.",
            "So the dual is going to make use of a set of variables Alpha variables.",
            "So Alpha Ky is a dual variable where K ranges over all training samples and why ranges over all possible structures.",
            "For that case.",
            "Training example.",
            "OK, so for now we're going to have a huge set of these dual variables will come later to how this can be dealt with.",
            "So Alpha Ky is a dual variable associated with a particular sequence on a particular training example.",
            "We'll define a mapping from these two variables to primal variables as follows, so W of Alpha is going to look like a sum over the feature vectors for the true labels, YK?",
            "Minus and this looks like an expectation.",
            "OK, so these alphas are actually constrained lie between zero and one.",
            "And for any particular training example K they sum to one.",
            "So they essentially defined distributions over these examples.",
            "So here we essentially have an expectation.",
            "This looks like an expected feature vector under these Alpha variables.",
            "So the dual objective is to minimize this function Q of Alpha under the simplex constraints that I've just shown you.",
            "And the dual looks like a sum of two terms.",
            "The second term is Ascentia Lee the regularizer term.",
            "So we have this function of the alphas.",
            "This primal parameter and we have the two norm of that.",
            "The first term is a kind of entropic term, so this looks like the entropy entropy of these Alpha Ky variables.",
            "OK, and I'm a convex duality.",
            "You can show that if you optimize queue.",
            "So if you find the Alpha that minimizes queue, you can recover the optimal primal parameters so you can recover the solution to this problem just through the mapping I've shown you here.",
            "OK, So what is this by this so bias?"
        ],
        [
            "So I'm going to talk about methods which use dual coordinate descent, so this is rather similar.",
            "For example to Smol classic algorithm for training support vector machines.",
            "The basic idea is going to be to choose a single training example at a time and update the dual variables on just that one training example.",
            "So, for example, we might look at the two variables in that one training example and try to explicitly optimize Q with respect to just those dual variables.",
            "So this method has an online flavor in that we're going to visit a single training example at a time, and as we'll see, this has advantages in terms of speed of convergence.",
            "On the other hand, it has a nice property, which is that it's simple enough to show that you can directly measure the impact.",
            "Of any changes you make to those two variables.",
            "So actually, if I make a change, the Jew variables on a particular example by reference to that example alone, I can see whether the dual objective has increased or decreased, and this allows us, for example, to do line search for a learning rate or to explicitly choose the step size in various different optimization."
        ],
        [
            "Steps.",
            "OK, so the method we use is based on exponentiated gradient updates.",
            "So these are popular, popularized by Kevin and Moose for example in online learning.",
            "Here we're going to apply these to this particular optimization task, so we have this dual objective.",
            "As initial values will choose, these Alpha Ky values to sum values such that they sum to 1, they're in the simplex constraints that I showed you, but they're strictly positive they must be non 0.",
            "We choose some learning rates and positive learning rate Ada.",
            "And then the algorithm proceeds as follows.",
            "So we take T steps and at each step we choose a training example uniformly at random.",
            "And actually we found it was critical both in the proofs and also in the experimental results to choose examples at random rather than for example cycling over the training samples in the same order repeatedly.",
            "So we choose a training example at random, and then we do these e.g updates.",
            "So what are these look like?",
            "Well, they're driven by the gradient, so natural OK, why is just the derivative of Q with respect to the dual variable for Ky?",
            "And the updates look like the following.",
            "We have the new Alpha is equal to the old alphas and then we have this gradient with the learning rate in the exponent here and these as I said we're in these kind of updates were introduced by Kevin Warmath.",
            "So one obvious property of these updates, given the normalization term here, is that they preserve the simplex constraints.",
            "The alphas stay in the."
        ],
        [
            "Plex so let me sketch some results that we found for these kind of updates.",
            "So one is in terms of rate of convergence.",
            "It's fairly efficient, so to get within epsilon of the optimal dual value for the.",
            "The jewel that I just showed you.",
            "Essentially need order log one over epsilon updates, so this is the linear convergence.",
            "In addition, our results have found that online updates have faster convergence than batch methods, both in the theoretical bounds and also in our practical results so."
        ],
        [
            "Each update would perform exactly these updates, but would update all of the Alpha variables at each."
        ],
        [
            "Yep, OK. Um?",
            "The next question is how do we deal with the fact that we have one dual variable for every possible sequence for every possible training example, we have an exponential number of dual variables.",
            "So it turns out that there is a compact and efficient implementation of this algorithm.",
            "I'll just sketch how this is done here.",
            "So instead of representing these Alpha variables explicitly, we make use of an alternative parameterization.",
            "So we now have alternative dual variable stator, and each theater is associated with the training example K with a position I and with the potential state transition.",
            "So it sort of mirrors feature vectors in terms of where these things are defined.",
            "So these are alternative dual variables that can take any value in the reals.",
            "And then we define these Alpha variables basically as it gives style distribution.",
            "OK, so this dual variable is now some over these thetas divided by a partition function.",
            "So we now have polynomial number of these alternative dual variables.",
            "Now it turns out that the algorithm I just I've just shown you collapses in that you can reframe everything in terms of updates to these data variables, so you never need to represent the Alpha variables explicitly.",
            "The state's can be updated.",
            "And the main cost in implementing this algorithm is then the forward backward algorithm.",
            "So it turns out that we need to be able to compute marginals under these kinds of distributions, and we can do that using forward, backward.",
            "So those are the two ideas.",
            "One this kind of compact representation of the dual variables and two re framing algorithm in terms of updates.",
            "These dual variables with repeated forward backward steps at each point to complete the picture.",
            "These methods are quite general.",
            "They can also be applied, for example, to the pausing problem where similar algorithms to forward backward also exist."
        ],
        [
            "So let me just talk about some results, then will finish up.",
            "So here are some comparisons to LBF so this is a conjugate gradient style optimizer.",
            "It's a batch gradient method.",
            "It's a very popular method for optimizing CRF's.",
            "And it's certainly not online.",
            "As I said, it's batch.",
            "So here is a graph showing convergence for the G algorithm in red.",
            "Now be FGS in green.",
            "You can see it's quite a bit quicker, so this is for particular regularizer constant.",
            "We found these kind of improvements across the board on the different."
        ],
        [
            "These are constants.",
            "His convergence in terms of test error, so the red line shows actually sorry, accuracy.",
            "Sorry this is on the dependency parsing problem and again we see in terms of parsing accuracy.",
            "Convergence is in roughly 20 or so iterations.",
            "It's quite a bit slower for LB FGS under a variety of regularization constants."
        ],
        [
            "So that pretty much ties it up.",
            "I just want to give some conclusions so we think about the three sections of the talk.",
            "The first part was on models and I guess the point I want to make here is that weighted grammars, for example PC FGS or tags or dependency grammars offer useful generalizations of Hmm's.",
            "So probably certainly a very familiar set of models within this Community of graphical models.",
            "Grammars are probably.",
            "Grammatical models from graphical models and quite different set of algorithms.",
            "The second point is really that lexicalized grammars, for example tanks or dependency grammars lead to interesting alternative parameters.",
            "Find methods very effective, at least in these kind of causing problems in making pausing efficient.",
            "And finally, again in parameter estimation, it seems online algorithms are key, but the average perceptron and e.g algorithms take barely 5 or 10 iterations over training set for conversions.",
            "Thanks.",
            "Question.",
            "So a model for what, sorry?",
            "Should I should I?",
            "My question is rather general, is it possible?",
            "What do you think?",
            "Is it realistic to create a unified language model?",
            "For example, all the grammars that you that you mean their language dependent.",
            "Is it possible to create a drama that are language independent?",
            "Oh gosh.",
            "I think the quote has been working a lot of languages, so typically the kind of tree banks like the Wall Street Journal treebank have been built in many other languages.",
            "Chinese, Arabic, many of the European languages, and so on.",
            "Having these types of models have been shown to transfer to those different training sets quite effectively.",
            "For example, Ryan McDonald's workers have been applied to many of these different languages quite effectively.",
            "But the idea of a model which you can train in one language then apply to another language.",
            "I think I can't see how we can do that, at least at this point.",
            "If that answers your question.",
            "Yes, it was my question, but it's the problem of machine learning.",
            "Now if you have one one grammar for over languages.",
            "You don't have problems to have a machine translation system.",
            "Programming languages are certainly different, right?",
            "Word order is different.",
            "The words are different in different languages, so the grammars have to differ at some level.",
            "But there are certainly that kind of representations.",
            "A lot of work in linguistics is trying to unify treatments of different languages and make them look as similar as possible in terms of the grammar so that that might be 1.",
            "Answer to your question, thank you.",
            "So.",
            "Oh, that's a very interesting question.",
            "Um?",
            "Yes, it seems that you might be able to do something very interesting there.",
            "I can't off the top my head think how that that might work, but.",
            "We're basically just looking at subsets of these junction operations, so the question is whether you can automatically choose the level of granularity in those models.",
            "Or maybe you have a cascaded set of models.",
            "Yeah, that would be a very interesting question, I really thought.",
            "Like seems to be very striking, the online version of so you present this online learning algorithm seems to.",
            "Work much faster than the LBFFG, which seems to be very striking.",
            "I'd like to if you can explain on how high level that what is the intuition is that you're you're updating the parameters after this thing.",
            "Every training example, so you're.",
            "Particularly if there's any redundancy in the training samples.",
            "If they're similar at all, then you'll tend to converge more quickly.",
            "This is an observation that's been made in your network training way back when, I think.",
            "So that would be that would be my main answer.",
            "It's a little difficult to describe why in the proofs you actually come up with a tighter upper bound in the online case, that's a little more involved to explain, but it definitely comes through for the G algorithms.",
            "Usually when people.",
            "Yeah, absolutely yeah.",
            "There's certainly context free expressive power.",
            "The main benefit in this case is the alternative parameterization that just drops out of this very similar to a dependency parsing parameterisation.",
            "You might of course use these simple tags as a simple model to prune the search space for a more complex tag, which would require a much larger pausing, pausing time.",
            "So while we're running a little bit over, I think it went to Custer Flex in shorts and those who have had questions for Michael things tracking down at the banquet.",
            "Text.",
            "No.",
            "No, thanks, that's right."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, I'd like to ask people to come on in and sit down.",
                    "label": 0
                },
                {
                    "sent": "And if there's an empty seat next to, you might want to slide over and kind of cuddle up with the person nearby you so you can make space for for other people.",
                    "label": 0
                },
                {
                    "sent": "So for those of you that don't know me, I'm William Cohen and the general chair for this conference this year, and general chair.",
                    "label": 0
                },
                {
                    "sent": "Of course they get to delegate almost every responsibility to either.",
                    "label": 0
                },
                {
                    "sent": "The local organizers have done a fantastic job or to the program chair.",
                    "label": 0
                },
                {
                    "sent": "Scientific element Amara way, so unfortunately couldn't be here today.",
                    "label": 0
                },
                {
                    "sent": "And they've done a great job and I get to just sort of reserve a few of the most prestigious and enjoyable activities.",
                    "label": 0
                },
                {
                    "sent": "So I decided to introduce Michael.",
                    "label": 0
                },
                {
                    "sent": "So I've known Michael, or at least I've known of Michael for probably 10 or 12 years, so I remember when Michael was a graduate student having a conversation with a bunch of my colleagues at AT&T Labs.",
                    "label": 0
                },
                {
                    "sent": "Asking them what was interesting and exciting in sort of the recent conferences they've been too, and everybody mentioned this fantastic results from the computational linguistics community.",
                    "label": 0
                },
                {
                    "sent": "So apparently some new graduate student that no one had really heard of had gone and built this brand new statistical parser that was very elegant and did better on the Penn treebank data than anybody else had ever done in the history of the world.",
                    "label": 0
                },
                {
                    "sent": "So they were very impressed at this.",
                    "label": 0
                },
                {
                    "sent": "We're really trying hard to get that graduate student up for an internship, so they were very excited about that possibility, which.",
                    "label": 0
                },
                {
                    "sent": "It's not sort of a usual thing.",
                    "label": 0
                },
                {
                    "sent": "You're not usually excited about.",
                    "label": 0
                },
                {
                    "sent": "Graduate student.",
                    "label": 0
                },
                {
                    "sent": "Interns are sort of a burden more than anything else.",
                    "label": 0
                },
                {
                    "sent": "Sorry, graduate students.",
                    "label": 0
                },
                {
                    "sent": "But so we were actually very lucky.",
                    "label": 0
                },
                {
                    "sent": "I don't remember whether he came as an intern, but a few years later Michael graduated from the University of Pennsylvania.",
                    "label": 0
                },
                {
                    "sent": "His advisor, I think was Marcus and he came to AT&T and I had the pleasure of working with him for a couple of years and he immediately jumped in, and although he came out of the computational linguistics community, there's clearly immediately contact, contributing in all sorts of interesting ways to the core of machine learning.",
                    "label": 0
                },
                {
                    "sent": "And he's been working in this intersection of natural language processing and the core of machine learning.",
                    "label": 1
                },
                {
                    "sent": "Basically, ever since then, and he's had a very illustrious career.",
                    "label": 0
                },
                {
                    "sent": "So I.",
                    "label": 0
                },
                {
                    "sent": "At AT&T he was part of were fairly large group.",
                    "label": 0
                },
                {
                    "sent": "I would say there's probably 60 people, 80 people in the lab, probably 25 or so.",
                    "label": 0
                },
                {
                    "sent": "We're doing machine learning, and Michael was honored in 2002 by 1 being one of I think, three people in machine learning that were not fired.",
                    "label": 0
                },
                {
                    "sent": "Biotin.",
                    "label": 0
                },
                {
                    "sent": "He went on to MIT and just to give you a slight sense of his career since then, in 2002 he had got a best paper award at the conference Empirical Methods on Natural Language Processing, which is the I guess actually.",
                    "label": 0
                },
                {
                    "sent": "The main computation linguistics conferences have all become very empirical.",
                    "label": 0
                },
                {
                    "sent": "MLP was the first one to be exclusively empirical or or.",
                    "label": 0
                },
                {
                    "sent": "Especially your purple, and he got the best paper award there in 2002.",
                    "label": 0
                },
                {
                    "sent": "He also got the best paper award in 2004.",
                    "label": 0
                },
                {
                    "sent": "I know you're wondering the same thing.",
                    "label": 0
                },
                {
                    "sent": "What about 2003 and 2003?",
                    "label": 0
                },
                {
                    "sent": "He was that he was the program Chair for Human LP, so of course he couldn't give the paper or best paper work for himself, but he did get 2 paper additional best paper awards in 2004 and 2005 at UI.",
                    "label": 0
                },
                {
                    "sent": "So he's had a fantastic career.",
                    "label": 0
                },
                {
                    "sent": "He's done a lot of really interesting work in an area that I'm very interested in structured prediction.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to turn things over now to my palms lightly, thanks.",
                    "label": 0
                },
                {
                    "sent": "So pretty much all the work I'm going to talk about today is joint work with this group chavier Carreras America, listen, Terry Koo.",
                    "label": 0
                },
                {
                    "sent": "So I just wanted to acknowledge them first.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So structured prediction problems.",
                    "label": 0
                },
                {
                    "sent": "So this is going to be the topic of today's talk, so I should explain what these are.",
                    "label": 0
                },
                {
                    "sent": "So we're going to be considering a supervised learning setting setting.",
                    "label": 0
                },
                {
                    "sent": "So as usual, I'll goal is to learn a function from some set X to some set Y, and we're assuming that we have a set of labeled examples of that desired mapping.",
                    "label": 0
                },
                {
                    "sent": "OK, very well understood problem or very well studied problem, at least is a binary classification where the set why is simply a set of two possible labels.",
                    "label": 0
                },
                {
                    "sent": "That's called minus 1 + 1 generalization of binary classification is multiclass classification.",
                    "label": 1
                },
                {
                    "sent": "In this case the set.",
                    "label": 0
                },
                {
                    "sent": "Is if K labels where K is generally a fairly small number.",
                    "label": 1
                },
                {
                    "sent": "So in recent years, structured prediction problems have shown quite a bit of work in the machine learning community.",
                    "label": 1
                },
                {
                    "sent": "These have the following characteristics, so the set Y of possible labels for any input is typically a very large set.",
                    "label": 0
                },
                {
                    "sent": "So, more often than not, it will grow exponentially fast with the size of the input X, so it's a very large set.",
                    "label": 0
                },
                {
                    "sent": "In addition, each member of this set has internal structure and we can leverage this internal structure in a couple of ways.",
                    "label": 1
                },
                {
                    "sent": "One in terms of how we represent these objects or labels, and Secondly in the algorithms we use in formulating these prediction problems.",
                    "label": 0
                },
                {
                    "sent": "So let me.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A little more concrete, let me give you some examples, so structured prediction problems arise in many fields.",
                    "label": 0
                },
                {
                    "sent": "Here's some examples.",
                    "label": 0
                },
                {
                    "sent": "One is speech recognition.",
                    "label": 0
                },
                {
                    "sent": "So here the problem is to map an acoustic input to a sentence.",
                    "label": 0
                },
                {
                    "sent": "OK, so each label innocence is an entire sentence.",
                    "label": 0
                },
                {
                    "sent": "In computer vision.",
                    "label": 0
                },
                {
                    "sent": "We might want to map images to underlying segmentations, for example using a Markov random field.",
                    "label": 0
                },
                {
                    "sent": "Again, the label in this case is a segmentation, again, a complex structured object, and these kind of problems arise very frequently in natural language processing so.",
                    "label": 0
                },
                {
                    "sent": "NLP focus focuses a great deal of energy on these types of problems.",
                    "label": 0
                },
                {
                    "sent": "Now I'm going to be talking about 2:00 today.",
                    "label": 0
                },
                {
                    "sent": "The first actually most of the talk will be about the problem of natural language parsing, so this is the problem of mapping strings to underlying parse trees.",
                    "label": 0
                },
                {
                    "sent": "In addition, on the way I'll talk a little bit about working machine translation, so here the problem is to match strings in one language, two strings in another language, and I'll talk at least briefly about how syntax or these parse trees can play a role within machine translation systems.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's talk a little bit about the pausing problem.",
                    "label": 0
                },
                {
                    "sent": "So as I said, the problem here is to take an underlying string and to map this to an underlying syntactic structure a little later in the talk, I'll talk about exactly what these structures represent.",
                    "label": 0
                },
                {
                    "sent": "OK, but just abstractly this looks like a labeled tree, so we have some hierarchical decomposition of a sentence, and in the form of a tree, and we have labels at these nodes.",
                    "label": 0
                },
                {
                    "sent": "For example, SNP, NVP, various linguistic categories.",
                    "label": 0
                },
                {
                    "sent": "So necessarily, most of the examples I'll show in this talk a fairly simple examples of this.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One, but he's a real pause tree from our datasets.",
                    "label": 0
                },
                {
                    "sent": "This is a 30 word sentence.",
                    "label": 0
                },
                {
                    "sent": "Here's the past tree, which I don't expect you to read.",
                    "label": 0
                },
                {
                    "sent": "Of course, the main point being that these are pretty intricate structures.",
                    "label": 0
                },
                {
                    "sent": "There's quite a lot of structure here.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of decisions on the way to building this full parse tree, and this is really quite a challenging problem.",
                    "label": 0
                },
                {
                    "sent": "One of the main challenges is actually ambiguity that there might be many possible parse trees for a particular input sentence.",
                    "label": 0
                },
                {
                    "sent": "We somehow have to sift through those past reason, select the most likely or most probable one.",
                    "label": 0
                },
                {
                    "sent": "So the main focus of this talk is going to be the following so.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Conditional random fields and related discriminative models.",
                    "label": 1
                },
                {
                    "sent": "For example, Max margin Markov networks have been shown to be a powerful alternative to Hmm's, and there actually are perhaps the most commonly used approach for structured prediction, and they have some particular advantages over Hmm's.",
                    "label": 1
                },
                {
                    "sent": "The main one, as I see it, is that they allow quite flexibel representations of structured objects.",
                    "label": 0
                },
                {
                    "sent": "A secondary advantage is that they combine those representations with discriminative learning methods.",
                    "label": 0
                },
                {
                    "sent": "So the main question is talk is going to be about how we can generalize these CRF style models to pausing to recovering these structures back here.",
                    "label": 0
                },
                {
                    "sent": "Now see reps have been around since the early 2006 since 2001, I think, but it's only in the last, maybe two or three years that we've already gotten these to scale to the full parsing problem.",
                    "label": 0
                },
                {
                    "sent": "Now talk about some problems and some solutions that we've gone through to get to this point.",
                    "label": 0
                },
                {
                    "sent": "So the first part of the talk.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Local focus on the choice of model structure or how we parameterized these pausing models.",
                    "label": 1
                },
                {
                    "sent": "How we generalize CRF's from, say, sequence labeling to recovering these recursive syntactic tree like structures, and so this will be primarily about different grammars and how they can be embedded within.",
                    "label": 0
                },
                {
                    "sent": "See refs.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second part of the talk will be an inference, so a major bottleneck in these models is inference.",
                    "label": 0
                },
                {
                    "sent": "Finding the most likely parse tree, for example, under a particular model, and this can be extremely expensive in the case of pausing exact inference, methods can be extremely expensive, and so I'll talk about some work on approximate inference that shows to be quite effective in this kind of scenario.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The third part of the talk will talk about parameter estimation for these models, and again one of the key issues here is efficiency.",
                    "label": 0
                },
                {
                    "sent": "With like parameter estimation methods, which for example take relatively few passes over the training set before they converge to a good parameter setting.",
                    "label": 0
                },
                {
                    "sent": "So in one sense, this talk will be a case study on this particular problem, natural language parsing, but I hope also that the lessons and the techniques that we see in each of these areas would carry across too many other structured prediction problems.",
                    "label": 0
                },
                {
                    "sent": "So there will hopefully be some general lessons there and I'll come back to these at the end of the talk.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here's an overview of the talk.",
                    "label": 0
                },
                {
                    "sent": "As I said, the main part of the talk, we models inference and learning for this problem.",
                    "label": 0
                },
                {
                    "sent": "In the middle I have an interlude where we talk about machine translation for a little while and I'll stop.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The background.",
                    "label": 0
                },
                {
                    "sent": "OK, So what are these piles?",
                    "label": 0
                },
                {
                    "sent": "Trees so?",
                    "label": 0
                },
                {
                    "sent": "For this let's go back to the very start.",
                    "label": 0
                },
                {
                    "sent": "So perhaps the most well known syntactic models for language for natural languages, context free grammars.",
                    "label": 0
                },
                {
                    "sent": "So many of you will be familiar with these.",
                    "label": 0
                },
                {
                    "sent": "They go back at least to Chomsky's PhD thesis over 50 years ago in their application to language.",
                    "label": 0
                },
                {
                    "sent": "So the basic element in a CFG is a set of rules.",
                    "label": 0
                },
                {
                    "sent": "OK, so the grammar is defined by a set of rules.",
                    "label": 1
                },
                {
                    "sent": "These look like rewrite rules, for example, saying that an S rewrites and NP followed by VP.",
                    "label": 0
                },
                {
                    "sent": "To paraphrase that, this is essentially saying that sentence can be decomposed as something called a noun phrase followed by verb phrase.",
                    "label": 1
                },
                {
                    "sent": "We then have additional rules for these other categories.",
                    "label": 0
                },
                {
                    "sent": "NP and VP.",
                    "label": 0
                },
                {
                    "sent": "OK, now given a set of rules like this, we have a set of well defined structures under the grammar.",
                    "label": 0
                },
                {
                    "sent": "And in particular, for a given sentence, for example, John saw Mary, we have a set of possible parse trees for that sentence.",
                    "label": 0
                },
                {
                    "sent": "There's just one past three in this case.",
                    "label": 1
                },
                {
                    "sent": "So Path tree looks like one of these labeled trees.",
                    "label": 0
                },
                {
                    "sent": "I showed you, but where each level in the tree corresponds to one of these rules.",
                    "label": 0
                },
                {
                    "sent": "So for example, we have the S goes to NP VP rule at the very top of the tree.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is perhaps the simplest model for language.",
                    "label": 0
                },
                {
                    "sent": "You can simplest model for syntax, we can write context free grammars of this form and then get back a set of possible pause trees for a sentence.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what are these trees useful for?",
                    "label": 0
                },
                {
                    "sent": "So try in a nutshell to give one of the main motivations for covering these trees.",
                    "label": 0
                },
                {
                    "sent": "Why these syntactic structures are useful in a quite wide variety of natural language applications.",
                    "label": 0
                },
                {
                    "sent": "So let's take a simple sentence like John saw Mary in Helsinki.",
                    "label": 1
                },
                {
                    "sent": "So basic level of analysis we'd like to do on the sentence is to recover the grammatical relations that we see in this sentence.",
                    "label": 0
                },
                {
                    "sent": "So, for example, one grammatical relation is between Soren John, more specifically that John is the subject of the verb sore.",
                    "label": 0
                },
                {
                    "sent": "Another grammatical relation is that Mary is the object of sore, and so on.",
                    "label": 0
                },
                {
                    "sent": "So these are the basic units of who did what to whom in the sentence.",
                    "label": 0
                },
                {
                    "sent": "For example, the verb and its different arguments.",
                    "label": 0
                },
                {
                    "sent": "OK, and we'd like to be able to recover this information.",
                    "label": 0
                },
                {
                    "sent": "So one property of the.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Syntactic structures.",
                    "label": 0
                },
                {
                    "sent": "These parse trees is that they fairly directly encode exactly these grammatical relations.",
                    "label": 0
                },
                {
                    "sent": "So once we've correctly identified a parse tree for a particular input, we can directly recover the grammatical relations of this form, and then that may be useful and quite a number of applications.",
                    "label": 0
                },
                {
                    "sent": "So here's one example.",
                    "label": 0
                },
                {
                    "sent": "Whenever we see this kind of tree for configuration, so it's SNPVP and then avoid below, we can essentially read off the grammatical relation between a subject in its verb.",
                    "label": 0
                },
                {
                    "sent": "So this template innocence fits in this position in the tree.",
                    "label": 0
                },
                {
                    "sent": "OK, so the tree in some sense is made.",
                    "label": 0
                },
                {
                    "sent": "This grammatical relation transparent.",
                    "label": 0
                },
                {
                    "sent": "You can just read it off the tree.",
                    "label": 0
                },
                {
                    "sent": "Now of course, again, I emphasize, these are simple examples, so this might look like an easy job for this particular example.",
                    "label": 0
                },
                {
                    "sent": "For the more complex sentences.",
                    "label": 0
                },
                {
                    "sent": "For longer sentences, recovering these grammatical relations is far from trivial.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's another example is a verb object example.",
                    "label": 0
                },
                {
                    "sent": "So here we have another syntactic configuration shown in red.",
                    "label": 0
                },
                {
                    "sent": "Here a VP dominating verb followed by a noun phrase, and again we can just read off a verb object relation in this case.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and there are others that are.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are other relations within these trees.",
                    "label": 0
                },
                {
                    "sent": "So that's perhaps one of the primary motivations this idea of grammatical relations.",
                    "label": 0
                },
                {
                    "sent": "Just to finish this section on background, let me first talk about one of the first models applied to machine learning for syntax, and perhaps one of the simplest models.",
                    "label": 0
                },
                {
                    "sent": "Many or all of you will be familiar with these.",
                    "label": 0
                },
                {
                    "sent": "This is the idea of probabilistic context free grammars.",
                    "label": 0
                },
                {
                    "sent": "OK so in PCF geez, we generalize CFG's by adding a probability to each rule production.",
                    "label": 0
                },
                {
                    "sent": "And here's one probability, one term.",
                    "label": 0
                },
                {
                    "sent": "This looks like the conditional probability given that we're expanding the symbol S of seeing these particular nonterminals below that NP 4 by VP.",
                    "label": 0
                },
                {
                    "sent": "OK, so this essentially looks like a probability associated with each rule in the grammar.",
                    "label": 0
                },
                {
                    "sent": "Given these probabilities, we can take an entire parse tree and define the probability of that tree as a product of these rule probabilities.",
                    "label": 0
                },
                {
                    "sent": "So that's a very simple direct model where we start to have probabilities over these structures, and we can use this to actually learn mappings from strings to these structures.",
                    "label": 0
                },
                {
                    "sent": "There's a problem in that PC FGS.",
                    "label": 0
                },
                {
                    "sent": "In this simple form as I've shown you here where we have a very small set of non terminals are actually extremely poor models of language.",
                    "label": 0
                },
                {
                    "sent": "In a couple of senses.",
                    "label": 0
                },
                {
                    "sent": "Firstly they perform very badly at ranking alternate parse trees for a particular input in terms of probability and Secondly they're very poor in terms of assigning probability distributions to strings.",
                    "label": 0
                },
                {
                    "sent": "And the main reason for that is that these nonterminals encode very little information about the surrounding context in the tree and that means they lead to quite poor models.",
                    "label": 0
                },
                {
                    "sent": "So a little later in the talk will come back to PC FGS.",
                    "label": 0
                },
                {
                    "sent": "And we'll see some tricks whereby you can considerably improve their performance, essentially by exploding the number of non terminals in the grammar.",
                    "label": 0
                },
                {
                    "sent": "That's one approach that's been used quite fruitfully.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now let's talk about.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The models I'll be using, so I guess the key points here are as follows.",
                    "label": 0
                },
                {
                    "sent": "Firstly, this is general point of how we can use probabilistic awaited grammars and machine learning, and more specifically I'll be describing how CRF style models can be generalized to learning with these weighted grammars.",
                    "label": 0
                },
                {
                    "sent": "The second main point in this section is to introduce tree adjoining grammars as an alternative grammar formalism to CFG's.",
                    "label": 0
                },
                {
                    "sent": "So true adjoining grammars are fairly well known in the natural language processing community, but probably much less familiar to the community here, and their efforts will advantages over context free grammars in terms of how they parameterized the parsing problem, so I'll try to describe the advantages these grammars grammars bring.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we're going to be talking about CRF, so let me just briefly describe conditional random fields.",
                    "label": 1
                },
                {
                    "sent": "I guess pretty much all of you would have seen these, but at least to define notation so, and let's talk about conditional random fields.",
                    "label": 0
                },
                {
                    "sent": "OK, so simplest problem.",
                    "label": 0
                },
                {
                    "sent": "You can apply conditional random field.",
                    "label": 0
                },
                {
                    "sent": "Two is sequence labeling.",
                    "label": 1
                },
                {
                    "sent": "So in this problem, our input X is a sequence of tokens.",
                    "label": 0
                },
                {
                    "sent": "For example sequence of words in a sentence and the output Y is a sequence of underlying states.",
                    "label": 0
                },
                {
                    "sent": "OK, so for example the output why might be a sequence of part of speech tags?",
                    "label": 0
                },
                {
                    "sent": "One part of speech for each word in the sentence?",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's a particular example.",
                    "label": 0
                },
                {
                    "sent": "We might have this input sentence, and this might be one candidate output sequence.",
                    "label": 0
                },
                {
                    "sent": "We have a noun, verb, determiner, preposition, and so on.",
                    "label": 0
                },
                {
                    "sent": "So a crucial idea in conditional random fields.",
                    "label": 0
                },
                {
                    "sent": "Really the building block for conditional random fields is that of feature vectors which are used to represent these sequence pairs.",
                    "label": 1
                },
                {
                    "sent": "So we have a feature vector mapping of the following form, so F. Um?",
                    "label": 1
                },
                {
                    "sent": "It's a feature vector and it takes as input an entire sentence XA position in that sentence.",
                    "label": 0
                },
                {
                    "sent": "For example, maybe we're looking at the fourth position and a state transition.",
                    "label": 0
                },
                {
                    "sent": "So a pair of tags which are adjacent in the sentence.",
                    "label": 1
                },
                {
                    "sent": "OK, now typically these feature vector representations will look at all kinds of features of the input sentence conjoined with these two tags.",
                    "label": 0
                },
                {
                    "sent": "So we might for example, be sensitive to the fact that cake is being assigned the tag noun, or maybe the tag noun is here and we have some surrounding context in the string, and so on and so on.",
                    "label": 0
                },
                {
                    "sent": "And really, one of the nice things about CRF's is the freedom with which you can define these feature vectors.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the model form for CRF looks like the following, so the optimal state sequence.",
                    "label": 0
                },
                {
                    "sent": "Why star for a particular input X is the way that maximizes the sum of terms.",
                    "label": 0
                },
                {
                    "sent": "So here I have a sum over the end positions in the sentence, so I'm summing over position by position and at each position I essentially have a state transition.",
                    "label": 0
                },
                {
                    "sent": "OK, so a pair of labels just I showed you, and each of those state transitions gets a score that's an inner product between the parameter vector W and the feature vector F. So you can think of these inner products as being a measure of the plausibility or probability of this particular state transition at this particular position in the sentence.",
                    "label": 1
                },
                {
                    "sent": "OK, we can find the highest scoring state sequence using the Viterbi algorithm, so because the last decomposes in this way, we can use dynamic programming to search through the space of possible structures efficiently.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's a CRF.",
                    "label": 0
                },
                {
                    "sent": "So the big question is, how do we generalize these serious style models to pausing?",
                    "label": 0
                },
                {
                    "sent": "So there are a couple of options, one which has been explored is to think about context free grammars.",
                    "label": 0
                },
                {
                    "sent": "The representations that context free grammars induce, and how these can be leveraged within CRF's.",
                    "label": 0
                },
                {
                    "sent": "The alternative, which I'll talk about next, is based on alternative tree adjoining grammars due to work by Arvind Joshi and many of his colleagues.",
                    "label": 1
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so tags motor tree adjoining grammars.",
                    "label": 0
                },
                {
                    "sent": "So there's really a rich literature on these grammars like I can't do anything close to justice for it.",
                    "label": 0
                },
                {
                    "sent": "Instead, I'm going to describe a fairly simple tag like formalism that we've applied recently to this particular problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so in context free grammars, the basic elements in the grammar, a set of these rewrite rules.",
                    "label": 0
                },
                {
                    "sent": "Entry adjoining grammars.",
                    "label": 0
                },
                {
                    "sent": "In contrast, the grammar is essentially defined by a set of water called elementary trees, so these look like tree fragments.",
                    "label": 1
                },
                {
                    "sent": "And in this particular case that we are using, our elementary trees are these simple spine like structures.",
                    "label": 0
                },
                {
                    "sent": "So the structures in our grammar look like a wooden lexical item, which is often called the anchor of the tree with some small sub structure above it.",
                    "label": 0
                },
                {
                    "sent": "Some small fragment of tree above it.",
                    "label": 0
                },
                {
                    "sent": "So for example, Mary has a couple of levels going up to an NP.",
                    "label": 1
                },
                {
                    "sent": "Each has a few more levels and so on.",
                    "label": 0
                },
                {
                    "sent": "So these are the basic units in tree adjoining grammars.",
                    "label": 0
                },
                {
                    "sent": "So the next question is how we build.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Build larger pieces of syntactic structure.",
                    "label": 0
                },
                {
                    "sent": "And so in general there are basic operations which can be used to combine these small tree fragments into larger pieces of structure.",
                    "label": 1
                },
                {
                    "sent": "And the one operation that we use today is something called Sister Adjunction.",
                    "label": 0
                },
                {
                    "sent": "So sister Junction basically takes two of these spines that I've just shown you and joins them together or combine them to form a larger piece of the syntactic structure.",
                    "label": 0
                },
                {
                    "sent": "In particular, this disjunction operation takes one spine called modifier, which is the Mary Spine.",
                    "label": 0
                },
                {
                    "sent": "Here another spine, which is called the head.",
                    "label": 0
                },
                {
                    "sent": "That's the spot in here.",
                    "label": 0
                },
                {
                    "sent": "In addition, it takes a position at which the modifier spine attaches underneath the head spine, so underneath this S position here.",
                    "label": 1
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is a primitive primitive operation that combines these two trees and essentially forms a rule.",
                    "label": 0
                },
                {
                    "sent": "S goes to N PvP.",
                    "label": 0
                },
                {
                    "sent": "At this point in the tree.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can keep going, and actually you can imagine a full parse tree for this full sentence.",
                    "label": 0
                },
                {
                    "sent": "Ascentia Lee being formed by a series of these kind of junction operations.",
                    "label": 0
                },
                {
                    "sent": "So later we'll see that the constraints on these junctions, operations or other costs so in regular grammars you might have some constraints on how these things can combine exactly where different elementary trees can adjoin.",
                    "label": 0
                },
                {
                    "sent": "In our case, we will simply use costs.",
                    "label": 0
                },
                {
                    "sent": "Essentially, will be learned in a CRF style model.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So intuitively, we can think about a full tree here when now decomposing this into a set of junction operations.",
                    "label": 0
                },
                {
                    "sent": "So pausing in A tag intuitively looks like a bottom up process where you first for each word in the input, choose one of these elementary trees.",
                    "label": 0
                },
                {
                    "sent": "So you choose a piece of projected structure, and then Secondly you choose how these different pieces of structure fit together to form a full parse tree.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what I might be why we be interested in these kind of grammars as an alternative, say to CFG.",
                    "label": 0
                },
                {
                    "sent": "So wanted to highlight a couple of advantages.",
                    "label": 0
                },
                {
                    "sent": "The first is the following.",
                    "label": 0
                },
                {
                    "sent": "If you think about the lexical entries in the grammar.",
                    "label": 1
                },
                {
                    "sent": "These quite naturally capture constraints associated with lexical items, so it's basically a constraint at least once or as a verb, but we see this piece of tree fragment above it, so this is quite a natural way to capture the influence of individual words on higher structure in the tree.",
                    "label": 1
                },
                {
                    "sent": "Perhaps more importantly, we can start to associate probabilities or costs with these combination operations.",
                    "label": 0
                },
                {
                    "sent": "I just spoke about.",
                    "label": 0
                },
                {
                    "sent": "So for example, one operation might be to combine these fragments associated with word eats and cake to form this structure with one of the sister junction operations I just showed you, and this can now have a probability or cost.",
                    "label": 0
                },
                {
                    "sent": "Now essentially what we're doing here when we form this structure is with choosing the word cake as the direct object of eats.",
                    "label": 0
                },
                {
                    "sent": "And by allowing parameters which can have access to this information, actually these kind of parameters have been shown time and time again to be very useful in disambiguating different power structures with choosing between different power structures.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you know to contrast with PCF.",
                    "label": 0
                },
                {
                    "sent": "Jeez, like I said in tags we associate costs or probabilities with these junction operations.",
                    "label": 0
                },
                {
                    "sent": "This contrasts with CFG's, where we associate probabilities or costs with rules.",
                    "label": 0
                },
                {
                    "sent": "So we have quite different where parameterising or thinking about the problem in these two cases.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the next trick is going to be to generalize ideas about feature vectors in conditional random fields to these adjunction operations.",
                    "label": 0
                },
                {
                    "sent": "So the models we've been using essentially map these junction operations to feature vectors.",
                    "label": 1
                },
                {
                    "sent": "So now we have a feature vector mapping which looks as follows.",
                    "label": 1
                },
                {
                    "sent": "We have an input X, which is a string.",
                    "label": 0
                },
                {
                    "sent": "And we have information which basically specifies everything I've shown you in red here.",
                    "label": 0
                },
                {
                    "sent": "So we have the indices of the head warden, the modifier word participating in this relationship.",
                    "label": 1
                },
                {
                    "sent": "So three and five.",
                    "label": 0
                },
                {
                    "sent": "In this case, the third word, and the fifth word.",
                    "label": 0
                },
                {
                    "sent": "We have the identity of those two spines.",
                    "label": 0
                },
                {
                    "sent": "Those two elementary trees that have been chosen.",
                    "label": 0
                },
                {
                    "sent": "And we have the position of where this junction operation happens, so we know that for example is NP is moving under this PP right here.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So the feature vectors could potentially track any subset of the information shown here on this slide.",
                    "label": 0
                },
                {
                    "sent": "In practice, we look at features.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The following the two words involved in this junction.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Relation, maybe information about the surrounding world.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "Maybe information about the parts of speech of that over words, verb and noun and.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Case.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in particular, subsets of the junction operation.",
                    "label": 0
                },
                {
                    "sent": "So these three non terminals are actually very useful in describing the grammatical relation.",
                    "label": 0
                },
                {
                    "sent": "In this case, a verb object relation in this particular junction operation.",
                    "label": 0
                },
                {
                    "sent": "So the feature vectors again, we can leverage the flexibility of these definitions and including quite broad features which look at all kinds of surface features of the string, as in regular CRF's combined with these junction operations.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, that's it for the model.",
                    "label": 0
                },
                {
                    "sent": "So just to sort of wrap this up, the goal as I said, is to map an input sentence X2, apostrophe, Y.",
                    "label": 0
                },
                {
                    "sent": "The model form looks rather similar to CRF's, so the optimal path tree for an input is now going to pause tree, which maximizes again some of schools.",
                    "label": 0
                },
                {
                    "sent": "But now, rather than associating schools with state transitions, we associate schools with these adjunction operations.",
                    "label": 0
                },
                {
                    "sent": "So we have a sum over the Czars, where each R is a tuple specifying one of these adjunction operations that's being used to build the tree.",
                    "label": 1
                },
                {
                    "sent": "So a tree is available, evaluated by the plausibility or score of the different junction operations which form that tree.",
                    "label": 0
                },
                {
                    "sent": "And you can compare this to model form for CRF so you can see it's fairly similar.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so how well does this model work?",
                    "label": 0
                },
                {
                    "sent": "Let me talk about some experiments, so inference and parameter estimation are things I'll come to next for training the model, we use the average Perceptron algorithm for inference.",
                    "label": 0
                },
                {
                    "sent": "We use forensic dynamic programming, which I'll talk about.",
                    "label": 0
                },
                {
                    "sent": "In terms of data, I'll talk about results on the Penn Wall Street Journal treebank.",
                    "label": 1
                },
                {
                    "sent": "This is a pretty standard valuation method metric.",
                    "label": 0
                },
                {
                    "sent": "And in terms of the evaluation metric that's being used, we can look at precision and recall in terms of recovering fragments of these piles.",
                    "label": 0
                },
                {
                    "sent": "Trees, in particular in recovering constituents within these past trees or rules within these parse trees so we can look at precision recall and F1 score in recovering these structures.",
                    "label": 1
                },
                {
                    "sent": "The main comparisons I'll make our two PCF based models.",
                    "label": 0
                },
                {
                    "sent": "So I've spoken a bit about these and let me talk.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About methods which are based on PCF just briefly so we can do the comparisons.",
                    "label": 0
                },
                {
                    "sent": "So like I said, PC FGS associate probabilities with rules in the tree and they have this problem in that in these kind of vanilla PC FGS weather very small number of nonterminals.",
                    "label": 0
                },
                {
                    "sent": "They really fail to capture a lot of the contextual dependencies you see in language.",
                    "label": 0
                },
                {
                    "sent": "And so this is.",
                    "label": 0
                },
                {
                    "sent": "This is a big problem.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The main method to deal with this has been to expand the set of non terminals in the grammar in various different ways and so here's one way, which is perhaps the simplest which is to add an annotation to each non terminal keeping track of its parent non terminal.",
                    "label": 0
                },
                {
                    "sent": "So call these parent annotations.",
                    "label": 1
                },
                {
                    "sent": "So for example.",
                    "label": 0
                },
                {
                    "sent": "Here we now have A tag which keeps track of the fact that this NP has an S right above it and similarly keep track of the fact that NP has a VP right above it.",
                    "label": 0
                },
                {
                    "sent": "So we've essentially added one level of context to the tree, and we now have these exploded nonterminals and we just estimate a regular PC FG using this method.",
                    "label": 0
                },
                {
                    "sent": "So this is perhaps the simplest method.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Why we do this?",
                    "label": 0
                },
                {
                    "sent": "Another option which is very well known is to use water called lexicalized PCF cheese and in some senses these are somewhat similar to the trade writing grammars I just showed you.",
                    "label": 0
                },
                {
                    "sent": "So in this case we add information to nonterminals, again by propagating words up through these trees.",
                    "label": 0
                },
                {
                    "sent": "OK, so for example, we propagate the word John or sore marry in various different ways and we now have additional information on these nonterminals specifying these headwords.",
                    "label": 0
                },
                {
                    "sent": "We end up with a regular PC FG again.",
                    "label": 0
                },
                {
                    "sent": "And of course we need quite a bit of care in estimating these rule production probabilities 'cause we have a very large number of nonterminals, very large number of set of possible rules and so estimating these parameters.",
                    "label": 0
                },
                {
                    "sent": "Take some care, but it can be done OK.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The final method of compared to, which is some really very interesting recent work, is due to Petra from client last year.",
                    "label": 0
                },
                {
                    "sent": "So here the idea is to gain allow the non terminals in the grammar to split in some sense.",
                    "label": 0
                },
                {
                    "sent": "Let's be refined, but to learn those splits automatically using M. So now imagine I take each non terminal, say an S non terminal and I say I can split this into 128 different possibilities.",
                    "label": 0
                },
                {
                    "sent": "Some number of possibilities.",
                    "label": 0
                },
                {
                    "sent": "Now these splits are latent there not observed in the training data.",
                    "label": 0
                },
                {
                    "sent": "And so we can use them essentially to recover these latent annotations.",
                    "label": 0
                },
                {
                    "sent": "And to really make this work, I think this is one major insider petrol from Klein.",
                    "label": 0
                },
                {
                    "sent": "You need ways of allowing different nonterminals to split to different degrees.",
                    "label": 0
                },
                {
                    "sent": "You need some kind of model selection and this is quite quite effective.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me give you some results and some main messages of these results, so parent annotation method certainly works better than a ropy CFG, but still has about 20% error in terms of recovering these constituents.",
                    "label": 0
                },
                {
                    "sent": "So that's evidence that you know even the simple well, even this method for refining on terminals doesn't work too well.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We compared to Lexicalized PC FGS.",
                    "label": 0
                },
                {
                    "sent": "We can see that there's about a 25% reduction in error if we look at the model, so the tag based model is down here.",
                    "label": 0
                },
                {
                    "sent": "And this is probably primarily due to the features which we can include in this model, which are really quite difficult to include.",
                    "label": 0
                },
                {
                    "sent": "And let's close PCF.",
                    "label": 0
                },
                {
                    "sent": "Geez, so we really leverage this idea of CLF style features to include Surface features and include various overlapping features which are very difficult to get into PC fegs.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finally, if we compared to petrol incline, we still see some improvements over their work, although the results are closer.",
                    "label": 0
                },
                {
                    "sent": "I think an interesting area for future work is to try to combine these two methods, for example using state splitting approaches for the tag based grammars.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "I'll get an interlude.",
                    "label": 0
                },
                {
                    "sent": "I wanted to talk a little bit about machine translation as one application of these kind of models.",
                    "label": 0
                },
                {
                    "sent": "An interesting thing here is that some of the techniques I've shown you in pausing and now being carried across to.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Machine learning for machine translation.",
                    "label": 1
                },
                {
                    "sent": "So in statistical approaches to translation again, we think of this as a structured prediction problem where we're mapping strings and, say, German to strings in English, and we have potentially a very large set of example translations.",
                    "label": 1
                },
                {
                    "sent": "Typically you might have a few 100,000 examples of translations between the two languages.",
                    "label": 0
                },
                {
                    "sent": "Now you could think of two possible approaches.",
                    "label": 0
                },
                {
                    "sent": "Roughly speaking, one an approach where you try to learn this mapping directly and you don't make so much use of latent structure.",
                    "label": 1
                },
                {
                    "sent": "A second possible approaches where you might use these syntactic structures.",
                    "label": 0
                },
                {
                    "sent": "Maybe these grammatical relations as a level of intermediate structure in performing this mapping.",
                    "label": 0
                },
                {
                    "sent": "So let's talk.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These two approaches, so the first is phrase based translation, so this was a major breakthrough in statistical machine translation, and it's really impressive how well these models have worked in phrase based models.",
                    "label": 0
                },
                {
                    "sent": "The basic component or the basic entries in the model is a lexecon of so-called phrase pairs.",
                    "label": 1
                },
                {
                    "sent": "So each entry in this lexecon looks like a German substring, for example in Denigan are paired with an English substring over here.",
                    "label": 0
                },
                {
                    "sent": "And we might potentially induce a very large lexecon of entries of this form from a set of example translations.",
                    "label": 1
                },
                {
                    "sent": "Now, given this lexecon, translation proceeds.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The following 2 steps.",
                    "label": 0
                },
                {
                    "sent": "So firstly we segment the input German string into a series of German segments and for each of those segments we choose an English translation.",
                    "label": 0
                },
                {
                    "sent": "So I've shown you this step.",
                    "label": 0
                },
                {
                    "sent": "Here we have a bunch of we have a segmentation and now we have a bunch of English fragments down here.",
                    "label": 0
                },
                {
                    "sent": "The translations withdrawn from this lexecon.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the second step, we choose some ordering of the resulting English phrases, so we might move things around a little bit.",
                    "label": 1
                },
                {
                    "sent": "So in particular, the subject here elections have come after the verb take in German.",
                    "label": 0
                },
                {
                    "sent": "German has quite flexible word order, so you often see this kind of thing was the subject always has to appear before the verb in English, and so we sort of scrambled these phrases, rearrange them to produce this new translation.",
                    "label": 0
                },
                {
                    "sent": "Now these two steps, the choice of the phrases and then the choice of the ordering of course had probabilities or associated costs which are somehow estimated from a from a corpus.",
                    "label": 0
                },
                {
                    "sent": "In particular, in the reordering step you make heavy use of a language model of prior model over strings in the language.",
                    "label": 0
                },
                {
                    "sent": "For example, the trigram language model.",
                    "label": 0
                },
                {
                    "sent": "So these models have gotten along way in translation, but they really don't make any direct use of syntactic information.",
                    "label": 0
                },
                {
                    "sent": "You might think, for example, that knowing where the different argument.",
                    "label": 0
                },
                {
                    "sent": "Arguments, whether different grammatical relations are on the German side might be useful in predicting the structure of the English tree.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So recent work for example at ISI and DBN 2 examples I've shown you here has started to frame the translation process essentially as opposing task.",
                    "label": 0
                },
                {
                    "sent": "It's really really very interesting take, I think on translation and have shown very really quite impressive results, particularly in Chinese, which is language which has very different word order from English.",
                    "label": 0
                },
                {
                    "sent": "So in this idea the phrases are augmented to carry with them pieces of syntactic structure.",
                    "label": 1
                },
                {
                    "sent": "So now a phrase entry in R model might pair this German string with an English string which has a piece of tree fragment above it.",
                    "label": 0
                },
                {
                    "sent": "So maybe we have paused the English corpus and annotated our English strings with these paltry fragments.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we have this additional information.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And now we gain have a two step process in translating a sentence.",
                    "label": 0
                },
                {
                    "sent": "So the first step is to segment the German sentence and again to choose a set of phrasal entries.",
                    "label": 1
                },
                {
                    "sent": "But now we have a set of these or sequence of these syntactic fragments.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have a sequence of these tree fragments.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The second step is to somehow assemble these tree fragments into a full parse tree.",
                    "label": 1
                },
                {
                    "sent": "So that essentially involves something very similar to the adjunction operations I showed you earlier in the talk.",
                    "label": 0
                },
                {
                    "sent": "When we start to fit these tree structures together with some costs and recover a in English pause tree.",
                    "label": 1
                },
                {
                    "sent": "And some reordering is allowed.",
                    "label": 0
                },
                {
                    "sent": "So we are actually allowed to move around the order.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With these different fragments.",
                    "label": 0
                },
                {
                    "sent": "So finally we get a translation.",
                    "label": 0
                },
                {
                    "sent": "The output of the system is this English poetry.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's sort of a sketch of this model.",
                    "label": 0
                },
                {
                    "sent": "A couple of things about this.",
                    "label": 0
                },
                {
                    "sent": "So what is that the search pro?",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So again, we're going to have some process that first searches for these fragments and then.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Looks at how to combine them.",
                    "label": 0
                },
                {
                    "sent": "This can be implemented using essentially extensions of algorithms used for regular parsing, and so this is what I mean by translation as a pausing problem.",
                    "label": 1
                },
                {
                    "sent": "We essentially.",
                    "label": 1
                },
                {
                    "sent": "Our formulate the translation problem is decoding under one of these pausing models.",
                    "label": 1
                },
                {
                    "sent": "And there's some potential advantages which have been shown in this work.",
                    "label": 0
                },
                {
                    "sent": "One is that you build an English pause tree and thereby have some control over dramatic aliti of that tree.",
                    "label": 0
                },
                {
                    "sent": "So you have some control over the fact that the verb needs a subject and an object, and all these different things.",
                    "label": 0
                },
                {
                    "sent": "The second thing is that in capturing the difference in Word order between 2 languages, for example, if we go back here, the fact that the subject comes after the verb in German before it in English, we can capture those kind of effects by reordering on these piles, trees which is kind of a natural way to capture world radio.",
                    "label": 0
                },
                {
                    "sent": "Reordering in the two languages.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's translation.",
                    "label": 0
                },
                {
                    "sent": "Let me now go to.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So inference under these models.",
                    "label": 0
                },
                {
                    "sent": "So one question I lead left open was how we find the most likely parse tree under the model I showed you.",
                    "label": 0
                },
                {
                    "sent": "So here again I've shown you the model form where a pause tree is scored by the sum of scores of these different junction operations.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So some main points of this part of the talk.",
                    "label": 0
                },
                {
                    "sent": "So dynamic programming algorithms exist for tank grammars, which actually or at least for the tank grammars I've shown.",
                    "label": 0
                },
                {
                    "sent": "You have very similar runtime to grammar algorithms for PCF Geez.",
                    "label": 0
                },
                {
                    "sent": "So for example cubic time runtime, where in the length of the sentence?",
                    "label": 0
                },
                {
                    "sent": "So that's somewhat promising.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, exact inference under these models is still very expensive.",
                    "label": 1
                },
                {
                    "sent": "Actually, prohibitively expensive.",
                    "label": 0
                },
                {
                    "sent": "You can easily spend minutes pausing a sentence.",
                    "label": 0
                },
                {
                    "sent": "An inference is a bottleneck both in applying the model to new sentences and also during training of the model.",
                    "label": 0
                },
                {
                    "sent": "OK, because the training algorithms will see for conditional random fields generally require decoding the training examples multiple times, and so inference is a bottleneck in training.",
                    "label": 0
                },
                {
                    "sent": "So one solution which has been found effective many times in the pausing literature is to use an idea called course.",
                    "label": 0
                },
                {
                    "sent": "Define dynamic programming.",
                    "label": 0
                },
                {
                    "sent": "For example, Czarniak has been using this for a long time and pausing models the basic idea here is to use a first pass model, which is a simple computationally cheap model to restrict the search space of the full parsing model and this can actually be remarkably effective in the models.",
                    "label": 1
                },
                {
                    "sent": "Will see.",
                    "label": 0
                },
                {
                    "sent": "So let me describe this idea.",
                    "label": 0
                },
                {
                    "sent": "So to understand.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The algorithms will actually go to another syntactic formalism.",
                    "label": 0
                },
                {
                    "sent": "We've seen context free grammars.",
                    "label": 0
                },
                {
                    "sent": "We've seen tags.",
                    "label": 0
                },
                {
                    "sent": "Now I'm going to talk a little bit about dependency grammars or dependency representations.",
                    "label": 0
                },
                {
                    "sent": "So the top of this slide is a dependency structure, which can be thought of as an alternative representation to context free trees.",
                    "label": 1
                },
                {
                    "sent": "So it basically is formed by a directed graph which forms a tree with this special node called star, the root node.",
                    "label": 0
                },
                {
                    "sent": "Sorry Malta star at the root of this tree.",
                    "label": 0
                },
                {
                    "sent": "And we have arcs between basically heads in their modifyers.",
                    "label": 0
                },
                {
                    "sent": "So I have a dependency for example between John's or basically corresponding to a subject verb dependency.",
                    "label": 1
                },
                {
                    "sent": "And the simplest form of these dependency structures there, just as I've shown you there.",
                    "label": 0
                },
                {
                    "sent": "In a slightly more refined form, you might have labels on these arcs, so you might actually have a label saying subject or label.",
                    "label": 0
                },
                {
                    "sent": "Saying movie is the object of sore and so on and so on.",
                    "label": 0
                },
                {
                    "sent": "So this can be thought of as an alternative representation.",
                    "label": 0
                },
                {
                    "sent": "It's again quite widely used in natural natural language community.",
                    "label": 1
                },
                {
                    "sent": "Another way of representing syntactic structures.",
                    "label": 0
                },
                {
                    "sent": "So a few years ago, Ryan McDonald's, an collaborators, came up with CRF style models for dependency parsing.",
                    "label": 0
                },
                {
                    "sent": "So now we can score these structures using again a CRF style model.",
                    "label": 0
                },
                {
                    "sent": "So the score of any structure is going to be a sum over the dependencies within that structure, and then each dependency gets a school, which is just another inner product.",
                    "label": 0
                },
                {
                    "sent": "Features now track dependencies and potentially any information about the input sentence.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "So one thing which is remarkable remarkable about these dependency structures is that they have very efficient decoding algorithms.",
                    "label": 0
                },
                {
                    "sent": "Juza Jason Eisner.",
                    "label": 0
                },
                {
                    "sent": "So for the structures I've just shown you, which have no labels, the most probable or lowest cost dependency structure under this kind of model can be found in cubic time in the length of the sentence.",
                    "label": 1
                },
                {
                    "sent": "With very low constants.",
                    "label": 0
                },
                {
                    "sent": "And the algorithms and it came up with a really quite beautiful algorithms that dynamic programming algorithms and they decompose dependency structures into subparts and quite ingenious way and a kind of dynamic programming bottom up search so it looks rather different from, say, passing the context free grammar, but the complexity is actually very similar to CFG's, which are also cubic in the length of the sentence.",
                    "label": 0
                },
                {
                    "sent": "McDonald's our leverage this to great effect in training up discriminative models.",
                    "label": 0
                },
                {
                    "sent": "Based on these dependency representations.",
                    "label": 0
                },
                {
                    "sent": "In fact, these representations are efficient enough that exhaustive pausing is really quite quite plausable, quite feasible.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what does this fit with the tank structures that I've shown you?",
                    "label": 0
                },
                {
                    "sent": "So if you think about these tag structures, they look very much like dependency structures with some additional information.",
                    "label": 0
                },
                {
                    "sent": "So essentially dependencies structures augmented with these spines and also invented with these positions of these junction points in these spines.",
                    "label": 1
                },
                {
                    "sent": "So for example, there is essentially a dependency between the words with and almonds, but now it's mediated by these two spines.",
                    "label": 0
                },
                {
                    "sent": "In this junction position.",
                    "label": 1
                },
                {
                    "sent": "So you can think of tank structures as being more elaborate dependency structures.",
                    "label": 0
                },
                {
                    "sent": "Essentially, there's a close connection between the two.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And that actually means that aizen's algorithms can also be applied to this formalism.",
                    "label": 0
                },
                {
                    "sent": "So most probable lowest cost pause under these tag grammars can again be found in cubic time in the length of the sentence, again in time, which is essentially the same as pausing a context free grammar.",
                    "label": 0
                },
                {
                    "sent": "We now have a grammar constant, so this constant G is actually a function of various things, mainly the number of possible spines for any potential word, and the maximum height of any spine.",
                    "label": 1
                },
                {
                    "sent": "So G can be thought of as a measure of the size of the grammar.",
                    "label": 0
                },
                {
                    "sent": "And as an additional constant in this runtime.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's extremely useful, but it doesn't quite get us there.",
                    "label": 0
                },
                {
                    "sent": "So the problem is that this grammar constant is prohibitive.",
                    "label": 1
                },
                {
                    "sent": "We can easily have situations where G is 1000 or even 10,000.",
                    "label": 1
                },
                {
                    "sent": "Potentially more so the grammar constant is really a cure in decoding with these models.",
                    "label": 0
                },
                {
                    "sent": "So the solution is as I said, to try to use a simpler model.",
                    "label": 0
                },
                {
                    "sent": "In our case, one of McDonald's dependency parsing models to prune the space of the full parser and the simple model has a much lower constant, say roughly equal to the number of non terminals in the grammar and is thereby efficient.",
                    "label": 1
                },
                {
                    "sent": "It can be applied efficiently.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me just elaborate on this idea a little bit.",
                    "label": 0
                },
                {
                    "sent": "So if we think of any adjunction operation in the textile grammar.",
                    "label": 0
                },
                {
                    "sent": "That essentially implies a dependency between 2 words.",
                    "label": 0
                },
                {
                    "sent": "So here again I have the example eaten cake modifying each other.",
                    "label": 0
                },
                {
                    "sent": "And we can see that essentially this implies a dependency between eaten cake.",
                    "label": 0
                },
                {
                    "sent": "Now it's a label dependency.",
                    "label": 0
                },
                {
                    "sent": "What I've done here is I pulled out these three labels that are involved here.",
                    "label": 0
                },
                {
                    "sent": "OK, so I now have a label doc.",
                    "label": 0
                },
                {
                    "sent": "And actually we build 3 different dependency parsing models similar to McDonald's, one for each of these three labels.",
                    "label": 0
                },
                {
                    "sent": "And these simpler models can essentially estimate the posterior probability of dependencies with these labels.",
                    "label": 0
                },
                {
                    "sent": "These these three different labels.",
                    "label": 0
                },
                {
                    "sent": "So now we have a way of scoring potential dependencies using a model with a much lower grammar constant.",
                    "label": 0
                },
                {
                    "sent": "And we can basically discard dependencies which have very low probability.",
                    "label": 0
                },
                {
                    "sent": "So the basic idea is to only allow junctions in the textile model which correspond to dependencies that have reasonably high probability under the similar models.",
                    "label": 1
                },
                {
                    "sent": "So let me show you some results for this.",
                    "label": 0
                },
                {
                    "sent": "For this kind of approach.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the road to look at is the last row.",
                    "label": 0
                },
                {
                    "sent": "This is the beam size we used.",
                    "label": 1
                },
                {
                    "sent": "So we can basically throw away all but about .3% of these potential junction points.",
                    "label": 0
                },
                {
                    "sent": "And still end up in a situation where an Oracle parser so a search process that sort of found the best tree possible.",
                    "label": 1
                },
                {
                    "sent": "Under that constraint recovered about 98 1/2% of the constituents.",
                    "label": 0
                },
                {
                    "sent": "OK, so we've vastly reduce the search space while still enabling a positive.",
                    "label": 0
                },
                {
                    "sent": "Potentially get very close to 100% accuracy.",
                    "label": 0
                },
                {
                    "sent": "So the main message here, I guess, is that these kind of course defined approach is a very effective in dynamic programming problems and have been found to be very effective in these.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Causing problems.",
                    "label": 0
                },
                {
                    "sent": "OK, so the last part of the talk I want to talk about optimization.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learning.",
                    "label": 0
                },
                {
                    "sent": "So the question is, how do we learn these parameter vectors W. Ann?",
                    "label": 0
                },
                {
                    "sent": "For simplicity, I'll go back to basic conditional random fields where feature vectors are associated with state transitions.",
                    "label": 0
                },
                {
                    "sent": "Just because I think people are more familiar with this.",
                    "label": 0
                },
                {
                    "sent": "So how do we?",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the parameters W. So gain efficiency is a bottleneck, so pausing and other problems in LP we often have many training samples, and in addition we have these structured problems where we'll have to perform inference again and again on these training examples.",
                    "label": 1
                },
                {
                    "sent": "So our experience is being very similar to a lot of experience machine learning, going back to stochastic gradient descent, training of neural networks, namely that online algorithms are often much more efficient than batch gradient methods so online at style algorithms which visit a single training example at a time and do an update on just that training example.",
                    "label": 1
                },
                {
                    "sent": "So I actually described two online algorithms that we use.",
                    "label": 0
                },
                {
                    "sent": "One is the perceptron or an average variant of it, and the 2nd is a method for training CRF's based on exponentiated gradient algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so here's the perceptron.",
                    "label": 0
                },
                {
                    "sent": "So this is a straightforward generalization of the regular perceptron algorithm as devised for classification to these structured problems.",
                    "label": 0
                },
                {
                    "sent": "So the method looks like as follows.",
                    "label": 0
                },
                {
                    "sent": "We initialize our parameters to 0.",
                    "label": 0
                },
                {
                    "sent": "And then we make some number of passes over the training set.",
                    "label": 0
                },
                {
                    "sent": "So we make big T passes over the training set.",
                    "label": 0
                },
                {
                    "sent": "T is typically fairly small, say 510 or 20.",
                    "label": 0
                },
                {
                    "sent": "So in each pass over the training set, we iterate over the training examples.",
                    "label": 0
                },
                {
                    "sent": "So we go over the training samples one by one.",
                    "label": 0
                },
                {
                    "sent": "Each training example is an XY pair, where X is a sentence.",
                    "label": 1
                },
                {
                    "sent": "And why is a sequence of labels?",
                    "label": 0
                },
                {
                    "sent": "Remember, we're doing a sequence labeling problem here.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So the first thing we do is we decode under the current model parameters.",
                    "label": 0
                },
                {
                    "sent": "So we choose Z to be state sequence that is most likely under our current parameters W. Now see is correct.",
                    "label": 0
                },
                {
                    "sent": "If it matches why we are conservative and we do know update to the parameters we leave them untouched.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if we make a mistake, if Z is not equal to why we do the following update?",
                    "label": 0
                },
                {
                    "sent": "So we say the new parameters W of the old parameters and then we have two terms.",
                    "label": 0
                },
                {
                    "sent": "The first term just looks like a summer feature.",
                    "label": 0
                },
                {
                    "sent": "Vectors in the truth, the target label Y and the second term looks like a summer feature vectors for all the state transitions in the incorrectly proposed state sequence said so intuitively, we're going to raise the weight of any parameters seen in the truth the target.",
                    "label": 0
                },
                {
                    "sent": "Decrease the weight in any parameters seen in this competitive sequence that we've just erroneously produced.",
                    "label": 0
                },
                {
                    "sent": "And then we just return W. So.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's the.",
                    "label": 0
                },
                {
                    "sent": "The structured perceptron.",
                    "label": 0
                },
                {
                    "sent": "A very important modification to it is averaging, so this was first introduced by Foreigner Japeri, essentially as a heuristic approximation to voting and voting has formal guarantees.",
                    "label": 0
                },
                {
                    "sent": "Averaging, I think, is much less well understood.",
                    "label": 0
                },
                {
                    "sent": "So and averaging we do the following in addition to the regular weight vector I showed you, we keep around the second vector of parameters, called the average parameters, and these are again additional set initially to 0.",
                    "label": 0
                },
                {
                    "sent": "And then we basically use these parameters to keep it running average.",
                    "label": 0
                },
                {
                    "sent": "So at every point we take average parameters and add in W. So we add in the parameter vector after the training example, whether we've done an update or not.",
                    "label": 0
                },
                {
                    "sent": "And finally we re scale the average parameters.",
                    "label": 0
                },
                {
                    "sent": "So we really have an average.",
                    "label": 0
                },
                {
                    "sent": "The rescaling step really isn't important, but will include it here.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the average variance of the set.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So a few notes on this, so in terms of theoretical guarantees, if there exists some parameter vector which correctly classifieds all the data, then you can show the perception will converge parameter values which which achieved that goal.",
                    "label": 0
                },
                {
                    "sent": "In addition, the number of errors it makes a number of updates IT users before it reaches a separating hyperplane is based on a margin, just as it's based on the margin.",
                    "label": 0
                },
                {
                    "sent": "In regular classification problems we can also relate these margin based properties to generalization properties.",
                    "label": 1
                },
                {
                    "sent": "Price, most importantly, let's talk about a few practical findings with the perceptron.",
                    "label": 0
                },
                {
                    "sent": "One thing we found time and time again is that averaging improves performance a lot.",
                    "label": 0
                },
                {
                    "sent": "Without averaging, the perceptron is a pretty crummy algorithm on these problems.",
                    "label": 0
                },
                {
                    "sent": "With averaging, it's very competitive.",
                    "label": 1
                },
                {
                    "sent": "In fact, it often performs nearly as well as full CRF training or Max margin networks, or often performs as well as these other training methods.",
                    "label": 0
                },
                {
                    "sent": "It typically reaches a good solution after only a few, say 5 iterations, which is very handy in these large scale problems.",
                    "label": 1
                },
                {
                    "sent": "Something else which is maybe underappreciated is that it returns relatively sparse solutions, so because these updates only depend on two sequences at every point, and because we only make a relatively small number of passes over the training set, it actually leaves a lot of the parameters equal to 0, so it returns pretty sparse solutions.",
                    "label": 0
                },
                {
                    "sent": "Again, a very useful property when you have high dimensional feature spaces.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is convergence actually on the pausing test just to illustrate the kind of speed of convergence you see with the Perceptron, here's the iteration versus accuracy.",
                    "label": 0
                },
                {
                    "sent": "You can see after five or six iterations it's almost there and you know sort of gets a bit further if we go beyond 10 iterations.",
                    "label": 0
                },
                {
                    "sent": "But the main point here is that it can often converge.",
                    "label": 0
                },
                {
                    "sent": "You often get to a very decent solution and barely five or six iterations.",
                    "label": 0
                },
                {
                    "sent": "This is test data.",
                    "label": 0
                },
                {
                    "sent": "This is valid.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is validation data.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me conclude talk about another training algorithm, which also has an online flavor, but which it looks rather different from the perceptron.",
                    "label": 0
                },
                {
                    "sent": "So say we'd like to train a regular regularised log likelihood objective, which is commonly used in training conditional random fields.",
                    "label": 0
                },
                {
                    "sent": "Let me just show you how to define this.",
                    "label": 0
                },
                {
                    "sent": "So another piece of notation will say F of X&Y is a feature vector associated with an entire sequence.",
                    "label": 0
                },
                {
                    "sent": "It's just a sum over the individual components.",
                    "label": 0
                },
                {
                    "sent": "Individual straight state transitions in that sequence.",
                    "label": 0
                },
                {
                    "sent": "Given this, we can define a conditional distribution over structures Y, which just looks like a regular Gibbs style distribution.",
                    "label": 0
                },
                {
                    "sent": "So we have an inner product in the exponent, then a partition function.",
                    "label": 0
                },
                {
                    "sent": "So the objective will talk about minimizing.",
                    "label": 0
                },
                {
                    "sent": "Looks like a combination of the negative log likelihood under this kind of model and two norm penalty or regularizer.",
                    "label": 0
                },
                {
                    "sent": "OK often you'd have some constant here dictating the level of regularization in the model.",
                    "label": 0
                },
                {
                    "sent": "For simplicity we'll just take this to be half in this part of the talk.",
                    "label": 0
                },
                {
                    "sent": "So how do we minimize this?",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Function.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to talk about an algorithm that makes use of a Joule fulmination.",
                    "label": 0
                },
                {
                    "sent": "So the dual is going to make use of a set of variables Alpha variables.",
                    "label": 0
                },
                {
                    "sent": "So Alpha Ky is a dual variable where K ranges over all training samples and why ranges over all possible structures.",
                    "label": 1
                },
                {
                    "sent": "For that case.",
                    "label": 0
                },
                {
                    "sent": "Training example.",
                    "label": 0
                },
                {
                    "sent": "OK, so for now we're going to have a huge set of these dual variables will come later to how this can be dealt with.",
                    "label": 0
                },
                {
                    "sent": "So Alpha Ky is a dual variable associated with a particular sequence on a particular training example.",
                    "label": 0
                },
                {
                    "sent": "We'll define a mapping from these two variables to primal variables as follows, so W of Alpha is going to look like a sum over the feature vectors for the true labels, YK?",
                    "label": 0
                },
                {
                    "sent": "Minus and this looks like an expectation.",
                    "label": 0
                },
                {
                    "sent": "OK, so these alphas are actually constrained lie between zero and one.",
                    "label": 0
                },
                {
                    "sent": "And for any particular training example K they sum to one.",
                    "label": 0
                },
                {
                    "sent": "So they essentially defined distributions over these examples.",
                    "label": 0
                },
                {
                    "sent": "So here we essentially have an expectation.",
                    "label": 0
                },
                {
                    "sent": "This looks like an expected feature vector under these Alpha variables.",
                    "label": 0
                },
                {
                    "sent": "So the dual objective is to minimize this function Q of Alpha under the simplex constraints that I've just shown you.",
                    "label": 0
                },
                {
                    "sent": "And the dual looks like a sum of two terms.",
                    "label": 0
                },
                {
                    "sent": "The second term is Ascentia Lee the regularizer term.",
                    "label": 0
                },
                {
                    "sent": "So we have this function of the alphas.",
                    "label": 0
                },
                {
                    "sent": "This primal parameter and we have the two norm of that.",
                    "label": 0
                },
                {
                    "sent": "The first term is a kind of entropic term, so this looks like the entropy entropy of these Alpha Ky variables.",
                    "label": 0
                },
                {
                    "sent": "OK, and I'm a convex duality.",
                    "label": 0
                },
                {
                    "sent": "You can show that if you optimize queue.",
                    "label": 0
                },
                {
                    "sent": "So if you find the Alpha that minimizes queue, you can recover the optimal primal parameters so you can recover the solution to this problem just through the mapping I've shown you here.",
                    "label": 0
                },
                {
                    "sent": "OK, So what is this by this so bias?",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm going to talk about methods which use dual coordinate descent, so this is rather similar.",
                    "label": 0
                },
                {
                    "sent": "For example to Smol classic algorithm for training support vector machines.",
                    "label": 0
                },
                {
                    "sent": "The basic idea is going to be to choose a single training example at a time and update the dual variables on just that one training example.",
                    "label": 1
                },
                {
                    "sent": "So, for example, we might look at the two variables in that one training example and try to explicitly optimize Q with respect to just those dual variables.",
                    "label": 0
                },
                {
                    "sent": "So this method has an online flavor in that we're going to visit a single training example at a time, and as we'll see, this has advantages in terms of speed of convergence.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, it has a nice property, which is that it's simple enough to show that you can directly measure the impact.",
                    "label": 0
                },
                {
                    "sent": "Of any changes you make to those two variables.",
                    "label": 0
                },
                {
                    "sent": "So actually, if I make a change, the Jew variables on a particular example by reference to that example alone, I can see whether the dual objective has increased or decreased, and this allows us, for example, to do line search for a learning rate or to explicitly choose the step size in various different optimization.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Steps.",
                    "label": 0
                },
                {
                    "sent": "OK, so the method we use is based on exponentiated gradient updates.",
                    "label": 1
                },
                {
                    "sent": "So these are popular, popularized by Kevin and Moose for example in online learning.",
                    "label": 0
                },
                {
                    "sent": "Here we're going to apply these to this particular optimization task, so we have this dual objective.",
                    "label": 0
                },
                {
                    "sent": "As initial values will choose, these Alpha Ky values to sum values such that they sum to 1, they're in the simplex constraints that I showed you, but they're strictly positive they must be non 0.",
                    "label": 0
                },
                {
                    "sent": "We choose some learning rates and positive learning rate Ada.",
                    "label": 1
                },
                {
                    "sent": "And then the algorithm proceeds as follows.",
                    "label": 0
                },
                {
                    "sent": "So we take T steps and at each step we choose a training example uniformly at random.",
                    "label": 1
                },
                {
                    "sent": "And actually we found it was critical both in the proofs and also in the experimental results to choose examples at random rather than for example cycling over the training samples in the same order repeatedly.",
                    "label": 0
                },
                {
                    "sent": "So we choose a training example at random, and then we do these e.g updates.",
                    "label": 0
                },
                {
                    "sent": "So what are these look like?",
                    "label": 0
                },
                {
                    "sent": "Well, they're driven by the gradient, so natural OK, why is just the derivative of Q with respect to the dual variable for Ky?",
                    "label": 0
                },
                {
                    "sent": "And the updates look like the following.",
                    "label": 0
                },
                {
                    "sent": "We have the new Alpha is equal to the old alphas and then we have this gradient with the learning rate in the exponent here and these as I said we're in these kind of updates were introduced by Kevin Warmath.",
                    "label": 0
                },
                {
                    "sent": "So one obvious property of these updates, given the normalization term here, is that they preserve the simplex constraints.",
                    "label": 0
                },
                {
                    "sent": "The alphas stay in the.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Plex so let me sketch some results that we found for these kind of updates.",
                    "label": 0
                },
                {
                    "sent": "So one is in terms of rate of convergence.",
                    "label": 0
                },
                {
                    "sent": "It's fairly efficient, so to get within epsilon of the optimal dual value for the.",
                    "label": 1
                },
                {
                    "sent": "The jewel that I just showed you.",
                    "label": 0
                },
                {
                    "sent": "Essentially need order log one over epsilon updates, so this is the linear convergence.",
                    "label": 0
                },
                {
                    "sent": "In addition, our results have found that online updates have faster convergence than batch methods, both in the theoretical bounds and also in our practical results so.",
                    "label": 1
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Each update would perform exactly these updates, but would update all of the Alpha variables at each.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yep, OK. Um?",
                    "label": 0
                },
                {
                    "sent": "The next question is how do we deal with the fact that we have one dual variable for every possible sequence for every possible training example, we have an exponential number of dual variables.",
                    "label": 0
                },
                {
                    "sent": "So it turns out that there is a compact and efficient implementation of this algorithm.",
                    "label": 0
                },
                {
                    "sent": "I'll just sketch how this is done here.",
                    "label": 0
                },
                {
                    "sent": "So instead of representing these Alpha variables explicitly, we make use of an alternative parameterization.",
                    "label": 0
                },
                {
                    "sent": "So we now have alternative dual variable stator, and each theater is associated with the training example K with a position I and with the potential state transition.",
                    "label": 0
                },
                {
                    "sent": "So it sort of mirrors feature vectors in terms of where these things are defined.",
                    "label": 0
                },
                {
                    "sent": "So these are alternative dual variables that can take any value in the reals.",
                    "label": 1
                },
                {
                    "sent": "And then we define these Alpha variables basically as it gives style distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, so this dual variable is now some over these thetas divided by a partition function.",
                    "label": 0
                },
                {
                    "sent": "So we now have polynomial number of these alternative dual variables.",
                    "label": 0
                },
                {
                    "sent": "Now it turns out that the algorithm I just I've just shown you collapses in that you can reframe everything in terms of updates to these data variables, so you never need to represent the Alpha variables explicitly.",
                    "label": 1
                },
                {
                    "sent": "The state's can be updated.",
                    "label": 0
                },
                {
                    "sent": "And the main cost in implementing this algorithm is then the forward backward algorithm.",
                    "label": 1
                },
                {
                    "sent": "So it turns out that we need to be able to compute marginals under these kinds of distributions, and we can do that using forward, backward.",
                    "label": 0
                },
                {
                    "sent": "So those are the two ideas.",
                    "label": 0
                },
                {
                    "sent": "One this kind of compact representation of the dual variables and two re framing algorithm in terms of updates.",
                    "label": 0
                },
                {
                    "sent": "These dual variables with repeated forward backward steps at each point to complete the picture.",
                    "label": 0
                },
                {
                    "sent": "These methods are quite general.",
                    "label": 0
                },
                {
                    "sent": "They can also be applied, for example, to the pausing problem where similar algorithms to forward backward also exist.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me just talk about some results, then will finish up.",
                    "label": 0
                },
                {
                    "sent": "So here are some comparisons to LBF so this is a conjugate gradient style optimizer.",
                    "label": 0
                },
                {
                    "sent": "It's a batch gradient method.",
                    "label": 0
                },
                {
                    "sent": "It's a very popular method for optimizing CRF's.",
                    "label": 0
                },
                {
                    "sent": "And it's certainly not online.",
                    "label": 0
                },
                {
                    "sent": "As I said, it's batch.",
                    "label": 0
                },
                {
                    "sent": "So here is a graph showing convergence for the G algorithm in red.",
                    "label": 0
                },
                {
                    "sent": "Now be FGS in green.",
                    "label": 0
                },
                {
                    "sent": "You can see it's quite a bit quicker, so this is for particular regularizer constant.",
                    "label": 1
                },
                {
                    "sent": "We found these kind of improvements across the board on the different.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are constants.",
                    "label": 0
                },
                {
                    "sent": "His convergence in terms of test error, so the red line shows actually sorry, accuracy.",
                    "label": 0
                },
                {
                    "sent": "Sorry this is on the dependency parsing problem and again we see in terms of parsing accuracy.",
                    "label": 0
                },
                {
                    "sent": "Convergence is in roughly 20 or so iterations.",
                    "label": 0
                },
                {
                    "sent": "It's quite a bit slower for LB FGS under a variety of regularization constants.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that pretty much ties it up.",
                    "label": 0
                },
                {
                    "sent": "I just want to give some conclusions so we think about the three sections of the talk.",
                    "label": 0
                },
                {
                    "sent": "The first part was on models and I guess the point I want to make here is that weighted grammars, for example PC FGS or tags or dependency grammars offer useful generalizations of Hmm's.",
                    "label": 1
                },
                {
                    "sent": "So probably certainly a very familiar set of models within this Community of graphical models.",
                    "label": 0
                },
                {
                    "sent": "Grammars are probably.",
                    "label": 1
                },
                {
                    "sent": "Grammatical models from graphical models and quite different set of algorithms.",
                    "label": 1
                },
                {
                    "sent": "The second point is really that lexicalized grammars, for example tanks or dependency grammars lead to interesting alternative parameters.",
                    "label": 1
                },
                {
                    "sent": "Find methods very effective, at least in these kind of causing problems in making pausing efficient.",
                    "label": 0
                },
                {
                    "sent": "And finally, again in parameter estimation, it seems online algorithms are key, but the average perceptron and e.g algorithms take barely 5 or 10 iterations over training set for conversions.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "So a model for what, sorry?",
                    "label": 0
                },
                {
                    "sent": "Should I should I?",
                    "label": 0
                },
                {
                    "sent": "My question is rather general, is it possible?",
                    "label": 0
                },
                {
                    "sent": "What do you think?",
                    "label": 0
                },
                {
                    "sent": "Is it realistic to create a unified language model?",
                    "label": 0
                },
                {
                    "sent": "For example, all the grammars that you that you mean their language dependent.",
                    "label": 0
                },
                {
                    "sent": "Is it possible to create a drama that are language independent?",
                    "label": 0
                },
                {
                    "sent": "Oh gosh.",
                    "label": 0
                },
                {
                    "sent": "I think the quote has been working a lot of languages, so typically the kind of tree banks like the Wall Street Journal treebank have been built in many other languages.",
                    "label": 0
                },
                {
                    "sent": "Chinese, Arabic, many of the European languages, and so on.",
                    "label": 0
                },
                {
                    "sent": "Having these types of models have been shown to transfer to those different training sets quite effectively.",
                    "label": 0
                },
                {
                    "sent": "For example, Ryan McDonald's workers have been applied to many of these different languages quite effectively.",
                    "label": 0
                },
                {
                    "sent": "But the idea of a model which you can train in one language then apply to another language.",
                    "label": 0
                },
                {
                    "sent": "I think I can't see how we can do that, at least at this point.",
                    "label": 0
                },
                {
                    "sent": "If that answers your question.",
                    "label": 0
                },
                {
                    "sent": "Yes, it was my question, but it's the problem of machine learning.",
                    "label": 0
                },
                {
                    "sent": "Now if you have one one grammar for over languages.",
                    "label": 0
                },
                {
                    "sent": "You don't have problems to have a machine translation system.",
                    "label": 0
                },
                {
                    "sent": "Programming languages are certainly different, right?",
                    "label": 0
                },
                {
                    "sent": "Word order is different.",
                    "label": 0
                },
                {
                    "sent": "The words are different in different languages, so the grammars have to differ at some level.",
                    "label": 0
                },
                {
                    "sent": "But there are certainly that kind of representations.",
                    "label": 0
                },
                {
                    "sent": "A lot of work in linguistics is trying to unify treatments of different languages and make them look as similar as possible in terms of the grammar so that that might be 1.",
                    "label": 0
                },
                {
                    "sent": "Answer to your question, thank you.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Oh, that's a very interesting question.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Yes, it seems that you might be able to do something very interesting there.",
                    "label": 0
                },
                {
                    "sent": "I can't off the top my head think how that that might work, but.",
                    "label": 0
                },
                {
                    "sent": "We're basically just looking at subsets of these junction operations, so the question is whether you can automatically choose the level of granularity in those models.",
                    "label": 0
                },
                {
                    "sent": "Or maybe you have a cascaded set of models.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that would be a very interesting question, I really thought.",
                    "label": 0
                },
                {
                    "sent": "Like seems to be very striking, the online version of so you present this online learning algorithm seems to.",
                    "label": 0
                },
                {
                    "sent": "Work much faster than the LBFFG, which seems to be very striking.",
                    "label": 0
                },
                {
                    "sent": "I'd like to if you can explain on how high level that what is the intuition is that you're you're updating the parameters after this thing.",
                    "label": 0
                },
                {
                    "sent": "Every training example, so you're.",
                    "label": 0
                },
                {
                    "sent": "Particularly if there's any redundancy in the training samples.",
                    "label": 0
                },
                {
                    "sent": "If they're similar at all, then you'll tend to converge more quickly.",
                    "label": 0
                },
                {
                    "sent": "This is an observation that's been made in your network training way back when, I think.",
                    "label": 0
                },
                {
                    "sent": "So that would be that would be my main answer.",
                    "label": 0
                },
                {
                    "sent": "It's a little difficult to describe why in the proofs you actually come up with a tighter upper bound in the online case, that's a little more involved to explain, but it definitely comes through for the G algorithms.",
                    "label": 0
                },
                {
                    "sent": "Usually when people.",
                    "label": 0
                },
                {
                    "sent": "Yeah, absolutely yeah.",
                    "label": 0
                },
                {
                    "sent": "There's certainly context free expressive power.",
                    "label": 0
                },
                {
                    "sent": "The main benefit in this case is the alternative parameterization that just drops out of this very similar to a dependency parsing parameterisation.",
                    "label": 0
                },
                {
                    "sent": "You might of course use these simple tags as a simple model to prune the search space for a more complex tag, which would require a much larger pausing, pausing time.",
                    "label": 0
                },
                {
                    "sent": "So while we're running a little bit over, I think it went to Custer Flex in shorts and those who have had questions for Michael things tracking down at the banquet.",
                    "label": 0
                },
                {
                    "sent": "Text.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "No, thanks, that's right.",
                    "label": 0
                }
            ]
        }
    }
}