{
    "id": "xul4kcfw672j2v7ei65ncaldxff5ovyw",
    "title": "The influence of weighting the k-occurrences on hubness-aware classification methods",
    "info": {
        "author": [
            "Nenad Toma\u0161ev, Artificial Intelligence Laboratory, Jo\u017eef Stefan Institute"
        ],
        "published": "Nov. 4, 2011",
        "recorded": "October 2011",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Machine Learning->Instance-based Learning",
            "Top->Computer Science->Machine Learning->Supervised Learning"
        ]
    },
    "url": "http://videolectures.net/sikdd2011_tomasev_hubness/",
    "segmentation": [
        [
            "So hello everyone.",
            "As Marco said, it is quite a long title.",
            "My field of research is concerned with nearest neighbor learning and more specifically nearest neighbor learning in high dimensional data for which hardness is 1 important phenomenon and I'm going to present here just one small aspect of the issue.",
            "Which."
        ],
        [
            "See question.",
            "So I'll proceed by first defining what this interesting phenomenon hardness is.",
            "Then what was the idea which we explored in this paper and summarize the impacts on?"
        ],
        [
            "Application dementional data.",
            "So nearest neighbor reasoning is quite simple.",
            "You have some.",
            "Objects which represent in some feature space and you sort of model the similarity between these objects as proximity in the feature space defined by some metric.",
            "Now and you learn about your point of interest by concerning about consulting its nearest neighbor.",
            "So it's K nearest neighbors.",
            "And this is a very frequent approach both for cluster."
        ],
        [
            "Classification and other forms like information retrieval.",
            "However, it is also affected by the well known curse of dimensionality, which plagues many machine learning methods.",
            "So nearest neighbor methods are not immune to this and hardness is 1 aspect of the dimensionality curves in nearest neighbor methods.",
            "So the problem with high dimensional data of course is that you have a big space of air space and only a few data points.",
            "Everything is sparse and very difficult to estimate.",
            "And also the distances tend to concentrate, which means that points tend to become relatively more similar to each other and the contrast between distance and close points decreases, so it is difficult to say what is close and what is distance.",
            "In other words, these nearest neighbors, which we obtain are becoming less and less meaningful under dimensionality increases.",
            "And hubs."
        ],
        [
            "So what are these hubs?",
            "Because we mean points which have a high degree in the K nearest neighbor graph on the date.",
            "In other words, points points which frequently occur as neighbors.",
            "I mean, what does this mean?",
            "In fact, when a point is a frequent neighbor and when we are performing some sort of nearest neighbor learning, this means that the particular point has a high influence on all the conclusions that we will draw on the data and high dimensional data these hubs.",
            "Are becoming more and more prominent, which means they are even more frequent compared to the rest of the points.",
            "Then in the low dimensional case, and since we have some points which are very frequent as neighbors, the majority of other points in turn must become less and less frequent, which means that we will also have points which never appear as neighbors or appear very rarely, and we turn them NT hubs, so some points have a high influence.",
            "On the classification, many points have 02 to some low in."
        ],
        [
            "On the classification process.",
            "So out of all these neighbor occurrences, we can say that some of them are good, some of them are bad based on what sort of influence they exhibit on the subsequent classification.",
            "So the key nearest neighbor method for classifying data points, which is very old and very well known, is essentially this.",
            "You take a point, find this key nearest neighbors, and do a majority vote and discover what is the majority class in its neighborhood.",
            "In other words, consult the labels of the neighbors.",
            "Now these labels.",
            "May or may not match the actual label of data points, so we have some specification.",
            "I mean no classification is 100% accurate anyway, so these label mismatches are bad occurrences because their influence is bad and the sum total of all these bad occurrences with term bad hardness.",
            "Now when we have some very influential points, some hubs, they can also be good hubs or bad hubs.",
            "We have some points which almost always occur within their own category.",
            "We have also some points which.",
            "Nearly never occur within their own categories, so there are very very bad hubs and exhibited high bad influence in classification."
        ],
        [
            "Oh S for taking this into account, improve classification, alot of work has been done recently.",
            "And this is just a brief summary.",
            "There is more more behind it.",
            "So some happiness where.",
            "Modifications of the basic nearest neighbor approach have been proposed which are aware of the underlying hardness of the data and reduce the influence of these prominent bad hubs."
        ],
        [
            "The simplest of the methods in the first to be proposed is awaiting scheme simple vote weighting scheme, which assigns a weight to each instance.",
            "So it's an instance specific weight and it's based on its bad hardness.",
            "This first line above is the total number number of occurrences in the good and the bad hardness.",
            "In turn, this is just standardizing the variable and in the end just like an exponential weighting scheme, and it has been shown to."
        ],
        [
            "Lead to significant improvements in high dimensional data.",
            "As for the other methods, they also use some form of distance waiting as well as taking class specific occurrences into account, so I don't have the time to go into details, but let's just keep it in this.",
            "Higher abstraction level for now.",
            "OK, so if.",
            "And this is not not just the property of these particular methods.",
            "Many nearest neighbor methods use distance waiting.",
            "You distance weighting is actually used during voting phase.",
            "A question is how will it affect hardness certification if it was also used during model learning phase?"
        ],
        [
            "So this is essentially the goal of."
        ],
        [
            "This small paper and it is contained within this slight modification of the original formula, so this is the weighted number of occurrences of a neighbor point XI.",
            "And all of its occurrences are taken into account, but they are given.",
            "This way, so this is the distance from a point to its nearest neighbor, first nearest neighbor.",
            "And this is the distance to the neighbor Point XI.",
            "Of course, since this is always greater, this is more than one and essentially.",
            "Why this is?",
            "An interesting idea is that.",
            "When you use a single K value across the entire data set, especially this case large, it may not always be appropriate 'cause you have large categories, small categories, rare categories, and it's difficult to find 1 one consistent K value and This is why distance weighting usually helps a little bit in the in the ending accuracy as you compensate for the included far neighbors now.",
            "OK, so this is a weighted occurrence count."
        ],
        [
            "Very simple now.",
            "So to test how what sort of influence this has, all these partners aware methods and on the hardness of the data in general we took some public datasets from UCI some image datasets which are processed to obtain the quantized sift representations and did some experiments for classification experiments.",
            "It was of course 10 * 10 fold cross validation."
        ],
        [
            "OK, so the results.",
            "This is the effect of the weight of the current count on the hardness of the data.",
            "In other words, on the skewness of the occurrence distribution on the data set.",
            "So these are the 15 datasets which have been used and what is meant here is the difference between the weighted schemeless and the original scheme of the data, and we see that adding weight to these occurrences quite consistently increases the skewness of the occurrence distribution on the data.",
            "This is mostly due.",
            "I mean, this is my interpretation to the fact that skewness is usually.",
            "Bigger or smaller values of K and this sort of waiting preserves this lower case.",
            "Q This is the case increased, so hence the difference, and this may turn out to be important when we try to do clustering at some point later on 'cause."
        ],
        [
            "It's OK.",
            "This process ification itself.",
            "I didn't, of course include the entire table on the slide, 'cause it would be unreadable.",
            "These are the averages the unweighted, the weighted variant.",
            "And we see that there is a change these algorithms which use class specific occurrence models gain an increase, while the weighting scheme is decreased by including the weights.",
            "The reason for this is simply that the influence of bad hubs is sort of underestimated here.",
            "While on the other hand also, this algorithm doesn't use distance weighting.",
            "On its own, well, here we see that it is definitely some improvement.",
            "However, most of this improvement is."
        ],
        [
            "Containing just a few datasets and.",
            "Like this is an example and on these datasets improvement is because these are the datasets where these actual algorithms do not prove better or significantly better than KN so."
        ],
        [
            "See what is happening we plotted.",
            "Graphs over different neighborhood sizes, so these are the key values of the accuracies of the algorithms, and we see that what this waiting essentially does.",
            "This is the red line is that it makes the accuracy decrease lower, so these are the datasets where increasing the K decreases the accuracy.",
            "These are datasets where one nearest neighbor is the best algorithm right?",
            "All of these datasets where there is improvement.",
            "So of course.",
            "This result is not really very useful because if 11 nearest neighbor is the best, then you're not going to be using any of this stuff anyway.",
            "But the point is that sometimes you don't know this stuff in advance, right?",
            "So including this as a sort of prevention measure anyway might might be a good idea."
        ],
        [
            "So just to conclude, there is definitely an impact of including this weights so things change.",
            "The skewness is affected and also it may help on some specific datasets when it comes to publication, so this would be sort of a summary of this."
        ],
        [
            "OK so thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So hello everyone.",
                    "label": 0
                },
                {
                    "sent": "As Marco said, it is quite a long title.",
                    "label": 0
                },
                {
                    "sent": "My field of research is concerned with nearest neighbor learning and more specifically nearest neighbor learning in high dimensional data for which hardness is 1 important phenomenon and I'm going to present here just one small aspect of the issue.",
                    "label": 0
                },
                {
                    "sent": "Which.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See question.",
                    "label": 0
                },
                {
                    "sent": "So I'll proceed by first defining what this interesting phenomenon hardness is.",
                    "label": 0
                },
                {
                    "sent": "Then what was the idea which we explored in this paper and summarize the impacts on?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Application dementional data.",
                    "label": 0
                },
                {
                    "sent": "So nearest neighbor reasoning is quite simple.",
                    "label": 0
                },
                {
                    "sent": "You have some.",
                    "label": 0
                },
                {
                    "sent": "Objects which represent in some feature space and you sort of model the similarity between these objects as proximity in the feature space defined by some metric.",
                    "label": 1
                },
                {
                    "sent": "Now and you learn about your point of interest by concerning about consulting its nearest neighbor.",
                    "label": 1
                },
                {
                    "sent": "So it's K nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "And this is a very frequent approach both for cluster.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Classification and other forms like information retrieval.",
                    "label": 0
                },
                {
                    "sent": "However, it is also affected by the well known curse of dimensionality, which plagues many machine learning methods.",
                    "label": 0
                },
                {
                    "sent": "So nearest neighbor methods are not immune to this and hardness is 1 aspect of the dimensionality curves in nearest neighbor methods.",
                    "label": 0
                },
                {
                    "sent": "So the problem with high dimensional data of course is that you have a big space of air space and only a few data points.",
                    "label": 0
                },
                {
                    "sent": "Everything is sparse and very difficult to estimate.",
                    "label": 1
                },
                {
                    "sent": "And also the distances tend to concentrate, which means that points tend to become relatively more similar to each other and the contrast between distance and close points decreases, so it is difficult to say what is close and what is distance.",
                    "label": 1
                },
                {
                    "sent": "In other words, these nearest neighbors, which we obtain are becoming less and less meaningful under dimensionality increases.",
                    "label": 0
                },
                {
                    "sent": "And hubs.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what are these hubs?",
                    "label": 0
                },
                {
                    "sent": "Because we mean points which have a high degree in the K nearest neighbor graph on the date.",
                    "label": 0
                },
                {
                    "sent": "In other words, points points which frequently occur as neighbors.",
                    "label": 0
                },
                {
                    "sent": "I mean, what does this mean?",
                    "label": 0
                },
                {
                    "sent": "In fact, when a point is a frequent neighbor and when we are performing some sort of nearest neighbor learning, this means that the particular point has a high influence on all the conclusions that we will draw on the data and high dimensional data these hubs.",
                    "label": 0
                },
                {
                    "sent": "Are becoming more and more prominent, which means they are even more frequent compared to the rest of the points.",
                    "label": 0
                },
                {
                    "sent": "Then in the low dimensional case, and since we have some points which are very frequent as neighbors, the majority of other points in turn must become less and less frequent, which means that we will also have points which never appear as neighbors or appear very rarely, and we turn them NT hubs, so some points have a high influence.",
                    "label": 1
                },
                {
                    "sent": "On the classification, many points have 02 to some low in.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the classification process.",
                    "label": 0
                },
                {
                    "sent": "So out of all these neighbor occurrences, we can say that some of them are good, some of them are bad based on what sort of influence they exhibit on the subsequent classification.",
                    "label": 0
                },
                {
                    "sent": "So the key nearest neighbor method for classifying data points, which is very old and very well known, is essentially this.",
                    "label": 0
                },
                {
                    "sent": "You take a point, find this key nearest neighbors, and do a majority vote and discover what is the majority class in its neighborhood.",
                    "label": 0
                },
                {
                    "sent": "In other words, consult the labels of the neighbors.",
                    "label": 0
                },
                {
                    "sent": "Now these labels.",
                    "label": 0
                },
                {
                    "sent": "May or may not match the actual label of data points, so we have some specification.",
                    "label": 0
                },
                {
                    "sent": "I mean no classification is 100% accurate anyway, so these label mismatches are bad occurrences because their influence is bad and the sum total of all these bad occurrences with term bad hardness.",
                    "label": 0
                },
                {
                    "sent": "Now when we have some very influential points, some hubs, they can also be good hubs or bad hubs.",
                    "label": 0
                },
                {
                    "sent": "We have some points which almost always occur within their own category.",
                    "label": 0
                },
                {
                    "sent": "We have also some points which.",
                    "label": 0
                },
                {
                    "sent": "Nearly never occur within their own categories, so there are very very bad hubs and exhibited high bad influence in classification.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Oh S for taking this into account, improve classification, alot of work has been done recently.",
                    "label": 0
                },
                {
                    "sent": "And this is just a brief summary.",
                    "label": 0
                },
                {
                    "sent": "There is more more behind it.",
                    "label": 0
                },
                {
                    "sent": "So some happiness where.",
                    "label": 0
                },
                {
                    "sent": "Modifications of the basic nearest neighbor approach have been proposed which are aware of the underlying hardness of the data and reduce the influence of these prominent bad hubs.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The simplest of the methods in the first to be proposed is awaiting scheme simple vote weighting scheme, which assigns a weight to each instance.",
                    "label": 0
                },
                {
                    "sent": "So it's an instance specific weight and it's based on its bad hardness.",
                    "label": 0
                },
                {
                    "sent": "This first line above is the total number number of occurrences in the good and the bad hardness.",
                    "label": 1
                },
                {
                    "sent": "In turn, this is just standardizing the variable and in the end just like an exponential weighting scheme, and it has been shown to.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lead to significant improvements in high dimensional data.",
                    "label": 0
                },
                {
                    "sent": "As for the other methods, they also use some form of distance waiting as well as taking class specific occurrences into account, so I don't have the time to go into details, but let's just keep it in this.",
                    "label": 0
                },
                {
                    "sent": "Higher abstraction level for now.",
                    "label": 0
                },
                {
                    "sent": "OK, so if.",
                    "label": 0
                },
                {
                    "sent": "And this is not not just the property of these particular methods.",
                    "label": 0
                },
                {
                    "sent": "Many nearest neighbor methods use distance waiting.",
                    "label": 0
                },
                {
                    "sent": "You distance weighting is actually used during voting phase.",
                    "label": 0
                },
                {
                    "sent": "A question is how will it affect hardness certification if it was also used during model learning phase?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is essentially the goal of.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This small paper and it is contained within this slight modification of the original formula, so this is the weighted number of occurrences of a neighbor point XI.",
                    "label": 0
                },
                {
                    "sent": "And all of its occurrences are taken into account, but they are given.",
                    "label": 0
                },
                {
                    "sent": "This way, so this is the distance from a point to its nearest neighbor, first nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "And this is the distance to the neighbor Point XI.",
                    "label": 1
                },
                {
                    "sent": "Of course, since this is always greater, this is more than one and essentially.",
                    "label": 0
                },
                {
                    "sent": "Why this is?",
                    "label": 0
                },
                {
                    "sent": "An interesting idea is that.",
                    "label": 0
                },
                {
                    "sent": "When you use a single K value across the entire data set, especially this case large, it may not always be appropriate 'cause you have large categories, small categories, rare categories, and it's difficult to find 1 one consistent K value and This is why distance weighting usually helps a little bit in the in the ending accuracy as you compensate for the included far neighbors now.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a weighted occurrence count.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very simple now.",
                    "label": 0
                },
                {
                    "sent": "So to test how what sort of influence this has, all these partners aware methods and on the hardness of the data in general we took some public datasets from UCI some image datasets which are processed to obtain the quantized sift representations and did some experiments for classification experiments.",
                    "label": 0
                },
                {
                    "sent": "It was of course 10 * 10 fold cross validation.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the results.",
                    "label": 0
                },
                {
                    "sent": "This is the effect of the weight of the current count on the hardness of the data.",
                    "label": 0
                },
                {
                    "sent": "In other words, on the skewness of the occurrence distribution on the data set.",
                    "label": 0
                },
                {
                    "sent": "So these are the 15 datasets which have been used and what is meant here is the difference between the weighted schemeless and the original scheme of the data, and we see that adding weight to these occurrences quite consistently increases the skewness of the occurrence distribution on the data.",
                    "label": 0
                },
                {
                    "sent": "This is mostly due.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is my interpretation to the fact that skewness is usually.",
                    "label": 0
                },
                {
                    "sent": "Bigger or smaller values of K and this sort of waiting preserves this lower case.",
                    "label": 0
                },
                {
                    "sent": "Q This is the case increased, so hence the difference, and this may turn out to be important when we try to do clustering at some point later on 'cause.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's OK.",
                    "label": 0
                },
                {
                    "sent": "This process ification itself.",
                    "label": 0
                },
                {
                    "sent": "I didn't, of course include the entire table on the slide, 'cause it would be unreadable.",
                    "label": 0
                },
                {
                    "sent": "These are the averages the unweighted, the weighted variant.",
                    "label": 0
                },
                {
                    "sent": "And we see that there is a change these algorithms which use class specific occurrence models gain an increase, while the weighting scheme is decreased by including the weights.",
                    "label": 0
                },
                {
                    "sent": "The reason for this is simply that the influence of bad hubs is sort of underestimated here.",
                    "label": 0
                },
                {
                    "sent": "While on the other hand also, this algorithm doesn't use distance weighting.",
                    "label": 0
                },
                {
                    "sent": "On its own, well, here we see that it is definitely some improvement.",
                    "label": 0
                },
                {
                    "sent": "However, most of this improvement is.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Containing just a few datasets and.",
                    "label": 0
                },
                {
                    "sent": "Like this is an example and on these datasets improvement is because these are the datasets where these actual algorithms do not prove better or significantly better than KN so.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See what is happening we plotted.",
                    "label": 0
                },
                {
                    "sent": "Graphs over different neighborhood sizes, so these are the key values of the accuracies of the algorithms, and we see that what this waiting essentially does.",
                    "label": 0
                },
                {
                    "sent": "This is the red line is that it makes the accuracy decrease lower, so these are the datasets where increasing the K decreases the accuracy.",
                    "label": 0
                },
                {
                    "sent": "These are datasets where one nearest neighbor is the best algorithm right?",
                    "label": 0
                },
                {
                    "sent": "All of these datasets where there is improvement.",
                    "label": 0
                },
                {
                    "sent": "So of course.",
                    "label": 0
                },
                {
                    "sent": "This result is not really very useful because if 11 nearest neighbor is the best, then you're not going to be using any of this stuff anyway.",
                    "label": 0
                },
                {
                    "sent": "But the point is that sometimes you don't know this stuff in advance, right?",
                    "label": 0
                },
                {
                    "sent": "So including this as a sort of prevention measure anyway might might be a good idea.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just to conclude, there is definitely an impact of including this weights so things change.",
                    "label": 0
                },
                {
                    "sent": "The skewness is affected and also it may help on some specific datasets when it comes to publication, so this would be sort of a summary of this.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so thanks.",
                    "label": 0
                }
            ]
        }
    }
}