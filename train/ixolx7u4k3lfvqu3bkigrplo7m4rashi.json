{
    "id": "ixolx7u4k3lfvqu3bkigrplo7m4rashi",
    "title": "Fast Direction-Aware Proximity for Graph Mining",
    "info": {
        "author": [
            "Hanghang Tong, Carnegie Mellon University"
        ],
        "published": "Aug. 14, 2007",
        "recorded": "August 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Structured Data"
        ]
    },
    "url": "http://videolectures.net/kdd07_hanghang_fdap/",
    "segmentation": [
        [
            "Hey good morning everyone I'm hung hung from CMU so today I will introduce our work on directionality aware proximities, so this is joint work with Yehuda from AT&T and my advisor crystals."
        ],
        [
            "So they're increasing interest on measuring proximities on graph, and we should point out most of those measurements are designed for the undirected graph.",
            "So here the question is, what's the proximity between A&B?",
            "However, many real graphs are directed."
        ],
        [
            "And in this work we want to claim that with every direction we can view the proximity from a new perspective.",
            "For example, in this graph, now with the edge direction, rather than asking what's a proximity between A&B, we can ask what's the proximity from A to B, or from B2A.",
            "So in this example in Q Teresa, proximity from HB is high while from B2A.",
            "Is low."
        ],
        [
            "So in this talk we try to answer the following three questions.",
            "First, how can we define such direction where proximity or dog?",
            "And then how can we do the computation efficiently?",
            "And finally, how can our depth benefit some relapse?"
        ],
        [
            "Locations.",
            "So this is a plan and I will start with definition of our data."
        ],
        [
            "So given the graph, we can naturally define a random walk on this graph.",
            "An hour proximity is based on the escape probability of random walk, which is the probability that random particle starting from node A will visit node B before it returns."
        ],
        [
            "To know that a for example in this graph, the escape probability from A to B is 1 mile from, B2A is .5.",
            "So which is consistent with our intuition."
        ],
        [
            "However, there might be a catch with the originalist capability and we need some practical modifica."
        ],
        [
            "Missions.",
            "In this example, intuitively, the proximity from A to B in the up figure should be larger than that in the bottom figure.",
            "However, in terms of escape probability, they are same and both are ones.",
            "So to address this issue, we can introduce a universal so inbound."
        ],
        [
            "Early in this graph, that is, we will introduce an additional nodes in this graph, and then for each origin node in the graph, there are some small probability to reach this bound at each step of random walk, and once it has reached this bound, it will stay there forever."
        ],
        [
            "So now by introducing this Black Horse as in the right figures, our proximity scores are consistent with our into."
        ],
        [
            "She.",
            "Another potential problem with the Origonal escape probability is for the Weekly Connect pair.",
            "So in this example A&B is a weekly Connect pair and both proximity from A to B and that from B2 AO.",
            "However, in some cases this may not be desirable, especially when there are some missing links in the graph.",
            "So to address this issue, we can take the partial symmetry operation.",
            "That is, whenever we observe edge from node I to know Jay, we will introduce a backward edge in the opposite direction, but with some smaller edge."
        ],
        [
            "Wait?",
            "So now by introducing these backward edges as in the bottom graphs, approximate scores in both directions are nonzeros."
        ],
        [
            "So this year definition for our dog and next is a computer."
        ],
        [
            "Final issue.",
            "So let me start with the traditional way to solve the escape probability.",
            "If P is a transition matrix associated with with the graph, which is the adjacent matrix by row normalization and then by solving a linear system we can get this formula to compute the escape probability from Lodi to know the J.",
            "So here Pi transposes the ice role of P by removing its eyes and JS.",
            "Rows and columns and P head is a submatrix of P by removing its eyes and adjacent rows and columns, and similarly, PJ is Jays column of matrix P by removing its Jay's eyes and trace elements.",
            "So here the major computational cost is a matrix inversion of size N -- 2 times and minus two and is number."
        ],
        [
            "Of nodes in the graph.",
            "So here is an example and this is a formula to compute the escape probability from node one to know the five in this graph."
        ],
        [
            "So here the major computational cost is a matrix inversion which correspond to the great parts."
        ],
        [
            "The transition matrix P. So by a similar procedure we can get this formula to compute our proximity score from Lodi.",
            "To know the J."
        ],
        [
            "So here again, the major computational cost is a matrix inversion of size N -- 2 times a month."
        ],
        [
            "Is 2.",
            "So the matrix inversion in computing our proximity scores brings the foreign challenges in computation.",
            "First, our media science graph stay with several thousand nodes.",
            "Well, the matrix inversion is feasable, however, by the formula we will need a different matrix inversion to compute a different proxy score.",
            "So in the case we will need a lot of approximate scores will need to do a lot of matrix inversions, which is not efficient even on medium size graph.",
            "So in this case.",
            "How can we reduce the number of matrix inversions if we're interested in all possible proximal scores in the other words, how can we simultaneously solve multiple linear system efficiently?",
            "On the other hand, on a larger size graph, say with hundreds of thousands of nodes, well, the matrix inversion is very slow if not impossible.",
            "So in this case, how can we completely avoid matrix inversion if we're interested in one single proximity score?",
            "In other words, how can we efficiently solve one linear system on a larger size graph?"
        ],
        [
            "So first on our media science graph, we are interested in efficiency computing all possible."
        ],
        [
            "Loprox in this course.",
            "In this example, if we are interested in the approximate school from node one, to know the five, then we will need a matrix inversion correspond to the great parts in the transition matrix.",
            "So now if we are interested in another different approximate score, say from #1 to #6, will need another matrix inversion which correspond to the great parts in the bottom transition matrix.",
            "So it seems we'll need two different matrix inversions to compute these two different proximity score."
        ],
        [
            "However, if we compare these two great parts will see their larger overlap between them.",
            "In other words, there is a lot of redundancy among different linear systems, so hopefully once we have solved one linear system, we could leverage this result to solve other linear."
        ],
        [
            "Systems efficiently.",
            "So I will not go into detail.",
            "It turns out all possible approximate scores are determined by this matrix Q, which is the inversion of I."
        ],
        [
            "NSCP, so in the case on a medium size graph, if we're interested in all possible proximity scores, we actually only need to do one rather than big ON square matrix inversions, which is a lot of savings in computer."
        ],
        [
            "Patient on the other hand, on a larger size graph we're interested in efficiently computing onesync."
        ],
        [
            "No proximity score.",
            "So from the previous result, we already know that all possible proximity scores are determined by the Matrix Q.",
            "And Moreover, for one single proximity school, it's only depends on four elements of matrix Q.",
            "In other words, we only need a partial information to compute one matrix to computer 1 proximity school.",
            "So if we can get these partial information so if elements or two columns.",
            "Only tease cute without matrix inversion.",
            "Then we get a fast algorithm to compute one single proximal school in the case on a larger size graph when the matrix inversion."
        ],
        [
            "Is infeasible.",
            "So here the question is, how can we get one column or matrix Q without matrix inversion?",
            "Well, thanks to the special structure of Matrix Q, it turns out we can get this formula by Taylor expansion and knows that this formula is actually the matrix correspondence of the Taylor expansion for 1 / 1 -- X when X is a scalar between zero and X."
        ],
        [
            "1.",
            "So based on this equation, we actually only needed to do some sparse matrix vector multiplications to get one column or."
        ],
        [
            "It is Q&A.",
            "Therefore we can easily get works.",
            "I'm sorry.",
            "And therefore we can easily get this iterative algorithm to compute A1 column."
        ],
        [
            "Matrix Q.",
            "So in terms of computational issue rather than CU on the number of the nodes in the graph, the algorithm is linear on the number of the edges in the graph."
        ],
        [
            "So next I will show."
        ],
        [
            "Some experimental results.",
            "We use five different datasets and all of them are directed graph such as web link graph.",
            "Who trusts whom who imaged whom and so."
        ],
        [
            "Well.",
            "So first I want to show our approximate score is an effective metric for predicting the existence of."
        ],
        [
            "Link so here for each pair of nodes we compute approximate score in two directions as the map and then process his gram.",
            "So for the up figures Red one, there is always a link between two nodes, while for the bottom figure the blue one.",
            "There is no link.",
            "So from the histogram it's clear that our proximity school is effective to distinguish these two classes."
        ],
        [
            "And here are some numbers."
        ],
        [
            "In terms of accuracy and here I want to show with our tab we can actually do something you.",
            "So for instance if we already know there is some link between two nodes and we want to know what's the direction of that link.",
            "So to this end we can simply compare the proximity score in two directions and if the proximity from I to J is much bigger than that in the opposite direction.",
            "So maybe we can see the edge from I to J.",
            "Rather than from J2I.",
            "So to demonstrate this week on track to testing set and for each pair of nodes I&J in the testing set, there is always a single side edge from node I to know J and then we compute the proximity from I to J minus proxy in the opposite direction and process his gram so we can see the histogram is imbalanced and they're much more probability mass in the."
        ],
        [
            "His own.",
            "So this is a running time for computing all possible proximity scores.",
            "Here X axis is the size of the graph and Y axis is response time and it's in the logarithm scale.",
            "So we can see the fast algorithm could save the computational time."
        ],
        [
            "What?",
            "And then we can draw the similar conclusions for computing one single."
        ],
        [
            "Proximity score"
        ],
        [
            "So now we're ready to answer the three questions we pose at the very beginning.",
            "1st to define a directionality aware proximity, we use escape probability together with some practical modifications, and then we introduce some fast solutions in two different scenario.",
            "And finally we show that out."
        ],
        [
            "Approximate school is effective for link prediction, so for time limited I would give some details of some stuffs such as how to generalize our definition to measure the relationship between two groups of nodes and also more."
        ],
        [
            "Applications of our dog, so that's it.",
            "Thank you."
        ],
        [
            "Question.",
            "Side question about you give us some intuitions about the sorts of applications that you can make this work.",
            "You mean other application?",
            "OK, so here I only use a link prediction as a as an example, and in the paper we do introduce some other applications of our dog, for example how to takes into account the directionality aware information when we do the connection server graph.",
            "And also I believe there is a lot of other applications could from this directionality.",
            "For example in a.",
            "You got a telephone network, so if we observe that some phone number is always received, a phone call from other numbers.",
            "But it really do not call the other numbers, so maybe it's an 800 number, right?",
            "So in the language of the proximity will say the proximity score from this phone number to the other numbers is very slow and very low, while in the office directions is very high and similarly.",
            "I think we can use uses directionality information to identify those potential spam in the email network, right?",
            "Since if somebody always send email to other never receive emails from the other, then we'll see.",
            "OK, maybe you are potential spammer, yeah?",
            "Did you click on the link?",
            "Heard you up.",
            "You know, possibly wasted.",
            "Today we do not profit, just cumulative weight.",
            "No.",
            "Yeah OK, that's a good point.",
            "In the paper.",
            "We do not compare it with those other existing method since I think the main goal of this paper is not to do link prediction and I think on the other hand, if you really care about link prediction.",
            "So maybe see a popular method for link prediction is to treat it as a classification task.",
            "So in this case you can use our proximity.",
            "Cool at the feature and maybe you can combine with those existing features as some you mentioned and hopefully you will get a better performance.",
            "Yeah.",
            "Easy or difficult.",
            "412 to what?",
            "Celebrations.",
            "Expand your work tools under control with wonderful sleep.",
            "So you mean how to compare that with the UN directed version of our proximities, right?",
            "So I think first in terms of computational issue, the fast solutions we introduce here can naturally applied to the UN direct case.",
            "And for the applications, I think it depends.",
            "Actually, we do try to apply this approximate score for the other applications.",
            "For example, for clustering invested in that case is we found that the directionality information seems do not help a lot.",
            "For class two class ring, but I think the directionality aware proximity has its own right, right, I think.",
            "And I believe that many applications could benefit from those directionality aware proximity, as some I have introduced, and you can also find more applications in our paper.",
            "And if you are interested in this, maybe we can discuss more about it offline."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hey good morning everyone I'm hung hung from CMU so today I will introduce our work on directionality aware proximities, so this is joint work with Yehuda from AT&T and my advisor crystals.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So they're increasing interest on measuring proximities on graph, and we should point out most of those measurements are designed for the undirected graph.",
                    "label": 0
                },
                {
                    "sent": "So here the question is, what's the proximity between A&B?",
                    "label": 0
                },
                {
                    "sent": "However, many real graphs are directed.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in this work we want to claim that with every direction we can view the proximity from a new perspective.",
                    "label": 0
                },
                {
                    "sent": "For example, in this graph, now with the edge direction, rather than asking what's a proximity between A&B, we can ask what's the proximity from A to B, or from B2A.",
                    "label": 1
                },
                {
                    "sent": "So in this example in Q Teresa, proximity from HB is high while from B2A.",
                    "label": 0
                },
                {
                    "sent": "Is low.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this talk we try to answer the following three questions.",
                    "label": 0
                },
                {
                    "sent": "First, how can we define such direction where proximity or dog?",
                    "label": 0
                },
                {
                    "sent": "And then how can we do the computation efficiently?",
                    "label": 0
                },
                {
                    "sent": "And finally, how can our depth benefit some relapse?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Locations.",
                    "label": 0
                },
                {
                    "sent": "So this is a plan and I will start with definition of our data.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So given the graph, we can naturally define a random walk on this graph.",
                    "label": 0
                },
                {
                    "sent": "An hour proximity is based on the escape probability of random walk, which is the probability that random particle starting from node A will visit node B before it returns.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To know that a for example in this graph, the escape probability from A to B is 1 mile from, B2A is .5.",
                    "label": 0
                },
                {
                    "sent": "So which is consistent with our intuition.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, there might be a catch with the originalist capability and we need some practical modifica.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Missions.",
                    "label": 0
                },
                {
                    "sent": "In this example, intuitively, the proximity from A to B in the up figure should be larger than that in the bottom figure.",
                    "label": 0
                },
                {
                    "sent": "However, in terms of escape probability, they are same and both are ones.",
                    "label": 0
                },
                {
                    "sent": "So to address this issue, we can introduce a universal so inbound.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Early in this graph, that is, we will introduce an additional nodes in this graph, and then for each origin node in the graph, there are some small probability to reach this bound at each step of random walk, and once it has reached this bound, it will stay there forever.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now by introducing this Black Horse as in the right figures, our proximity scores are consistent with our into.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "She.",
                    "label": 0
                },
                {
                    "sent": "Another potential problem with the Origonal escape probability is for the Weekly Connect pair.",
                    "label": 0
                },
                {
                    "sent": "So in this example A&B is a weekly Connect pair and both proximity from A to B and that from B2 AO.",
                    "label": 0
                },
                {
                    "sent": "However, in some cases this may not be desirable, especially when there are some missing links in the graph.",
                    "label": 0
                },
                {
                    "sent": "So to address this issue, we can take the partial symmetry operation.",
                    "label": 1
                },
                {
                    "sent": "That is, whenever we observe edge from node I to know Jay, we will introduce a backward edge in the opposite direction, but with some smaller edge.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Wait?",
                    "label": 0
                },
                {
                    "sent": "So now by introducing these backward edges as in the bottom graphs, approximate scores in both directions are nonzeros.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this year definition for our dog and next is a computer.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Final issue.",
                    "label": 0
                },
                {
                    "sent": "So let me start with the traditional way to solve the escape probability.",
                    "label": 0
                },
                {
                    "sent": "If P is a transition matrix associated with with the graph, which is the adjacent matrix by row normalization and then by solving a linear system we can get this formula to compute the escape probability from Lodi to know the J.",
                    "label": 1
                },
                {
                    "sent": "So here Pi transposes the ice role of P by removing its eyes and JS.",
                    "label": 0
                },
                {
                    "sent": "Rows and columns and P head is a submatrix of P by removing its eyes and adjacent rows and columns, and similarly, PJ is Jays column of matrix P by removing its Jay's eyes and trace elements.",
                    "label": 1
                },
                {
                    "sent": "So here the major computational cost is a matrix inversion of size N -- 2 times and minus two and is number.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of nodes in the graph.",
                    "label": 0
                },
                {
                    "sent": "So here is an example and this is a formula to compute the escape probability from node one to know the five in this graph.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here the major computational cost is a matrix inversion which correspond to the great parts.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The transition matrix P. So by a similar procedure we can get this formula to compute our proximity score from Lodi.",
                    "label": 0
                },
                {
                    "sent": "To know the J.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here again, the major computational cost is a matrix inversion of size N -- 2 times a month.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is 2.",
                    "label": 0
                },
                {
                    "sent": "So the matrix inversion in computing our proximity scores brings the foreign challenges in computation.",
                    "label": 0
                },
                {
                    "sent": "First, our media science graph stay with several thousand nodes.",
                    "label": 0
                },
                {
                    "sent": "Well, the matrix inversion is feasable, however, by the formula we will need a different matrix inversion to compute a different proxy score.",
                    "label": 1
                },
                {
                    "sent": "So in the case we will need a lot of approximate scores will need to do a lot of matrix inversions, which is not efficient even on medium size graph.",
                    "label": 0
                },
                {
                    "sent": "So in this case.",
                    "label": 0
                },
                {
                    "sent": "How can we reduce the number of matrix inversions if we're interested in all possible proximal scores in the other words, how can we simultaneously solve multiple linear system efficiently?",
                    "label": 1
                },
                {
                    "sent": "On the other hand, on a larger size graph, say with hundreds of thousands of nodes, well, the matrix inversion is very slow if not impossible.",
                    "label": 0
                },
                {
                    "sent": "So in this case, how can we completely avoid matrix inversion if we're interested in one single proximity score?",
                    "label": 0
                },
                {
                    "sent": "In other words, how can we efficiently solve one linear system on a larger size graph?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first on our media science graph, we are interested in efficiency computing all possible.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Loprox in this course.",
                    "label": 0
                },
                {
                    "sent": "In this example, if we are interested in the approximate school from node one, to know the five, then we will need a matrix inversion correspond to the great parts in the transition matrix.",
                    "label": 0
                },
                {
                    "sent": "So now if we are interested in another different approximate score, say from #1 to #6, will need another matrix inversion which correspond to the great parts in the bottom transition matrix.",
                    "label": 0
                },
                {
                    "sent": "So it seems we'll need two different matrix inversions to compute these two different proximity score.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, if we compare these two great parts will see their larger overlap between them.",
                    "label": 0
                },
                {
                    "sent": "In other words, there is a lot of redundancy among different linear systems, so hopefully once we have solved one linear system, we could leverage this result to solve other linear.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Systems efficiently.",
                    "label": 0
                },
                {
                    "sent": "So I will not go into detail.",
                    "label": 0
                },
                {
                    "sent": "It turns out all possible approximate scores are determined by this matrix Q, which is the inversion of I.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "NSCP, so in the case on a medium size graph, if we're interested in all possible proximity scores, we actually only need to do one rather than big ON square matrix inversions, which is a lot of savings in computer.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Patient on the other hand, on a larger size graph we're interested in efficiently computing onesync.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No proximity score.",
                    "label": 0
                },
                {
                    "sent": "So from the previous result, we already know that all possible proximity scores are determined by the Matrix Q.",
                    "label": 0
                },
                {
                    "sent": "And Moreover, for one single proximity school, it's only depends on four elements of matrix Q.",
                    "label": 0
                },
                {
                    "sent": "In other words, we only need a partial information to compute one matrix to computer 1 proximity school.",
                    "label": 0
                },
                {
                    "sent": "So if we can get these partial information so if elements or two columns.",
                    "label": 0
                },
                {
                    "sent": "Only tease cute without matrix inversion.",
                    "label": 0
                },
                {
                    "sent": "Then we get a fast algorithm to compute one single proximal school in the case on a larger size graph when the matrix inversion.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is infeasible.",
                    "label": 0
                },
                {
                    "sent": "So here the question is, how can we get one column or matrix Q without matrix inversion?",
                    "label": 0
                },
                {
                    "sent": "Well, thanks to the special structure of Matrix Q, it turns out we can get this formula by Taylor expansion and knows that this formula is actually the matrix correspondence of the Taylor expansion for 1 / 1 -- X when X is a scalar between zero and X.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "1.",
                    "label": 0
                },
                {
                    "sent": "So based on this equation, we actually only needed to do some sparse matrix vector multiplications to get one column or.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is Q&A.",
                    "label": 0
                },
                {
                    "sent": "Therefore we can easily get works.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "And therefore we can easily get this iterative algorithm to compute A1 column.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Matrix Q.",
                    "label": 0
                },
                {
                    "sent": "So in terms of computational issue rather than CU on the number of the nodes in the graph, the algorithm is linear on the number of the edges in the graph.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So next I will show.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some experimental results.",
                    "label": 0
                },
                {
                    "sent": "We use five different datasets and all of them are directed graph such as web link graph.",
                    "label": 0
                },
                {
                    "sent": "Who trusts whom who imaged whom and so.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "So first I want to show our approximate score is an effective metric for predicting the existence of.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Link so here for each pair of nodes we compute approximate score in two directions as the map and then process his gram.",
                    "label": 0
                },
                {
                    "sent": "So for the up figures Red one, there is always a link between two nodes, while for the bottom figure the blue one.",
                    "label": 0
                },
                {
                    "sent": "There is no link.",
                    "label": 0
                },
                {
                    "sent": "So from the histogram it's clear that our proximity school is effective to distinguish these two classes.",
                    "label": 1
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here are some numbers.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In terms of accuracy and here I want to show with our tab we can actually do something you.",
                    "label": 0
                },
                {
                    "sent": "So for instance if we already know there is some link between two nodes and we want to know what's the direction of that link.",
                    "label": 1
                },
                {
                    "sent": "So to this end we can simply compare the proximity score in two directions and if the proximity from I to J is much bigger than that in the opposite direction.",
                    "label": 0
                },
                {
                    "sent": "So maybe we can see the edge from I to J.",
                    "label": 0
                },
                {
                    "sent": "Rather than from J2I.",
                    "label": 0
                },
                {
                    "sent": "So to demonstrate this week on track to testing set and for each pair of nodes I&J in the testing set, there is always a single side edge from node I to know J and then we compute the proximity from I to J minus proxy in the opposite direction and process his gram so we can see the histogram is imbalanced and they're much more probability mass in the.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "His own.",
                    "label": 0
                },
                {
                    "sent": "So this is a running time for computing all possible proximity scores.",
                    "label": 0
                },
                {
                    "sent": "Here X axis is the size of the graph and Y axis is response time and it's in the logarithm scale.",
                    "label": 1
                },
                {
                    "sent": "So we can see the fast algorithm could save the computational time.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "And then we can draw the similar conclusions for computing one single.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Proximity score",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now we're ready to answer the three questions we pose at the very beginning.",
                    "label": 0
                },
                {
                    "sent": "1st to define a directionality aware proximity, we use escape probability together with some practical modifications, and then we introduce some fast solutions in two different scenario.",
                    "label": 1
                },
                {
                    "sent": "And finally we show that out.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Approximate school is effective for link prediction, so for time limited I would give some details of some stuffs such as how to generalize our definition to measure the relationship between two groups of nodes and also more.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Applications of our dog, so that's it.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Side question about you give us some intuitions about the sorts of applications that you can make this work.",
                    "label": 0
                },
                {
                    "sent": "You mean other application?",
                    "label": 0
                },
                {
                    "sent": "OK, so here I only use a link prediction as a as an example, and in the paper we do introduce some other applications of our dog, for example how to takes into account the directionality aware information when we do the connection server graph.",
                    "label": 0
                },
                {
                    "sent": "And also I believe there is a lot of other applications could from this directionality.",
                    "label": 0
                },
                {
                    "sent": "For example in a.",
                    "label": 0
                },
                {
                    "sent": "You got a telephone network, so if we observe that some phone number is always received, a phone call from other numbers.",
                    "label": 0
                },
                {
                    "sent": "But it really do not call the other numbers, so maybe it's an 800 number, right?",
                    "label": 0
                },
                {
                    "sent": "So in the language of the proximity will say the proximity score from this phone number to the other numbers is very slow and very low, while in the office directions is very high and similarly.",
                    "label": 0
                },
                {
                    "sent": "I think we can use uses directionality information to identify those potential spam in the email network, right?",
                    "label": 0
                },
                {
                    "sent": "Since if somebody always send email to other never receive emails from the other, then we'll see.",
                    "label": 0
                },
                {
                    "sent": "OK, maybe you are potential spammer, yeah?",
                    "label": 0
                },
                {
                    "sent": "Did you click on the link?",
                    "label": 0
                },
                {
                    "sent": "Heard you up.",
                    "label": 0
                },
                {
                    "sent": "You know, possibly wasted.",
                    "label": 0
                },
                {
                    "sent": "Today we do not profit, just cumulative weight.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Yeah OK, that's a good point.",
                    "label": 0
                },
                {
                    "sent": "In the paper.",
                    "label": 0
                },
                {
                    "sent": "We do not compare it with those other existing method since I think the main goal of this paper is not to do link prediction and I think on the other hand, if you really care about link prediction.",
                    "label": 0
                },
                {
                    "sent": "So maybe see a popular method for link prediction is to treat it as a classification task.",
                    "label": 0
                },
                {
                    "sent": "So in this case you can use our proximity.",
                    "label": 0
                },
                {
                    "sent": "Cool at the feature and maybe you can combine with those existing features as some you mentioned and hopefully you will get a better performance.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Easy or difficult.",
                    "label": 0
                },
                {
                    "sent": "412 to what?",
                    "label": 0
                },
                {
                    "sent": "Celebrations.",
                    "label": 0
                },
                {
                    "sent": "Expand your work tools under control with wonderful sleep.",
                    "label": 0
                },
                {
                    "sent": "So you mean how to compare that with the UN directed version of our proximities, right?",
                    "label": 0
                },
                {
                    "sent": "So I think first in terms of computational issue, the fast solutions we introduce here can naturally applied to the UN direct case.",
                    "label": 0
                },
                {
                    "sent": "And for the applications, I think it depends.",
                    "label": 0
                },
                {
                    "sent": "Actually, we do try to apply this approximate score for the other applications.",
                    "label": 0
                },
                {
                    "sent": "For example, for clustering invested in that case is we found that the directionality information seems do not help a lot.",
                    "label": 0
                },
                {
                    "sent": "For class two class ring, but I think the directionality aware proximity has its own right, right, I think.",
                    "label": 0
                },
                {
                    "sent": "And I believe that many applications could benefit from those directionality aware proximity, as some I have introduced, and you can also find more applications in our paper.",
                    "label": 0
                },
                {
                    "sent": "And if you are interested in this, maybe we can discuss more about it offline.",
                    "label": 0
                }
            ]
        }
    }
}