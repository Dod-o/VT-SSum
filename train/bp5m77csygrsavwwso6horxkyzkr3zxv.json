{
    "id": "bp5m77csygrsavwwso6horxkyzkr3zxv",
    "title": "Inducing Implicit Relations from Text using Distantly Supervised Deep Nets",
    "info": {
        "author": [
            "Alfio Massimiliano Gliozzo, IBM Research"
        ],
        "published": "Nov. 22, 2018",
        "recorded": "October 2018",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2018_gliozzo_implicit_relations_distantly/",
    "segmentation": [
        [
            "OK, thank you the this joint work with Michael Aktay, who is actually there and and Anna and get on this been performed at IBM Research and while nandina and get to know where intern with us now actually in and then I work with us and get Anna is going to join soon."
        ],
        [
            "So I'm going to talk about.",
            "Very interesting and practical use keys for the use of deep learning and ontologies in the industry and also very interesting for science, which is how to train relation instruction systems using distance supervision.",
            "And I'm gonna phase.",
            "Mostly 1 interesting challenge, which is how can we use this approach to deal with implicit knowledge?",
            "Meaning.",
            "How to recognize relation between entities that do not occur in the same sentence at all but still are related.",
            "So for this purpose we spend substantial time in several research projects and we ended up with two different approaches that could be used to solve this problem.",
            "One is to use what we call unary relations.",
            "And the other is to use composite context for this.",
            "Aim so we evaluate this approach in research benchmarks, showing that basically we can larging please recall and we also apply that to the SWC challenge, organized when year ago.",
            "And we ended up winning the challenge.",
            "So this talk is about all that.",
            "Let me introduce the problem first."
        ],
        [
            "So the idea is that if we have a knowledge graph on one hand and domain corpus.",
            "With documents about the same topic or domain of the Knowledge Graph.",
            "We can try to leverage the information documents to gather more relations to populate the pre existing knowledge graph.",
            "For example, if we have a financial knowledge graph like Permadi from Thomson Reuters.",
            "And we have a financial corpus we can find out more about companies if we have the pedia and web scale corpus.",
            "Like common crawl, we can extend the PDS semi automatically.",
            "So the approach that we're looking for here is to leverage combination of a pre existing entity detection and linking system, which is a prerequisite for this work.",
            "Together occurrences of entities in the ontology and identify the occurrence of those entities in the corpus.",
            "So we can collect context where they are located, and finally, trainer relation extraction system from those context without any human intervention.",
            "Any labeling task.",
            "So it is, I would say 100% automatic with the only thing that you need to have a pre existing anti rejection linking system to start from and then ontology."
        ],
        [
            "So the idea is that the state of the art for distant supervision, relation extraction, is using binary context.",
            "Which is the simplest approach we can get occurrences of entities belonging to different types from text.",
            "In this case we have companies and countries.",
            "And if they are actually related in the Knowledge Graph, so there is a triple about them, we can.",
            "Use them as positive example to train a system.",
            "Otherwise.",
            "We can get other appearances that are still of the same types, but they are not in the ontology.",
            "In that case they are negative examples to train our system.",
            "So."
        ],
        [
            "So we can collect a very large amount of these training data and we can feed them into a deep net.",
            "Was architecturally described here.",
            "And the deep net does the job of analyzing several occurrences of the same entity pairs, feed them into a convolutional net that is basically identifying the is, analyzing the left, the center and the right context, then is aggregating all that, and we can do that when is 1 sentence at a time.",
            "But more interesting, we can also feed multiple sentences containing the same two entities and aggregate occurrence across multiple.",
            "Enter this in a second layer where all these mentions are aggregated and using a network in network approach we can find out ultimately a finer layer where the actual relation to predicted to be predicted is represented by a dimension and we train the network this way.",
            "So.",
            "This network can be applied to very large problems.",
            "And this kind of state of the art we just modified slightly an existing work made by Lynn.",
            "It's all so adding some layer, but that's not the main contribution of this paper."
        ],
        [
            "The main contribution of the paper is.",
            "How can we extend this architecture to go from binary context to implicit context, where these two entities are not together at all?",
            "So for that we define an architecture that we call Sokratis.",
            "It is able to combine binary, unar and composite contexts and merge them together to find out more information than what you can get with explicit information only.",
            "So let me start talking about the unary relation case."
        ],
        [
            "Are you going to election is the right from a bunch of binary usually order relation?",
            "Once we fix one argument and we take the other argument free.",
            "So in this case we have this location relation between companies and countries and if we fix the second argument we can find several occurrence of things located in the other states, many other located in Canada in India in UK so.",
            "What we did was to put together the relation and argument to create a unary predicate and the examples to train this predicate is the aggregation of all the argument on the left.",
            "So this same in this way we can collect what we call unity contexts that are very similar to the binary context, with the difference that we need just one entity at the time and not anymore two entities there.",
            "So in this example here we have this company JB Hifi and we have multiple occurrence."
        ],
        [
            "Of this company and we want to figure out whether this located in Australia so the unit relation is location Australia.",
            "So we can identify a lot of good interesting signal to train the net by looking at surrounding words.",
            "For example, we can see that many other words around our actress, trillion companies.",
            "They provide good signal.",
            "To figure out the counter of this company, we also find that the Australian Stock Exchange is there.",
            "So there is not the explicit mention that this company is located in Australia.",
            "The World Australia is not there at all, but we still are able to train a network."
        ],
        [
            "By aggregating many of these unity sets and feed basically the network represented before, where instead of putting 2 arguments and position embedding for two arguments, we just use position embedding forward argument only and then the last layer is made of all possible unary predicates.",
            "You can think about and there could be up to several thousands of them.",
            "The good news is that deep learning is scalable enough to really recognize thousands of relation at the same time, so we can scale it out."
        ],
        [
            "So this is 1 approach.",
            "The other approach we tried is what we call composite context, so this is applicable in another use case when instead of having a regular corpus of document, we have what we call title oriented documents that are documents about a specific entity.",
            "For example, you can think about Wikipedia pages.",
            "You can think about web page of a specific company.",
            "So anytime you have the documents about that entity, the assumption that we can be made is that.",
            "Most of the information there is related to the target focus entity, so we can somehow collect implicit information in this way.",
            "So.",
            "And let me start.",
            "Now I'm going to describe this approach.",
            "So this is an example.",
            "So if we have these."
        ],
        [
            "Company ABC Electronic Canada and we have a website about that company.",
            "So in there interested in recognizing the relation at parties phone numbers.",
            "So how can we do that?",
            "We can first of all use an anti rejection linking system to recognize phone numbers and there are several off the shelf.",
            "It's not a big deal.",
            "Then we can derive what we call composite context out of those, which is basically the the unary context of the just the number itself plus the name of the company in the.",
            "Attached to it and then we can label them as positive or negative depending if they are in the original knowledge graph or they're not.",
            "So in this case, the real phone number was labeled as positive because the ontology itself, the knowledge base, tell us that that's the phone number, so we can train a system this way."
        ],
        [
            "So let me go to the evaluation part of my.",
            "Golf."
        ],
        [
            "So we evaluated first our baseline approach.",
            "So how good is our system to recognize banner relation?",
            "So we just took when standard benchmark which is being released by Sebastian Riedel and the call in another.",
            "And this being used widely, so is a pretty small one with 57 relation and comparing our system to the state of the art.",
            "Because you download of their code and run it on the same data set, we just show that is compatible a little bit better, but that's not what we care about.",
            "It was just to test whether this is a good enough approach.",
            "Then we."
        ],
        [
            "Tested the unary approach for dinner approach.",
            "The prerequisite is that you need a very large ontology, so such a tiny corpus cannot be used for that.",
            "We develop a very large scale corpus made out of DB pedia and common crawl.",
            "So in this corpus is much bigger.",
            "We have millions of sentences.",
            "Hundreds of different relation types and we can collect millions of context.",
            "So in this case what we tested was the ability to predict Una relacion versus binary relation.",
            "And as you can see they basically perform similarly.",
            "But if you combine the two hour recall double meaning that they are looking for totally different things, so the combination of the two is basically.",
            "Doing what the job is supposed to do, improving recall.",
            "OK then."
        ],
        [
            "Part of my evolution is the application to the SWC challenge."
        ],
        [
            "So this challenge is particularly difficult because is about predicting the private version of Permadi Thomson Reuters from the public portion.",
            "So meaning that the organizer provide the public permit which is actually open for everybody and the test set was made out of very unknown company that they have in their private portion and they wanted us to recognize the foundation here.",
            "That quarter.",
            "Phone numbers, the Corder country given just the website URL as an input and the company name.",
            "This so this same what we do."
        ],
        [
            "We collected all the websites of this company and we ran our approach.",
            "We had off the shelf name and definition system as described before and we were able to collect 80,000 training company title oriented document for them.",
            "We were able to collect roughly 90% of the company in the test set and then we use this Socrates approach for the foundation.",
            "Here in the court's phone number.",
            "And.",
            "The evaluation showed that the."
        ],
        [
            "System outperform both every other system in the competition, so we want so we actually submit the two, run, run, run.",
            "That also uses some additional knowledge base also.",
            "But our approach, the one I'm presenting here is mostly what is called server.",
            "This Ki just based on distance supervision only and the baseline was far below.",
            "And this is the evaluation of what we call data."
        ],
        [
            "The prediction we have several waited for validation, so validation is a different problem.",
            "They give you the triple and they want to know whether that's true or false, so the baseline was 50% random because they were 50% for some through another system was able to really improve that baseline by fire, and that's the second performance of the second system.",
            "So in."
        ],
        [
            "Conclusion.",
            "What they just presented is that we can use distance supervision as a scalable and effective solution to extend knowledge basis, and that's actually a very interesting problem also for the industry not also not only for research.",
            "However, this band relation approach is very limited because he's very low in recall and there is no way to get all the mention with two entities together, especially for rare entities, which is what really interesting at the end of the day.",
            "So what we did was to impose these two approaches, unit relation and composite context that you said oriented document and experience demonstrated that what we claim is true.",
            "So we can largely improve recall.",
            "And we also want the challenge, by the way, and I think was a very interesting research, but now we're following it up because there is much more we can do and what we are doing these days is, first of all, focusing on the entry detection problem.",
            "So again, we do the introduction itself with distance supervision, because this system cannot be really applied out of our knowledge base, only you need to have this in the election system that are not out of the box.",
            "In many cases doesn't exist for a lot of domains.",
            "So second and more interesting, I would say from a research perspective we are using probabilistic reasoning to leverage constraints from the ontology on top of the induced knowledge, and that's something very good.",
            "It works, I mean I cannot anticipate what they're doing now, but there is something very good going on in this direction.",
            "And finally we are using knowledge base completion validation on top of distracted knowledge to further improve this.",
            "Coverage, precision and recall, and for that we use deep learning methods.",
            "That's it, and thank you so much for your attention."
        ],
        [
            "So time for questions.",
            "We have one here.",
            "Hi, thanks for the talk.",
            "I have a quick question.",
            "I want you.",
            "Let's say the relations captured between the two and there can be multiple relations example like if there's a person in the movie could be director off or it could be a performer of like years for company could be found.",
            "It could be bankrupted.",
            "I wonder if you've looked into that area.",
            "What would be the insights to identify to solve those kind of problems?",
            "So I would say that's actually our future works.",
            "To do that to you can take into account multiple relation at the same time, and that's what you infer that we can use knowledge base completion and validation approaches.",
            "Why?",
            "Because they basically learned representation about different relations and entities concurrently.",
            "So that's the way to handle what you suggested and.",
            "I mean, it's pretty effective and hopefully we're going to have some publication out.",
            "About OK, have a question myself so that you are using CNN's in order to identify the different patterns.",
            "I wonder have you consider other alternative architectures like Colossians by listing homes or something like that?",
            "Yeah, there are variety of them and in reality we experimented with many.",
            "We ended up with the architectural just described.",
            "Also because this was the state of the art and I would say that was good enough for our partners.",
            "Our job here was just to try.",
            "How we can deal with different relation that are not just explicit?",
            "But yes we can apply less DMS and others and they believe.",
            "There's interesting research there, but it's more about optimization.",
            "I would say that in my opinion, the most interesting part here is how to go out to the knowledge side.",
            "More questions.",
            "Well, I have another one.",
            "Still in the meantime.",
            "The buildings that ministered you generate are.",
            "I mean, if I want to use my own embeddings in your architecture, can I do it or do I need to generate and balance within your system in order to test what the system does is to use pre trained word?",
            "Two VEC, embeddings to when it's initialized?",
            "I can show you the network here.",
            "OK, so we basically use pretraining worth work embedding.",
            "But then there are further training during the training of the final system.",
            "Yeah, you can use either embedding as well.",
            "I wonder if you needed to encode some of their relations.",
            "I mean so if the property is coming from their relational knowledge in debating sensors, what they've done actually is not in this slide because it was difficult to explain.",
            "But for example, we encode the types.",
            "And we add another layer somewhere.",
            "Over here where you can also put the tight constraints as an embedding and there are a lot of other things that are difficult to explain this chart, but if you read the paper you're going to see that there are a lot of.",
            "Small tricks here and there.",
            "OK, time for another question.",
            "Where were over.",
            "OK, so I think we're done.",
            "Let's thank again the speaker."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, thank you the this joint work with Michael Aktay, who is actually there and and Anna and get on this been performed at IBM Research and while nandina and get to know where intern with us now actually in and then I work with us and get Anna is going to join soon.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm going to talk about.",
                    "label": 0
                },
                {
                    "sent": "Very interesting and practical use keys for the use of deep learning and ontologies in the industry and also very interesting for science, which is how to train relation instruction systems using distance supervision.",
                    "label": 0
                },
                {
                    "sent": "And I'm gonna phase.",
                    "label": 0
                },
                {
                    "sent": "Mostly 1 interesting challenge, which is how can we use this approach to deal with implicit knowledge?",
                    "label": 1
                },
                {
                    "sent": "Meaning.",
                    "label": 0
                },
                {
                    "sent": "How to recognize relation between entities that do not occur in the same sentence at all but still are related.",
                    "label": 0
                },
                {
                    "sent": "So for this purpose we spend substantial time in several research projects and we ended up with two different approaches that could be used to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "One is to use what we call unary relations.",
                    "label": 1
                },
                {
                    "sent": "And the other is to use composite context for this.",
                    "label": 0
                },
                {
                    "sent": "Aim so we evaluate this approach in research benchmarks, showing that basically we can larging please recall and we also apply that to the SWC challenge, organized when year ago.",
                    "label": 0
                },
                {
                    "sent": "And we ended up winning the challenge.",
                    "label": 0
                },
                {
                    "sent": "So this talk is about all that.",
                    "label": 0
                },
                {
                    "sent": "Let me introduce the problem first.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the idea is that if we have a knowledge graph on one hand and domain corpus.",
                    "label": 1
                },
                {
                    "sent": "With documents about the same topic or domain of the Knowledge Graph.",
                    "label": 0
                },
                {
                    "sent": "We can try to leverage the information documents to gather more relations to populate the pre existing knowledge graph.",
                    "label": 0
                },
                {
                    "sent": "For example, if we have a financial knowledge graph like Permadi from Thomson Reuters.",
                    "label": 0
                },
                {
                    "sent": "And we have a financial corpus we can find out more about companies if we have the pedia and web scale corpus.",
                    "label": 0
                },
                {
                    "sent": "Like common crawl, we can extend the PDS semi automatically.",
                    "label": 1
                },
                {
                    "sent": "So the approach that we're looking for here is to leverage combination of a pre existing entity detection and linking system, which is a prerequisite for this work.",
                    "label": 0
                },
                {
                    "sent": "Together occurrences of entities in the ontology and identify the occurrence of those entities in the corpus.",
                    "label": 0
                },
                {
                    "sent": "So we can collect context where they are located, and finally, trainer relation extraction system from those context without any human intervention.",
                    "label": 0
                },
                {
                    "sent": "Any labeling task.",
                    "label": 0
                },
                {
                    "sent": "So it is, I would say 100% automatic with the only thing that you need to have a pre existing anti rejection linking system to start from and then ontology.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the idea is that the state of the art for distant supervision, relation extraction, is using binary context.",
                    "label": 0
                },
                {
                    "sent": "Which is the simplest approach we can get occurrences of entities belonging to different types from text.",
                    "label": 0
                },
                {
                    "sent": "In this case we have companies and countries.",
                    "label": 0
                },
                {
                    "sent": "And if they are actually related in the Knowledge Graph, so there is a triple about them, we can.",
                    "label": 0
                },
                {
                    "sent": "Use them as positive example to train a system.",
                    "label": 0
                },
                {
                    "sent": "Otherwise.",
                    "label": 0
                },
                {
                    "sent": "We can get other appearances that are still of the same types, but they are not in the ontology.",
                    "label": 0
                },
                {
                    "sent": "In that case they are negative examples to train our system.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can collect a very large amount of these training data and we can feed them into a deep net.",
                    "label": 0
                },
                {
                    "sent": "Was architecturally described here.",
                    "label": 0
                },
                {
                    "sent": "And the deep net does the job of analyzing several occurrences of the same entity pairs, feed them into a convolutional net that is basically identifying the is, analyzing the left, the center and the right context, then is aggregating all that, and we can do that when is 1 sentence at a time.",
                    "label": 0
                },
                {
                    "sent": "But more interesting, we can also feed multiple sentences containing the same two entities and aggregate occurrence across multiple.",
                    "label": 0
                },
                {
                    "sent": "Enter this in a second layer where all these mentions are aggregated and using a network in network approach we can find out ultimately a finer layer where the actual relation to predicted to be predicted is represented by a dimension and we train the network this way.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This network can be applied to very large problems.",
                    "label": 0
                },
                {
                    "sent": "And this kind of state of the art we just modified slightly an existing work made by Lynn.",
                    "label": 0
                },
                {
                    "sent": "It's all so adding some layer, but that's not the main contribution of this paper.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The main contribution of the paper is.",
                    "label": 0
                },
                {
                    "sent": "How can we extend this architecture to go from binary context to implicit context, where these two entities are not together at all?",
                    "label": 0
                },
                {
                    "sent": "So for that we define an architecture that we call Sokratis.",
                    "label": 0
                },
                {
                    "sent": "It is able to combine binary, unar and composite contexts and merge them together to find out more information than what you can get with explicit information only.",
                    "label": 0
                },
                {
                    "sent": "So let me start talking about the unary relation case.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are you going to election is the right from a bunch of binary usually order relation?",
                    "label": 0
                },
                {
                    "sent": "Once we fix one argument and we take the other argument free.",
                    "label": 0
                },
                {
                    "sent": "So in this case we have this location relation between companies and countries and if we fix the second argument we can find several occurrence of things located in the other states, many other located in Canada in India in UK so.",
                    "label": 0
                },
                {
                    "sent": "What we did was to put together the relation and argument to create a unary predicate and the examples to train this predicate is the aggregation of all the argument on the left.",
                    "label": 0
                },
                {
                    "sent": "So this same in this way we can collect what we call unity contexts that are very similar to the binary context, with the difference that we need just one entity at the time and not anymore two entities there.",
                    "label": 0
                },
                {
                    "sent": "So in this example here we have this company JB Hifi and we have multiple occurrence.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of this company and we want to figure out whether this located in Australia so the unit relation is location Australia.",
                    "label": 0
                },
                {
                    "sent": "So we can identify a lot of good interesting signal to train the net by looking at surrounding words.",
                    "label": 0
                },
                {
                    "sent": "For example, we can see that many other words around our actress, trillion companies.",
                    "label": 0
                },
                {
                    "sent": "They provide good signal.",
                    "label": 0
                },
                {
                    "sent": "To figure out the counter of this company, we also find that the Australian Stock Exchange is there.",
                    "label": 1
                },
                {
                    "sent": "So there is not the explicit mention that this company is located in Australia.",
                    "label": 0
                },
                {
                    "sent": "The World Australia is not there at all, but we still are able to train a network.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By aggregating many of these unity sets and feed basically the network represented before, where instead of putting 2 arguments and position embedding for two arguments, we just use position embedding forward argument only and then the last layer is made of all possible unary predicates.",
                    "label": 0
                },
                {
                    "sent": "You can think about and there could be up to several thousands of them.",
                    "label": 0
                },
                {
                    "sent": "The good news is that deep learning is scalable enough to really recognize thousands of relation at the same time, so we can scale it out.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is 1 approach.",
                    "label": 0
                },
                {
                    "sent": "The other approach we tried is what we call composite context, so this is applicable in another use case when instead of having a regular corpus of document, we have what we call title oriented documents that are documents about a specific entity.",
                    "label": 0
                },
                {
                    "sent": "For example, you can think about Wikipedia pages.",
                    "label": 0
                },
                {
                    "sent": "You can think about web page of a specific company.",
                    "label": 0
                },
                {
                    "sent": "So anytime you have the documents about that entity, the assumption that we can be made is that.",
                    "label": 0
                },
                {
                    "sent": "Most of the information there is related to the target focus entity, so we can somehow collect implicit information in this way.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And let me start.",
                    "label": 0
                },
                {
                    "sent": "Now I'm going to describe this approach.",
                    "label": 0
                },
                {
                    "sent": "So this is an example.",
                    "label": 0
                },
                {
                    "sent": "So if we have these.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Company ABC Electronic Canada and we have a website about that company.",
                    "label": 0
                },
                {
                    "sent": "So in there interested in recognizing the relation at parties phone numbers.",
                    "label": 0
                },
                {
                    "sent": "So how can we do that?",
                    "label": 0
                },
                {
                    "sent": "We can first of all use an anti rejection linking system to recognize phone numbers and there are several off the shelf.",
                    "label": 0
                },
                {
                    "sent": "It's not a big deal.",
                    "label": 0
                },
                {
                    "sent": "Then we can derive what we call composite context out of those, which is basically the the unary context of the just the number itself plus the name of the company in the.",
                    "label": 0
                },
                {
                    "sent": "Attached to it and then we can label them as positive or negative depending if they are in the original knowledge graph or they're not.",
                    "label": 0
                },
                {
                    "sent": "So in this case, the real phone number was labeled as positive because the ontology itself, the knowledge base, tell us that that's the phone number, so we can train a system this way.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me go to the evaluation part of my.",
                    "label": 0
                },
                {
                    "sent": "Golf.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we evaluated first our baseline approach.",
                    "label": 0
                },
                {
                    "sent": "So how good is our system to recognize banner relation?",
                    "label": 0
                },
                {
                    "sent": "So we just took when standard benchmark which is being released by Sebastian Riedel and the call in another.",
                    "label": 0
                },
                {
                    "sent": "And this being used widely, so is a pretty small one with 57 relation and comparing our system to the state of the art.",
                    "label": 0
                },
                {
                    "sent": "Because you download of their code and run it on the same data set, we just show that is compatible a little bit better, but that's not what we care about.",
                    "label": 0
                },
                {
                    "sent": "It was just to test whether this is a good enough approach.",
                    "label": 0
                },
                {
                    "sent": "Then we.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tested the unary approach for dinner approach.",
                    "label": 0
                },
                {
                    "sent": "The prerequisite is that you need a very large ontology, so such a tiny corpus cannot be used for that.",
                    "label": 0
                },
                {
                    "sent": "We develop a very large scale corpus made out of DB pedia and common crawl.",
                    "label": 0
                },
                {
                    "sent": "So in this corpus is much bigger.",
                    "label": 0
                },
                {
                    "sent": "We have millions of sentences.",
                    "label": 0
                },
                {
                    "sent": "Hundreds of different relation types and we can collect millions of context.",
                    "label": 0
                },
                {
                    "sent": "So in this case what we tested was the ability to predict Una relacion versus binary relation.",
                    "label": 0
                },
                {
                    "sent": "And as you can see they basically perform similarly.",
                    "label": 0
                },
                {
                    "sent": "But if you combine the two hour recall double meaning that they are looking for totally different things, so the combination of the two is basically.",
                    "label": 0
                },
                {
                    "sent": "Doing what the job is supposed to do, improving recall.",
                    "label": 0
                },
                {
                    "sent": "OK then.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Part of my evolution is the application to the SWC challenge.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this challenge is particularly difficult because is about predicting the private version of Permadi Thomson Reuters from the public portion.",
                    "label": 1
                },
                {
                    "sent": "So meaning that the organizer provide the public permit which is actually open for everybody and the test set was made out of very unknown company that they have in their private portion and they wanted us to recognize the foundation here.",
                    "label": 0
                },
                {
                    "sent": "That quarter.",
                    "label": 0
                },
                {
                    "sent": "Phone numbers, the Corder country given just the website URL as an input and the company name.",
                    "label": 0
                },
                {
                    "sent": "This so this same what we do.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We collected all the websites of this company and we ran our approach.",
                    "label": 0
                },
                {
                    "sent": "We had off the shelf name and definition system as described before and we were able to collect 80,000 training company title oriented document for them.",
                    "label": 0
                },
                {
                    "sent": "We were able to collect roughly 90% of the company in the test set and then we use this Socrates approach for the foundation.",
                    "label": 1
                },
                {
                    "sent": "Here in the court's phone number.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The evaluation showed that the.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "System outperform both every other system in the competition, so we want so we actually submit the two, run, run, run.",
                    "label": 0
                },
                {
                    "sent": "That also uses some additional knowledge base also.",
                    "label": 0
                },
                {
                    "sent": "But our approach, the one I'm presenting here is mostly what is called server.",
                    "label": 0
                },
                {
                    "sent": "This Ki just based on distance supervision only and the baseline was far below.",
                    "label": 0
                },
                {
                    "sent": "And this is the evaluation of what we call data.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The prediction we have several waited for validation, so validation is a different problem.",
                    "label": 0
                },
                {
                    "sent": "They give you the triple and they want to know whether that's true or false, so the baseline was 50% random because they were 50% for some through another system was able to really improve that baseline by fire, and that's the second performance of the second system.",
                    "label": 0
                },
                {
                    "sent": "So in.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Conclusion.",
                    "label": 0
                },
                {
                    "sent": "What they just presented is that we can use distance supervision as a scalable and effective solution to extend knowledge basis, and that's actually a very interesting problem also for the industry not also not only for research.",
                    "label": 1
                },
                {
                    "sent": "However, this band relation approach is very limited because he's very low in recall and there is no way to get all the mention with two entities together, especially for rare entities, which is what really interesting at the end of the day.",
                    "label": 0
                },
                {
                    "sent": "So what we did was to impose these two approaches, unit relation and composite context that you said oriented document and experience demonstrated that what we claim is true.",
                    "label": 0
                },
                {
                    "sent": "So we can largely improve recall.",
                    "label": 0
                },
                {
                    "sent": "And we also want the challenge, by the way, and I think was a very interesting research, but now we're following it up because there is much more we can do and what we are doing these days is, first of all, focusing on the entry detection problem.",
                    "label": 0
                },
                {
                    "sent": "So again, we do the introduction itself with distance supervision, because this system cannot be really applied out of our knowledge base, only you need to have this in the election system that are not out of the box.",
                    "label": 0
                },
                {
                    "sent": "In many cases doesn't exist for a lot of domains.",
                    "label": 0
                },
                {
                    "sent": "So second and more interesting, I would say from a research perspective we are using probabilistic reasoning to leverage constraints from the ontology on top of the induced knowledge, and that's something very good.",
                    "label": 1
                },
                {
                    "sent": "It works, I mean I cannot anticipate what they're doing now, but there is something very good going on in this direction.",
                    "label": 0
                },
                {
                    "sent": "And finally we are using knowledge base completion validation on top of distracted knowledge to further improve this.",
                    "label": 0
                },
                {
                    "sent": "Coverage, precision and recall, and for that we use deep learning methods.",
                    "label": 0
                },
                {
                    "sent": "That's it, and thank you so much for your attention.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So time for questions.",
                    "label": 0
                },
                {
                    "sent": "We have one here.",
                    "label": 0
                },
                {
                    "sent": "Hi, thanks for the talk.",
                    "label": 0
                },
                {
                    "sent": "I have a quick question.",
                    "label": 0
                },
                {
                    "sent": "I want you.",
                    "label": 0
                },
                {
                    "sent": "Let's say the relations captured between the two and there can be multiple relations example like if there's a person in the movie could be director off or it could be a performer of like years for company could be found.",
                    "label": 0
                },
                {
                    "sent": "It could be bankrupted.",
                    "label": 0
                },
                {
                    "sent": "I wonder if you've looked into that area.",
                    "label": 0
                },
                {
                    "sent": "What would be the insights to identify to solve those kind of problems?",
                    "label": 0
                },
                {
                    "sent": "So I would say that's actually our future works.",
                    "label": 0
                },
                {
                    "sent": "To do that to you can take into account multiple relation at the same time, and that's what you infer that we can use knowledge base completion and validation approaches.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Because they basically learned representation about different relations and entities concurrently.",
                    "label": 0
                },
                {
                    "sent": "So that's the way to handle what you suggested and.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's pretty effective and hopefully we're going to have some publication out.",
                    "label": 0
                },
                {
                    "sent": "About OK, have a question myself so that you are using CNN's in order to identify the different patterns.",
                    "label": 0
                },
                {
                    "sent": "I wonder have you consider other alternative architectures like Colossians by listing homes or something like that?",
                    "label": 0
                },
                {
                    "sent": "Yeah, there are variety of them and in reality we experimented with many.",
                    "label": 0
                },
                {
                    "sent": "We ended up with the architectural just described.",
                    "label": 0
                },
                {
                    "sent": "Also because this was the state of the art and I would say that was good enough for our partners.",
                    "label": 0
                },
                {
                    "sent": "Our job here was just to try.",
                    "label": 0
                },
                {
                    "sent": "How we can deal with different relation that are not just explicit?",
                    "label": 0
                },
                {
                    "sent": "But yes we can apply less DMS and others and they believe.",
                    "label": 0
                },
                {
                    "sent": "There's interesting research there, but it's more about optimization.",
                    "label": 0
                },
                {
                    "sent": "I would say that in my opinion, the most interesting part here is how to go out to the knowledge side.",
                    "label": 0
                },
                {
                    "sent": "More questions.",
                    "label": 0
                },
                {
                    "sent": "Well, I have another one.",
                    "label": 0
                },
                {
                    "sent": "Still in the meantime.",
                    "label": 0
                },
                {
                    "sent": "The buildings that ministered you generate are.",
                    "label": 0
                },
                {
                    "sent": "I mean, if I want to use my own embeddings in your architecture, can I do it or do I need to generate and balance within your system in order to test what the system does is to use pre trained word?",
                    "label": 0
                },
                {
                    "sent": "Two VEC, embeddings to when it's initialized?",
                    "label": 0
                },
                {
                    "sent": "I can show you the network here.",
                    "label": 0
                },
                {
                    "sent": "OK, so we basically use pretraining worth work embedding.",
                    "label": 0
                },
                {
                    "sent": "But then there are further training during the training of the final system.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you can use either embedding as well.",
                    "label": 0
                },
                {
                    "sent": "I wonder if you needed to encode some of their relations.",
                    "label": 0
                },
                {
                    "sent": "I mean so if the property is coming from their relational knowledge in debating sensors, what they've done actually is not in this slide because it was difficult to explain.",
                    "label": 0
                },
                {
                    "sent": "But for example, we encode the types.",
                    "label": 0
                },
                {
                    "sent": "And we add another layer somewhere.",
                    "label": 0
                },
                {
                    "sent": "Over here where you can also put the tight constraints as an embedding and there are a lot of other things that are difficult to explain this chart, but if you read the paper you're going to see that there are a lot of.",
                    "label": 0
                },
                {
                    "sent": "Small tricks here and there.",
                    "label": 0
                },
                {
                    "sent": "OK, time for another question.",
                    "label": 0
                },
                {
                    "sent": "Where were over.",
                    "label": 0
                },
                {
                    "sent": "OK, so I think we're done.",
                    "label": 0
                },
                {
                    "sent": "Let's thank again the speaker.",
                    "label": 0
                }
            ]
        }
    }
}