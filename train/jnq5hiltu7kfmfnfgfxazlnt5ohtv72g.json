{
    "id": "jnq5hiltu7kfmfnfgfxazlnt5ohtv72g",
    "title": "Sharing Features among Dynamical Systems with Beta Processes",
    "info": {
        "author": [
            "Emily Fox, Department of Statistics, University of Washington"
        ],
        "published": "Jan. 19, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Markov Processes",
            "Top->Computer Science->Algorithmic Information Theory"
        ]
    },
    "url": "http://videolectures.net/nips09_fox_sfa/",
    "segmentation": [
        [
            "So in this talk, we're going to describe a method of relating multiple time series within a Bayesian nonparametric framework.",
            "So there's a large class of datasets that can be described as exhibiting very complex but pattern behaviors.",
            "For example, honey bees which appear to have rather chaotic motion are actually switching between a set of dances, each of which can be described using simple linear dynamical models.",
            "Likewise, human motion conference audio in stock data can be described as switches between a set of simpler models based on some underlying behavior state of the system, and previously each of these datasets has been modeled using Markov switching processes.",
            "And more recently, basean nonparametric models have been applied in order to allow us to learn the number of behavior states."
        ],
        [
            "From the data, however, each of these models is focused on analyzing a single time series.",
            "So one open question is, how do you jointly model multiple related time series?",
            "So, for example, imagine you have a collection of videos of people performing a set of exercise routines.",
            "How do you transfer knowledge between these datasets in order to improve things such as parameter estimates?",
            "Also, it allows us to find interesting structure and how they relate to one another.",
            "So an ideal model."
        ],
        [
            "Is one that allows each time series to have an unbounded number of possible behavior states while encouraging the use of a sparse subset.",
            "Also, we would like our model to encourage sharing of dynamic behaviors between the different time series, but also allowing some variability.",
            "So for example, maybe in one movie Sundays doing side twists, but that motion doesn't appear in any of the other movies."
        ],
        [
            "So to build up to such a model, it's useful to start by considering a method of describing a single time series with these types of pattern behaviors.",
            "So a classical approach here is the hidden Markov model, and it assumes that there's an underlying discrete valued state sequence represented by the random variables, which is Markov with respect to some transition distributions Pi.",
            "So for the motion capture data, this might be a sequence of labels of jumping jacks, side twists, and squats, and so on.",
            "Then condition on the state sequence.",
            "The observations are independent emissions from some class of distributions, which in this case we take to be Gaussian.",
            "So back to the mocap data.",
            "This might be body position observations and one can view a sample path of the state sequence as a walk through the following state versus time lattice.",
            "Imagine that at the first time step you start, in the second state, which here corresponds to maybe jumping jacks state and you have the following set of possible transitions and the relative weights of the air."
        ],
        [
            "As shown here, indicate the probability of taking each of these transitions and is captured by that states transition distribution.",
            "And then let's imagine at the next time step we persist in the same jumping jacks state.",
            "Have the same set of possible transitions and then we transition to squat behavior state and have a different set of possible transitions and so on.",
            "While returning to the case where you have multiple related time series, let's start by modeling each one of them via an HMM.",
            "Then if we like to relate the time Series A common approach is just simply to assume that they are all generated from the same set of models.",
            "And if we want to allow for infinitely many possible behaviors.",
            "One attractive approach is to appeal to Bayesian Nonparametrics.",
            "And one possibility is to build."
        ],
        [
            "In a previous basean nonparametric hmm known as the hierarchical dearsley process, or HTP hmm?",
            "And this model allows for infinitely many possible behaviors, encourages the use of sparse subset, so that's great.",
            "However, by taking this approach, it assumes that each of the time series is switching between exactly the same set of states in exactly the same fashion, and that's clearly not as flexible of a model as we'd wanted.",
            "So instead, we're going."
        ],
        [
            "Show how an alternative Bayesian nonparametric method, specifically building on the beta process, allows us to achieve a model where each time series can have different behaviors.",
            "That is switching between and they can switch between them in different ways."
        ],
        [
            "OK, so we start by providing some background on the beta process and a realization from a beta process can be described as a collection of weights and atoms that are a draw from a nonhomogeneous Passan process.",
            "Each of the weights is constrained to be between zero and one, so you can just think of it as defining a coin, flipping probability, and through this construction we define an infinite collection of these coin weights, and so we show a realization here and we can use such a realization to define a set of observations from a Bernoulli process.",
            "In each of these observations can be described as a collection of unit mass atoms that are the result of coin flips at each location in our beta process measure.",
            "So, for example, we walk along our beta process realization and flip a coin where the probability of getting a heads is determined by the weight of that Atom.",
            "So here we got ahead.",
            "Another heads a tail, and so on, and we keep flipping coins at each location.",
            "And because the beta process random measure has finite total expected mass.",
            "We're going to get a finite number of successes in this infinite coin flipping sequence, and this is later going to relate to the fact that each of our time series is switching between a finite number of dynamic behaviors.",
            "And so we're going to think of each of these observations from this Bernoulli process as defining a feature vector.",
            "An infinite dimensional feature vector, where the set of chosen features are exactly the locations in which we got a heads.",
            "And here we show series of observations from the Bernoulli process and through the sharing the same base measure the same beta process realization.",
            "We see that we've defined, in essence a collection of feature vectors that have a lot of sharing, but also some variability.",
            "OK, so in the construction so far."
        ],
        [
            "We've assumed that we have this beta process realization and independent draws from the Bernoulli process.",
            "However, we can actually marginalise over the random measure and look at the predicted distribution on a new feature vector based on the feature vectors we've observed so far, and this process is commonly referred to as the Indian buffet process.",
            "So imagine you have a restaurant with a buffet line with infinitely many dishes in each dish corresponds to a different feature.",
            "Customers arrive at this restaurant and choose dishes from this buffet line.",
            "The first customer simply.",
            "Comes in and he chooses some Passan number of dishes.",
            "The next customer comes in and is more likely to sample the dishes at the first customer show."
        ],
        [
            "And then chooses some prasan number of new dishes in this process continues with each customer sampling dishes in proportion to how often they were sampled before, and then choosing some new dishes.",
            "And here we show."
        ],
        [
            "Feature matrix associated with the realization from the Indian buffet process.",
            "Each row corresponds to a different customer in each column to different dish and the white squares indicate the set of selected dishes.",
            "OK, so now we're going to describe how we use such a feature model in order to relate multiple time series.",
            "So we're going to start by modeling each one of our end individual time series VN.",
            "Hmm, then the feature matrix we had before, which described a series of customers eating a collection of dishes is now going to describe a collection of time series switching between a set of dynamic behaviors.",
            "So specifically, we define a shared library of infinitely many possible behaviors, and then each object has a feature vector which is a row of this matrix.",
            "So it's an infinite dimensional feature vector that defines which behaviors that object switches between and the way it does that is by constraining a set of transition distributions by driving some elements to 0."
        ],
        [
            "So that that object can only switch between the set of behaviors it is chosen.",
            "So specifically, imagine that you define a collection of transition distributions for that object in some infinite dimensional space.",
            "Then you do an elementwise product of that distribution with that objects feature vector renormalize."
        ],
        [
            "And that defines what we call our feature constrained transition distributions.",
            "So here's the overall general process and what you see as we place a dearsley prior on the transition distributions where the feature vector operates on the parameters of that distribution.",
            "Then the state evolves accord."
        ],
        [
            "Into these feature constrained transition distributions and that state indexes into our shared collection of infinitely many possible behaviors to generate the observation.",
            "In this Kappa, parameters simply increases the prior probability of a self transition.",
            "OK, so we then tie together all of the feature vectors under a common beta process prior, just as in the model we."
        ],
        [
            "Describe before, so specifically, each one of the feature vectors is a result of an infinite coin flipping sequence based on a shared data process measure.",
            "And through this construction, we're encouraging sharing of dynamic behaviors between the different time series, but allowing for some variability, and we refer to this model as a BP HMM.",
            "So for."
        ],
        [
            "Motion capture data we showed earlier on, we're actually going to rely on a slightly more complicated model because if you use an HMM to model that data, what it says is that the body position measurements within a behavior state such as jumping jacks are going to be independent.",
            "And that's clearly not true.",
            "So to deal with that, we're going to add in these extra edges in order to add more correlation structure in the observations.",
            "And this model is referred to as a switching VAR process, and in order are.",
            "Our process."
        ],
        [
            "Assumes that each observation vector is a linear combination of the previous R."
        ],
        [
            "Observation vectors plus some additive Gaussian noise.",
            "So in reality, we're going to be considering this slightly more complicated model that has this added correlation structure on the observations, and we refer to this model as the BP AR HMM.",
            "OK, so for inference we're going to rely on an MCMC sampler.",
            "And one thing we're going to exploit over and over again is the fact that if you condition on the feature matrix, our model decomposes into a collection of finite switching far processes, so without any truncation, we are back into the world of finite switching for processes."
        ],
        [
            "OK, So what we're going to focus on in this talk is simply how do we re sample this feature matrix?",
            "And when we do that, we're going to look at the posterior distribution on each of these features, which decomposes into."
        ],
        [
            "Iron likelihood term.",
            "The likelihood term you know."
        ],
        [
            "This marginalizes over the latent state sequence, and even though there's an exponentially large number of possible state sequences for each of these time series, we can compute this very efficiently by harnessing the induced finite switching var process, and we simply run forward backward algorithm.",
            "Marginalizing over the latent state sequence to compute the likelihood of the observations.",
            "Then that likelihood is weighted by a prior term given by the ICP.",
            "We're going to focus on this term little bit now.",
            "So let's assume that you have a feature matrix.",
            "Instantiate it and you simply want to re sample the features associated with the time series.",
            "Well, the IVP treats shared and unique features separately, so we're going to look at these two cases separately as well.",
            "For each one of the shared features, we're simply going to walk along and sample that feature with prior probability proportional to how many of the other time series also sampled that dynamic behavior, and that's then waited by the likelihood computed using the forward backward algorithm.",
            "So we."
        ],
        [
            "Along re sample are shared features.",
            "Then we get to the set of unique features."
        ],
        [
            "And the ICP dictates that we choose."
        ],
        [
            "Some prasan Alpha over N new features.",
            "However."
        ],
        [
            "Because our model is not conjugate, we cannot compute the posterior probability of this event in closed form.",
            "In previous models that have looked at this non conjugate ICP case have either relied on finite truncations of the ICP or proposals from the prasan prior that result in very low acceptance rates in the high dimensional applications, we're going to consider so instead, we develop a birth death, reversible jump, and Sam C algorithm, and it.",
            "What that does is it takes our unique set of features and proposes either the birth of a new unique feature or the death of one of the existing unique features.",
            "OK, so to test our be par hmm, we examined a collection of videos of people performing a set of exercise routines.",
            "And these videos were taken from the CMU MOCAP database and we get observations of 62 joint angle and body position measurements, and we're only going to choose the 12 that correspond to gross motor movement because for this application."
        ],
        [
            "We don't care about things like what the fingers are doing.",
            "And so the goal here is to discover common behaviors between each of these different movies.",
            "And as you see, we're actually able to achieve this.",
            "What I plot here is a series of skeleton plots where each skeleton plot corresponds to learned."
        ],
        [
            "Take you a segment of at least 2 seconds of data.",
            "Then I grouped together all of the segments that were generated from the same behavior state and the color of the box indicates the true behavior category."
        ],
        [
            "So what you can see from this is the fact that we've been able to identify and group together 6 examples of jumping jacks."
        ],
        [
            "Some side."
        ],
        [
            "Swiss arm circles and squats."
        ],
        [
            "And one nice thing about the model is the fact that we're able to identify a collection of behaviors that appeared in one movie."
        ],
        [
            "But no other movie.",
            "We did however, split a few motion categ."
        ],
        [
            "And one of them is knee raises.",
            "But if you look closely at the data, there's a reason for this.",
            "In one case, the person has a lot of side to side upperbody movement where the other person is doing the more standard motion.",
            "And similarly for the running motion."
        ],
        [
            "And one person is running with their hands In Sync with their knees and the other person.",
            "The more standard out of sync motion and this other person at the end is in the middle of doing a jumping Jack as it running.",
            "They're just generally confused, and I guess that's what happens when you ask computer science students to do exercises.",
            "But"
        ],
        [
            "Offense with you guys, but overall.",
            "You see that we're clearly able to identify a set of behaviors that appear in multiple movies, but allow for some variability."
        ],
        [
            "Tween these.",
            "So we also compared the performance of our algorithm to a couple others proposed in literature.",
            "One is a Gaussian mixture model and the other is a hidden Markov model, both using first difference observations.",
            "And then we also compared to model that uses the shared hierarchical dearsley process prior.",
            "And here I show the true feature matrix and here is the feature matrix in Ferd by our proposed be par hmm and it's averaged over a collection of MCMC samples on the bottom I show the feature matrices for these alternative GMM and HMM algorithms and for these algorithms I set the number of states to 12, which is the truth.",
            "Learn the models using EM, Run 10 initializations, choose the most likely.",
            "So we're trying to be as nice as possible to these algorithms.",
            "However, by pulling all of the data together.",
            "These models assume a lot of sharing, which you can see in these strong bands of white.",
            "And here's the matrix that's learned by the model with that shared HTP prior.",
            "But what you see is that our proposed be par hmm is able to pick up on this unique feature structure without adding too many new states.",
            "So in conclusion, we examined a method of relating multiple time series within invasion on pair."
        ],
        [
            "Metric framework by utilizing the beta process.",
            "We then developed an efficient MCMC inference scheme for the non conjugate ICP case that's useful for high dimensional applications.",
            "And finally, we demonstrated some impressive performance on a challenging motion capture data set that showed performance exceeding that of alternative methods.",
            "Thank you.",
            "OK, we have some time for questions.",
            "Hi so in this case you choose a set of motions that have more lesser, clearer structure, right?",
            "But if you think of the United case of human motion, you know this case is not going to be.",
            "I mean, it's not going to be like that, right?",
            "So I have a couple of questions.",
            "Do you think this will scale two more general continuous motions?",
            "And how is the complexity of the method?",
            "Could you use, you know?",
            "Large collection of motions.",
            "OK so.",
            "To answer all these questions.",
            "I think that this set of motions that you actually see in these exercise routines encapsulates the kind of motions you see people doing when they're walking or moving, and I think because it's a beige and nonparametric approach, you might not get labels that are jumping jacks side twists.",
            "You know these type of descriptions, but you will be able to cluster on common commonality's between the behaviors, even though it might not be something that we as humans could say.",
            "This is what we call it and in terms of.",
            "Dealing with large numbers of behaviors.",
            "Once again, that's a nice thing with patient nonparametric approaches.",
            "And as you get more data, it helps you infer these different behaviors.",
            "But if there is something that's significantly different in some qualitative sense, we're not fixing the number of behaviors, and we're not using any training training data.",
            "This is fully unsupervised.",
            "In that sense.",
            "You know there's some hope in being able to capture that.",
            "Is the segmentation into states related to the Kappa parameter?",
            "Is it sensitive to that Kappa parameter which is so the cap was a self transition parameter?",
            "Definitely if you don't have that parameter it does over segment the data and so that's it's related to papers that we've had before showing how.",
            "If you have self transition bias built into your prior, it helps with real world data in terms of getting the type of segmentations you expect to see where motions persist over periods of time, so it's moving towards this semi Markov type type of model.",
            "And then did you have to put some sort of prior on that capital parameter so all related to the lengths of the motions?",
            "No, we actually put a rather uninformative prior, so we put priors on that sticky Kappa parameter aprior on.",
            "There was another parameter that determine things about the transition distribution, which is.",
            "And then there's the ICP parameter, so there are three different hyperparameters, and we put priors on those and they.",
            "Our results were rather insensitive to the settings of those priors, is more sensitive to how you initialize the data.",
            "If you stumble from your model, can you tell us what it looks like and you compare it to IBM's so I actually got that question yesterday, and since then I haven't been able to do that.",
            "I haven't looked at that yet for this model.",
            "For other models I've worked on, I have looked at sampling from it, but.",
            "My guess is it's not going to look quite like human motion if you just sample from the from the prior of this model.",
            "But I think there are interesting future directions that could constrain it more and get even better formulation directly applied to human motion, if that's what you're interested in and what was.",
            "Sorry, Kevin, the last part was.",
            "Oh no, I I yeah it's an interesting direction.",
            "I haven't looked at that.",
            "OK, let's thank the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this talk, we're going to describe a method of relating multiple time series within a Bayesian nonparametric framework.",
                    "label": 0
                },
                {
                    "sent": "So there's a large class of datasets that can be described as exhibiting very complex but pattern behaviors.",
                    "label": 0
                },
                {
                    "sent": "For example, honey bees which appear to have rather chaotic motion are actually switching between a set of dances, each of which can be described using simple linear dynamical models.",
                    "label": 0
                },
                {
                    "sent": "Likewise, human motion conference audio in stock data can be described as switches between a set of simpler models based on some underlying behavior state of the system, and previously each of these datasets has been modeled using Markov switching processes.",
                    "label": 0
                },
                {
                    "sent": "And more recently, basean nonparametric models have been applied in order to allow us to learn the number of behavior states.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "From the data, however, each of these models is focused on analyzing a single time series.",
                    "label": 1
                },
                {
                    "sent": "So one open question is, how do you jointly model multiple related time series?",
                    "label": 0
                },
                {
                    "sent": "So, for example, imagine you have a collection of videos of people performing a set of exercise routines.",
                    "label": 0
                },
                {
                    "sent": "How do you transfer knowledge between these datasets in order to improve things such as parameter estimates?",
                    "label": 0
                },
                {
                    "sent": "Also, it allows us to find interesting structure and how they relate to one another.",
                    "label": 0
                },
                {
                    "sent": "So an ideal model.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is one that allows each time series to have an unbounded number of possible behavior states while encouraging the use of a sparse subset.",
                    "label": 0
                },
                {
                    "sent": "Also, we would like our model to encourage sharing of dynamic behaviors between the different time series, but also allowing some variability.",
                    "label": 1
                },
                {
                    "sent": "So for example, maybe in one movie Sundays doing side twists, but that motion doesn't appear in any of the other movies.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to build up to such a model, it's useful to start by considering a method of describing a single time series with these types of pattern behaviors.",
                    "label": 0
                },
                {
                    "sent": "So a classical approach here is the hidden Markov model, and it assumes that there's an underlying discrete valued state sequence represented by the random variables, which is Markov with respect to some transition distributions Pi.",
                    "label": 1
                },
                {
                    "sent": "So for the motion capture data, this might be a sequence of labels of jumping jacks, side twists, and squats, and so on.",
                    "label": 0
                },
                {
                    "sent": "Then condition on the state sequence.",
                    "label": 0
                },
                {
                    "sent": "The observations are independent emissions from some class of distributions, which in this case we take to be Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So back to the mocap data.",
                    "label": 0
                },
                {
                    "sent": "This might be body position observations and one can view a sample path of the state sequence as a walk through the following state versus time lattice.",
                    "label": 0
                },
                {
                    "sent": "Imagine that at the first time step you start, in the second state, which here corresponds to maybe jumping jacks state and you have the following set of possible transitions and the relative weights of the air.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As shown here, indicate the probability of taking each of these transitions and is captured by that states transition distribution.",
                    "label": 0
                },
                {
                    "sent": "And then let's imagine at the next time step we persist in the same jumping jacks state.",
                    "label": 0
                },
                {
                    "sent": "Have the same set of possible transitions and then we transition to squat behavior state and have a different set of possible transitions and so on.",
                    "label": 0
                },
                {
                    "sent": "While returning to the case where you have multiple related time series, let's start by modeling each one of them via an HMM.",
                    "label": 1
                },
                {
                    "sent": "Then if we like to relate the time Series A common approach is just simply to assume that they are all generated from the same set of models.",
                    "label": 0
                },
                {
                    "sent": "And if we want to allow for infinitely many possible behaviors.",
                    "label": 1
                },
                {
                    "sent": "One attractive approach is to appeal to Bayesian Nonparametrics.",
                    "label": 0
                },
                {
                    "sent": "And one possibility is to build.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In a previous basean nonparametric hmm known as the hierarchical dearsley process, or HTP hmm?",
                    "label": 0
                },
                {
                    "sent": "And this model allows for infinitely many possible behaviors, encourages the use of sparse subset, so that's great.",
                    "label": 1
                },
                {
                    "sent": "However, by taking this approach, it assumes that each of the time series is switching between exactly the same set of states in exactly the same fashion, and that's clearly not as flexible of a model as we'd wanted.",
                    "label": 0
                },
                {
                    "sent": "So instead, we're going.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Show how an alternative Bayesian nonparametric method, specifically building on the beta process, allows us to achieve a model where each time series can have different behaviors.",
                    "label": 0
                },
                {
                    "sent": "That is switching between and they can switch between them in different ways.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we start by providing some background on the beta process and a realization from a beta process can be described as a collection of weights and atoms that are a draw from a nonhomogeneous Passan process.",
                    "label": 0
                },
                {
                    "sent": "Each of the weights is constrained to be between zero and one, so you can just think of it as defining a coin, flipping probability, and through this construction we define an infinite collection of these coin weights, and so we show a realization here and we can use such a realization to define a set of observations from a Bernoulli process.",
                    "label": 0
                },
                {
                    "sent": "In each of these observations can be described as a collection of unit mass atoms that are the result of coin flips at each location in our beta process measure.",
                    "label": 0
                },
                {
                    "sent": "So, for example, we walk along our beta process realization and flip a coin where the probability of getting a heads is determined by the weight of that Atom.",
                    "label": 0
                },
                {
                    "sent": "So here we got ahead.",
                    "label": 0
                },
                {
                    "sent": "Another heads a tail, and so on, and we keep flipping coins at each location.",
                    "label": 0
                },
                {
                    "sent": "And because the beta process random measure has finite total expected mass.",
                    "label": 1
                },
                {
                    "sent": "We're going to get a finite number of successes in this infinite coin flipping sequence, and this is later going to relate to the fact that each of our time series is switching between a finite number of dynamic behaviors.",
                    "label": 0
                },
                {
                    "sent": "And so we're going to think of each of these observations from this Bernoulli process as defining a feature vector.",
                    "label": 0
                },
                {
                    "sent": "An infinite dimensional feature vector, where the set of chosen features are exactly the locations in which we got a heads.",
                    "label": 0
                },
                {
                    "sent": "And here we show series of observations from the Bernoulli process and through the sharing the same base measure the same beta process realization.",
                    "label": 0
                },
                {
                    "sent": "We see that we've defined, in essence a collection of feature vectors that have a lot of sharing, but also some variability.",
                    "label": 0
                },
                {
                    "sent": "OK, so in the construction so far.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We've assumed that we have this beta process realization and independent draws from the Bernoulli process.",
                    "label": 0
                },
                {
                    "sent": "However, we can actually marginalise over the random measure and look at the predicted distribution on a new feature vector based on the feature vectors we've observed so far, and this process is commonly referred to as the Indian buffet process.",
                    "label": 1
                },
                {
                    "sent": "So imagine you have a restaurant with a buffet line with infinitely many dishes in each dish corresponds to a different feature.",
                    "label": 0
                },
                {
                    "sent": "Customers arrive at this restaurant and choose dishes from this buffet line.",
                    "label": 0
                },
                {
                    "sent": "The first customer simply.",
                    "label": 0
                },
                {
                    "sent": "Comes in and he chooses some Passan number of dishes.",
                    "label": 0
                },
                {
                    "sent": "The next customer comes in and is more likely to sample the dishes at the first customer show.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then chooses some prasan number of new dishes in this process continues with each customer sampling dishes in proportion to how often they were sampled before, and then choosing some new dishes.",
                    "label": 0
                },
                {
                    "sent": "And here we show.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Feature matrix associated with the realization from the Indian buffet process.",
                    "label": 1
                },
                {
                    "sent": "Each row corresponds to a different customer in each column to different dish and the white squares indicate the set of selected dishes.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we're going to describe how we use such a feature model in order to relate multiple time series.",
                    "label": 0
                },
                {
                    "sent": "So we're going to start by modeling each one of our end individual time series VN.",
                    "label": 0
                },
                {
                    "sent": "Hmm, then the feature matrix we had before, which described a series of customers eating a collection of dishes is now going to describe a collection of time series switching between a set of dynamic behaviors.",
                    "label": 0
                },
                {
                    "sent": "So specifically, we define a shared library of infinitely many possible behaviors, and then each object has a feature vector which is a row of this matrix.",
                    "label": 0
                },
                {
                    "sent": "So it's an infinite dimensional feature vector that defines which behaviors that object switches between and the way it does that is by constraining a set of transition distributions by driving some elements to 0.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that that object can only switch between the set of behaviors it is chosen.",
                    "label": 0
                },
                {
                    "sent": "So specifically, imagine that you define a collection of transition distributions for that object in some infinite dimensional space.",
                    "label": 0
                },
                {
                    "sent": "Then you do an elementwise product of that distribution with that objects feature vector renormalize.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that defines what we call our feature constrained transition distributions.",
                    "label": 0
                },
                {
                    "sent": "So here's the overall general process and what you see as we place a dearsley prior on the transition distributions where the feature vector operates on the parameters of that distribution.",
                    "label": 0
                },
                {
                    "sent": "Then the state evolves accord.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Into these feature constrained transition distributions and that state indexes into our shared collection of infinitely many possible behaviors to generate the observation.",
                    "label": 0
                },
                {
                    "sent": "In this Kappa, parameters simply increases the prior probability of a self transition.",
                    "label": 0
                },
                {
                    "sent": "OK, so we then tie together all of the feature vectors under a common beta process prior, just as in the model we.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Describe before, so specifically, each one of the feature vectors is a result of an infinite coin flipping sequence based on a shared data process measure.",
                    "label": 0
                },
                {
                    "sent": "And through this construction, we're encouraging sharing of dynamic behaviors between the different time series, but allowing for some variability, and we refer to this model as a BP HMM.",
                    "label": 0
                },
                {
                    "sent": "So for.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Motion capture data we showed earlier on, we're actually going to rely on a slightly more complicated model because if you use an HMM to model that data, what it says is that the body position measurements within a behavior state such as jumping jacks are going to be independent.",
                    "label": 0
                },
                {
                    "sent": "And that's clearly not true.",
                    "label": 0
                },
                {
                    "sent": "So to deal with that, we're going to add in these extra edges in order to add more correlation structure in the observations.",
                    "label": 0
                },
                {
                    "sent": "And this model is referred to as a switching VAR process, and in order are.",
                    "label": 1
                },
                {
                    "sent": "Our process.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Assumes that each observation vector is a linear combination of the previous R.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Observation vectors plus some additive Gaussian noise.",
                    "label": 0
                },
                {
                    "sent": "So in reality, we're going to be considering this slightly more complicated model that has this added correlation structure on the observations, and we refer to this model as the BP AR HMM.",
                    "label": 0
                },
                {
                    "sent": "OK, so for inference we're going to rely on an MCMC sampler.",
                    "label": 0
                },
                {
                    "sent": "And one thing we're going to exploit over and over again is the fact that if you condition on the feature matrix, our model decomposes into a collection of finite switching far processes, so without any truncation, we are back into the world of finite switching for processes.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what we're going to focus on in this talk is simply how do we re sample this feature matrix?",
                    "label": 0
                },
                {
                    "sent": "And when we do that, we're going to look at the posterior distribution on each of these features, which decomposes into.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Iron likelihood term.",
                    "label": 0
                },
                {
                    "sent": "The likelihood term you know.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This marginalizes over the latent state sequence, and even though there's an exponentially large number of possible state sequences for each of these time series, we can compute this very efficiently by harnessing the induced finite switching var process, and we simply run forward backward algorithm.",
                    "label": 0
                },
                {
                    "sent": "Marginalizing over the latent state sequence to compute the likelihood of the observations.",
                    "label": 0
                },
                {
                    "sent": "Then that likelihood is weighted by a prior term given by the ICP.",
                    "label": 0
                },
                {
                    "sent": "We're going to focus on this term little bit now.",
                    "label": 0
                },
                {
                    "sent": "So let's assume that you have a feature matrix.",
                    "label": 0
                },
                {
                    "sent": "Instantiate it and you simply want to re sample the features associated with the time series.",
                    "label": 0
                },
                {
                    "sent": "Well, the IVP treats shared and unique features separately, so we're going to look at these two cases separately as well.",
                    "label": 1
                },
                {
                    "sent": "For each one of the shared features, we're simply going to walk along and sample that feature with prior probability proportional to how many of the other time series also sampled that dynamic behavior, and that's then waited by the likelihood computed using the forward backward algorithm.",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Along re sample are shared features.",
                    "label": 0
                },
                {
                    "sent": "Then we get to the set of unique features.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the ICP dictates that we choose.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some prasan Alpha over N new features.",
                    "label": 0
                },
                {
                    "sent": "However.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Because our model is not conjugate, we cannot compute the posterior probability of this event in closed form.",
                    "label": 0
                },
                {
                    "sent": "In previous models that have looked at this non conjugate ICP case have either relied on finite truncations of the ICP or proposals from the prasan prior that result in very low acceptance rates in the high dimensional applications, we're going to consider so instead, we develop a birth death, reversible jump, and Sam C algorithm, and it.",
                    "label": 0
                },
                {
                    "sent": "What that does is it takes our unique set of features and proposes either the birth of a new unique feature or the death of one of the existing unique features.",
                    "label": 1
                },
                {
                    "sent": "OK, so to test our be par hmm, we examined a collection of videos of people performing a set of exercise routines.",
                    "label": 0
                },
                {
                    "sent": "And these videos were taken from the CMU MOCAP database and we get observations of 62 joint angle and body position measurements, and we're only going to choose the 12 that correspond to gross motor movement because for this application.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We don't care about things like what the fingers are doing.",
                    "label": 0
                },
                {
                    "sent": "And so the goal here is to discover common behaviors between each of these different movies.",
                    "label": 1
                },
                {
                    "sent": "And as you see, we're actually able to achieve this.",
                    "label": 0
                },
                {
                    "sent": "What I plot here is a series of skeleton plots where each skeleton plot corresponds to learned.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Take you a segment of at least 2 seconds of data.",
                    "label": 0
                },
                {
                    "sent": "Then I grouped together all of the segments that were generated from the same behavior state and the color of the box indicates the true behavior category.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what you can see from this is the fact that we've been able to identify and group together 6 examples of jumping jacks.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some side.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Swiss arm circles and squats.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And one nice thing about the model is the fact that we're able to identify a collection of behaviors that appeared in one movie.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But no other movie.",
                    "label": 0
                },
                {
                    "sent": "We did however, split a few motion categ.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And one of them is knee raises.",
                    "label": 0
                },
                {
                    "sent": "But if you look closely at the data, there's a reason for this.",
                    "label": 0
                },
                {
                    "sent": "In one case, the person has a lot of side to side upperbody movement where the other person is doing the more standard motion.",
                    "label": 0
                },
                {
                    "sent": "And similarly for the running motion.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And one person is running with their hands In Sync with their knees and the other person.",
                    "label": 0
                },
                {
                    "sent": "The more standard out of sync motion and this other person at the end is in the middle of doing a jumping Jack as it running.",
                    "label": 0
                },
                {
                    "sent": "They're just generally confused, and I guess that's what happens when you ask computer science students to do exercises.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Offense with you guys, but overall.",
                    "label": 0
                },
                {
                    "sent": "You see that we're clearly able to identify a set of behaviors that appear in multiple movies, but allow for some variability.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tween these.",
                    "label": 0
                },
                {
                    "sent": "So we also compared the performance of our algorithm to a couple others proposed in literature.",
                    "label": 0
                },
                {
                    "sent": "One is a Gaussian mixture model and the other is a hidden Markov model, both using first difference observations.",
                    "label": 0
                },
                {
                    "sent": "And then we also compared to model that uses the shared hierarchical dearsley process prior.",
                    "label": 0
                },
                {
                    "sent": "And here I show the true feature matrix and here is the feature matrix in Ferd by our proposed be par hmm and it's averaged over a collection of MCMC samples on the bottom I show the feature matrices for these alternative GMM and HMM algorithms and for these algorithms I set the number of states to 12, which is the truth.",
                    "label": 1
                },
                {
                    "sent": "Learn the models using EM, Run 10 initializations, choose the most likely.",
                    "label": 0
                },
                {
                    "sent": "So we're trying to be as nice as possible to these algorithms.",
                    "label": 0
                },
                {
                    "sent": "However, by pulling all of the data together.",
                    "label": 0
                },
                {
                    "sent": "These models assume a lot of sharing, which you can see in these strong bands of white.",
                    "label": 0
                },
                {
                    "sent": "And here's the matrix that's learned by the model with that shared HTP prior.",
                    "label": 0
                },
                {
                    "sent": "But what you see is that our proposed be par hmm is able to pick up on this unique feature structure without adding too many new states.",
                    "label": 0
                },
                {
                    "sent": "So in conclusion, we examined a method of relating multiple time series within invasion on pair.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Metric framework by utilizing the beta process.",
                    "label": 1
                },
                {
                    "sent": "We then developed an efficient MCMC inference scheme for the non conjugate ICP case that's useful for high dimensional applications.",
                    "label": 0
                },
                {
                    "sent": "And finally, we demonstrated some impressive performance on a challenging motion capture data set that showed performance exceeding that of alternative methods.",
                    "label": 1
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "OK, we have some time for questions.",
                    "label": 0
                },
                {
                    "sent": "Hi so in this case you choose a set of motions that have more lesser, clearer structure, right?",
                    "label": 0
                },
                {
                    "sent": "But if you think of the United case of human motion, you know this case is not going to be.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's not going to be like that, right?",
                    "label": 0
                },
                {
                    "sent": "So I have a couple of questions.",
                    "label": 0
                },
                {
                    "sent": "Do you think this will scale two more general continuous motions?",
                    "label": 0
                },
                {
                    "sent": "And how is the complexity of the method?",
                    "label": 0
                },
                {
                    "sent": "Could you use, you know?",
                    "label": 0
                },
                {
                    "sent": "Large collection of motions.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "To answer all these questions.",
                    "label": 0
                },
                {
                    "sent": "I think that this set of motions that you actually see in these exercise routines encapsulates the kind of motions you see people doing when they're walking or moving, and I think because it's a beige and nonparametric approach, you might not get labels that are jumping jacks side twists.",
                    "label": 0
                },
                {
                    "sent": "You know these type of descriptions, but you will be able to cluster on common commonality's between the behaviors, even though it might not be something that we as humans could say.",
                    "label": 0
                },
                {
                    "sent": "This is what we call it and in terms of.",
                    "label": 0
                },
                {
                    "sent": "Dealing with large numbers of behaviors.",
                    "label": 0
                },
                {
                    "sent": "Once again, that's a nice thing with patient nonparametric approaches.",
                    "label": 0
                },
                {
                    "sent": "And as you get more data, it helps you infer these different behaviors.",
                    "label": 0
                },
                {
                    "sent": "But if there is something that's significantly different in some qualitative sense, we're not fixing the number of behaviors, and we're not using any training training data.",
                    "label": 0
                },
                {
                    "sent": "This is fully unsupervised.",
                    "label": 0
                },
                {
                    "sent": "In that sense.",
                    "label": 0
                },
                {
                    "sent": "You know there's some hope in being able to capture that.",
                    "label": 0
                },
                {
                    "sent": "Is the segmentation into states related to the Kappa parameter?",
                    "label": 0
                },
                {
                    "sent": "Is it sensitive to that Kappa parameter which is so the cap was a self transition parameter?",
                    "label": 0
                },
                {
                    "sent": "Definitely if you don't have that parameter it does over segment the data and so that's it's related to papers that we've had before showing how.",
                    "label": 0
                },
                {
                    "sent": "If you have self transition bias built into your prior, it helps with real world data in terms of getting the type of segmentations you expect to see where motions persist over periods of time, so it's moving towards this semi Markov type type of model.",
                    "label": 0
                },
                {
                    "sent": "And then did you have to put some sort of prior on that capital parameter so all related to the lengths of the motions?",
                    "label": 0
                },
                {
                    "sent": "No, we actually put a rather uninformative prior, so we put priors on that sticky Kappa parameter aprior on.",
                    "label": 0
                },
                {
                    "sent": "There was another parameter that determine things about the transition distribution, which is.",
                    "label": 0
                },
                {
                    "sent": "And then there's the ICP parameter, so there are three different hyperparameters, and we put priors on those and they.",
                    "label": 0
                },
                {
                    "sent": "Our results were rather insensitive to the settings of those priors, is more sensitive to how you initialize the data.",
                    "label": 0
                },
                {
                    "sent": "If you stumble from your model, can you tell us what it looks like and you compare it to IBM's so I actually got that question yesterday, and since then I haven't been able to do that.",
                    "label": 0
                },
                {
                    "sent": "I haven't looked at that yet for this model.",
                    "label": 0
                },
                {
                    "sent": "For other models I've worked on, I have looked at sampling from it, but.",
                    "label": 0
                },
                {
                    "sent": "My guess is it's not going to look quite like human motion if you just sample from the from the prior of this model.",
                    "label": 0
                },
                {
                    "sent": "But I think there are interesting future directions that could constrain it more and get even better formulation directly applied to human motion, if that's what you're interested in and what was.",
                    "label": 0
                },
                {
                    "sent": "Sorry, Kevin, the last part was.",
                    "label": 0
                },
                {
                    "sent": "Oh no, I I yeah it's an interesting direction.",
                    "label": 0
                },
                {
                    "sent": "I haven't looked at that.",
                    "label": 0
                },
                {
                    "sent": "OK, let's thank the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}