{
    "id": "xjxu4bxyghwf2q2qoljioasjxqpm4psy",
    "title": "Feature Selection for Value Function Approximation Using Bayesian Model Selection",
    "info": {
        "author": [
            "Tobias Jung, Department of Computer Science, University of Texas at Austin"
        ],
        "published": "Oct. 20, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Feature Selection"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd09_jung_fsvfaubms/",
    "segmentation": [
        [
            "Now let's talk is about.",
            "If you turn selection in reinforcement learning, that's something that's very rarely done, so I hope you will learn some new ideas of how this could be done.",
            "So the basic problem."
        ],
        [
            "That we're trying to solve is an optimization problem, overtime so."
        ],
        [
            "I have a dynamical system.",
            "We have states we have actions every time we choose a state.",
            "Every time you choose an action, the state of the system changes and this change is described by a probability distribution.",
            "So we have stochastic transitions and every time we make such a transition we obtain a scalar award and this reward depends on the state we're in and the state we're going into and the action we are choosing probably, and the overall goal of this kind of problem is.",
            "To find actions or determine actions such that the sum of the rewards is optimized."
        ],
        [
            "So to do this, we have a policy of policies in map from states to actions, and for any given policy we consider the value function which is a function is defined for all the states in the state space and that assigns to every state in the state space the expected long-term reward.",
            "And the Belmont equation says that the expected long-term reward, which is just the infinite sum over all the rewards that we're going to get the expectation of the infinite sum of other words that we're going to get if we follow this policy, can be written in this way.",
            "So it's just the immediate reward, and the value of the successor state that we achieve when when following the policy.",
            "OK, so."
        ],
        [
            "There's some core component, and to find the policy an optimal policy will.",
            "Yeah, we just we're just looking to find an optimal policy, so that's our goal.",
            "And of course there are lots of applications that can be formulated within this framework, but that's not what I'm going to talk today.",
            "What I'm going to do."
        ],
        [
            "Day is about how to find such a policy and one framework to find such a policy is the policy iteration framework, which works like this.",
            "We guess an initial policy.",
            "We compute the value function that is induced by those policy, which is called the policy early action step.",
            "And once we have obtained those value function, we compute and improve policy from this value function.",
            "Then we iterate these steps and eventually will get an optimal policy.",
            "The problem is this only works in theory.",
            "So in practice we have lots of problems and they cannot."
        ],
        [
            "About them today.",
            "So I only talk about one particular problem and one particular problem is the policy evaluation problem, which is the step where we compute the value function for a given fixed policy.",
            "So why is this a problem?",
            "The problem is, well, usually we have a large state space."
        ],
        [
            "We have continuous state space.",
            "We cannot just discretize, we cannot use grids.",
            "We have high dimensional observations.",
            "So what we need is a kind of function approximation and one good choice for function approximation is a linear approach, so we're usually using a linear approach.",
            "And the second problem is that the dynamics of the system, the transition probabilities and rewards are unknown.",
            "Instead, what we have our sample transitions.",
            "So we have sample transitions from following a policy.",
            "So we have a sequence of States and rewards that we get while following the policy.",
            "And this is our data and just from this data we have to estimate the value function.",
            "OK, the good news is that this actually works usually.",
            "So if we have samples and good basis functions, then this problem of approximate policy vision is well understood.",
            "We have lots of algorithms that all converge.",
            "Basically the same solution that just minor differences.",
            "So TD and LCD in LSP and residual whatever."
        ],
        [
            "The bad news is or what makes the problem interesting is the question what are be good feature?"
        ],
        [
            "So the question is, how can we find them if we are just given the data and now in reinforcement learning?",
            "The city."
        ],
        [
            "Creation is, well, actually no one really or hardly anyone cares about how these basis functions are shows.",
            "Most of them just choose them manually.",
            "Some radial basis functions.",
            "Then you will find a paper when they wrote are.",
            "Sometimes it works, sometimes it doesn't it.",
            "I don't know and.",
            "Everything is crap and the point is that of course I mean if we choose the basis functions poorly then we will get a very bad approximation.",
            "What we actually want is if you can.",
            "If we have the true value function in the upper left corner, then we want approximation that looks like this probably and.",
            "The goal of this paper is to propose a framework using model selection to get an approximation like this automatically just using the samples that we have.",
            "OK, so."
        ],
        [
            "In order to do this, we will employ Gaussian processes cause the processes are nonparametric approach.",
            "So essentially we don't have to worry about basis functions.",
            "And also one advantage of Gaussian processes that they offer as a principled framework to address the problem of model selection.",
            "And so the novelty and the contribution of this work is."
        ],
        [
            "A framework for automatic model selection, thus solving the approximate policy evaluation problem completely automatically, and we also have a framework for feature selection.",
            "Because we can use the same framework to find and eliminate irrelevant state variables and doing this allows us to improve generalization and.",
            "Also important for reinforcement learning, doing this also allows us to reduce the runtime complexity because we can get sparse solutions and we also have some very exciting grid worlds to demonstrate that it all works.",
            "OK, so let's start with some background information using Gaussian processes in reinforcement learning for temporal difference.",
            "Learning is nothing new IT was proposed by Yakov Angle some back sometime in 2003 or 2005 and for some reason it didn't really catch on.",
            "So I hope to give it a little boost here so."
        ],
        [
            "Why would we use excuse me?",
            "Sorry OK, so why would we use Gaussian processes for approximate policy violation?",
            "Well, the nice thing about Gaussian processes that instead of working with individual basis functions we have whole class of functions and these functions share certain smoothness properties and smoothness means here in this context how much are the?",
            "The function values allowed to vary in output space.",
            "In relation to the distance of the arguments in input space.",
            "So that's basically what smoothness means, and it goes in processes.",
            "The class of functions become the distribution of functions of cells in Pryor and smooth is just the covariance.",
            "And to give you.",
            "Anna Stration of why this is so."
        ],
        [
            "Nice, let's look at how it works in practice.",
            "So in practice we have.",
            "Just to specify the covariance, an one example of a covariance is given here, which is this squared exponential.",
            "It's just a Gaussian without normalization and.",
            "The whole thing depends on age, which is the length scale and if we vary the length scale.",
            "For example here we have length scale one and here we have length scale 1000 we get different types of functions, different smooth functions.",
            "So here we have left side.",
            "We have very smooth functions drawn from discussing process.",
            "Prior with this covariance and on the right hand side we have more wiggly more complex functions that we can achieve.",
            "All this effect just by varying H. So that's very convenient.",
            "OK, so Gaussian process have lots of practical advantages."
        ],
        [
            "They are easy to use, very elegant.",
            "Just because we only have to specify those one hyperparameter just one scalar.",
            "They are also a linear method, so they are very efficient and robust.",
            "We have closed form solution for the approximate policy evaluation problem.",
            "We have.",
            "Well, all it involves is just some simple linear algebra.",
            "We can implement it nicely and also there is there is convergence for approximate policy evaluation using a linear function approximator.",
            "So all of these very nice properties are retained.",
            "Also we have framework for model selection and in practice Gaussian processes just work well.",
            "So no reason not to consider them.",
            "OK, so now we want to use Gaussian processes to solve the policy evaluation."
        ],
        [
            "Problem OK so we have our data.",
            "In this case is our observed transitions under the policy \u03c0.",
            "So we have a sequence of states.",
            "And States and we have.",
            "N -- 1 associated rewards.",
            "And.",
            "No one problem with temporal difference.",
            "Learning with policy violation is that unlike an ordinary regression, we actually cannot observe samples from the function that we're trying to estimate because we have this recursive relation where value of 1 state is the value of successive state plus reward and expectation around."
        ],
        [
            "And so we need a different model to to explain the data.",
            "So we cannot just use the same model that's used for ordinary Gaussian process regression and."
        ],
        [
            "We're using here is exactly the thing that was proposed by angle for stochastic transitions, so.",
            "It's basically linearly transformed Gaussian process with the different choice of a noise model and I cannot explain it fully right now, but it's just well you have your rewards that are distributed within normal distribution with the covariance matrix is given well, it's just the same as in usual Gaussian process is just a little bit transformed.",
            "Different choice of a noise model and under this model to predict a new function value cannot really point, so it's bad.",
            "I think the new function value we have again.",
            "A normal distribution over the new function value over the over the state that we want that whose value we want to predict, and the meaning of this distribution is just this expression here.",
            "So we have again our feature vector in this world can be thought of as our weights and we have the variance.",
            "Usually we don't need the variance, we are only interested in the meat because that's the value that we are interested in, and that's used to derive a new policy from.",
            "OK, so essentially to make all of this work, we only have to compute.",
            "This vector here and well this this inverse matrix here and.",
            "This this Q is the thing that depends on the hyperparameters data, so it's all a very, very simple and very elegant approach.",
            "You don't have to worry about anything if you have data.",
            "If you have hyperparameters, you're done.",
            "OK."
        ],
        [
            "Well, yeah, there's some minor problem.",
            "Some minor computational problems with the trading complexity is in Cuba, and that's that's rather expensive, so we need some kind of little approximations.",
            "We use the subset of regressors approach, something that's well known in the context of ordinary Gaussian presidency.",
            "We just dumped it for the case solved."
        ],
        [
            "And the basic idea is to approximate the kernel from from much smaller subset that the whole thing can be motivated from anystream approximation.",
            "So we have this this subset of training data of cardinality M. So if you have like 10,000 training data and you can choose M like 100, you have a vast reduction of computational complexity because we only have to solve a reduce problem, which is has a trading complexity of O&M square.",
            "And if we fix the M which is the size of the subset.",
            "Then the whole thing scales linearly in the number of training examples, which is pretty good.",
            "OK, so the second question is.",
            "How?"
        ],
        [
            "How do we get our subset and general?",
            "There are many different approaches proposed and different papers, and so their supervised or unsupervised or random or whatever.",
            "So here we use an unsupervised approach.",
            "We use the incompleteness getting composition of kernel matrix and this corresponds to an iterative partial agreement 'cause it didn't feature space.",
            "But it's not a point so it's just a selection procedure and.",
            "The essence of it is that the number of elements that we select using this procedure will depend on the effective rank of the kernel matrix and internally affective rank.",
            "So Agnelli or something, metrics depend on the complexity of the solution, and this can be seen later on.",
            "The bottom line is that if we have a simpler solution, then we will have better generalization and.",
            "Be cause lower complexity means lower M at runtime.",
            "Which is important for reinforcement learning because we don't sell just one regression problem we have to solve it over and over again during the policy iteration cycle.",
            "OK, so.",
            "Now we know how."
        ],
        [
            "Compute value function if we know that parameters Now we have to worry about by the parameters and the parameters forgotten process.",
            "Temporal difference learning can be found the same way as they can be found for gossip process.",
            "So we can."
        ],
        [
            "The likelihood of the data which is a gas distribution.",
            "We consider that as a function of the hyperparameters take the logarithm and get the log likelihood, which is just the equation given in yellow gold whatever color.",
            "And this is a function that depends on the hyperparameters data and we want to minimize this likelihood with respect to Theta.",
            "That's a nonlinear problem, so we need some gradient based solver is not really a problem and also the gradient of this expression can be obtained in closed form so.",
            "Nothing to worry about too much, and if we look at this likelihood function we see that it consists of two conflicting terms.",
            "So the first term here, this determinant of Q.",
            "This is the complexity term and if you remember the determinant product of eigenvalues that explains how everything is connected and the second term here.",
            "This is.",
            "It's not immediately obvious, but it's actually the data fit.",
            "It measures how well your approximation fits the data.",
            "It's the value of the objective function of a.",
            "Related to this quest problem.",
            "And generally with function approximation we have the usual dilemma.",
            "So we either have a solution that has large bandwidth, which means we have a high complexity weekly solution which will have a large Rankin K and which may produce a low data error an we have functions that have small bandwidth and low complexity and probably high data and we want to combine both of that.",
            "That's all audio function is to me.",
            "OK, so one advantage of."
        ],
        [
            "Adjusting and determining hyperparameters by such an automated procedure is that we can use a covariance function that depends on a larger number of parameters, so we just don't have to use one and set that by hand.",
            "We can use many more and set them automatically.",
            "An doing this allows us to better fit the data an actually.",
            "Yeah, find out what is relevant and what's irrelevant.",
            "So here we consider.",
            "Three variants of this squared exponential kernel.",
            "One which has the very number one which is just the identity matrix which has a uniform length scale along all dimensions and dimensions.",
            "Here means state variables.",
            "So which is uniform along Allstate variables.",
            "Variant #2 has one parameter for each state variable that are in dependently adjusted and #3 is the same #2 but also allows for rotations and the point is now if we look at these two variances.",
            "If we set by para meters automatically from the data and model selection works as everybody hopes it works, then one selection will automatically drive those values to zero that are not important.",
            "So for example, if one of the coordinates of the state variable is not as important as another one, then it will go to zero and we can actually find out that we can remove it from the whole problem.",
            "Which is nice.",
            "OK."
        ],
        [
            "So let's see some experiments.",
            "First one is."
        ],
        [
            "One of our beloved gridworld.",
            "So we have 11 * 11 cells.",
            "In the middle of the center column is our goal state and every other step outside the goals that give us a reward of minus one.",
            "We have stochastic transitions.",
            "And as you can see.",
            "We have a state that consists of the X coordinate and the Y coordinate, but the Y coordinate is completely irrelevant for predicting the value function.",
            "So this is the true value function for this problem.",
            "So we have 0 value at the gold state and then minus 1 -- 2 and so on.",
            "So the other states.",
            "And now we draw 500 sample transitions from this policy.",
            "And run our algorithm using the uniform kernel and the one that has one length scale for every dimension.",
            "So number 1 #2.",
            "And as we can see the one having a independent parameter for every dimension is of course much better than the one that doesn't.",
            "So this is far more quickly than this solution here, so this is the better solution and now look we now we look in some detail at the."
        ],
        [
            "Also.",
            "Let's see what model selection actually produces.",
            "So just look at the first 2 rows.",
            "So we see likelihood score, which is the thing that we're trying to minimize.",
            "So lower scores are better.",
            "We see that model number 2, which is the one that has independent parameter.",
            "Every dimension is has lower negative likelihood.",
            "Also a higher likelihood, and such is a better model.",
            "And if we see at the two terms that make up this likely what we see that.",
            "The complexity of the solution #2 is lower and the data fit is better, so it's a better model.",
            "And the complexity can also be seen in this frontier, which shows you the eigenvalues.",
            "So the upper line is the eigenvalues of model number one.",
            "This is model number 2 and now comes the interesting point.",
            "If we look at the hyperparameter so produced from model number two, we see that the state variable that corresponds to the Y coordinate has a very low value and we can ask ourselves could we remove it from the model?",
            "And this is exactly what we are examining in the third case.",
            "So we now we set.",
            "The.",
            "The lengthscale corresponding to the white is zero.",
            "Compute the likelihood of this model.",
            "The likelihood goes further down and now we see why it goes further down.",
            "So the complexity is further reduced.",
            "But the data fit stays the same, so we can safely remove the irrelevant state variable and the only thing that we need to decide that is the likelihood and complexity of the data FIT scores, which we can compute.",
            "Another experiment, now we had some more."
        ],
        [
            "Relevant state variables just to see if it still works.",
            "And so we have a 6 dimensional observation now.",
            "And here we see how this optimization over the parameters proceeds.",
            "So initially we start with every dimension having the same relevance, having the same length scale and as the optimization proceeds only the state variable that corresponds to the X coordinate, which is the one that only measures survives and every other state variable goes to 0, which is exactly what we would.",
            "Like to see so.",
            "This confirms that everything is working as hoped now."
        ],
        [
            "OK, so another experiment is a more realistic benchmark, not artificially created.",
            "This is a pendulum balancing inverted pendulum and here we are trying to approximate the optimal value function which is shown on the right.",
            "An yeah, we see all the three.",
            "Results for the three covariance, so this is number one.",
            "Those #2 and #3.",
            "And as we can see.",
            "Lot #1 and two are not not really that much different, but that's not really surprising because if we look at the value function shown at the top, we see that there is really no one single dimension that we could eliminate.",
            "Cause because the value of a state box depends on the X and the Y coordinate.",
            "On the other hand, if we look more closely at this function, we see that there that there is a sort of, well, dominant direction.",
            "So if we look at this diagonal going from the left to the right, we see that the variance is much higher than the orthogonal diagonal to this.",
            "So there is.",
            "So this direction is more.",
            "Important then this direction, and if we could identify this direction then we would be very happy and this is exactly what the third model achieves to do and as a consequence achieves a much lower prediction error as the other two models.",
            "OK, so another consequence of this approach of this more tighter model is that.",
            "The computational complexity is reduced something."
        ],
        [
            "As a consequence of having a lower effective rank.",
            "So here we see the number of elements that the Incompleteness Commission selects for the subset of regressors approach.",
            "Given a fixed level of tolerance.",
            "So if we for example, let's say we want to approximate within .1, then one number one chooses about.",
            "Like say 150 model number 2 chooses like 130 and model number 3 chooses like 90 so there is vast computational savings can be gained by."
        ],
        [
            "By using a better model, because this number M entrance with the number of square into the computational complexity.",
            "OK, so to finish we have seen that automatic feature selection in reinforcement learning is possible.",
            "We have proposed we have proposed an approach that is based on Gaussian processes as the underlying function approximator and.",
            "As a policy evaluation method we're using, we're using accounting process temporal difference turning, which corresponds to LLC one and want to call a rollout.",
            "We have proposed a likelihood based model selection method and yeah, the bad news is that the whole thing doesn't actually come with any guarantees because there are some violation of some very minor independence assumptions.",
            "But it seems to work in practice and that is all we care about.",
            "Well, it's all that I care about and.",
            "Yeah, well, there's there's some ongoing work of course, because right now we're only solving the policy allocation problem.",
            "What we really want to do is to solve the policy iteration problem.",
            "Service requires policy improvement, exploration or strategy for sample generation.",
            "Also, perhaps an extension to the state action Space is an.",
            "Of course, we don't want to upgrade roads all our lives, so some more complex experiments and simulations and more theoretical insights when the whole thing will fail.",
            "And compare it with all the other methods that there are and.",
            "Basically that's it.",
            "OK, so here's some related work.",
            "The original work where Gaussian process returning was proposed.",
            "That's like."
        ],
        [
            "Five years, six years ago.",
            "And there's some other work with adaptation of basis functions in reinforcement learning, but they don't consider the spacing approach and they don't have this tradeoff between complexity and data fit, so they are likely to overfit the solution and some we are well related approaches and that they are not not the same whether not really tuning the hyperparameters.",
            "OK, that's it.",
            "Thank you.",
            "What do you expect?",
            "What numbers are features?",
            "Yes, yes.",
            "You mean the dimensionality of the state space?",
            "Yeah, well I hope that it's still going to work.",
            "That's the whole reason that I propose that.",
            "Yes.",
            "Yes, yes yes we need.",
            "We probably need more transitions, more data to say that more certainty.",
            "Anymore questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's talk is about.",
                    "label": 0
                },
                {
                    "sent": "If you turn selection in reinforcement learning, that's something that's very rarely done, so I hope you will learn some new ideas of how this could be done.",
                    "label": 0
                },
                {
                    "sent": "So the basic problem.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That we're trying to solve is an optimization problem, overtime so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I have a dynamical system.",
                    "label": 0
                },
                {
                    "sent": "We have states we have actions every time we choose a state.",
                    "label": 0
                },
                {
                    "sent": "Every time you choose an action, the state of the system changes and this change is described by a probability distribution.",
                    "label": 0
                },
                {
                    "sent": "So we have stochastic transitions and every time we make such a transition we obtain a scalar award and this reward depends on the state we're in and the state we're going into and the action we are choosing probably, and the overall goal of this kind of problem is.",
                    "label": 0
                },
                {
                    "sent": "To find actions or determine actions such that the sum of the rewards is optimized.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to do this, we have a policy of policies in map from states to actions, and for any given policy we consider the value function which is a function is defined for all the states in the state space and that assigns to every state in the state space the expected long-term reward.",
                    "label": 0
                },
                {
                    "sent": "And the Belmont equation says that the expected long-term reward, which is just the infinite sum over all the rewards that we're going to get the expectation of the infinite sum of other words that we're going to get if we follow this policy, can be written in this way.",
                    "label": 0
                },
                {
                    "sent": "So it's just the immediate reward, and the value of the successor state that we achieve when when following the policy.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's some core component, and to find the policy an optimal policy will.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we just we're just looking to find an optimal policy, so that's our goal.",
                    "label": 0
                },
                {
                    "sent": "And of course there are lots of applications that can be formulated within this framework, but that's not what I'm going to talk today.",
                    "label": 0
                },
                {
                    "sent": "What I'm going to do.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Day is about how to find such a policy and one framework to find such a policy is the policy iteration framework, which works like this.",
                    "label": 0
                },
                {
                    "sent": "We guess an initial policy.",
                    "label": 0
                },
                {
                    "sent": "We compute the value function that is induced by those policy, which is called the policy early action step.",
                    "label": 0
                },
                {
                    "sent": "And once we have obtained those value function, we compute and improve policy from this value function.",
                    "label": 0
                },
                {
                    "sent": "Then we iterate these steps and eventually will get an optimal policy.",
                    "label": 0
                },
                {
                    "sent": "The problem is this only works in theory.",
                    "label": 0
                },
                {
                    "sent": "So in practice we have lots of problems and they cannot.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About them today.",
                    "label": 0
                },
                {
                    "sent": "So I only talk about one particular problem and one particular problem is the policy evaluation problem, which is the step where we compute the value function for a given fixed policy.",
                    "label": 0
                },
                {
                    "sent": "So why is this a problem?",
                    "label": 0
                },
                {
                    "sent": "The problem is, well, usually we have a large state space.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have continuous state space.",
                    "label": 0
                },
                {
                    "sent": "We cannot just discretize, we cannot use grids.",
                    "label": 0
                },
                {
                    "sent": "We have high dimensional observations.",
                    "label": 0
                },
                {
                    "sent": "So what we need is a kind of function approximation and one good choice for function approximation is a linear approach, so we're usually using a linear approach.",
                    "label": 0
                },
                {
                    "sent": "And the second problem is that the dynamics of the system, the transition probabilities and rewards are unknown.",
                    "label": 0
                },
                {
                    "sent": "Instead, what we have our sample transitions.",
                    "label": 0
                },
                {
                    "sent": "So we have sample transitions from following a policy.",
                    "label": 0
                },
                {
                    "sent": "So we have a sequence of States and rewards that we get while following the policy.",
                    "label": 0
                },
                {
                    "sent": "And this is our data and just from this data we have to estimate the value function.",
                    "label": 0
                },
                {
                    "sent": "OK, the good news is that this actually works usually.",
                    "label": 0
                },
                {
                    "sent": "So if we have samples and good basis functions, then this problem of approximate policy vision is well understood.",
                    "label": 0
                },
                {
                    "sent": "We have lots of algorithms that all converge.",
                    "label": 0
                },
                {
                    "sent": "Basically the same solution that just minor differences.",
                    "label": 0
                },
                {
                    "sent": "So TD and LCD in LSP and residual whatever.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The bad news is or what makes the problem interesting is the question what are be good feature?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the question is, how can we find them if we are just given the data and now in reinforcement learning?",
                    "label": 0
                },
                {
                    "sent": "The city.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Creation is, well, actually no one really or hardly anyone cares about how these basis functions are shows.",
                    "label": 0
                },
                {
                    "sent": "Most of them just choose them manually.",
                    "label": 0
                },
                {
                    "sent": "Some radial basis functions.",
                    "label": 0
                },
                {
                    "sent": "Then you will find a paper when they wrote are.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it works, sometimes it doesn't it.",
                    "label": 0
                },
                {
                    "sent": "I don't know and.",
                    "label": 0
                },
                {
                    "sent": "Everything is crap and the point is that of course I mean if we choose the basis functions poorly then we will get a very bad approximation.",
                    "label": 0
                },
                {
                    "sent": "What we actually want is if you can.",
                    "label": 0
                },
                {
                    "sent": "If we have the true value function in the upper left corner, then we want approximation that looks like this probably and.",
                    "label": 0
                },
                {
                    "sent": "The goal of this paper is to propose a framework using model selection to get an approximation like this automatically just using the samples that we have.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In order to do this, we will employ Gaussian processes cause the processes are nonparametric approach.",
                    "label": 0
                },
                {
                    "sent": "So essentially we don't have to worry about basis functions.",
                    "label": 0
                },
                {
                    "sent": "And also one advantage of Gaussian processes that they offer as a principled framework to address the problem of model selection.",
                    "label": 0
                },
                {
                    "sent": "And so the novelty and the contribution of this work is.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A framework for automatic model selection, thus solving the approximate policy evaluation problem completely automatically, and we also have a framework for feature selection.",
                    "label": 1
                },
                {
                    "sent": "Because we can use the same framework to find and eliminate irrelevant state variables and doing this allows us to improve generalization and.",
                    "label": 0
                },
                {
                    "sent": "Also important for reinforcement learning, doing this also allows us to reduce the runtime complexity because we can get sparse solutions and we also have some very exciting grid worlds to demonstrate that it all works.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's start with some background information using Gaussian processes in reinforcement learning for temporal difference.",
                    "label": 0
                },
                {
                    "sent": "Learning is nothing new IT was proposed by Yakov Angle some back sometime in 2003 or 2005 and for some reason it didn't really catch on.",
                    "label": 0
                },
                {
                    "sent": "So I hope to give it a little boost here so.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Why would we use excuse me?",
                    "label": 0
                },
                {
                    "sent": "Sorry OK, so why would we use Gaussian processes for approximate policy violation?",
                    "label": 0
                },
                {
                    "sent": "Well, the nice thing about Gaussian processes that instead of working with individual basis functions we have whole class of functions and these functions share certain smoothness properties and smoothness means here in this context how much are the?",
                    "label": 0
                },
                {
                    "sent": "The function values allowed to vary in output space.",
                    "label": 0
                },
                {
                    "sent": "In relation to the distance of the arguments in input space.",
                    "label": 0
                },
                {
                    "sent": "So that's basically what smoothness means, and it goes in processes.",
                    "label": 0
                },
                {
                    "sent": "The class of functions become the distribution of functions of cells in Pryor and smooth is just the covariance.",
                    "label": 0
                },
                {
                    "sent": "And to give you.",
                    "label": 0
                },
                {
                    "sent": "Anna Stration of why this is so.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nice, let's look at how it works in practice.",
                    "label": 0
                },
                {
                    "sent": "So in practice we have.",
                    "label": 0
                },
                {
                    "sent": "Just to specify the covariance, an one example of a covariance is given here, which is this squared exponential.",
                    "label": 0
                },
                {
                    "sent": "It's just a Gaussian without normalization and.",
                    "label": 0
                },
                {
                    "sent": "The whole thing depends on age, which is the length scale and if we vary the length scale.",
                    "label": 0
                },
                {
                    "sent": "For example here we have length scale one and here we have length scale 1000 we get different types of functions, different smooth functions.",
                    "label": 0
                },
                {
                    "sent": "So here we have left side.",
                    "label": 0
                },
                {
                    "sent": "We have very smooth functions drawn from discussing process.",
                    "label": 0
                },
                {
                    "sent": "Prior with this covariance and on the right hand side we have more wiggly more complex functions that we can achieve.",
                    "label": 0
                },
                {
                    "sent": "All this effect just by varying H. So that's very convenient.",
                    "label": 0
                },
                {
                    "sent": "OK, so Gaussian process have lots of practical advantages.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "They are easy to use, very elegant.",
                    "label": 1
                },
                {
                    "sent": "Just because we only have to specify those one hyperparameter just one scalar.",
                    "label": 0
                },
                {
                    "sent": "They are also a linear method, so they are very efficient and robust.",
                    "label": 0
                },
                {
                    "sent": "We have closed form solution for the approximate policy evaluation problem.",
                    "label": 0
                },
                {
                    "sent": "We have.",
                    "label": 0
                },
                {
                    "sent": "Well, all it involves is just some simple linear algebra.",
                    "label": 0
                },
                {
                    "sent": "We can implement it nicely and also there is there is convergence for approximate policy evaluation using a linear function approximator.",
                    "label": 0
                },
                {
                    "sent": "So all of these very nice properties are retained.",
                    "label": 0
                },
                {
                    "sent": "Also we have framework for model selection and in practice Gaussian processes just work well.",
                    "label": 0
                },
                {
                    "sent": "So no reason not to consider them.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we want to use Gaussian processes to solve the policy evaluation.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem OK so we have our data.",
                    "label": 0
                },
                {
                    "sent": "In this case is our observed transitions under the policy \u03c0.",
                    "label": 0
                },
                {
                    "sent": "So we have a sequence of states.",
                    "label": 0
                },
                {
                    "sent": "And States and we have.",
                    "label": 0
                },
                {
                    "sent": "N -- 1 associated rewards.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "No one problem with temporal difference.",
                    "label": 0
                },
                {
                    "sent": "Learning with policy violation is that unlike an ordinary regression, we actually cannot observe samples from the function that we're trying to estimate because we have this recursive relation where value of 1 state is the value of successive state plus reward and expectation around.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so we need a different model to to explain the data.",
                    "label": 0
                },
                {
                    "sent": "So we cannot just use the same model that's used for ordinary Gaussian process regression and.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're using here is exactly the thing that was proposed by angle for stochastic transitions, so.",
                    "label": 0
                },
                {
                    "sent": "It's basically linearly transformed Gaussian process with the different choice of a noise model and I cannot explain it fully right now, but it's just well you have your rewards that are distributed within normal distribution with the covariance matrix is given well, it's just the same as in usual Gaussian process is just a little bit transformed.",
                    "label": 0
                },
                {
                    "sent": "Different choice of a noise model and under this model to predict a new function value cannot really point, so it's bad.",
                    "label": 0
                },
                {
                    "sent": "I think the new function value we have again.",
                    "label": 0
                },
                {
                    "sent": "A normal distribution over the new function value over the over the state that we want that whose value we want to predict, and the meaning of this distribution is just this expression here.",
                    "label": 0
                },
                {
                    "sent": "So we have again our feature vector in this world can be thought of as our weights and we have the variance.",
                    "label": 0
                },
                {
                    "sent": "Usually we don't need the variance, we are only interested in the meat because that's the value that we are interested in, and that's used to derive a new policy from.",
                    "label": 0
                },
                {
                    "sent": "OK, so essentially to make all of this work, we only have to compute.",
                    "label": 0
                },
                {
                    "sent": "This vector here and well this this inverse matrix here and.",
                    "label": 0
                },
                {
                    "sent": "This this Q is the thing that depends on the hyperparameters data, so it's all a very, very simple and very elegant approach.",
                    "label": 0
                },
                {
                    "sent": "You don't have to worry about anything if you have data.",
                    "label": 0
                },
                {
                    "sent": "If you have hyperparameters, you're done.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, yeah, there's some minor problem.",
                    "label": 0
                },
                {
                    "sent": "Some minor computational problems with the trading complexity is in Cuba, and that's that's rather expensive, so we need some kind of little approximations.",
                    "label": 0
                },
                {
                    "sent": "We use the subset of regressors approach, something that's well known in the context of ordinary Gaussian presidency.",
                    "label": 0
                },
                {
                    "sent": "We just dumped it for the case solved.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the basic idea is to approximate the kernel from from much smaller subset that the whole thing can be motivated from anystream approximation.",
                    "label": 0
                },
                {
                    "sent": "So we have this this subset of training data of cardinality M. So if you have like 10,000 training data and you can choose M like 100, you have a vast reduction of computational complexity because we only have to solve a reduce problem, which is has a trading complexity of O&M square.",
                    "label": 0
                },
                {
                    "sent": "And if we fix the M which is the size of the subset.",
                    "label": 0
                },
                {
                    "sent": "Then the whole thing scales linearly in the number of training examples, which is pretty good.",
                    "label": 0
                },
                {
                    "sent": "OK, so the second question is.",
                    "label": 0
                },
                {
                    "sent": "How?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How do we get our subset and general?",
                    "label": 0
                },
                {
                    "sent": "There are many different approaches proposed and different papers, and so their supervised or unsupervised or random or whatever.",
                    "label": 0
                },
                {
                    "sent": "So here we use an unsupervised approach.",
                    "label": 0
                },
                {
                    "sent": "We use the incompleteness getting composition of kernel matrix and this corresponds to an iterative partial agreement 'cause it didn't feature space.",
                    "label": 0
                },
                {
                    "sent": "But it's not a point so it's just a selection procedure and.",
                    "label": 0
                },
                {
                    "sent": "The essence of it is that the number of elements that we select using this procedure will depend on the effective rank of the kernel matrix and internally affective rank.",
                    "label": 0
                },
                {
                    "sent": "So Agnelli or something, metrics depend on the complexity of the solution, and this can be seen later on.",
                    "label": 0
                },
                {
                    "sent": "The bottom line is that if we have a simpler solution, then we will have better generalization and.",
                    "label": 0
                },
                {
                    "sent": "Be cause lower complexity means lower M at runtime.",
                    "label": 0
                },
                {
                    "sent": "Which is important for reinforcement learning because we don't sell just one regression problem we have to solve it over and over again during the policy iteration cycle.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Now we know how.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Compute value function if we know that parameters Now we have to worry about by the parameters and the parameters forgotten process.",
                    "label": 0
                },
                {
                    "sent": "Temporal difference learning can be found the same way as they can be found for gossip process.",
                    "label": 0
                },
                {
                    "sent": "So we can.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The likelihood of the data which is a gas distribution.",
                    "label": 1
                },
                {
                    "sent": "We consider that as a function of the hyperparameters take the logarithm and get the log likelihood, which is just the equation given in yellow gold whatever color.",
                    "label": 0
                },
                {
                    "sent": "And this is a function that depends on the hyperparameters data and we want to minimize this likelihood with respect to Theta.",
                    "label": 1
                },
                {
                    "sent": "That's a nonlinear problem, so we need some gradient based solver is not really a problem and also the gradient of this expression can be obtained in closed form so.",
                    "label": 0
                },
                {
                    "sent": "Nothing to worry about too much, and if we look at this likelihood function we see that it consists of two conflicting terms.",
                    "label": 0
                },
                {
                    "sent": "So the first term here, this determinant of Q.",
                    "label": 0
                },
                {
                    "sent": "This is the complexity term and if you remember the determinant product of eigenvalues that explains how everything is connected and the second term here.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "It's not immediately obvious, but it's actually the data fit.",
                    "label": 0
                },
                {
                    "sent": "It measures how well your approximation fits the data.",
                    "label": 0
                },
                {
                    "sent": "It's the value of the objective function of a.",
                    "label": 0
                },
                {
                    "sent": "Related to this quest problem.",
                    "label": 1
                },
                {
                    "sent": "And generally with function approximation we have the usual dilemma.",
                    "label": 0
                },
                {
                    "sent": "So we either have a solution that has large bandwidth, which means we have a high complexity weekly solution which will have a large Rankin K and which may produce a low data error an we have functions that have small bandwidth and low complexity and probably high data and we want to combine both of that.",
                    "label": 0
                },
                {
                    "sent": "That's all audio function is to me.",
                    "label": 0
                },
                {
                    "sent": "OK, so one advantage of.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Adjusting and determining hyperparameters by such an automated procedure is that we can use a covariance function that depends on a larger number of parameters, so we just don't have to use one and set that by hand.",
                    "label": 0
                },
                {
                    "sent": "We can use many more and set them automatically.",
                    "label": 0
                },
                {
                    "sent": "An doing this allows us to better fit the data an actually.",
                    "label": 0
                },
                {
                    "sent": "Yeah, find out what is relevant and what's irrelevant.",
                    "label": 0
                },
                {
                    "sent": "So here we consider.",
                    "label": 0
                },
                {
                    "sent": "Three variants of this squared exponential kernel.",
                    "label": 0
                },
                {
                    "sent": "One which has the very number one which is just the identity matrix which has a uniform length scale along all dimensions and dimensions.",
                    "label": 0
                },
                {
                    "sent": "Here means state variables.",
                    "label": 0
                },
                {
                    "sent": "So which is uniform along Allstate variables.",
                    "label": 0
                },
                {
                    "sent": "Variant #2 has one parameter for each state variable that are in dependently adjusted and #3 is the same #2 but also allows for rotations and the point is now if we look at these two variances.",
                    "label": 0
                },
                {
                    "sent": "If we set by para meters automatically from the data and model selection works as everybody hopes it works, then one selection will automatically drive those values to zero that are not important.",
                    "label": 0
                },
                {
                    "sent": "So for example, if one of the coordinates of the state variable is not as important as another one, then it will go to zero and we can actually find out that we can remove it from the whole problem.",
                    "label": 0
                },
                {
                    "sent": "Which is nice.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's see some experiments.",
                    "label": 0
                },
                {
                    "sent": "First one is.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One of our beloved gridworld.",
                    "label": 0
                },
                {
                    "sent": "So we have 11 * 11 cells.",
                    "label": 0
                },
                {
                    "sent": "In the middle of the center column is our goal state and every other step outside the goals that give us a reward of minus one.",
                    "label": 0
                },
                {
                    "sent": "We have stochastic transitions.",
                    "label": 0
                },
                {
                    "sent": "And as you can see.",
                    "label": 0
                },
                {
                    "sent": "We have a state that consists of the X coordinate and the Y coordinate, but the Y coordinate is completely irrelevant for predicting the value function.",
                    "label": 0
                },
                {
                    "sent": "So this is the true value function for this problem.",
                    "label": 0
                },
                {
                    "sent": "So we have 0 value at the gold state and then minus 1 -- 2 and so on.",
                    "label": 0
                },
                {
                    "sent": "So the other states.",
                    "label": 0
                },
                {
                    "sent": "And now we draw 500 sample transitions from this policy.",
                    "label": 0
                },
                {
                    "sent": "And run our algorithm using the uniform kernel and the one that has one length scale for every dimension.",
                    "label": 0
                },
                {
                    "sent": "So number 1 #2.",
                    "label": 0
                },
                {
                    "sent": "And as we can see the one having a independent parameter for every dimension is of course much better than the one that doesn't.",
                    "label": 0
                },
                {
                    "sent": "So this is far more quickly than this solution here, so this is the better solution and now look we now we look in some detail at the.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also.",
                    "label": 0
                },
                {
                    "sent": "Let's see what model selection actually produces.",
                    "label": 0
                },
                {
                    "sent": "So just look at the first 2 rows.",
                    "label": 0
                },
                {
                    "sent": "So we see likelihood score, which is the thing that we're trying to minimize.",
                    "label": 0
                },
                {
                    "sent": "So lower scores are better.",
                    "label": 0
                },
                {
                    "sent": "We see that model number 2, which is the one that has independent parameter.",
                    "label": 0
                },
                {
                    "sent": "Every dimension is has lower negative likelihood.",
                    "label": 0
                },
                {
                    "sent": "Also a higher likelihood, and such is a better model.",
                    "label": 0
                },
                {
                    "sent": "And if we see at the two terms that make up this likely what we see that.",
                    "label": 0
                },
                {
                    "sent": "The complexity of the solution #2 is lower and the data fit is better, so it's a better model.",
                    "label": 0
                },
                {
                    "sent": "And the complexity can also be seen in this frontier, which shows you the eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "So the upper line is the eigenvalues of model number one.",
                    "label": 0
                },
                {
                    "sent": "This is model number 2 and now comes the interesting point.",
                    "label": 0
                },
                {
                    "sent": "If we look at the hyperparameter so produced from model number two, we see that the state variable that corresponds to the Y coordinate has a very low value and we can ask ourselves could we remove it from the model?",
                    "label": 0
                },
                {
                    "sent": "And this is exactly what we are examining in the third case.",
                    "label": 0
                },
                {
                    "sent": "So we now we set.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "The lengthscale corresponding to the white is zero.",
                    "label": 0
                },
                {
                    "sent": "Compute the likelihood of this model.",
                    "label": 0
                },
                {
                    "sent": "The likelihood goes further down and now we see why it goes further down.",
                    "label": 0
                },
                {
                    "sent": "So the complexity is further reduced.",
                    "label": 0
                },
                {
                    "sent": "But the data fit stays the same, so we can safely remove the irrelevant state variable and the only thing that we need to decide that is the likelihood and complexity of the data FIT scores, which we can compute.",
                    "label": 0
                },
                {
                    "sent": "Another experiment, now we had some more.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Relevant state variables just to see if it still works.",
                    "label": 0
                },
                {
                    "sent": "And so we have a 6 dimensional observation now.",
                    "label": 0
                },
                {
                    "sent": "And here we see how this optimization over the parameters proceeds.",
                    "label": 0
                },
                {
                    "sent": "So initially we start with every dimension having the same relevance, having the same length scale and as the optimization proceeds only the state variable that corresponds to the X coordinate, which is the one that only measures survives and every other state variable goes to 0, which is exactly what we would.",
                    "label": 0
                },
                {
                    "sent": "Like to see so.",
                    "label": 0
                },
                {
                    "sent": "This confirms that everything is working as hoped now.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so another experiment is a more realistic benchmark, not artificially created.",
                    "label": 0
                },
                {
                    "sent": "This is a pendulum balancing inverted pendulum and here we are trying to approximate the optimal value function which is shown on the right.",
                    "label": 0
                },
                {
                    "sent": "An yeah, we see all the three.",
                    "label": 0
                },
                {
                    "sent": "Results for the three covariance, so this is number one.",
                    "label": 0
                },
                {
                    "sent": "Those #2 and #3.",
                    "label": 0
                },
                {
                    "sent": "And as we can see.",
                    "label": 0
                },
                {
                    "sent": "Lot #1 and two are not not really that much different, but that's not really surprising because if we look at the value function shown at the top, we see that there is really no one single dimension that we could eliminate.",
                    "label": 0
                },
                {
                    "sent": "Cause because the value of a state box depends on the X and the Y coordinate.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if we look more closely at this function, we see that there that there is a sort of, well, dominant direction.",
                    "label": 0
                },
                {
                    "sent": "So if we look at this diagonal going from the left to the right, we see that the variance is much higher than the orthogonal diagonal to this.",
                    "label": 0
                },
                {
                    "sent": "So there is.",
                    "label": 0
                },
                {
                    "sent": "So this direction is more.",
                    "label": 0
                },
                {
                    "sent": "Important then this direction, and if we could identify this direction then we would be very happy and this is exactly what the third model achieves to do and as a consequence achieves a much lower prediction error as the other two models.",
                    "label": 0
                },
                {
                    "sent": "OK, so another consequence of this approach of this more tighter model is that.",
                    "label": 0
                },
                {
                    "sent": "The computational complexity is reduced something.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As a consequence of having a lower effective rank.",
                    "label": 0
                },
                {
                    "sent": "So here we see the number of elements that the Incompleteness Commission selects for the subset of regressors approach.",
                    "label": 0
                },
                {
                    "sent": "Given a fixed level of tolerance.",
                    "label": 0
                },
                {
                    "sent": "So if we for example, let's say we want to approximate within .1, then one number one chooses about.",
                    "label": 0
                },
                {
                    "sent": "Like say 150 model number 2 chooses like 130 and model number 3 chooses like 90 so there is vast computational savings can be gained by.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "By using a better model, because this number M entrance with the number of square into the computational complexity.",
                    "label": 0
                },
                {
                    "sent": "OK, so to finish we have seen that automatic feature selection in reinforcement learning is possible.",
                    "label": 1
                },
                {
                    "sent": "We have proposed we have proposed an approach that is based on Gaussian processes as the underlying function approximator and.",
                    "label": 0
                },
                {
                    "sent": "As a policy evaluation method we're using, we're using accounting process temporal difference turning, which corresponds to LLC one and want to call a rollout.",
                    "label": 0
                },
                {
                    "sent": "We have proposed a likelihood based model selection method and yeah, the bad news is that the whole thing doesn't actually come with any guarantees because there are some violation of some very minor independence assumptions.",
                    "label": 0
                },
                {
                    "sent": "But it seems to work in practice and that is all we care about.",
                    "label": 1
                },
                {
                    "sent": "Well, it's all that I care about and.",
                    "label": 0
                },
                {
                    "sent": "Yeah, well, there's there's some ongoing work of course, because right now we're only solving the policy allocation problem.",
                    "label": 0
                },
                {
                    "sent": "What we really want to do is to solve the policy iteration problem.",
                    "label": 1
                },
                {
                    "sent": "Service requires policy improvement, exploration or strategy for sample generation.",
                    "label": 1
                },
                {
                    "sent": "Also, perhaps an extension to the state action Space is an.",
                    "label": 0
                },
                {
                    "sent": "Of course, we don't want to upgrade roads all our lives, so some more complex experiments and simulations and more theoretical insights when the whole thing will fail.",
                    "label": 0
                },
                {
                    "sent": "And compare it with all the other methods that there are and.",
                    "label": 0
                },
                {
                    "sent": "Basically that's it.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's some related work.",
                    "label": 0
                },
                {
                    "sent": "The original work where Gaussian process returning was proposed.",
                    "label": 0
                },
                {
                    "sent": "That's like.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Five years, six years ago.",
                    "label": 0
                },
                {
                    "sent": "And there's some other work with adaptation of basis functions in reinforcement learning, but they don't consider the spacing approach and they don't have this tradeoff between complexity and data fit, so they are likely to overfit the solution and some we are well related approaches and that they are not not the same whether not really tuning the hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "OK, that's it.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "What do you expect?",
                    "label": 0
                },
                {
                    "sent": "What numbers are features?",
                    "label": 0
                },
                {
                    "sent": "Yes, yes.",
                    "label": 0
                },
                {
                    "sent": "You mean the dimensionality of the state space?",
                    "label": 0
                },
                {
                    "sent": "Yeah, well I hope that it's still going to work.",
                    "label": 0
                },
                {
                    "sent": "That's the whole reason that I propose that.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes yes we need.",
                    "label": 0
                },
                {
                    "sent": "We probably need more transitions, more data to say that more certainty.",
                    "label": 0
                },
                {
                    "sent": "Anymore questions.",
                    "label": 0
                }
            ]
        }
    }
}