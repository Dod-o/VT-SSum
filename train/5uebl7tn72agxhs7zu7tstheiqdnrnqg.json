{
    "id": "5uebl7tn72agxhs7zu7tstheiqdnrnqg",
    "title": "Detecting Similar Linked Datasets Using Topic Modelling",
    "info": {
        "author": [
            "Michael R\u00f6der, Agile Knowledge Engineering and Semantic Web (AKSW), University of Leipzig"
        ],
        "published": "July 28, 2016",
        "recorded": "May 2016",
        "category": [
            "Top->Computer Science->Big Data",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/eswc2016_roeder_topic_modelling/",
    "segmentation": [
        [
            "That's the."
        ],
        [
            "One of my talk and the head is missing again, but OK, let's go on.",
            "First, I want to start with my motivation.",
            "What we try to solve.",
            "And then after that I will give a brief introduction into topic modeling.",
            "Cause most of you won't know it.",
            "And before I explain the workflow of our research, prototype and evaluation."
        ],
        [
            "So let's assume we have created.",
            "RDF data set.",
            "Because we want to publish our data, we want to be a good data scientist, so we want to have these five star linked open data.",
            "That means that we created an RDF data set and now we have to link it to already existing datasets to give it the context.",
            "But the problem is that the linked open data cloud that we see on the right hand side is very big and in most cases we won't know all the datasets that are inside this data cloud.",
            "So actually we need some help that.",
            "We need some service that gives us a set of RDF datasets that are interesting for linking them with our data set, and we assume that.",
            "Datasets that are topically similar are also interesting for linking our data to their data.",
            "OK."
        ],
        [
            "And topical similarity is already known from the work with natural language documents, and this is called probabilistic topic modeling.",
            "Anne."
        ],
        [
            "It looks like this.",
            "So on the left hand side we have topics.",
            "We assume that these are latent dimensions and they consist of set set of words and we see two topics.",
            "And on the right hand side we see our documents and we can assign the words of the document to a single topic that has created this this word.",
            "And we see we can see with document two that the document can comprise more than one single topic.",
            "OK."
        ],
        [
            "But in reality, it looks like this, so we have the documents, but we don't know the topics.",
            "We don't know the which topic has created which document.",
            "And for that reason for that problem, there are already established solutions, so called inference algorithms that simply run and statistical inference on that and the result."
        ],
        [
            "This inference process.",
            "That I don't want to explain in detail is a set of topics like we see it here.",
            "So we have a set of every topic has a list of words that it can create and probability.",
            "For every word."
        ],
        [
            "And what we can do then after that is we can create.",
            "We can assign the important words of a document to a topic and then we can.",
            "Create a distribution over topics for this single single document.",
            "Is the statistics that we can see on the right hand side, and then we can see OK. Topic two is the most important topic for this document and for example we don't see the green topic at all there in that document.",
            "OK."
        ],
        [
            "And our research prototype, which is called tap yoga, makes use of this topic modeling."
        ],
        [
            "Solve our problem.",
            "This is the workflow that I'm going to explain in detail and we start with the meta data extraction step.",
            "So we take the RDF data set and we extract meta data which is.",
            "The tee box information of the data set and there we have three variants that we evaluated in the first variant, we create only the classes that are used inside the data set and their number of entities, and the second variant we extracted properties and their number of triples.",
            "And in the third variant we extracted both."
        ],
        [
            "And then we see a sort short example of an RDF datasets that simply states that Michelle Radar is has a type researcher and he attends the SWC 2016 anti presents.",
            "Tapioca and tapioca is a research prototype and from this RDF data set we would extract the green colored your eyes.",
            "So in that case we extract properties and classes and the result is simply the list on the bottom.",
            "We see the signal, your eyes and accounts."
        ],
        [
            "After that we create documents out of the data set out of this meta data and in the first step we filter your eyes cause we can assume that these well known your eyes do not add information about the topic of the data set.",
            "And after that we derive labels for every UI that we still have after this filter step.",
            "We do this either by dereferencing the UI and trying to get a label for it, or we are using the last part of the UI."
        ],
        [
            "For example, it looks like it looks like this.",
            "The RDF type is filtered out as a blue colored research prototype is.",
            "We assume in this example that we can do references UI and we got a label which is called prototype created by a researcher and for the other three we couldn't retrieve any labels, so we are simply using the last part of the UI for it."
        ],
        [
            "And then we filter out typical stop words that we now have and.",
            "We insert these words into the documents that from now on represent these RDF datasets and here we have again two variants.",
            "The first one is we use only unique words.",
            "That means every word that we see is.",
            "Inserted only once.",
            "And the other variant is that we use the logarithm of the count.",
            "We don't want to use account directly becausw.",
            "RDF datasets tend to get to become very large like the DB pedia or other large datasets.",
            "With up to thousands of triples and you can easily imagine that nearly every triple we create at least one word, so we have a huge document and then we can get into trouble with our probabilistic topic modeling.",
            "Without that, we are getting much more information, but just repeating the words again and again."
        ],
        [
            "OK, how does it look?",
            "In our example we have the words and the labels and accounts.",
            "Then we remove stop words like by and a.",
            "And we have the single terms with account and then we create our.",
            "Document, it doesn't matter which variant we use becausw.",
            "Our counts are pretty low.",
            "OK, and in the last step we derive our topic model.",
            "And based on this topic model.",
            "We can generate.",
            "The topic vector for every document."
        ],
        [
            "For this step we used later in directly allocation and a given number of topics as a parimeter for our approach."
        ],
        [
            "So now we can help our user who has created the.",
            "The new datasets down there we do the same preprocessing.",
            "We use our topic model to infer a topic vector for this new data set and then at the end we can calculate the cosine similarity between the vectors and we can retrieve the most similar RDF datasets."
        ],
        [
            "OK, how do we evaluated our approach?"
        ],
        [
            "We used a lot stats data set, it comprised of 1680 RDF datasets with.",
            "More than 700 million triples and we randomly sampled 100 RDF datasets to create a gold standard from it.",
            "These 100 datasets have more than 3 million triples and two research.",
            "Two researchers rated whether two datasets are topically similar to each other or not.",
            "And we had an interrater agreement of more than 97%."
        ],
        [
            "We compared ourselves with several baselines.",
            "The first baseline is the well known TF IDF measure.",
            "That means that for every document we calculate the TF IDF.",
            "Vector.",
            "Which is simply TF is the term frequency.",
            "So how often term occurs inside the document and IDF is the inverse document frequency that you can see on the right hand side.",
            "So this IDF term becomes lower the often the more often the word is used over all documents.",
            "The second baseline comes from.",
            "Related work from concerned our they.",
            "Proposed a search engine that is based on filters and one of these filters is a so-called topical aspect.",
            "And this topical aspect defines that.",
            "That two data sets D1 and D2.",
            "Are the similarity is calculated by the vocabularies that are used inside the datasets?",
            "That means that every vocabulary gets awaiting that we can see on the right hand side.",
            "The weighting works like the IDF term, so the more often the vocabulary is used in the inside.",
            "The known RDF datasets, the lower it's waiting.",
            "And the G function simply returns the one if the vocabulary is used in the data set.",
            "So this topical aspect is simply the sum of the weightings of the vocabularies that are used in both datasets.",
            "And."
        ],
        [
            "The last baseline is Apache Lucene that internally uses.",
            "And variation of TF IDF.",
            "The evaluation itself.",
            "Of for the evaluation we used the leave one out method.",
            "That means that we.",
            "Indexed 99 documents and used one document as a query and we repeated that for all documents.",
            "So that means that our system hasn't seen the one document that we use as a query."
        ],
        [
            "And these are the results for the different variants.",
            "We can see the six step yoga variations.",
            "As two columns are, one is a left top yoga column is with logarithmic counts.",
            "The right column is with unit counts and then the single lines are whether we use the classes or only properties or both.",
            "And we can see that.",
            "The version of the variant with a logarithm account that uses only properties is the best in our evaluation, and it's better than the baselines that you can see on the right hand side.",
            "So I already said that the number of topics is a parameter for our topic modeling.",
            "So that means that we can take a look how the parimeter.",
            "Which influence our parameter has and we can see that for the unique accounts, the.",
            "The variant that uses both classes and properties stays very low.",
            "The red one that uses only classes stays very low, two and the.",
            "Variant that uses the properties goes a little bit up, but.",
            "More interesting is the.",
            "This diagram that shows the variance with the logarithm logarithm account.",
            "So we can see that the classes are still very low.",
            "Overall number of topics.",
            "The.",
            "Variant that uses the properties goes up and has a maximum.",
            "I think at 61 topics.",
            "And the variant that uses both still is still rising at A at number of at 200 topics.",
            "The."
        ],
        [
            "It's why we increased the number of topics for this variant, and it achieves its maximum at the very right and shortly before 500.",
            "I think it was 498 topics.",
            "And.",
            "Yes.",
            "We repeated this evaluation with a complete lot stats data set.",
            "That means that we indexed all.",
            "1679 datasets again leaving out one that we used for a query and repeated that for all 100."
        ],
        [
            "Are part of our gold standard.",
            "And we see that our variant our approach is.",
            "Still the best compared to the baselines.",
            "And at the bottom we can see the influence of the number of topics."
        ],
        [
            "OK, to summarize, using topic modeling for determining the topical similarity between RDF datasets perform better than the baseline approach that we used.",
            "And the usage of properties and logarithm account perform best in our evaluation, and there's a lot of future work that I think has to be done here.",
            "I just picked out these three, so determining a good number of topics is still an open issue in the topic modeling community, but an alternative way would be to use a nonparametric nonparametric topic modeling approach.",
            "Then we have a problem with properties that have no English label, so we somehow have to handle them.",
            "And we have a prototype, but we still have to increase its usability so that it's really helpful for the community."
        ],
        [
            "Then thanks for your attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's the.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One of my talk and the head is missing again, but OK, let's go on.",
                    "label": 0
                },
                {
                    "sent": "First, I want to start with my motivation.",
                    "label": 0
                },
                {
                    "sent": "What we try to solve.",
                    "label": 0
                },
                {
                    "sent": "And then after that I will give a brief introduction into topic modeling.",
                    "label": 1
                },
                {
                    "sent": "Cause most of you won't know it.",
                    "label": 0
                },
                {
                    "sent": "And before I explain the workflow of our research, prototype and evaluation.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's assume we have created.",
                    "label": 0
                },
                {
                    "sent": "RDF data set.",
                    "label": 0
                },
                {
                    "sent": "Because we want to publish our data, we want to be a good data scientist, so we want to have these five star linked open data.",
                    "label": 0
                },
                {
                    "sent": "That means that we created an RDF data set and now we have to link it to already existing datasets to give it the context.",
                    "label": 0
                },
                {
                    "sent": "But the problem is that the linked open data cloud that we see on the right hand side is very big and in most cases we won't know all the datasets that are inside this data cloud.",
                    "label": 0
                },
                {
                    "sent": "So actually we need some help that.",
                    "label": 0
                },
                {
                    "sent": "We need some service that gives us a set of RDF datasets that are interesting for linking them with our data set, and we assume that.",
                    "label": 0
                },
                {
                    "sent": "Datasets that are topically similar are also interesting for linking our data to their data.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And topical similarity is already known from the work with natural language documents, and this is called probabilistic topic modeling.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It looks like this.",
                    "label": 0
                },
                {
                    "sent": "So on the left hand side we have topics.",
                    "label": 0
                },
                {
                    "sent": "We assume that these are latent dimensions and they consist of set set of words and we see two topics.",
                    "label": 0
                },
                {
                    "sent": "And on the right hand side we see our documents and we can assign the words of the document to a single topic that has created this this word.",
                    "label": 0
                },
                {
                    "sent": "And we see we can see with document two that the document can comprise more than one single topic.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But in reality, it looks like this, so we have the documents, but we don't know the topics.",
                    "label": 0
                },
                {
                    "sent": "We don't know the which topic has created which document.",
                    "label": 0
                },
                {
                    "sent": "And for that reason for that problem, there are already established solutions, so called inference algorithms that simply run and statistical inference on that and the result.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This inference process.",
                    "label": 0
                },
                {
                    "sent": "That I don't want to explain in detail is a set of topics like we see it here.",
                    "label": 0
                },
                {
                    "sent": "So we have a set of every topic has a list of words that it can create and probability.",
                    "label": 0
                },
                {
                    "sent": "For every word.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what we can do then after that is we can create.",
                    "label": 0
                },
                {
                    "sent": "We can assign the important words of a document to a topic and then we can.",
                    "label": 0
                },
                {
                    "sent": "Create a distribution over topics for this single single document.",
                    "label": 0
                },
                {
                    "sent": "Is the statistics that we can see on the right hand side, and then we can see OK. Topic two is the most important topic for this document and for example we don't see the green topic at all there in that document.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And our research prototype, which is called tap yoga, makes use of this topic modeling.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Solve our problem.",
                    "label": 0
                },
                {
                    "sent": "This is the workflow that I'm going to explain in detail and we start with the meta data extraction step.",
                    "label": 0
                },
                {
                    "sent": "So we take the RDF data set and we extract meta data which is.",
                    "label": 0
                },
                {
                    "sent": "The tee box information of the data set and there we have three variants that we evaluated in the first variant, we create only the classes that are used inside the data set and their number of entities, and the second variant we extracted properties and their number of triples.",
                    "label": 1
                },
                {
                    "sent": "And in the third variant we extracted both.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we see a sort short example of an RDF datasets that simply states that Michelle Radar is has a type researcher and he attends the SWC 2016 anti presents.",
                    "label": 0
                },
                {
                    "sent": "Tapioca and tapioca is a research prototype and from this RDF data set we would extract the green colored your eyes.",
                    "label": 0
                },
                {
                    "sent": "So in that case we extract properties and classes and the result is simply the list on the bottom.",
                    "label": 0
                },
                {
                    "sent": "We see the signal, your eyes and accounts.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "After that we create documents out of the data set out of this meta data and in the first step we filter your eyes cause we can assume that these well known your eyes do not add information about the topic of the data set.",
                    "label": 0
                },
                {
                    "sent": "And after that we derive labels for every UI that we still have after this filter step.",
                    "label": 1
                },
                {
                    "sent": "We do this either by dereferencing the UI and trying to get a label for it, or we are using the last part of the UI.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For example, it looks like it looks like this.",
                    "label": 0
                },
                {
                    "sent": "The RDF type is filtered out as a blue colored research prototype is.",
                    "label": 0
                },
                {
                    "sent": "We assume in this example that we can do references UI and we got a label which is called prototype created by a researcher and for the other three we couldn't retrieve any labels, so we are simply using the last part of the UI for it.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then we filter out typical stop words that we now have and.",
                    "label": 1
                },
                {
                    "sent": "We insert these words into the documents that from now on represent these RDF datasets and here we have again two variants.",
                    "label": 0
                },
                {
                    "sent": "The first one is we use only unique words.",
                    "label": 1
                },
                {
                    "sent": "That means every word that we see is.",
                    "label": 0
                },
                {
                    "sent": "Inserted only once.",
                    "label": 0
                },
                {
                    "sent": "And the other variant is that we use the logarithm of the count.",
                    "label": 0
                },
                {
                    "sent": "We don't want to use account directly becausw.",
                    "label": 0
                },
                {
                    "sent": "RDF datasets tend to get to become very large like the DB pedia or other large datasets.",
                    "label": 0
                },
                {
                    "sent": "With up to thousands of triples and you can easily imagine that nearly every triple we create at least one word, so we have a huge document and then we can get into trouble with our probabilistic topic modeling.",
                    "label": 0
                },
                {
                    "sent": "Without that, we are getting much more information, but just repeating the words again and again.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, how does it look?",
                    "label": 0
                },
                {
                    "sent": "In our example we have the words and the labels and accounts.",
                    "label": 0
                },
                {
                    "sent": "Then we remove stop words like by and a.",
                    "label": 0
                },
                {
                    "sent": "And we have the single terms with account and then we create our.",
                    "label": 0
                },
                {
                    "sent": "Document, it doesn't matter which variant we use becausw.",
                    "label": 0
                },
                {
                    "sent": "Our counts are pretty low.",
                    "label": 0
                },
                {
                    "sent": "OK, and in the last step we derive our topic model.",
                    "label": 0
                },
                {
                    "sent": "And based on this topic model.",
                    "label": 0
                },
                {
                    "sent": "We can generate.",
                    "label": 0
                },
                {
                    "sent": "The topic vector for every document.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For this step we used later in directly allocation and a given number of topics as a parimeter for our approach.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we can help our user who has created the.",
                    "label": 0
                },
                {
                    "sent": "The new datasets down there we do the same preprocessing.",
                    "label": 0
                },
                {
                    "sent": "We use our topic model to infer a topic vector for this new data set and then at the end we can calculate the cosine similarity between the vectors and we can retrieve the most similar RDF datasets.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, how do we evaluated our approach?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We used a lot stats data set, it comprised of 1680 RDF datasets with.",
                    "label": 0
                },
                {
                    "sent": "More than 700 million triples and we randomly sampled 100 RDF datasets to create a gold standard from it.",
                    "label": 1
                },
                {
                    "sent": "These 100 datasets have more than 3 million triples and two research.",
                    "label": 0
                },
                {
                    "sent": "Two researchers rated whether two datasets are topically similar to each other or not.",
                    "label": 0
                },
                {
                    "sent": "And we had an interrater agreement of more than 97%.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We compared ourselves with several baselines.",
                    "label": 0
                },
                {
                    "sent": "The first baseline is the well known TF IDF measure.",
                    "label": 0
                },
                {
                    "sent": "That means that for every document we calculate the TF IDF.",
                    "label": 1
                },
                {
                    "sent": "Vector.",
                    "label": 0
                },
                {
                    "sent": "Which is simply TF is the term frequency.",
                    "label": 0
                },
                {
                    "sent": "So how often term occurs inside the document and IDF is the inverse document frequency that you can see on the right hand side.",
                    "label": 0
                },
                {
                    "sent": "So this IDF term becomes lower the often the more often the word is used over all documents.",
                    "label": 0
                },
                {
                    "sent": "The second baseline comes from.",
                    "label": 0
                },
                {
                    "sent": "Related work from concerned our they.",
                    "label": 0
                },
                {
                    "sent": "Proposed a search engine that is based on filters and one of these filters is a so-called topical aspect.",
                    "label": 1
                },
                {
                    "sent": "And this topical aspect defines that.",
                    "label": 0
                },
                {
                    "sent": "That two data sets D1 and D2.",
                    "label": 0
                },
                {
                    "sent": "Are the similarity is calculated by the vocabularies that are used inside the datasets?",
                    "label": 0
                },
                {
                    "sent": "That means that every vocabulary gets awaiting that we can see on the right hand side.",
                    "label": 0
                },
                {
                    "sent": "The weighting works like the IDF term, so the more often the vocabulary is used in the inside.",
                    "label": 1
                },
                {
                    "sent": "The known RDF datasets, the lower it's waiting.",
                    "label": 0
                },
                {
                    "sent": "And the G function simply returns the one if the vocabulary is used in the data set.",
                    "label": 0
                },
                {
                    "sent": "So this topical aspect is simply the sum of the weightings of the vocabularies that are used in both datasets.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The last baseline is Apache Lucene that internally uses.",
                    "label": 0
                },
                {
                    "sent": "And variation of TF IDF.",
                    "label": 0
                },
                {
                    "sent": "The evaluation itself.",
                    "label": 0
                },
                {
                    "sent": "Of for the evaluation we used the leave one out method.",
                    "label": 0
                },
                {
                    "sent": "That means that we.",
                    "label": 0
                },
                {
                    "sent": "Indexed 99 documents and used one document as a query and we repeated that for all documents.",
                    "label": 0
                },
                {
                    "sent": "So that means that our system hasn't seen the one document that we use as a query.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And these are the results for the different variants.",
                    "label": 0
                },
                {
                    "sent": "We can see the six step yoga variations.",
                    "label": 0
                },
                {
                    "sent": "As two columns are, one is a left top yoga column is with logarithmic counts.",
                    "label": 0
                },
                {
                    "sent": "The right column is with unit counts and then the single lines are whether we use the classes or only properties or both.",
                    "label": 0
                },
                {
                    "sent": "And we can see that.",
                    "label": 0
                },
                {
                    "sent": "The version of the variant with a logarithm account that uses only properties is the best in our evaluation, and it's better than the baselines that you can see on the right hand side.",
                    "label": 0
                },
                {
                    "sent": "So I already said that the number of topics is a parameter for our topic modeling.",
                    "label": 0
                },
                {
                    "sent": "So that means that we can take a look how the parimeter.",
                    "label": 0
                },
                {
                    "sent": "Which influence our parameter has and we can see that for the unique accounts, the.",
                    "label": 0
                },
                {
                    "sent": "The variant that uses both classes and properties stays very low.",
                    "label": 0
                },
                {
                    "sent": "The red one that uses only classes stays very low, two and the.",
                    "label": 0
                },
                {
                    "sent": "Variant that uses the properties goes a little bit up, but.",
                    "label": 0
                },
                {
                    "sent": "More interesting is the.",
                    "label": 0
                },
                {
                    "sent": "This diagram that shows the variance with the logarithm logarithm account.",
                    "label": 0
                },
                {
                    "sent": "So we can see that the classes are still very low.",
                    "label": 0
                },
                {
                    "sent": "Overall number of topics.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Variant that uses the properties goes up and has a maximum.",
                    "label": 0
                },
                {
                    "sent": "I think at 61 topics.",
                    "label": 0
                },
                {
                    "sent": "And the variant that uses both still is still rising at A at number of at 200 topics.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's why we increased the number of topics for this variant, and it achieves its maximum at the very right and shortly before 500.",
                    "label": 0
                },
                {
                    "sent": "I think it was 498 topics.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "We repeated this evaluation with a complete lot stats data set.",
                    "label": 0
                },
                {
                    "sent": "That means that we indexed all.",
                    "label": 0
                },
                {
                    "sent": "1679 datasets again leaving out one that we used for a query and repeated that for all 100.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are part of our gold standard.",
                    "label": 0
                },
                {
                    "sent": "And we see that our variant our approach is.",
                    "label": 0
                },
                {
                    "sent": "Still the best compared to the baselines.",
                    "label": 0
                },
                {
                    "sent": "And at the bottom we can see the influence of the number of topics.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, to summarize, using topic modeling for determining the topical similarity between RDF datasets perform better than the baseline approach that we used.",
                    "label": 1
                },
                {
                    "sent": "And the usage of properties and logarithm account perform best in our evaluation, and there's a lot of future work that I think has to be done here.",
                    "label": 0
                },
                {
                    "sent": "I just picked out these three, so determining a good number of topics is still an open issue in the topic modeling community, but an alternative way would be to use a nonparametric nonparametric topic modeling approach.",
                    "label": 0
                },
                {
                    "sent": "Then we have a problem with properties that have no English label, so we somehow have to handle them.",
                    "label": 0
                },
                {
                    "sent": "And we have a prototype, but we still have to increase its usability so that it's really helpful for the community.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then thanks for your attention.",
                    "label": 0
                }
            ]
        }
    }
}