{
    "id": "l4jiarx62mdgsudhklvih7ad2sklfeig",
    "title": "Semi-Supervised Learning and Learning via Similarity Functions: two key settings for Data-dependent Concept Spaces",
    "info": {
        "author": [
            "Avrim Blum, School of Computer Science, Carnegie Mellon University"
        ],
        "published": "Dec. 20, 2008",
        "recorded": "December 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/wehys08_blum_ssll/",
    "segmentation": [
        [
            "Alright, so welcome everyone to do this."
        ],
        [
            "Workshop so the theme of this workshop is data dependent concept spaces.",
            "And So what we mean by data dependent concept spaces.",
            "So using the data that determine the set of functions that you're viewing is most natural for your problem.",
            "So classic example is like the notion of a large margin separator, where Apri Ori, before seeing any data, is not like one separators anymore large margin than another.",
            "But this, but you're using the data.",
            "Take this sort of pre bias or this belief that the world ought to be a large margin separated that plus the data helps you instantiate that.",
            "Into a more concrete bias that then gives you a way of of ordering or preference ordering over over functions.",
            "And often this happens.",
            "Much of it is even without considering the labels of the data.",
            "How you get your preference ordering here, so I want."
        ],
        [
            "So in this talk, is talk about two nice settings where this is especially helpful, and one of them is semi supervised learning and another is learning from similarity function.",
            "So kernels but also more general other pairwise similarity functions.",
            "Alright."
        ],
        [
            "So let me start with Semi supervised learning.",
            "So the idea and semi supervised learning is well, can we use unlabeled data to help us augment a small labeled sample to improve learning?",
            "And, well, you know unlabeled data is missing the most important information.",
            "So it's not so clear what it can do for us.",
            "But the hope is, well, maybe you can likely unless last slide.",
            "Maybe you can help us instantiate our biases, help us in our searching over functions."
        ],
        [
            "So I want to do it so this talks with these two parts in the first half I'll start by just kind of briefly serving a few examples of some very different semi supervised learning algorithms that have different kinds of biases, and then describe a way of thinking about these in terms of data dependent distribution dependent concept spaces.",
            "This is joint work with Nina Balcan, and the idea is that we can use this to help us understand questions like, well, ran an unlabeled data help us.",
            "And and try to help make sense of what's going on in these different sorts of algorithms.",
            "Height."
        ],
        [
            "Good so I want to give 3 examples of some semi supervised learning algorithms, so Co training is 1 so Co training.",
            "The idea is that while many problems have two different sources of information that you can use to determine the label, so we can think of examples, a pair of two different types of information and.",
            "One nice example.",
            "This is classifying web pages.",
            "You really have two different kinds of information.",
            "You can use words on the page itself.",
            "This is my page words on the page itself.",
            "Or you could have words on links playing to the page.",
            "And and both both are different sources of information.",
            "They tell you about what's going on there, what."
        ],
        [
            "The page that is.",
            "And the idea, an iterative code training is where we could maybe use a small labeled sample to learn some initial rules.",
            "So for instance you see the word my advisor pointing to the page.",
            "It's good chance it's a faculty member, home page.",
            "Or if you see on the page that talks about I am teaching blah blah blah, that's another indicator.",
            "Maybe that's a faculty members home page."
        ],
        [
            "And then, given that initial information, so these are these.",
            "These two little algorithms and have learned a little bit.",
            "You could look for unlabeled data where one of them is confident.",
            "Maybe on this example here there's a link that says my advisor pointed some page and they can take the other half and send it over and say, well, I think this is probably a faculty member homepage to alter the other algorithm and and.",
            "Help that one learn.",
            "And they can each help each other, and so we could train 2 classifiers.",
            "Why any type of information and try to use each to help the other.",
            "So that's the idea of it."
        ],
        [
            "Co training and just to give a simple example, there's a simple theoretical example of this.",
            "I think helps illustrate what's going on.",
            "Imagine that, so one of the simplest case of learning is we're trying to learn intervals so we have data on a line and target function.",
            "Some interval positives inside and negative outside, and if you have two views, it's natural to imagine that given interval on view number one over here and interval interval on view #2 which were plotting up here, and so the positive should be inside both intervals, negative should be outside of both intervals, and so in that case if we're just giving a little bit of labeled data, like say, this one.",
            "Positive and that one is positive and all the rest are unlabeled.",
            "Well, in this case, view number two could say, well, hey if I'm interval and this is positive.",
            "MRSA positive this guy who's in between all the positive also and label that is positive.",
            "Send it down to this algorithm.",
            "This algorithm can.",
            "So that's positive this is positive.",
            "This is positive and so this guy will be positive.",
            "Also send that label to the other guy and so forth.",
            "In this way they can help out each other.",
            "OK, so that's code train."
        ],
        [
            "That's iterative code training and another you can view this as kind of an iterative approximation to what you might imagine directly optimizing a joint objective where you've got some labeled data.",
            "You've got some unlabeled data and you want to optimize on the one hand accuracy on your label data.",
            "An agreement between the two halves on your unlabeled data.",
            "OK."
        ],
        [
            "And then this example of some problems where you can do this within different kinds of preprocessing.",
            "For visual images, for instance to get your two different views."
        ],
        [
            "OK, so another example is transductive support vector machines or semi supervised support vector machines and their.",
            "Well suppose we believe the target separator goes through low density regions of the space is a different kind of belief we might have.",
            "We believe it should have a large margin, then it's natural to look for a separator has a large margin with respect to both the labeled and the unlabeled data, so accurate unlabeled data, large margin over the whole thing.",
            "OK, and then the idea here is that well, if you only had labeled data, you might not have enough to really help understand what your margin really is.",
            "So if you're labeled data, maybe this looks like the largest margin separator, but given your unlabeled data, you realize that this is a better margin separator, and so you can use your your unlabeled data to help you instantiate this bias and looking for a large margin separated over your distribution."
        ],
        [
            "Unfortunately, optimization problems now harder, so we just had labeled data.",
            "Finding a large margin separates nice and convex unlabeled data.",
            "Not so nice, so it's more painful to do and various different sort of local optimization branch and bound other sort of algorithms people have tried for actually doing it."
        ],
        [
            "And then one.",
            "One more example, kind of different from both of these are graph based methods.",
            "Suppose we believe that very similar examples.",
            "They probably have the same label, so that's a natural belief that kind of motivates the nearest neighbor kind of algorithm.",
            "If you have a lot of labeled data, so enough labeled data so that you know you expect there to be given some new test point.",
            "Actually something close to it.",
            "You don't have enough labeled data, maybe there's nothing close to it that you have labeled.",
            "If you have unlabeled data, maybe you can use them as stepping stones.",
            "Between labeled points, so it may be that example from Jerry Zhu.",
            "So you might have some points are not very similar, but there's a sequence of unlabeled points in between them that can help tell you that they are actually similar to each other in some way.",
            "So so if I believe that similar examples ought to have the same label, unlabeled data can help me, because I could say, well know what either of these labels are.",
            "But they're probably the same and use that to start connecting between exam."
        ],
        [
            "Apples.",
            "So you could construct a graph with edges between very similar examples and then try to use for the partition.",
            "This graph in some way.",
            "OK, good."
        ],
        [
            "Here's another example from some.",
            "My Co author, Nina some visual image data.",
            "We're looking at visual images and trying to figure out who's in the image and the idea is that unlabeled data can help because we may be hard to tell that you know, one image looks directly like another, but you have a sequence and say oh this is similar to this in one way, maybe similar to that.",
            "Another way you can help you make that inference."
        ],
        [
            "And then you could various objectives.",
            "You can then solve for given this graph.",
            "OK, so those are."
        ],
        [
            "Some examples and then.",
            "Stepping back with this kind of a unifying principle, whether these have in common, they seem to have very different biases.",
            "One that, well, I think they all have a large margin separation.",
            "Another this thing is about 2 views.",
            "What have in common and how can we think about this theoretic?"
        ],
        [
            "So.",
            "A nice model for so one way of thinking about this is we can say well what's happening here is unlabeled data is helping us if we have beliefs, not just about the form the target has like we think the target is when your separator or small decision tree, but about how it relates to the distribution.",
            "So we have some beliefs about how the target relates the underlying distribution and if we're."
        ],
        [
            "That situation, then we can kind of model that theoretically like this.",
            "So in the standard pack model, we think of our biases, a set of functions like linear separators or maybe a scoring and ordering or scoring over over functions like with decision trees, while every Boolean functions decision tree.",
            "But you really mean small decision trees.",
            "Somehow you're more preferable, large decision trees less so.",
            "So those concept class well.",
            "So what we can do is we can replace that sort of class of functions were scoring over functions.",
            "We can think of is we can have a scoring over function distribution pairs.",
            "OK, so a class of function distribution pairs.",
            "And if we have that then.",
            "So we think of this as well.",
            "How compatible is some function with some underlying distribution that these two make sense together?",
            "If we have something like that, then given a distribution given information about distribution from unlabeled data, we can view this scoring over function distribution pairs.",
            "If you like as a mapping from distributions to scoring over functions.",
            "So if we have a notion of how natural a particular function distribution, how well they go together and given information about the distribution that gives us.",
            "Which gives us a way of ordering our functions, and so it's a distribution dependent concept class now.",
            "Now we want something a little bit more.",
            "We actually want that this is something that we can estimate from a finite sample.",
            "So if we have a notion of how well a function distribution go together, but we can't actually estimate that from a finite sample of data, it's not going to be very useful for us, as we don't usually don't have the distribution kind of in our hands, so we want some notion of how how well they go together that we can estimate from a finite sample, and that can let us estimate the true score of some function approximately from sample.",
            "OK, so that's the idea of."
        ],
        [
            "Of of.",
            "This model, so the idea here is we can augment the notion of a concept class with a compatibility score.",
            "Chi how compatible with some hypothesis some distribution.",
            "So instead of learning a function, we would learn learning a class.",
            "We would learn this class with respect to some notion of compatibility.",
            "Expresses our beliefs or hopes about the world, for instance, at the.",
            "Target ought to be a large margin separator and then just like we're saying, we could then given are a big class of functions like linear separators and given this of abstract prior belief, we take unlabeled data and instantiate it.",
            "To say large margins with respect to the actual distribution that we have.",
            "And then we want this to be estimated from a final.",
            "You can estimate from a finite sample."
        ],
        [
            "And so one way of formally instantiating this is, so we don't need to be so strict.",
            "But but when it turns out to be nice is it supposes we can require compatibility of a function distribution to be an expectation over individual examples, and actually many of our notions of compatibility, like the ones in those examples.",
            "You can view this way either expectation over single examples are over pairs of examples, let's say over single examples.",
            "If you do this, actually something very nice happens.",
            "It means you can view incompatibility how unnatural some function is as a kind of unlabelled.",
            "Error rate and you can view incompatibility on.",
            "You can view it as an unlabeled loss function over over example.",
            "So you can say given a hypothesis you have an unlabeled loss function over individual examples and the notion of unlabeled error rate, which is well, what is the expected value of this quality?",
            "What is your?",
            "Expected unlabeled loss.",
            "So for instance code training we can think of it this way so we can think of an unlabeled, unlabeled error for a given hypothesis.",
            "An example is did it disagree on the two house?",
            "So given some hypothesis that, did you say something different in 1/2 and the and the other, and your unlabeled error rate is, then this expectation is the probability mass on examples where they disagree and so.",
            "So we can really think of that as a.",
            "Is a kind of an unlabeled there.",
            "Of course we have labeled with the true error, which is, well, you know what's your error, but give us an unlabeled error rate.",
            "Yeah, and then the compatibility is 1 minus the only."
        ],
        [
            "There or in semi supervised support vector machines.",
            "We can think of our unlabeled error rate is the probability mass near to the separator or perhaps some function?",
            "Of how close you are to the separator.",
            "OK, so we can in these examples.",
            "In many cases you can take the priors that you have and view them as a notion of unlabeled error rate."
        ],
        [
            "So if you do this, actually something kind of interesting happens because the high level question here is.",
            "Well, when is unlabeled data going to help?",
            "And by how much?",
            "And what do you need?",
            "So here's the simplest kind of theorem you can prove in this setting, but I think it helps illustrate.",
            "Some of the nice principles here, so imagine that we believe our target really is compatible with the distribution.",
            "It really does go along with our belief.",
            "So then there's a natural distribution dependent concept class, which is let's look at all the concepts whose unlabeled errors at most some epsilon, the ones that are highly compatible or a little a little slightly incompatible.",
            "But.",
            "Mostly so, for instance for Co training hypotheses, I have at most epsilon disagreement under our distribution on the two house, so in that case we can get it use our usual sort of the simple kind of bounds that you get for a finite classes.",
            "What happens is your unlabeled sample that the usual bound you get.",
            "How much labeled data becomes a bound on unlabeled data?",
            "And so if we see at least this much unlabeled data, that's enough to estimate how.",
            "Compatible, our functions are and then.",
            "The amount of labeled data we need is just a function of the size of this class.",
            "And then we can say well with high probability, all our functions that have are consistent with our labeled sample and compatible with our unlabeled sample with high probability.",
            "They actually have low to error.",
            "What I think I mean.",
            "So what I think is kind of nice conceptually, here is what's happening is there's this is saying is this really two things we want and need to be true in order for unlabeled data help us?",
            "One is what we're hoping here that really our beliefs are correct.",
            "Our target really is compatible, but the other is that there's a notion of distributions being helpful and not helpful.",
            "So helpful distribution, because two things we want.",
            "One is that we want.",
            "Our target to belong to this class and the other is that we want not too much.",
            "Other stuff belongs to this class also, so even it's not enough just for our beliefs to be right.",
            "We also need the distribution to help us out in helping US knockout other things.",
            "Otherwise we didn't actually get anywhere from here to here."
        ],
        [
            "OK, so for example in Co training if you say well this stuff with two views are too complicated.",
            "I'll just replicate the first few over here.",
            "You know the target can be compatible with everything else, and so you didn't really get anywhere.",
            "Yeah."
        ],
        [
            "And then you can also extend this to the case where the target is not fully compatible, and then what you care about, or this other functions that are only a little bit less compatible with your beliefs than the target was.",
            "Another so for instance, another case of this would be if you have a margin belief and the distribution is uniform, well then all the functions are just as compatible as a target, so it's not helping if you have a nice distribution and then OK."
        ],
        [
            "You could also have an agnostic version of theorems like this, and then you end up with something that looks.",
            "A little mess here, but the idea then, if you're not so, there's two things to be agnostic about.",
            "Now.",
            "One is, I don't know how exactly compatible my target function is with my beliefs, and also I don't know.",
            "Um?",
            "If there's a 0 error function in my class, or maybe to get to 0, I have to go to some really high complexity level.",
            "So then the natural thing you can do is try to minimize your empirical label there, plus some penalty function that has to do with your unlabeled there and then you can prove.",
            "Natural type of thing, which is that the error you find will be nearly as good as the best function.",
            "Where best is some combination of the error it has on the labeled data and its penalty.",
            "So if there's a you know 20% error rate function is very highly compatible in the 10% error rate functions a little bit less so and so forth.",
            "And you want to.",
            "You can balance those off and then and then you get some penalty of some usual thing depending on the capacity of the class of functions whose compatibility is not too much worse than the hypothesis here.",
            "In the equation learning the.",
            "Head size of knowledge of their compatible sets.",
            "Where does that?",
            "Yeah, so that's.",
            "So this is a little this is the small term.",
            "This is the big term.",
            "This is just an extra little square root.",
            "This is this is this is kind of like a dependent structural risk minimization.",
            "This is the little one for going over the different teas.",
            "The Big one is here.",
            "Yeah, yeah, it's in the penalty.",
            "I just yeah yeah.",
            "So the big one is here.",
            "And this is the empirical kind of size.",
            "And then you want to say the empirical size will be close to the true size.",
            "As long as your unlabeled data set is large enough.",
            "So you want your only, so ideally you want, so I'm imagining here that your beliefs are respect to the true distribution, and so you want enough unlabeled data so that what you actually estimate on your sample.",
            "It's the class so that the function over your sample look compatible to certain level, and then you want that class to be close to the same for the distribution.",
            "Alright.",
            "One thing that comes up here is when you say how much unlabeled data do you need.",
            "Then there's actually two things that come in.",
            "One is how complicated my class of functions and the other is.",
            "How complicated is my notion of compatibility so I could have a very simple class of functions like.",
            "Intervals on the line, but if I have a really weird notion of what it means to be compatible with distribution that some bizarre thing, then I need a lot more unlabeled data too.",
            "Estimate this."
        ],
        [
            "OK, so like a typical example is Eva.",
            "Target is a large write the separator and ice distribution would be say uniform on some number of blobs.",
            "If you have enough unlabeled data, that would be enough to knock out the functions except for ones.",
            "Like like."
        ],
        [
            "So.",
            "One interesting feature of this.",
            "One interesting feature of the setting is often this two kinds of bounds.",
            "People often give uniform convergence bounds, those previous ones, or epsilon cover bounds.",
            "Epsilon cover bounds require more of your algorithm.",
            "They require your algorithm to take your set of functions, pick in advance some cover, then take label data and use it to select from those as opposed to uniform convergence bounds would say no, just go ahead and find me something.",
            "OK, I don't care what that's.",
            "Consistent and compatible, and one thing you get here is a lot of natural examples.",
            "There's actually a big gap between these two kinds of bounds, and I just want to end this part of the talk with an example of this.",
            "So here's a nice example.",
            "It's like a simple version of code training, so imagine that examples are pairs of points in 01 to the end.",
            "OK, so normally Coltrane this 2 views.",
            "I'm thinking of the two views is really being in the same space.",
            "So an example.",
            "Here's an example.",
            "It's a pair of points.",
            "They're both living in the same space.",
            "The target is a single linear separator, so it's just think of it.",
            "We've locked the two views together.",
            "OK, So what does the world look like?",
            "I'm looking at linear separators, unlabeled data.",
            "Are these little like line segments?",
            "OK, so examples of pair of points like that and what am I looking for?",
            "I'm looking for linear separator that doesn't slice through these line segments.",
            "OK, 'cause they're both part of the same example and agrees with the labeled data.",
            "In general, it's an NP hard problem.",
            "So that's annoying.",
            "But here I'm just worried about the sample issue and so can example here is compatible with both parts, or positive or both parts negative, so they're both on the same side.",
            "OK, so let's think about this case, so it's kind of an interesting case to think about.",
            "Suppose that the distribution is independent given the label so meaning that these two points are equal to random points subject to be on this side.",
            "And here it's 2 random points subject to being on that side.",
            "Friends maybe is the uniform distribution conditioned on?",
            "On each side, some balls say so.",
            "One thing you can say is that if you have.",
            "A polynomial and end number of dimensions, unlabeled examples and little O of log in labeled examples.",
            "Not that much labeled, but still then.",
            "You're not going to get yet.",
            "Get uniform convergence.",
            "There will still be many bad compatible.",
            "Meaning not slicing through these guys.",
            "Consistent meaning getting the label data correct functions.",
            "Why is that?",
            "Because it's so.",
            "Think about the Boolean cube.",
            "If you have not too many labeled points, you can have a hypothesis that basically slices those guys off from almost everything else.",
            "OK, you can.",
            "You give me a small number of positives and negatives.",
            "I can just say take the positive.",
            "Basically just slice those off and the negative and everything else in the world, all the unlabeled data to go on the other side, and it's perfectly compatible, but it's.",
            "If you think about what is that doing it just taking those few positives and slicing for everything else is probably not going to be very good.",
            "OK, especially if I promise you in advance the target 5050.",
            "On the other hand, if you just look at the unlabeled data, the unlabeled data would be enough to a polynomial number, unlabeled examples, enough to have when you look at you realize, even though there are many.",
            "Compatible functions we haven't labeled data yet.",
            "There's many compatible functions.",
            "There's only four kinds of compatible functions.",
            "All the ones that are compatible either look like the target or pretty close the opposite.",
            "The target labeling.",
            "Almost everything is positive.",
            "Labeling almost everything is negative.",
            "They all look like that.",
            "So first you pick one.",
            "One of each type.",
            "You don't know which is which, but I guess you can tell these two.",
            "But and then you look at your label data.",
            "Then you can say, OK, you know which one of those is really only four kinds of functions, even though you don't get uniform convergence yet.",
            "So so so in this setting often these epsilon cover bounds are.",
            "Helpful.",
            "In general, algorithmically there are more painful, although in this particular case, with independence, given the label, you can actually implement this efficiently.",
            "So I think it's a nice feature, OK?",
            "Well, so suppose I so here.",
            "The claim is if I have a lot of unlabeled data, then all the compatible functions will be close to one of these.",
            "So if I first pick a cover, I just pick one from this type one from this type.",
            "One of this type, one of this type, and then I look at my labeled data and then that's enough to throw out 'cause I only have one.",
            "I see I've taken this whole class of functions, they're the ones that label almost everything negative and just taking one.",
            "Um?",
            "Of those, and then they could use label data to pick from those.",
            "I mean so.",
            "Well, no, so I mean, if long as this is.",
            "So what we have from this, if we believe our target is compatible, then.",
            "At.",
            "Then we know it's gotta be one of these four types.",
            "So in this case, fully complete right?",
            "So it's true that because this is this is saying you can use less labeled data than even the amount needed to verify, because it's like one of these four.",
            "And if you think about it, actually, if if the target is roughly 5050 you can immediately throw out these two because.",
            "Otherwise, if the target, once you see this, if you realize you pick an instance that we pick one of these one representative.",
            "These one of these.",
            "One of these.",
            "If these guys are if one of your four functions roughly 5050, you know that's gotta be one of these two and not one of these days.",
            "You throw these out, you end up with two representatives.",
            "One of them is almost the target.",
            "One of 'em is almost the opposite of target.",
            "It only takes one labeled example to figure out whether it's this one or this one.",
            "Only money.",
            "Oh well, this means that.",
            "So this is a very strong assumption.",
            "This independence, given the label, means that there's only one right?",
            "So this is very, very strong, right?",
            "So I'm not.",
            "This is very extremely sorry.",
            "I'm just so the point of this slide is just to point out that this can give you better numbers than this.",
            "But but I agree, this is a strong.",
            "Please yes.",
            "OK, alright so now let me."
        ],
        [
            "02 OK, so the conclusion, so data dependent concept spaces can give a nice way of thinking about semi supervised learning.",
            "We have beliefs about how the target should relate to distribution.",
            "We can view this notion of unlabeled error rate.",
            "We can use data dependent structural risk minimization.",
            "The tradeoff, unlabeled error rate, and labeled error rate if we like.",
            "And we're using unlabeled data.",
            "Help estimate our bias.",
            "Unlabeled data too."
        ],
        [
            "To pick.",
            "We want to get back onto the onto the public schedule."
        ],
        [
            "OK, OK, so I'll do this part.",
            "OK, so.",
            "So kernels are, so I want to talk about learning with kernels and more general similarity functions.",
            "And some nice ways of data dependent concept spaces coming here.",
            "So let's be the five minute version.",
            "OK, so a kernel is a legal definition of dot product, so so it's a function where you take your two objects you take, you run your code and outputs a number that corresponds some implicit dot product.",
            "And they're very nice.",
            "'cause many learning algorithms be written so they only interact with the data by dot product.",
            "So wherever you want to take that product, you stick in the kernel and it's acting as if they're in this implicit space."
        ],
        [
            "Which is great and we have a nice theory that says, well, if data is linearly separable by a large margin, we don't need too much data to generalize well, and so if data several by a large margin, this implicit space, we can have some confidence in how well we're doing, which is excellent.",
            "Kernels are very useful in practice for many different kinds of."
        ],
        [
            "The one issue I want to get out here is that there's a little bit of a disconnect between the theory and how we think about kernel, so two different ways we think about kernels.",
            "One is it intuitively we think of a good kernel for a problem is one that makes sense is a notion of similarity.",
            "So if I say I've got some kind of data I don't know DNA sequences or something come up with a good kernel.",
            "You're thinking what's a good way of measuring how similar two sequences are, but our theory is talking about margin, implicit space, so you're not thinking about margins and implicit space.",
            "We were trying to think about how am I going to measure how similar two strings are.",
            "And it would be nice to sort of bring this little closer together.",
            "And so."
        ],
        [
            "Thing we've been doing this is joint work with Nina Balcan.",
            "Naughty Srebro is looking at trying to find a good.",
            "A good measure of similarity, so it's a quality notion like large margin and implicit space, but other kinds of quality notions that talk in terms of more direct properties and maybe not don't even require this notion of similarity to be illegal kernel.",
            "Ideally, these notions would include the usual notion of what we mean by a good kernel, so something that has a large margin.",
            "It's implicit space would also be a good similarity function.",
            "According to this notion.",
            "And if something has this notion, we were able to learn, and maybe we can even learn things that you couldn't do with large margin kernels.",
            "And this notion of data dependent concepts."
        ],
        [
            "This will come come right in so.",
            "So here, so let me define two properties, one of warm up and then and then the main property.",
            "So here's a very intuitive property, so we have some learning problem.",
            "I'm going to think of a distribution on data.",
            "Examples are labeled by some target function.",
            "We have a measure of similarity between pairs of objects.",
            "So X&Y here are two examples.",
            "1 means they're very similar, minus one means they're very dissimilar.",
            "A natural property is we would like a great thing to have would be if most examples are on average more similar to other examples of their label.",
            "Of their type, then to random examples of the other type.",
            "So most of the positives are more similar.",
            "On average, the positives random pauses into random negatives and vice versa, so that's just in math.",
            "Most examples X are on average more similar to random wise or the same label then to random labels examples why the other labeled by some gap gamma.",
            "If we had that learning would be nice easy.",
            "The world would be would be great.",
            "All we have to do to learn."
        ],
        [
            "One is we just take a bunch of data, some positives, some negatives, basically positives to estimate this quantity.",
            "Here negative steps to make that quality there and then we just classify new example based on are you more similar in average to these examples or more similar?",
            "Which of those examples average nearest neighbor?",
            "And this is enough data so that with high probability you haven't you basically.",
            "Have very good estimates of these expectations."
        ],
        [
            "Unfortunately, this notion of quality is a Pretty Little bit strict.",
            "It doesn't capture what we mean by a good kernel, and here's a simple example of that.",
            "If our data looks like negatives here, pauses there.",
            "In positives there and similarities just dot product, so there's no 55 spaces, just the space here.",
            "This is a large mark.",
            "This is a nice kernel.",
            "There's a large margin separator, but these examples by DOT product are actually more similar.",
            "2 random negative examples.",
            "If this is 30 degrees, these have dot product 1/2 then they are the random positive examples 'cause they have dot product one with themselves but negative 1/2 with those that averages only 1/4 and so.",
            "These are actually more similar to negatives and positives.",
            "They didn't satisfy the condition we wanted."
        ],
        [
            "So one solution to this is we can broaden our notion that we can say the similarity function is good.",
            "If well, there's a region of reasonable points so that most examples are on average more similar to the reasonable points of their label then to the reasonable points of the other label.",
            "Well, maybe they even know in advance what those results are.",
            "We just want them to."
        ],
        [
            "And so we can define that we can say, OK, a similar function.",
            "Ideally we wanted the previous thing with the previous one.",
            "At least.",
            "Hopefully there's a reason set of reasonable points such that most examples are on average more similar to the reasonable points of their label then to the regional points of the other label by some gap gamma.",
            "Now.",
            "OK, something about hinge loss.",
            "So claim is that this is actually legitimate way to think about good kernels.",
            "If a kernel has margin, gamma is implicit space then it will also satisfy this condition.",
            "You have a little bit of loss that gap gamma.",
            "Get square.",
            "That's a bad thing.",
            "'cause gamma is less than one and you're squaring it and so the margin you had before it gets to be smaller.",
            "But by quadratic and then there's this sort of little bit of some examples don't satisfy this.",
            "That's this first parameter here.",
            "And then there's a size of the reasonable set you would like to be large, but that's that will get.",
            "There's a little bit of a trade off.",
            "But the big main thing is the gamma get squared."
        ],
        [
            "The other thing is that you can learn given one of these, even if you don't know what the reasonable points are in advance.",
            "OK, and you can learn with a certain number, not too many labeled examples.",
            "Your one over gamma squared term comes in the usual thing."
        ],
        [
            "And this describe what an algorithm is to learn here.",
            "'cause is where the data dependent concept spaces come in is the natural algorithm.",
            "Here is one that people use in learning, but this gives gives, I think, a nice justification 'cause I think.",
            "This algorithm is pick a bunch of random examples.",
            "Think of them as landmarks.",
            "And now they could be unlabeled, view them as landmarks and now use it to explicitly map new data into an explicit new feature space were given example, you just say how similar am I to each landmark, so how similar my landmark one this feature one?",
            "How similar my landmark too is feature to have some of my landmark in this feature and so it's like I guess in the like.",
            "In the language of manual, so these are measurements were measuring, but that's not to say it has to be dot product is just how similar you are at each one and then explicitly in that space we can now do our learning.",
            "So we've now done explicit mapping and the point is that this definition, here this means that with high probability in this space there is a separator of Lowell, one margin.",
            "In particular there's a set even if we don't know what ours advanced.",
            "There exists in this space and I separator.",
            "There's one where the weights are zero for the points that are not one of these.",
            "In this reasonable set an weights one over the number of reasonable points, the number of reasonable landmarks.",
            "For the ones that are reasonable.",
            "And this times plus one or minus one depending on their label.",
            "OK, so so we do enough landmarks that with high probability Tao is the probability mass of the reasonable set.",
            "This is enough so that with high probability somewhere in here there's one over gamma squared reasonable points.",
            "And if you think about this weight vector zero for the unreasonable ones.",
            "One over the number of reasonable for the reason ones times, times the label.",
            "This is exactly estimating this expectation, and so we have in this space with high probability.",
            "There's a nice separator, and we can.",
            "Now we just use our favorite algorithm for learning linear separators.",
            "They have good L1 margins.",
            "And so the point here is that we have a nice, day dependent concept space where we're using unlabeled data to even define, you know.",
            "What are spaces?"
        ],
        [
            "OK, so let me let me stop here.",
            "So so we're using landmarks that define an explicit data dependent concept space we're considering with separators of large one margin, that space, which again this dependence is actually two different uses of data dependent concept spaces.",
            "OK, so let me let me stop here."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so welcome everyone to do this.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Workshop so the theme of this workshop is data dependent concept spaces.",
                    "label": 1
                },
                {
                    "sent": "And So what we mean by data dependent concept spaces.",
                    "label": 0
                },
                {
                    "sent": "So using the data that determine the set of functions that you're viewing is most natural for your problem.",
                    "label": 1
                },
                {
                    "sent": "So classic example is like the notion of a large margin separator, where Apri Ori, before seeing any data, is not like one separators anymore large margin than another.",
                    "label": 0
                },
                {
                    "sent": "But this, but you're using the data.",
                    "label": 0
                },
                {
                    "sent": "Take this sort of pre bias or this belief that the world ought to be a large margin separated that plus the data helps you instantiate that.",
                    "label": 0
                },
                {
                    "sent": "Into a more concrete bias that then gives you a way of of ordering or preference ordering over over functions.",
                    "label": 1
                },
                {
                    "sent": "And often this happens.",
                    "label": 0
                },
                {
                    "sent": "Much of it is even without considering the labels of the data.",
                    "label": 0
                },
                {
                    "sent": "How you get your preference ordering here, so I want.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this talk, is talk about two nice settings where this is especially helpful, and one of them is semi supervised learning and another is learning from similarity function.",
                    "label": 1
                },
                {
                    "sent": "So kernels but also more general other pairwise similarity functions.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me start with Semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "So the idea and semi supervised learning is well, can we use unlabeled data to help us augment a small labeled sample to improve learning?",
                    "label": 1
                },
                {
                    "sent": "And, well, you know unlabeled data is missing the most important information.",
                    "label": 0
                },
                {
                    "sent": "So it's not so clear what it can do for us.",
                    "label": 0
                },
                {
                    "sent": "But the hope is, well, maybe you can likely unless last slide.",
                    "label": 0
                },
                {
                    "sent": "Maybe you can help us instantiate our biases, help us in our searching over functions.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I want to do it so this talks with these two parts in the first half I'll start by just kind of briefly serving a few examples of some very different semi supervised learning algorithms that have different kinds of biases, and then describe a way of thinking about these in terms of data dependent distribution dependent concept spaces.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with Nina Balcan, and the idea is that we can use this to help us understand questions like, well, ran an unlabeled data help us.",
                    "label": 1
                },
                {
                    "sent": "And and try to help make sense of what's going on in these different sorts of algorithms.",
                    "label": 1
                },
                {
                    "sent": "Height.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good so I want to give 3 examples of some semi supervised learning algorithms, so Co training is 1 so Co training.",
                    "label": 0
                },
                {
                    "sent": "The idea is that while many problems have two different sources of information that you can use to determine the label, so we can think of examples, a pair of two different types of information and.",
                    "label": 1
                },
                {
                    "sent": "One nice example.",
                    "label": 0
                },
                {
                    "sent": "This is classifying web pages.",
                    "label": 0
                },
                {
                    "sent": "You really have two different kinds of information.",
                    "label": 1
                },
                {
                    "sent": "You can use words on the page itself.",
                    "label": 1
                },
                {
                    "sent": "This is my page words on the page itself.",
                    "label": 0
                },
                {
                    "sent": "Or you could have words on links playing to the page.",
                    "label": 0
                },
                {
                    "sent": "And and both both are different sources of information.",
                    "label": 0
                },
                {
                    "sent": "They tell you about what's going on there, what.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The page that is.",
                    "label": 0
                },
                {
                    "sent": "And the idea, an iterative code training is where we could maybe use a small labeled sample to learn some initial rules.",
                    "label": 1
                },
                {
                    "sent": "So for instance you see the word my advisor pointing to the page.",
                    "label": 0
                },
                {
                    "sent": "It's good chance it's a faculty member, home page.",
                    "label": 1
                },
                {
                    "sent": "Or if you see on the page that talks about I am teaching blah blah blah, that's another indicator.",
                    "label": 0
                },
                {
                    "sent": "Maybe that's a faculty members home page.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then, given that initial information, so these are these.",
                    "label": 0
                },
                {
                    "sent": "These two little algorithms and have learned a little bit.",
                    "label": 0
                },
                {
                    "sent": "You could look for unlabeled data where one of them is confident.",
                    "label": 1
                },
                {
                    "sent": "Maybe on this example here there's a link that says my advisor pointed some page and they can take the other half and send it over and say, well, I think this is probably a faculty member homepage to alter the other algorithm and and.",
                    "label": 0
                },
                {
                    "sent": "Help that one learn.",
                    "label": 0
                },
                {
                    "sent": "And they can each help each other, and so we could train 2 classifiers.",
                    "label": 1
                },
                {
                    "sent": "Why any type of information and try to use each to help the other.",
                    "label": 0
                },
                {
                    "sent": "So that's the idea of it.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Co training and just to give a simple example, there's a simple theoretical example of this.",
                    "label": 1
                },
                {
                    "sent": "I think helps illustrate what's going on.",
                    "label": 0
                },
                {
                    "sent": "Imagine that, so one of the simplest case of learning is we're trying to learn intervals so we have data on a line and target function.",
                    "label": 0
                },
                {
                    "sent": "Some interval positives inside and negative outside, and if you have two views, it's natural to imagine that given interval on view number one over here and interval interval on view #2 which were plotting up here, and so the positive should be inside both intervals, negative should be outside of both intervals, and so in that case if we're just giving a little bit of labeled data, like say, this one.",
                    "label": 0
                },
                {
                    "sent": "Positive and that one is positive and all the rest are unlabeled.",
                    "label": 0
                },
                {
                    "sent": "Well, in this case, view number two could say, well, hey if I'm interval and this is positive.",
                    "label": 0
                },
                {
                    "sent": "MRSA positive this guy who's in between all the positive also and label that is positive.",
                    "label": 0
                },
                {
                    "sent": "Send it down to this algorithm.",
                    "label": 0
                },
                {
                    "sent": "This algorithm can.",
                    "label": 0
                },
                {
                    "sent": "So that's positive this is positive.",
                    "label": 0
                },
                {
                    "sent": "This is positive and so this guy will be positive.",
                    "label": 0
                },
                {
                    "sent": "Also send that label to the other guy and so forth.",
                    "label": 0
                },
                {
                    "sent": "In this way they can help out each other.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's code train.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's iterative code training and another you can view this as kind of an iterative approximation to what you might imagine directly optimizing a joint objective where you've got some labeled data.",
                    "label": 0
                },
                {
                    "sent": "You've got some unlabeled data and you want to optimize on the one hand accuracy on your label data.",
                    "label": 1
                },
                {
                    "sent": "An agreement between the two halves on your unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then this example of some problems where you can do this within different kinds of preprocessing.",
                    "label": 0
                },
                {
                    "sent": "For visual images, for instance to get your two different views.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so another example is transductive support vector machines or semi supervised support vector machines and their.",
                    "label": 0
                },
                {
                    "sent": "Well suppose we believe the target separator goes through low density regions of the space is a different kind of belief we might have.",
                    "label": 1
                },
                {
                    "sent": "We believe it should have a large margin, then it's natural to look for a separator has a large margin with respect to both the labeled and the unlabeled data, so accurate unlabeled data, large margin over the whole thing.",
                    "label": 0
                },
                {
                    "sent": "OK, and then the idea here is that well, if you only had labeled data, you might not have enough to really help understand what your margin really is.",
                    "label": 0
                },
                {
                    "sent": "So if you're labeled data, maybe this looks like the largest margin separator, but given your unlabeled data, you realize that this is a better margin separator, and so you can use your your unlabeled data to help you instantiate this bias and looking for a large margin separated over your distribution.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Unfortunately, optimization problems now harder, so we just had labeled data.",
                    "label": 1
                },
                {
                    "sent": "Finding a large margin separates nice and convex unlabeled data.",
                    "label": 1
                },
                {
                    "sent": "Not so nice, so it's more painful to do and various different sort of local optimization branch and bound other sort of algorithms people have tried for actually doing it.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then one.",
                    "label": 0
                },
                {
                    "sent": "One more example, kind of different from both of these are graph based methods.",
                    "label": 0
                },
                {
                    "sent": "Suppose we believe that very similar examples.",
                    "label": 1
                },
                {
                    "sent": "They probably have the same label, so that's a natural belief that kind of motivates the nearest neighbor kind of algorithm.",
                    "label": 1
                },
                {
                    "sent": "If you have a lot of labeled data, so enough labeled data so that you know you expect there to be given some new test point.",
                    "label": 0
                },
                {
                    "sent": "Actually something close to it.",
                    "label": 0
                },
                {
                    "sent": "You don't have enough labeled data, maybe there's nothing close to it that you have labeled.",
                    "label": 0
                },
                {
                    "sent": "If you have unlabeled data, maybe you can use them as stepping stones.",
                    "label": 1
                },
                {
                    "sent": "Between labeled points, so it may be that example from Jerry Zhu.",
                    "label": 0
                },
                {
                    "sent": "So you might have some points are not very similar, but there's a sequence of unlabeled points in between them that can help tell you that they are actually similar to each other in some way.",
                    "label": 0
                },
                {
                    "sent": "So so if I believe that similar examples ought to have the same label, unlabeled data can help me, because I could say, well know what either of these labels are.",
                    "label": 0
                },
                {
                    "sent": "But they're probably the same and use that to start connecting between exam.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Apples.",
                    "label": 0
                },
                {
                    "sent": "So you could construct a graph with edges between very similar examples and then try to use for the partition.",
                    "label": 1
                },
                {
                    "sent": "This graph in some way.",
                    "label": 0
                },
                {
                    "sent": "OK, good.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's another example from some.",
                    "label": 0
                },
                {
                    "sent": "My Co author, Nina some visual image data.",
                    "label": 0
                },
                {
                    "sent": "We're looking at visual images and trying to figure out who's in the image and the idea is that unlabeled data can help because we may be hard to tell that you know, one image looks directly like another, but you have a sequence and say oh this is similar to this in one way, maybe similar to that.",
                    "label": 1
                },
                {
                    "sent": "Another way you can help you make that inference.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then you could various objectives.",
                    "label": 0
                },
                {
                    "sent": "You can then solve for given this graph.",
                    "label": 0
                },
                {
                    "sent": "OK, so those are.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some examples and then.",
                    "label": 0
                },
                {
                    "sent": "Stepping back with this kind of a unifying principle, whether these have in common, they seem to have very different biases.",
                    "label": 1
                },
                {
                    "sent": "One that, well, I think they all have a large margin separation.",
                    "label": 0
                },
                {
                    "sent": "Another this thing is about 2 views.",
                    "label": 0
                },
                {
                    "sent": "What have in common and how can we think about this theoretic?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "A nice model for so one way of thinking about this is we can say well what's happening here is unlabeled data is helping us if we have beliefs, not just about the form the target has like we think the target is when your separator or small decision tree, but about how it relates to the distribution.",
                    "label": 1
                },
                {
                    "sent": "So we have some beliefs about how the target relates the underlying distribution and if we're.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That situation, then we can kind of model that theoretically like this.",
                    "label": 0
                },
                {
                    "sent": "So in the standard pack model, we think of our biases, a set of functions like linear separators or maybe a scoring and ordering or scoring over over functions like with decision trees, while every Boolean functions decision tree.",
                    "label": 0
                },
                {
                    "sent": "But you really mean small decision trees.",
                    "label": 0
                },
                {
                    "sent": "Somehow you're more preferable, large decision trees less so.",
                    "label": 0
                },
                {
                    "sent": "So those concept class well.",
                    "label": 1
                },
                {
                    "sent": "So what we can do is we can replace that sort of class of functions were scoring over functions.",
                    "label": 0
                },
                {
                    "sent": "We can think of is we can have a scoring over function distribution pairs.",
                    "label": 0
                },
                {
                    "sent": "OK, so a class of function distribution pairs.",
                    "label": 0
                },
                {
                    "sent": "And if we have that then.",
                    "label": 0
                },
                {
                    "sent": "So we think of this as well.",
                    "label": 0
                },
                {
                    "sent": "How compatible is some function with some underlying distribution that these two make sense together?",
                    "label": 0
                },
                {
                    "sent": "If we have something like that, then given a distribution given information about distribution from unlabeled data, we can view this scoring over function distribution pairs.",
                    "label": 0
                },
                {
                    "sent": "If you like as a mapping from distributions to scoring over functions.",
                    "label": 0
                },
                {
                    "sent": "So if we have a notion of how natural a particular function distribution, how well they go together and given information about the distribution that gives us.",
                    "label": 0
                },
                {
                    "sent": "Which gives us a way of ordering our functions, and so it's a distribution dependent concept class now.",
                    "label": 0
                },
                {
                    "sent": "Now we want something a little bit more.",
                    "label": 0
                },
                {
                    "sent": "We actually want that this is something that we can estimate from a finite sample.",
                    "label": 1
                },
                {
                    "sent": "So if we have a notion of how well a function distribution go together, but we can't actually estimate that from a finite sample of data, it's not going to be very useful for us, as we don't usually don't have the distribution kind of in our hands, so we want some notion of how how well they go together that we can estimate from a finite sample, and that can let us estimate the true score of some function approximately from sample.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the idea of.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of of.",
                    "label": 0
                },
                {
                    "sent": "This model, so the idea here is we can augment the notion of a concept class with a compatibility score.",
                    "label": 1
                },
                {
                    "sent": "Chi how compatible with some hypothesis some distribution.",
                    "label": 0
                },
                {
                    "sent": "So instead of learning a function, we would learn learning a class.",
                    "label": 0
                },
                {
                    "sent": "We would learn this class with respect to some notion of compatibility.",
                    "label": 1
                },
                {
                    "sent": "Expresses our beliefs or hopes about the world, for instance, at the.",
                    "label": 1
                },
                {
                    "sent": "Target ought to be a large margin separator and then just like we're saying, we could then given are a big class of functions like linear separators and given this of abstract prior belief, we take unlabeled data and instantiate it.",
                    "label": 1
                },
                {
                    "sent": "To say large margins with respect to the actual distribution that we have.",
                    "label": 0
                },
                {
                    "sent": "And then we want this to be estimated from a final.",
                    "label": 1
                },
                {
                    "sent": "You can estimate from a finite sample.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so one way of formally instantiating this is, so we don't need to be so strict.",
                    "label": 1
                },
                {
                    "sent": "But but when it turns out to be nice is it supposes we can require compatibility of a function distribution to be an expectation over individual examples, and actually many of our notions of compatibility, like the ones in those examples.",
                    "label": 0
                },
                {
                    "sent": "You can view this way either expectation over single examples are over pairs of examples, let's say over single examples.",
                    "label": 0
                },
                {
                    "sent": "If you do this, actually something very nice happens.",
                    "label": 1
                },
                {
                    "sent": "It means you can view incompatibility how unnatural some function is as a kind of unlabelled.",
                    "label": 0
                },
                {
                    "sent": "Error rate and you can view incompatibility on.",
                    "label": 0
                },
                {
                    "sent": "You can view it as an unlabeled loss function over over example.",
                    "label": 0
                },
                {
                    "sent": "So you can say given a hypothesis you have an unlabeled loss function over individual examples and the notion of unlabeled error rate, which is well, what is the expected value of this quality?",
                    "label": 0
                },
                {
                    "sent": "What is your?",
                    "label": 0
                },
                {
                    "sent": "Expected unlabeled loss.",
                    "label": 0
                },
                {
                    "sent": "So for instance code training we can think of it this way so we can think of an unlabeled, unlabeled error for a given hypothesis.",
                    "label": 0
                },
                {
                    "sent": "An example is did it disagree on the two house?",
                    "label": 0
                },
                {
                    "sent": "So given some hypothesis that, did you say something different in 1/2 and the and the other, and your unlabeled error rate is, then this expectation is the probability mass on examples where they disagree and so.",
                    "label": 0
                },
                {
                    "sent": "So we can really think of that as a.",
                    "label": 0
                },
                {
                    "sent": "Is a kind of an unlabeled there.",
                    "label": 0
                },
                {
                    "sent": "Of course we have labeled with the true error, which is, well, you know what's your error, but give us an unlabeled error rate.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and then the compatibility is 1 minus the only.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There or in semi supervised support vector machines.",
                    "label": 0
                },
                {
                    "sent": "We can think of our unlabeled error rate is the probability mass near to the separator or perhaps some function?",
                    "label": 1
                },
                {
                    "sent": "Of how close you are to the separator.",
                    "label": 0
                },
                {
                    "sent": "OK, so we can in these examples.",
                    "label": 0
                },
                {
                    "sent": "In many cases you can take the priors that you have and view them as a notion of unlabeled error rate.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if you do this, actually something kind of interesting happens because the high level question here is.",
                    "label": 0
                },
                {
                    "sent": "Well, when is unlabeled data going to help?",
                    "label": 0
                },
                {
                    "sent": "And by how much?",
                    "label": 0
                },
                {
                    "sent": "And what do you need?",
                    "label": 0
                },
                {
                    "sent": "So here's the simplest kind of theorem you can prove in this setting, but I think it helps illustrate.",
                    "label": 0
                },
                {
                    "sent": "Some of the nice principles here, so imagine that we believe our target really is compatible with the distribution.",
                    "label": 0
                },
                {
                    "sent": "It really does go along with our belief.",
                    "label": 0
                },
                {
                    "sent": "So then there's a natural distribution dependent concept class, which is let's look at all the concepts whose unlabeled errors at most some epsilon, the ones that are highly compatible or a little a little slightly incompatible.",
                    "label": 0
                },
                {
                    "sent": "But.",
                    "label": 0
                },
                {
                    "sent": "Mostly so, for instance for Co training hypotheses, I have at most epsilon disagreement under our distribution on the two house, so in that case we can get it use our usual sort of the simple kind of bounds that you get for a finite classes.",
                    "label": 0
                },
                {
                    "sent": "What happens is your unlabeled sample that the usual bound you get.",
                    "label": 0
                },
                {
                    "sent": "How much labeled data becomes a bound on unlabeled data?",
                    "label": 0
                },
                {
                    "sent": "And so if we see at least this much unlabeled data, that's enough to estimate how.",
                    "label": 0
                },
                {
                    "sent": "Compatible, our functions are and then.",
                    "label": 0
                },
                {
                    "sent": "The amount of labeled data we need is just a function of the size of this class.",
                    "label": 1
                },
                {
                    "sent": "And then we can say well with high probability, all our functions that have are consistent with our labeled sample and compatible with our unlabeled sample with high probability.",
                    "label": 0
                },
                {
                    "sent": "They actually have low to error.",
                    "label": 0
                },
                {
                    "sent": "What I think I mean.",
                    "label": 0
                },
                {
                    "sent": "So what I think is kind of nice conceptually, here is what's happening is there's this is saying is this really two things we want and need to be true in order for unlabeled data help us?",
                    "label": 0
                },
                {
                    "sent": "One is what we're hoping here that really our beliefs are correct.",
                    "label": 0
                },
                {
                    "sent": "Our target really is compatible, but the other is that there's a notion of distributions being helpful and not helpful.",
                    "label": 1
                },
                {
                    "sent": "So helpful distribution, because two things we want.",
                    "label": 0
                },
                {
                    "sent": "One is that we want.",
                    "label": 0
                },
                {
                    "sent": "Our target to belong to this class and the other is that we want not too much.",
                    "label": 0
                },
                {
                    "sent": "Other stuff belongs to this class also, so even it's not enough just for our beliefs to be right.",
                    "label": 0
                },
                {
                    "sent": "We also need the distribution to help us out in helping US knockout other things.",
                    "label": 0
                },
                {
                    "sent": "Otherwise we didn't actually get anywhere from here to here.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so for example in Co training if you say well this stuff with two views are too complicated.",
                    "label": 0
                },
                {
                    "sent": "I'll just replicate the first few over here.",
                    "label": 0
                },
                {
                    "sent": "You know the target can be compatible with everything else, and so you didn't really get anywhere.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then you can also extend this to the case where the target is not fully compatible, and then what you care about, or this other functions that are only a little bit less compatible with your beliefs than the target was.",
                    "label": 0
                },
                {
                    "sent": "Another so for instance, another case of this would be if you have a margin belief and the distribution is uniform, well then all the functions are just as compatible as a target, so it's not helping if you have a nice distribution and then OK.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You could also have an agnostic version of theorems like this, and then you end up with something that looks.",
                    "label": 1
                },
                {
                    "sent": "A little mess here, but the idea then, if you're not so, there's two things to be agnostic about.",
                    "label": 1
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "One is, I don't know how exactly compatible my target function is with my beliefs, and also I don't know.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "If there's a 0 error function in my class, or maybe to get to 0, I have to go to some really high complexity level.",
                    "label": 0
                },
                {
                    "sent": "So then the natural thing you can do is try to minimize your empirical label there, plus some penalty function that has to do with your unlabeled there and then you can prove.",
                    "label": 0
                },
                {
                    "sent": "Natural type of thing, which is that the error you find will be nearly as good as the best function.",
                    "label": 0
                },
                {
                    "sent": "Where best is some combination of the error it has on the labeled data and its penalty.",
                    "label": 0
                },
                {
                    "sent": "So if there's a you know 20% error rate function is very highly compatible in the 10% error rate functions a little bit less so and so forth.",
                    "label": 0
                },
                {
                    "sent": "And you want to.",
                    "label": 0
                },
                {
                    "sent": "You can balance those off and then and then you get some penalty of some usual thing depending on the capacity of the class of functions whose compatibility is not too much worse than the hypothesis here.",
                    "label": 0
                },
                {
                    "sent": "In the equation learning the.",
                    "label": 0
                },
                {
                    "sent": "Head size of knowledge of their compatible sets.",
                    "label": 0
                },
                {
                    "sent": "Where does that?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so that's.",
                    "label": 0
                },
                {
                    "sent": "So this is a little this is the small term.",
                    "label": 0
                },
                {
                    "sent": "This is the big term.",
                    "label": 0
                },
                {
                    "sent": "This is just an extra little square root.",
                    "label": 0
                },
                {
                    "sent": "This is this is this is kind of like a dependent structural risk minimization.",
                    "label": 0
                },
                {
                    "sent": "This is the little one for going over the different teas.",
                    "label": 0
                },
                {
                    "sent": "The Big one is here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, it's in the penalty.",
                    "label": 0
                },
                {
                    "sent": "I just yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "So the big one is here.",
                    "label": 0
                },
                {
                    "sent": "And this is the empirical kind of size.",
                    "label": 0
                },
                {
                    "sent": "And then you want to say the empirical size will be close to the true size.",
                    "label": 0
                },
                {
                    "sent": "As long as your unlabeled data set is large enough.",
                    "label": 0
                },
                {
                    "sent": "So you want your only, so ideally you want, so I'm imagining here that your beliefs are respect to the true distribution, and so you want enough unlabeled data so that what you actually estimate on your sample.",
                    "label": 0
                },
                {
                    "sent": "It's the class so that the function over your sample look compatible to certain level, and then you want that class to be close to the same for the distribution.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "One thing that comes up here is when you say how much unlabeled data do you need.",
                    "label": 0
                },
                {
                    "sent": "Then there's actually two things that come in.",
                    "label": 0
                },
                {
                    "sent": "One is how complicated my class of functions and the other is.",
                    "label": 0
                },
                {
                    "sent": "How complicated is my notion of compatibility so I could have a very simple class of functions like.",
                    "label": 0
                },
                {
                    "sent": "Intervals on the line, but if I have a really weird notion of what it means to be compatible with distribution that some bizarre thing, then I need a lot more unlabeled data too.",
                    "label": 0
                },
                {
                    "sent": "Estimate this.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so like a typical example is Eva.",
                    "label": 0
                },
                {
                    "sent": "Target is a large write the separator and ice distribution would be say uniform on some number of blobs.",
                    "label": 1
                },
                {
                    "sent": "If you have enough unlabeled data, that would be enough to knock out the functions except for ones.",
                    "label": 0
                },
                {
                    "sent": "Like like.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "One interesting feature of this.",
                    "label": 0
                },
                {
                    "sent": "One interesting feature of the setting is often this two kinds of bounds.",
                    "label": 0
                },
                {
                    "sent": "People often give uniform convergence bounds, those previous ones, or epsilon cover bounds.",
                    "label": 1
                },
                {
                    "sent": "Epsilon cover bounds require more of your algorithm.",
                    "label": 0
                },
                {
                    "sent": "They require your algorithm to take your set of functions, pick in advance some cover, then take label data and use it to select from those as opposed to uniform convergence bounds would say no, just go ahead and find me something.",
                    "label": 0
                },
                {
                    "sent": "OK, I don't care what that's.",
                    "label": 0
                },
                {
                    "sent": "Consistent and compatible, and one thing you get here is a lot of natural examples.",
                    "label": 0
                },
                {
                    "sent": "There's actually a big gap between these two kinds of bounds, and I just want to end this part of the talk with an example of this.",
                    "label": 0
                },
                {
                    "sent": "So here's a nice example.",
                    "label": 0
                },
                {
                    "sent": "It's like a simple version of code training, so imagine that examples are pairs of points in 01 to the end.",
                    "label": 1
                },
                {
                    "sent": "OK, so normally Coltrane this 2 views.",
                    "label": 0
                },
                {
                    "sent": "I'm thinking of the two views is really being in the same space.",
                    "label": 0
                },
                {
                    "sent": "So an example.",
                    "label": 0
                },
                {
                    "sent": "Here's an example.",
                    "label": 0
                },
                {
                    "sent": "It's a pair of points.",
                    "label": 0
                },
                {
                    "sent": "They're both living in the same space.",
                    "label": 0
                },
                {
                    "sent": "The target is a single linear separator, so it's just think of it.",
                    "label": 0
                },
                {
                    "sent": "We've locked the two views together.",
                    "label": 0
                },
                {
                    "sent": "OK, So what does the world look like?",
                    "label": 0
                },
                {
                    "sent": "I'm looking at linear separators, unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "Are these little like line segments?",
                    "label": 0
                },
                {
                    "sent": "OK, so examples of pair of points like that and what am I looking for?",
                    "label": 0
                },
                {
                    "sent": "I'm looking for linear separator that doesn't slice through these line segments.",
                    "label": 0
                },
                {
                    "sent": "OK, 'cause they're both part of the same example and agrees with the labeled data.",
                    "label": 0
                },
                {
                    "sent": "In general, it's an NP hard problem.",
                    "label": 0
                },
                {
                    "sent": "So that's annoying.",
                    "label": 0
                },
                {
                    "sent": "But here I'm just worried about the sample issue and so can example here is compatible with both parts, or positive or both parts negative, so they're both on the same side.",
                    "label": 1
                },
                {
                    "sent": "OK, so let's think about this case, so it's kind of an interesting case to think about.",
                    "label": 0
                },
                {
                    "sent": "Suppose that the distribution is independent given the label so meaning that these two points are equal to random points subject to be on this side.",
                    "label": 0
                },
                {
                    "sent": "And here it's 2 random points subject to being on that side.",
                    "label": 0
                },
                {
                    "sent": "Friends maybe is the uniform distribution conditioned on?",
                    "label": 0
                },
                {
                    "sent": "On each side, some balls say so.",
                    "label": 0
                },
                {
                    "sent": "One thing you can say is that if you have.",
                    "label": 0
                },
                {
                    "sent": "A polynomial and end number of dimensions, unlabeled examples and little O of log in labeled examples.",
                    "label": 0
                },
                {
                    "sent": "Not that much labeled, but still then.",
                    "label": 1
                },
                {
                    "sent": "You're not going to get yet.",
                    "label": 1
                },
                {
                    "sent": "Get uniform convergence.",
                    "label": 0
                },
                {
                    "sent": "There will still be many bad compatible.",
                    "label": 0
                },
                {
                    "sent": "Meaning not slicing through these guys.",
                    "label": 0
                },
                {
                    "sent": "Consistent meaning getting the label data correct functions.",
                    "label": 0
                },
                {
                    "sent": "Why is that?",
                    "label": 0
                },
                {
                    "sent": "Because it's so.",
                    "label": 0
                },
                {
                    "sent": "Think about the Boolean cube.",
                    "label": 0
                },
                {
                    "sent": "If you have not too many labeled points, you can have a hypothesis that basically slices those guys off from almost everything else.",
                    "label": 0
                },
                {
                    "sent": "OK, you can.",
                    "label": 0
                },
                {
                    "sent": "You give me a small number of positives and negatives.",
                    "label": 0
                },
                {
                    "sent": "I can just say take the positive.",
                    "label": 0
                },
                {
                    "sent": "Basically just slice those off and the negative and everything else in the world, all the unlabeled data to go on the other side, and it's perfectly compatible, but it's.",
                    "label": 0
                },
                {
                    "sent": "If you think about what is that doing it just taking those few positives and slicing for everything else is probably not going to be very good.",
                    "label": 0
                },
                {
                    "sent": "OK, especially if I promise you in advance the target 5050.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if you just look at the unlabeled data, the unlabeled data would be enough to a polynomial number, unlabeled examples, enough to have when you look at you realize, even though there are many.",
                    "label": 0
                },
                {
                    "sent": "Compatible functions we haven't labeled data yet.",
                    "label": 0
                },
                {
                    "sent": "There's many compatible functions.",
                    "label": 0
                },
                {
                    "sent": "There's only four kinds of compatible functions.",
                    "label": 0
                },
                {
                    "sent": "All the ones that are compatible either look like the target or pretty close the opposite.",
                    "label": 0
                },
                {
                    "sent": "The target labeling.",
                    "label": 0
                },
                {
                    "sent": "Almost everything is positive.",
                    "label": 0
                },
                {
                    "sent": "Labeling almost everything is negative.",
                    "label": 0
                },
                {
                    "sent": "They all look like that.",
                    "label": 0
                },
                {
                    "sent": "So first you pick one.",
                    "label": 0
                },
                {
                    "sent": "One of each type.",
                    "label": 0
                },
                {
                    "sent": "You don't know which is which, but I guess you can tell these two.",
                    "label": 0
                },
                {
                    "sent": "But and then you look at your label data.",
                    "label": 0
                },
                {
                    "sent": "Then you can say, OK, you know which one of those is really only four kinds of functions, even though you don't get uniform convergence yet.",
                    "label": 0
                },
                {
                    "sent": "So so so in this setting often these epsilon cover bounds are.",
                    "label": 0
                },
                {
                    "sent": "Helpful.",
                    "label": 0
                },
                {
                    "sent": "In general, algorithmically there are more painful, although in this particular case, with independence, given the label, you can actually implement this efficiently.",
                    "label": 0
                },
                {
                    "sent": "So I think it's a nice feature, OK?",
                    "label": 0
                },
                {
                    "sent": "Well, so suppose I so here.",
                    "label": 0
                },
                {
                    "sent": "The claim is if I have a lot of unlabeled data, then all the compatible functions will be close to one of these.",
                    "label": 0
                },
                {
                    "sent": "So if I first pick a cover, I just pick one from this type one from this type.",
                    "label": 0
                },
                {
                    "sent": "One of this type, one of this type, and then I look at my labeled data and then that's enough to throw out 'cause I only have one.",
                    "label": 0
                },
                {
                    "sent": "I see I've taken this whole class of functions, they're the ones that label almost everything negative and just taking one.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Of those, and then they could use label data to pick from those.",
                    "label": 0
                },
                {
                    "sent": "I mean so.",
                    "label": 0
                },
                {
                    "sent": "Well, no, so I mean, if long as this is.",
                    "label": 0
                },
                {
                    "sent": "So what we have from this, if we believe our target is compatible, then.",
                    "label": 0
                },
                {
                    "sent": "At.",
                    "label": 0
                },
                {
                    "sent": "Then we know it's gotta be one of these four types.",
                    "label": 0
                },
                {
                    "sent": "So in this case, fully complete right?",
                    "label": 0
                },
                {
                    "sent": "So it's true that because this is this is saying you can use less labeled data than even the amount needed to verify, because it's like one of these four.",
                    "label": 0
                },
                {
                    "sent": "And if you think about it, actually, if if the target is roughly 5050 you can immediately throw out these two because.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, if the target, once you see this, if you realize you pick an instance that we pick one of these one representative.",
                    "label": 0
                },
                {
                    "sent": "These one of these.",
                    "label": 0
                },
                {
                    "sent": "One of these.",
                    "label": 0
                },
                {
                    "sent": "If these guys are if one of your four functions roughly 5050, you know that's gotta be one of these two and not one of these days.",
                    "label": 0
                },
                {
                    "sent": "You throw these out, you end up with two representatives.",
                    "label": 0
                },
                {
                    "sent": "One of them is almost the target.",
                    "label": 0
                },
                {
                    "sent": "One of 'em is almost the opposite of target.",
                    "label": 0
                },
                {
                    "sent": "It only takes one labeled example to figure out whether it's this one or this one.",
                    "label": 0
                },
                {
                    "sent": "Only money.",
                    "label": 0
                },
                {
                    "sent": "Oh well, this means that.",
                    "label": 0
                },
                {
                    "sent": "So this is a very strong assumption.",
                    "label": 0
                },
                {
                    "sent": "This independence, given the label, means that there's only one right?",
                    "label": 0
                },
                {
                    "sent": "So this is very, very strong, right?",
                    "label": 0
                },
                {
                    "sent": "So I'm not.",
                    "label": 0
                },
                {
                    "sent": "This is very extremely sorry.",
                    "label": 0
                },
                {
                    "sent": "I'm just so the point of this slide is just to point out that this can give you better numbers than this.",
                    "label": 0
                },
                {
                    "sent": "But but I agree, this is a strong.",
                    "label": 0
                },
                {
                    "sent": "Please yes.",
                    "label": 0
                },
                {
                    "sent": "OK, alright so now let me.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "02 OK, so the conclusion, so data dependent concept spaces can give a nice way of thinking about semi supervised learning.",
                    "label": 1
                },
                {
                    "sent": "We have beliefs about how the target should relate to distribution.",
                    "label": 1
                },
                {
                    "sent": "We can view this notion of unlabeled error rate.",
                    "label": 1
                },
                {
                    "sent": "We can use data dependent structural risk minimization.",
                    "label": 0
                },
                {
                    "sent": "The tradeoff, unlabeled error rate, and labeled error rate if we like.",
                    "label": 0
                },
                {
                    "sent": "And we're using unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "Help estimate our bias.",
                    "label": 0
                },
                {
                    "sent": "Unlabeled data too.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To pick.",
                    "label": 0
                },
                {
                    "sent": "We want to get back onto the onto the public schedule.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, OK, so I'll do this part.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So kernels are, so I want to talk about learning with kernels and more general similarity functions.",
                    "label": 0
                },
                {
                    "sent": "And some nice ways of data dependent concept spaces coming here.",
                    "label": 0
                },
                {
                    "sent": "So let's be the five minute version.",
                    "label": 0
                },
                {
                    "sent": "OK, so a kernel is a legal definition of dot product, so so it's a function where you take your two objects you take, you run your code and outputs a number that corresponds some implicit dot product.",
                    "label": 1
                },
                {
                    "sent": "And they're very nice.",
                    "label": 0
                },
                {
                    "sent": "'cause many learning algorithms be written so they only interact with the data by dot product.",
                    "label": 1
                },
                {
                    "sent": "So wherever you want to take that product, you stick in the kernel and it's acting as if they're in this implicit space.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which is great and we have a nice theory that says, well, if data is linearly separable by a large margin, we don't need too much data to generalize well, and so if data several by a large margin, this implicit space, we can have some confidence in how well we're doing, which is excellent.",
                    "label": 0
                },
                {
                    "sent": "Kernels are very useful in practice for many different kinds of.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The one issue I want to get out here is that there's a little bit of a disconnect between the theory and how we think about kernel, so two different ways we think about kernels.",
                    "label": 0
                },
                {
                    "sent": "One is it intuitively we think of a good kernel for a problem is one that makes sense is a notion of similarity.",
                    "label": 1
                },
                {
                    "sent": "So if I say I've got some kind of data I don't know DNA sequences or something come up with a good kernel.",
                    "label": 0
                },
                {
                    "sent": "You're thinking what's a good way of measuring how similar two sequences are, but our theory is talking about margin, implicit space, so you're not thinking about margins and implicit space.",
                    "label": 0
                },
                {
                    "sent": "We were trying to think about how am I going to measure how similar two strings are.",
                    "label": 0
                },
                {
                    "sent": "And it would be nice to sort of bring this little closer together.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thing we've been doing this is joint work with Nina Balcan.",
                    "label": 0
                },
                {
                    "sent": "Naughty Srebro is looking at trying to find a good.",
                    "label": 1
                },
                {
                    "sent": "A good measure of similarity, so it's a quality notion like large margin and implicit space, but other kinds of quality notions that talk in terms of more direct properties and maybe not don't even require this notion of similarity to be illegal kernel.",
                    "label": 1
                },
                {
                    "sent": "Ideally, these notions would include the usual notion of what we mean by a good kernel, so something that has a large margin.",
                    "label": 0
                },
                {
                    "sent": "It's implicit space would also be a good similarity function.",
                    "label": 0
                },
                {
                    "sent": "According to this notion.",
                    "label": 1
                },
                {
                    "sent": "And if something has this notion, we were able to learn, and maybe we can even learn things that you couldn't do with large margin kernels.",
                    "label": 0
                },
                {
                    "sent": "And this notion of data dependent concepts.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This will come come right in so.",
                    "label": 0
                },
                {
                    "sent": "So here, so let me define two properties, one of warm up and then and then the main property.",
                    "label": 0
                },
                {
                    "sent": "So here's a very intuitive property, so we have some learning problem.",
                    "label": 0
                },
                {
                    "sent": "I'm going to think of a distribution on data.",
                    "label": 0
                },
                {
                    "sent": "Examples are labeled by some target function.",
                    "label": 0
                },
                {
                    "sent": "We have a measure of similarity between pairs of objects.",
                    "label": 0
                },
                {
                    "sent": "So X&Y here are two examples.",
                    "label": 0
                },
                {
                    "sent": "1 means they're very similar, minus one means they're very dissimilar.",
                    "label": 0
                },
                {
                    "sent": "A natural property is we would like a great thing to have would be if most examples are on average more similar to other examples of their label.",
                    "label": 0
                },
                {
                    "sent": "Of their type, then to random examples of the other type.",
                    "label": 0
                },
                {
                    "sent": "So most of the positives are more similar.",
                    "label": 0
                },
                {
                    "sent": "On average, the positives random pauses into random negatives and vice versa, so that's just in math.",
                    "label": 0
                },
                {
                    "sent": "Most examples X are on average more similar to random wise or the same label then to random labels examples why the other labeled by some gap gamma.",
                    "label": 1
                },
                {
                    "sent": "If we had that learning would be nice easy.",
                    "label": 0
                },
                {
                    "sent": "The world would be would be great.",
                    "label": 0
                },
                {
                    "sent": "All we have to do to learn.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One is we just take a bunch of data, some positives, some negatives, basically positives to estimate this quantity.",
                    "label": 0
                },
                {
                    "sent": "Here negative steps to make that quality there and then we just classify new example based on are you more similar in average to these examples or more similar?",
                    "label": 1
                },
                {
                    "sent": "Which of those examples average nearest neighbor?",
                    "label": 0
                },
                {
                    "sent": "And this is enough data so that with high probability you haven't you basically.",
                    "label": 0
                },
                {
                    "sent": "Have very good estimates of these expectations.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Unfortunately, this notion of quality is a Pretty Little bit strict.",
                    "label": 0
                },
                {
                    "sent": "It doesn't capture what we mean by a good kernel, and here's a simple example of that.",
                    "label": 0
                },
                {
                    "sent": "If our data looks like negatives here, pauses there.",
                    "label": 0
                },
                {
                    "sent": "In positives there and similarities just dot product, so there's no 55 spaces, just the space here.",
                    "label": 0
                },
                {
                    "sent": "This is a large mark.",
                    "label": 0
                },
                {
                    "sent": "This is a nice kernel.",
                    "label": 0
                },
                {
                    "sent": "There's a large margin separator, but these examples by DOT product are actually more similar.",
                    "label": 1
                },
                {
                    "sent": "2 random negative examples.",
                    "label": 0
                },
                {
                    "sent": "If this is 30 degrees, these have dot product 1/2 then they are the random positive examples 'cause they have dot product one with themselves but negative 1/2 with those that averages only 1/4 and so.",
                    "label": 0
                },
                {
                    "sent": "These are actually more similar to negatives and positives.",
                    "label": 1
                },
                {
                    "sent": "They didn't satisfy the condition we wanted.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one solution to this is we can broaden our notion that we can say the similarity function is good.",
                    "label": 0
                },
                {
                    "sent": "If well, there's a region of reasonable points so that most examples are on average more similar to the reasonable points of their label then to the reasonable points of the other label.",
                    "label": 1
                },
                {
                    "sent": "Well, maybe they even know in advance what those results are.",
                    "label": 0
                },
                {
                    "sent": "We just want them to.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so we can define that we can say, OK, a similar function.",
                    "label": 0
                },
                {
                    "sent": "Ideally we wanted the previous thing with the previous one.",
                    "label": 0
                },
                {
                    "sent": "At least.",
                    "label": 0
                },
                {
                    "sent": "Hopefully there's a reason set of reasonable points such that most examples are on average more similar to the reasonable points of their label then to the regional points of the other label by some gap gamma.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "OK, something about hinge loss.",
                    "label": 0
                },
                {
                    "sent": "So claim is that this is actually legitimate way to think about good kernels.",
                    "label": 1
                },
                {
                    "sent": "If a kernel has margin, gamma is implicit space then it will also satisfy this condition.",
                    "label": 0
                },
                {
                    "sent": "You have a little bit of loss that gap gamma.",
                    "label": 0
                },
                {
                    "sent": "Get square.",
                    "label": 0
                },
                {
                    "sent": "That's a bad thing.",
                    "label": 0
                },
                {
                    "sent": "'cause gamma is less than one and you're squaring it and so the margin you had before it gets to be smaller.",
                    "label": 0
                },
                {
                    "sent": "But by quadratic and then there's this sort of little bit of some examples don't satisfy this.",
                    "label": 0
                },
                {
                    "sent": "That's this first parameter here.",
                    "label": 0
                },
                {
                    "sent": "And then there's a size of the reasonable set you would like to be large, but that's that will get.",
                    "label": 0
                },
                {
                    "sent": "There's a little bit of a trade off.",
                    "label": 0
                },
                {
                    "sent": "But the big main thing is the gamma get squared.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The other thing is that you can learn given one of these, even if you don't know what the reasonable points are in advance.",
                    "label": 1
                },
                {
                    "sent": "OK, and you can learn with a certain number, not too many labeled examples.",
                    "label": 1
                },
                {
                    "sent": "Your one over gamma squared term comes in the usual thing.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this describe what an algorithm is to learn here.",
                    "label": 0
                },
                {
                    "sent": "'cause is where the data dependent concept spaces come in is the natural algorithm.",
                    "label": 0
                },
                {
                    "sent": "Here is one that people use in learning, but this gives gives, I think, a nice justification 'cause I think.",
                    "label": 0
                },
                {
                    "sent": "This algorithm is pick a bunch of random examples.",
                    "label": 0
                },
                {
                    "sent": "Think of them as landmarks.",
                    "label": 0
                },
                {
                    "sent": "And now they could be unlabeled, view them as landmarks and now use it to explicitly map new data into an explicit new feature space were given example, you just say how similar am I to each landmark, so how similar my landmark one this feature one?",
                    "label": 1
                },
                {
                    "sent": "How similar my landmark too is feature to have some of my landmark in this feature and so it's like I guess in the like.",
                    "label": 0
                },
                {
                    "sent": "In the language of manual, so these are measurements were measuring, but that's not to say it has to be dot product is just how similar you are at each one and then explicitly in that space we can now do our learning.",
                    "label": 0
                },
                {
                    "sent": "So we've now done explicit mapping and the point is that this definition, here this means that with high probability in this space there is a separator of Lowell, one margin.",
                    "label": 0
                },
                {
                    "sent": "In particular there's a set even if we don't know what ours advanced.",
                    "label": 1
                },
                {
                    "sent": "There exists in this space and I separator.",
                    "label": 0
                },
                {
                    "sent": "There's one where the weights are zero for the points that are not one of these.",
                    "label": 0
                },
                {
                    "sent": "In this reasonable set an weights one over the number of reasonable points, the number of reasonable landmarks.",
                    "label": 0
                },
                {
                    "sent": "For the ones that are reasonable.",
                    "label": 0
                },
                {
                    "sent": "And this times plus one or minus one depending on their label.",
                    "label": 0
                },
                {
                    "sent": "OK, so so we do enough landmarks that with high probability Tao is the probability mass of the reasonable set.",
                    "label": 0
                },
                {
                    "sent": "This is enough so that with high probability somewhere in here there's one over gamma squared reasonable points.",
                    "label": 0
                },
                {
                    "sent": "And if you think about this weight vector zero for the unreasonable ones.",
                    "label": 0
                },
                {
                    "sent": "One over the number of reasonable for the reason ones times, times the label.",
                    "label": 0
                },
                {
                    "sent": "This is exactly estimating this expectation, and so we have in this space with high probability.",
                    "label": 0
                },
                {
                    "sent": "There's a nice separator, and we can.",
                    "label": 1
                },
                {
                    "sent": "Now we just use our favorite algorithm for learning linear separators.",
                    "label": 0
                },
                {
                    "sent": "They have good L1 margins.",
                    "label": 0
                },
                {
                    "sent": "And so the point here is that we have a nice, day dependent concept space where we're using unlabeled data to even define, you know.",
                    "label": 0
                },
                {
                    "sent": "What are spaces?",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let me let me stop here.",
                    "label": 0
                },
                {
                    "sent": "So so we're using landmarks that define an explicit data dependent concept space we're considering with separators of large one margin, that space, which again this dependence is actually two different uses of data dependent concept spaces.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me let me stop here.",
                    "label": 0
                }
            ]
        }
    }
}