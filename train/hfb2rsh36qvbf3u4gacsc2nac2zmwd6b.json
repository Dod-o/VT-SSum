{
    "id": "hfb2rsh36qvbf3u4gacsc2nac2zmwd6b",
    "title": "Identifying Good Patterns for Relation Extraction",
    "info": {
        "author": [
            "Janez Starc, Artificial Intelligence Laboratory, Jo\u017eef Stefan Institute"
        ],
        "published": "Nov. 16, 2012",
        "recorded": "October 2012",
        "category": [
            "Top->Computer Science->Pattern Recognition"
        ]
    },
    "url": "http://videolectures.net/is2012_starc_relation_extraction/",
    "segmentation": [
        [
            "So we did some work on translating text into logic.",
            "So."
        ],
        [
            "So basically what we want to do is to extract as much information from news media, Wikipedia, or some informal discourse is an we want to assert this knowledge to.",
            "To target ontology, some knowledge based and then we can reason about this knowledge.",
            "So for example, we have here a sentence that Jim Whitehurst, CEO of Red Hat, just delivered a terrific opening keynote presentation for links can so one relation here that we want to extract is that just Jim Whitehurst is the CEO of Red Hat so.",
            "Here is the pattern that extracts this information.",
            "So person, position of organization and here is the here can be one representation of the of this relation in one ontology.",
            "So the predicate is position of person in organization and the arguments are Jim Whitehurst, Red Hat and CIO.",
            "So the main part of our work was to find good patterns.",
            "So these are the patterns that produce semantic, semantically useful relations with high precision and recall so."
        ],
        [
            "So, as Marco said, our approach is similar to frequent negram extraction, so the algorithm runs through very large corpora and does it really fast because it doesn't not exploit any syntactic analysis such as part of speech tags and fresh chunks.",
            "Maybe it's worth saying here, so somehow that approaches aims also to be should work also across the languages later on.",
            "So English is just just case now.",
            "Oh so."
        ],
        [
            "These are the type of patterns that we investigated, so the first types of patterns have.",
            "Their arguments are only named entities.",
            "So like person, organization, location, money, that and so on.",
            "So we got those from enricher.",
            "These were these were annotated by Enricher.",
            "So for the first type of the pattern.",
            "The result is the relation.",
            "So here we have an example of pattern, organization, coach, person and the result is that we can assert one relation that Doc Rivers is the coach of Boston Celtics.",
            "And while the other is a result of the other type of patterns, is a concept so we can extract from the from the text.",
            "That concept of marriage father.",
            "So this this enables us to recursively extract all the information from the from the sentence or from the text.",
            "So and the last type of patterns.",
            "Are called one variable length gap patterns, and this variable length gaps can occur only in the middle becausw somehow then the pattern itself.",
            "Toms describes determines the length of this gap, so we have the barrel length gap argument position, and here is it determinated that.",
            "Position is CEO and President, so this could be one word just CEO Ann or could be also more than 3 words.",
            "Recursively, Oh yeah he could.",
            "He could maybe be some person and this person could be marriage father instead of the argument position."
        ],
        [
            "So then we basically wanted to somehow filter an rank.",
            "These huge amounts of patterns so that we could find the good ones.",
            "So here are a few statistics that we investigated.",
            "So basically the most important one is of course the frequency.",
            "This enables us to have big recall on the data and then we have.",
            "Then we can also.",
            "We also looked at number of arguments, so we don't want to have patterns to have two too many arguments or or maybe no arguments.",
            "And there are also a number of stop words if the pattern has a lot of stop words, then basically it's not so meaningful and the one that showed that the one statistic that showed that is very important is minimal token frequency.",
            "So this is the frequency of the words that occurs the least in the corpus and for the last.",
            "For last statistics is the normalized expectations.",
            "This does weather words in the pattern frequently Co occur with each other in this corpus."
        ],
        [
            "So this is an example of the table of these patterns representing NE grams of length 6, and these are their statistics.",
            "So we filtered out for the context you took this from the news.",
            "Yes, yes yes.",
            "So this is all.",
            "This is all from the news media language so.",
            "So the first column is the frequency.",
            "We eliminated those patterns that have frequency less than 20.",
            "And we wanted to have patterns that have more than two arguments, so this is described in the second column.",
            "We didn't.",
            "Exploit the third column, which is the stopwords.",
            "But we ranked this table according to the minimal token frequency and one hypothesis is that the.",
            "The patterns which have reload minimal token frequency are somehow better.",
            "So for example we see here that in the last row the the word that has least frequencies the word Member, it's about 13,400 and the and the last row is the normalized expectation."
        ],
        [
            "So then we then we try to evaluate these patterns by asserting them to the knowledge base.",
            "In this case it was sex knowledge base.",
            "First the patterns need to be need to be transformed to the to the language of psych cycle.",
            "So basically first we first identified the matches in the corpus by simple pattern matching.",
            "Rhythm, then we made some handcrafted translation like this one, and then we evaluated the pattern by pattern.",
            "So here we have one pattern that describes that a particular person said something in the said the statement.",
            "So this pattern is very frequent in news media, almost one of the most frequent ones, and so the patterns that somebody said the statement.",
            "A person said the statement and this is the cykel pattern behind it, so it's a little bit complicated, but.",
            "It says that there exists an informing and the sender of this information is the person and string that the statement that he stated.",
            "Is this info transferred natural language string so?",
            "And these are the results so.",
            "We have found 379 matches in the corpus and we identified 22127 new new concepts and 35 concepts were already there in ontology.",
            "This was basically persons and there were a total of 88130.",
            "Assertions Becausw 111 match can have multi multiple relations because we didn't apply the word sense disambiguation, so there were 37 matches with multiple assertions and from that we checked whether some of these assertions are valid, that the meaning stayed in the relation and we found that.",
            "About 90% of these.",
            "Relations were valid."
        ],
        [
            "So let me conclude with the future work.",
            "So we will try to examine more types of patterns.",
            "So now the variable gapped length arguments are only in the middle, so will try to make it on the beginning or at the end.",
            "So.",
            "But we need to.",
            "I think we need to exploit some part of speech tags or some fresh chunks to do that.",
            "And also we didn't apply a word disambiguation so.",
            "What sense disambiguation so we have to take care of this also and?",
            "One thing is that we have to.",
            "Somehow resolve the type of the argument.",
            "So in that case where we had a pattern describing the position of the person position was somehow determined by the human, so we could then easily also determine this by some kind of classification or clustering.",
            "And the other thing is to analyze patterns in different domains and somehow compare how patterns.",
            "Occur in different domains.",
            "And of course, to make to identify how to make good translation with less human involvement.",
            "Yes, and I'll conclude this."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we did some work on translating text into logic.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So basically what we want to do is to extract as much information from news media, Wikipedia, or some informal discourse is an we want to assert this knowledge to.",
                    "label": 0
                },
                {
                    "sent": "To target ontology, some knowledge based and then we can reason about this knowledge.",
                    "label": 0
                },
                {
                    "sent": "So for example, we have here a sentence that Jim Whitehurst, CEO of Red Hat, just delivered a terrific opening keynote presentation for links can so one relation here that we want to extract is that just Jim Whitehurst is the CEO of Red Hat so.",
                    "label": 1
                },
                {
                    "sent": "Here is the pattern that extracts this information.",
                    "label": 0
                },
                {
                    "sent": "So person, position of organization and here is the here can be one representation of the of this relation in one ontology.",
                    "label": 0
                },
                {
                    "sent": "So the predicate is position of person in organization and the arguments are Jim Whitehurst, Red Hat and CIO.",
                    "label": 0
                },
                {
                    "sent": "So the main part of our work was to find good patterns.",
                    "label": 0
                },
                {
                    "sent": "So these are the patterns that produce semantic, semantically useful relations with high precision and recall so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, as Marco said, our approach is similar to frequent negram extraction, so the algorithm runs through very large corpora and does it really fast because it doesn't not exploit any syntactic analysis such as part of speech tags and fresh chunks.",
                    "label": 1
                },
                {
                    "sent": "Maybe it's worth saying here, so somehow that approaches aims also to be should work also across the languages later on.",
                    "label": 0
                },
                {
                    "sent": "So English is just just case now.",
                    "label": 0
                },
                {
                    "sent": "Oh so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These are the type of patterns that we investigated, so the first types of patterns have.",
                    "label": 0
                },
                {
                    "sent": "Their arguments are only named entities.",
                    "label": 1
                },
                {
                    "sent": "So like person, organization, location, money, that and so on.",
                    "label": 0
                },
                {
                    "sent": "So we got those from enricher.",
                    "label": 0
                },
                {
                    "sent": "These were these were annotated by Enricher.",
                    "label": 0
                },
                {
                    "sent": "So for the first type of the pattern.",
                    "label": 1
                },
                {
                    "sent": "The result is the relation.",
                    "label": 0
                },
                {
                    "sent": "So here we have an example of pattern, organization, coach, person and the result is that we can assert one relation that Doc Rivers is the coach of Boston Celtics.",
                    "label": 1
                },
                {
                    "sent": "And while the other is a result of the other type of patterns, is a concept so we can extract from the from the text.",
                    "label": 0
                },
                {
                    "sent": "That concept of marriage father.",
                    "label": 0
                },
                {
                    "sent": "So this this enables us to recursively extract all the information from the from the sentence or from the text.",
                    "label": 1
                },
                {
                    "sent": "So and the last type of patterns.",
                    "label": 1
                },
                {
                    "sent": "Are called one variable length gap patterns, and this variable length gaps can occur only in the middle becausw somehow then the pattern itself.",
                    "label": 0
                },
                {
                    "sent": "Toms describes determines the length of this gap, so we have the barrel length gap argument position, and here is it determinated that.",
                    "label": 0
                },
                {
                    "sent": "Position is CEO and President, so this could be one word just CEO Ann or could be also more than 3 words.",
                    "label": 0
                },
                {
                    "sent": "Recursively, Oh yeah he could.",
                    "label": 0
                },
                {
                    "sent": "He could maybe be some person and this person could be marriage father instead of the argument position.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So then we basically wanted to somehow filter an rank.",
                    "label": 0
                },
                {
                    "sent": "These huge amounts of patterns so that we could find the good ones.",
                    "label": 0
                },
                {
                    "sent": "So here are a few statistics that we investigated.",
                    "label": 0
                },
                {
                    "sent": "So basically the most important one is of course the frequency.",
                    "label": 0
                },
                {
                    "sent": "This enables us to have big recall on the data and then we have.",
                    "label": 0
                },
                {
                    "sent": "Then we can also.",
                    "label": 0
                },
                {
                    "sent": "We also looked at number of arguments, so we don't want to have patterns to have two too many arguments or or maybe no arguments.",
                    "label": 0
                },
                {
                    "sent": "And there are also a number of stop words if the pattern has a lot of stop words, then basically it's not so meaningful and the one that showed that the one statistic that showed that is very important is minimal token frequency.",
                    "label": 0
                },
                {
                    "sent": "So this is the frequency of the words that occurs the least in the corpus and for the last.",
                    "label": 1
                },
                {
                    "sent": "For last statistics is the normalized expectations.",
                    "label": 0
                },
                {
                    "sent": "This does weather words in the pattern frequently Co occur with each other in this corpus.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is an example of the table of these patterns representing NE grams of length 6, and these are their statistics.",
                    "label": 0
                },
                {
                    "sent": "So we filtered out for the context you took this from the news.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes yes.",
                    "label": 0
                },
                {
                    "sent": "So this is all.",
                    "label": 0
                },
                {
                    "sent": "This is all from the news media language so.",
                    "label": 0
                },
                {
                    "sent": "So the first column is the frequency.",
                    "label": 0
                },
                {
                    "sent": "We eliminated those patterns that have frequency less than 20.",
                    "label": 0
                },
                {
                    "sent": "And we wanted to have patterns that have more than two arguments, so this is described in the second column.",
                    "label": 0
                },
                {
                    "sent": "We didn't.",
                    "label": 0
                },
                {
                    "sent": "Exploit the third column, which is the stopwords.",
                    "label": 0
                },
                {
                    "sent": "But we ranked this table according to the minimal token frequency and one hypothesis is that the.",
                    "label": 0
                },
                {
                    "sent": "The patterns which have reload minimal token frequency are somehow better.",
                    "label": 0
                },
                {
                    "sent": "So for example we see here that in the last row the the word that has least frequencies the word Member, it's about 13,400 and the and the last row is the normalized expectation.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So then we then we try to evaluate these patterns by asserting them to the knowledge base.",
                    "label": 0
                },
                {
                    "sent": "In this case it was sex knowledge base.",
                    "label": 0
                },
                {
                    "sent": "First the patterns need to be need to be transformed to the to the language of psych cycle.",
                    "label": 0
                },
                {
                    "sent": "So basically first we first identified the matches in the corpus by simple pattern matching.",
                    "label": 0
                },
                {
                    "sent": "Rhythm, then we made some handcrafted translation like this one, and then we evaluated the pattern by pattern.",
                    "label": 0
                },
                {
                    "sent": "So here we have one pattern that describes that a particular person said something in the said the statement.",
                    "label": 0
                },
                {
                    "sent": "So this pattern is very frequent in news media, almost one of the most frequent ones, and so the patterns that somebody said the statement.",
                    "label": 0
                },
                {
                    "sent": "A person said the statement and this is the cykel pattern behind it, so it's a little bit complicated, but.",
                    "label": 0
                },
                {
                    "sent": "It says that there exists an informing and the sender of this information is the person and string that the statement that he stated.",
                    "label": 0
                },
                {
                    "sent": "Is this info transferred natural language string so?",
                    "label": 0
                },
                {
                    "sent": "And these are the results so.",
                    "label": 0
                },
                {
                    "sent": "We have found 379 matches in the corpus and we identified 22127 new new concepts and 35 concepts were already there in ontology.",
                    "label": 0
                },
                {
                    "sent": "This was basically persons and there were a total of 88130.",
                    "label": 0
                },
                {
                    "sent": "Assertions Becausw 111 match can have multi multiple relations because we didn't apply the word sense disambiguation, so there were 37 matches with multiple assertions and from that we checked whether some of these assertions are valid, that the meaning stayed in the relation and we found that.",
                    "label": 0
                },
                {
                    "sent": "About 90% of these.",
                    "label": 0
                },
                {
                    "sent": "Relations were valid.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me conclude with the future work.",
                    "label": 0
                },
                {
                    "sent": "So we will try to examine more types of patterns.",
                    "label": 1
                },
                {
                    "sent": "So now the variable gapped length arguments are only in the middle, so will try to make it on the beginning or at the end.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "But we need to.",
                    "label": 0
                },
                {
                    "sent": "I think we need to exploit some part of speech tags or some fresh chunks to do that.",
                    "label": 1
                },
                {
                    "sent": "And also we didn't apply a word disambiguation so.",
                    "label": 0
                },
                {
                    "sent": "What sense disambiguation so we have to take care of this also and?",
                    "label": 0
                },
                {
                    "sent": "One thing is that we have to.",
                    "label": 0
                },
                {
                    "sent": "Somehow resolve the type of the argument.",
                    "label": 1
                },
                {
                    "sent": "So in that case where we had a pattern describing the position of the person position was somehow determined by the human, so we could then easily also determine this by some kind of classification or clustering.",
                    "label": 0
                },
                {
                    "sent": "And the other thing is to analyze patterns in different domains and somehow compare how patterns.",
                    "label": 1
                },
                {
                    "sent": "Occur in different domains.",
                    "label": 1
                },
                {
                    "sent": "And of course, to make to identify how to make good translation with less human involvement.",
                    "label": 0
                },
                {
                    "sent": "Yes, and I'll conclude this.",
                    "label": 0
                }
            ]
        }
    }
}