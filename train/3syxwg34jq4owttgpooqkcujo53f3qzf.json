{
    "id": "3syxwg34jq4owttgpooqkcujo53f3qzf",
    "title": "Training a Binary Classifier with the Quantum Adiabatic Algorithm",
    "info": {
        "author": [
            "Hartmut Neven, Google, Inc."
        ],
        "published": "Dec. 20, 2008",
        "recorded": "December 2008",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/opt08_neven_tabcwt/",
    "segmentation": [
        [
            "So we're going to talk about training a binary classifier with the quantum adiabatic algorithm."
        ],
        [
            "Arm the outline of the talk is we assume that most are not that familiar with quantum computing.",
            "In particular, the adiabatic version.",
            "So we will explain this.",
            "Then we will go to our particular framework of training a binary classifier and then report on some experimental result."
        ],
        [
            "So they better quantum computing and don't have to explain this community here.",
            "I think we're getting together in order to apply optimization techniques to machine learning, so optimization, traditionally formalized as finding the minimum of some objective function.",
            "And if you look closely, most, or let's say a standard optimization method would just be gradient descent.",
            "Or a fancy flavor of this, but if you think about it, gradient descent in some way, it's a physical process.",
            "You can think of a bit of a ball rolling down.",
            "And this is actually quite true in general.",
            "Ultimately, all information is physically represented.",
            "That means or manipulation of information and how we can solve such problems efficiently is determined by the laws of physics, and it seems like most communities like ours actually employed in 19th century physics rather than more modern physics.",
            "And what I mean by this, if you wouldn't think of this.",
            "So the ball here as a classical mechanical entity, but rather think of it as an electron in some potential function.",
            "Then quantum mechanics would apply and some new effects come to bear, which might or which can help us in solving optimization problems.",
            "For example, most of you will be familiar with what's called the tunneling effect.",
            "Alma gradient descent, of course has a problem or danger of being stuck in a local minimum, but you may know that if this is a quantum mechanical particle, there is a finite probability to tunnels through this little Hill here.",
            "So in some ways you can say is that the quantum mechanical particle sees the global optimum of this function, and this will cause attractive in an optimization setting."
        ],
        [
            "Um, to make this really work, you want to do it such that the bumps you have to tunnel through are as small as possible, because the probability of tunneling through decreases with the size of those bumps.",
            "So the way this is done, and this is basically the quantum adiabatic algorithm developed by Edward Fiende collaborators.",
            "You start with an objective function.",
            "That has just one easy to find global minimum and then you slowly hence adiabatic deform it to turn into the objective function.",
            "Since your problem Hamiltonian and I show you a little video.",
            "Houses.",
            "Comes about so we start with a simple.",
            "Um, Hamiltonian only one global minimum, then we deform it, and this is a semiclassical depiction of what's happening.",
            "Here is a quantum magic not happen essentially thought of something that classically you cannot have, and then we continue assembly.",
            "So basically you have this deforming Hamiltonian that goes from start to problem Hamiltonian, and if you do this slowly enough, you're guaranteed to stay.",
            "At all times, at the global minimum of your objective function, as I say, this is actually not quite the right picture, but if you just remember this about quantum adiabatic algorithm, I think you have for not too bad notion how this works."
        ],
        [
            "Quickly look at a little bit more formal quantum mechanical analysis, so there.",
            "Better picture looks rather like this.",
            "Again, you have the same situation.",
            "You have a start Hamiltonian and then you look at the energy levels cause solving the problem means finding the lowest energy level.",
            "But in quantum mechanics, hence the name quantized.",
            "The energy levels are discrete, you have a set of discrete energy levels and now we do the same as in this little movie.",
            "We deform the Hamiltonian, then the energy levels will start to change.",
            "And that is from the 20s.",
            "A theorem called the adiabatic theorem that if you do this slowly enough and you start out in the ground state, you will at all times stay in the ground state.",
            "And of course, you can imagine the most dangerous spot is where the ground state gets closest to the first excited state.",
            "That's called the gap, and the smaller the gap, the higher the risk.",
            "That by changing the Hamiltonian you inject enough energy in the system.",
            "Or let's say some environmental noise bring some additional energy into your system and then you hop into the excited States and then you're not guaranteed anymore to wind up in the global.",
            "Minimum.",
            "So basically the time with which you do the sweep from start to problem, Hamiltonian has to go with something like the inverse of this gap square.",
            "Is that seem more random?"
        ],
        [
            "Version.",
            "So why bother using this?",
            "It has been shown that there's the possibility of expecting exponential or at least polynomial speedups over classical approaches.",
            "And of course, we hope that this can have significant impact for machine learning problems."
        ],
        [
            "Well.",
            "Now this is not complete pine.",
            "These guys are actually companies out there that build hardware.",
            "Based on this principle and one producer, one opposes along with this is the waves are based in Vancouver.",
            "And what you can see here are the masks of chips that have built actually approach they chose.",
            "They took a macroscopic found quantum phenomenon superconductivity.",
            "So the chip essentially consists of little superconducting loops that are magnetically coupled in a nearest neighbor fashion.",
            "So you can think of it as an Ising model.",
            "What it really implements, and this if you would look closely, you could see these.",
            "Little loops out of niobium and actually in 07.",
            "We use the 28 cubic ship to do some image matching.",
            "Problem with this.",
            "Here you see the latest chip or this actually in the making this 128 qubit chip and we are eagerly awaiting this.",
            "Coming back from the fab and then hopefully it works as advertised and we can run our problems on this chip."
        ],
        [
            "Um so.",
            "The D wave hardware of course only implements a specific Hamiltonian, not any, and this specific Hamiltonian it implements.",
            "They say that's like an Ising model, so basically it implements the energy function of Ising model, or a mathematician would call it.",
            "What this does this chip does.",
            "You can think of it as a coprocessor to solve a specific optimization problem, and this optimization problem is a quadratic.",
            "Constraint binary optimization kuebel problem as we call it.",
            "So again, the hardware we have is a coprocessor to find the minimum energy state of this function, and that's basically all you need to know.",
            "As a practitioner you can kind of start forgetting all the quantum mechanics below it or from mathematical point of you need to remember you have now a new way of finding the ground state of this function and the name of the game applying it anywhere.",
            "For example in machine learning is you need to take your specific machine learning.",
            "Problem and map it so that it fits this problem format so that you can apply the coprocessor.",
            "And because there are few nastiness is one is.",
            "For example, this is a binary optimization.",
            "Now, traditionally machine learning is not really treated as an integer problem or as a binary optimization problem.",
            "There are some other engineering nastiness is for now.",
            "Also, you cannot have arbitrary bit resolution on your coefficients, nor can the matrix J.",
            "The quadratic matrix have full connectivity and so on, but they are minor and will be relaxed as hardware."
        ],
        [
            "Develops further, so how do we now picked a base problem in machine learning just to show how this can be done?",
            "Training binary classifier and want to go about mapping this to the cube of problem we can solve.",
            "So we chose a formulation that is more derived from a boosting setting.",
            "The cause is in the area I'm working on.",
            "Image recognition boosting seems to be the best performing workhorse algorithm.",
            "So basically we have a set of re classifiers.",
            "Hi, we have binary outputs.",
            "We have linear superposition and we picked the sign of this linear superposition which then forms in boosting language.",
            "Our strong classifier.",
            "So how do we train it?",
            "Was you have the last term here we picked the 01 loss or the BF H as a heavy side function and we want to have sparsity.",
            "Enforce them heads on so we just pick the zero norm.",
            "So we want to optimize or.",
            "Solve this training problem.",
            "Now in the first challenge that arises the WSR traditionally chosen to be a real valued, so we need to get rid of this and represent of course formally you can always take a real valued number and expanded into a binary expansion and have them formally an integer problem if you stop at some machine resolution, but we want to be a little bit more frugal with our bits than that so."
        ],
        [
            "How many?",
            "Bits, do we really need to solve training problem like this and there we did the following analysis.",
            "Every sample that we bring on essentially.",
            "Invokes an inequality constraint like this.",
            "Or maybe I show this."
        ],
        [
            "Or graphically here for the case N = 3.",
            "So we have three weights.",
            "Then each sample we pick represents a hyperplane diagonal hyperplanes, a space, and depending on whether wires plus one or minus one, you want your weights to lie on the one side or the other side of the hyperplane.",
            "So if you know, pick the next hyperplane.",
            "Same thing applies.",
            "So basically the different classifiers you can construct.",
            "Also correspond to the different chambers that emerge from these intersecting hyperplanes.",
            "So if now we don't have continuous weights, but we have.",
            "But weights that are represented with a finite number of bits, then those weight configurations lie on a cubic hyper lettuce.",
            "Here we have shown the simple case where we have only one bit, so we have this hyper lettuce and.",
            "We would have enough power in the binary representation if every vertex on the hyper lattice lies in one of our solution chambers if you have.",
            "That condition then we don't lose anything.",
            "Every classifier that can be represented with real numbers can be represented with the binary representation in this fashion."
        ],
        [
            "And if you.",
            "So roughly followed through with this goal that you have roughly one lattice point for each chamber in the geometry.",
            "I just showed you.",
            "Then you and there's a little technicality that we should only look at regions in the positive quadrant.",
            "'cause that's how we arrange our weak classifier of dictionaries.",
            "But basically once this quotient to be larger than one, and if you.",
            "Look closer, you will then find that the number of bits, meaning how many points I have on one edge of my hyper lettuce has to grow as the logarithm of F&F.",
            "Here is the number of samples of training samples I have divided by the number of weak classifiers.",
            "So here's a little constant we can solve.",
            "Ignore this.",
            "So basically you have this logarithmic dependency and that's good news.",
            "That means you don't need all that many bits.",
            "In order to represent your classifiers and successfully, because in practical settings F is often smaller than one, or at least let's say in the cases we are working on, is hardly ever larger than 10."
        ],
        [
            "Um?",
            "So we need so we have one important modification done.",
            "We turned our.",
            "Optimization over the real numbers into an integer problem.",
            "And the second modification we need to do is say this is not in a restriction that comes from adiabatic quantum computing per say, but comes from the fact that the D wave chip only allows for quadratic energy functions.",
            "So we changed the loss to become quadratic loss, and then we finally are left with a quadratic optimization problem such that it can be solved by the D wave chip.",
            "Yeah, maybe I handed over at this point to a Brazil who will explain in the experiments we did with this setting.",
            "Thank you very much."
        ],
        [
            "OK, so we implemented the two formulations, 01 loss and square error in Matlab and we fixed dictionaries in this form.",
            "So basically we have what we call order one order two dictionaries which consists of monomial's of order, one order two.",
            "Each of them also consists of positive and negative decision stumps.",
            "You can see also that we use thresholds in each of them, so the purpose of the thresholds is to minimize the training error.",
            "That's due to each particular weak classifier, and this is just a trivial computation to find an optimal operating point for each class."
        ],
        [
            "Fire.",
            "So we use simulated annealing for minimizing the 01 loss formulation.",
            "There we had to tune the cooling schedule in order to achieve optimal optimization an in square.",
            "The square are formulation we use taboo.",
            "Search tuned to work with cable.",
            "Another thing that we had to add was a an optimal threshold for the final strong classifier, which we compute just as the average of the responses of the selected weak classifiers.",
            "After the training is done.",
            "And here we assume that the number of positive and negative training examples is equal.",
            "In order for this to work.",
            "If we just take an average.",
            "So we modify the final classifier.",
            "To make the categorical decision in the end, taking into account that optimal threshold.",
            "In order to find an optimal regularization strength, we had to do a 30 fold cross validation.",
            "Looking at which value for Lambda gives the best generalization error, and picking that one and also we limited the choice of weak classifiers to be at most half of the dictionary that we have available."
        ],
        [
            "So we did experiments with both synthetic and natural data, and here is a visualization of how our synthetic data looks.",
            "Basically we generate 2 clouds, one positive and one negative from Gaussian distribution.",
            "The two clouds have different variances and in the beginning we have them separated far apart.",
            "Obviously the classification task here is easy, and then we gradually start pushing them towards each other until they're completely overlapped, at which point.",
            "Hardest classification task.",
            "And we plot our error curves."
        ],
        [
            "Respect to this movement of the two clouds.",
            "So here we did.",
            "A comparison of our global optimization methods.",
            "Both the square error and 01 loss with the state of the art.",
            "Greedy outer boost method.",
            "So the top two curves here that you see are produced by.",
            "A Matlab toolbox called Gmel which provides a standard implementation of other boosts.",
            "This is the purple curve and the red one is our own implementation of other boosts, so you can clearly see that the global optimization methods outperform.",
            "Other boosts.",
            "Here are the figure.",
            "On the left has the parameter F set to 1, where F again was the ratio between the number of training samples that we have and number of weak classifiers and this one to the right has this parameter F set to 8.",
            "So we can also notice that not only do we achieve lower error rates, but also the big constraint also doesn't hurt us at all.",
            "I mean, we only did the square or formulation.",
            "Which we named QP with one and three bits and the 01 loss formulation.",
            "We compare 1, three and 64 bits, whereas Adaboost is always done with double precision.",
            "Or"
        ],
        [
            "So we also did the experiments with the 2nd order dictionary and here we see that.",
            "We achieved much lower error rates, but still we outperformed other boost by."
        ],
        [
            "And Lastly, our natural data experiments here in each cell of the tables we have two numbers.",
            "One is the achieved error rates and the other is the number of weak classifiers that are used.",
            "And here's the interesting thing to note is that we achieve we compute strong classifiers that use 50% fewer weak classifiers than other boost.",
            "So we achieve much greater sparsity here."
        ],
        [
            "And to conclude, we've seen in this work how global optimization compete successfully with greedy methods such as Adaboost, and also we made this interesting discovery that a bit constrained learning machines exhibit or could exhibit lower generalization errors.",
            "So this can be interpreted intuitively as an intrinsic regularization, that this bit constraining is doing.",
            "So maybe that's one of the reasons why this is happening.",
            "And also the good thing that we found out is that Fortunately the required precision grows only logarithmically with S overrun, which in practice would be not a large number at all."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we're going to talk about training a binary classifier with the quantum adiabatic algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Arm the outline of the talk is we assume that most are not that familiar with quantum computing.",
                    "label": 1
                },
                {
                    "sent": "In particular, the adiabatic version.",
                    "label": 0
                },
                {
                    "sent": "So we will explain this.",
                    "label": 0
                },
                {
                    "sent": "Then we will go to our particular framework of training a binary classifier and then report on some experimental result.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So they better quantum computing and don't have to explain this community here.",
                    "label": 1
                },
                {
                    "sent": "I think we're getting together in order to apply optimization techniques to machine learning, so optimization, traditionally formalized as finding the minimum of some objective function.",
                    "label": 0
                },
                {
                    "sent": "And if you look closely, most, or let's say a standard optimization method would just be gradient descent.",
                    "label": 0
                },
                {
                    "sent": "Or a fancy flavor of this, but if you think about it, gradient descent in some way, it's a physical process.",
                    "label": 0
                },
                {
                    "sent": "You can think of a bit of a ball rolling down.",
                    "label": 0
                },
                {
                    "sent": "And this is actually quite true in general.",
                    "label": 0
                },
                {
                    "sent": "Ultimately, all information is physically represented.",
                    "label": 0
                },
                {
                    "sent": "That means or manipulation of information and how we can solve such problems efficiently is determined by the laws of physics, and it seems like most communities like ours actually employed in 19th century physics rather than more modern physics.",
                    "label": 0
                },
                {
                    "sent": "And what I mean by this, if you wouldn't think of this.",
                    "label": 0
                },
                {
                    "sent": "So the ball here as a classical mechanical entity, but rather think of it as an electron in some potential function.",
                    "label": 1
                },
                {
                    "sent": "Then quantum mechanics would apply and some new effects come to bear, which might or which can help us in solving optimization problems.",
                    "label": 0
                },
                {
                    "sent": "For example, most of you will be familiar with what's called the tunneling effect.",
                    "label": 0
                },
                {
                    "sent": "Alma gradient descent, of course has a problem or danger of being stuck in a local minimum, but you may know that if this is a quantum mechanical particle, there is a finite probability to tunnels through this little Hill here.",
                    "label": 0
                },
                {
                    "sent": "So in some ways you can say is that the quantum mechanical particle sees the global optimum of this function, and this will cause attractive in an optimization setting.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um, to make this really work, you want to do it such that the bumps you have to tunnel through are as small as possible, because the probability of tunneling through decreases with the size of those bumps.",
                    "label": 0
                },
                {
                    "sent": "So the way this is done, and this is basically the quantum adiabatic algorithm developed by Edward Fiende collaborators.",
                    "label": 0
                },
                {
                    "sent": "You start with an objective function.",
                    "label": 0
                },
                {
                    "sent": "That has just one easy to find global minimum and then you slowly hence adiabatic deform it to turn into the objective function.",
                    "label": 0
                },
                {
                    "sent": "Since your problem Hamiltonian and I show you a little video.",
                    "label": 0
                },
                {
                    "sent": "Houses.",
                    "label": 0
                },
                {
                    "sent": "Comes about so we start with a simple.",
                    "label": 0
                },
                {
                    "sent": "Um, Hamiltonian only one global minimum, then we deform it, and this is a semiclassical depiction of what's happening.",
                    "label": 0
                },
                {
                    "sent": "Here is a quantum magic not happen essentially thought of something that classically you cannot have, and then we continue assembly.",
                    "label": 0
                },
                {
                    "sent": "So basically you have this deforming Hamiltonian that goes from start to problem Hamiltonian, and if you do this slowly enough, you're guaranteed to stay.",
                    "label": 0
                },
                {
                    "sent": "At all times, at the global minimum of your objective function, as I say, this is actually not quite the right picture, but if you just remember this about quantum adiabatic algorithm, I think you have for not too bad notion how this works.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Quickly look at a little bit more formal quantum mechanical analysis, so there.",
                    "label": 0
                },
                {
                    "sent": "Better picture looks rather like this.",
                    "label": 0
                },
                {
                    "sent": "Again, you have the same situation.",
                    "label": 0
                },
                {
                    "sent": "You have a start Hamiltonian and then you look at the energy levels cause solving the problem means finding the lowest energy level.",
                    "label": 0
                },
                {
                    "sent": "But in quantum mechanics, hence the name quantized.",
                    "label": 0
                },
                {
                    "sent": "The energy levels are discrete, you have a set of discrete energy levels and now we do the same as in this little movie.",
                    "label": 0
                },
                {
                    "sent": "We deform the Hamiltonian, then the energy levels will start to change.",
                    "label": 0
                },
                {
                    "sent": "And that is from the 20s.",
                    "label": 0
                },
                {
                    "sent": "A theorem called the adiabatic theorem that if you do this slowly enough and you start out in the ground state, you will at all times stay in the ground state.",
                    "label": 1
                },
                {
                    "sent": "And of course, you can imagine the most dangerous spot is where the ground state gets closest to the first excited state.",
                    "label": 0
                },
                {
                    "sent": "That's called the gap, and the smaller the gap, the higher the risk.",
                    "label": 0
                },
                {
                    "sent": "That by changing the Hamiltonian you inject enough energy in the system.",
                    "label": 0
                },
                {
                    "sent": "Or let's say some environmental noise bring some additional energy into your system and then you hop into the excited States and then you're not guaranteed anymore to wind up in the global.",
                    "label": 0
                },
                {
                    "sent": "Minimum.",
                    "label": 0
                },
                {
                    "sent": "So basically the time with which you do the sweep from start to problem, Hamiltonian has to go with something like the inverse of this gap square.",
                    "label": 0
                },
                {
                    "sent": "Is that seem more random?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Version.",
                    "label": 0
                },
                {
                    "sent": "So why bother using this?",
                    "label": 1
                },
                {
                    "sent": "It has been shown that there's the possibility of expecting exponential or at least polynomial speedups over classical approaches.",
                    "label": 0
                },
                {
                    "sent": "And of course, we hope that this can have significant impact for machine learning problems.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "Now this is not complete pine.",
                    "label": 0
                },
                {
                    "sent": "These guys are actually companies out there that build hardware.",
                    "label": 0
                },
                {
                    "sent": "Based on this principle and one producer, one opposes along with this is the waves are based in Vancouver.",
                    "label": 0
                },
                {
                    "sent": "And what you can see here are the masks of chips that have built actually approach they chose.",
                    "label": 0
                },
                {
                    "sent": "They took a macroscopic found quantum phenomenon superconductivity.",
                    "label": 0
                },
                {
                    "sent": "So the chip essentially consists of little superconducting loops that are magnetically coupled in a nearest neighbor fashion.",
                    "label": 0
                },
                {
                    "sent": "So you can think of it as an Ising model.",
                    "label": 0
                },
                {
                    "sent": "What it really implements, and this if you would look closely, you could see these.",
                    "label": 0
                },
                {
                    "sent": "Little loops out of niobium and actually in 07.",
                    "label": 0
                },
                {
                    "sent": "We use the 28 cubic ship to do some image matching.",
                    "label": 0
                },
                {
                    "sent": "Problem with this.",
                    "label": 0
                },
                {
                    "sent": "Here you see the latest chip or this actually in the making this 128 qubit chip and we are eagerly awaiting this.",
                    "label": 0
                },
                {
                    "sent": "Coming back from the fab and then hopefully it works as advertised and we can run our problems on this chip.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um so.",
                    "label": 0
                },
                {
                    "sent": "The D wave hardware of course only implements a specific Hamiltonian, not any, and this specific Hamiltonian it implements.",
                    "label": 0
                },
                {
                    "sent": "They say that's like an Ising model, so basically it implements the energy function of Ising model, or a mathematician would call it.",
                    "label": 1
                },
                {
                    "sent": "What this does this chip does.",
                    "label": 0
                },
                {
                    "sent": "You can think of it as a coprocessor to solve a specific optimization problem, and this optimization problem is a quadratic.",
                    "label": 0
                },
                {
                    "sent": "Constraint binary optimization kuebel problem as we call it.",
                    "label": 0
                },
                {
                    "sent": "So again, the hardware we have is a coprocessor to find the minimum energy state of this function, and that's basically all you need to know.",
                    "label": 1
                },
                {
                    "sent": "As a practitioner you can kind of start forgetting all the quantum mechanics below it or from mathematical point of you need to remember you have now a new way of finding the ground state of this function and the name of the game applying it anywhere.",
                    "label": 0
                },
                {
                    "sent": "For example in machine learning is you need to take your specific machine learning.",
                    "label": 0
                },
                {
                    "sent": "Problem and map it so that it fits this problem format so that you can apply the coprocessor.",
                    "label": 0
                },
                {
                    "sent": "And because there are few nastiness is one is.",
                    "label": 1
                },
                {
                    "sent": "For example, this is a binary optimization.",
                    "label": 0
                },
                {
                    "sent": "Now, traditionally machine learning is not really treated as an integer problem or as a binary optimization problem.",
                    "label": 0
                },
                {
                    "sent": "There are some other engineering nastiness is for now.",
                    "label": 0
                },
                {
                    "sent": "Also, you cannot have arbitrary bit resolution on your coefficients, nor can the matrix J.",
                    "label": 0
                },
                {
                    "sent": "The quadratic matrix have full connectivity and so on, but they are minor and will be relaxed as hardware.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Develops further, so how do we now picked a base problem in machine learning just to show how this can be done?",
                    "label": 0
                },
                {
                    "sent": "Training binary classifier and want to go about mapping this to the cube of problem we can solve.",
                    "label": 1
                },
                {
                    "sent": "So we chose a formulation that is more derived from a boosting setting.",
                    "label": 0
                },
                {
                    "sent": "The cause is in the area I'm working on.",
                    "label": 0
                },
                {
                    "sent": "Image recognition boosting seems to be the best performing workhorse algorithm.",
                    "label": 0
                },
                {
                    "sent": "So basically we have a set of re classifiers.",
                    "label": 0
                },
                {
                    "sent": "Hi, we have binary outputs.",
                    "label": 0
                },
                {
                    "sent": "We have linear superposition and we picked the sign of this linear superposition which then forms in boosting language.",
                    "label": 0
                },
                {
                    "sent": "Our strong classifier.",
                    "label": 0
                },
                {
                    "sent": "So how do we train it?",
                    "label": 0
                },
                {
                    "sent": "Was you have the last term here we picked the 01 loss or the BF H as a heavy side function and we want to have sparsity.",
                    "label": 0
                },
                {
                    "sent": "Enforce them heads on so we just pick the zero norm.",
                    "label": 0
                },
                {
                    "sent": "So we want to optimize or.",
                    "label": 0
                },
                {
                    "sent": "Solve this training problem.",
                    "label": 0
                },
                {
                    "sent": "Now in the first challenge that arises the WSR traditionally chosen to be a real valued, so we need to get rid of this and represent of course formally you can always take a real valued number and expanded into a binary expansion and have them formally an integer problem if you stop at some machine resolution, but we want to be a little bit more frugal with our bits than that so.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How many?",
                    "label": 0
                },
                {
                    "sent": "Bits, do we really need to solve training problem like this and there we did the following analysis.",
                    "label": 0
                },
                {
                    "sent": "Every sample that we bring on essentially.",
                    "label": 0
                },
                {
                    "sent": "Invokes an inequality constraint like this.",
                    "label": 1
                },
                {
                    "sent": "Or maybe I show this.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or graphically here for the case N = 3.",
                    "label": 0
                },
                {
                    "sent": "So we have three weights.",
                    "label": 0
                },
                {
                    "sent": "Then each sample we pick represents a hyperplane diagonal hyperplanes, a space, and depending on whether wires plus one or minus one, you want your weights to lie on the one side or the other side of the hyperplane.",
                    "label": 0
                },
                {
                    "sent": "So if you know, pick the next hyperplane.",
                    "label": 0
                },
                {
                    "sent": "Same thing applies.",
                    "label": 0
                },
                {
                    "sent": "So basically the different classifiers you can construct.",
                    "label": 0
                },
                {
                    "sent": "Also correspond to the different chambers that emerge from these intersecting hyperplanes.",
                    "label": 0
                },
                {
                    "sent": "So if now we don't have continuous weights, but we have.",
                    "label": 0
                },
                {
                    "sent": "But weights that are represented with a finite number of bits, then those weight configurations lie on a cubic hyper lettuce.",
                    "label": 0
                },
                {
                    "sent": "Here we have shown the simple case where we have only one bit, so we have this hyper lettuce and.",
                    "label": 0
                },
                {
                    "sent": "We would have enough power in the binary representation if every vertex on the hyper lattice lies in one of our solution chambers if you have.",
                    "label": 0
                },
                {
                    "sent": "That condition then we don't lose anything.",
                    "label": 0
                },
                {
                    "sent": "Every classifier that can be represented with real numbers can be represented with the binary representation in this fashion.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And if you.",
                    "label": 0
                },
                {
                    "sent": "So roughly followed through with this goal that you have roughly one lattice point for each chamber in the geometry.",
                    "label": 1
                },
                {
                    "sent": "I just showed you.",
                    "label": 0
                },
                {
                    "sent": "Then you and there's a little technicality that we should only look at regions in the positive quadrant.",
                    "label": 1
                },
                {
                    "sent": "'cause that's how we arrange our weak classifier of dictionaries.",
                    "label": 0
                },
                {
                    "sent": "But basically once this quotient to be larger than one, and if you.",
                    "label": 1
                },
                {
                    "sent": "Look closer, you will then find that the number of bits, meaning how many points I have on one edge of my hyper lettuce has to grow as the logarithm of F&F.",
                    "label": 0
                },
                {
                    "sent": "Here is the number of samples of training samples I have divided by the number of weak classifiers.",
                    "label": 0
                },
                {
                    "sent": "So here's a little constant we can solve.",
                    "label": 0
                },
                {
                    "sent": "Ignore this.",
                    "label": 0
                },
                {
                    "sent": "So basically you have this logarithmic dependency and that's good news.",
                    "label": 0
                },
                {
                    "sent": "That means you don't need all that many bits.",
                    "label": 0
                },
                {
                    "sent": "In order to represent your classifiers and successfully, because in practical settings F is often smaller than one, or at least let's say in the cases we are working on, is hardly ever larger than 10.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So we need so we have one important modification done.",
                    "label": 0
                },
                {
                    "sent": "We turned our.",
                    "label": 0
                },
                {
                    "sent": "Optimization over the real numbers into an integer problem.",
                    "label": 0
                },
                {
                    "sent": "And the second modification we need to do is say this is not in a restriction that comes from adiabatic quantum computing per say, but comes from the fact that the D wave chip only allows for quadratic energy functions.",
                    "label": 0
                },
                {
                    "sent": "So we changed the loss to become quadratic loss, and then we finally are left with a quadratic optimization problem such that it can be solved by the D wave chip.",
                    "label": 0
                },
                {
                    "sent": "Yeah, maybe I handed over at this point to a Brazil who will explain in the experiments we did with this setting.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we implemented the two formulations, 01 loss and square error in Matlab and we fixed dictionaries in this form.",
                    "label": 0
                },
                {
                    "sent": "So basically we have what we call order one order two dictionaries which consists of monomial's of order, one order two.",
                    "label": 0
                },
                {
                    "sent": "Each of them also consists of positive and negative decision stumps.",
                    "label": 0
                },
                {
                    "sent": "You can see also that we use thresholds in each of them, so the purpose of the thresholds is to minimize the training error.",
                    "label": 0
                },
                {
                    "sent": "That's due to each particular weak classifier, and this is just a trivial computation to find an optimal operating point for each class.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Fire.",
                    "label": 0
                },
                {
                    "sent": "So we use simulated annealing for minimizing the 01 loss formulation.",
                    "label": 1
                },
                {
                    "sent": "There we had to tune the cooling schedule in order to achieve optimal optimization an in square.",
                    "label": 0
                },
                {
                    "sent": "The square are formulation we use taboo.",
                    "label": 0
                },
                {
                    "sent": "Search tuned to work with cable.",
                    "label": 0
                },
                {
                    "sent": "Another thing that we had to add was a an optimal threshold for the final strong classifier, which we compute just as the average of the responses of the selected weak classifiers.",
                    "label": 0
                },
                {
                    "sent": "After the training is done.",
                    "label": 0
                },
                {
                    "sent": "And here we assume that the number of positive and negative training examples is equal.",
                    "label": 0
                },
                {
                    "sent": "In order for this to work.",
                    "label": 0
                },
                {
                    "sent": "If we just take an average.",
                    "label": 1
                },
                {
                    "sent": "So we modify the final classifier.",
                    "label": 0
                },
                {
                    "sent": "To make the categorical decision in the end, taking into account that optimal threshold.",
                    "label": 1
                },
                {
                    "sent": "In order to find an optimal regularization strength, we had to do a 30 fold cross validation.",
                    "label": 0
                },
                {
                    "sent": "Looking at which value for Lambda gives the best generalization error, and picking that one and also we limited the choice of weak classifiers to be at most half of the dictionary that we have available.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we did experiments with both synthetic and natural data, and here is a visualization of how our synthetic data looks.",
                    "label": 0
                },
                {
                    "sent": "Basically we generate 2 clouds, one positive and one negative from Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "The two clouds have different variances and in the beginning we have them separated far apart.",
                    "label": 0
                },
                {
                    "sent": "Obviously the classification task here is easy, and then we gradually start pushing them towards each other until they're completely overlapped, at which point.",
                    "label": 0
                },
                {
                    "sent": "Hardest classification task.",
                    "label": 0
                },
                {
                    "sent": "And we plot our error curves.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Respect to this movement of the two clouds.",
                    "label": 0
                },
                {
                    "sent": "So here we did.",
                    "label": 0
                },
                {
                    "sent": "A comparison of our global optimization methods.",
                    "label": 0
                },
                {
                    "sent": "Both the square error and 01 loss with the state of the art.",
                    "label": 0
                },
                {
                    "sent": "Greedy outer boost method.",
                    "label": 0
                },
                {
                    "sent": "So the top two curves here that you see are produced by.",
                    "label": 0
                },
                {
                    "sent": "A Matlab toolbox called Gmel which provides a standard implementation of other boosts.",
                    "label": 0
                },
                {
                    "sent": "This is the purple curve and the red one is our own implementation of other boosts, so you can clearly see that the global optimization methods outperform.",
                    "label": 0
                },
                {
                    "sent": "Other boosts.",
                    "label": 0
                },
                {
                    "sent": "Here are the figure.",
                    "label": 0
                },
                {
                    "sent": "On the left has the parameter F set to 1, where F again was the ratio between the number of training samples that we have and number of weak classifiers and this one to the right has this parameter F set to 8.",
                    "label": 0
                },
                {
                    "sent": "So we can also notice that not only do we achieve lower error rates, but also the big constraint also doesn't hurt us at all.",
                    "label": 0
                },
                {
                    "sent": "I mean, we only did the square or formulation.",
                    "label": 0
                },
                {
                    "sent": "Which we named QP with one and three bits and the 01 loss formulation.",
                    "label": 0
                },
                {
                    "sent": "We compare 1, three and 64 bits, whereas Adaboost is always done with double precision.",
                    "label": 0
                },
                {
                    "sent": "Or",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we also did the experiments with the 2nd order dictionary and here we see that.",
                    "label": 0
                },
                {
                    "sent": "We achieved much lower error rates, but still we outperformed other boost by.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And Lastly, our natural data experiments here in each cell of the tables we have two numbers.",
                    "label": 0
                },
                {
                    "sent": "One is the achieved error rates and the other is the number of weak classifiers that are used.",
                    "label": 0
                },
                {
                    "sent": "And here's the interesting thing to note is that we achieve we compute strong classifiers that use 50% fewer weak classifiers than other boost.",
                    "label": 0
                },
                {
                    "sent": "So we achieve much greater sparsity here.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And to conclude, we've seen in this work how global optimization compete successfully with greedy methods such as Adaboost, and also we made this interesting discovery that a bit constrained learning machines exhibit or could exhibit lower generalization errors.",
                    "label": 1
                },
                {
                    "sent": "So this can be interpreted intuitively as an intrinsic regularization, that this bit constraining is doing.",
                    "label": 0
                },
                {
                    "sent": "So maybe that's one of the reasons why this is happening.",
                    "label": 1
                },
                {
                    "sent": "And also the good thing that we found out is that Fortunately the required precision grows only logarithmically with S overrun, which in practice would be not a large number at all.",
                    "label": 0
                }
            ]
        }
    }
}