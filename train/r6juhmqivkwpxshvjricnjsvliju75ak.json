{
    "id": "r6juhmqivkwpxshvjricnjsvliju75ak",
    "title": "Deep Gaussian processes",
    "info": {
        "author": [
            "Neil D. Lawrence, Department of Computer Science, University of Sheffield"
        ],
        "published": "Oct. 29, 2014",
        "recorded": "September 2014",
        "category": [
            "Top->Computer Science->Digital Signal Processing",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Information Theory",
            "Top->Mathematics->Statistics"
        ]
    },
    "url": "http://videolectures.net/sahd2014_lawrence_gaussian_processes/",
    "segmentation": [
        [
            "Thanks to the organizers for the invitation to speak.",
            "So I'm going to talk today about, I guess the main thing we're working on in the Group 1.",
            "One of several things, but it's a big thing that we're working on several people, which is a model sort of known as deep Gaussian."
        ],
        [
            "Processes.",
            "So I'll start with this sort of introduction to."
        ],
        [
            "Honest, I actually got into machine learning 'cause I used to like drawing figures like this.",
            "It seemed a little fun, much better than doing real math, so that was quite a long time ago in sort of 1996 and I have a real pleasure of sort of going to some of the talks talk by Jan in the machine Learning Generalization Workshop at Noon Institute where these models were sort of preeminent in terms of classification before support vector machine and these other things came along now since then."
        ],
        [
            "I had all that beaten out of me and I was taught that actually all these things were really just linear algebra, so I'm going to use a sort of linear algebraic representation to recast this model in a different way and what I'm trying to show here is I've given some input and the task, maybe classification or regression, whatever, but I've got some input.",
            "Let's say it's an image that I'm trying to classify, and what I do in a sort of neural network setting is I multiply that input by a matrix W and then I have some sort of nonlinear basis function set.",
            "And that gives me an output H which I then apply the same treatment to cascading through the neural network.",
            "So these are sort of adaptive basis function models and.",
            "Why are they interesting?",
            "Well, they're interesting 'cause you keep putting one layer on top of the other and what seems to be going on at the moment is people are showing more.",
            "The more you do that actually the more power you often get with your modeling.",
            "And this structure always sort of impressed me.",
            "But actually what I ended up doing with something quite different, something quite flat now, so I'd represent that model in this form with these eight stands for hidden.",
            "And W stands for weights.",
            "That's the sort of machine learning time notation.",
            "So mathematically we just have this.",
            "We've got some input."
        ],
        [
            "X that goes through these nonlinear transfer functions after a matrix multiply and then so on so forth."
        ],
        [
            "Now what I want to point out and I'm not sure if they don't sign this Jan might say whether someone's done this, but actually one problem you're starting to get with these models, even with very large amounts of data they like to have a lot of nodes in these layers, so you tend to get these very large letters.",
            "I know thousands of ages and then so you can end up with 1000 by 1000 parameters in this matrix W, and so you get an enormous amount of parameters and you can get problems with overfitting.",
            "So sort of thing people have been suggesting is something called dropout, which is a scheme where you.",
            "Only optimize some parameters at the time.",
            "It's got.",
            "Some interesting very good performance, but So what?",
            "I'd point out is that it's actually very easy way of dealing with this parameter explosion, but I'm not sure if people have tried.",
            "I know people have looked at deep networks, the parameters afterwards, and seeing that they actually have this structure.",
            "There's work by Nando de Freitas on this where you say well rather than this massive matrix with maybe 1000 by 1000 thousand Rosa thousand columns rather than represent the whole thing you could sort of structure it with a singular value decomposition.",
            "So you should say it has some inherent low rank form.",
            "Now you here, I'm assuming these two orthonormal matrices, but actually in down here I'm dropping that assumption, I'm just assume their two matrices there.",
            "First make the W is originally K1 by K2 or K1 and K2 of the width of the two of those layers very large thousands, and what we're going to do is we're just substitute that with the product of two matrices which is K1 by Q and K2 bike you.",
            "So you get something which has many fewer parameters, because if Q is 2 you've just got sort of two times K 1 + 2 * K Two parameters.",
            "So this is sort of common trick use.",
            "It's in effect what principal component analysis is doing.",
            "OK, so now I want to redraw that model."
        ],
        [
            "This because what I'm saying now is you have something slightly different.",
            "You've got an input layer and then you're going to multiply that input layer by a matrix with sort of dimensionality queue and then sort of project it down through a bottleneck and project it back up again so you don't have all these parameters in these matrices now because you've got this sort of odd low rank form.",
            "Now, why do I like that?",
            "Well, now I've sort of had to put more mass in in order to illustrate what's going on.",
            "So at this first level here, I'm just multiplying the inputs by some.",
            "I'm projecting down to some lower dimensional space and then I'm projecting back up again and then I'm continuing to do this, But the nice thing about that form is if I substitute."
        ],
        [
            "You should've said threes back in here.",
            "What I get is what I would recognize from."
        ],
        [
            "1996 as a series of what we would call multi lab set.",
            "Well single their hidden layer perceptrons, just neural networks with one hidden."
        ],
        [
            "Yeah, So what we're trying to do here is see a situation where we've gone from this structure here, which has all these basis functions mapping into each other.",
            "So actually a cascade of separate basis function models.",
            "So this is the structure in time to pull out of this thing to sort of show it's in there.",
            "So you're going from sort of some inputs down, projecting and then you got some sort of new set of inputs which goes through nonlinear basis to a set of outputs.",
            "So that's like a basis function model on its own, and this is like a basis function model on its own.",
            "Of course this is more general than the thing we saw before, because you can always set.",
            "These values such that you're recovering the full matrix.",
            "You can always set Q to the appropriate value such that you're not losing any information between here.",
            "Everyone sort of happy with that, so it's sort of a cascade of basis function models, yeah?",
            "Now now I've got that.",
            "What I want to do is throw that away."
        ],
        [
            "Because what we did.",
            "When I came into the field entrance by these interesting neural network models, which were like the brain and going to solve everything because Yan was managing to scan USPS codes at agency at the time.",
            "Will this suddenly switched off from all this sort of thing instead of actually introduce these kernel methods?",
            "And in my particular case, Gaussian process is where you should just throw that away and you actually start looking at the functions themselves.",
            "So used to say instead of this parameterisation of our model, which is just an adaptive basis function model, so these are multi output adaptive basis function models 'cause you can change these use a fixed basis function model wouldn't allow you to change these nonlinearities, you just throw that away and you start looking at the functions directly.",
            "Now the field went in two ways.",
            "One of the ways was sort of a.",
            "Reproducing kernel Hilbert space of objects like this, where you think about infinitor feature spaces and the other way of thinking about objects like this is thinking about them as Gaussian processes, which is the sort of way I went.",
            "So the thing that I'm sort of proposing is that we look at this model, and I think why would we want to do that?",
            "Well, number one.",
            "It makes everything apparently computationally harder, which isn't a good reason to want to do it, so that's a down reason, but #2 these models have a lot nicer theoretical properties.",
            "They're not easier to analyze and study.",
            "Um and this actually very often you can encode this sort of information.",
            "You might want to encode in these models.",
            "They also have these interesting properties that you can see them as these neural network models.",
            "If you put Gaussian priors over Vt here.",
            "You don't actually have to put Gaussian prior of you, but if you put a Gaussian prior over Vt and then you extend the number of basis functions that you consider to Infinity, then these models are Gaussian processes.",
            "So they sort of a what we're doing when we say we're going to use a Gaussian process is effectively with saying the width of these layers is infinite.",
            "So we've got infinite basis functions in this model, so we've kind of moved the capacity.",
            "Traditionally, people think about the capacity of these models as being in the number of basis functions you use, but I think Yang would argue you should always be using more and therefore to use Infinity sounds good.",
            "Actually, it turns out if you use Infinity here, and if you use Infinity here, everything just becomes a Gaussian process again.",
            "So if you make this infinite and this infinite, then you just get a Gaussian process with a particular covariance function, which is just the kernel of the kernel of the kernel of the things you put together.",
            "So that sort of less interesting because as we'll see, Gaussian processes don't do everything that we want, and This is why we sort of look at this.",
            "So from this perspective, what is a Gaussian process?"
        ],
        [
            "I've got a long slide.",
            "First of all, I don't believe I should have to introduce Gaussian process anymore.",
            "I think you should all learn what they are so I don't have to spend time introducing them.",
            "I mean people don't have to use linear regression and they are such fundamental objects.",
            "They really are.",
            "And it's shocking how little is known about them.",
            "But so I always have to spend time doing it, so I've kind of reduced it to a slide which is just stolen off one of my postdocs, Nicola Duran, who did it nicely in one slide.",
            "So in a Gaussian process you've got an object that is capable of generating functions and inferencing.",
            "Gaussian process is just says, well, I have a prior distribution that allows me to generate as many functions as I like, and they have infinite basis.",
            "This is what's going on here.",
            "We haven't specified where the bases are across the whole line.",
            "They would look like this.",
            "There stationary processes, and in this case the chosen to be infinitely differentiable as well.",
            "So if we generate many, many of these functions, what we do when we're doing?",
            "It's in Gaussian process is we try and generate what this is Bayesian inference in general perspective from a probabilistic programming perspective.",
            "You have some way of generating functions.",
            "In this case, it's assuming that the correlations between the points on the functions are captured by gaussianity, cider, the distribution between the points sample from the functions are Gaussian across the points.",
            "An inference in this process involves combining that with data.",
            "So here's three data points and all Bayesian inference is just saying I've got a method of generating something.",
            "Some data that I observe my posterior that I'm interested in is to retain the functions that are consistent with those points.",
            "Now that's Bayesian inference.",
            "In general, Gaussian process is a beautiful, because this process is sort of trivial, it's just linear algebra.",
            "For most Bayesian models, it's a nightmare to go from this set of functions to this.",
            "I mean, you can do it by sampling, but if you wanna do it mathematically, it can be a real problem in the Gaussian process.",
            "If this is all you're doing, this is just linear algebra operations, which is why there such beautiful models to work with.",
            "So of course we're going to mess all that up."
        ],
        [
            "Because what we're going to do.",
            "Is we're going to make it so everything is nonline?"
        ],
        [
            "So this to me is a deep model.",
            "That's all that's going on.",
            "It's just a composite function function of a function of a function of a function of a function.",
            "So you might think, well, what's the point in having a function of the function of the function of function and modeling all the individual functions when you can just model the final function well.",
            "The reason you want to do that is because this function may have very complex properties which are well captured by Gaussian processes and will see that as an example later.",
            "There's actually many reasons why you want to do that.",
            "You can introduce the sort of things they do in the deep learning community.",
            "You can introduce structure about how you expect these convolution networks about translation invariants.",
            "In these functions you can put all sorts of prior knowledge in these functions, ignoring everything they do, even if you are interested in Gaussian process, is in.",
            "Even if you are interested in all that and you're just interested in mapping from one function to another, this is a fun thing to do because it turns out I can get a much richer, richer class of distributions by doing this and placing Gaussian process priors over each of these functions now.",
            "For example, so why deep?",
            "And this is a sort of work by David Governor."
        ],
        [
            "Linda and colleagues at Cambridge, including Zubin and Ryan Adams in Harvard, which highlights this nice feature of these models.",
            "The derivatives of a Gaussian process are also Gaussian distributed, so if you want to look at the sort of it's a really nice feature so you can look at the joint distribution over the derivatives and the function itself.",
            "And the derivatives are also Gaussian distributed, but as soon as you know that you know that you're not going to be able to model jumps with them, because if you've got a function, whether jumps you don't have Gaussian distributed derivatives, you've got very heavy tailed.",
            "You've got sort of does it most of the time derivative zero and then you get a jump where it's large, so you basically know you're not going to be able to jump with the Gaussian process.",
            "Now in this nice piece of work that was I stats in Iceland.",
            "What they showed is if you start stacking these models as you stack them, you increase the.",
            "If you look at the tails of the distribution, you increase the tails.",
            "You get heavier tailed derivative distributions and I'll show an example of that being fitted later in that people they didn't actually do any, so they just looked at theory of the models they didn't actually run.",
            "The models, but we've got a little example that we presented in a late breaking poster at that conference done by by done by James Henslin, who's working with me on this where he actually fits.",
            "This is the model, and I'll show that in a moment.",
            "So that's one reason there's many reasons, but I just think we've got an example for that reason, so I just sort of include that."
        ],
        [
            "Now what's the problem with doing this?",
            "Well, the problem we're doing this?",
            "I mean what you want to do in this sort of thing is you're taking some space over which replacing a distribution.",
            "And in fact, it's a Gaussian process distribution and you're feeding into another potentially nonlinear space.",
            "So this is a bit of a nightmare because what you're doing is, let's say, so.",
            "One of the things we can do very easily with these models is unsupervised learning.",
            "So instead of having input."
        ],
        [
            "So we can just place Gaussian priors over these models and build latent variable models.",
            "So consider that set up and assume I've got a Gaussian prior over this space here and then I project it through a nonlinear function to 3 dimensional space.",
            "The resulting distribution is obviously very non tractable.",
            "In this example, one D21 D, you get the same sort of effect.",
            "I start with the Gaussian distribution and Gaussian.",
            "This process is always output Gaussian."
        ],
        [
            "Marginal distributions, so you're always having to be propagating through this thing.",
            "A series of Gaussian distributions, and they always come out the end looking ugly.",
            "So I mean, that's just a visualization of the intractability, is how they actually occur in the model is even nastier.",
            "What you actually end up doing is trying to look at how you've got to propagate a Gaussian distribution through a matrix inverse in a kind of nasty way."
        ],
        [
            "So on."
        ],
        [
            "Of that, you've got some problems of complexity, so in Gaussian process is the standard complexity is N cubed.",
            "In general?",
            "I mean you can come up with lots of ways of getting around this for specific covariance functions, but in general it's N cubed and squared in storage, so that's a nightmare because the sort of things that they do with these deep models involve billions of data and this makes it very difficult to compete, so there's two sort of problems with this modeling approach.",
            "Now."
        ],
        [
            "Now both of these problems are actually solved by going down this road, which originally was suggested by my postdoc McCullough City as when he was in Manchester with me.",
            "So what you do in this case is you actually build a variational approximation.",
            "I'm not going to go through the details of the variational approximation 'cause that on its own would take more than 25 minutes, and I don't have much time left, but I'm going to try and give you a sort of hand WAVY idea.",
            "These approximations work in the following way.",
            "Gaussian processes are nonparametric models and when you have a nonparametric model, they're very powerful, flexible models.",
            "But you'll start with this problem that you actually have to remember all the data and used all the data at all times, which is a bit of a downside because your data goes up for big data applications.",
            "You're remembering an awful lot of things.",
            "These inducing valuable approximations we've sort of come up with the term for this, which we're calling variational compression are compression schemes.",
            "You can very clearly see their compression schemes, where instead of remembering the whole data set, you remember a fixed number of observations from the data, but they aren't real observations from the data you consider optimize over them.",
            "You can change them.",
            "You can choose to remember something you didn't see.",
            "You can even choose to remember under these schemes.",
            "Choose remember the frequency of the signal even though you never directly observed it, because it turns out that a Gaussian process because 3A transform is a linear operation.",
            "You could choose to make your observation in Fourier space that you remember, even though you never saw an observation in that space, so quite powerful flexible methods.",
            "And it turns out that if you use these methods, you can significantly reduce complexity.",
            "That's one thing that happens, and then the other thing that happens, which is very exciting.",
            "Is that they render the likelihoods that you play with on the certain conditioning, independent across the data.",
            "This is a major challenge for a Gaussian process.",
            "Is that normally when you look at your likelihood, people used to independent likelihoods and pretty much every algorithm for scaling up model to very very large data relies on independence of likelihoods, stochastic gradient, descent.",
            "There's many ways you can paralyze things if you got independence of likelihood, but it turns out in this model, if you condition on these inducing variables in this approximation.",
            "You get a likelihood that condition on those inducing variables is independent.",
            "And that means I'm not giving you the details here that you can start applying all these techniques that people like, Dan, Rob, Fergus.",
            "All these people have been applying to these very large models, so it's kind of exciting because it means well and alongside it.",
            "It also turns out that you can start doing variational approximations of this sort of deep Gaussian process model I just described.",
            "So we're very excited about this and people are kind of starting to work on this, so we sort of talked about Gaussian process for big data last year.",
            "I think it you AI.",
            "James Hensman, parallelization work coming out of Cambridge.",
            "Yaron Garland, collaborators and my own post Doc Gen.",
            "When dye is looking at using GPU's to speed these things up.",
            "So we're finally in the Gaussian process world getting around to actually looking at some of the things that people do to look at serious data.",
            "So this is an idea of how these sort of models look.",
            "If you take a square and you propagate it through a random Gaussian process, these are random function draws.",
            "Each of these levels you can sort of get these.",
            "Distortions."
        ],
        [
            "The square and the thing to notice here is if once you put this distortion in it, it stays in all the time.",
            "You can't sort of undo it, so you can get these.",
            "They're very powerful for multi task learning.",
            "You can have some sort of general distortion at the top and then you can have different GPS coming out to model different datasets.",
            "I'll show brief exam."
        ],
        [
            "All of that, so this paper.",
            "OK, this paper is something we had a stats last year and then James."
        ],
        [
            "This one is actually come up with a way called collapse Deep GPS and this is a very exciting approximation because actually the heart of this approximation looks just like a deep neural network.",
            "But then there's a bunch of capacity control terms, so this sort of thing people you need to optimize here looks very much like a deep neural network apart from the basis functions are replaced by these inducing variables and that's kind of what kind of excited about that.",
            "And I'm going to show you a little bit of results.",
            "OK, yeah, this is so this."
        ],
        [
            "OK, so this is like let's not make this does or problem.",
            "OK people you know they all the Swiss roll problem?",
            "Is it dumb, silly problem that we're going to model the step function?",
            "But this is just to illustrate why Gaussian processes don't work very well.",
            "So this is a noisy step.",
            "And now the distribution of the derivatives in the Gaussian process is Gaussian, so they got the model, doesn't want to model it.",
            "So what it does is it brings the landscape very small.",
            "There's lots of proofs about oh, it will model it if you get infinite data, But basically you just get a nightmare.",
            "Running around very much between the dates of these blue shaded things, you can't see that well are samples from the GP weekend.",
            "Their samples by just by taking putting things on the inputs and sampling at the output.",
            "The reason we do it there is because we have to do it here when we go to the four layer model, we can't analytically compute the distribution anymore, and So what you see is this thing is that some quite steep angle.",
            "There's some sort of compromise.",
            "Now if we make a two layer deep GPS or not very deep, we get a bit better performance, but we still see these sort of artifacts that it wants to sort of do these things here.",
            "It's using the short length scales to do this.",
            "But slightly better.",
            "Funny thing though is if you go to former DGP on this.",
            "Now this is the key point.",
            "It's not.",
            "You know you might be thinking.",
            "Oh well, what's the point that because you can just do that with one sigmoid function you know and just squash it to make it sort of step.",
            "But what the model sang is I don't know outside the data.",
            "It's either one or the other is nonparametric.",
            "So if you're sampling from this model, it jumped and it jumps and it stays level because the degree distribution for this section here is very heavy tailed, so most of the time the derivatives are sort of close to 0.",
            "And in occasionally the derivatives are enormous, so it basically can create functions that look like Telegraph noise out of a Gaussian process.",
            "Which I think is really really nice.",
            "And it was actually David Delano and collaborators, who sort of prove that this was going on, and that made us say, oh, let's get the model to step function.",
            "And then there's this sort of question.",
            "They actually said it was a pathology, which I totally disagree with, and I take issue with that section of the paper, because actually the model doesn't have to do that.",
            "And the reason the model doesn't have to do that, we can see in the plot below, model chooses how does the model choose?",
            "Well, these are sort of input locations and they have to go.",
            "This is the Gaussian process that the model has at the first layer.",
            "So what the model does is it says I'm going to take these guys and I'm going to try and split them up so ignore the grade distributions.",
            "That's the mapping of the inducing points and I don't want to go too much detail about.",
            "Look at the black distributions, so this is the input locations and then at input things are being mapped either side and this is continued as you go through multiple layers until you get to a point here where your input.",
            "So what we're seeing the output from here is the input to the next Gaussian process.",
            "Yeah, so the output from here is the input to the next Gaussian process.",
            "And if you look at the black, what's going on is it's basically just pushing.",
            "Pushing these original continuous inputs out until there sort of on a very small region of the process and the length scale in this region is very very flat.",
            "So basically here it's flat, but if you make one small jump at this level across this point here you flip from being in this cluster to this cluster and that's why you see if you sample through the model you get this effect.",
            "David.",
            "Concerned about the so usually Gaussian processes and when you go outside the range of the data they have a nice certainly characterization and it looks like there may be underestimating uncertainty outside their data.",
            "Yeah, so here you've got quite a broad uncertainty that it could be across all those range, but in some sense this is just a multiplier over models by putting in.",
            "Four layers we just giving the model.",
            "The ability to have very highly her tales on its disagree distribution.",
            "So we you know in that sense we could potentially overfit because we're actually coming up with a more powerful model.",
            "So absolutely, but I think if on the other hand, if one were to say I knew that I had Telegraph noise.",
            "You know that I didn't.",
            "I had some sort of binary process.",
            "What you can see is you are modeling it with a series of cascade of Gaussian processes.",
            "Is that the usual DP with just sort of a stationary covariance function?",
            "That's very much a straw man, and there there are these other proposals out there like like multiresolution Gaussian processes and the nested Gaussian process, and very few of them allow you to have infinite numbers of jumps, so there may be unless you start getting nonparametric about where the jumps occur.",
            "So we've done work where like you include discontinuity's.",
            "It's very easy to say OK at this point we have a discontinuity and we moved to another thing, but you don't typically have parameterized that.",
            "And then you have to do one of two things.",
            "You have to say how many discontinuities you're going to look for, or you have to put a nonparametric prior of the number of discontinuity's that's going to be which kind of makes the model relatively complex.",
            "Here, the model is allowed.",
            "It's you know if we go off to Infinity here, it's still jumping around whatever you said here.",
            "It believes that this process goes on, so you know, definitely our process is, but it's just one thing it can do.",
            "I mean, those of other things.",
            "We can do other questions.",
            "OK, so this is another little example, which is.",
            "It's a, it's a one.",
            "It's a time as an input to aggression we're doing.",
            "We've got a data set which is someone walking around the University of Washington.",
            "Very nice electrical engineering, computer science building."
        ],
        [
            "It's Brian Ferris doing actually measuring honor some device.",
            "The strength the signal strength of Wi-Fi is that he can read and the idea is that you could navigate by doing this.",
            "This is some work we did collaboration with these folks and Brian some years ago, and this is the sort of true path.",
            "Now what we do here is we have a Gaussian process right at the top that says time is now the input and then we go through only a two layer Gaussian process cascade.",
            "So our input to the regression is time and our observation.",
            "Signal strength of all these Wi-Fi access points.",
            "But what we want to see, he walks around in a circle, and so this is known as loop closure.",
            "In robotics, we want to see that the model works out that he walks around in a circle.",
            "Now, because the Gaussian process is a prior at the top level, what we see at the top level does not look like the origin, because that isn't easy to get from a Gaussian process with an explanation of the quadratic covariance.",
            "It looks much more like that.",
            "So what then the model starts tending to do?",
            "Is it Maps to the next layer and starts putting commas in this sort of features that are genuinely in the data now?",
            "You might think that's not a brilliantly representation of that.",
            "Well, it's only a two layer model and the data is horribly noisy, and the nastiest bit of the noise is this, and we're not modeling this correctly in any sense.",
            "Away is that when the signal the way this device."
        ],
        [
            "Turned out to work is it could only store in cash.",
            "I don't know 6 Wi-Fi access points so it just drops the one that has the lowest value, so this isn't really 0 here.",
            "It's something up here.",
            "It's an observed and we're not telling the model that, but the model is still capable of going in and saying that that's what's going on.",
            "Fitting this very nasty nonstationary curve which would be very difficult to do with the standard Gaussian process."
        ],
        [
            "I mean, as David says, there are approaches, but it's you have to work OK. And then I guess this."
        ],
        [
            "Made it there's two more examples.",
            "This one.",
            "Here is an example where we have two datasets.",
            "This is of two people walking together and high fiving it's motion capture data.",
            "What we're doing here is we have a Gaussian process here, which is a priority is unsupervised learning, so we put a Gaussian prior over this space and then also maybe a dynamical prior.",
            "I can't remember.",
            "Maybe dynamical prior of this space and then what we do is we mapped to a new space which is shared between these two datasets.",
            "So each of these figures is controlled by a Gaussian process that has part of its inputs shared and part of its inputs not shared.",
            "This is approach we call MRD manifold relevance determination and it's really effective for to combining datasets together.",
            "So what you end up getting is a shared space which is affecting both characters and then some private information in each other character.",
            "So that's the silver status structure.",
            "Learning we can do 'cause this is structure learn.",
            "We didn't specify this apriori.",
            "The model learns that structure that there needs private and shared information."
        ],
        [
            "And then the fight."
        ],
        [
            "Example is another warmer where someone asked about the.",
            "Maybe without you, David or someone else that this is a hierarchy.",
            "This is the these args parameters that actually select the relevance of each feature and this is unsupervised learning task.",
            "Based here we've got very few digits.",
            "I think the 100 and or 200 or 300 digits.",
            "And we were interested in OK with only 300 digits.",
            "Do you need all these parameters?",
            "'cause you might say, well, you don't because the data set simple.",
            "I think there's really strong reasons why you do, but this is what we did.",
            "Is we fitted a model Anwer got a lower bound on the Bayesian evidence for the model and we try different depths of model and we end up selecting that we need a five layer model to model these digits.",
            "This visualization here is meant to encourage the fact that these low levels here when you're generating data you incur be very.",
            "Local features and actually the feature that is trying to show here is the oh closing and opening.",
            "These are the sort of things people show in deep learning papers.",
            "This level here you're encoding much more global features.",
            "So how thing how you move between A6 and a zero and at the top level here?",
            "By moving in this one dimension here so we only use 160.",
            "That's why you see another digit there at this top level here it's basically 1 dimensional and as you move along that one dimension your digit changes from one to six to zero.",
            "So that was a sort of sanity check that these sort of things that people see in deep learning and this is unsupervised.",
            "Learning, which is challenging to do with the regular deep learning methods, and this is actually the first thing we did.",
            "That's in Andrea's paper."
        ],
        [
            "OK, so very quickly this is the sort of thing we want to do.",
            "Why do we like models like this?",
            "Because I'm interested.",
            "I call this deep health, so I want to know everything about someone's health and Gaussian process is a beautiful for doing this because even if you've got missing data, it's really easy to handle missing data.",
            "If you take these nodes out for some individual fine, you can just handle it.",
            "And I very much believe that you need these hierarchies to sort of deal with the different scales of data.",
            "You know we're not doing that next week, but I can actually point to sections of this diagram that we've already done so.",
            "If you look at almost all our papers, they are some section of this diagram and the idea is to try and crew altogether.",
            "I mean, we model a genotype two RPG types of gene expression with modeling clinical notes.",
            "At the moment Alan saw my PhD student is working on survival analysis within Gaussian processes and I think all these things need integrating together, and I think that this might."
        ],
        [
            "A good way of doing it, so that's the end of that or depressing posts is there about allowed.",
            "They allow unsupervised and supervised deep learning.",
            "If you look at a lot of like there's all these blog posts is.",
            "If you follow Joshua on Google Plus or Jan, there's an enormous amount of debate about what the challenges are in deep learning and one that people keep saying is unsupervised learning.",
            "Multitask learning, these are all we know how to do them with these models, but we don't have the massive datasets shown up yet, so we really need to get much more engineering about what we're doing and workout how to get these things running on the scales that people run deep learning algorithms so we can scale this modeling.",
            "This is a massive data set."
        ],
        [
            "That's a everyone else seems to do, and that's it.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks to the organizers for the invitation to speak.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to talk today about, I guess the main thing we're working on in the Group 1.",
                    "label": 0
                },
                {
                    "sent": "One of several things, but it's a big thing that we're working on several people, which is a model sort of known as deep Gaussian.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Processes.",
                    "label": 0
                },
                {
                    "sent": "So I'll start with this sort of introduction to.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Honest, I actually got into machine learning 'cause I used to like drawing figures like this.",
                    "label": 0
                },
                {
                    "sent": "It seemed a little fun, much better than doing real math, so that was quite a long time ago in sort of 1996 and I have a real pleasure of sort of going to some of the talks talk by Jan in the machine Learning Generalization Workshop at Noon Institute where these models were sort of preeminent in terms of classification before support vector machine and these other things came along now since then.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I had all that beaten out of me and I was taught that actually all these things were really just linear algebra, so I'm going to use a sort of linear algebraic representation to recast this model in a different way and what I'm trying to show here is I've given some input and the task, maybe classification or regression, whatever, but I've got some input.",
                    "label": 0
                },
                {
                    "sent": "Let's say it's an image that I'm trying to classify, and what I do in a sort of neural network setting is I multiply that input by a matrix W and then I have some sort of nonlinear basis function set.",
                    "label": 0
                },
                {
                    "sent": "And that gives me an output H which I then apply the same treatment to cascading through the neural network.",
                    "label": 0
                },
                {
                    "sent": "So these are sort of adaptive basis function models and.",
                    "label": 0
                },
                {
                    "sent": "Why are they interesting?",
                    "label": 0
                },
                {
                    "sent": "Well, they're interesting 'cause you keep putting one layer on top of the other and what seems to be going on at the moment is people are showing more.",
                    "label": 0
                },
                {
                    "sent": "The more you do that actually the more power you often get with your modeling.",
                    "label": 0
                },
                {
                    "sent": "And this structure always sort of impressed me.",
                    "label": 0
                },
                {
                    "sent": "But actually what I ended up doing with something quite different, something quite flat now, so I'd represent that model in this form with these eight stands for hidden.",
                    "label": 0
                },
                {
                    "sent": "And W stands for weights.",
                    "label": 0
                },
                {
                    "sent": "That's the sort of machine learning time notation.",
                    "label": 0
                },
                {
                    "sent": "So mathematically we just have this.",
                    "label": 0
                },
                {
                    "sent": "We've got some input.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "X that goes through these nonlinear transfer functions after a matrix multiply and then so on so forth.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now what I want to point out and I'm not sure if they don't sign this Jan might say whether someone's done this, but actually one problem you're starting to get with these models, even with very large amounts of data they like to have a lot of nodes in these layers, so you tend to get these very large letters.",
                    "label": 0
                },
                {
                    "sent": "I know thousands of ages and then so you can end up with 1000 by 1000 parameters in this matrix W, and so you get an enormous amount of parameters and you can get problems with overfitting.",
                    "label": 0
                },
                {
                    "sent": "So sort of thing people have been suggesting is something called dropout, which is a scheme where you.",
                    "label": 0
                },
                {
                    "sent": "Only optimize some parameters at the time.",
                    "label": 0
                },
                {
                    "sent": "It's got.",
                    "label": 0
                },
                {
                    "sent": "Some interesting very good performance, but So what?",
                    "label": 0
                },
                {
                    "sent": "I'd point out is that it's actually very easy way of dealing with this parameter explosion, but I'm not sure if people have tried.",
                    "label": 0
                },
                {
                    "sent": "I know people have looked at deep networks, the parameters afterwards, and seeing that they actually have this structure.",
                    "label": 0
                },
                {
                    "sent": "There's work by Nando de Freitas on this where you say well rather than this massive matrix with maybe 1000 by 1000 thousand Rosa thousand columns rather than represent the whole thing you could sort of structure it with a singular value decomposition.",
                    "label": 0
                },
                {
                    "sent": "So you should say it has some inherent low rank form.",
                    "label": 0
                },
                {
                    "sent": "Now you here, I'm assuming these two orthonormal matrices, but actually in down here I'm dropping that assumption, I'm just assume their two matrices there.",
                    "label": 0
                },
                {
                    "sent": "First make the W is originally K1 by K2 or K1 and K2 of the width of the two of those layers very large thousands, and what we're going to do is we're just substitute that with the product of two matrices which is K1 by Q and K2 bike you.",
                    "label": 0
                },
                {
                    "sent": "So you get something which has many fewer parameters, because if Q is 2 you've just got sort of two times K 1 + 2 * K Two parameters.",
                    "label": 0
                },
                {
                    "sent": "So this is sort of common trick use.",
                    "label": 0
                },
                {
                    "sent": "It's in effect what principal component analysis is doing.",
                    "label": 0
                },
                {
                    "sent": "OK, so now I want to redraw that model.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This because what I'm saying now is you have something slightly different.",
                    "label": 0
                },
                {
                    "sent": "You've got an input layer and then you're going to multiply that input layer by a matrix with sort of dimensionality queue and then sort of project it down through a bottleneck and project it back up again so you don't have all these parameters in these matrices now because you've got this sort of odd low rank form.",
                    "label": 0
                },
                {
                    "sent": "Now, why do I like that?",
                    "label": 0
                },
                {
                    "sent": "Well, now I've sort of had to put more mass in in order to illustrate what's going on.",
                    "label": 0
                },
                {
                    "sent": "So at this first level here, I'm just multiplying the inputs by some.",
                    "label": 0
                },
                {
                    "sent": "I'm projecting down to some lower dimensional space and then I'm projecting back up again and then I'm continuing to do this, But the nice thing about that form is if I substitute.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You should've said threes back in here.",
                    "label": 0
                },
                {
                    "sent": "What I get is what I would recognize from.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "1996 as a series of what we would call multi lab set.",
                    "label": 0
                },
                {
                    "sent": "Well single their hidden layer perceptrons, just neural networks with one hidden.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, So what we're trying to do here is see a situation where we've gone from this structure here, which has all these basis functions mapping into each other.",
                    "label": 0
                },
                {
                    "sent": "So actually a cascade of separate basis function models.",
                    "label": 1
                },
                {
                    "sent": "So this is the structure in time to pull out of this thing to sort of show it's in there.",
                    "label": 0
                },
                {
                    "sent": "So you're going from sort of some inputs down, projecting and then you got some sort of new set of inputs which goes through nonlinear basis to a set of outputs.",
                    "label": 0
                },
                {
                    "sent": "So that's like a basis function model on its own, and this is like a basis function model on its own.",
                    "label": 0
                },
                {
                    "sent": "Of course this is more general than the thing we saw before, because you can always set.",
                    "label": 0
                },
                {
                    "sent": "These values such that you're recovering the full matrix.",
                    "label": 0
                },
                {
                    "sent": "You can always set Q to the appropriate value such that you're not losing any information between here.",
                    "label": 0
                },
                {
                    "sent": "Everyone sort of happy with that, so it's sort of a cascade of basis function models, yeah?",
                    "label": 0
                },
                {
                    "sent": "Now now I've got that.",
                    "label": 0
                },
                {
                    "sent": "What I want to do is throw that away.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Because what we did.",
                    "label": 0
                },
                {
                    "sent": "When I came into the field entrance by these interesting neural network models, which were like the brain and going to solve everything because Yan was managing to scan USPS codes at agency at the time.",
                    "label": 0
                },
                {
                    "sent": "Will this suddenly switched off from all this sort of thing instead of actually introduce these kernel methods?",
                    "label": 0
                },
                {
                    "sent": "And in my particular case, Gaussian process is where you should just throw that away and you actually start looking at the functions themselves.",
                    "label": 0
                },
                {
                    "sent": "So used to say instead of this parameterisation of our model, which is just an adaptive basis function model, so these are multi output adaptive basis function models 'cause you can change these use a fixed basis function model wouldn't allow you to change these nonlinearities, you just throw that away and you start looking at the functions directly.",
                    "label": 0
                },
                {
                    "sent": "Now the field went in two ways.",
                    "label": 0
                },
                {
                    "sent": "One of the ways was sort of a.",
                    "label": 0
                },
                {
                    "sent": "Reproducing kernel Hilbert space of objects like this, where you think about infinitor feature spaces and the other way of thinking about objects like this is thinking about them as Gaussian processes, which is the sort of way I went.",
                    "label": 0
                },
                {
                    "sent": "So the thing that I'm sort of proposing is that we look at this model, and I think why would we want to do that?",
                    "label": 0
                },
                {
                    "sent": "Well, number one.",
                    "label": 0
                },
                {
                    "sent": "It makes everything apparently computationally harder, which isn't a good reason to want to do it, so that's a down reason, but #2 these models have a lot nicer theoretical properties.",
                    "label": 0
                },
                {
                    "sent": "They're not easier to analyze and study.",
                    "label": 0
                },
                {
                    "sent": "Um and this actually very often you can encode this sort of information.",
                    "label": 0
                },
                {
                    "sent": "You might want to encode in these models.",
                    "label": 0
                },
                {
                    "sent": "They also have these interesting properties that you can see them as these neural network models.",
                    "label": 0
                },
                {
                    "sent": "If you put Gaussian priors over Vt here.",
                    "label": 0
                },
                {
                    "sent": "You don't actually have to put Gaussian prior of you, but if you put a Gaussian prior over Vt and then you extend the number of basis functions that you consider to Infinity, then these models are Gaussian processes.",
                    "label": 1
                },
                {
                    "sent": "So they sort of a what we're doing when we say we're going to use a Gaussian process is effectively with saying the width of these layers is infinite.",
                    "label": 0
                },
                {
                    "sent": "So we've got infinite basis functions in this model, so we've kind of moved the capacity.",
                    "label": 0
                },
                {
                    "sent": "Traditionally, people think about the capacity of these models as being in the number of basis functions you use, but I think Yang would argue you should always be using more and therefore to use Infinity sounds good.",
                    "label": 0
                },
                {
                    "sent": "Actually, it turns out if you use Infinity here, and if you use Infinity here, everything just becomes a Gaussian process again.",
                    "label": 0
                },
                {
                    "sent": "So if you make this infinite and this infinite, then you just get a Gaussian process with a particular covariance function, which is just the kernel of the kernel of the kernel of the things you put together.",
                    "label": 0
                },
                {
                    "sent": "So that sort of less interesting because as we'll see, Gaussian processes don't do everything that we want, and This is why we sort of look at this.",
                    "label": 0
                },
                {
                    "sent": "So from this perspective, what is a Gaussian process?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I've got a long slide.",
                    "label": 0
                },
                {
                    "sent": "First of all, I don't believe I should have to introduce Gaussian process anymore.",
                    "label": 0
                },
                {
                    "sent": "I think you should all learn what they are so I don't have to spend time introducing them.",
                    "label": 0
                },
                {
                    "sent": "I mean people don't have to use linear regression and they are such fundamental objects.",
                    "label": 0
                },
                {
                    "sent": "They really are.",
                    "label": 0
                },
                {
                    "sent": "And it's shocking how little is known about them.",
                    "label": 0
                },
                {
                    "sent": "But so I always have to spend time doing it, so I've kind of reduced it to a slide which is just stolen off one of my postdocs, Nicola Duran, who did it nicely in one slide.",
                    "label": 0
                },
                {
                    "sent": "So in a Gaussian process you've got an object that is capable of generating functions and inferencing.",
                    "label": 0
                },
                {
                    "sent": "Gaussian process is just says, well, I have a prior distribution that allows me to generate as many functions as I like, and they have infinite basis.",
                    "label": 0
                },
                {
                    "sent": "This is what's going on here.",
                    "label": 0
                },
                {
                    "sent": "We haven't specified where the bases are across the whole line.",
                    "label": 0
                },
                {
                    "sent": "They would look like this.",
                    "label": 0
                },
                {
                    "sent": "There stationary processes, and in this case the chosen to be infinitely differentiable as well.",
                    "label": 0
                },
                {
                    "sent": "So if we generate many, many of these functions, what we do when we're doing?",
                    "label": 0
                },
                {
                    "sent": "It's in Gaussian process is we try and generate what this is Bayesian inference in general perspective from a probabilistic programming perspective.",
                    "label": 0
                },
                {
                    "sent": "You have some way of generating functions.",
                    "label": 0
                },
                {
                    "sent": "In this case, it's assuming that the correlations between the points on the functions are captured by gaussianity, cider, the distribution between the points sample from the functions are Gaussian across the points.",
                    "label": 0
                },
                {
                    "sent": "An inference in this process involves combining that with data.",
                    "label": 0
                },
                {
                    "sent": "So here's three data points and all Bayesian inference is just saying I've got a method of generating something.",
                    "label": 0
                },
                {
                    "sent": "Some data that I observe my posterior that I'm interested in is to retain the functions that are consistent with those points.",
                    "label": 0
                },
                {
                    "sent": "Now that's Bayesian inference.",
                    "label": 0
                },
                {
                    "sent": "In general, Gaussian process is a beautiful, because this process is sort of trivial, it's just linear algebra.",
                    "label": 0
                },
                {
                    "sent": "For most Bayesian models, it's a nightmare to go from this set of functions to this.",
                    "label": 0
                },
                {
                    "sent": "I mean, you can do it by sampling, but if you wanna do it mathematically, it can be a real problem in the Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "If this is all you're doing, this is just linear algebra operations, which is why there such beautiful models to work with.",
                    "label": 0
                },
                {
                    "sent": "So of course we're going to mess all that up.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because what we're going to do.",
                    "label": 0
                },
                {
                    "sent": "Is we're going to make it so everything is nonline?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this to me is a deep model.",
                    "label": 0
                },
                {
                    "sent": "That's all that's going on.",
                    "label": 0
                },
                {
                    "sent": "It's just a composite function function of a function of a function of a function of a function.",
                    "label": 0
                },
                {
                    "sent": "So you might think, well, what's the point in having a function of the function of the function of function and modeling all the individual functions when you can just model the final function well.",
                    "label": 0
                },
                {
                    "sent": "The reason you want to do that is because this function may have very complex properties which are well captured by Gaussian processes and will see that as an example later.",
                    "label": 0
                },
                {
                    "sent": "There's actually many reasons why you want to do that.",
                    "label": 0
                },
                {
                    "sent": "You can introduce the sort of things they do in the deep learning community.",
                    "label": 0
                },
                {
                    "sent": "You can introduce structure about how you expect these convolution networks about translation invariants.",
                    "label": 0
                },
                {
                    "sent": "In these functions you can put all sorts of prior knowledge in these functions, ignoring everything they do, even if you are interested in Gaussian process, is in.",
                    "label": 0
                },
                {
                    "sent": "Even if you are interested in all that and you're just interested in mapping from one function to another, this is a fun thing to do because it turns out I can get a much richer, richer class of distributions by doing this and placing Gaussian process priors over each of these functions now.",
                    "label": 0
                },
                {
                    "sent": "For example, so why deep?",
                    "label": 0
                },
                {
                    "sent": "And this is a sort of work by David Governor.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Linda and colleagues at Cambridge, including Zubin and Ryan Adams in Harvard, which highlights this nice feature of these models.",
                    "label": 0
                },
                {
                    "sent": "The derivatives of a Gaussian process are also Gaussian distributed, so if you want to look at the sort of it's a really nice feature so you can look at the joint distribution over the derivatives and the function itself.",
                    "label": 0
                },
                {
                    "sent": "And the derivatives are also Gaussian distributed, but as soon as you know that you know that you're not going to be able to model jumps with them, because if you've got a function, whether jumps you don't have Gaussian distributed derivatives, you've got very heavy tailed.",
                    "label": 0
                },
                {
                    "sent": "You've got sort of does it most of the time derivative zero and then you get a jump where it's large, so you basically know you're not going to be able to jump with the Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "Now in this nice piece of work that was I stats in Iceland.",
                    "label": 0
                },
                {
                    "sent": "What they showed is if you start stacking these models as you stack them, you increase the.",
                    "label": 0
                },
                {
                    "sent": "If you look at the tails of the distribution, you increase the tails.",
                    "label": 0
                },
                {
                    "sent": "You get heavier tailed derivative distributions and I'll show an example of that being fitted later in that people they didn't actually do any, so they just looked at theory of the models they didn't actually run.",
                    "label": 0
                },
                {
                    "sent": "The models, but we've got a little example that we presented in a late breaking poster at that conference done by by done by James Henslin, who's working with me on this where he actually fits.",
                    "label": 0
                },
                {
                    "sent": "This is the model, and I'll show that in a moment.",
                    "label": 0
                },
                {
                    "sent": "So that's one reason there's many reasons, but I just think we've got an example for that reason, so I just sort of include that.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now what's the problem with doing this?",
                    "label": 0
                },
                {
                    "sent": "Well, the problem we're doing this?",
                    "label": 0
                },
                {
                    "sent": "I mean what you want to do in this sort of thing is you're taking some space over which replacing a distribution.",
                    "label": 0
                },
                {
                    "sent": "And in fact, it's a Gaussian process distribution and you're feeding into another potentially nonlinear space.",
                    "label": 0
                },
                {
                    "sent": "So this is a bit of a nightmare because what you're doing is, let's say, so.",
                    "label": 0
                },
                {
                    "sent": "One of the things we can do very easily with these models is unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "So instead of having input.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we can just place Gaussian priors over these models and build latent variable models.",
                    "label": 0
                },
                {
                    "sent": "So consider that set up and assume I've got a Gaussian prior over this space here and then I project it through a nonlinear function to 3 dimensional space.",
                    "label": 1
                },
                {
                    "sent": "The resulting distribution is obviously very non tractable.",
                    "label": 0
                },
                {
                    "sent": "In this example, one D21 D, you get the same sort of effect.",
                    "label": 0
                },
                {
                    "sent": "I start with the Gaussian distribution and Gaussian.",
                    "label": 0
                },
                {
                    "sent": "This process is always output Gaussian.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Marginal distributions, so you're always having to be propagating through this thing.",
                    "label": 0
                },
                {
                    "sent": "A series of Gaussian distributions, and they always come out the end looking ugly.",
                    "label": 0
                },
                {
                    "sent": "So I mean, that's just a visualization of the intractability, is how they actually occur in the model is even nastier.",
                    "label": 0
                },
                {
                    "sent": "What you actually end up doing is trying to look at how you've got to propagate a Gaussian distribution through a matrix inverse in a kind of nasty way.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So on.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of that, you've got some problems of complexity, so in Gaussian process is the standard complexity is N cubed.",
                    "label": 0
                },
                {
                    "sent": "In general?",
                    "label": 0
                },
                {
                    "sent": "I mean you can come up with lots of ways of getting around this for specific covariance functions, but in general it's N cubed and squared in storage, so that's a nightmare because the sort of things that they do with these deep models involve billions of data and this makes it very difficult to compete, so there's two sort of problems with this modeling approach.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now both of these problems are actually solved by going down this road, which originally was suggested by my postdoc McCullough City as when he was in Manchester with me.",
                    "label": 0
                },
                {
                    "sent": "So what you do in this case is you actually build a variational approximation.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go through the details of the variational approximation 'cause that on its own would take more than 25 minutes, and I don't have much time left, but I'm going to try and give you a sort of hand WAVY idea.",
                    "label": 0
                },
                {
                    "sent": "These approximations work in the following way.",
                    "label": 0
                },
                {
                    "sent": "Gaussian processes are nonparametric models and when you have a nonparametric model, they're very powerful, flexible models.",
                    "label": 0
                },
                {
                    "sent": "But you'll start with this problem that you actually have to remember all the data and used all the data at all times, which is a bit of a downside because your data goes up for big data applications.",
                    "label": 0
                },
                {
                    "sent": "You're remembering an awful lot of things.",
                    "label": 0
                },
                {
                    "sent": "These inducing valuable approximations we've sort of come up with the term for this, which we're calling variational compression are compression schemes.",
                    "label": 1
                },
                {
                    "sent": "You can very clearly see their compression schemes, where instead of remembering the whole data set, you remember a fixed number of observations from the data, but they aren't real observations from the data you consider optimize over them.",
                    "label": 0
                },
                {
                    "sent": "You can change them.",
                    "label": 0
                },
                {
                    "sent": "You can choose to remember something you didn't see.",
                    "label": 0
                },
                {
                    "sent": "You can even choose to remember under these schemes.",
                    "label": 0
                },
                {
                    "sent": "Choose remember the frequency of the signal even though you never directly observed it, because it turns out that a Gaussian process because 3A transform is a linear operation.",
                    "label": 0
                },
                {
                    "sent": "You could choose to make your observation in Fourier space that you remember, even though you never saw an observation in that space, so quite powerful flexible methods.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that if you use these methods, you can significantly reduce complexity.",
                    "label": 0
                },
                {
                    "sent": "That's one thing that happens, and then the other thing that happens, which is very exciting.",
                    "label": 0
                },
                {
                    "sent": "Is that they render the likelihoods that you play with on the certain conditioning, independent across the data.",
                    "label": 0
                },
                {
                    "sent": "This is a major challenge for a Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "Is that normally when you look at your likelihood, people used to independent likelihoods and pretty much every algorithm for scaling up model to very very large data relies on independence of likelihoods, stochastic gradient, descent.",
                    "label": 0
                },
                {
                    "sent": "There's many ways you can paralyze things if you got independence of likelihood, but it turns out in this model, if you condition on these inducing variables in this approximation.",
                    "label": 0
                },
                {
                    "sent": "You get a likelihood that condition on those inducing variables is independent.",
                    "label": 0
                },
                {
                    "sent": "And that means I'm not giving you the details here that you can start applying all these techniques that people like, Dan, Rob, Fergus.",
                    "label": 0
                },
                {
                    "sent": "All these people have been applying to these very large models, so it's kind of exciting because it means well and alongside it.",
                    "label": 0
                },
                {
                    "sent": "It also turns out that you can start doing variational approximations of this sort of deep Gaussian process model I just described.",
                    "label": 0
                },
                {
                    "sent": "So we're very excited about this and people are kind of starting to work on this, so we sort of talked about Gaussian process for big data last year.",
                    "label": 0
                },
                {
                    "sent": "I think it you AI.",
                    "label": 0
                },
                {
                    "sent": "James Hensman, parallelization work coming out of Cambridge.",
                    "label": 0
                },
                {
                    "sent": "Yaron Garland, collaborators and my own post Doc Gen.",
                    "label": 0
                },
                {
                    "sent": "When dye is looking at using GPU's to speed these things up.",
                    "label": 0
                },
                {
                    "sent": "So we're finally in the Gaussian process world getting around to actually looking at some of the things that people do to look at serious data.",
                    "label": 0
                },
                {
                    "sent": "So this is an idea of how these sort of models look.",
                    "label": 0
                },
                {
                    "sent": "If you take a square and you propagate it through a random Gaussian process, these are random function draws.",
                    "label": 0
                },
                {
                    "sent": "Each of these levels you can sort of get these.",
                    "label": 0
                },
                {
                    "sent": "Distortions.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The square and the thing to notice here is if once you put this distortion in it, it stays in all the time.",
                    "label": 0
                },
                {
                    "sent": "You can't sort of undo it, so you can get these.",
                    "label": 0
                },
                {
                    "sent": "They're very powerful for multi task learning.",
                    "label": 0
                },
                {
                    "sent": "You can have some sort of general distortion at the top and then you can have different GPS coming out to model different datasets.",
                    "label": 0
                },
                {
                    "sent": "I'll show brief exam.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All of that, so this paper.",
                    "label": 0
                },
                {
                    "sent": "OK, this paper is something we had a stats last year and then James.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This one is actually come up with a way called collapse Deep GPS and this is a very exciting approximation because actually the heart of this approximation looks just like a deep neural network.",
                    "label": 0
                },
                {
                    "sent": "But then there's a bunch of capacity control terms, so this sort of thing people you need to optimize here looks very much like a deep neural network apart from the basis functions are replaced by these inducing variables and that's kind of what kind of excited about that.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to show you a little bit of results.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah, this is so this.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is like let's not make this does or problem.",
                    "label": 0
                },
                {
                    "sent": "OK people you know they all the Swiss roll problem?",
                    "label": 0
                },
                {
                    "sent": "Is it dumb, silly problem that we're going to model the step function?",
                    "label": 0
                },
                {
                    "sent": "But this is just to illustrate why Gaussian processes don't work very well.",
                    "label": 0
                },
                {
                    "sent": "So this is a noisy step.",
                    "label": 0
                },
                {
                    "sent": "And now the distribution of the derivatives in the Gaussian process is Gaussian, so they got the model, doesn't want to model it.",
                    "label": 1
                },
                {
                    "sent": "So what it does is it brings the landscape very small.",
                    "label": 0
                },
                {
                    "sent": "There's lots of proofs about oh, it will model it if you get infinite data, But basically you just get a nightmare.",
                    "label": 0
                },
                {
                    "sent": "Running around very much between the dates of these blue shaded things, you can't see that well are samples from the GP weekend.",
                    "label": 0
                },
                {
                    "sent": "Their samples by just by taking putting things on the inputs and sampling at the output.",
                    "label": 0
                },
                {
                    "sent": "The reason we do it there is because we have to do it here when we go to the four layer model, we can't analytically compute the distribution anymore, and So what you see is this thing is that some quite steep angle.",
                    "label": 0
                },
                {
                    "sent": "There's some sort of compromise.",
                    "label": 0
                },
                {
                    "sent": "Now if we make a two layer deep GPS or not very deep, we get a bit better performance, but we still see these sort of artifacts that it wants to sort of do these things here.",
                    "label": 0
                },
                {
                    "sent": "It's using the short length scales to do this.",
                    "label": 0
                },
                {
                    "sent": "But slightly better.",
                    "label": 0
                },
                {
                    "sent": "Funny thing though is if you go to former DGP on this.",
                    "label": 0
                },
                {
                    "sent": "Now this is the key point.",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "You know you might be thinking.",
                    "label": 0
                },
                {
                    "sent": "Oh well, what's the point that because you can just do that with one sigmoid function you know and just squash it to make it sort of step.",
                    "label": 0
                },
                {
                    "sent": "But what the model sang is I don't know outside the data.",
                    "label": 0
                },
                {
                    "sent": "It's either one or the other is nonparametric.",
                    "label": 0
                },
                {
                    "sent": "So if you're sampling from this model, it jumped and it jumps and it stays level because the degree distribution for this section here is very heavy tailed, so most of the time the derivatives are sort of close to 0.",
                    "label": 0
                },
                {
                    "sent": "And in occasionally the derivatives are enormous, so it basically can create functions that look like Telegraph noise out of a Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "Which I think is really really nice.",
                    "label": 0
                },
                {
                    "sent": "And it was actually David Delano and collaborators, who sort of prove that this was going on, and that made us say, oh, let's get the model to step function.",
                    "label": 0
                },
                {
                    "sent": "And then there's this sort of question.",
                    "label": 0
                },
                {
                    "sent": "They actually said it was a pathology, which I totally disagree with, and I take issue with that section of the paper, because actually the model doesn't have to do that.",
                    "label": 1
                },
                {
                    "sent": "And the reason the model doesn't have to do that, we can see in the plot below, model chooses how does the model choose?",
                    "label": 0
                },
                {
                    "sent": "Well, these are sort of input locations and they have to go.",
                    "label": 0
                },
                {
                    "sent": "This is the Gaussian process that the model has at the first layer.",
                    "label": 0
                },
                {
                    "sent": "So what the model does is it says I'm going to take these guys and I'm going to try and split them up so ignore the grade distributions.",
                    "label": 0
                },
                {
                    "sent": "That's the mapping of the inducing points and I don't want to go too much detail about.",
                    "label": 0
                },
                {
                    "sent": "Look at the black distributions, so this is the input locations and then at input things are being mapped either side and this is continued as you go through multiple layers until you get to a point here where your input.",
                    "label": 0
                },
                {
                    "sent": "So what we're seeing the output from here is the input to the next Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so the output from here is the input to the next Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "And if you look at the black, what's going on is it's basically just pushing.",
                    "label": 0
                },
                {
                    "sent": "Pushing these original continuous inputs out until there sort of on a very small region of the process and the length scale in this region is very very flat.",
                    "label": 0
                },
                {
                    "sent": "So basically here it's flat, but if you make one small jump at this level across this point here you flip from being in this cluster to this cluster and that's why you see if you sample through the model you get this effect.",
                    "label": 1
                },
                {
                    "sent": "David.",
                    "label": 0
                },
                {
                    "sent": "Concerned about the so usually Gaussian processes and when you go outside the range of the data they have a nice certainly characterization and it looks like there may be underestimating uncertainty outside their data.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so here you've got quite a broad uncertainty that it could be across all those range, but in some sense this is just a multiplier over models by putting in.",
                    "label": 0
                },
                {
                    "sent": "Four layers we just giving the model.",
                    "label": 0
                },
                {
                    "sent": "The ability to have very highly her tales on its disagree distribution.",
                    "label": 0
                },
                {
                    "sent": "So we you know in that sense we could potentially overfit because we're actually coming up with a more powerful model.",
                    "label": 0
                },
                {
                    "sent": "So absolutely, but I think if on the other hand, if one were to say I knew that I had Telegraph noise.",
                    "label": 0
                },
                {
                    "sent": "You know that I didn't.",
                    "label": 0
                },
                {
                    "sent": "I had some sort of binary process.",
                    "label": 0
                },
                {
                    "sent": "What you can see is you are modeling it with a series of cascade of Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "Is that the usual DP with just sort of a stationary covariance function?",
                    "label": 0
                },
                {
                    "sent": "That's very much a straw man, and there there are these other proposals out there like like multiresolution Gaussian processes and the nested Gaussian process, and very few of them allow you to have infinite numbers of jumps, so there may be unless you start getting nonparametric about where the jumps occur.",
                    "label": 0
                },
                {
                    "sent": "So we've done work where like you include discontinuity's.",
                    "label": 0
                },
                {
                    "sent": "It's very easy to say OK at this point we have a discontinuity and we moved to another thing, but you don't typically have parameterized that.",
                    "label": 0
                },
                {
                    "sent": "And then you have to do one of two things.",
                    "label": 0
                },
                {
                    "sent": "You have to say how many discontinuities you're going to look for, or you have to put a nonparametric prior of the number of discontinuity's that's going to be which kind of makes the model relatively complex.",
                    "label": 0
                },
                {
                    "sent": "Here, the model is allowed.",
                    "label": 0
                },
                {
                    "sent": "It's you know if we go off to Infinity here, it's still jumping around whatever you said here.",
                    "label": 0
                },
                {
                    "sent": "It believes that this process goes on, so you know, definitely our process is, but it's just one thing it can do.",
                    "label": 1
                },
                {
                    "sent": "I mean, those of other things.",
                    "label": 0
                },
                {
                    "sent": "We can do other questions.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is another little example, which is.",
                    "label": 0
                },
                {
                    "sent": "It's a, it's a one.",
                    "label": 0
                },
                {
                    "sent": "It's a time as an input to aggression we're doing.",
                    "label": 0
                },
                {
                    "sent": "We've got a data set which is someone walking around the University of Washington.",
                    "label": 0
                },
                {
                    "sent": "Very nice electrical engineering, computer science building.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's Brian Ferris doing actually measuring honor some device.",
                    "label": 0
                },
                {
                    "sent": "The strength the signal strength of Wi-Fi is that he can read and the idea is that you could navigate by doing this.",
                    "label": 0
                },
                {
                    "sent": "This is some work we did collaboration with these folks and Brian some years ago, and this is the sort of true path.",
                    "label": 0
                },
                {
                    "sent": "Now what we do here is we have a Gaussian process right at the top that says time is now the input and then we go through only a two layer Gaussian process cascade.",
                    "label": 0
                },
                {
                    "sent": "So our input to the regression is time and our observation.",
                    "label": 0
                },
                {
                    "sent": "Signal strength of all these Wi-Fi access points.",
                    "label": 0
                },
                {
                    "sent": "But what we want to see, he walks around in a circle, and so this is known as loop closure.",
                    "label": 0
                },
                {
                    "sent": "In robotics, we want to see that the model works out that he walks around in a circle.",
                    "label": 0
                },
                {
                    "sent": "Now, because the Gaussian process is a prior at the top level, what we see at the top level does not look like the origin, because that isn't easy to get from a Gaussian process with an explanation of the quadratic covariance.",
                    "label": 0
                },
                {
                    "sent": "It looks much more like that.",
                    "label": 0
                },
                {
                    "sent": "So what then the model starts tending to do?",
                    "label": 0
                },
                {
                    "sent": "Is it Maps to the next layer and starts putting commas in this sort of features that are genuinely in the data now?",
                    "label": 0
                },
                {
                    "sent": "You might think that's not a brilliantly representation of that.",
                    "label": 0
                },
                {
                    "sent": "Well, it's only a two layer model and the data is horribly noisy, and the nastiest bit of the noise is this, and we're not modeling this correctly in any sense.",
                    "label": 0
                },
                {
                    "sent": "Away is that when the signal the way this device.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Turned out to work is it could only store in cash.",
                    "label": 0
                },
                {
                    "sent": "I don't know 6 Wi-Fi access points so it just drops the one that has the lowest value, so this isn't really 0 here.",
                    "label": 0
                },
                {
                    "sent": "It's something up here.",
                    "label": 0
                },
                {
                    "sent": "It's an observed and we're not telling the model that, but the model is still capable of going in and saying that that's what's going on.",
                    "label": 0
                },
                {
                    "sent": "Fitting this very nasty nonstationary curve which would be very difficult to do with the standard Gaussian process.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I mean, as David says, there are approaches, but it's you have to work OK. And then I guess this.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Made it there's two more examples.",
                    "label": 0
                },
                {
                    "sent": "This one.",
                    "label": 0
                },
                {
                    "sent": "Here is an example where we have two datasets.",
                    "label": 0
                },
                {
                    "sent": "This is of two people walking together and high fiving it's motion capture data.",
                    "label": 1
                },
                {
                    "sent": "What we're doing here is we have a Gaussian process here, which is a priority is unsupervised learning, so we put a Gaussian prior over this space and then also maybe a dynamical prior.",
                    "label": 0
                },
                {
                    "sent": "I can't remember.",
                    "label": 0
                },
                {
                    "sent": "Maybe dynamical prior of this space and then what we do is we mapped to a new space which is shared between these two datasets.",
                    "label": 0
                },
                {
                    "sent": "So each of these figures is controlled by a Gaussian process that has part of its inputs shared and part of its inputs not shared.",
                    "label": 0
                },
                {
                    "sent": "This is approach we call MRD manifold relevance determination and it's really effective for to combining datasets together.",
                    "label": 0
                },
                {
                    "sent": "So what you end up getting is a shared space which is affecting both characters and then some private information in each other character.",
                    "label": 0
                },
                {
                    "sent": "So that's the silver status structure.",
                    "label": 0
                },
                {
                    "sent": "Learning we can do 'cause this is structure learn.",
                    "label": 0
                },
                {
                    "sent": "We didn't specify this apriori.",
                    "label": 0
                },
                {
                    "sent": "The model learns that structure that there needs private and shared information.",
                    "label": 1
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then the fight.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Example is another warmer where someone asked about the.",
                    "label": 0
                },
                {
                    "sent": "Maybe without you, David or someone else that this is a hierarchy.",
                    "label": 0
                },
                {
                    "sent": "This is the these args parameters that actually select the relevance of each feature and this is unsupervised learning task.",
                    "label": 0
                },
                {
                    "sent": "Based here we've got very few digits.",
                    "label": 0
                },
                {
                    "sent": "I think the 100 and or 200 or 300 digits.",
                    "label": 0
                },
                {
                    "sent": "And we were interested in OK with only 300 digits.",
                    "label": 0
                },
                {
                    "sent": "Do you need all these parameters?",
                    "label": 0
                },
                {
                    "sent": "'cause you might say, well, you don't because the data set simple.",
                    "label": 1
                },
                {
                    "sent": "I think there's really strong reasons why you do, but this is what we did.",
                    "label": 0
                },
                {
                    "sent": "Is we fitted a model Anwer got a lower bound on the Bayesian evidence for the model and we try different depths of model and we end up selecting that we need a five layer model to model these digits.",
                    "label": 1
                },
                {
                    "sent": "This visualization here is meant to encourage the fact that these low levels here when you're generating data you incur be very.",
                    "label": 0
                },
                {
                    "sent": "Local features and actually the feature that is trying to show here is the oh closing and opening.",
                    "label": 0
                },
                {
                    "sent": "These are the sort of things people show in deep learning papers.",
                    "label": 0
                },
                {
                    "sent": "This level here you're encoding much more global features.",
                    "label": 0
                },
                {
                    "sent": "So how thing how you move between A6 and a zero and at the top level here?",
                    "label": 0
                },
                {
                    "sent": "By moving in this one dimension here so we only use 160.",
                    "label": 0
                },
                {
                    "sent": "That's why you see another digit there at this top level here it's basically 1 dimensional and as you move along that one dimension your digit changes from one to six to zero.",
                    "label": 0
                },
                {
                    "sent": "So that was a sort of sanity check that these sort of things that people see in deep learning and this is unsupervised.",
                    "label": 0
                },
                {
                    "sent": "Learning, which is challenging to do with the regular deep learning methods, and this is actually the first thing we did.",
                    "label": 0
                },
                {
                    "sent": "That's in Andrea's paper.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so very quickly this is the sort of thing we want to do.",
                    "label": 0
                },
                {
                    "sent": "Why do we like models like this?",
                    "label": 0
                },
                {
                    "sent": "Because I'm interested.",
                    "label": 0
                },
                {
                    "sent": "I call this deep health, so I want to know everything about someone's health and Gaussian process is a beautiful for doing this because even if you've got missing data, it's really easy to handle missing data.",
                    "label": 0
                },
                {
                    "sent": "If you take these nodes out for some individual fine, you can just handle it.",
                    "label": 0
                },
                {
                    "sent": "And I very much believe that you need these hierarchies to sort of deal with the different scales of data.",
                    "label": 0
                },
                {
                    "sent": "You know we're not doing that next week, but I can actually point to sections of this diagram that we've already done so.",
                    "label": 0
                },
                {
                    "sent": "If you look at almost all our papers, they are some section of this diagram and the idea is to try and crew altogether.",
                    "label": 0
                },
                {
                    "sent": "I mean, we model a genotype two RPG types of gene expression with modeling clinical notes.",
                    "label": 0
                },
                {
                    "sent": "At the moment Alan saw my PhD student is working on survival analysis within Gaussian processes and I think all these things need integrating together, and I think that this might.",
                    "label": 1
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A good way of doing it, so that's the end of that or depressing posts is there about allowed.",
                    "label": 0
                },
                {
                    "sent": "They allow unsupervised and supervised deep learning.",
                    "label": 0
                },
                {
                    "sent": "If you look at a lot of like there's all these blog posts is.",
                    "label": 0
                },
                {
                    "sent": "If you follow Joshua on Google Plus or Jan, there's an enormous amount of debate about what the challenges are in deep learning and one that people keep saying is unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "Multitask learning, these are all we know how to do them with these models, but we don't have the massive datasets shown up yet, so we really need to get much more engineering about what we're doing and workout how to get these things running on the scales that people run deep learning algorithms so we can scale this modeling.",
                    "label": 0
                },
                {
                    "sent": "This is a massive data set.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's a everyone else seems to do, and that's it.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}