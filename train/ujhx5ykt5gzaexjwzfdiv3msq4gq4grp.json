{
    "id": "ujhx5ykt5gzaexjwzfdiv3msq4gq4grp",
    "title": "Fast learning of Document Ranking Functions with the Committee Perceptron",
    "info": {
        "author": [
            "Jonathan Elsas, Language Technologies Institute, Carnegie Mellon University"
        ],
        "published": "Feb. 25, 2008",
        "recorded": "February 2008",
        "category": [
            "Top->Computer Science->Information Retrieval"
        ]
    },
    "url": "http://videolectures.net/wsdm08_elsas_fldr/",
    "segmentation": [
        [
            "I'm John alsys.",
            "Here are my."
        ],
        [
            "Collaborators Victor and Jamie."
        ],
        [
            "So first I'd like to talk about a little bit about features in information."
        ],
        [
            "Retrieval early retrieval systems were basically exact match unranked Boolean which was not as effective as people would have liked, so more sophisticated features were started."
        ],
        [
            "We're working to retrieval algorithms like term frequency, inverse document frequency with algorithms like BM 25."
        ],
        [
            "As collections evolved, more features became useful, like link structure, anchor text, document structure."
        ],
        [
            "And now we have a slew of features that may be useful for retrieval of things like social annotations or clickthrough data."
        ],
        [
            "Just to clarify what I'm talking about, a few of these example features."
        ],
        [
            "Are things like raw term frequency of the query terms in a document?",
            "Query."
        ],
        [
            "Hood in a language modeling framework."
        ],
        [
            "Say query likelihood restricted to the title of Doc."
        ],
        [
            "Events or query independent features like page rank.",
            "So all of these can be represented by a real valued function over the query in the document.",
            "We aren't talking about the whole vocabulary here."
        ],
        [
            "So the question is, how do we use these features?"
        ],
        [
            "There's many features used in real world search engines.",
            "Probably in the hundreds."
        ],
        [
            "And ideally we'd like to adapt these feature weights across users in a cross across tasks.",
            "The question is how do we wait these effectively for for better retrieval?"
        ],
        [
            "And the solution that we pursue here is a machine learning approach where we learn from previous queries and relevance judgments."
        ],
        [
            "This isn't this isn't a new solution to the problem brank learning or learning to rank has recently become a popular search area, and here's a few of the.",
            "Recent algorithms proposed."
        ],
        [
            "This task.",
            "Our approach here takes a pairwise preference approach.",
            "So in this scenario, our training data consists of query document pairs where there's some preference ordering over the pairs of documents for the given query, and the goal is to learn a function that preserves that preference ordering so it produces a higher score for the more preferred documents and lower score for the less preferred and in this sense it reduces to a classification problem over pairs of documents.",
            "So a positive correct classification would be ranking the two documents.",
            "Correctly and incorrect classification would be the opposite."
        ],
        [
            "The goal in this scenario is to minimize the number of Miss Rank document pairs.",
            "So this is given by the simple 01 loss function over document pairs.",
            "We incur some loss whenever document there is miss."
        ],
        [
            "Ranked.",
            "There are some advantages to this approach to learning ranking functions, so most classification algorithms can be adapted to this task, and it generalizes to any type of relevance information.",
            "Any preference information, any type of graded relevance levels, or any full or partial ordering of the data.",
            "And there's some evidence that this pairwise preference assessment is easier for assessors.",
            "We"
        ],
        [
            "Show in this paper to the minimizing the number of miss ranked pairs places a lower bound on many of the widely used retrieval performance measures like mean average precision or are."
        ],
        [
            "Position.",
            "So in our setting, we're concerned with linear ranking functions, so our documents are represented by a. Vector of real valued feature functions and these are features that I was talking about earlier, like raw term frequency or page rank.",
            "And we're learning a linear scoring function, so a simple dot product between a weight vector and these document vectors.",
            "We can rewrite our loss function as some hinge loss in this in this case."
        ],
        [
            "The algorithm we're presenting here is a variant of the perceptron algorithm in older algorithm, originally proposed in 1958.",
            "It's an online algorithm which implies fast learner and low memory requirements.",
            "We only are concerned with one instance or one document pair at a time and we update our current hypothesis or weight vector whenever we Miss Rank, a pair of documents.",
            "This algorithm has some appealing theoretic properties, so if the data set is linearly separable, the algorithms guaranteed converge on a solution that separates the data.",
            "But where we're concerned with here is that this algorithm can scale to larger datasets."
        ],
        [
            "So remember our pairwise preference loss function is given up at the top and the Perceptron algorithm learns by applying this iterative additive update rule whenever a pair of documents is miss ranked.",
            "As an online algorithm, it can't directly minimize this pairwise loss function, but because of the bounds, the provable bounds of the perceptron algorithm we can.",
            "Minimize the number of Miss rankings we make during training."
        ],
        [
            "So I'll walk through how the Perceptron algorithm learns.",
            "Given a stream of document pair examples.",
            "On the left we have our training data are document pairs and on the right we have some sequence of previously learned hypothesis.",
            "So these weight vectors here on the right are represented just as a vector in a 2D plane.",
            "So say we're learning to combine two different features for effective retrieval.",
            "They also have some success counter or some quality measure associated with them represented by stars here.",
            "And in the middle we have our current hypothesis."
        ],
        [
            "So this training proceeds we look at."
        ],
        [
            "Our next document pair in our stream and projected into our.",
            "2D feature space in this case."
        ],
        [
            "We scored these documents and in this case we've made a correct ranking decision.",
            "And."
        ],
        [
            "We increment a success counter on this hypothesis."
        ],
        [
            "We move on to the next document pair."
        ],
        [
            "Again.",
            "Project these days."
        ],
        [
            "Comments and score them when you make another correct ranking decision.",
            "Training proceeds and say."
        ],
        [
            "On the next.",
            "Document."
        ],
        [
            "There we incur."
        ],
        [
            "Only rank these.",
            "In this case we make occur."
        ],
        [
            "A copy of the current hypothesis and added to our previous."
        ],
        [
            "We learn hypothesis.",
            "Excuse me"
        ],
        [
            "And then we update this hypothesis with our additive update rule.",
            "This is designed to better better rank similar document pair examples the next time they're presented to the algorithm.",
            "So this update rule adjusts the current hypothesis and training proceeds through the."
        ],
        [
            "Training set."
        ],
        [
            "Until we typically iterate through the training data several times until some stopping stopping criterion is met."
        ],
        [
            "So we have a list of previously learned hypothesis on the left, and I'll walk through a couple of perceptron variants that our algorithm builds on.",
            "The."
        ],
        [
            "Original perceptron algorithm.",
            "Just returns the most recently learned weight vector.",
            "In this case you can see it was a poor hypothesis to settle on because it only correctly classified one or correctly ranked one pair of Doc."
        ],
        [
            "It's the pocket Perceptron is a extension to this which keeps the best learned hypothesis through the training.",
            "But both of these are susceptible to the order in which the data was presented to the algorithm, and can be fairly volatile hypothesis."
        ],
        [
            "So the average."
        ],
        [
            "Voted perceptron algorithms were proposed to get around that, and so the average perceptron creates a weighted average of the previously learned hypothesis, weighted by the success counter and the voter perception.",
            "Does something similar.",
            "It ranks documents with each hypothesis, and then there's a linear combination of those ranks.",
            "So this is equivalent to board accounting, which is a well known rank aggregation."
        ],
        [
            "Rhythm."
        ],
        [
            "As I mentioned, the original Perceptron, we could possibly learn a very bad final hypothesis and because of the sensitivity to the recently exposed examples, it could be unstable."
        ],
        [
            "Hypothesis pocket perception."
        ],
        [
            "Improves on that, but again, it can be an unstable hypothesis.",
            "The average perceptron improves the stability, but as will show, it's a relatively slow convergence to a stable solution."
        ],
        [
            "In the voter perceptron.",
            "It's also more stable, but it has quite slow test time.",
            "We need to maintain all of the hypothesis we've learned in order to make test time decision."
        ],
        [
            "So the 1st three variants can be learned with constant space complexity in constant test time complexity, but the voter perception in the number of hypothesis we need to maintain grows linearly with the number of mistakes we make in our training data.",
            "Although voted Perceptron does give us a nonlinear and possibly more flexible ranking.",
            "So."
        ],
        [
            "Surface.",
            "In our tests on a relatively small collection, in just 5050 iterations we had, we had to maintain about 1 1/2 million hypothesis for the voted Perceptron, which makes this algorithm somewhat impractical in practice."
        ],
        [
            "The committee perceptron, as I said, is a generalization of these previous variants where we only use the best K."
        ],
        [
            "Apophysis, we've learned.",
            "So if these are the previously learned hypothesis along the bottom.",
            "Ordered by their success count."
        ],
        [
            "Our pocket perceptron selects the best."
        ],
        [
            "On are average and voted Perceptrons.",
            "Select an unlimited number of hypothesis."
        ],
        [
            "And the committee perceptron selects some fixed number of hypothesis.",
            "We don't need to actually maintain all of the hypothesis we learn through the training.",
            "We can maintain a fixed size queue of hypothesis to keep memory and space memory and test time complexity constant."
        ],
        [
            "This algorithm is empirically faster training than the other perceptron variants.",
            "As I mentioned, it has constant space and time complexity.",
            "Excuse me and comparable or better performance to the baseline algorithms we compare to in this test."
        ],
        [
            "So the test collection we use here was recently was released last year by the Microsoft Research Asia Group.",
            "Most of the data consists of track data.",
            "Ranging from 52 just over 100 queries per test set.",
            "It's a standard feature set, so term, frequency, inverse document, frequency based features just like the features I showed earlier.",
            "And we have two and three level relevance judgments."
        ],
        [
            "This data set comes with two baseline algorithm performance.",
            "Rank SVM in rank boost rank boost is a boosting algorithm and rank SVM is a maximum margin approach.",
            "Rank SVM is optimizing a very similar loss function to what I showed earlier than another pairwise preference learn.",
            "As of about a year ago, I would say these were probably strong baselines to compare against and.",
            "There have been many.",
            "More powerful algorithms produce.",
            "More recently, but we didn't do those comparisons for this study."
        ],
        [
            "So first showing the.",
            "Comparison, a cross perceptron variance.",
            "The red line on the bottom is the average perceptron, which actually has very comparable performance to the voter perceptron.",
            "As you can see, it's a.",
            "Slow learning curve still improving after 500 iterations of the data.",
            "The pocket perceptron has fairly erratic behavior.",
            "Which is to be expected and the committee perceptron.",
            "Outperforms both of those variants and really approaches a stable level of performance in about 50 iterations or so."
        ],
        [
            "Now training time is compared to the one of the baseline."
        ],
        [
            "Items rank SVM.",
            "This algorithm took about almost six hours per per training fold on the ocean.",
            "Med data set.",
            "One of our test sets."
        ],
        [
            "The Committee Perceptron, on the other hand, just took a few minutes per fold.",
            "This is."
        ],
        [
            "Greater than a 45 fold reduction in training time so fairly sizable performance improvement over ranks."
        ],
        [
            "Jim.",
            "And as far as performance compared with rank, SVM and rank boost we see on the Ocean Med data set at least we have comparable or better performance really across.",
            "A wide range of performance measures.",
            "This is N DCG at various rank cut offs.",
            "The rank on the X axis here.",
            "And we can see across the across the range of rank cut offs.",
            "The committee perceptron in this case of voted average committee size 20.",
            "Outperforms the other two algorithms."
        ],
        [
            "Performance is similar with mean average precision, and here I'm showing two different committee perceptron variants in blue we have the average Committee an in red.",
            "We have a voted committee, so again, this is the board accounting."
        ],
        [
            "Similar performance trend on the 2003 track collection."
        ],
        [
            "And on the 2004 track collection, the performance pictures slightly different rank boost performs quite well in this collection."
        ],
        [
            "So we presented a new variant of the Perceptron algorithm.",
            "That's a fast learner for document learning document ranking functions.",
            "It's faster and more stable than the other perceptron variance and maintains constant space and test time complexity.",
            "This algorithm shows up more than 45 fold reduction in training time versus rank SVM with comparable or better performance than ranking SVM and rank boost on a variety of tests."
        ],
        [
            "OK, thank you.",
            "Any questions?",
            "Yeah.",
            "Excuse me because you keep a maximum length of each one.",
            "How stable is this to noise in the training data?",
            "So.",
            "We use a trick in the paper called the Alpha bound which is designed to limit the influence of any single example on the final hypothesis.",
            "What this basically does is if a mistake is made on a single example for some number of times, that example is removed from the training data for further iterations, and so that makes this algorithm more robust to noise in the data.",
            "And check out the paper.",
            "I guess if you need more details on that.",
            "Any other questions?",
            "So you mentioned as a baseline this rank, SVM and rank boost, so I didn't see the comparison.",
            "R."
        ],
        [
            "Uh, sorry.",
            "OK. Actually I have a question.",
            "Did you check the dependence of the performance and quality of your algorithm on the committee size?",
            "We did an."
        ],
        [
            "And it showed very stable performance across a range of committee sizes.",
            "So beyond about 10 committee members up to about 100, it showed very comparable performance.",
            "An between averaging and voting.",
            "As you can see, comparable performance between those two modes also.",
            "Let's thank the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm John alsys.",
                    "label": 0
                },
                {
                    "sent": "Here are my.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Collaborators Victor and Jamie.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first I'd like to talk about a little bit about features in information.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Retrieval early retrieval systems were basically exact match unranked Boolean which was not as effective as people would have liked, so more sophisticated features were started.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're working to retrieval algorithms like term frequency, inverse document frequency with algorithms like BM 25.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As collections evolved, more features became useful, like link structure, anchor text, document structure.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now we have a slew of features that may be useful for retrieval of things like social annotations or clickthrough data.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just to clarify what I'm talking about, a few of these example features.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are things like raw term frequency of the query terms in a document?",
                    "label": 0
                },
                {
                    "sent": "Query.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hood in a language modeling framework.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Say query likelihood restricted to the title of Doc.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Events or query independent features like page rank.",
                    "label": 0
                },
                {
                    "sent": "So all of these can be represented by a real valued function over the query in the document.",
                    "label": 0
                },
                {
                    "sent": "We aren't talking about the whole vocabulary here.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the question is, how do we use these features?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's many features used in real world search engines.",
                    "label": 0
                },
                {
                    "sent": "Probably in the hundreds.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And ideally we'd like to adapt these feature weights across users in a cross across tasks.",
                    "label": 0
                },
                {
                    "sent": "The question is how do we wait these effectively for for better retrieval?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the solution that we pursue here is a machine learning approach where we learn from previous queries and relevance judgments.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This isn't this isn't a new solution to the problem brank learning or learning to rank has recently become a popular search area, and here's a few of the.",
                    "label": 0
                },
                {
                    "sent": "Recent algorithms proposed.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This task.",
                    "label": 0
                },
                {
                    "sent": "Our approach here takes a pairwise preference approach.",
                    "label": 1
                },
                {
                    "sent": "So in this scenario, our training data consists of query document pairs where there's some preference ordering over the pairs of documents for the given query, and the goal is to learn a function that preserves that preference ordering so it produces a higher score for the more preferred documents and lower score for the less preferred and in this sense it reduces to a classification problem over pairs of documents.",
                    "label": 1
                },
                {
                    "sent": "So a positive correct classification would be ranking the two documents.",
                    "label": 0
                },
                {
                    "sent": "Correctly and incorrect classification would be the opposite.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The goal in this scenario is to minimize the number of Miss Rank document pairs.",
                    "label": 1
                },
                {
                    "sent": "So this is given by the simple 01 loss function over document pairs.",
                    "label": 0
                },
                {
                    "sent": "We incur some loss whenever document there is miss.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ranked.",
                    "label": 0
                },
                {
                    "sent": "There are some advantages to this approach to learning ranking functions, so most classification algorithms can be adapted to this task, and it generalizes to any type of relevance information.",
                    "label": 1
                },
                {
                    "sent": "Any preference information, any type of graded relevance levels, or any full or partial ordering of the data.",
                    "label": 1
                },
                {
                    "sent": "And there's some evidence that this pairwise preference assessment is easier for assessors.",
                    "label": 0
                },
                {
                    "sent": "We",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Show in this paper to the minimizing the number of miss ranked pairs places a lower bound on many of the widely used retrieval performance measures like mean average precision or are.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Position.",
                    "label": 0
                },
                {
                    "sent": "So in our setting, we're concerned with linear ranking functions, so our documents are represented by a. Vector of real valued feature functions and these are features that I was talking about earlier, like raw term frequency or page rank.",
                    "label": 1
                },
                {
                    "sent": "And we're learning a linear scoring function, so a simple dot product between a weight vector and these document vectors.",
                    "label": 1
                },
                {
                    "sent": "We can rewrite our loss function as some hinge loss in this in this case.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The algorithm we're presenting here is a variant of the perceptron algorithm in older algorithm, originally proposed in 1958.",
                    "label": 0
                },
                {
                    "sent": "It's an online algorithm which implies fast learner and low memory requirements.",
                    "label": 1
                },
                {
                    "sent": "We only are concerned with one instance or one document pair at a time and we update our current hypothesis or weight vector whenever we Miss Rank, a pair of documents.",
                    "label": 1
                },
                {
                    "sent": "This algorithm has some appealing theoretic properties, so if the data set is linearly separable, the algorithms guaranteed converge on a solution that separates the data.",
                    "label": 0
                },
                {
                    "sent": "But where we're concerned with here is that this algorithm can scale to larger datasets.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So remember our pairwise preference loss function is given up at the top and the Perceptron algorithm learns by applying this iterative additive update rule whenever a pair of documents is miss ranked.",
                    "label": 1
                },
                {
                    "sent": "As an online algorithm, it can't directly minimize this pairwise loss function, but because of the bounds, the provable bounds of the perceptron algorithm we can.",
                    "label": 0
                },
                {
                    "sent": "Minimize the number of Miss rankings we make during training.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'll walk through how the Perceptron algorithm learns.",
                    "label": 0
                },
                {
                    "sent": "Given a stream of document pair examples.",
                    "label": 1
                },
                {
                    "sent": "On the left we have our training data are document pairs and on the right we have some sequence of previously learned hypothesis.",
                    "label": 0
                },
                {
                    "sent": "So these weight vectors here on the right are represented just as a vector in a 2D plane.",
                    "label": 0
                },
                {
                    "sent": "So say we're learning to combine two different features for effective retrieval.",
                    "label": 0
                },
                {
                    "sent": "They also have some success counter or some quality measure associated with them represented by stars here.",
                    "label": 1
                },
                {
                    "sent": "And in the middle we have our current hypothesis.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this training proceeds we look at.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our next document pair in our stream and projected into our.",
                    "label": 0
                },
                {
                    "sent": "2D feature space in this case.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We scored these documents and in this case we've made a correct ranking decision.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We increment a success counter on this hypothesis.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We move on to the next document pair.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again.",
                    "label": 0
                },
                {
                    "sent": "Project these days.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Comments and score them when you make another correct ranking decision.",
                    "label": 0
                },
                {
                    "sent": "Training proceeds and say.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the next.",
                    "label": 0
                },
                {
                    "sent": "Document.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There we incur.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Only rank these.",
                    "label": 0
                },
                {
                    "sent": "In this case we make occur.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A copy of the current hypothesis and added to our previous.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We learn hypothesis.",
                    "label": 0
                },
                {
                    "sent": "Excuse me",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then we update this hypothesis with our additive update rule.",
                    "label": 0
                },
                {
                    "sent": "This is designed to better better rank similar document pair examples the next time they're presented to the algorithm.",
                    "label": 1
                },
                {
                    "sent": "So this update rule adjusts the current hypothesis and training proceeds through the.",
                    "label": 1
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Training set.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Until we typically iterate through the training data several times until some stopping stopping criterion is met.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have a list of previously learned hypothesis on the left, and I'll walk through a couple of perceptron variants that our algorithm builds on.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Original perceptron algorithm.",
                    "label": 0
                },
                {
                    "sent": "Just returns the most recently learned weight vector.",
                    "label": 0
                },
                {
                    "sent": "In this case you can see it was a poor hypothesis to settle on because it only correctly classified one or correctly ranked one pair of Doc.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's the pocket Perceptron is a extension to this which keeps the best learned hypothesis through the training.",
                    "label": 0
                },
                {
                    "sent": "But both of these are susceptible to the order in which the data was presented to the algorithm, and can be fairly volatile hypothesis.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the average.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Voted perceptron algorithms were proposed to get around that, and so the average perceptron creates a weighted average of the previously learned hypothesis, weighted by the success counter and the voter perception.",
                    "label": 1
                },
                {
                    "sent": "Does something similar.",
                    "label": 0
                },
                {
                    "sent": "It ranks documents with each hypothesis, and then there's a linear combination of those ranks.",
                    "label": 0
                },
                {
                    "sent": "So this is equivalent to board accounting, which is a well known rank aggregation.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rhythm.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As I mentioned, the original Perceptron, we could possibly learn a very bad final hypothesis and because of the sensitivity to the recently exposed examples, it could be unstable.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hypothesis pocket perception.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Improves on that, but again, it can be an unstable hypothesis.",
                    "label": 0
                },
                {
                    "sent": "The average perceptron improves the stability, but as will show, it's a relatively slow convergence to a stable solution.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the voter perceptron.",
                    "label": 0
                },
                {
                    "sent": "It's also more stable, but it has quite slow test time.",
                    "label": 1
                },
                {
                    "sent": "We need to maintain all of the hypothesis we've learned in order to make test time decision.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the 1st three variants can be learned with constant space complexity in constant test time complexity, but the voter perception in the number of hypothesis we need to maintain grows linearly with the number of mistakes we make in our training data.",
                    "label": 1
                },
                {
                    "sent": "Although voted Perceptron does give us a nonlinear and possibly more flexible ranking.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Surface.",
                    "label": 0
                },
                {
                    "sent": "In our tests on a relatively small collection, in just 5050 iterations we had, we had to maintain about 1 1/2 million hypothesis for the voted Perceptron, which makes this algorithm somewhat impractical in practice.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The committee perceptron, as I said, is a generalization of these previous variants where we only use the best K.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Apophysis, we've learned.",
                    "label": 0
                },
                {
                    "sent": "So if these are the previously learned hypothesis along the bottom.",
                    "label": 0
                },
                {
                    "sent": "Ordered by their success count.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our pocket perceptron selects the best.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On are average and voted Perceptrons.",
                    "label": 0
                },
                {
                    "sent": "Select an unlimited number of hypothesis.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the committee perceptron selects some fixed number of hypothesis.",
                    "label": 1
                },
                {
                    "sent": "We don't need to actually maintain all of the hypothesis we learn through the training.",
                    "label": 0
                },
                {
                    "sent": "We can maintain a fixed size queue of hypothesis to keep memory and space memory and test time complexity constant.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This algorithm is empirically faster training than the other perceptron variants.",
                    "label": 1
                },
                {
                    "sent": "As I mentioned, it has constant space and time complexity.",
                    "label": 1
                },
                {
                    "sent": "Excuse me and comparable or better performance to the baseline algorithms we compare to in this test.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the test collection we use here was recently was released last year by the Microsoft Research Asia Group.",
                    "label": 1
                },
                {
                    "sent": "Most of the data consists of track data.",
                    "label": 0
                },
                {
                    "sent": "Ranging from 52 just over 100 queries per test set.",
                    "label": 0
                },
                {
                    "sent": "It's a standard feature set, so term, frequency, inverse document, frequency based features just like the features I showed earlier.",
                    "label": 1
                },
                {
                    "sent": "And we have two and three level relevance judgments.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This data set comes with two baseline algorithm performance.",
                    "label": 1
                },
                {
                    "sent": "Rank SVM in rank boost rank boost is a boosting algorithm and rank SVM is a maximum margin approach.",
                    "label": 0
                },
                {
                    "sent": "Rank SVM is optimizing a very similar loss function to what I showed earlier than another pairwise preference learn.",
                    "label": 0
                },
                {
                    "sent": "As of about a year ago, I would say these were probably strong baselines to compare against and.",
                    "label": 0
                },
                {
                    "sent": "There have been many.",
                    "label": 0
                },
                {
                    "sent": "More powerful algorithms produce.",
                    "label": 0
                },
                {
                    "sent": "More recently, but we didn't do those comparisons for this study.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first showing the.",
                    "label": 0
                },
                {
                    "sent": "Comparison, a cross perceptron variance.",
                    "label": 0
                },
                {
                    "sent": "The red line on the bottom is the average perceptron, which actually has very comparable performance to the voter perceptron.",
                    "label": 1
                },
                {
                    "sent": "As you can see, it's a.",
                    "label": 0
                },
                {
                    "sent": "Slow learning curve still improving after 500 iterations of the data.",
                    "label": 0
                },
                {
                    "sent": "The pocket perceptron has fairly erratic behavior.",
                    "label": 1
                },
                {
                    "sent": "Which is to be expected and the committee perceptron.",
                    "label": 0
                },
                {
                    "sent": "Outperforms both of those variants and really approaches a stable level of performance in about 50 iterations or so.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now training time is compared to the one of the baseline.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Items rank SVM.",
                    "label": 0
                },
                {
                    "sent": "This algorithm took about almost six hours per per training fold on the ocean.",
                    "label": 0
                },
                {
                    "sent": "Med data set.",
                    "label": 0
                },
                {
                    "sent": "One of our test sets.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The Committee Perceptron, on the other hand, just took a few minutes per fold.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Greater than a 45 fold reduction in training time so fairly sizable performance improvement over ranks.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Jim.",
                    "label": 0
                },
                {
                    "sent": "And as far as performance compared with rank, SVM and rank boost we see on the Ocean Med data set at least we have comparable or better performance really across.",
                    "label": 0
                },
                {
                    "sent": "A wide range of performance measures.",
                    "label": 0
                },
                {
                    "sent": "This is N DCG at various rank cut offs.",
                    "label": 0
                },
                {
                    "sent": "The rank on the X axis here.",
                    "label": 0
                },
                {
                    "sent": "And we can see across the across the range of rank cut offs.",
                    "label": 0
                },
                {
                    "sent": "The committee perceptron in this case of voted average committee size 20.",
                    "label": 0
                },
                {
                    "sent": "Outperforms the other two algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Performance is similar with mean average precision, and here I'm showing two different committee perceptron variants in blue we have the average Committee an in red.",
                    "label": 0
                },
                {
                    "sent": "We have a voted committee, so again, this is the board accounting.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Similar performance trend on the 2003 track collection.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And on the 2004 track collection, the performance pictures slightly different rank boost performs quite well in this collection.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we presented a new variant of the Perceptron algorithm.",
                    "label": 0
                },
                {
                    "sent": "That's a fast learner for document learning document ranking functions.",
                    "label": 1
                },
                {
                    "sent": "It's faster and more stable than the other perceptron variance and maintains constant space and test time complexity.",
                    "label": 1
                },
                {
                    "sent": "This algorithm shows up more than 45 fold reduction in training time versus rank SVM with comparable or better performance than ranking SVM and rank boost on a variety of tests.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "Any questions?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Excuse me because you keep a maximum length of each one.",
                    "label": 0
                },
                {
                    "sent": "How stable is this to noise in the training data?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We use a trick in the paper called the Alpha bound which is designed to limit the influence of any single example on the final hypothesis.",
                    "label": 0
                },
                {
                    "sent": "What this basically does is if a mistake is made on a single example for some number of times, that example is removed from the training data for further iterations, and so that makes this algorithm more robust to noise in the data.",
                    "label": 0
                },
                {
                    "sent": "And check out the paper.",
                    "label": 0
                },
                {
                    "sent": "I guess if you need more details on that.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "So you mentioned as a baseline this rank, SVM and rank boost, so I didn't see the comparison.",
                    "label": 0
                },
                {
                    "sent": "R.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Uh, sorry.",
                    "label": 0
                },
                {
                    "sent": "OK. Actually I have a question.",
                    "label": 0
                },
                {
                    "sent": "Did you check the dependence of the performance and quality of your algorithm on the committee size?",
                    "label": 0
                },
                {
                    "sent": "We did an.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it showed very stable performance across a range of committee sizes.",
                    "label": 0
                },
                {
                    "sent": "So beyond about 10 committee members up to about 100, it showed very comparable performance.",
                    "label": 0
                },
                {
                    "sent": "An between averaging and voting.",
                    "label": 0
                },
                {
                    "sent": "As you can see, comparable performance between those two modes also.",
                    "label": 0
                },
                {
                    "sent": "Let's thank the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}