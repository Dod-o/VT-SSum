{
    "id": "z6irgnx2x2v54ul5zks7b7bcooicxysj",
    "title": "Multiclass Learning Approaches: A Theoretical Comparison with Implications",
    "info": {
        "author": [
            "Amit Daniely, Einstein Institute of Mathematics, The Hebrew University of Jerusalem"
        ],
        "published": "Jan. 14, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Supervised Learning",
            "Top->Computer Science->Machine Learning->Classification",
            "Top->Computer Science->Machine Learning->Statistical Learning"
        ]
    },
    "url": "http://videolectures.net/machine_daniely_learning/",
    "segmentation": [
        [
            "So in this work we analyze in the park model the task of multiclass classification, which is the task of classifying an object into one of several classes, and we analyze and compare 5 popular methods for handling this task.",
            "The first is 1 versus oh when for each label a binary classifier is trained to distinguish between this label to the rest of the labels.",
            "The second method is all pair in which a binary classifier is trained for each pair of label.",
            "Two other multiclass to binary reduction are error correcting output codes entries and the last method which analyzes multiclass SVM, and we know that four out of five of these methods are a multiclass to binary reduction, an we take half spaces in our D as our binary classifiers."
        ],
        [
            "So how should we analyze a this methods?",
            "Well, recent paper used the analysis.",
            "The analysis which we refer as Russia wants in which in which they bound the ratio between the error of the multiclass classifier to the error of the binary to the average error of the binary classifiers involved in the process.",
            "And this analysis suffers from 2 problems.",
            "The first is.",
            "That as we show, in certain cases the error of the binary classifier is likely to be very, very large.",
            "Which make those Mount non informative and the second problem is that different method train different kind of binary classifiers and consequently this analysis does not allow us to compare different methods.",
            "And we propose to go back to the basic.",
            "We associate with each method and hypothesis class F and we study its estimation error, which is the difference between the error of the hypothesis returned by the ERM algorithm and the error of the best hypothesis in the class.",
            "And we also study the approximation error which is the error of the best hypothesis in the class.",
            "And we emphasize that we work in the distribution free model, which means that we assume essentially nothing about the underlying distribution.",
            "So."
        ],
        [
            "Here is a summary of our results in the middle column you can see the estimation error of the different methods upon seeing an example and in the right column you can see our result about the approximation error and we note that there are few surprises in that table.",
            "For example, consider the multiclass SVM method.",
            "The tree method and the one versus all method.",
            "As you can see, all these methods have the same estimation error.",
            "However the multiclass SVM will always have.",
            "Better approximation error than the two other methods.",
            "And another surprise is that in the error correcting output codes method, large distance might result in worse estimation error and there are a few other surprises.",
            "And."
        ],
        [
            "Actually, we hope that you'll find interest also in our proof technique, so the estimation estimation error is analyzed, usually through the graph dimension, which is a variant of the VC dimension.",
            "And another proof technique we employ is the use of visitor in order to establish results in approximation theory an that's it.",
            "Thank you and feel today at poster #58."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this work we analyze in the park model the task of multiclass classification, which is the task of classifying an object into one of several classes, and we analyze and compare 5 popular methods for handling this task.",
                    "label": 0
                },
                {
                    "sent": "The first is 1 versus oh when for each label a binary classifier is trained to distinguish between this label to the rest of the labels.",
                    "label": 0
                },
                {
                    "sent": "The second method is all pair in which a binary classifier is trained for each pair of label.",
                    "label": 0
                },
                {
                    "sent": "Two other multiclass to binary reduction are error correcting output codes entries and the last method which analyzes multiclass SVM, and we know that four out of five of these methods are a multiclass to binary reduction, an we take half spaces in our D as our binary classifiers.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how should we analyze a this methods?",
                    "label": 0
                },
                {
                    "sent": "Well, recent paper used the analysis.",
                    "label": 0
                },
                {
                    "sent": "The analysis which we refer as Russia wants in which in which they bound the ratio between the error of the multiclass classifier to the error of the binary to the average error of the binary classifiers involved in the process.",
                    "label": 1
                },
                {
                    "sent": "And this analysis suffers from 2 problems.",
                    "label": 0
                },
                {
                    "sent": "The first is.",
                    "label": 0
                },
                {
                    "sent": "That as we show, in certain cases the error of the binary classifier is likely to be very, very large.",
                    "label": 0
                },
                {
                    "sent": "Which make those Mount non informative and the second problem is that different method train different kind of binary classifiers and consequently this analysis does not allow us to compare different methods.",
                    "label": 0
                },
                {
                    "sent": "And we propose to go back to the basic.",
                    "label": 0
                },
                {
                    "sent": "We associate with each method and hypothesis class F and we study its estimation error, which is the difference between the error of the hypothesis returned by the ERM algorithm and the error of the best hypothesis in the class.",
                    "label": 1
                },
                {
                    "sent": "And we also study the approximation error which is the error of the best hypothesis in the class.",
                    "label": 0
                },
                {
                    "sent": "And we emphasize that we work in the distribution free model, which means that we assume essentially nothing about the underlying distribution.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is a summary of our results in the middle column you can see the estimation error of the different methods upon seeing an example and in the right column you can see our result about the approximation error and we note that there are few surprises in that table.",
                    "label": 0
                },
                {
                    "sent": "For example, consider the multiclass SVM method.",
                    "label": 0
                },
                {
                    "sent": "The tree method and the one versus all method.",
                    "label": 0
                },
                {
                    "sent": "As you can see, all these methods have the same estimation error.",
                    "label": 1
                },
                {
                    "sent": "However the multiclass SVM will always have.",
                    "label": 0
                },
                {
                    "sent": "Better approximation error than the two other methods.",
                    "label": 1
                },
                {
                    "sent": "And another surprise is that in the error correcting output codes method, large distance might result in worse estimation error and there are a few other surprises.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Actually, we hope that you'll find interest also in our proof technique, so the estimation estimation error is analyzed, usually through the graph dimension, which is a variant of the VC dimension.",
                    "label": 1
                },
                {
                    "sent": "And another proof technique we employ is the use of visitor in order to establish results in approximation theory an that's it.",
                    "label": 0
                },
                {
                    "sent": "Thank you and feel today at poster #58.",
                    "label": 0
                }
            ]
        }
    }
}