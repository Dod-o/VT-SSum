{
    "id": "l3ikwimrdqxd2lnoii2yxwyvmphur44c",
    "title": "Dirichlet Component Analysis: Feature Extraction for Compositional Data",
    "info": {
        "author": [
            "Hua-Yan Wang, National Laboratory On Machine Perception, Peking University"
        ],
        "published": "July 29, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Principal Component Analysis",
            "Top->Computer Science->Machine Learning->Bayesian Learning->Dirichlet Processes"
        ]
    },
    "url": "http://videolectures.net/icml08_wang_dca/",
    "segmentation": [
        [
            "OK, thank you just as the last one.",
            "I'm going to talk about something about Nonactive quantities, except that we are using a different family of transformations.",
            "And these are the coauthors of this paper.",
            "Since you know sometimes it's very hard to pronounce this Chinese names.",
            "We are one hyejin young Chong Ching Hong and John being OK, just for reference.",
            "That's how we do it.",
            "OK, OK."
        ],
        [
            "OK, so here is.",
            "Here's our story line.",
            "First I will give a brief introduction to the general concepts and."
        ],
        [
            "Background Caso feature extraction, sometimes known as dimensionality reduction, is useful in many aspects, including avoiding overfitting of classification and regression models, improving domain understanding, and reduce computational expense and which."
        ],
        [
            "Lucian OK so.",
            "So basically we are investigating feature extraction for composition data.",
            "So what is compositional data?",
            "Basically they are normalized histograms representing relative proportion of different ingredients in object.",
            "OK, so just as shown in this figure, convolutional data are positive constants.",
            "Some real vectors.",
            "Geometrically they are points in the simplex.",
            "So in a more general setting, compositional data can be regarded as the probability distribution over a discrete set of events, or or."
        ],
        [
            "Objects.",
            "OK, so we will see how is our approach motivated through."
        ],
        [
            "My toy example.",
            "Let's try example.",
            "We just suppose we have some rock samples.",
            "Rock samples collected says something like this.",
            "OK, so so in a geologic study, these samples are decomposed by some chemical approach, so we can record well to proportion of three major elements in these samples abmc.",
            "So in such a setting the this data set is represented as points as points, resulting in.",
            "Two simplex.",
            "So we have each small acts representing a rock sample and three vertices of the simplex in correspondence with abmc.",
            "OK, so so we can observe that we have three peaks in this two simplex and they are in correspondence with three substances that have fixed composition in terms of a B&C.",
            "OK, so so basically the major patterns, which means the peaks of this data set explained by linear combination of the variables or feed."
        ],
        [
            "Others.",
            "As we had what we are now in PTA, we try to explain the major patterns, which means variance of data separately by individual variables instead of the new combinations, and this is done by diagnosing the covariance matrix.",
            "OK, so so this is what PCA does."
        ],
        [
            "Analogously, in this data set, what we want to ask is that is it possible to find a new representation for this toy example in which the major patterns, which means the peaks are expand, explained separately by individual variables instead of the linear."
        ],
        [
            "Combinations.",
            "So, so we're actually finding some representation like this shown in the right figure.",
            "In this representation, the peaks in the datasets just reside close to the vertices of the of the simplex, so we can see that around 90% of the mass in each data in each data point is explained by just one variable.",
            "So we are approaching something like a sparsity or in."
        ],
        [
            "Things like that.",
            "OK, So what we learned from this from this toy example is that sometimes we need to extract features for compositions and these new features is also have a natural interpretation as compositions as in this example.",
            "They are these underlying substance is represented by these chemical formulations.",
            "OK, you may argue that there's there's not such things in chemistry, but since we're talking about machine learning, so whatever.",
            "Um?",
            "OK, so so in one word where you want to extract new compositions from old compositions that leads to the problem of feature extraction for compositional data.",
            "So our problem is how to do this?"
        ],
        [
            "OK, so we will go to our framework of DSA and we'll see how does it work."
        ],
        [
            "First, the 1 -- 1 simplex is denoted as SN.",
            "OK, so so in the in this talk and in in our paper the variables in compositional data are referred as components.",
            "And for on the end minus one simplex is just the points.",
            "We all with all the variables known active and some is a constant.",
            "So the first thing we want to do is to identify the family of linear projections that preserve the preserved simplex constraint.",
            "So so.",
            "So basically we have the following post a proposition for linear projections.",
            "Represented by projection matrix are the projected data is in SK8 four, 4X in SN IF and only if the projection matrix satisfies these two constraints as First Wednesdays at all the elements should be nonnegative and second one states that all the columns should some constant.",
            "OK, so So what is interesting about bout nonactive quantities is that they can be regarded as mass so so we can.",
            "So such projections should be viewed as rearranging mass from these original components to the key to the key new components.",
            "While the Law of conservation of Mass is satisfied.",
            "So the second constraint is actually the law of conservation of mass.",
            "OK, so so we just refer to such projections as rearrangements."
        ],
        [
            "But with such a definition we could have degenerate case such as they are shown on the right, which projects the whole simplex tool to vertical to vertex, which is of course what we want to avoid.",
            "Are to avoid such degenerate case.",
            "We further require the roles of the projection matrix being constant, some with this leads to the definition of balanced rearrangements.",
            "We just add the third constraint to the first 2.",
            "Which requires that order or the roles of the projection matrix should be constant."
        ],
        [
            "OK, so so far we've identified family of simplex simplex, nondegenerate linear projections.",
            "Cancel, however, such projections have the following awkward property.",
            "Which states that the minimum minimum variable the projected data is always known less than the minimum element of the original data.",
            "So the intuition about this is that the balance rearrangement is always.",
            "Is shrinking the data set into the central area of the simplex.",
            "Which is not what we want, because according to our toy example, we want what we want to do is to expand the data set so the peaks reside near the vertices of the simplex.",
            "So this is not consistent."
        ],
        [
            "So 202.",
            "Solve this problem, we would find another operator on compositional data, which is the regularization operator.",
            "'cause this is this is done by our first project, the parallel projection to the great points.",
            "OK, the black points our original data points and we will perform a performer program projection.",
            "Then we perform radial projection.",
            "So we just expand the data set from the black ones, 2 white ones.",
            "We of course we have a formal definition of."
        ],
        [
            "OK, so with the with all the above preparation preparation, we can define the framework of DCA.",
            "As we all know in PDA we have a solution space of orthogonal projections and the objective is empirical Gaussian variance.",
            "For DCA we have the solutions based off balance rearrangements we just defined.",
            "And we have the and we said objective be empirical to Rachel at precision.",
            "So the intuition about direct empirical duration precision is shown in this figure.",
            "These are samples from the University richlite distribution.",
            "With a large precision we we have the data set constant or concentrated in the central area of simplex and with with very small precision we get what we want.",
            "That is, all the data points concentrate news.",
            "Legacies of the simplex.",
            "So so we actually we are approaching something like sparsity."
        ],
        [
            "Cancel this is can be formulated find as fund the balance rearrangement which would apply to data together with the regularization operator.",
            "Minimize the empirical direction like precision.",
            "So so the first the second problem is of course optimization of the problem.",
            "But we have no fun, obvious, efficient solution to this problem due to we have simplex constraint and we have the regularization operator, which is somewhat awkward too.",
            "Optimization process.",
            "OK, so so our current improved implementation to this problem is based on the genetic algorithm."
        ],
        [
            "Firstly, just randomly initialized population of balance rearrangements which are candidate solutions.",
            "Then we apply them to data.",
            "Then we apply the regularization to data.",
            "Then for each candidate we just estimate the objective function of the transformed data.",
            "Then we compute a fitness score for each candidate by this objective function.",
            "The fitness score is our plays very important role in genetic algorithm because it serves as a ways in sampling the population and generating new candidates by linear combination.",
            "So it is noticeable that according to our definition of linear linear combination of two, balance rearrangement is also balanced rearrangement.",
            "So by this we have the new generation of candidates and we just iterate from the beginning.",
            "OK, I will not go to too much details of the algorithm.",
            "You can refer to our papers."
        ],
        [
            "So finally I will show some experiment results on this."
        ],
        [
            "1st first is the data set is a synthetic data set, which is the one we used in our toy example and the left one is the original data set and the right one is the one obtained by our optimization algorithm.",
            "You can see that away successfully put the.",
            "Our peaks in this they are setting around the vertices of the simplex, so so the.",
            "A large portion of this data set can be explained by just one variable instead of linear combinations."
        ],
        [
            "As a second toy example, we use the synthetic data set in three simplex, which is.",
            "Which is 4 with four variables.",
            "Curtis is.",
            "Term frequency.",
            "Turn frequencies of four words in.",
            "OK, these 4 words are a geology, Terran economy and market.",
            "So based on the semantic relationships of these 4 words we have the data documents distributed in this space like like this.",
            "And the right on the right figure shows new representation obtained by our optimization algorithm.",
            "We are.",
            "We are reducing the dimensionality of their cell phone 422.",
            "Actually it's one is distribution segment because we have a constraint.",
            "So as we can see, we have two underlying classes in this data set and the perfect separated in the."
        ],
        [
            "Regarding the representation.",
            "OK, so this is a real world data set on the left figure shows some sites where we can collect water samples from River is River in Spain.",
            "And these datasets downloaded from the Internet.",
            "OK, so so the red line in this in this figure shows that we can we can we can put this size into two classes upstream and downstream.",
            "So the corresponding samples are also classified into upstream samples and downstream samples.",
            "In total we have 485 symbols, each of them is.",
            "14 dimensional vector recording concentration of these major items in the water.",
            "So we just apply DCA to this data set to option 2 dimensional visualization of the data set so we can observe that these two underlying classes is almost perfect separated.",
            "So I think this successful example of visualization of high dimensional data."
        ],
        [
            "OK, so so are we also.",
            "Also applied our method to bag of word data.",
            "To validate the effect of our method in avoiding overfitting of classification models which we used.",
            "Linear SVM here or especially when the training set is extremely smaller, we compared our method to.",
            "Some other popular dimensionality reduction methods, such as PCA and LDA, which we mean to Rachel at latent directional allocation."
        ],
        [
            "The original data set is over 2000 dimensions and we have we used the three methods to reduce the dimensionality chuchen journey and 50.",
            "And we compared the error rate on test set of linear SVM with different number of training samples.",
            "So as we can see with a very small number of training samples.",
            "Our our method performs the best, which means it's avoid the effect of overfitting better than other methods."
        ],
        [
            "Can you show?",
            "So I think that's it."
        ],
        [
            "As we are heading for lunch, I think you're welcome to the multiple instance learning session where we have a more interesting one.",
            "You questions before lunch.",
            "No.",
            "Launch the.",
            "My question so.",
            "One of the one of the things that run games is talk is, or at least in the paper, gives a good description of dealing with the hyperparameters in the Jewish life.",
            "Yeah, that's actually quite important.",
            "Yeah, if you set those certain high level access, you should also get your balance constraints which.",
            "Condition.",
            "So I guess in some way.",
            "LDI is has some similarities with that easy fiddle with parameters, yeah.",
            "Yeah, I think I think the to me the most interesting part of this of the work is we can consider these kind of projections or transforms in other more sophisticated models whenever we are encounter with something like probabilities over a discrete set of events or objects.",
            "You know so.",
            "So I mean this our current framework is not the only way to.",
            "To use this kind of intuition to expand their set to achieve sparsity.",
            "Yep.",
            "Done.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, thank you just as the last one.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about something about Nonactive quantities, except that we are using a different family of transformations.",
                    "label": 0
                },
                {
                    "sent": "And these are the coauthors of this paper.",
                    "label": 0
                },
                {
                    "sent": "Since you know sometimes it's very hard to pronounce this Chinese names.",
                    "label": 0
                },
                {
                    "sent": "We are one hyejin young Chong Ching Hong and John being OK, just for reference.",
                    "label": 0
                },
                {
                    "sent": "That's how we do it.",
                    "label": 0
                },
                {
                    "sent": "OK, OK.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so here is.",
                    "label": 0
                },
                {
                    "sent": "Here's our story line.",
                    "label": 0
                },
                {
                    "sent": "First I will give a brief introduction to the general concepts and.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Background Caso feature extraction, sometimes known as dimensionality reduction, is useful in many aspects, including avoiding overfitting of classification and regression models, improving domain understanding, and reduce computational expense and which.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Lucian OK so.",
                    "label": 0
                },
                {
                    "sent": "So basically we are investigating feature extraction for composition data.",
                    "label": 1
                },
                {
                    "sent": "So what is compositional data?",
                    "label": 0
                },
                {
                    "sent": "Basically they are normalized histograms representing relative proportion of different ingredients in object.",
                    "label": 0
                },
                {
                    "sent": "OK, so just as shown in this figure, convolutional data are positive constants.",
                    "label": 0
                },
                {
                    "sent": "Some real vectors.",
                    "label": 0
                },
                {
                    "sent": "Geometrically they are points in the simplex.",
                    "label": 0
                },
                {
                    "sent": "So in a more general setting, compositional data can be regarded as the probability distribution over a discrete set of events, or or.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Objects.",
                    "label": 0
                },
                {
                    "sent": "OK, so we will see how is our approach motivated through.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My toy example.",
                    "label": 0
                },
                {
                    "sent": "Let's try example.",
                    "label": 0
                },
                {
                    "sent": "We just suppose we have some rock samples.",
                    "label": 0
                },
                {
                    "sent": "Rock samples collected says something like this.",
                    "label": 0
                },
                {
                    "sent": "OK, so so in a geologic study, these samples are decomposed by some chemical approach, so we can record well to proportion of three major elements in these samples abmc.",
                    "label": 0
                },
                {
                    "sent": "So in such a setting the this data set is represented as points as points, resulting in.",
                    "label": 0
                },
                {
                    "sent": "Two simplex.",
                    "label": 0
                },
                {
                    "sent": "So we have each small acts representing a rock sample and three vertices of the simplex in correspondence with abmc.",
                    "label": 0
                },
                {
                    "sent": "OK, so so we can observe that we have three peaks in this two simplex and they are in correspondence with three substances that have fixed composition in terms of a B&C.",
                    "label": 0
                },
                {
                    "sent": "OK, so so basically the major patterns, which means the peaks of this data set explained by linear combination of the variables or feed.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Others.",
                    "label": 0
                },
                {
                    "sent": "As we had what we are now in PTA, we try to explain the major patterns, which means variance of data separately by individual variables instead of the new combinations, and this is done by diagnosing the covariance matrix.",
                    "label": 1
                },
                {
                    "sent": "OK, so so this is what PCA does.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Analogously, in this data set, what we want to ask is that is it possible to find a new representation for this toy example in which the major patterns, which means the peaks are expand, explained separately by individual variables instead of the linear.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Combinations.",
                    "label": 0
                },
                {
                    "sent": "So, so we're actually finding some representation like this shown in the right figure.",
                    "label": 0
                },
                {
                    "sent": "In this representation, the peaks in the datasets just reside close to the vertices of the of the simplex, so we can see that around 90% of the mass in each data in each data point is explained by just one variable.",
                    "label": 0
                },
                {
                    "sent": "So we are approaching something like a sparsity or in.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Things like that.",
                    "label": 0
                },
                {
                    "sent": "OK, So what we learned from this from this toy example is that sometimes we need to extract features for compositions and these new features is also have a natural interpretation as compositions as in this example.",
                    "label": 1
                },
                {
                    "sent": "They are these underlying substance is represented by these chemical formulations.",
                    "label": 0
                },
                {
                    "sent": "OK, you may argue that there's there's not such things in chemistry, but since we're talking about machine learning, so whatever.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, so so in one word where you want to extract new compositions from old compositions that leads to the problem of feature extraction for compositional data.",
                    "label": 0
                },
                {
                    "sent": "So our problem is how to do this?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we will go to our framework of DSA and we'll see how does it work.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First, the 1 -- 1 simplex is denoted as SN.",
                    "label": 0
                },
                {
                    "sent": "OK, so so in the in this talk and in in our paper the variables in compositional data are referred as components.",
                    "label": 0
                },
                {
                    "sent": "And for on the end minus one simplex is just the points.",
                    "label": 0
                },
                {
                    "sent": "We all with all the variables known active and some is a constant.",
                    "label": 0
                },
                {
                    "sent": "So the first thing we want to do is to identify the family of linear projections that preserve the preserved simplex constraint.",
                    "label": 0
                },
                {
                    "sent": "So so.",
                    "label": 0
                },
                {
                    "sent": "So basically we have the following post a proposition for linear projections.",
                    "label": 0
                },
                {
                    "sent": "Represented by projection matrix are the projected data is in SK8 four, 4X in SN IF and only if the projection matrix satisfies these two constraints as First Wednesdays at all the elements should be nonnegative and second one states that all the columns should some constant.",
                    "label": 0
                },
                {
                    "sent": "OK, so So what is interesting about bout nonactive quantities is that they can be regarded as mass so so we can.",
                    "label": 0
                },
                {
                    "sent": "So such projections should be viewed as rearranging mass from these original components to the key to the key new components.",
                    "label": 0
                },
                {
                    "sent": "While the Law of conservation of Mass is satisfied.",
                    "label": 0
                },
                {
                    "sent": "So the second constraint is actually the law of conservation of mass.",
                    "label": 0
                },
                {
                    "sent": "OK, so so we just refer to such projections as rearrangements.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But with such a definition we could have degenerate case such as they are shown on the right, which projects the whole simplex tool to vertical to vertex, which is of course what we want to avoid.",
                    "label": 0
                },
                {
                    "sent": "Are to avoid such degenerate case.",
                    "label": 0
                },
                {
                    "sent": "We further require the roles of the projection matrix being constant, some with this leads to the definition of balanced rearrangements.",
                    "label": 0
                },
                {
                    "sent": "We just add the third constraint to the first 2.",
                    "label": 0
                },
                {
                    "sent": "Which requires that order or the roles of the projection matrix should be constant.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so so far we've identified family of simplex simplex, nondegenerate linear projections.",
                    "label": 0
                },
                {
                    "sent": "Cancel, however, such projections have the following awkward property.",
                    "label": 0
                },
                {
                    "sent": "Which states that the minimum minimum variable the projected data is always known less than the minimum element of the original data.",
                    "label": 0
                },
                {
                    "sent": "So the intuition about this is that the balance rearrangement is always.",
                    "label": 0
                },
                {
                    "sent": "Is shrinking the data set into the central area of the simplex.",
                    "label": 1
                },
                {
                    "sent": "Which is not what we want, because according to our toy example, we want what we want to do is to expand the data set so the peaks reside near the vertices of the simplex.",
                    "label": 0
                },
                {
                    "sent": "So this is not consistent.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So 202.",
                    "label": 0
                },
                {
                    "sent": "Solve this problem, we would find another operator on compositional data, which is the regularization operator.",
                    "label": 0
                },
                {
                    "sent": "'cause this is this is done by our first project, the parallel projection to the great points.",
                    "label": 1
                },
                {
                    "sent": "OK, the black points our original data points and we will perform a performer program projection.",
                    "label": 0
                },
                {
                    "sent": "Then we perform radial projection.",
                    "label": 0
                },
                {
                    "sent": "So we just expand the data set from the black ones, 2 white ones.",
                    "label": 0
                },
                {
                    "sent": "We of course we have a formal definition of.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so with the with all the above preparation preparation, we can define the framework of DCA.",
                    "label": 0
                },
                {
                    "sent": "As we all know in PDA we have a solution space of orthogonal projections and the objective is empirical Gaussian variance.",
                    "label": 0
                },
                {
                    "sent": "For DCA we have the solutions based off balance rearrangements we just defined.",
                    "label": 0
                },
                {
                    "sent": "And we have the and we said objective be empirical to Rachel at precision.",
                    "label": 0
                },
                {
                    "sent": "So the intuition about direct empirical duration precision is shown in this figure.",
                    "label": 0
                },
                {
                    "sent": "These are samples from the University richlite distribution.",
                    "label": 0
                },
                {
                    "sent": "With a large precision we we have the data set constant or concentrated in the central area of simplex and with with very small precision we get what we want.",
                    "label": 0
                },
                {
                    "sent": "That is, all the data points concentrate news.",
                    "label": 0
                },
                {
                    "sent": "Legacies of the simplex.",
                    "label": 0
                },
                {
                    "sent": "So so we actually we are approaching something like sparsity.",
                    "label": 1
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cancel this is can be formulated find as fund the balance rearrangement which would apply to data together with the regularization operator.",
                    "label": 0
                },
                {
                    "sent": "Minimize the empirical direction like precision.",
                    "label": 0
                },
                {
                    "sent": "So so the first the second problem is of course optimization of the problem.",
                    "label": 0
                },
                {
                    "sent": "But we have no fun, obvious, efficient solution to this problem due to we have simplex constraint and we have the regularization operator, which is somewhat awkward too.",
                    "label": 0
                },
                {
                    "sent": "Optimization process.",
                    "label": 0
                },
                {
                    "sent": "OK, so so our current improved implementation to this problem is based on the genetic algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Firstly, just randomly initialized population of balance rearrangements which are candidate solutions.",
                    "label": 0
                },
                {
                    "sent": "Then we apply them to data.",
                    "label": 0
                },
                {
                    "sent": "Then we apply the regularization to data.",
                    "label": 1
                },
                {
                    "sent": "Then for each candidate we just estimate the objective function of the transformed data.",
                    "label": 0
                },
                {
                    "sent": "Then we compute a fitness score for each candidate by this objective function.",
                    "label": 0
                },
                {
                    "sent": "The fitness score is our plays very important role in genetic algorithm because it serves as a ways in sampling the population and generating new candidates by linear combination.",
                    "label": 1
                },
                {
                    "sent": "So it is noticeable that according to our definition of linear linear combination of two, balance rearrangement is also balanced rearrangement.",
                    "label": 0
                },
                {
                    "sent": "So by this we have the new generation of candidates and we just iterate from the beginning.",
                    "label": 0
                },
                {
                    "sent": "OK, I will not go to too much details of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "You can refer to our papers.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So finally I will show some experiment results on this.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "1st first is the data set is a synthetic data set, which is the one we used in our toy example and the left one is the original data set and the right one is the one obtained by our optimization algorithm.",
                    "label": 0
                },
                {
                    "sent": "You can see that away successfully put the.",
                    "label": 0
                },
                {
                    "sent": "Our peaks in this they are setting around the vertices of the simplex, so so the.",
                    "label": 0
                },
                {
                    "sent": "A large portion of this data set can be explained by just one variable instead of linear combinations.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As a second toy example, we use the synthetic data set in three simplex, which is.",
                    "label": 0
                },
                {
                    "sent": "Which is 4 with four variables.",
                    "label": 0
                },
                {
                    "sent": "Curtis is.",
                    "label": 0
                },
                {
                    "sent": "Term frequency.",
                    "label": 0
                },
                {
                    "sent": "Turn frequencies of four words in.",
                    "label": 0
                },
                {
                    "sent": "OK, these 4 words are a geology, Terran economy and market.",
                    "label": 0
                },
                {
                    "sent": "So based on the semantic relationships of these 4 words we have the data documents distributed in this space like like this.",
                    "label": 0
                },
                {
                    "sent": "And the right on the right figure shows new representation obtained by our optimization algorithm.",
                    "label": 0
                },
                {
                    "sent": "We are.",
                    "label": 0
                },
                {
                    "sent": "We are reducing the dimensionality of their cell phone 422.",
                    "label": 0
                },
                {
                    "sent": "Actually it's one is distribution segment because we have a constraint.",
                    "label": 0
                },
                {
                    "sent": "So as we can see, we have two underlying classes in this data set and the perfect separated in the.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Regarding the representation.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a real world data set on the left figure shows some sites where we can collect water samples from River is River in Spain.",
                    "label": 0
                },
                {
                    "sent": "And these datasets downloaded from the Internet.",
                    "label": 0
                },
                {
                    "sent": "OK, so so the red line in this in this figure shows that we can we can we can put this size into two classes upstream and downstream.",
                    "label": 0
                },
                {
                    "sent": "So the corresponding samples are also classified into upstream samples and downstream samples.",
                    "label": 0
                },
                {
                    "sent": "In total we have 485 symbols, each of them is.",
                    "label": 0
                },
                {
                    "sent": "14 dimensional vector recording concentration of these major items in the water.",
                    "label": 0
                },
                {
                    "sent": "So we just apply DCA to this data set to option 2 dimensional visualization of the data set so we can observe that these two underlying classes is almost perfect separated.",
                    "label": 0
                },
                {
                    "sent": "So I think this successful example of visualization of high dimensional data.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so so are we also.",
                    "label": 0
                },
                {
                    "sent": "Also applied our method to bag of word data.",
                    "label": 0
                },
                {
                    "sent": "To validate the effect of our method in avoiding overfitting of classification models which we used.",
                    "label": 0
                },
                {
                    "sent": "Linear SVM here or especially when the training set is extremely smaller, we compared our method to.",
                    "label": 0
                },
                {
                    "sent": "Some other popular dimensionality reduction methods, such as PCA and LDA, which we mean to Rachel at latent directional allocation.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The original data set is over 2000 dimensions and we have we used the three methods to reduce the dimensionality chuchen journey and 50.",
                    "label": 0
                },
                {
                    "sent": "And we compared the error rate on test set of linear SVM with different number of training samples.",
                    "label": 0
                },
                {
                    "sent": "So as we can see with a very small number of training samples.",
                    "label": 0
                },
                {
                    "sent": "Our our method performs the best, which means it's avoid the effect of overfitting better than other methods.",
                    "label": 1
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can you show?",
                    "label": 0
                },
                {
                    "sent": "So I think that's it.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As we are heading for lunch, I think you're welcome to the multiple instance learning session where we have a more interesting one.",
                    "label": 0
                },
                {
                    "sent": "You questions before lunch.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Launch the.",
                    "label": 0
                },
                {
                    "sent": "My question so.",
                    "label": 0
                },
                {
                    "sent": "One of the one of the things that run games is talk is, or at least in the paper, gives a good description of dealing with the hyperparameters in the Jewish life.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's actually quite important.",
                    "label": 0
                },
                {
                    "sent": "Yeah, if you set those certain high level access, you should also get your balance constraints which.",
                    "label": 0
                },
                {
                    "sent": "Condition.",
                    "label": 0
                },
                {
                    "sent": "So I guess in some way.",
                    "label": 0
                },
                {
                    "sent": "LDI is has some similarities with that easy fiddle with parameters, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think I think the to me the most interesting part of this of the work is we can consider these kind of projections or transforms in other more sophisticated models whenever we are encounter with something like probabilities over a discrete set of events or objects.",
                    "label": 0
                },
                {
                    "sent": "You know so.",
                    "label": 0
                },
                {
                    "sent": "So I mean this our current framework is not the only way to.",
                    "label": 0
                },
                {
                    "sent": "To use this kind of intuition to expand their set to achieve sparsity.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Done.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}