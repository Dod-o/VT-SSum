{
    "id": "m56ixidwvj5l3smpvgfprktg6iytywaa",
    "title": "The Sample-Computational Tradeo ff",
    "info": {
        "author": [
            "Shai Shalev-Shwartz, School of Computer Science and Engineering, The Hebrew University of Jerusalem"
        ],
        "published": "Jan. 16, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Supervised Learning",
            "Top->Computer Science->Machine Learning->Statistical Learning"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2012_shalev_shwartz_tradeoff/",
    "segmentation": [
        [
            "So."
        ],
        [
            "I'm I'm going to talk about the sample computational tradeoff, something that I work on.",
            "Sorry, something that I worked on for awhile, but this specific talk will be based on 4 works from the last year.",
            "The first is with Satian calendar at Hassan from called 2012.",
            "Second is with my student Aaron Brown from these snips.",
            "Word is.",
            "Joint work with submitted nearly and anti lineal, which was submitted, and.",
            "The last is so hard from here and there and drummer from a stats.",
            "Um?"
        ],
        [
            "So let me quickly just build the notation.",
            "We're dealing with the classical setting of agnostic PAC learning.",
            "There is a hypothesis class which is a set of functions from an input domain X and label domain Y.",
            "We have a loss function which takes a hypothesis and an example and output real number.",
            "This is a distribution which is unknown to us over the product of X * Y.",
            "The true risk of hypothesis is just the expected loss when you choose an example XY at random according to the distribution D. And the learner receives a training set which is M pairs of instance and label, and the goal of the learner is to use the training set so as to find the hypothesis which I denote by H sub S such that with high probability the true risk of AHS is not too far from the true risk of the optimal hypothesis in the hypothesis class.",
            "OK, and one particular rule is an empirical risk minimization rule which simply minimizes empirical loss which is average loss over the examples in the training set.",
            "OK, so this is very standard agnostic PAC learning model OK.",
            "So."
        ],
        [
            "When we analyze agnostic PAC learning, it is common to the compose the error of in learning algorithm, in particular the ERM rule into two terms.",
            "The first term is approximation error, which is simply the minimal risk of the optimal path ASIS in the hypothesis class OK, and the second error term is a difference between the true error of the RMA prothesis.",
            "And the approximation error.",
            "So this decomposition is trivially true.",
            "OK, but it gives us a natural tradeoff between approximation error.",
            "The approximation error measures how well our prior knowledge is reflected by the hypothesis class.",
            "H behaves well on the word as reflected by the distribution D. OK, so it's just a measure of the hypothesis class in the distribution.",
            "While the estimation error follows from the fact that we only observe a finite training set S and we do not know the true distribution, so this is a difference between between the risk of the RMI party SIS what performs best on the training set and the true hypothesis.",
            "OK, so this is a classical trade off."
        ],
        [
            "Practice, however.",
            "In many cases we cannot actually find the near my party SIS, so we find some another hypothesis and then we have 1/3 error term that comes into the picture.",
            "The further error term I call it the optimization error.",
            "This is a difference between the true risk of what the algorithm actually finds and the true risk of the RMA prothesis.",
            "OK, so again the composition is trivial, is just adding and subtracting terms, but it gives us.",
            "Some nice way to understand what's going on.",
            "OK, so now when we come to a specific problem we have two resources.",
            "We have samples and we have runtime and there is a actually a tradeoff between samples and runtime.",
            "I would like to show you that sometime you can ask for more examples and gaining runtime or the other way around.",
            "OK and this tradeoff was first described by a work of DECA tour.",
            "Goldwire can drone.",
            "In 1998 is called the sample computational complexity.",
            "OK, so."
        ],
        [
            "To be more specific, recall that our goal is to find the hypothesis.",
            "Which is not too far away from the optimal hypothesis in the hypothesis class."
        ],
        [
            "The sample complexity is how many examples are needed to achieve this goal and the time complexity is how much time is needed to achieve this goal."
        ],
        [
            "And now we need to define a joint times sample computational.",
            "So you can think about it as a function T that depends on the target epsilon and the hypothesis class H. And it is a function of the number of available training examples, which I denote by M. And this function measures how much time is needed when the size of the training set is M. So.",
            "Usually we study sample complexity, which is this line here.",
            "So sample complexity is even if we ignore computational complexity, we have infinite amount of time.",
            "There is a barrier of the number of examples that are required to achieve the goal.",
            "OK, the other symptomatic is called the data Laden domain.",
            "This is what happens when examples are not an issue.",
            "You have infinite amount of examples.",
            "The only resource which is limited.",
            "Is time?",
            "And then the question is how?",
            "What is the minimal amount of time you need in order to solve this problem when you can ask for whatever number of examples that you want, OK?",
            "But there is an interesting thing in between.",
            "I would like to show you that there is an actually decreasing function, so the time is decreasing as a function of the number of examples.",
            "And if we can understand this curve, we can control the tradeoff between time and samples."
        ],
        [
            "So I'm going to start with a 2 examples of real world problems in which I conjecture that there is a such a tradeoff, but the conjecture is based on upper bounds.",
            "OK, and the last one is a synthetic problem in which we formally established that there is such a phenomenon of function.",
            "The time is decreasing with the number of examples.",
            "OK, so the first thing is agnostic learning of."
        ],
        [
            "References it's a very simple but still nice problem.",
            "The domain set is just the cross product of a D numbers from one 2D OK and the label set is just 01.",
            "Given an instance which is just a pair I&J.",
            "We should predict if I is preferrable over J. OK. And the label determines if I indeed preferable over J.",
            "Now we need to define a hypothesis class and the natural hypothesis class for this problem is just is just a set of permutations over this.",
            "So think about DSD teams and each IJ is a match between two teams and we need to determine if I is going to win or Jay is going to win and hypothesis class is just a global ranking of all the teams and then we predict according to whether I comes before J.",
            "In the permutation OK and the last function is just 01.",
            "If you are wrong, you pay one, and if you are correct you pay 0.",
            "OK, so let's start with the first approach.",
            "For this problem, simply apply the empirical risk minimization rule over the set of permutations over the.",
            "So the sample complexity if we ignore computational complexity, the sample complexity is simply D over epsilon square.",
            "The reason is because this hypothesis classes visit dimension D. OK, so it's the standard sample complexity analysis for agnostic back learning.",
            "OK, but what about time complexity?",
            "So Varoon Canadian Thomas Standke showed that if RP not equal to NP, then it is not possible to efficiently find an epsilon accurate permutation.",
            "So it seems like we cannot learn efficiently this class.",
            "OK, but what I'm going to show you is it is that if the number of example is greater than this square over epsilon square, so it's more than the sample complexity.",
            "But just polynomial in more than the sample complexity, then it is possible to find the predictor with error smaller than epsilon in polynomial time charter.",
            "That was for then you have the over epsilon squared samples, no?",
            "So the actually this this.",
            "This hardener is hardness.",
            "Results is for any number of examples.",
            "So why there is not a contradiction between these two bullets?",
            "So let me explain, so here the lower bound tells you that you cannot find an epsilon accurate permutation, so you must output function from the hypothesis class permutation and this is difficult to do computationally, but here I'm not going to tell you that.",
            "That I will output a permutation, I will just output a function from X to Y, which is which has good prediction accuracy.",
            "OK, but it will not be a permutation.",
            "This is sometimes called improper learning.",
            "But there is nothing improper in improper learning.",
            "OK, so the difference is clear.",
            "Exactly this is what I'm going to do.",
            "OK, so how we achieve how we achieve this positive result?",
            "So the question was it if in the original in the distribution the labels are determined by permutations?",
            "Answer is not because we are dealing with agnostic setting, so we assume nothing about the true distribution.",
            "OK, we just compare ourselves to the best permanent."
        ],
        [
            "OK, so the idea that the intuitive idea of how to how to obtain an efficient algorithm is that the ERM problem is a search problem over the set of permutation, and this is a difficult search problem because the space is not nice.",
            "But if we can construct a nicer search problem.",
            "Maybe we will have a larger space, but it will be nicer to search over then maybe we can do it efficiently when maybe we can obtain the search implemented search problem more efficiently and the idea is simple.",
            "So let H and you be the set of all functions from X to Y.",
            "So clearly it contains also all the functions that are defined by permutation.",
            "OK, so it's a super set of H Now.",
            "To perform the empirical risk minimization over this set is very easy because what we need to do is just look at the sample look at all the pairs IJ in the sample and count the number of times they appear with the label one and the number the number of time they appear with the label zero, and then take a majority vote.",
            "OK, so in one Passover the training set we can implement ERM over the new class.",
            "OK, it it has a nice structure, everything is decomposable OK, but on the other hand the sample complexity of the of the Nook of the new classes of EC dimension of the new class over epsilon square.",
            "So it will be now D squared over epsilon squared.",
            "So we need more examples in order to learn the larger class.",
            "So statistically we need more examples, but once we have more examples we can.",
            "We don't have a statistical problem.",
            "And the search problem becomes efficient.",
            "OK so.",
            "What do you mean more examples?",
            "When that was, you could possibly have this financing.",
            "So the number of even if you have so the number of examples to learn this problem is really the without any assumptions.",
            "Is this squared over epsilon squared?",
            "OK, because there are these square possible pairs.",
            "OK, but then for each pair you have a problem of determining the bias of the coin.",
            "OK, so you need one over epsilon squared.",
            "Examples for each pair.",
            "OK, so overall you need D squared over epsilon squared to find the bias of all the coins accurately enough.",
            "OK, so David David asked if there."
        ],
        [
            "Is a lower bound.",
            "I will get to your question in a minute.",
            "Let me just show the current situation so we know this curve.",
            "We know two points on these curves.",
            "We know that if you if you have the examples.",
            "Then it seems like you need to go over all the permutations, so you need the factorial D time, but once you have this query examples.",
            "Then one Passover the training set gives you an ERM 4H U, so you need time this square.",
            "OK, but this this is based on upper bounds.",
            "OK, so the question is what happens in between and are these upper bounds really tight?",
            "OK."
        ],
        [
            "So the question is of is it possible to learn efficiently with D log D examples was posed an open problem.",
            "By Jake abernathy.",
            "In cold 2010 and by Kleiber get all in.",
            "Machine learning, 2010.",
            "And in a joint work with a lot of his own in certain color, we showed that actually you can learn efficiently with D log to the power of 3D.",
            "Over epsilon squared example, so it's still not D over epsilon square, but it's very close to it, OK?",
            "Which emphasizes the need for lower bounds OK."
        ],
        [
            "So according again, according to our current knowledge, according to the upper bounds, we have the current situation we have with the samples.",
            "We need the factorial time with slightly more samples.",
            "We already need polynomial time D to the power of four, and when you have much more examples this query examples, the time decreases even more OK, But again these curve is based on upper bounds.",
            "OK, and it is still open to obtain lower bound for this curve.",
            "Let me just briefly explain the idea of the proof of this point here.",
            "OK, so how are we managed to learn efficiently with the log D?"
        ],
        [
            "Examples, so the idea is that each permutation can be written as a matrix, so if you Simply put in the IJ element one.",
            "If I if I appears before J in the permutation OK now.",
            "We define a property of matrices which is interesting in other situations as well, but also in this situation.",
            "So we say that the matrix is better tell the composable if it symmetrization can be written as a difference between 2 positive semidefinite matrices P and North.",
            "That both of them have traced bounded by Tao and the diagonal entries are bounded by better, so better in Tao.",
            "Tao is closely related to the trace norm of the Matrix and beta is closely related to the Max norm of the matrix, but it's a little bit different and what we showed that there is an online algorithm with a regret of square root of Tau beta and logarithmic terms in the dimension.",
            "So this is true for predicting the elements of any better.",
            "Tell the composable mattress is now we need to show that permutation matrices are better.",
            "Tell the composable and indeed we show that they are better to with log D&D log D. So we have a regret of D log D and regret is buy online to batch conversions can be translated to the sample complexity.",
            "OK, what do you call a symmetrization of the matrix?",
            "So it's just to put to take the matrix and put put it here and the transpose of it here, and put zeros here OK. OK."
        ],
        [
            "So we have a first example which I like because it's very simple, but it already demonstrates the tradeoff between time and sample.",
            "Now I'm going to.",
            "Discuss another problem.",
            "Learning margin based half spaces.",
            "It is a little bit more involved and it is a hard problem and I will discuss another tradeoff which is approximation factor that we can achieve as a sample of the as a function of the number of samples available."
        ],
        [
            "So what is a statement of the problem?",
            "We need to find some classifier and mapping from X which will which here can be a Hilbert space or the unit ball for Hilbert spares an into plus minus one such that the probability to air.",
            "So this is a risk of HS should be at most one plus Alpha times the margin error rate.",
            "Of the best half space.",
            "So I'm taking a vector W with the unit norm and I'm looking at all the examples that are either on the wrong side of the half space or are too close to the half space.",
            "Have a margin violation OK?",
            "Plus epsilon, so we have here 2 parameters.",
            "One is the usual epsilon parameter, the other one is multiplicative approximation factor Alpha.",
            "So the question is what are find epsilon or achievable?",
            "Gamma you can think about Gamma is a parameter of the of the problem, but we will study this problem as a function of gamma.",
            "So like before as we studied the problem as a function of the VC dimension of hypothesis class.",
            "Here one over Gamma will serve as a VC dimension or the complexity of the hypothesis class and we study the problem as a function of one over gamma.",
            "OK.",
            "So what we know about this problem?",
            "So if we want Alpha to be 0.",
            "Which is the usual definition of margin based learning.",
            "Then we have problem with runtime.",
            "So the number of a been David and Simon showed that using the sample complexity which is here one over Gamma Square epsilon squared, one over Gamma Square is a complexity of the hypothesis class.",
            "So if you do empirical risk minimization, you obtain this guarantee with up with Alpha equals to 0.",
            "So the problem is that the runtime is not polynomial, but you can have runtime of exponential in one over Gamma Square.",
            "The idea is just to look at all the subsets of the training set of size one over gamma squared.",
            "So we know by the perceptual mistake bound that there is some subset that gives you the right answer.",
            "So just innamorate all of these subsets and take the one that minimizes the risk.",
            "This takes time exponential in one over gamma squared, and here we know that obtaining Alpha equals zero efficiently is impossible.",
            "So here we also have a lower bound.",
            "For the case Alpha equals 0.",
            "OK. What people do in practice is support vector machine support.",
            "Support vector machines you replace 01 loss with a hinge loss.",
            "But there is a large cost for doing this in the approximation factor, so you can show that the approximation factor of using the hinge loss is huge is actually one over the margin.",
            "It means that if the margin is, say, .01, which is considered a large margin, then the approximation factor that you have is 100 + 1.",
            "Which is horrible.",
            "But this is all what we have.",
            "On the other hand, the time becomes polynomial in one over gamma.",
            "So here we see a tradeoff between the approximation factor and the runtime.",
            "If we want an approximation factor.",
            "Zero, we need a lot of time and if we want time polynomial then we have a large approximation factor.",
            "And a natural question is, again, what is a what what's going on in between zero and one over gamma?"
        ],
        [
            "So in this nips, together with my student, aren't being bound, we show the following.",
            "We show that you can achieve Alpha approximation using time and sample complexity polynomial in one over gamma times, something which is exponential in one over gamma Alpha squared.",
            "From this it follows well if the approximation factor Alpha is one over Gamma's loss.",
            "Then here we have a constant in that OK, but even if Alpha is slightly smaller, so it will be one over gamma Times Square root of log one over gamma, so slightly better than what we obtain from the hinge loss, it will still be polynomial OK, But once you want Alpha.",
            "Which is considerably smaller than that.",
            "Then according to this upper bound.",
            "You will be again exponentiel OK. And let me briefly explain the proof idea.",
            "OK."
        ],
        [
            "So what we are doing in SVM instead of looking at the 01 loss, we look at the hinge loss.",
            "So the height here is one over gamma plus one, and this is the reason there is a multiplicative gap between one here and one over gamma plus one here, which gives you the horrible approximation factor.",
            "OK, So what if we can compose a hinge loss over a polynomial?",
            "So instead of taking this linear function here?",
            "Www.x over gamma.",
            "Let's take a polynomial of WX.",
            "OK, so if the polynomial behaves nicely, then we can have something better and this one will give gives us a better approximation factor.",
            "OK, so this looks great, but what's the problem?",
            "The loss function is no longer convex OK.",
            "The last function is is no logging convex.",
            "So how we overcome this problem?",
            "We use a kernel trick."
        ],
        [
            "So it's again the same story as before.",
            "What we are doing, so let P of X equals to sum over J better JX to the power of JB the polynomials that you want for the nice loss function what I depicted before was a 7 degree polynomial.",
            "OK, so we have this polynomial now.",
            "The original class is mapping of X to the composition of the polynomial over the linear function www.x.",
            "So define a kernel.",
            "With absolute values of the coefficients of the polynomial.",
            "So this is a positive semidefinite kernel.",
            "And then define a new class to be linear functions.",
            "Over a constructed, this space website is a mapping corresponding to the kernel that I just described.",
            "OK, now this class the new class can be learned efficiently because now it's linear.",
            "So we take composition of the hinge loss over linear function.",
            "We can solve it using kernel SVM with this kernel.",
            "OK, so this is we know how to solve.",
            "However we took a larger class.",
            "This class contains the original class but it contains more functions.",
            "So now the complexity, the sample complexity of learning this class is bound.",
            "On the norm of weight of V that we need OK and this so the sample complexity is B squared over epsilon squared and B controls the approximation factor that we will have.",
            "So we will control the tradeoff between the sample complexity and approximation factor.",
            "OK.",
            "So this is how we prove that you can have a smooth interpolation between the hinge loss and the case of a no approximation Alpha equals zero.",
            "OK, exact learning, but in exponential time OK we just control a family of such kernel functions.",
            "OK. Again, there is no lower bound."
        ],
        [
            "So the question is, maybe we came up with some polynomial, maybe someone can came up with can come up with a better polynomial OK and obtain better results.",
            "Which brings me to the 3rd result.",
            "We thought about this problem and then we came up with a disappointing lower amount.",
            "So this is, we say, amid an alien material.",
            "The lower bound tells you that for every kernel, so trying to run SVM with any kernel, you cannot obtain something which is significant significantly smaller than one over gamma Poly log gamma.",
            "OK, it should be sorry it should be Poly.",
            "Log one over gamma.",
            "OK so you cannot improve much because it is the upper bound that we have was one over Gamma Square root log one over gamma so it was Poly.",
            "Log one over gamma.",
            "OK so maybe you can.",
            "Get something better than the square root, but it will still be polylogarithmic in one over gamma, so the there will not be a large improvement.",
            "So you cannot do it in polynomial number of samples OK?",
            "But this is a lower bound for a specific technique for the idea of using SVM with kernels.",
            "Another problem is.",
            "Lower bounds for other techniques or for any technique optimally OK. Or maybe there is a technique in which the situation is not that bad, OK?",
            "OK, so few words."
        ],
        [
            "About how we prove this lower bound.",
            "It's actually quite involved, but let me try to give the main ingredients of the proof.",
            "So we start with a 1 dimensional problem which looks very very simple.",
            "So we take the distribution to be a convex combination of two distribution.",
            "D1 is just, there is all the examples are on either on minus gamma or on gamma.",
            "OK, so there are negative examples on minus gamma and positive examples on gamma and it's separable.",
            "Everything is nice and perfect OK?",
            "This is the one and think about Lambda is being very small, so the data is almost perfect.",
            "OK now we just add a little bit noise and the noise is just take the uniform distribution over.",
            "The interval between mine minus 1/2 and 1/2 so it doesn't look that bad.",
            "It's quite simple distribution.",
            "Nevertheless.",
            "It is not very difficult to show that every low degree polynomial.",
            "With Hingeless smaller than one.",
            "Must have the same prediction on gamma and minus gamma.",
            "The reason is that if the polynomial, so the reason is that polynomial of flow degree cannot have.",
            "Cannot look like a sigmoid OK, and so if you want the polynomial to to say minus one here and one here, it must do.",
            "It turn so if it if it is doing a turn then it will behave crazily.",
            "On the rest of the domain and then you will suffer from the little bit noise that there is here.",
            "Just a moment and otherwise if it behaves.",
            "If it should be something that close to 0 not to suffer because of the D2, then it will behave the same on gamma N minus gamma.",
            "So we start with just a 1 dimensional simple problem.",
            "Now we pull back the distribution to high dimension.",
            "And then we use.",
            "We start with a case of.",
            "RK Chase with symmetric kernels, so symmetric kernels means that the kernel only depends on the inner product between between the vectors, so KXX prime is a function of the inner product between XX prime like polynomial kernels for example.",
            "So for symmetric kernels we use some characterization of Hilbert spaces.",
            "Which enables us to write any function in this Hilbert space using Legendre polynomials and then do some reduction to the one dimensional case.",
            "OK. And then what about something that is not like polynomial kernel?",
            "Maybe wavelet, maybe some other kernel.",
            "So here we have a nice trick of averaging the kernel over the group of all linear is ametris over of our D. So by doing this averaging technique it's a little bit like the probabilistic method.",
            "We show that you will have some data distribution.",
            "OK that.",
            "Will make any kernel bad, so even wavelets or whatever.",
            "If you are in distribution free.",
            "Problem.",
            "Nothing you can do with kernels.",
            "OK, so this is quite strong, lower bound for kernel methods, But again it's for worst case scenarios.",
            "OK."
        ],
        [
            "So so I described it to two problems in which it seems that there is a tradeoff between samples and computational complexity.",
            "But all of them even, even though I showed you lower bounds, the lower bounds are for specific techniques.",
            "They are not global, so we were curious about can we find a problem on which we can really prove that there is a tradeoff between time and sample.",
            "So we have a synthetic problem on which we can prove.",
            "So the theorem joint work with Ohio MMR run terminal.",
            "It relies on the existence of one way functions and this is.",
            "This seems to be necessary to assume some cryptographic assumption becausw obtaining lower bounds for a for improper learning without cryptographic assumption seems to be impossible, but if you believe that there is one way permutation functions, which is widely believed in cryptography, then there exists an agnostic learning problem such that.",
            "We have the curve of time as a function of M that depends like that.",
            "That looks like this.",
            "So we have this part.",
            "So with one over Epsilon square examples you can learn with exponential number of with exponential time OK.",
            "But before you see log N examples, you cannot learn in polynomial time.",
            "OK, so we have a regime where polynomial time learning is impossible.",
            "On the other hand, once you have an over epsilon squared examples, so you have more more examples, then times becomes polynomial.",
            "OK, so we have a proof of the three arrows here, which shows you that there is this tradeoff between time and sample so."
        ],
        [
            "I will not go go over the proof of these three.",
            "These three terms.",
            "This is an illustration of one-way functions OK.",
            "I.",
            "So let me just."
        ],
        [
            "The."
        ],
        [
            "Say."
        ],
        [
            "Is that the idea of OK, so this is."
        ],
        [
            "Lower bound."
        ],
        [
            "And."
        ],
        [
            "The other one, the last."
        ],
        [
            "Upper bound the idea is again the same.",
            "The same trick of constructing a larger hypothesis class."
        ],
        [
            "As we saw."
        ],
        [
            "Or OK, so just to wrap up the bias variance tradeoff is well understood.",
            "But it ignores computational complexity.",
            "We studied the computational complexity computational sample tradeoff.",
            "Show you some evidence that more data can reduce the runtime of learning algorithms.",
            "There are many open questions in this talk.",
            "I focused on a single technique, the technique of constructing a larger hypothesis class, but there are other techniques as well.",
            "And the main open question is stronger.",
            "Lower bounds like I'll show you for the synthetic problem, but this time for real world problems.",
            "So thank you very much.",
            "So Peter asking if there are examples of within the polynomial time regime of reducing runtime as a function of more examples.",
            "OK, so actually I showed such an example again based on upper bound in the first row."
        ],
        [
            "Of learning preferences what, oh lower bounds.",
            "This would be very difficult to obtain.",
            "I mean lower bounds within the polynomial hierarchy.",
            "But if you're in the polynomial part of the code is.",
            "So you want you want to show a lower bound for something exists, right?",
            "So."
        ],
        [
            "So to show a lower bound here you need to prove that with this number of examples.",
            "There is no algorithm that runs in time disk where you must use the to the power of four, which in general in computational complexity is very difficult, so I.",
            "It would be amazing if someone can, but I doubt that it is possible.",
            "I mean we have few very few examples of lower bounds for the runtime within the polynomial hierarchy.",
            "Maybe for inverting matrices or so you've been asked if what if the hypothesis classes of polynomial size?",
            "Right?",
            "So I think this is not very interesting regime, because if the hypothesis classes of polynomial size, then you can implement the ERM in polynomial time.",
            "So I mean, maybe you can gain in.",
            "The power of the polynomial, but there is no.",
            "Hope for something.",
            "Very, very different.",
            "Other questions.",
            "Or I might get one.",
            "I'm just wondering if you have some result about when you resolve the ERM on HM, you know are are far you are from the best solution in in H we have some result about that.",
            "Well we have this."
        ],
        [
            "We have this.",
            "Hardness result, which tells you that you cannot be epsilon close, because otherwise it will contradict this result.",
            "So in general you can be we can be more than epsilon Phi.",
            "Other questions.",
            "So let's take the invite again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm I'm going to talk about the sample computational tradeoff, something that I work on.",
                    "label": 0
                },
                {
                    "sent": "Sorry, something that I worked on for awhile, but this specific talk will be based on 4 works from the last year.",
                    "label": 0
                },
                {
                    "sent": "The first is with Satian calendar at Hassan from called 2012.",
                    "label": 0
                },
                {
                    "sent": "Second is with my student Aaron Brown from these snips.",
                    "label": 0
                },
                {
                    "sent": "Word is.",
                    "label": 0
                },
                {
                    "sent": "Joint work with submitted nearly and anti lineal, which was submitted, and.",
                    "label": 1
                },
                {
                    "sent": "The last is so hard from here and there and drummer from a stats.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me quickly just build the notation.",
                    "label": 0
                },
                {
                    "sent": "We're dealing with the classical setting of agnostic PAC learning.",
                    "label": 1
                },
                {
                    "sent": "There is a hypothesis class which is a set of functions from an input domain X and label domain Y.",
                    "label": 0
                },
                {
                    "sent": "We have a loss function which takes a hypothesis and an example and output real number.",
                    "label": 0
                },
                {
                    "sent": "This is a distribution which is unknown to us over the product of X * Y.",
                    "label": 0
                },
                {
                    "sent": "The true risk of hypothesis is just the expected loss when you choose an example XY at random according to the distribution D. And the learner receives a training set which is M pairs of instance and label, and the goal of the learner is to use the training set so as to find the hypothesis which I denote by H sub S such that with high probability the true risk of AHS is not too far from the true risk of the optimal hypothesis in the hypothesis class.",
                    "label": 1
                },
                {
                    "sent": "OK, and one particular rule is an empirical risk minimization rule which simply minimizes empirical loss which is average loss over the examples in the training set.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is very standard agnostic PAC learning model OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When we analyze agnostic PAC learning, it is common to the compose the error of in learning algorithm, in particular the ERM rule into two terms.",
                    "label": 0
                },
                {
                    "sent": "The first term is approximation error, which is simply the minimal risk of the optimal path ASIS in the hypothesis class OK, and the second error term is a difference between the true error of the RMA prothesis.",
                    "label": 0
                },
                {
                    "sent": "And the approximation error.",
                    "label": 0
                },
                {
                    "sent": "So this decomposition is trivially true.",
                    "label": 0
                },
                {
                    "sent": "OK, but it gives us a natural tradeoff between approximation error.",
                    "label": 0
                },
                {
                    "sent": "The approximation error measures how well our prior knowledge is reflected by the hypothesis class.",
                    "label": 0
                },
                {
                    "sent": "H behaves well on the word as reflected by the distribution D. OK, so it's just a measure of the hypothesis class in the distribution.",
                    "label": 0
                },
                {
                    "sent": "While the estimation error follows from the fact that we only observe a finite training set S and we do not know the true distribution, so this is a difference between between the risk of the RMI party SIS what performs best on the training set and the true hypothesis.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a classical trade off.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Practice, however.",
                    "label": 0
                },
                {
                    "sent": "In many cases we cannot actually find the near my party SIS, so we find some another hypothesis and then we have 1/3 error term that comes into the picture.",
                    "label": 0
                },
                {
                    "sent": "The further error term I call it the optimization error.",
                    "label": 1
                },
                {
                    "sent": "This is a difference between the true risk of what the algorithm actually finds and the true risk of the RMA prothesis.",
                    "label": 0
                },
                {
                    "sent": "OK, so again the composition is trivial, is just adding and subtracting terms, but it gives us.",
                    "label": 0
                },
                {
                    "sent": "Some nice way to understand what's going on.",
                    "label": 1
                },
                {
                    "sent": "OK, so now when we come to a specific problem we have two resources.",
                    "label": 0
                },
                {
                    "sent": "We have samples and we have runtime and there is a actually a tradeoff between samples and runtime.",
                    "label": 1
                },
                {
                    "sent": "I would like to show you that sometime you can ask for more examples and gaining runtime or the other way around.",
                    "label": 0
                },
                {
                    "sent": "OK and this tradeoff was first described by a work of DECA tour.",
                    "label": 0
                },
                {
                    "sent": "Goldwire can drone.",
                    "label": 0
                },
                {
                    "sent": "In 1998 is called the sample computational complexity.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To be more specific, recall that our goal is to find the hypothesis.",
                    "label": 0
                },
                {
                    "sent": "Which is not too far away from the optimal hypothesis in the hypothesis class.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The sample complexity is how many examples are needed to achieve this goal and the time complexity is how much time is needed to achieve this goal.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And now we need to define a joint times sample computational.",
                    "label": 0
                },
                {
                    "sent": "So you can think about it as a function T that depends on the target epsilon and the hypothesis class H. And it is a function of the number of available training examples, which I denote by M. And this function measures how much time is needed when the size of the training set is M. So.",
                    "label": 1
                },
                {
                    "sent": "Usually we study sample complexity, which is this line here.",
                    "label": 0
                },
                {
                    "sent": "So sample complexity is even if we ignore computational complexity, we have infinite amount of time.",
                    "label": 0
                },
                {
                    "sent": "There is a barrier of the number of examples that are required to achieve the goal.",
                    "label": 1
                },
                {
                    "sent": "OK, the other symptomatic is called the data Laden domain.",
                    "label": 0
                },
                {
                    "sent": "This is what happens when examples are not an issue.",
                    "label": 0
                },
                {
                    "sent": "You have infinite amount of examples.",
                    "label": 0
                },
                {
                    "sent": "The only resource which is limited.",
                    "label": 0
                },
                {
                    "sent": "Is time?",
                    "label": 0
                },
                {
                    "sent": "And then the question is how?",
                    "label": 0
                },
                {
                    "sent": "What is the minimal amount of time you need in order to solve this problem when you can ask for whatever number of examples that you want, OK?",
                    "label": 0
                },
                {
                    "sent": "But there is an interesting thing in between.",
                    "label": 0
                },
                {
                    "sent": "I would like to show you that there is an actually decreasing function, so the time is decreasing as a function of the number of examples.",
                    "label": 0
                },
                {
                    "sent": "And if we can understand this curve, we can control the tradeoff between time and samples.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm going to start with a 2 examples of real world problems in which I conjecture that there is a such a tradeoff, but the conjecture is based on upper bounds.",
                    "label": 0
                },
                {
                    "sent": "OK, and the last one is a synthetic problem in which we formally established that there is such a phenomenon of function.",
                    "label": 0
                },
                {
                    "sent": "The time is decreasing with the number of examples.",
                    "label": 0
                },
                {
                    "sent": "OK, so the first thing is agnostic learning of.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "References it's a very simple but still nice problem.",
                    "label": 0
                },
                {
                    "sent": "The domain set is just the cross product of a D numbers from one 2D OK and the label set is just 01.",
                    "label": 0
                },
                {
                    "sent": "Given an instance which is just a pair I&J.",
                    "label": 0
                },
                {
                    "sent": "We should predict if I is preferrable over J. OK. And the label determines if I indeed preferable over J.",
                    "label": 1
                },
                {
                    "sent": "Now we need to define a hypothesis class and the natural hypothesis class for this problem is just is just a set of permutations over this.",
                    "label": 0
                },
                {
                    "sent": "So think about DSD teams and each IJ is a match between two teams and we need to determine if I is going to win or Jay is going to win and hypothesis class is just a global ranking of all the teams and then we predict according to whether I comes before J.",
                    "label": 0
                },
                {
                    "sent": "In the permutation OK and the last function is just 01.",
                    "label": 0
                },
                {
                    "sent": "If you are wrong, you pay one, and if you are correct you pay 0.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's start with the first approach.",
                    "label": 0
                },
                {
                    "sent": "For this problem, simply apply the empirical risk minimization rule over the set of permutations over the.",
                    "label": 0
                },
                {
                    "sent": "So the sample complexity if we ignore computational complexity, the sample complexity is simply D over epsilon square.",
                    "label": 0
                },
                {
                    "sent": "The reason is because this hypothesis classes visit dimension D. OK, so it's the standard sample complexity analysis for agnostic back learning.",
                    "label": 0
                },
                {
                    "sent": "OK, but what about time complexity?",
                    "label": 0
                },
                {
                    "sent": "So Varoon Canadian Thomas Standke showed that if RP not equal to NP, then it is not possible to efficiently find an epsilon accurate permutation.",
                    "label": 1
                },
                {
                    "sent": "So it seems like we cannot learn efficiently this class.",
                    "label": 0
                },
                {
                    "sent": "OK, but what I'm going to show you is it is that if the number of example is greater than this square over epsilon square, so it's more than the sample complexity.",
                    "label": 0
                },
                {
                    "sent": "But just polynomial in more than the sample complexity, then it is possible to find the predictor with error smaller than epsilon in polynomial time charter.",
                    "label": 1
                },
                {
                    "sent": "That was for then you have the over epsilon squared samples, no?",
                    "label": 0
                },
                {
                    "sent": "So the actually this this.",
                    "label": 0
                },
                {
                    "sent": "This hardener is hardness.",
                    "label": 0
                },
                {
                    "sent": "Results is for any number of examples.",
                    "label": 0
                },
                {
                    "sent": "So why there is not a contradiction between these two bullets?",
                    "label": 0
                },
                {
                    "sent": "So let me explain, so here the lower bound tells you that you cannot find an epsilon accurate permutation, so you must output function from the hypothesis class permutation and this is difficult to do computationally, but here I'm not going to tell you that.",
                    "label": 0
                },
                {
                    "sent": "That I will output a permutation, I will just output a function from X to Y, which is which has good prediction accuracy.",
                    "label": 0
                },
                {
                    "sent": "OK, but it will not be a permutation.",
                    "label": 0
                },
                {
                    "sent": "This is sometimes called improper learning.",
                    "label": 0
                },
                {
                    "sent": "But there is nothing improper in improper learning.",
                    "label": 0
                },
                {
                    "sent": "OK, so the difference is clear.",
                    "label": 0
                },
                {
                    "sent": "Exactly this is what I'm going to do.",
                    "label": 0
                },
                {
                    "sent": "OK, so how we achieve how we achieve this positive result?",
                    "label": 0
                },
                {
                    "sent": "So the question was it if in the original in the distribution the labels are determined by permutations?",
                    "label": 0
                },
                {
                    "sent": "Answer is not because we are dealing with agnostic setting, so we assume nothing about the true distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, we just compare ourselves to the best permanent.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the idea that the intuitive idea of how to how to obtain an efficient algorithm is that the ERM problem is a search problem over the set of permutation, and this is a difficult search problem because the space is not nice.",
                    "label": 0
                },
                {
                    "sent": "But if we can construct a nicer search problem.",
                    "label": 0
                },
                {
                    "sent": "Maybe we will have a larger space, but it will be nicer to search over then maybe we can do it efficiently when maybe we can obtain the search implemented search problem more efficiently and the idea is simple.",
                    "label": 0
                },
                {
                    "sent": "So let H and you be the set of all functions from X to Y.",
                    "label": 1
                },
                {
                    "sent": "So clearly it contains also all the functions that are defined by permutation.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's a super set of H Now.",
                    "label": 0
                },
                {
                    "sent": "To perform the empirical risk minimization over this set is very easy because what we need to do is just look at the sample look at all the pairs IJ in the sample and count the number of times they appear with the label one and the number the number of time they appear with the label zero, and then take a majority vote.",
                    "label": 0
                },
                {
                    "sent": "OK, so in one Passover the training set we can implement ERM over the new class.",
                    "label": 0
                },
                {
                    "sent": "OK, it it has a nice structure, everything is decomposable OK, but on the other hand the sample complexity of the of the Nook of the new classes of EC dimension of the new class over epsilon square.",
                    "label": 0
                },
                {
                    "sent": "So it will be now D squared over epsilon squared.",
                    "label": 0
                },
                {
                    "sent": "So we need more examples in order to learn the larger class.",
                    "label": 0
                },
                {
                    "sent": "So statistically we need more examples, but once we have more examples we can.",
                    "label": 0
                },
                {
                    "sent": "We don't have a statistical problem.",
                    "label": 0
                },
                {
                    "sent": "And the search problem becomes efficient.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "What do you mean more examples?",
                    "label": 0
                },
                {
                    "sent": "When that was, you could possibly have this financing.",
                    "label": 0
                },
                {
                    "sent": "So the number of even if you have so the number of examples to learn this problem is really the without any assumptions.",
                    "label": 0
                },
                {
                    "sent": "Is this squared over epsilon squared?",
                    "label": 0
                },
                {
                    "sent": "OK, because there are these square possible pairs.",
                    "label": 0
                },
                {
                    "sent": "OK, but then for each pair you have a problem of determining the bias of the coin.",
                    "label": 0
                },
                {
                    "sent": "OK, so you need one over epsilon squared.",
                    "label": 0
                },
                {
                    "sent": "Examples for each pair.",
                    "label": 0
                },
                {
                    "sent": "OK, so overall you need D squared over epsilon squared to find the bias of all the coins accurately enough.",
                    "label": 0
                },
                {
                    "sent": "OK, so David David asked if there.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is a lower bound.",
                    "label": 0
                },
                {
                    "sent": "I will get to your question in a minute.",
                    "label": 0
                },
                {
                    "sent": "Let me just show the current situation so we know this curve.",
                    "label": 0
                },
                {
                    "sent": "We know two points on these curves.",
                    "label": 0
                },
                {
                    "sent": "We know that if you if you have the examples.",
                    "label": 0
                },
                {
                    "sent": "Then it seems like you need to go over all the permutations, so you need the factorial D time, but once you have this query examples.",
                    "label": 0
                },
                {
                    "sent": "Then one Passover the training set gives you an ERM 4H U, so you need time this square.",
                    "label": 0
                },
                {
                    "sent": "OK, but this this is based on upper bounds.",
                    "label": 0
                },
                {
                    "sent": "OK, so the question is what happens in between and are these upper bounds really tight?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the question is of is it possible to learn efficiently with D log D examples was posed an open problem.",
                    "label": 1
                },
                {
                    "sent": "By Jake abernathy.",
                    "label": 0
                },
                {
                    "sent": "In cold 2010 and by Kleiber get all in.",
                    "label": 0
                },
                {
                    "sent": "Machine learning, 2010.",
                    "label": 0
                },
                {
                    "sent": "And in a joint work with a lot of his own in certain color, we showed that actually you can learn efficiently with D log to the power of 3D.",
                    "label": 0
                },
                {
                    "sent": "Over epsilon squared example, so it's still not D over epsilon square, but it's very close to it, OK?",
                    "label": 0
                },
                {
                    "sent": "Which emphasizes the need for lower bounds OK.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So according again, according to our current knowledge, according to the upper bounds, we have the current situation we have with the samples.",
                    "label": 0
                },
                {
                    "sent": "We need the factorial time with slightly more samples.",
                    "label": 0
                },
                {
                    "sent": "We already need polynomial time D to the power of four, and when you have much more examples this query examples, the time decreases even more OK, But again these curve is based on upper bounds.",
                    "label": 0
                },
                {
                    "sent": "OK, and it is still open to obtain lower bound for this curve.",
                    "label": 0
                },
                {
                    "sent": "Let me just briefly explain the idea of the proof of this point here.",
                    "label": 0
                },
                {
                    "sent": "OK, so how are we managed to learn efficiently with the log D?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Examples, so the idea is that each permutation can be written as a matrix, so if you Simply put in the IJ element one.",
                    "label": 1
                },
                {
                    "sent": "If I if I appears before J in the permutation OK now.",
                    "label": 0
                },
                {
                    "sent": "We define a property of matrices which is interesting in other situations as well, but also in this situation.",
                    "label": 1
                },
                {
                    "sent": "So we say that the matrix is better tell the composable if it symmetrization can be written as a difference between 2 positive semidefinite matrices P and North.",
                    "label": 1
                },
                {
                    "sent": "That both of them have traced bounded by Tao and the diagonal entries are bounded by better, so better in Tao.",
                    "label": 0
                },
                {
                    "sent": "Tao is closely related to the trace norm of the Matrix and beta is closely related to the Max norm of the matrix, but it's a little bit different and what we showed that there is an online algorithm with a regret of square root of Tau beta and logarithmic terms in the dimension.",
                    "label": 1
                },
                {
                    "sent": "So this is true for predicting the elements of any better.",
                    "label": 0
                },
                {
                    "sent": "Tell the composable mattress is now we need to show that permutation matrices are better.",
                    "label": 0
                },
                {
                    "sent": "Tell the composable and indeed we show that they are better to with log D&D log D. So we have a regret of D log D and regret is buy online to batch conversions can be translated to the sample complexity.",
                    "label": 0
                },
                {
                    "sent": "OK, what do you call a symmetrization of the matrix?",
                    "label": 0
                },
                {
                    "sent": "So it's just to put to take the matrix and put put it here and the transpose of it here, and put zeros here OK. OK.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have a first example which I like because it's very simple, but it already demonstrates the tradeoff between time and sample.",
                    "label": 0
                },
                {
                    "sent": "Now I'm going to.",
                    "label": 0
                },
                {
                    "sent": "Discuss another problem.",
                    "label": 0
                },
                {
                    "sent": "Learning margin based half spaces.",
                    "label": 0
                },
                {
                    "sent": "It is a little bit more involved and it is a hard problem and I will discuss another tradeoff which is approximation factor that we can achieve as a sample of the as a function of the number of samples available.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what is a statement of the problem?",
                    "label": 0
                },
                {
                    "sent": "We need to find some classifier and mapping from X which will which here can be a Hilbert space or the unit ball for Hilbert spares an into plus minus one such that the probability to air.",
                    "label": 0
                },
                {
                    "sent": "So this is a risk of HS should be at most one plus Alpha times the margin error rate.",
                    "label": 0
                },
                {
                    "sent": "Of the best half space.",
                    "label": 0
                },
                {
                    "sent": "So I'm taking a vector W with the unit norm and I'm looking at all the examples that are either on the wrong side of the half space or are too close to the half space.",
                    "label": 0
                },
                {
                    "sent": "Have a margin violation OK?",
                    "label": 0
                },
                {
                    "sent": "Plus epsilon, so we have here 2 parameters.",
                    "label": 0
                },
                {
                    "sent": "One is the usual epsilon parameter, the other one is multiplicative approximation factor Alpha.",
                    "label": 0
                },
                {
                    "sent": "So the question is what are find epsilon or achievable?",
                    "label": 0
                },
                {
                    "sent": "Gamma you can think about Gamma is a parameter of the of the problem, but we will study this problem as a function of gamma.",
                    "label": 0
                },
                {
                    "sent": "So like before as we studied the problem as a function of the VC dimension of hypothesis class.",
                    "label": 0
                },
                {
                    "sent": "Here one over Gamma will serve as a VC dimension or the complexity of the hypothesis class and we study the problem as a function of one over gamma.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So what we know about this problem?",
                    "label": 0
                },
                {
                    "sent": "So if we want Alpha to be 0.",
                    "label": 0
                },
                {
                    "sent": "Which is the usual definition of margin based learning.",
                    "label": 0
                },
                {
                    "sent": "Then we have problem with runtime.",
                    "label": 0
                },
                {
                    "sent": "So the number of a been David and Simon showed that using the sample complexity which is here one over Gamma Square epsilon squared, one over Gamma Square is a complexity of the hypothesis class.",
                    "label": 0
                },
                {
                    "sent": "So if you do empirical risk minimization, you obtain this guarantee with up with Alpha equals to 0.",
                    "label": 0
                },
                {
                    "sent": "So the problem is that the runtime is not polynomial, but you can have runtime of exponential in one over Gamma Square.",
                    "label": 0
                },
                {
                    "sent": "The idea is just to look at all the subsets of the training set of size one over gamma squared.",
                    "label": 0
                },
                {
                    "sent": "So we know by the perceptual mistake bound that there is some subset that gives you the right answer.",
                    "label": 0
                },
                {
                    "sent": "So just innamorate all of these subsets and take the one that minimizes the risk.",
                    "label": 0
                },
                {
                    "sent": "This takes time exponential in one over gamma squared, and here we know that obtaining Alpha equals zero efficiently is impossible.",
                    "label": 0
                },
                {
                    "sent": "So here we also have a lower bound.",
                    "label": 0
                },
                {
                    "sent": "For the case Alpha equals 0.",
                    "label": 1
                },
                {
                    "sent": "OK. What people do in practice is support vector machine support.",
                    "label": 0
                },
                {
                    "sent": "Support vector machines you replace 01 loss with a hinge loss.",
                    "label": 0
                },
                {
                    "sent": "But there is a large cost for doing this in the approximation factor, so you can show that the approximation factor of using the hinge loss is huge is actually one over the margin.",
                    "label": 0
                },
                {
                    "sent": "It means that if the margin is, say, .01, which is considered a large margin, then the approximation factor that you have is 100 + 1.",
                    "label": 0
                },
                {
                    "sent": "Which is horrible.",
                    "label": 0
                },
                {
                    "sent": "But this is all what we have.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, the time becomes polynomial in one over gamma.",
                    "label": 0
                },
                {
                    "sent": "So here we see a tradeoff between the approximation factor and the runtime.",
                    "label": 0
                },
                {
                    "sent": "If we want an approximation factor.",
                    "label": 1
                },
                {
                    "sent": "Zero, we need a lot of time and if we want time polynomial then we have a large approximation factor.",
                    "label": 0
                },
                {
                    "sent": "And a natural question is, again, what is a what what's going on in between zero and one over gamma?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this nips, together with my student, aren't being bound, we show the following.",
                    "label": 0
                },
                {
                    "sent": "We show that you can achieve Alpha approximation using time and sample complexity polynomial in one over gamma times, something which is exponential in one over gamma Alpha squared.",
                    "label": 1
                },
                {
                    "sent": "From this it follows well if the approximation factor Alpha is one over Gamma's loss.",
                    "label": 0
                },
                {
                    "sent": "Then here we have a constant in that OK, but even if Alpha is slightly smaller, so it will be one over gamma Times Square root of log one over gamma, so slightly better than what we obtain from the hinge loss, it will still be polynomial OK, But once you want Alpha.",
                    "label": 0
                },
                {
                    "sent": "Which is considerably smaller than that.",
                    "label": 0
                },
                {
                    "sent": "Then according to this upper bound.",
                    "label": 0
                },
                {
                    "sent": "You will be again exponentiel OK. And let me briefly explain the proof idea.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we are doing in SVM instead of looking at the 01 loss, we look at the hinge loss.",
                    "label": 0
                },
                {
                    "sent": "So the height here is one over gamma plus one, and this is the reason there is a multiplicative gap between one here and one over gamma plus one here, which gives you the horrible approximation factor.",
                    "label": 0
                },
                {
                    "sent": "OK, So what if we can compose a hinge loss over a polynomial?",
                    "label": 1
                },
                {
                    "sent": "So instead of taking this linear function here?",
                    "label": 0
                },
                {
                    "sent": "Www.x over gamma.",
                    "label": 0
                },
                {
                    "sent": "Let's take a polynomial of WX.",
                    "label": 0
                },
                {
                    "sent": "OK, so if the polynomial behaves nicely, then we can have something better and this one will give gives us a better approximation factor.",
                    "label": 0
                },
                {
                    "sent": "OK, so this looks great, but what's the problem?",
                    "label": 0
                },
                {
                    "sent": "The loss function is no longer convex OK.",
                    "label": 0
                },
                {
                    "sent": "The last function is is no logging convex.",
                    "label": 0
                },
                {
                    "sent": "So how we overcome this problem?",
                    "label": 0
                },
                {
                    "sent": "We use a kernel trick.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it's again the same story as before.",
                    "label": 0
                },
                {
                    "sent": "What we are doing, so let P of X equals to sum over J better JX to the power of JB the polynomials that you want for the nice loss function what I depicted before was a 7 degree polynomial.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have this polynomial now.",
                    "label": 0
                },
                {
                    "sent": "The original class is mapping of X to the composition of the polynomial over the linear function www.x.",
                    "label": 1
                },
                {
                    "sent": "So define a kernel.",
                    "label": 0
                },
                {
                    "sent": "With absolute values of the coefficients of the polynomial.",
                    "label": 0
                },
                {
                    "sent": "So this is a positive semidefinite kernel.",
                    "label": 0
                },
                {
                    "sent": "And then define a new class to be linear functions.",
                    "label": 1
                },
                {
                    "sent": "Over a constructed, this space website is a mapping corresponding to the kernel that I just described.",
                    "label": 1
                },
                {
                    "sent": "OK, now this class the new class can be learned efficiently because now it's linear.",
                    "label": 0
                },
                {
                    "sent": "So we take composition of the hinge loss over linear function.",
                    "label": 0
                },
                {
                    "sent": "We can solve it using kernel SVM with this kernel.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is we know how to solve.",
                    "label": 0
                },
                {
                    "sent": "However we took a larger class.",
                    "label": 1
                },
                {
                    "sent": "This class contains the original class but it contains more functions.",
                    "label": 0
                },
                {
                    "sent": "So now the complexity, the sample complexity of learning this class is bound.",
                    "label": 0
                },
                {
                    "sent": "On the norm of weight of V that we need OK and this so the sample complexity is B squared over epsilon squared and B controls the approximation factor that we will have.",
                    "label": 0
                },
                {
                    "sent": "So we will control the tradeoff between the sample complexity and approximation factor.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this is how we prove that you can have a smooth interpolation between the hinge loss and the case of a no approximation Alpha equals zero.",
                    "label": 0
                },
                {
                    "sent": "OK, exact learning, but in exponential time OK we just control a family of such kernel functions.",
                    "label": 0
                },
                {
                    "sent": "OK. Again, there is no lower bound.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the question is, maybe we came up with some polynomial, maybe someone can came up with can come up with a better polynomial OK and obtain better results.",
                    "label": 0
                },
                {
                    "sent": "Which brings me to the 3rd result.",
                    "label": 0
                },
                {
                    "sent": "We thought about this problem and then we came up with a disappointing lower amount.",
                    "label": 0
                },
                {
                    "sent": "So this is, we say, amid an alien material.",
                    "label": 0
                },
                {
                    "sent": "The lower bound tells you that for every kernel, so trying to run SVM with any kernel, you cannot obtain something which is significant significantly smaller than one over gamma Poly log gamma.",
                    "label": 1
                },
                {
                    "sent": "OK, it should be sorry it should be Poly.",
                    "label": 0
                },
                {
                    "sent": "Log one over gamma.",
                    "label": 0
                },
                {
                    "sent": "OK so you cannot improve much because it is the upper bound that we have was one over Gamma Square root log one over gamma so it was Poly.",
                    "label": 0
                },
                {
                    "sent": "Log one over gamma.",
                    "label": 0
                },
                {
                    "sent": "OK so maybe you can.",
                    "label": 0
                },
                {
                    "sent": "Get something better than the square root, but it will still be polylogarithmic in one over gamma, so the there will not be a large improvement.",
                    "label": 0
                },
                {
                    "sent": "So you cannot do it in polynomial number of samples OK?",
                    "label": 0
                },
                {
                    "sent": "But this is a lower bound for a specific technique for the idea of using SVM with kernels.",
                    "label": 0
                },
                {
                    "sent": "Another problem is.",
                    "label": 0
                },
                {
                    "sent": "Lower bounds for other techniques or for any technique optimally OK. Or maybe there is a technique in which the situation is not that bad, OK?",
                    "label": 1
                },
                {
                    "sent": "OK, so few words.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "About how we prove this lower bound.",
                    "label": 0
                },
                {
                    "sent": "It's actually quite involved, but let me try to give the main ingredients of the proof.",
                    "label": 0
                },
                {
                    "sent": "So we start with a 1 dimensional problem which looks very very simple.",
                    "label": 0
                },
                {
                    "sent": "So we take the distribution to be a convex combination of two distribution.",
                    "label": 0
                },
                {
                    "sent": "D1 is just, there is all the examples are on either on minus gamma or on gamma.",
                    "label": 0
                },
                {
                    "sent": "OK, so there are negative examples on minus gamma and positive examples on gamma and it's separable.",
                    "label": 0
                },
                {
                    "sent": "Everything is nice and perfect OK?",
                    "label": 0
                },
                {
                    "sent": "This is the one and think about Lambda is being very small, so the data is almost perfect.",
                    "label": 0
                },
                {
                    "sent": "OK now we just add a little bit noise and the noise is just take the uniform distribution over.",
                    "label": 0
                },
                {
                    "sent": "The interval between mine minus 1/2 and 1/2 so it doesn't look that bad.",
                    "label": 0
                },
                {
                    "sent": "It's quite simple distribution.",
                    "label": 0
                },
                {
                    "sent": "Nevertheless.",
                    "label": 0
                },
                {
                    "sent": "It is not very difficult to show that every low degree polynomial.",
                    "label": 1
                },
                {
                    "sent": "With Hingeless smaller than one.",
                    "label": 0
                },
                {
                    "sent": "Must have the same prediction on gamma and minus gamma.",
                    "label": 0
                },
                {
                    "sent": "The reason is that if the polynomial, so the reason is that polynomial of flow degree cannot have.",
                    "label": 0
                },
                {
                    "sent": "Cannot look like a sigmoid OK, and so if you want the polynomial to to say minus one here and one here, it must do.",
                    "label": 0
                },
                {
                    "sent": "It turn so if it if it is doing a turn then it will behave crazily.",
                    "label": 0
                },
                {
                    "sent": "On the rest of the domain and then you will suffer from the little bit noise that there is here.",
                    "label": 0
                },
                {
                    "sent": "Just a moment and otherwise if it behaves.",
                    "label": 0
                },
                {
                    "sent": "If it should be something that close to 0 not to suffer because of the D2, then it will behave the same on gamma N minus gamma.",
                    "label": 0
                },
                {
                    "sent": "So we start with just a 1 dimensional simple problem.",
                    "label": 0
                },
                {
                    "sent": "Now we pull back the distribution to high dimension.",
                    "label": 1
                },
                {
                    "sent": "And then we use.",
                    "label": 0
                },
                {
                    "sent": "We start with a case of.",
                    "label": 0
                },
                {
                    "sent": "RK Chase with symmetric kernels, so symmetric kernels means that the kernel only depends on the inner product between between the vectors, so KXX prime is a function of the inner product between XX prime like polynomial kernels for example.",
                    "label": 1
                },
                {
                    "sent": "So for symmetric kernels we use some characterization of Hilbert spaces.",
                    "label": 0
                },
                {
                    "sent": "Which enables us to write any function in this Hilbert space using Legendre polynomials and then do some reduction to the one dimensional case.",
                    "label": 1
                },
                {
                    "sent": "OK. And then what about something that is not like polynomial kernel?",
                    "label": 0
                },
                {
                    "sent": "Maybe wavelet, maybe some other kernel.",
                    "label": 0
                },
                {
                    "sent": "So here we have a nice trick of averaging the kernel over the group of all linear is ametris over of our D. So by doing this averaging technique it's a little bit like the probabilistic method.",
                    "label": 0
                },
                {
                    "sent": "We show that you will have some data distribution.",
                    "label": 0
                },
                {
                    "sent": "OK that.",
                    "label": 0
                },
                {
                    "sent": "Will make any kernel bad, so even wavelets or whatever.",
                    "label": 0
                },
                {
                    "sent": "If you are in distribution free.",
                    "label": 0
                },
                {
                    "sent": "Problem.",
                    "label": 0
                },
                {
                    "sent": "Nothing you can do with kernels.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is quite strong, lower bound for kernel methods, But again it's for worst case scenarios.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So so I described it to two problems in which it seems that there is a tradeoff between samples and computational complexity.",
                    "label": 0
                },
                {
                    "sent": "But all of them even, even though I showed you lower bounds, the lower bounds are for specific techniques.",
                    "label": 0
                },
                {
                    "sent": "They are not global, so we were curious about can we find a problem on which we can really prove that there is a tradeoff between time and sample.",
                    "label": 0
                },
                {
                    "sent": "So we have a synthetic problem on which we can prove.",
                    "label": 0
                },
                {
                    "sent": "So the theorem joint work with Ohio MMR run terminal.",
                    "label": 0
                },
                {
                    "sent": "It relies on the existence of one way functions and this is.",
                    "label": 0
                },
                {
                    "sent": "This seems to be necessary to assume some cryptographic assumption becausw obtaining lower bounds for a for improper learning without cryptographic assumption seems to be impossible, but if you believe that there is one way permutation functions, which is widely believed in cryptography, then there exists an agnostic learning problem such that.",
                    "label": 0
                },
                {
                    "sent": "We have the curve of time as a function of M that depends like that.",
                    "label": 0
                },
                {
                    "sent": "That looks like this.",
                    "label": 0
                },
                {
                    "sent": "So we have this part.",
                    "label": 0
                },
                {
                    "sent": "So with one over Epsilon square examples you can learn with exponential number of with exponential time OK.",
                    "label": 0
                },
                {
                    "sent": "But before you see log N examples, you cannot learn in polynomial time.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have a regime where polynomial time learning is impossible.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, once you have an over epsilon squared examples, so you have more more examples, then times becomes polynomial.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have a proof of the three arrows here, which shows you that there is this tradeoff between time and sample so.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I will not go go over the proof of these three.",
                    "label": 0
                },
                {
                    "sent": "These three terms.",
                    "label": 0
                },
                {
                    "sent": "This is an illustration of one-way functions OK.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "So let me just.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Say.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that the idea of OK, so this is.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lower bound.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The other one, the last.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Upper bound the idea is again the same.",
                    "label": 0
                },
                {
                    "sent": "The same trick of constructing a larger hypothesis class.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As we saw.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or OK, so just to wrap up the bias variance tradeoff is well understood.",
                    "label": 0
                },
                {
                    "sent": "But it ignores computational complexity.",
                    "label": 0
                },
                {
                    "sent": "We studied the computational complexity computational sample tradeoff.",
                    "label": 0
                },
                {
                    "sent": "Show you some evidence that more data can reduce the runtime of learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "There are many open questions in this talk.",
                    "label": 0
                },
                {
                    "sent": "I focused on a single technique, the technique of constructing a larger hypothesis class, but there are other techniques as well.",
                    "label": 0
                },
                {
                    "sent": "And the main open question is stronger.",
                    "label": 0
                },
                {
                    "sent": "Lower bounds like I'll show you for the synthetic problem, but this time for real world problems.",
                    "label": 0
                },
                {
                    "sent": "So thank you very much.",
                    "label": 0
                },
                {
                    "sent": "So Peter asking if there are examples of within the polynomial time regime of reducing runtime as a function of more examples.",
                    "label": 0
                },
                {
                    "sent": "OK, so actually I showed such an example again based on upper bound in the first row.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of learning preferences what, oh lower bounds.",
                    "label": 0
                },
                {
                    "sent": "This would be very difficult to obtain.",
                    "label": 0
                },
                {
                    "sent": "I mean lower bounds within the polynomial hierarchy.",
                    "label": 0
                },
                {
                    "sent": "But if you're in the polynomial part of the code is.",
                    "label": 0
                },
                {
                    "sent": "So you want you want to show a lower bound for something exists, right?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to show a lower bound here you need to prove that with this number of examples.",
                    "label": 0
                },
                {
                    "sent": "There is no algorithm that runs in time disk where you must use the to the power of four, which in general in computational complexity is very difficult, so I.",
                    "label": 0
                },
                {
                    "sent": "It would be amazing if someone can, but I doubt that it is possible.",
                    "label": 0
                },
                {
                    "sent": "I mean we have few very few examples of lower bounds for the runtime within the polynomial hierarchy.",
                    "label": 1
                },
                {
                    "sent": "Maybe for inverting matrices or so you've been asked if what if the hypothesis classes of polynomial size?",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So I think this is not very interesting regime, because if the hypothesis classes of polynomial size, then you can implement the ERM in polynomial time.",
                    "label": 0
                },
                {
                    "sent": "So I mean, maybe you can gain in.",
                    "label": 0
                },
                {
                    "sent": "The power of the polynomial, but there is no.",
                    "label": 0
                },
                {
                    "sent": "Hope for something.",
                    "label": 0
                },
                {
                    "sent": "Very, very different.",
                    "label": 0
                },
                {
                    "sent": "Other questions.",
                    "label": 0
                },
                {
                    "sent": "Or I might get one.",
                    "label": 0
                },
                {
                    "sent": "I'm just wondering if you have some result about when you resolve the ERM on HM, you know are are far you are from the best solution in in H we have some result about that.",
                    "label": 0
                },
                {
                    "sent": "Well we have this.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have this.",
                    "label": 0
                },
                {
                    "sent": "Hardness result, which tells you that you cannot be epsilon close, because otherwise it will contradict this result.",
                    "label": 0
                },
                {
                    "sent": "So in general you can be we can be more than epsilon Phi.",
                    "label": 0
                },
                {
                    "sent": "Other questions.",
                    "label": 0
                },
                {
                    "sent": "So let's take the invite again.",
                    "label": 0
                }
            ]
        }
    }
}