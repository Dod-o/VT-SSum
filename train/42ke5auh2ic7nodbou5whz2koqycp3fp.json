{
    "id": "42ke5auh2ic7nodbou5whz2koqycp3fp",
    "title": "Topic-Link LDA: Joint Models of Topic and Author Community",
    "info": {
        "author": [
            "Yan Liu, Computer Science Department, University of Southern California"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_liu_tllda/",
    "segmentation": [
        [
            "Alright, so hello everyone.",
            "My name is Ellie and now I'm going to talk about our recent work on Topic Link LDA which is an extension of the LDA model to jointly model topic and author communities.",
            "So in the practice of machine learning and data mining where you're really facing a lot of apps."
        ],
        [
            "Vacations actually with two types of information together we call it linked documents.",
            "Essentially, there will be 2 parts.",
            "One is tax that will contain unstructured information describing the property of the end entities and also we will have link that is a graph information connecting the relationship between entities.",
            "And here we just give a few examples of the many we have the block analysis and then we'll have the post with hyperlink.",
            "Connecting to other posts on the other side, we will have scientific publication with papers citing other papers on similar topic and also in this movie review we will have user and movie connecting to each other at the same time we will have text describing the content of the movie in those type of applications where usually facing 3."
        ],
        [
            "Tasks and this will include topic identification.",
            "That is, we want to discover the underlying hidden topics and we are also interested in the Community discovery.",
            "That is, who is friend of whom.",
            "And the third is the link prediction that is in the future whether there will be a link between these entities.",
            "And previous work on those kind of applications usually treat them independently.",
            "For example, in this blog analysis we have derived a lot of graphical model approach for topic modeling and at the same time we have graph mining approach in data mining for community."
        ],
        [
            "Discovery on service side.",
            "We will have combined the topic and content together for the link prediction and more recently people have been developing a series of models to make use of the two type of information together.",
            "Most of them is trying to use the link information to help guide the topic modeling to give you an example.",
            "More recently we have the relation relational topic models.",
            "They will assume that the top link will be generated based on the topic.",
            "Based on a multinomial distribution which is parameterized based on the content similarity an.",
            "In this work we are actually discovering a new aspect of this particular trend.",
            "That is, we kind of view that actually the content similarity may not necessarily capture the link formation itself.",
            "And here in this slide I'm going to give a motivation of what we're going to do in this paper.",
            "On one of the data set we are working on which I will explain later.",
            "An experiment in detail, but for a collection what we are trying to plot in this particular picture is that we will have a collection of documents and we will select a pair of dark pair of documents based on whether there will be a link between them.",
            "We label them as positive and negative and then we will plot the content similarity for those two type of distributions.",
            "And we can definitely observe that kind of different and we want to draw attention on two extremes.",
            "That is, on the left extreme we will have content similarity as really low, as zero are kind of slow as kind of low, but we will see that a lot of positive examples in there on the other side we will also see on the other extreme that we will have positive will have content similarity which is really high, but people don't necessarily linked to each other.",
            "And we were thinking that this might be contained some additional information in the link that cannot be explained by the content similarity itself, and specifically in this blog analysis we might infer that if the two posts linked to each other, but they share little unknown information, maybe these two bloggers are friends to each other.",
            "They just like to cite each other on the other side is that if we are actually talking about exactly the same thing, but we are not.",
            "We are not aware of the existence of each other.",
            "Then we may not necessarily put a link to each other.",
            "So in this particular work, what we're trying to do is introducing additional author community hidden variable inside the latent original allocation and then use that model.",
            "The link generation and here."
        ],
        [
            "Is the graphical model representation of the topic link LDA model.",
            "On the left will have the original LDA model.",
            "I won't go into the detail since we are now familiar with it thanks to the previous presenters, and now I will go directly to our topic.",
            "Link out the model.",
            "The difference in there is that we will introduce additional author community hidden variables that it will be a different racial distribution.",
            "Parameterized by Copa.",
            "And then the link generation G will actually be determined jointly by the content similarity that is the topic proportion and also the Community similarity determined by the Community propulsion mill, and specifically we model the link.",
            "The existence of a link by a Bernoulli distribution, which is a logistic function determined by a linear combination of content similarity and community similarity.",
            "And as we will show later that in this part using this particular model, we are actually able to do 3 tasks at the same model.",
            "That is, we can do this topic modeling using the out thanks to the LDA model and then we will infer the author community using the hidden variable we introduced.",
            "And we can also make the link prediction based on the assumptions we have in this Bernoulli distribution.",
            "And we're going to go over a little bit."
        ],
        [
            "Detail in terms of the inference and learning in this particular topic.",
            "Link LDA.",
            "Here we saw the likelihood of the data and we kind of decompose them into three parts.",
            "The first part I want to go through is this text modeling using the LDA, which is similar, which is exactly the same as the LDA model and then we have the Community information and then we will have the links.",
            "To do the inference and learning for this particular model, we have to borrow the variational EM algorithm, which I will describe in detail.",
            "But to joy a little bit attention on the details for those who are interested here, we introduced this logistic function which will be difficult to handle in this variational framework, and therefore we also have to borrow another variational function for this logistic function.",
            "And for now."
        ],
        [
            "Next we will go through the Eastep Anam step for the Eastern.",
            "We are going to estimate the expectations of the hidden variables.",
            "In this particular chart, we label the hidden variables using two colors.",
            "One is green, which is actually different from the LDA and the other is blue, which is the same that I won't go through in the detail for the green that we will have the topic proportion and we will have the Community proportion I will just use the topic proportion as an example.",
            "Here we will have the expectation of the likelihood that is associated with the topic proportion Theta and.",
            "The corresponding variation of variable will be gamma and to go through the detail a little bit I just want to decompose this particular expectation into two parts.",
            "One is similar to the LDA model and the other on the 1st part is something we introduce Additionally and this we want to only emphasize that this is the part we want to bring in the content similarity and community similarity together so that we can use that as a regular right?"
        ],
        [
            "Awesome.",
            "And for the parameter estimation we will just use the eastep and we can derive them.",
            "Point updating equations for the ways for the coefficients and that is the towel in the LDA model."
        ],
        [
            "And now I'm going to the experiment setup, which might be more interesting.",
            "In this experiment, we conduct our experiment on three different datasets.",
            "This will include two block data set on topic Web 2.0 technologies and the other is on US politics and the third data set is the Scientific paper citation data from sites here and that is the coral data set.",
            "We only select papers on machine learning data mining.",
            "So that people get familiar with the topics in there.",
            "The 1st result we're going to show is."
        ],
        [
            "Is the topic modeling and in this evaluation where using the perplexity of the models on the holdout set in this particular chat, we plot the perplexity of the holdout set with the very varying the number of hidden topics, and in general we can see that the topic link LDA model is able to achieve lower perplexity than the LDA model, and the competing link LDA model.",
            "And we just here show some examples of identified topics on the Cora data set, and it seems kind of reasonable because it kind of identify major topics in machine learning.",
            "That is, the reinforcement learning the genetic networks, the classification, and it's natural.",
            "On the second set of experiment results we will."
        ],
        [
            "Show the community discovery.",
            "Here we are comparing the spectral clustering, which only uses the graph information.",
            "And here we show just the matrix that for each author which community they will be belong to.",
            "On the first side we can see the topic LDA model is able to achieve a more balanced classification or clustering of the author communities and when we look at in detail we want to use the political.",
            "Dataset as an example for the spectral clustering.",
            "Because the graph or the political data set is relatively sparse, it outputs an exact output.",
            "The classroom results with only one big clusters on the other side.",
            "Our topic LDA model is able to make use both of the content information and the link information together, and we identified two major communities and when we examine those communities, we actually find that one of them belong to the Democratic and the other belong to.",
            "The the other side, so it will be interesting to see how those two communities interact with each other.",
            "And here we need some of the examples for the Discover community on the Cora data set.",
            "We have to mention this is relatively old data set, so it may not necessarily reflecting the current status of friendlies within the Community, but with Tom's permission.",
            "I want to use the example on the 4th group that we have Tom Dietrich on the.",
            "On this particular, in this particular cluster, and yesterday I saw him to get sitting together with William Cohen.",
            "But I'm going to vote it out recently."
        ],
        [
            "And the third re set of results we are going to show is the link formation that is where interested in discovering how the content similarity and community similarity contribute to the formation of a link.",
            "We want to define the contribution of Community similarity accountant similarities and we use a relatively loose definition by using the corresponding coefficients.",
            "Multiply the estimated mean of the similarity scores.",
            "And here in this chart we show the proportion of Community similarity, similarity, contribution and content contribution and with the number varying the number of hidden topics.",
            "In general, we will see that the proportion will be lower than one, which means that still counting similarity will contribute a significant amount in link formation.",
            "But there is some interesting points in there is that.",
            "It seems that on the political data set, people pay strong attention to the communities more than the scientific discovery or web two point or focus on technologies, which seems to indicate that we technology people are more interested about the content without trying to fighting with each other in person.",
            "And the full set of results we're going to show."
        ],
        [
            "Is the link prediction results where comparing with the graph based algorithm using preferential attachment and also the other baselines content based algorithm using the content similarity.",
            "We evaluate this using the precision, recall and F1 and here we show the results in general.",
            "Is that our topic link LDA model is able to achieve better score than the graph based and content based algorithm but we have to admit that the current score is still pretty loaded.",
            "It may not necessarily be applicable to the real applications.",
            "And."
        ],
        [
            "Finally we come to the conclusion in this paper while trying to develop a topic link LDA model to joint model the topic and author community together and the motivation we are using is that the formation of a link between two documents will be determined together by topic similarity, Anna Community similarity and advantage of our model compared with all the existing methods is that we're able to achieve those three tasks.",
            "That is, typing modeling, community discovery and link prediction in one unified model and at the same time and for future work will be interested in analyzing the time series linked documents.",
            "And finally, we want to thank people for useful discuss."
        ],
        [
            "Oceans and thank you."
        ],
        [
            "Attention."
        ],
        [
            "Time for questions.",
            "So could you go back to the slide that showed the graphical model, sure.",
            "Yeah, OK so.",
            "So the link existence is sort of acting as a constraint.",
            "Then the couple together, the other community and the optical.",
            "If you have data where so, so when you doing prediction then you assume you have data where you don't have the links.",
            "Yes, for the link prediction, we actually assume the author community remains stable on the training on how to set.",
            "When you showed the topics that came out of the corrugated says look like you were applying.",
            "Stuff.",
            "So we find it, that's.",
            "I think people go back and forth to living standards.",
            "Useful.",
            "You see any benefit or instrumental is for these personal.",
            "Yes, in the paper we do compare our results with the link Audi model in terms of the topic, we discovered, it seems more compact in the sense that we are able to identify the main topics in the area instead of specific, maybe isolated topics.",
            "Compared with the LDA model, but unfortunately didn't throw it in this picture.",
            "Yes, humans have a tendency to see patterns even in random data, and I think it's the next slide where you have these communities of researchers.",
            "I mean by now hypothesis is that that's random.",
            "If I look for double Gary Cottrell, who's interested, we exclusively and neural networks in Cognitive Sciences Group 3 and then all the mangasarian whose interests are pretty exclusively an optimization algorithm.",
            "Support vector machines is there's really there's no overlap between characters well and.",
            "All the necessary and I know them both personally and.",
            "Princess is pretty old, but I've never been allowed back in.",
            "That would be an overlapping.",
            "The research interests of those people and.",
            "Yeah.",
            "Michael Kerns and Food Shop medical running so heavy and Michael Flynn.",
            "OK, just copying other groups to make our group real groups being pushed into.",
            "There might be a chance because right now we're only seeing the card data set, and then it might dependent on the status set that we selected because we're actually excluding a lot of the paper not published in those communities, there might be a chance that it will be due to the data we selected, but I agree with childhood alot of the results we see here.",
            "Some of them make sense, some of them doesn't make that much sense in general.",
            "But it's just a way of discovering the community compared with other just link based algorithm itself.",
            "What weather warning for more than four groups?",
            "Oh yes, we actually sat to the group number as 10 and there will be more groups.",
            "We just don't have enough space to show them.",
            "And you're welcome to come to the poster session.",
            "We can show you more.",
            "OK, thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so hello everyone.",
                    "label": 0
                },
                {
                    "sent": "My name is Ellie and now I'm going to talk about our recent work on Topic Link LDA which is an extension of the LDA model to jointly model topic and author communities.",
                    "label": 1
                },
                {
                    "sent": "So in the practice of machine learning and data mining where you're really facing a lot of apps.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Vacations actually with two types of information together we call it linked documents.",
                    "label": 1
                },
                {
                    "sent": "Essentially, there will be 2 parts.",
                    "label": 0
                },
                {
                    "sent": "One is tax that will contain unstructured information describing the property of the end entities and also we will have link that is a graph information connecting the relationship between entities.",
                    "label": 1
                },
                {
                    "sent": "And here we just give a few examples of the many we have the block analysis and then we'll have the post with hyperlink.",
                    "label": 0
                },
                {
                    "sent": "Connecting to other posts on the other side, we will have scientific publication with papers citing other papers on similar topic and also in this movie review we will have user and movie connecting to each other at the same time we will have text describing the content of the movie in those type of applications where usually facing 3.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tasks and this will include topic identification.",
                    "label": 0
                },
                {
                    "sent": "That is, we want to discover the underlying hidden topics and we are also interested in the Community discovery.",
                    "label": 0
                },
                {
                    "sent": "That is, who is friend of whom.",
                    "label": 0
                },
                {
                    "sent": "And the third is the link prediction that is in the future whether there will be a link between these entities.",
                    "label": 0
                },
                {
                    "sent": "And previous work on those kind of applications usually treat them independently.",
                    "label": 0
                },
                {
                    "sent": "For example, in this blog analysis we have derived a lot of graphical model approach for topic modeling and at the same time we have graph mining approach in data mining for community.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Discovery on service side.",
                    "label": 0
                },
                {
                    "sent": "We will have combined the topic and content together for the link prediction and more recently people have been developing a series of models to make use of the two type of information together.",
                    "label": 0
                },
                {
                    "sent": "Most of them is trying to use the link information to help guide the topic modeling to give you an example.",
                    "label": 1
                },
                {
                    "sent": "More recently we have the relation relational topic models.",
                    "label": 0
                },
                {
                    "sent": "They will assume that the top link will be generated based on the topic.",
                    "label": 0
                },
                {
                    "sent": "Based on a multinomial distribution which is parameterized based on the content similarity an.",
                    "label": 1
                },
                {
                    "sent": "In this work we are actually discovering a new aspect of this particular trend.",
                    "label": 1
                },
                {
                    "sent": "That is, we kind of view that actually the content similarity may not necessarily capture the link formation itself.",
                    "label": 0
                },
                {
                    "sent": "And here in this slide I'm going to give a motivation of what we're going to do in this paper.",
                    "label": 0
                },
                {
                    "sent": "On one of the data set we are working on which I will explain later.",
                    "label": 0
                },
                {
                    "sent": "An experiment in detail, but for a collection what we are trying to plot in this particular picture is that we will have a collection of documents and we will select a pair of dark pair of documents based on whether there will be a link between them.",
                    "label": 0
                },
                {
                    "sent": "We label them as positive and negative and then we will plot the content similarity for those two type of distributions.",
                    "label": 1
                },
                {
                    "sent": "And we can definitely observe that kind of different and we want to draw attention on two extremes.",
                    "label": 0
                },
                {
                    "sent": "That is, on the left extreme we will have content similarity as really low, as zero are kind of slow as kind of low, but we will see that a lot of positive examples in there on the other side we will also see on the other extreme that we will have positive will have content similarity which is really high, but people don't necessarily linked to each other.",
                    "label": 0
                },
                {
                    "sent": "And we were thinking that this might be contained some additional information in the link that cannot be explained by the content similarity itself, and specifically in this blog analysis we might infer that if the two posts linked to each other, but they share little unknown information, maybe these two bloggers are friends to each other.",
                    "label": 1
                },
                {
                    "sent": "They just like to cite each other on the other side is that if we are actually talking about exactly the same thing, but we are not.",
                    "label": 0
                },
                {
                    "sent": "We are not aware of the existence of each other.",
                    "label": 0
                },
                {
                    "sent": "Then we may not necessarily put a link to each other.",
                    "label": 0
                },
                {
                    "sent": "So in this particular work, what we're trying to do is introducing additional author community hidden variable inside the latent original allocation and then use that model.",
                    "label": 0
                },
                {
                    "sent": "The link generation and here.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is the graphical model representation of the topic link LDA model.",
                    "label": 1
                },
                {
                    "sent": "On the left will have the original LDA model.",
                    "label": 0
                },
                {
                    "sent": "I won't go into the detail since we are now familiar with it thanks to the previous presenters, and now I will go directly to our topic.",
                    "label": 0
                },
                {
                    "sent": "Link out the model.",
                    "label": 0
                },
                {
                    "sent": "The difference in there is that we will introduce additional author community hidden variables that it will be a different racial distribution.",
                    "label": 0
                },
                {
                    "sent": "Parameterized by Copa.",
                    "label": 0
                },
                {
                    "sent": "And then the link generation G will actually be determined jointly by the content similarity that is the topic proportion and also the Community similarity determined by the Community propulsion mill, and specifically we model the link.",
                    "label": 0
                },
                {
                    "sent": "The existence of a link by a Bernoulli distribution, which is a logistic function determined by a linear combination of content similarity and community similarity.",
                    "label": 1
                },
                {
                    "sent": "And as we will show later that in this part using this particular model, we are actually able to do 3 tasks at the same model.",
                    "label": 1
                },
                {
                    "sent": "That is, we can do this topic modeling using the out thanks to the LDA model and then we will infer the author community using the hidden variable we introduced.",
                    "label": 0
                },
                {
                    "sent": "And we can also make the link prediction based on the assumptions we have in this Bernoulli distribution.",
                    "label": 0
                },
                {
                    "sent": "And we're going to go over a little bit.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Detail in terms of the inference and learning in this particular topic.",
                    "label": 1
                },
                {
                    "sent": "Link LDA.",
                    "label": 0
                },
                {
                    "sent": "Here we saw the likelihood of the data and we kind of decompose them into three parts.",
                    "label": 1
                },
                {
                    "sent": "The first part I want to go through is this text modeling using the LDA, which is similar, which is exactly the same as the LDA model and then we have the Community information and then we will have the links.",
                    "label": 0
                },
                {
                    "sent": "To do the inference and learning for this particular model, we have to borrow the variational EM algorithm, which I will describe in detail.",
                    "label": 0
                },
                {
                    "sent": "But to joy a little bit attention on the details for those who are interested here, we introduced this logistic function which will be difficult to handle in this variational framework, and therefore we also have to borrow another variational function for this logistic function.",
                    "label": 0
                },
                {
                    "sent": "And for now.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Next we will go through the Eastep Anam step for the Eastern.",
                    "label": 0
                },
                {
                    "sent": "We are going to estimate the expectations of the hidden variables.",
                    "label": 1
                },
                {
                    "sent": "In this particular chart, we label the hidden variables using two colors.",
                    "label": 0
                },
                {
                    "sent": "One is green, which is actually different from the LDA and the other is blue, which is the same that I won't go through in the detail for the green that we will have the topic proportion and we will have the Community proportion I will just use the topic proportion as an example.",
                    "label": 1
                },
                {
                    "sent": "Here we will have the expectation of the likelihood that is associated with the topic proportion Theta and.",
                    "label": 0
                },
                {
                    "sent": "The corresponding variation of variable will be gamma and to go through the detail a little bit I just want to decompose this particular expectation into two parts.",
                    "label": 0
                },
                {
                    "sent": "One is similar to the LDA model and the other on the 1st part is something we introduce Additionally and this we want to only emphasize that this is the part we want to bring in the content similarity and community similarity together so that we can use that as a regular right?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Awesome.",
                    "label": 0
                },
                {
                    "sent": "And for the parameter estimation we will just use the eastep and we can derive them.",
                    "label": 0
                },
                {
                    "sent": "Point updating equations for the ways for the coefficients and that is the towel in the LDA model.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And now I'm going to the experiment setup, which might be more interesting.",
                    "label": 0
                },
                {
                    "sent": "In this experiment, we conduct our experiment on three different datasets.",
                    "label": 0
                },
                {
                    "sent": "This will include two block data set on topic Web 2.0 technologies and the other is on US politics and the third data set is the Scientific paper citation data from sites here and that is the coral data set.",
                    "label": 1
                },
                {
                    "sent": "We only select papers on machine learning data mining.",
                    "label": 0
                },
                {
                    "sent": "So that people get familiar with the topics in there.",
                    "label": 0
                },
                {
                    "sent": "The 1st result we're going to show is.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is the topic modeling and in this evaluation where using the perplexity of the models on the holdout set in this particular chat, we plot the perplexity of the holdout set with the very varying the number of hidden topics, and in general we can see that the topic link LDA model is able to achieve lower perplexity than the LDA model, and the competing link LDA model.",
                    "label": 1
                },
                {
                    "sent": "And we just here show some examples of identified topics on the Cora data set, and it seems kind of reasonable because it kind of identify major topics in machine learning.",
                    "label": 1
                },
                {
                    "sent": "That is, the reinforcement learning the genetic networks, the classification, and it's natural.",
                    "label": 0
                },
                {
                    "sent": "On the second set of experiment results we will.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Show the community discovery.",
                    "label": 0
                },
                {
                    "sent": "Here we are comparing the spectral clustering, which only uses the graph information.",
                    "label": 0
                },
                {
                    "sent": "And here we show just the matrix that for each author which community they will be belong to.",
                    "label": 0
                },
                {
                    "sent": "On the first side we can see the topic LDA model is able to achieve a more balanced classification or clustering of the author communities and when we look at in detail we want to use the political.",
                    "label": 0
                },
                {
                    "sent": "Dataset as an example for the spectral clustering.",
                    "label": 0
                },
                {
                    "sent": "Because the graph or the political data set is relatively sparse, it outputs an exact output.",
                    "label": 0
                },
                {
                    "sent": "The classroom results with only one big clusters on the other side.",
                    "label": 0
                },
                {
                    "sent": "Our topic LDA model is able to make use both of the content information and the link information together, and we identified two major communities and when we examine those communities, we actually find that one of them belong to the Democratic and the other belong to.",
                    "label": 0
                },
                {
                    "sent": "The the other side, so it will be interesting to see how those two communities interact with each other.",
                    "label": 0
                },
                {
                    "sent": "And here we need some of the examples for the Discover community on the Cora data set.",
                    "label": 0
                },
                {
                    "sent": "We have to mention this is relatively old data set, so it may not necessarily reflecting the current status of friendlies within the Community, but with Tom's permission.",
                    "label": 0
                },
                {
                    "sent": "I want to use the example on the 4th group that we have Tom Dietrich on the.",
                    "label": 0
                },
                {
                    "sent": "On this particular, in this particular cluster, and yesterday I saw him to get sitting together with William Cohen.",
                    "label": 0
                },
                {
                    "sent": "But I'm going to vote it out recently.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the third re set of results we are going to show is the link formation that is where interested in discovering how the content similarity and community similarity contribute to the formation of a link.",
                    "label": 1
                },
                {
                    "sent": "We want to define the contribution of Community similarity accountant similarities and we use a relatively loose definition by using the corresponding coefficients.",
                    "label": 1
                },
                {
                    "sent": "Multiply the estimated mean of the similarity scores.",
                    "label": 0
                },
                {
                    "sent": "And here in this chart we show the proportion of Community similarity, similarity, contribution and content contribution and with the number varying the number of hidden topics.",
                    "label": 0
                },
                {
                    "sent": "In general, we will see that the proportion will be lower than one, which means that still counting similarity will contribute a significant amount in link formation.",
                    "label": 0
                },
                {
                    "sent": "But there is some interesting points in there is that.",
                    "label": 0
                },
                {
                    "sent": "It seems that on the political data set, people pay strong attention to the communities more than the scientific discovery or web two point or focus on technologies, which seems to indicate that we technology people are more interested about the content without trying to fighting with each other in person.",
                    "label": 0
                },
                {
                    "sent": "And the full set of results we're going to show.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is the link prediction results where comparing with the graph based algorithm using preferential attachment and also the other baselines content based algorithm using the content similarity.",
                    "label": 1
                },
                {
                    "sent": "We evaluate this using the precision, recall and F1 and here we show the results in general.",
                    "label": 0
                },
                {
                    "sent": "Is that our topic link LDA model is able to achieve better score than the graph based and content based algorithm but we have to admit that the current score is still pretty loaded.",
                    "label": 0
                },
                {
                    "sent": "It may not necessarily be applicable to the real applications.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Finally we come to the conclusion in this paper while trying to develop a topic link LDA model to joint model the topic and author community together and the motivation we are using is that the formation of a link between two documents will be determined together by topic similarity, Anna Community similarity and advantage of our model compared with all the existing methods is that we're able to achieve those three tasks.",
                    "label": 1
                },
                {
                    "sent": "That is, typing modeling, community discovery and link prediction in one unified model and at the same time and for future work will be interested in analyzing the time series linked documents.",
                    "label": 0
                },
                {
                    "sent": "And finally, we want to thank people for useful discuss.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oceans and thank you.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Attention.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Time for questions.",
                    "label": 0
                },
                {
                    "sent": "So could you go back to the slide that showed the graphical model, sure.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK so.",
                    "label": 0
                },
                {
                    "sent": "So the link existence is sort of acting as a constraint.",
                    "label": 0
                },
                {
                    "sent": "Then the couple together, the other community and the optical.",
                    "label": 0
                },
                {
                    "sent": "If you have data where so, so when you doing prediction then you assume you have data where you don't have the links.",
                    "label": 0
                },
                {
                    "sent": "Yes, for the link prediction, we actually assume the author community remains stable on the training on how to set.",
                    "label": 0
                },
                {
                    "sent": "When you showed the topics that came out of the corrugated says look like you were applying.",
                    "label": 0
                },
                {
                    "sent": "Stuff.",
                    "label": 0
                },
                {
                    "sent": "So we find it, that's.",
                    "label": 0
                },
                {
                    "sent": "I think people go back and forth to living standards.",
                    "label": 0
                },
                {
                    "sent": "Useful.",
                    "label": 0
                },
                {
                    "sent": "You see any benefit or instrumental is for these personal.",
                    "label": 0
                },
                {
                    "sent": "Yes, in the paper we do compare our results with the link Audi model in terms of the topic, we discovered, it seems more compact in the sense that we are able to identify the main topics in the area instead of specific, maybe isolated topics.",
                    "label": 0
                },
                {
                    "sent": "Compared with the LDA model, but unfortunately didn't throw it in this picture.",
                    "label": 0
                },
                {
                    "sent": "Yes, humans have a tendency to see patterns even in random data, and I think it's the next slide where you have these communities of researchers.",
                    "label": 0
                },
                {
                    "sent": "I mean by now hypothesis is that that's random.",
                    "label": 0
                },
                {
                    "sent": "If I look for double Gary Cottrell, who's interested, we exclusively and neural networks in Cognitive Sciences Group 3 and then all the mangasarian whose interests are pretty exclusively an optimization algorithm.",
                    "label": 0
                },
                {
                    "sent": "Support vector machines is there's really there's no overlap between characters well and.",
                    "label": 0
                },
                {
                    "sent": "All the necessary and I know them both personally and.",
                    "label": 0
                },
                {
                    "sent": "Princess is pretty old, but I've never been allowed back in.",
                    "label": 0
                },
                {
                    "sent": "That would be an overlapping.",
                    "label": 0
                },
                {
                    "sent": "The research interests of those people and.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Michael Kerns and Food Shop medical running so heavy and Michael Flynn.",
                    "label": 0
                },
                {
                    "sent": "OK, just copying other groups to make our group real groups being pushed into.",
                    "label": 0
                },
                {
                    "sent": "There might be a chance because right now we're only seeing the card data set, and then it might dependent on the status set that we selected because we're actually excluding a lot of the paper not published in those communities, there might be a chance that it will be due to the data we selected, but I agree with childhood alot of the results we see here.",
                    "label": 0
                },
                {
                    "sent": "Some of them make sense, some of them doesn't make that much sense in general.",
                    "label": 0
                },
                {
                    "sent": "But it's just a way of discovering the community compared with other just link based algorithm itself.",
                    "label": 0
                },
                {
                    "sent": "What weather warning for more than four groups?",
                    "label": 0
                },
                {
                    "sent": "Oh yes, we actually sat to the group number as 10 and there will be more groups.",
                    "label": 0
                },
                {
                    "sent": "We just don't have enough space to show them.",
                    "label": 0
                },
                {
                    "sent": "And you're welcome to come to the poster session.",
                    "label": 0
                },
                {
                    "sent": "We can show you more.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}