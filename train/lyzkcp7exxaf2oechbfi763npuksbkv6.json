{
    "id": "lyzkcp7exxaf2oechbfi763npuksbkv6",
    "title": "Regularization Strategies and Empirical Bayesian Learning for MKL",
    "info": {
        "author": [
            "Ryota Tomioka, Toyota Technological Institute at Chicago"
        ],
        "published": "Jan. 12, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning",
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2010_tomioka_rse/",
    "segmentation": [
        [
            "So what I want to."
        ],
        [
            "Convey in this talk is the first topic is relationships between different regularization strategies.",
            "So you might have heard about even off regularization or Tikhonov regularization.",
            "These are regularization strategies on the kernel weights and the other one is a block norm formulation which involves no kernel weights.",
            "So the question is, are they equivalent an if they're equivalent?",
            "In which way their equivalent and the second topic is very much related to what Reco just right now told us it's about.",
            "Empirical Bayesian learning of so it maximizes my Sun Life.",
            "Likelihood for elearning MCL and it can be viewed as a non separable regularizer.",
            "So it's another regularization strategy on the kernel weight.",
            "So first before."
        ],
        [
            "Going into multiple kernel learning, I want to stress this point, so we first think about learning with a fixed kernel combination.",
            "So just fix the kernel weights which is non negative but doesn't necessarily sum to one.",
            "And this is our kernel function and we optimize over functions lying in the Hilbert space associated.",
            "This combined kernel, so we learn single function with standard regulator and this is actually equivalent to learning M functions lying in individual feature spaces.",
            "F1 to FM and then we predict with some of these guys and the regularization term is the arcatus norm squared of these guys divided by the M and this is very important effect and actually it's pointed out already in the 1950 paper and also in Michelin Pontile paper.",
            "But it seems to have been overlooked a little bit in MCL community and the very important thing about this is that in the second formulation you have the kernel weights.",
            "Explicitly in the optimization objective, so you can think about how they optimize not only with respect to the functions, but also with respect to the kernel weights.",
            "But obviously if we minimize this guy also with reflected the kernel weights, then we obviously have a overfitting, because the more you increase the M, the smaller is the regularization term becomes.",
            "So we need some kind of regularization here."
        ],
        [
            "So one way is of course the well known, even if regulators review some, these complexity functions H which is convex and increasing, and say that this is constrained by 1 right and from the former argument we can always come back to a learning single function setting.",
            "So if it's now lying in the combined feature space and we minimize this guy, this SVM objective for example under the constraint that the kernel wait some too.",
            "The one under some convex general convex function, and Alternatively you can do."
        ],
        [
            "Thick enough regulation now instead of saying that this should sum to one, you add a penalty term.",
            "Here with Parimeter Mu that controls the balance between the first regularization term and the second regularization term.",
            "So if he come back again to the learning one function setting, then we can see that this is kind of a hierarchical Bayesian map kind of estimation, so we have a data fit term which can be interpreted like a likelihood, and the second term is a Gaussian process buyer over F and.",
            "This is a hyper prior on the kernel weights, so this is what leads us to the second formulation.",
            "But actually the question here is that."
        ],
        [
            "Are these even often pick on Africa lization really equivalent?",
            "And previously?",
            "Yeah, of course they are equivalent, but there are two parameters.",
            "See the regularization constant C and the."
        ],
        [
            "The weighting coefficient mu.",
            "So these two guys have to be twisted around a little bit so that two formulations become really equivalent.",
            "But actually we're showing that if you give up this one constant here in the even of formulation, these two formulations are really equivalent through two related block nor in formulations.",
            "And actually we show that CMU can be chosen independently, so there is no additional parameter in the taken of formulation, so there is no reason to prefer taken off or even off.",
            "In this formulation, just give up this constant one.",
            "OK, so we'll show this in the example.",
            "So let's choose HDM to BDM to the power penises called LP, Norm MCL proposed by Clough to tell an.",
            "But as you probably know, by using Jensen's inequality within edit with this block Norm formulation, which is the norm of FM and pick the Q norm of this guy and squared an obviously from the yen sensing.",
            "Inequality the minimum set obtained if you take them to be proportional to FM to the power of this guy and so Q share is defined as two P / 1 + B.",
            "So that means if PS1 then Q S1 and FPS Infinity goes to two and the case be goes to Infinity QX2 and you can see that this guy cancels out.",
            "And actually this is equivalent to the first guy with ODM equal to 1.",
            "That is uniform combination of kernel weights.",
            "So Q = 2.",
            "Respond to uniform weight, which is Pecos, Infinity, and."
        ],
        [
            "Actually, in the case of taken off, the story is almost the same, but one choice you have to make us just simply choose mu equals one or repeat and then you get this regularization term and then use Youngs inequality instead of Jensen's inequality.",
            "And then we get this block Norm formulation which is almost the same as the previous block.",
            "More information, but it's simply just part of the Q&A queue is defined the same and the minimum is obtained by taking the M which is non normalized version of the kernel weights we have seen in the previous slide.",
            "So if we compare."
        ],
        [
            "Are these two formulations that on the top we have the book learn formulation one obtained from Ivana Fertilization, which is the the block to norm Squared and the second block nor formulation we obtained from Tikhonov regularization.",
            "It simply affirm to the power of two, so obviously these two problems are equivalent and you just have to map fee.",
            "Regulation constancy here.",
            "An regularization constants with pillar here.",
            "So the choice of see it's not coupled with the choices new we have just fixed mu as one over P an OK so another difference said the implied.",
            "So these are block norm formulations so there is no kernel weights involved in this formulation, they're just implied from the minimality condition of Jens and inequality or Young's inequality.",
            "But in the case of the first block North formulation, the.",
            "Implied kernel weights are normalized, and the second case, the book nerd formulation.",
            "The kernel weights is not normalized, so that's the only difference.",
            "So actually these."
        ],
        [
            "Cloud formations don't involve any kernel weights, so you can actually start from block Norm formulation.",
            "So here is the general life bucko in formulation you have a function general concave function G here.",
            "So this is another way of stating new MTO problem and for example you can easily come up with Elastic net MCL by choosing GX to be like a convex combination of square root of XNX O. Sqrt X is obviously a concave function.",
            "Right, and if you substitute this into here then you get linear penalty term which is the L1 term and the squared penalty, which is quadratic term.",
            "And then you have you know this constant Lambda taking balance of these two terms.",
            "So this is purely like you know.",
            "Thinking purely in terms of the block norms here and not in terms of the kernel weights here, but of course we want to come back to either taken off or even if regulating station and see how it corresponds to kernel wait, wait based regularization.",
            "So here is the theory."
        ],
        [
            "So it's actually quite simple.",
            "The regularization term mu H&J Tikhonov regularization formulation is related to this G function through this concave conjugate up operation.",
            "So you take the concave conjugative D and difficulty star and put one over it to the M in inside and you get the correspondence between block North formulation and the Tikhonov regularization formulation.",
            "So the perfect.",
            "Pretty easy, just use the concavity of G and you can decompose this guy into two parts and what we want to show here is that if you minimize this guy together with a regularizer here, you end up with this guy.",
            "So that's what this inequality is saying, and this is generated can be considered as generalization of Youngs inequality.",
            "We used and taken off sitting right."
        ],
        [
            "So here couple of examples.",
            "If you choose GX to be square root of X, we get a linear penalty and this is L1 MCL.",
            "And if you choose GX to be X to the power of 2, / 2 and Q is somewhere between one and two.",
            "So again GX is concave and in this case by just computing this you get DMT therapy penalty here, so it's an NP gnome MTO and you can do the same for.",
            "For elastic net MCL as well."
        ],
        [
            "So then we get this nice table of showing different MCL models and the block norm regularizer.",
            "The concave function and the kernel weight regulator and the constant mu that corresponds to formulations, right?",
            "So yeah, so there are many things here.",
            "But The thing is that maybe if you come up with your idea in the book learn formation, but maybe it's easier to optimize in the kernel weight formulation, right?",
            "So on the other hand, if you might come up with some new MKO formulation in the kernel weight formulation.",
            "But maybe it's easier to optimize in the block Norm formulation, so that's how.",
            "Maybe this table is going to be useful and there are a lot of generalizations you can think about here."
        ],
        [
            "OK, so this was the end of the first part, and so let's switch gear, little bit and go to the basin view of MCL.",
            "So actually already as I stated and take on a organization formulation, you can see that there is a nice likelihood term and the prior over each function if want FM an hyper prior on the kernel wave here.",
            "So if I may write it like this you have a function that is generating.",
            "The kernel way through this exponential model, an Gaussian process buyer over each function FM, which have covariance matrix KM scaled by the kernel weight DM.",
            "So that means more larger the DM then you allow more variation in each component and the likelihood term is simply the exponentials negative loss, right?",
            "And it is not easy to do this for general loss, but for quadratic loss."
        ],
        [
            "It's easy and you can analytically compute the marginalized likelihood.",
            "It's has another nice decomposition here instead of FM.",
            "We have map solution for fixed kernel weights here, so this is like heard prior, but instead of the last term we have a volume based regularization term which comes from the logdet term of Gaussian likelihood.",
            "So Kate Bardi is here defined as the sum of observation noise plus the sum of kernels.",
            "So if we compare this to the original MK."
        ],
        [
            "Problem the original MKL is hyper prior map problem.",
            "That means that you minimize the sum of these three guys.",
            "An empirical basically minimize almost the same thing right until the second term is the same but the last term is different.",
            "So in MTL your hyper prior is separable into each component, so it's the sum of N terms.",
            "But in the case of empirical base MCL your regularizer is somehow nonseparable, it's a log determinant term so this is kind of more tricky to minimize.",
            "But EXE, I'm not going to details, but you can do this minimization efficiently and here."
        ],
        [
            "Are some empirical results, so this is very simple experiment.",
            "It's occultic 101, it's just 2 two class Canton versus Cup an actually the number of kernels is pretty large.",
            "It's like 1700 kernels and what you can see here is that of course uniform Colonel uniform MCL should go pretty well and almost the same or a little bitter is elastic net MTL, which weights the L1 and L2 terms.",
            "By the weight of Lambda equals 0.1 and somehow a little bit disappointing.",
            "Leaders basem TL doesn't perform so well.",
            "Maybe if the number of sample is large it's OK, but when the number of sample is small, it's it's not performing as good as uniform.",
            "Or yeah, it's performing as good as the basic naive MCL.",
            "So here this lawsuit is MCL with logistic loss, so this is almost like SM, but it performs almost the same as.",
            "The square loss to MCL, so all other elastic net or base MCL uses squared loss here OK, but this is just a performance so maybe."
        ],
        [
            "We should look into what kernels are selected and here we can see that we have constructed 1700 kernels by combining various features, an visual recognition and you can see that MKO models simply get pretty sparse.",
            "It only selects a couple of kernels, an uniform.",
            "It's of course uniform and Bayes MK also gets pretty sparse right?",
            "But elastic net MTO is doing something in between so it selects like half of the kernels.",
            "But leaves out half of the kernels, so this is a little bit now."
        ],
        [
            "And we can look into detail and we can see that in the case of Elastic net MTO it selects mostly CHI squared kernel and almost no Gaussian kernels.",
            "So this is it's good because you know in computer vision people have sought Chi Square is better than Gaussian because we're comparing histograms right?",
            "So elastic name Kayla can automatically figure this out.",
            "And also there are 10 different hyper.",
            "Parameter values here an elastic net like.",
            "Select the 9 out of 10.",
            "So in the case of Gaussian it uses the first one and leaves out all the other so it can also do some hyperparameter selection but not as sparse as MCL, but performance much better and base is kind of strange.",
            "It's selecting something different from the standard MCL, but yeah, performance of base, an MCL standard MCL is almost the same."
        ],
        [
            "OK, so to summarize.",
            "OK, I'm on time so there are a couple of different formulations of MCL, but what I want to say here is that there is no additional tuning parameter and Tikhonov formulation so you can be happy to play around with both.",
            "OK, so there are couple of correspondence I mentioned.",
            "You either use Jensen's inequality or Young's inequality, get to the generalized block.",
            "More information an the second part was that probabilistic view of MCL.",
            "It's a hierarchical Gaussian process model.",
            "You can do a map inference or.",
            "Or evidence maximization.",
            "And that's what we called empirical base MKL.",
            "So let's see clip.",
            "MCL performs almost as good as uniform and it selects like half of the kernel.",
            "So it's probably interesting in some application domain to look into which kernels are selected.",
            "Empirical Bayes are a little bit strange.",
            "I haven't figured out.",
            "And of course we haven't used any hyper prior on the kernel weights here as not as different from Raquel's talk, so of course that's the reason it gets so sparse.",
            "K The code for electronet MK is available on if you want to try it out and yeah."
        ],
        [
            "Thank you for your attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what I want to.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Convey in this talk is the first topic is relationships between different regularization strategies.",
                    "label": 1
                },
                {
                    "sent": "So you might have heard about even off regularization or Tikhonov regularization.",
                    "label": 0
                },
                {
                    "sent": "These are regularization strategies on the kernel weights and the other one is a block norm formulation which involves no kernel weights.",
                    "label": 1
                },
                {
                    "sent": "So the question is, are they equivalent an if they're equivalent?",
                    "label": 1
                },
                {
                    "sent": "In which way their equivalent and the second topic is very much related to what Reco just right now told us it's about.",
                    "label": 0
                },
                {
                    "sent": "Empirical Bayesian learning of so it maximizes my Sun Life.",
                    "label": 0
                },
                {
                    "sent": "Likelihood for elearning MCL and it can be viewed as a non separable regularizer.",
                    "label": 0
                },
                {
                    "sent": "So it's another regularization strategy on the kernel weight.",
                    "label": 0
                },
                {
                    "sent": "So first before.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Going into multiple kernel learning, I want to stress this point, so we first think about learning with a fixed kernel combination.",
                    "label": 1
                },
                {
                    "sent": "So just fix the kernel weights which is non negative but doesn't necessarily sum to one.",
                    "label": 0
                },
                {
                    "sent": "And this is our kernel function and we optimize over functions lying in the Hilbert space associated.",
                    "label": 1
                },
                {
                    "sent": "This combined kernel, so we learn single function with standard regulator and this is actually equivalent to learning M functions lying in individual feature spaces.",
                    "label": 0
                },
                {
                    "sent": "F1 to FM and then we predict with some of these guys and the regularization term is the arcatus norm squared of these guys divided by the M and this is very important effect and actually it's pointed out already in the 1950 paper and also in Michelin Pontile paper.",
                    "label": 0
                },
                {
                    "sent": "But it seems to have been overlooked a little bit in MCL community and the very important thing about this is that in the second formulation you have the kernel weights.",
                    "label": 0
                },
                {
                    "sent": "Explicitly in the optimization objective, so you can think about how they optimize not only with respect to the functions, but also with respect to the kernel weights.",
                    "label": 0
                },
                {
                    "sent": "But obviously if we minimize this guy also with reflected the kernel weights, then we obviously have a overfitting, because the more you increase the M, the smaller is the regularization term becomes.",
                    "label": 0
                },
                {
                    "sent": "So we need some kind of regularization here.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one way is of course the well known, even if regulators review some, these complexity functions H which is convex and increasing, and say that this is constrained by 1 right and from the former argument we can always come back to a learning single function setting.",
                    "label": 0
                },
                {
                    "sent": "So if it's now lying in the combined feature space and we minimize this guy, this SVM objective for example under the constraint that the kernel wait some too.",
                    "label": 0
                },
                {
                    "sent": "The one under some convex general convex function, and Alternatively you can do.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thick enough regulation now instead of saying that this should sum to one, you add a penalty term.",
                    "label": 0
                },
                {
                    "sent": "Here with Parimeter Mu that controls the balance between the first regularization term and the second regularization term.",
                    "label": 0
                },
                {
                    "sent": "So if he come back again to the learning one function setting, then we can see that this is kind of a hierarchical Bayesian map kind of estimation, so we have a data fit term which can be interpreted like a likelihood, and the second term is a Gaussian process buyer over F and.",
                    "label": 0
                },
                {
                    "sent": "This is a hyper prior on the kernel weights, so this is what leads us to the second formulation.",
                    "label": 0
                },
                {
                    "sent": "But actually the question here is that.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are these even often pick on Africa lization really equivalent?",
                    "label": 0
                },
                {
                    "sent": "And previously?",
                    "label": 0
                },
                {
                    "sent": "Yeah, of course they are equivalent, but there are two parameters.",
                    "label": 0
                },
                {
                    "sent": "See the regularization constant C and the.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The weighting coefficient mu.",
                    "label": 0
                },
                {
                    "sent": "So these two guys have to be twisted around a little bit so that two formulations become really equivalent.",
                    "label": 0
                },
                {
                    "sent": "But actually we're showing that if you give up this one constant here in the even of formulation, these two formulations are really equivalent through two related block nor in formulations.",
                    "label": 0
                },
                {
                    "sent": "And actually we show that CMU can be chosen independently, so there is no additional parameter in the taken of formulation, so there is no reason to prefer taken off or even off.",
                    "label": 0
                },
                {
                    "sent": "In this formulation, just give up this constant one.",
                    "label": 0
                },
                {
                    "sent": "OK, so we'll show this in the example.",
                    "label": 0
                },
                {
                    "sent": "So let's choose HDM to BDM to the power penises called LP, Norm MCL proposed by Clough to tell an.",
                    "label": 0
                },
                {
                    "sent": "But as you probably know, by using Jensen's inequality within edit with this block Norm formulation, which is the norm of FM and pick the Q norm of this guy and squared an obviously from the yen sensing.",
                    "label": 0
                },
                {
                    "sent": "Inequality the minimum set obtained if you take them to be proportional to FM to the power of this guy and so Q share is defined as two P / 1 + B.",
                    "label": 0
                },
                {
                    "sent": "So that means if PS1 then Q S1 and FPS Infinity goes to two and the case be goes to Infinity QX2 and you can see that this guy cancels out.",
                    "label": 0
                },
                {
                    "sent": "And actually this is equivalent to the first guy with ODM equal to 1.",
                    "label": 0
                },
                {
                    "sent": "That is uniform combination of kernel weights.",
                    "label": 0
                },
                {
                    "sent": "So Q = 2.",
                    "label": 0
                },
                {
                    "sent": "Respond to uniform weight, which is Pecos, Infinity, and.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually, in the case of taken off, the story is almost the same, but one choice you have to make us just simply choose mu equals one or repeat and then you get this regularization term and then use Youngs inequality instead of Jensen's inequality.",
                    "label": 0
                },
                {
                    "sent": "And then we get this block Norm formulation which is almost the same as the previous block.",
                    "label": 0
                },
                {
                    "sent": "More information, but it's simply just part of the Q&A queue is defined the same and the minimum is obtained by taking the M which is non normalized version of the kernel weights we have seen in the previous slide.",
                    "label": 0
                },
                {
                    "sent": "So if we compare.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Are these two formulations that on the top we have the book learn formulation one obtained from Ivana Fertilization, which is the the block to norm Squared and the second block nor formulation we obtained from Tikhonov regularization.",
                    "label": 0
                },
                {
                    "sent": "It simply affirm to the power of two, so obviously these two problems are equivalent and you just have to map fee.",
                    "label": 1
                },
                {
                    "sent": "Regulation constancy here.",
                    "label": 0
                },
                {
                    "sent": "An regularization constants with pillar here.",
                    "label": 0
                },
                {
                    "sent": "So the choice of see it's not coupled with the choices new we have just fixed mu as one over P an OK so another difference said the implied.",
                    "label": 0
                },
                {
                    "sent": "So these are block norm formulations so there is no kernel weights involved in this formulation, they're just implied from the minimality condition of Jens and inequality or Young's inequality.",
                    "label": 0
                },
                {
                    "sent": "But in the case of the first block North formulation, the.",
                    "label": 1
                },
                {
                    "sent": "Implied kernel weights are normalized, and the second case, the book nerd formulation.",
                    "label": 0
                },
                {
                    "sent": "The kernel weights is not normalized, so that's the only difference.",
                    "label": 0
                },
                {
                    "sent": "So actually these.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cloud formations don't involve any kernel weights, so you can actually start from block Norm formulation.",
                    "label": 0
                },
                {
                    "sent": "So here is the general life bucko in formulation you have a function general concave function G here.",
                    "label": 0
                },
                {
                    "sent": "So this is another way of stating new MTO problem and for example you can easily come up with Elastic net MCL by choosing GX to be like a convex combination of square root of XNX O. Sqrt X is obviously a concave function.",
                    "label": 0
                },
                {
                    "sent": "Right, and if you substitute this into here then you get linear penalty term which is the L1 term and the squared penalty, which is quadratic term.",
                    "label": 0
                },
                {
                    "sent": "And then you have you know this constant Lambda taking balance of these two terms.",
                    "label": 0
                },
                {
                    "sent": "So this is purely like you know.",
                    "label": 0
                },
                {
                    "sent": "Thinking purely in terms of the block norms here and not in terms of the kernel weights here, but of course we want to come back to either taken off or even if regulating station and see how it corresponds to kernel wait, wait based regularization.",
                    "label": 0
                },
                {
                    "sent": "So here is the theory.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it's actually quite simple.",
                    "label": 0
                },
                {
                    "sent": "The regularization term mu H&J Tikhonov regularization formulation is related to this G function through this concave conjugate up operation.",
                    "label": 0
                },
                {
                    "sent": "So you take the concave conjugative D and difficulty star and put one over it to the M in inside and you get the correspondence between block North formulation and the Tikhonov regularization formulation.",
                    "label": 1
                },
                {
                    "sent": "So the perfect.",
                    "label": 1
                },
                {
                    "sent": "Pretty easy, just use the concavity of G and you can decompose this guy into two parts and what we want to show here is that if you minimize this guy together with a regularizer here, you end up with this guy.",
                    "label": 0
                },
                {
                    "sent": "So that's what this inequality is saying, and this is generated can be considered as generalization of Youngs inequality.",
                    "label": 0
                },
                {
                    "sent": "We used and taken off sitting right.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here couple of examples.",
                    "label": 0
                },
                {
                    "sent": "If you choose GX to be square root of X, we get a linear penalty and this is L1 MCL.",
                    "label": 0
                },
                {
                    "sent": "And if you choose GX to be X to the power of 2, / 2 and Q is somewhere between one and two.",
                    "label": 0
                },
                {
                    "sent": "So again GX is concave and in this case by just computing this you get DMT therapy penalty here, so it's an NP gnome MTO and you can do the same for.",
                    "label": 0
                },
                {
                    "sent": "For elastic net MCL as well.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So then we get this nice table of showing different MCL models and the block norm regularizer.",
                    "label": 0
                },
                {
                    "sent": "The concave function and the kernel weight regulator and the constant mu that corresponds to formulations, right?",
                    "label": 0
                },
                {
                    "sent": "So yeah, so there are many things here.",
                    "label": 0
                },
                {
                    "sent": "But The thing is that maybe if you come up with your idea in the book learn formation, but maybe it's easier to optimize in the kernel weight formulation, right?",
                    "label": 0
                },
                {
                    "sent": "So on the other hand, if you might come up with some new MKO formulation in the kernel weight formulation.",
                    "label": 0
                },
                {
                    "sent": "But maybe it's easier to optimize in the block Norm formulation, so that's how.",
                    "label": 0
                },
                {
                    "sent": "Maybe this table is going to be useful and there are a lot of generalizations you can think about here.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this was the end of the first part, and so let's switch gear, little bit and go to the basin view of MCL.",
                    "label": 0
                },
                {
                    "sent": "So actually already as I stated and take on a organization formulation, you can see that there is a nice likelihood term and the prior over each function if want FM an hyper prior on the kernel wave here.",
                    "label": 1
                },
                {
                    "sent": "So if I may write it like this you have a function that is generating.",
                    "label": 1
                },
                {
                    "sent": "The kernel way through this exponential model, an Gaussian process buyer over each function FM, which have covariance matrix KM scaled by the kernel weight DM.",
                    "label": 0
                },
                {
                    "sent": "So that means more larger the DM then you allow more variation in each component and the likelihood term is simply the exponentials negative loss, right?",
                    "label": 0
                },
                {
                    "sent": "And it is not easy to do this for general loss, but for quadratic loss.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's easy and you can analytically compute the marginalized likelihood.",
                    "label": 1
                },
                {
                    "sent": "It's has another nice decomposition here instead of FM.",
                    "label": 0
                },
                {
                    "sent": "We have map solution for fixed kernel weights here, so this is like heard prior, but instead of the last term we have a volume based regularization term which comes from the logdet term of Gaussian likelihood.",
                    "label": 1
                },
                {
                    "sent": "So Kate Bardi is here defined as the sum of observation noise plus the sum of kernels.",
                    "label": 0
                },
                {
                    "sent": "So if we compare this to the original MK.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem the original MKL is hyper prior map problem.",
                    "label": 0
                },
                {
                    "sent": "That means that you minimize the sum of these three guys.",
                    "label": 0
                },
                {
                    "sent": "An empirical basically minimize almost the same thing right until the second term is the same but the last term is different.",
                    "label": 0
                },
                {
                    "sent": "So in MTL your hyper prior is separable into each component, so it's the sum of N terms.",
                    "label": 0
                },
                {
                    "sent": "But in the case of empirical base MCL your regularizer is somehow nonseparable, it's a log determinant term so this is kind of more tricky to minimize.",
                    "label": 0
                },
                {
                    "sent": "But EXE, I'm not going to details, but you can do this minimization efficiently and here.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are some empirical results, so this is very simple experiment.",
                    "label": 0
                },
                {
                    "sent": "It's occultic 101, it's just 2 two class Canton versus Cup an actually the number of kernels is pretty large.",
                    "label": 0
                },
                {
                    "sent": "It's like 1700 kernels and what you can see here is that of course uniform Colonel uniform MCL should go pretty well and almost the same or a little bitter is elastic net MTL, which weights the L1 and L2 terms.",
                    "label": 0
                },
                {
                    "sent": "By the weight of Lambda equals 0.1 and somehow a little bit disappointing.",
                    "label": 0
                },
                {
                    "sent": "Leaders basem TL doesn't perform so well.",
                    "label": 0
                },
                {
                    "sent": "Maybe if the number of sample is large it's OK, but when the number of sample is small, it's it's not performing as good as uniform.",
                    "label": 0
                },
                {
                    "sent": "Or yeah, it's performing as good as the basic naive MCL.",
                    "label": 0
                },
                {
                    "sent": "So here this lawsuit is MCL with logistic loss, so this is almost like SM, but it performs almost the same as.",
                    "label": 0
                },
                {
                    "sent": "The square loss to MCL, so all other elastic net or base MCL uses squared loss here OK, but this is just a performance so maybe.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We should look into what kernels are selected and here we can see that we have constructed 1700 kernels by combining various features, an visual recognition and you can see that MKO models simply get pretty sparse.",
                    "label": 0
                },
                {
                    "sent": "It only selects a couple of kernels, an uniform.",
                    "label": 0
                },
                {
                    "sent": "It's of course uniform and Bayes MK also gets pretty sparse right?",
                    "label": 0
                },
                {
                    "sent": "But elastic net MTO is doing something in between so it selects like half of the kernels.",
                    "label": 0
                },
                {
                    "sent": "But leaves out half of the kernels, so this is a little bit now.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we can look into detail and we can see that in the case of Elastic net MTO it selects mostly CHI squared kernel and almost no Gaussian kernels.",
                    "label": 0
                },
                {
                    "sent": "So this is it's good because you know in computer vision people have sought Chi Square is better than Gaussian because we're comparing histograms right?",
                    "label": 0
                },
                {
                    "sent": "So elastic name Kayla can automatically figure this out.",
                    "label": 0
                },
                {
                    "sent": "And also there are 10 different hyper.",
                    "label": 0
                },
                {
                    "sent": "Parameter values here an elastic net like.",
                    "label": 0
                },
                {
                    "sent": "Select the 9 out of 10.",
                    "label": 0
                },
                {
                    "sent": "So in the case of Gaussian it uses the first one and leaves out all the other so it can also do some hyperparameter selection but not as sparse as MCL, but performance much better and base is kind of strange.",
                    "label": 0
                },
                {
                    "sent": "It's selecting something different from the standard MCL, but yeah, performance of base, an MCL standard MCL is almost the same.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so to summarize.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm on time so there are a couple of different formulations of MCL, but what I want to say here is that there is no additional tuning parameter and Tikhonov formulation so you can be happy to play around with both.",
                    "label": 0
                },
                {
                    "sent": "OK, so there are couple of correspondence I mentioned.",
                    "label": 0
                },
                {
                    "sent": "You either use Jensen's inequality or Young's inequality, get to the generalized block.",
                    "label": 0
                },
                {
                    "sent": "More information an the second part was that probabilistic view of MCL.",
                    "label": 0
                },
                {
                    "sent": "It's a hierarchical Gaussian process model.",
                    "label": 1
                },
                {
                    "sent": "You can do a map inference or.",
                    "label": 0
                },
                {
                    "sent": "Or evidence maximization.",
                    "label": 0
                },
                {
                    "sent": "And that's what we called empirical base MKL.",
                    "label": 0
                },
                {
                    "sent": "So let's see clip.",
                    "label": 0
                },
                {
                    "sent": "MCL performs almost as good as uniform and it selects like half of the kernel.",
                    "label": 0
                },
                {
                    "sent": "So it's probably interesting in some application domain to look into which kernels are selected.",
                    "label": 0
                },
                {
                    "sent": "Empirical Bayes are a little bit strange.",
                    "label": 0
                },
                {
                    "sent": "I haven't figured out.",
                    "label": 0
                },
                {
                    "sent": "And of course we haven't used any hyper prior on the kernel weights here as not as different from Raquel's talk, so of course that's the reason it gets so sparse.",
                    "label": 0
                },
                {
                    "sent": "K The code for electronet MK is available on if you want to try it out and yeah.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you for your attention.",
                    "label": 0
                }
            ]
        }
    }
}