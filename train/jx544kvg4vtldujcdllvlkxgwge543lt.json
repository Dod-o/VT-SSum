{
    "id": "jx544kvg4vtldujcdllvlkxgwge543lt",
    "title": "Towards On-the-fly Large Scale Video Search",
    "info": {
        "author": [
            "Andrew Zisserman, Department of Engineering Science, University of Oxford"
        ],
        "published": "July 30, 2014",
        "recorded": "September 2013",
        "category": [
            "Top->Computer Science->Computer Vision",
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/bmvc2013_zisserman_video_search/",
    "segmentation": [
        [
            "Anne."
        ],
        [
            "What I'm going to talk about is visual search and.",
            "The goal here is to make everything searchable, so any image, any video we should be able to search for anything that's our go and buy anything.",
            "We mean objects, categories like airplanes, cars will touch it for people we should be able to search for people in particular poses.",
            "We should be able to search for people doing actions, events we should be able to search for anything.",
            "That's all go and not only will be able to search, we should be able to do it.",
            "With high precision, high recall, much like Google can search the web, getting results immediately.",
            "That's what we'd like to be able to do for any visual material.",
            "For anything, OK, that's all go now.",
            "Life would be easy if all our images were annotated with text that would then be very straightforward, but unfortunate."
        ],
        [
            "I.",
            "There's a problem.",
            "There are many datasets, maybe your own personal image collections which have no annotation apart from the Dayton GPS.",
            "This is sad, but this is true.",
            "So what are we going to do about that?",
            "Fortunately."
        ],
        [
            "There are other datasets like the web where there's plenty of annotation.",
            "We know that with Google we can type in some.",
            "String and we can find pretty much anything we want.",
            "OK, so we have these data sets which have no annotation.",
            "We have these datasets which are very rich annotation and the story of this talk."
        ],
        [
            "Is how to go from these vain?",
            "These datasets, which are very rich annotation to get information from these in order to be able to search these datasets with impoverished annotation.",
            "OK, that's what I'm going to talk about today.",
            "So."
        ],
        [
            "To make this definite.",
            "Say you wanted to find Boris Johnson.",
            "Some of you might not know Boris Johnson is this flamboyant Mayor of London.",
            "OK, now you might not know that.",
            "And so I say fine.",
            "Boris Johnson.",
            "The first thing you might do is go to Google and get images of Boris Johnson so you can see what he looks like and that's all we need to do to get a model.",
            "Now we can take these images, train a model.",
            "In this case we're classifier.",
            "Then we can use this classifier to go to our datasets.",
            "We have this big data set.",
            "We want to find if there any images of Boris Johnson that data set.",
            "We run the classifier, the data set and find images of Boris Johnson.",
            "The data set.",
            "OK, so the point here is that you can get training examples from image web search engine's effortlessly and at runtime this is the on the fly.",
            "You can learn a model and then use that to search.",
            "OK, that's the idea.",
            "So."
        ],
        [
            "That's what I'm concerned.",
            "Hopefully if demos work is show you demonstrations on this data set, this is a data set that apply to us by the BBC for research purposes, and it consists of about five months of broadcasts from primetime television.",
            "It's about 3000 hours and that corresponds to about 3 million keyframes.",
            "And I tried out to show you.",
            "Testing 1 two.",
            "So here we are.",
            "I'm going to say I want to find a painting.",
            "I want to find say, the Mona Lisa.",
            "An in this BBC data set."
        ],
        [
            "So what's happening now is I'm going to Google Image search, getting examples of Mona Lisa, who knows what looks like using this to search this 3 million keyframes, and that's the result.",
            "So what you're seeing here are keyframes from this BBC data set and below each keyframe you see the name of the program that the painting appears in, so you're seeing here paintings.",
            "There's a program here on Leonardo that's not so surprising.",
            "And see what else is you seeing.",
            "Variations in color variations in viewpoint that are being retrieved.",
            "So that's that's the idea.",
            "Something you're going to see.",
            "And."
        ],
        [
            "I'm going to divide the talk into three parts, each one based on a different type of objects that we want to retrieve.",
            "The first part is going to be retrieving specific objects.",
            "Like paintings that showed you but also could be logos, could be buildings and.",
            "That's that's what's called instant search.",
            "Then second part will generalize this instead of just looking for specific objects, will look for classes of objects like airplanes or cars courses, so there's some generalization there.",
            "And then in the third part will at Retreiving people searching for people on the fly, so this is a specialized specialized object class, obviously very important, and in each case what I'm going to do is.",
            "Hang on to this description of the state of the art.",
            "What's changed recently to make this.",
            "These areas perform well what visual technology is used.",
            "Just to give some background, this idea of using web search engine's to get training."
        ],
        [
            "Samples have been around for a long time, pretty much since web search engine started for images, and this is just a sample of the papers which have in the past learned from image search models merch."
        ],
        [
            "OK, so start off with."
        ],
        [
            "Instant search, so let's define the problem.",
            "So for image search, the idea is we're looking for a specific object, and usually that's defined by a query image.",
            "So say here we're looking for this logo.",
            "We have a query image of this.",
            "What we want to do is defined in our data set all the instances of this object, but we want to find them unaffected by scale, unaffected by viewpoint.",
            "So foreshortening perspective affects unaffected by lighting unaffected by partial occlusion and OK, that's that's our goal.",
            "Not only that, we want to find these instances instantly, so immediately in the manner of Google.",
            "OK, so there's double use of instance here and it's.",
            "Spinner outside has been around for a decade or so now, and it's very standard pipeline for doing it, which I think."
        ],
        [
            "Many of you will be aware.",
            "Also, I'm just going to sketch how this works.",
            "So we have this bag of words pipeline.",
            "So say we want to find this building in our data set.",
            "The processing training first of all, has to get representation for this image and what's used here is so called affine covariant detectors.",
            "So these are detectors like affine Harris, MCR from my task, look for regions in the image which are adapted to viewpoint.",
            "You find these regions, usually... And then each of these... is defined is described by SIFT descriptor.",
            "So we get to this stage here where we have a couple of thousands of regions, each one defined by SIFT descriptors.",
            "And now we have a couple of 1000 sift descriptors representing this image.",
            "OK, now by this stage we've done the invariant, so because of the regions we found and the SIFT descriptors we've got as much invariants as we're going to get to lighting and viewpoint.",
            "That's built in OK. Now I'm going to assume that you know what shift is, because this shift is going to flow through this work, just like it throws through flows through computer vision.",
            "So SIFT is 128 dimensional descriptor from David Lowe from 99 OK.",
            "So and that's going to be used for matching throughout.",
            "Right, like now comes making this inefficient.",
            "So in advance we have millions of these descriptors.",
            "They've been vector quantized using something like K means into what we call visual words.",
            "That's the kamien centers have say a million K might be a million here.",
            "Each of these shifts are assigned to their nearest kamien center visual word, and then we have a histogram of how many shifts are assigned to each of the visual words that's representation.",
            "Dimensional histogram is quite sparse.",
            "Then there's some weighting which comes from information retrieval TF IDF.",
            "Now that's a representation of this query image.",
            "Now before hand for all the images in the data set for the BBC data set, this same description has been built and also what is being built is what called an inverted index, so that for each visual word we know all the images it appears in and that's where we get the speed from.",
            "So now we have a visual word coming from the query.",
            "We can quickly find using this inverted index all the images it appears in.",
            "So for the entire visual words which appear in the query we get.",
            "A list of images which have something in common and they're ranked initially by this TF.",
            "IDF ranking now.",
            "So far so good, we get to here and then we're going to do some re ranking based on spatial consistency between the query and these target images.",
            "In this short list, or say something about that now just to explain it."
        ],
        [
            "So imagine we start with this image.",
            "This is our query.",
            "Want to find this bridge?",
            "Here we search for our database and."
        ],
        [
            "Get things like this so you're finding the bridge of different sizes and the yellow region here indicates the matching region and how that's got and how we rank this is."
        ],
        [
            "Using this spatial consistency.",
            "So here is our query.",
            "Here are the regions within that for each image in our short list.",
            "This might be 2 or 300 images.",
            "We compute an affine transformation between this region and and corresponding region Maps what's found in the target image that's done using RANSAC, something or some sort of sampling algorithm, and the number of matches here, which would be like 100 or so here.",
            "That's going to determine the ranking of this image.",
            "OK, so that's what goes on."
        ],
        [
            "Here we have a.",
            "We have this short list, a few 100 images for each one.",
            "We compute this transformation.",
            "The number of matches.",
            "A number of inliers.",
            "And gives the ranking of that image, so that's all done at runtime.",
            "That's what you saw in the painting example.",
            "So in the painting example we downloaded the images and then they use those to match and rerank using the spatial consistency."
        ],
        [
            "Right, so now I'm going to that.",
            "The background that's a sketch of the pipeline.",
            "Now I'm going to move on to something which has been done in the last few years, which is improved the performance.",
            "Now we're going to talk about performance.",
            "We have some way of measuring performance, so here's a standard data set that's used in this area for measuring performance, which is Boxford building data set.",
            "So this consists of a number of queries.",
            "So we call these landmarks and for each landmark we have 5 queries, 'cause 11 landmarks, 5 queries that, 55 queries in total, and for a data set of about 5000 images we know where all these buildings occur.",
            "So we know the ground truth.",
            "So the idea is we take one of these queries, do a search and then we can see how well we're doing, well, how many of the ground truth instances we retrieve?",
            "OK, that's how we're going to measure performance."
        ],
        [
            "And this is a typical results.",
            "We're getting five years ago, not ours, but people working in this area.",
            "So here are five queries for the same building, and here are the five corresponding precision recall curves so.",
            "Just explain precision recall curves.",
            "Precision is the proportion of the returns we get, which are correct, and recall is the proportion of the true instances in the data set that we get.",
            "So what proportion of all the instances do we get?",
            "These are typical curves.",
            "You get what we'd like to get is this.",
            "This would be a perfect precision recall curve.",
            "Would like to get that where we get back all the instances that will be the total recall and we don't make any mistakes, but we get.",
            "Is this OK so you can see this is typical retrieval systems at low recall.",
            "We're getting good precision.",
            "We're not making a mistake, so this is like the first few pages of Google.",
            "Everything is correct.",
            "And then higher recall depending on the query the position falls off and this would be recalled one where we retrieve all the instances.",
            "None of the queries in this case return all the instances, and I say we'd like."
        ],
        [
            "To be here and a lot of research in the last five or six years has been trying to get to hear.",
            "OK, I want to mention one thing which has happened in the last couple of years which is beneficial everywhere."
        ],
        [
            "And that's a way of improving Sift.",
            "OK, so we start with Sift.",
            "If we can improve safety then everything will improve and this isn't just true in these.",
            "Instant Search this is true for our computer vision because of ubiquity of sift and you think yourself help sift it's done by David Lowe.",
            "How could you improve on Sift?",
            "How could that be possible?",
            "And here's a thought process.",
            "We know from other work that if you're matching histograms, it's not a good idea to use Euclidean distance.",
            "Often you get better performance if you're comparing histograms by using something designed for histograms like the Earth movers, distance Chi Squared, Hellinger Colonel Intersection kernel, lots of histogram comparisons.",
            "We know that works best with histograms, then using Euclidean distance now and then.",
            "You think Sift is a histogram because it's a spatial binning of gradients.",
            "So maybe if we use the histogram better instead of Euclidean distance, which is normally used for sift, we could get better performance that thought process and the answer is you can go bit of mass to go through that now."
        ],
        [
            "So here's the this is a histogram kernel that we're going to use.",
            "So Helen Journal and it just what this is your your your histogram to L1 normalized so they add up to one and the kernel between them is simply take found by taking the square root of each element of this rummage bin value and then summing it.",
            "OK, that's how does your kernel.",
            "Now we need to go between distances and kernels, and there's a standard relationship for doing that.",
            "So that's done here.",
            "So now just imagine for the moment now the X&Y L2 normalized, so they have recreated normal one like Sift.",
            "Sift is L2 normalized.",
            "And usually we're sift.",
            "You would look at the distance between 2 subvectors X&Y by looking very clean distance, maybe squaring it or not, and then using low secondary neighbor tests or something like that.",
            "If we expand this and straightforward way because these are unit norm, each one of these once we get to hear and then minus two times a scalar product and I've written it out here and that was the Colonel in this linear case.",
            "OK, so this is a relationship between distance between two vectors in the kernel.",
            "Now if you compare this formula to this formula.",
            "You can see that to go from here to here we have two L1 normalize these vectors instead of to normalize them and then take the square root element wise and then that will make this equal to this.",
            "So now if we do that on both sides of the equation 'cause of course we must do that.",
            "So we replace X&Y, which were's effectors L2 normalized by first of all L1, normalizing them and then taking the square root element wise."
        ],
        [
            "Then we get to this.",
            "And it's like a trick, but what we've done is on the right here.",
            "Now we have the header kernel and on the left it says that if instead of using SIFT which is L2 normalized U L1 normalize it and take the elementwise square root, then instead of using and what you end up using to compare safety is going to be headed kernel which is a histogram measure.",
            "It feels to completely true."
        ],
        [
            "It's very simple, so here's the summarize of the steps.",
            "We want to get the internal to compare, Sift, Uiowa, normalize and then take the elementwise square root.",
            "We call that routes.",
            "If so, we dignify this with the name and the great thing here, and This is why we chose the header kernel is, it doesn't change the dimension of SIFT.",
            "OK, so we've done is taking the square root.",
            "So let's see."
        ],
        [
            "This working so say we plug this into this pipeline.",
            "What do we have to change?",
            "We've already computed loss if so."
        ],
        [
            "To do is take my square root so."
        ],
        [
            "We go from using Sift."
        ],
        [
            "Using root sift that also is done in K means or any other step.",
            "Wherever sifted used, we now replace it by root, sift and now we can look at the performance improvement."
        ],
        [
            "OK, so we start off with numbers so we have this 5000 dimensional sorry 5000 image dataset, 55 queries.",
            "We could get a single number by taking the mean of the average position over all those queries.",
            "OK, that's what measure is going to be.",
            "We're going to test that on the 5000 data set and also adding 100,000 other images, which actors distractors which don't have those queries in.",
            "So then that will be 105.",
            "1000 day set.",
            "So we have two datasets we're going to test on each case using the average precision averaged over all the mean of that overall 55 queries so you can look at the two lines where the arrows are.",
            "Here you can see that the changes in performance, so this is going from Sifter using roots if throughout.",
            "And just by doing that change, so just by taking these square root you get a 5% improvement for the smaller datasets and for the larger data set where the performance is generally lower.",
            "The 6% improvement simply by making that change.",
            "OK, so here's the idea or message.",
            "You can make everything work better if you use root sift.",
            "OK, so if you're using safety issues."
        ],
        [
            "Shift and things work better.",
            "And it's depressing recall curves.",
            "You concentrate on the red curve.",
            "That's where shift is being replaced by root shift.",
            "And as you see, they move towards the right, which is the way we want to go to get to our total recall.",
            "OK, so that's all you have to do."
        ],
        [
            "Now why it works?",
            "The intuition is that with.",
            "You created distance.",
            "It tends to be dominated by the large components by the large bins of a histogram that tends to dominate, and what the root does is to change the relative proportion.",
            "So if you look on the bottom, we start with X.",
            "If you have two bins, one we've sized .8 and one 5.2.",
            "We then plug it or pump it through the root and starts off by being four times as big .8 times point to .2 after it goes through the square root.",
            "Then it's only about twice as big, so we've changed the relative proportions and so that.",
            "Reduces dominance or that of these large bins.",
            "That's why it works.",
            "Is also a kernel.",
            "This is an explicit feature map.",
            "That's what's going on here."
        ],
        [
            "So here is the take home message for this part of the talk, OK?",
            "All you have to do to make your system work better is this line of Matlab.",
            "OK, so we have to.",
            "And you'll find that anywhere you sift it will now start working better.",
            "So this is true in image classification.",
            "In some vector quantization anyway you sift it will work better.",
            "Right, so I'll call this magic bullet."
        ],
        [
            "Now I've got time to talk about this, but there are many other pieces of work by other researchers which have been trying to improve the precision recall of these systems and you can look at this on the slides afterwards.",
            "These other methods."
        ],
        [
            "Now back to on the fly.",
            "So now we know the engine behind it.",
            "I just leave a message with the on the fly system so offline we have our 3 million images from this PC data set and of course I'm showing it on the BBC data set, but it could be your own images or any datasets very general and offline.",
            "For each image we compute these often covariant regions sift through SIFT, in this case of course now vector quantized into visual words and store.",
            "Is 1,000,000 dimensional sparse vector for each of these, we compute an inverted index, which so for each word we know all the images it occurs in, so that's all done offline, then online.",
            "So we're looking for the Earth.",
            "We use Google Image search to get images of the Earth, compute the same visual word representation, use the inverted index to find all the images.",
            "In the data set and get rank lists of these.",
            "OK so.",
            "Anne."
        ],
        [
            "This one issue then and how we plug this in, which is how do we combine the results from all these images we downloaded so we got say 20 images which describe our specific object.",
            "How do we combine the results?",
            "So here are two choices I present to you.",
            "One is we take all the.",
            "Vectors describing these images we downloaded.",
            "We average them.",
            "Now why would you do that?",
            "So it could be that in some.",
            "Some images of the detectors don't fire properly, so you miss visual words if you average them together then you're going to get a very good descriptor in which will catch some of the ones that we missed.",
            "In one image.",
            "You'll catch them.",
            "You have a super vector 4.",
            "Retreating from, you have to do one search.",
            "OK if you have to get the vantages.",
            "That's number 1 #2 is you query of each one, even though it might be missing some visual words and then you combine the results now.",
            "You can't really.",
            "You can't read use a wireless network at the moment, so I can actually ask you a question.",
            "So let's have a have a think about it.",
            "Which one do you think of these two is better?",
            "I'm going to.",
            "I'm not going to ask her to vote an informed vote.",
            "Who would like who thinks one is the best?",
            "Thank you for being an 2.",
            "OK, so it turns out two is better.",
            "One for for these sort of obvious this is the reasoning.",
            "OK, so the reasoning is if we have images like this, is the White House.",
            "We have two images like this.",
            "Then taking the average of these two makes a lot of sense because there's different elimination conditions you could imagine that you'll get different descriptors in these two cases.",
            "So taking average of these two is a good idea.",
            "But if you have a building you also get different viewpoints.",
            "So this is front and back of the White House and inside the White House.",
            "You don't have these together, so in general it seems to be better to do a search for each each one individually, and then the combined the results.",
            "And that's simple just by the number of matches to the query image.",
            "OK, so that seems to be the better way to do it."
        ],
        [
            "OK, I think I have another demo now.",
            "Demo, see.",
            "Let's go."
        ],
        [
            "Go back so I would look for a building and I've chosen to look for Buckingham Palace.",
            "So we're getting the training images Buckingham Palace and we're going to get different viewpoints of it.",
            "And then for each of these, as you know now we do a query.",
            "We do a query.",
            "OK, so that was that was searching for shoes million datasets.",
            "And we can see many, many programs or bucking Palace.",
            "So expect a Royal theme here.",
            "So we get to hear some programs, Diamond Queen Royal Bodyguard News at 10.",
            "And we can also organize this by programs to actually look at the programs that we are.",
            "This is a funny choice festival.",
            "Harris Bike is best to push in the Diamond Queen Diamond Queen, so these are various programs on the Queen, but also general news programs as well.",
            "OK and of course I said we can play these things.",
            "OK. At the end of the talk, at this time I'll let you shout out some queries we can do, but let me go through 1st."
        ],
        [
            "OK, so now I want to move on to the second part of the talk, which is category search so.",
            "Now we're going to go from specific objects.",
            "Two classes of objects.",
            "OK, so now we need some generalization, so we're looking for airplanes, not particularly subject to class of objects, and the technology here changes slightly because we're going to have generalization.",
            "We're going to now learn a classifier, and so we need training images positive negative.",
            "It's been some discriminative learning."
        ],
        [
            "OK, so now the technology we're using is shown here.",
            "We're going to have positive images, which is the ones we're going to get from the web, and we also have some negative images, so this is actually a set of images here.",
            "There will be some image encoding which will take us from the images to feature vector for each image and then learning which is going to be in our case be a classifier and we're going to use a linear SVM.",
            "Why do we use a linear SVM?",
            "We do a linear SVM becausw linear SVM is fast at runtime.",
            "OK, linear SVM, you have a single vector wallet W and we classify the image have a image represented by X.",
            "We classify the images by taking the scalar product dot X.",
            "That's very very fast.",
            "Just depends on the size of these vectors.",
            "OK, so that's why we're using a linear SVM, so we learn discriminate W vector.",
            "Then for testing.",
            "So this is now for the data set for each image we encode it in the same way.",
            "Have a feature vector, then we score it WX and that's the thing that is being the distance from the discriminative plane.",
            "That's a scoring that gives us a score for each image.",
            "Then we sort based on that and that gives us our ranking.",
            "And here's the examples for airplanes.",
            "So the first images will be airplanes.",
            "OK, that's that's the mechanism that underpins it.",
            "What I want to do is before we get onto the on the fly aspect, I'm going to talk about."
        ],
        [
            "The encoding part.",
            "How do we go from the image to the feature vector?",
            "'cause that's where there's been a lot of development over the last five or six years, and then after that I'll talk about how this Maps onto the on the fly and the issues there are.",
            "If you're going to make this thing work in real time, you have to be aware of memory footprint, how much memory you are using because you have to get everything into memory and think about size in order to get the right speed and performance."
        ],
        [
            "Right?",
            "Now back to image encoding and sift again.",
            "So here's the standard pipeline that was developed maybe 10 years ago now.",
            "Certainly helped 2006 of how to classify an image.",
            "In other words, how to decide how to decide an image has a particular object inside it.",
            "That's what we're looking at here.",
            "So we start with the image and the descriptor we get.",
            "We start with is again using Sift.",
            "So now instead of using these affine covariant region detectors, what was done is what called dense.",
            "If so, you cover the image with these sift footprints.",
            "And you do that multiple scales.",
            "That's now the representation you're starting with the image.",
            "OK, so again, again again, Sift might be 10s of thousands of these would be measured with 128 dimensional vectors.",
            "His shifts there, then vector quantized in the same way.",
            "So we use a bag of visual words representation.",
            "Now these are the shifts assigned to their various code words assigned to the visual words.",
            "That's what the color coding means here.",
            "Then you might bring some spatial information in, so you have this idea from Kristen Grauman traffic barrel of having a pyramid that was brought down to spatial by lacnic at all.",
            "So now you have spatial information by tiles.",
            "And then in each of these tiles you represent the histogram of visual words.",
            "Again, that's what's being shown here.",
            "Now if you're.",
            "Yo Kai in yo Kai means number of words.",
            "Your vocabulary, say 1000 dimensional.",
            "Then the size of this vector is 1000 times a number of spatial tiles you're using.",
            "So if you get, this can start getting large.",
            "OK, you do this for all your training images.",
            "That gives you positive training examples you have negative training examples which don't complain.",
            "Dogs in this case you learn the linear SVM and that's then your classifier, which we're going to use ranking OK, so that's the pipeline."
        ],
        [
            "Has been a lot of improvement in this area over the last since 2000.",
            "Six 2007.",
            "I'm I'm I want to briefly mention some of the improvements.",
            "Typically I want talk about Vlad and Fisher Vector to the improvements that happened.",
            "OK, again you can look at this list on the PDF afterwards."
        ],
        [
            "So the intuition here thing you have to keep in mind as you got these say 20,000 shifts you're measuring from image and what you'd like to represent.",
            "The distribution of these shifts in safe space.",
            "You have 128 dimensional space.",
            "You have all these vectors which have come from the image and you want to represent distribution and what the bag of words does is represented in a very coarse way.",
            "So here are the voran ourselves.",
            "This comes from the K means, so these are cells in the safe space.",
            "And these are the centers that they mean have been found.",
            "These these visual words W and we start with the shifts measured in the image there map to view in ourselves and then the representation is to count how many shifts are assigned to this cell.",
            "OK, so the representation of the sift, this distribution of shifts is a histogram of how many shifts are mapped to each of these visual words, OK?",
            "So this is very it's quite coarse representation of that distribution and what's happened over the last few years is try make this better.",
            "How can we?"
        ],
        [
            "This better and one thing is rather than just storing the count, you can store something about the distribution within a cell and one natural."
        ],
        [
            "To do is maybe store the mean of vectors, so what's being shown here is here.",
            "All the vectors assigned to this cell and also with across were showing the mean of those, which won't necessarily correspond to the visual word the came in center OK. That's when he's starting to represent a bit more about the distribution, and we can go even further and represent the mean and the covariance, or at least the variance of the descriptors.",
            "OK, so this one where we represent the mean is what Vlad Dazzle supervector stars."
        ],
        [
            "We represent the mean and the covariance or diagonal covariance matrix.",
            "In fact, is the Fisher vector OK, so that's what it's trying to do.",
            "It's trying to represent more about the distribution.",
            "And go into the flat one in."
        ],
        [
            "More detail.",
            "So we have this is there on my cell.",
            "Here are the shifts assigned that role in myself, and what Vlad actually sources is the sum of the residuals.",
            "So it stores the difference between the center here that came in Victor and the system should be assigned.",
            "It sums up these and that's what's stored.",
            "Now to go into that, imagine your K the number of ways 1000.",
            "If you're going to store these visuals.",
            "Each one of these, if you're using shift, is 128 dimensional.",
            "So your descriptor, which previously would say 1000 times the just account for each of those, now becomes 1000 times the size of 50,000 * 28, so it becomes bigger.",
            "You also have to multiply that by if you're doing Spata spatial encoding by the number of tiles you got in your spatial encoding, so that it becomes bigger still.",
            "So now you're getting these huge.",
            "Vectors, So what happens in Vlad and Fisher vectors?",
            "You reduce the size of recovery to start with.",
            "Down to 256 you also reduce the size of shift by using PCA before it goes in, so you get these example manageable sizes, but they can still be large.",
            "OK now this idea of reps anymore about the distribution does."
        ],
        [
            "Pay off then So what you're seeing here.",
            "Is how the performance has changed as we go at these different encodings, so we're going from the left.",
            "This is someone logical says 2007 up to say 2013 and I'll go through.",
            "What changed in the second?",
            "We're measuring this on the Pascal 2007 test set, so it's this is if you're not familiar with its twenty object classes, cars, airplanes, cats, dogs.",
            "Say horses, cats etc.",
            "About 5000 images that are being assessed OK, but it's not so important.",
            "We're testing on more the trend we're seeing.",
            "So let's go through it off with original bag of word descriptor with 4000 vocabulary you get some improvement if you increase the vocabulary size.",
            "So increase the K from 4 to 25 K. OK, so in fact that some improvement next improvement comes by not doing hard assignment, but by doing some form of soft assignment.",
            "This was the LLC assignment, so that gives an improvement and then the big improvement comes by sort by representing not just the count but now the residuals are mean of vectors, shifts assigned to the cells.",
            "We see the boost you get there, you get a further boost by representing the diagonal covariance as well.",
            "It's official vector and with various things we've learned in the last few years about Normalization, Square rooting, etc we can get.",
            "A larger boost still, so we're up here.",
            "So you can see that in these six years the performance is increased by 10%.",
            "All starting from the same descriptor, so we start with dense sift.",
            "In all cases this is.",
            "We start with dense.",
            "If computed across the image, just changing the encoding method is given this 10% improvement.",
            "So no extra cost over the original.",
            "An measurements in the image so it's impressive.",
            "Element over the field.",
            "I think now also need to pick.",
            "Start paying attention to the size of these scriptures to remember the size.",
            "Depends on the vocabulary size, how much of the safety represent and also the number of spatial tiles.",
            "I'll come back the second what we're going to use for the on the fly system is Vlad.",
            "Explain why after I show you."
        ],
        [
            "Let me again go through the anatomy of how these, how this is mapped onto the online system so.",
            "Offline we have our data set assuming images and we compute the.",
            "In our case of lab descriptors of all of these are computed.",
            "In advance and they need to be stored in memory so that they can be accessed quickly.",
            "Now at runtime we're going to get training examples using Google Image Search, so we're looking for motorbike.",
            "These would be the positive examples were doing discriminative learning now.",
            "So unlike the search I showed you before, we need negative images and we have a Bank of negative images that we precompute.",
            "It doesn't matter what they are, so much is just a general set of images which can be hopefully different from the ones we download.",
            "So again we've precomputed those descriptors.",
            "So at runtime we get these positives, encode them.",
            "Using Glad learner linear SVM.",
            "And then rank all the images in the data set using it OK.",
            "So."
        ],
        [
            "Do an example."
        ],
        [
            "So now.",
            "Search categories so the most boring category in the world I look for for cars.",
            "So now we're getting training examples of cars research, doing research.",
            "OK, some interesting cars mode ones.",
            "So viewpoints of cars.",
            "And now it's encoding all these, encoding them using black descriptors.",
            "And it's trained the SVM now its ranking.",
            "And this retreat.",
            "All these programs which have caused in it for some delay on the network.",
            "I can take.",
            "So let's see what programs have cars in it, some sex and suspicion Top Gear.",
            "That's no no prize.",
            "Hot like us, ridiculous.",
            "But also some dramas like Casualty.",
            "Top Gear one show the news.",
            "OK, so you see all these programs with cause.",
            "They will so we can see the generalization we got as well, so this is good."
        ],
        [
            "Right now.",
            "That's that's."
        ],
        [
            "It was behind it.",
            "Now let's go to how do we?",
            "Why do we choose Vlad and how do we map it onto the real time system?",
            "So look then at the dimension of the codes is the dimension of the feature vectors, and we're choosing Vlad because it's got not too bad performance compared to Fisher vectors.",
            "Some not the best, but not so bad, but it's also smaller because Fisher vectors you have to store the diagonal covariance as well as the mean or the residuals, which twice as large also Fisher vectors.",
            "It's based on Gaussian mixture models, so the actual.",
            "Building the descriptor takes longer, so This is why we're using Vlad.",
            "But now I just go through.",
            "I want to go through sort of the size implications of this.",
            "So Vlad here you can see is 28,000 dimensional vector and it's dense.",
            "OK, so it's large."
        ],
        [
            "So we have just going to work through the size here we got 3 million keyframes.",
            "OK, now we have a 328 K dimensional vector, so 3 million keyframes.",
            "Here's the mass.",
            "Each 4 bytes for each float.",
            "So this is the size which is like 4 terabytes.",
            "If we store these descriptors.",
            "And we have to get this into memory to be fast.",
            "OK, so there's a problem there.",
            "If you're going to use these labs.",
            "OK, So what do you do?",
            "At the moment we can't do that.",
            "Also, we take a long time to load.",
            "So what we do is we reduced and there are various ways to downsize reduction, and the simplest possible one is to use PCA, and this is what we do.",
            "You can do better than this and you'll hear more about that later today, I think.",
            "But if we reduce using PCA, so this is really low dimension.",
            "328 down to 8 K is in PCA, we get some performance loss, but we get a manageable 96 gigabytes.",
            "Now nowadays it's not so hard to get massive bikes into into memory, so we could do that.",
            "But this is almost like the second take home message of this talk.",
            "There is a very very good vector compression method called product quantization.",
            "And if we use product quantization, we can produce by factor of 16 with virtually no performance loss at all, and that gets us down to 6 gig, which is nothing.",
            "Nothing we need to get this into memory.",
            "OK now product quantization is due to Jogu and Co.",
            "Authors, and the idea there is it's a vector quantization method.",
            "Is this compression method?",
            "What K means you take the whole vector and you then find close by vectors and vector quantizers together.",
            "The idea in product quantization is you break the vector into blocks.",
            "In our case we're breaking into blocks of four components, and you vector quantized those separately, and it gives a much finer representation for the same total storage.",
            "OK, so this is a very clever idea, and everyone should know about it.",
            "I sort of take second part of it is that at runtime you can pre because using this product you can precompute a lot of the scalar products.",
            "So actually when we are going through our 3 million vectors, we can use a look up table to make the scalar products we have to compute very, very quick.",
            "You should know about this, so as you can see, we get down to a manageable amount and schedule products quick."
        ],
        [
            "It's how we do it.",
            "I was going to do one more demo here.",
            "On this."
        ],
        [
            "System now this is one of the program chairs of BBC is Penguin obsessed so this is too low so I thought I would look for Penguins for him.",
            "So the question is, does the BBC ever do programs Penguins?",
            "That's the question.",
            "OK, so now we're getting the training examples of Penguins from Google Image Search and let's find out.",
            "Tonight happy Penguins.",
            "Will there be any programs on Penguins?",
            "Hello.",
            "Some warmed up.",
            "Now it's going to be faster.",
            "There are programs about Penguins that's good.",
            "OK, so."
        ],
        [
            "OK. OK."
        ],
        [
            "So I'm going to 3rd part of the talk which is face search and look for people by their faces.",
            "So this is a particular class.",
            "It's going to be discriminative learning again, and the difference is going to be how we represent the image."
        ],
        [
            "Instead of representing the whole image, now we're going to represent just the face region of the image, so we're targeting the special object class.",
            "We know how to do this.",
            "This is very straightforward.",
            "We detect faces standard pipeline.",
            "For doing this we detect faces using violent Jones or whatever, then detect facial features like corners of the eyes, corner of the mouth.",
            "We use that to normalize the face, give some alignment to the face and we can also use those facial features to build the descriptor.",
            "So that's what's being shown here.",
            "We've got these facial features around the eyes, mouth, nose.",
            "We around those we form patches.",
            "We can customize patches and that gives us our feature vector, which in this case is about four K dimensional size.",
            "OK, so that's what you do for a face in a single image, but we're looking at video database here and we don't need to do is to represent every single image or every single keyframe independently.",
            "What we can do is within a shot we can detect faces and we can group them together in some way.",
            "So you think of it as tracking or clustering so that that's what's being shown here.",
            "This is time going here.",
            "All these faces are the same person are being grouped together.",
            "This is automatic and very robust, doesn't fail, so now the ground reality reduces from the number of keyframes to the number of tracks, the number of clusters of a person.",
            "So now say this person here.",
            "You've got maybe 100 faces clustered together, each one represented by the feature vector 4000 natural feature vector, and you can do some form of aggregation across this track.",
            "You could take the mean.",
            "What we do is we take the single feature vector corresponding to the facial features which have facial feature which has the highest score.",
            "So when we detect the eyes, nose, mouth, the one which has the highest score, we use the feature vector of that frame.",
            "Give that person.",
            "But it means this whole track.",
            "This whole set of detections represented by a single feature vector.",
            "OK."
        ],
        [
            "And then apart from that.",
            "It's a very similar method.",
            "We gonna fire system to what you just saw for categories.",
            "So offline we have the video corpus.",
            "We detect.",
            "These face tracks represent each face tracked by this feature vector.",
            "We also have some negative images.",
            "Can discriminative learning OK, then, for the runtime system online we have a text stream to find a person.",
            "The different series.",
            "We don't just download general images, we download images which Google thinks his face is in them.",
            "So this specialization their compute their descriptors runtime train a linear classifier and then rank all the face tracks by by this classifier."
        ],
        [
            "OK, let's do a demo."
        ],
        [
            "Sick.",
            "I'm doing a role theme here, so I'm going to look for Queen Elizabeth.",
            "That's my plan.",
            "This is our Queen.",
            "OK, this is what she looks like.",
            "OK, so it's downloading the image.",
            "It's training a linear classifier.",
            "Ranking with images and they were so turns out the Queen appears on power notes.",
            "She appears in TV programs.",
            "She wears different site, different hats or else.",
            "OK, so this is good that works.",
            "Let's go back now.",
            "So you've got the system.",
            "Now we can find particular people.",
            "That's good.",
            "Now we can play with this idea of it."
        ],
        [
            "So.",
            "Say something in coding for we do that.",
            "So here because we're using these face tracks were not talking about Keyframes anymore.",
            "And what matters is how many face tracks we get.",
            "It turns out first of all, the only .68 million of the shots have faces in them, so it's not all about people.",
            "And after we've found all the face tracks, we get .8 million because it could be several people in the same shot.",
            "The science of descriptors is about 4000 times the number of tracks it's this will fit into memory, but we can use this product quantization again to reduce by factor of 16.",
            "So we get down to a tiny .8 gig.",
            "OK, so for 3000 hours of video, .8 giga's expenses are very scalable representation.",
            "OK."
        ],
        [
            "Now back to playing with the idea.",
            "So few years ago there was work came out of Columbia.",
            "Very nice work.",
            "By Kuma Belama Nya called face Tracer and the idea here was to be able to search for people based on facial attributes so their gender, their race, whether they're smiling weather there.",
            "I have a massage where they were in glasses.",
            "The idea here is the idea there was that it was a person independent search.",
            "You could train person using person independent information that's way easier.",
            "They got many, many students to annotate.",
            "Hundreds of thousands of images.",
            "Independent way because you can you can look for whether somebody is wearing glasses irrespective of who they are, OK. Now what we can do here is use the same idea but avoid all the student training, so we can."
        ],
        [
            "Training information on the fly from Google Image Search.",
            "That's what I'm going to do now.",
            "I'm going to look for people who have mustaches.",
            "That's the plan.",
            "I'm looking for people.",
            "For now.",
            "We got it.",
            "We got network problem so maybe even the wide internets gone down.",
            "So you want to get this demo.",
            "Think about this.",
            "OK, nice.",
            "It's alive again, your Internet life.",
            "So here are people have massages from Google Maps.",
            "You get some funny ones actually.",
            "Look for stars like only just saw this one post.",
            "And now what you've retrieved here, irrespective of the person here are people have massage in.",
            "So can you can you can do the same thing for babies if you're into babies you can get pictures of babies if you're into people wearing glasses, anything, any facial attribute you can search for using exactly the same representation.",
            "I want to do one more demo and come towards the end actually.",
            "So."
        ],
        [
            "Let's now explore the scalability of these ideas.",
            "So, So what we've been looking at so far is a data set based on five months of BBC broadcasts, which is about 3000 hours of video.",
            "Now let's up the amount of data for years, and that corresponds to about 40,000 hours of video, and in terms of key frames, that corresponds about 34 million keyframes.",
            "Now, I've been expressing this idea of making scalable representations that fit into main memory, so now we look at going between these two datasets.",
            "So we start."
        ],
        [
            "If I've got something precomputed, so we look, let's see.",
            "Look for Obama.",
            "Actually, we're going to run live, so this is President Obama being downloaded.",
            "Research, so we're going to look for him in a data set of.",
            "3000 hours and of course, because he appears in the news a lot, we expect to find a fair number, OK?",
            "Now there's news programs of Obama in them, so that's pretty good.",
            "And we can maybe go on the second page, just see how many we get to him.",
            "This is on the second page now.",
            "And I'm getting a couple of mistakes, but he's still there a bit.",
            "Now I'm going to go to the 10 times larger data set and look for Obama.",
            "So we don't have.",
            "We don't have to be quick.",
            "We don't have to recompute everything this cache.",
            "Actually, I think that was so fast.",
            "Say on this data set.",
            "We can go page after page.",
            "Looking for Obama purely because of the size of the data set.",
            "We're just finding many more examples with the same classifier.",
            "That's the point here.",
            "We haven't changed the classifier.",
            "It is getting pages and pages of him without any mistakes actually.",
            "I'm still looking for mistake.",
            "Put.",
            "This number OK, we will give up looking for mistakes, so that's the benefit of having large datasets.",
            "Customers do very well even that strong, but the point is in the other talks I talked about how to make the technology work better in this one, I'm going to leave you with."
        ],
        [
            "A paper that Karen Stimulans presenting this afternoon, which is how to make these face descriptors work better.",
            "So there is improvement in this area as well and you'll hear about that later later today, actually.",
            "OK."
        ],
        [
            "Come to the end of the talk now.",
            "So I started off by saying he was.",
            "The vision is what we wanted to do and.",
            "You've seen that we can search for people who wish to see that patient attributes as well.",
            "We can search specific objects, some Pacific objects, and we can search for certain classes.",
            "We've seen that, so we're sort of up to here on what we want to do, sculptures we haven't.",
            "We haven't seen.",
            "Maybe we can do some of them.",
            "This bottom row.",
            "We have seen examples of that, and that's because in terms in computer vision were not up to the same standard there.",
            "Yet this is looking for actions like phoning, playing an instrument.",
            "Coses activities events birthday parties.",
            "We can't do that yet, but I think over the next 4 five years will get there and this is really a challenge to you.",
            "This is this is where we should be.",
            "In five years we should be able to search for 10 years.",
            "We should be able to search for all of these things as effortlessly.",
            "You just saw today Becausw with our work together we can make these things.",
            "Before as well, so I'm stopping there."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What I'm going to talk about is visual search and.",
                    "label": 0
                },
                {
                    "sent": "The goal here is to make everything searchable, so any image, any video we should be able to search for anything that's our go and buy anything.",
                    "label": 0
                },
                {
                    "sent": "We mean objects, categories like airplanes, cars will touch it for people we should be able to search for people in particular poses.",
                    "label": 0
                },
                {
                    "sent": "We should be able to search for people doing actions, events we should be able to search for anything.",
                    "label": 0
                },
                {
                    "sent": "That's all go and not only will be able to search, we should be able to do it.",
                    "label": 0
                },
                {
                    "sent": "With high precision, high recall, much like Google can search the web, getting results immediately.",
                    "label": 1
                },
                {
                    "sent": "That's what we'd like to be able to do for any visual material.",
                    "label": 0
                },
                {
                    "sent": "For anything, OK, that's all go now.",
                    "label": 0
                },
                {
                    "sent": "Life would be easy if all our images were annotated with text that would then be very straightforward, but unfortunate.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "There's a problem.",
                    "label": 0
                },
                {
                    "sent": "There are many datasets, maybe your own personal image collections which have no annotation apart from the Dayton GPS.",
                    "label": 1
                },
                {
                    "sent": "This is sad, but this is true.",
                    "label": 0
                },
                {
                    "sent": "So what are we going to do about that?",
                    "label": 0
                },
                {
                    "sent": "Fortunately.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are other datasets like the web where there's plenty of annotation.",
                    "label": 0
                },
                {
                    "sent": "We know that with Google we can type in some.",
                    "label": 0
                },
                {
                    "sent": "String and we can find pretty much anything we want.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have these data sets which have no annotation.",
                    "label": 0
                },
                {
                    "sent": "We have these datasets which are very rich annotation and the story of this talk.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is how to go from these vain?",
                    "label": 0
                },
                {
                    "sent": "These datasets, which are very rich annotation to get information from these in order to be able to search these datasets with impoverished annotation.",
                    "label": 0
                },
                {
                    "sent": "OK, that's what I'm going to talk about today.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To make this definite.",
                    "label": 0
                },
                {
                    "sent": "Say you wanted to find Boris Johnson.",
                    "label": 1
                },
                {
                    "sent": "Some of you might not know Boris Johnson is this flamboyant Mayor of London.",
                    "label": 0
                },
                {
                    "sent": "OK, now you might not know that.",
                    "label": 0
                },
                {
                    "sent": "And so I say fine.",
                    "label": 0
                },
                {
                    "sent": "Boris Johnson.",
                    "label": 0
                },
                {
                    "sent": "The first thing you might do is go to Google and get images of Boris Johnson so you can see what he looks like and that's all we need to do to get a model.",
                    "label": 0
                },
                {
                    "sent": "Now we can take these images, train a model.",
                    "label": 0
                },
                {
                    "sent": "In this case we're classifier.",
                    "label": 0
                },
                {
                    "sent": "Then we can use this classifier to go to our datasets.",
                    "label": 0
                },
                {
                    "sent": "We have this big data set.",
                    "label": 0
                },
                {
                    "sent": "We want to find if there any images of Boris Johnson that data set.",
                    "label": 0
                },
                {
                    "sent": "We run the classifier, the data set and find images of Boris Johnson.",
                    "label": 0
                },
                {
                    "sent": "The data set.",
                    "label": 0
                },
                {
                    "sent": "OK, so the point here is that you can get training examples from image web search engine's effortlessly and at runtime this is the on the fly.",
                    "label": 0
                },
                {
                    "sent": "You can learn a model and then use that to search.",
                    "label": 0
                },
                {
                    "sent": "OK, that's the idea.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's what I'm concerned.",
                    "label": 0
                },
                {
                    "sent": "Hopefully if demos work is show you demonstrations on this data set, this is a data set that apply to us by the BBC for research purposes, and it consists of about five months of broadcasts from primetime television.",
                    "label": 0
                },
                {
                    "sent": "It's about 3000 hours and that corresponds to about 3 million keyframes.",
                    "label": 0
                },
                {
                    "sent": "And I tried out to show you.",
                    "label": 0
                },
                {
                    "sent": "Testing 1 two.",
                    "label": 0
                },
                {
                    "sent": "So here we are.",
                    "label": 0
                },
                {
                    "sent": "I'm going to say I want to find a painting.",
                    "label": 0
                },
                {
                    "sent": "I want to find say, the Mona Lisa.",
                    "label": 0
                },
                {
                    "sent": "An in this BBC data set.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what's happening now is I'm going to Google Image search, getting examples of Mona Lisa, who knows what looks like using this to search this 3 million keyframes, and that's the result.",
                    "label": 1
                },
                {
                    "sent": "So what you're seeing here are keyframes from this BBC data set and below each keyframe you see the name of the program that the painting appears in, so you're seeing here paintings.",
                    "label": 0
                },
                {
                    "sent": "There's a program here on Leonardo that's not so surprising.",
                    "label": 0
                },
                {
                    "sent": "And see what else is you seeing.",
                    "label": 0
                },
                {
                    "sent": "Variations in color variations in viewpoint that are being retrieved.",
                    "label": 0
                },
                {
                    "sent": "So that's that's the idea.",
                    "label": 0
                },
                {
                    "sent": "Something you're going to see.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to divide the talk into three parts, each one based on a different type of objects that we want to retrieve.",
                    "label": 0
                },
                {
                    "sent": "The first part is going to be retrieving specific objects.",
                    "label": 0
                },
                {
                    "sent": "Like paintings that showed you but also could be logos, could be buildings and.",
                    "label": 0
                },
                {
                    "sent": "That's that's what's called instant search.",
                    "label": 0
                },
                {
                    "sent": "Then second part will generalize this instead of just looking for specific objects, will look for classes of objects like airplanes or cars courses, so there's some generalization there.",
                    "label": 0
                },
                {
                    "sent": "And then in the third part will at Retreiving people searching for people on the fly, so this is a specialized specialized object class, obviously very important, and in each case what I'm going to do is.",
                    "label": 0
                },
                {
                    "sent": "Hang on to this description of the state of the art.",
                    "label": 0
                },
                {
                    "sent": "What's changed recently to make this.",
                    "label": 0
                },
                {
                    "sent": "These areas perform well what visual technology is used.",
                    "label": 0
                },
                {
                    "sent": "Just to give some background, this idea of using web search engine's to get training.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Samples have been around for a long time, pretty much since web search engine started for images, and this is just a sample of the papers which have in the past learned from image search models merch.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so start off with.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Instant search, so let's define the problem.",
                    "label": 0
                },
                {
                    "sent": "So for image search, the idea is we're looking for a specific object, and usually that's defined by a query image.",
                    "label": 0
                },
                {
                    "sent": "So say here we're looking for this logo.",
                    "label": 0
                },
                {
                    "sent": "We have a query image of this.",
                    "label": 1
                },
                {
                    "sent": "What we want to do is defined in our data set all the instances of this object, but we want to find them unaffected by scale, unaffected by viewpoint.",
                    "label": 0
                },
                {
                    "sent": "So foreshortening perspective affects unaffected by lighting unaffected by partial occlusion and OK, that's that's our goal.",
                    "label": 1
                },
                {
                    "sent": "Not only that, we want to find these instances instantly, so immediately in the manner of Google.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's double use of instance here and it's.",
                    "label": 0
                },
                {
                    "sent": "Spinner outside has been around for a decade or so now, and it's very standard pipeline for doing it, which I think.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Many of you will be aware.",
                    "label": 0
                },
                {
                    "sent": "Also, I'm just going to sketch how this works.",
                    "label": 0
                },
                {
                    "sent": "So we have this bag of words pipeline.",
                    "label": 1
                },
                {
                    "sent": "So say we want to find this building in our data set.",
                    "label": 0
                },
                {
                    "sent": "The processing training first of all, has to get representation for this image and what's used here is so called affine covariant detectors.",
                    "label": 0
                },
                {
                    "sent": "So these are detectors like affine Harris, MCR from my task, look for regions in the image which are adapted to viewpoint.",
                    "label": 0
                },
                {
                    "sent": "You find these regions, usually... And then each of these... is defined is described by SIFT descriptor.",
                    "label": 0
                },
                {
                    "sent": "So we get to this stage here where we have a couple of thousands of regions, each one defined by SIFT descriptors.",
                    "label": 1
                },
                {
                    "sent": "And now we have a couple of 1000 sift descriptors representing this image.",
                    "label": 0
                },
                {
                    "sent": "OK, now by this stage we've done the invariant, so because of the regions we found and the SIFT descriptors we've got as much invariants as we're going to get to lighting and viewpoint.",
                    "label": 0
                },
                {
                    "sent": "That's built in OK. Now I'm going to assume that you know what shift is, because this shift is going to flow through this work, just like it throws through flows through computer vision.",
                    "label": 0
                },
                {
                    "sent": "So SIFT is 128 dimensional descriptor from David Lowe from 99 OK.",
                    "label": 0
                },
                {
                    "sent": "So and that's going to be used for matching throughout.",
                    "label": 0
                },
                {
                    "sent": "Right, like now comes making this inefficient.",
                    "label": 1
                },
                {
                    "sent": "So in advance we have millions of these descriptors.",
                    "label": 0
                },
                {
                    "sent": "They've been vector quantized using something like K means into what we call visual words.",
                    "label": 0
                },
                {
                    "sent": "That's the kamien centers have say a million K might be a million here.",
                    "label": 0
                },
                {
                    "sent": "Each of these shifts are assigned to their nearest kamien center visual word, and then we have a histogram of how many shifts are assigned to each of the visual words that's representation.",
                    "label": 0
                },
                {
                    "sent": "Dimensional histogram is quite sparse.",
                    "label": 0
                },
                {
                    "sent": "Then there's some weighting which comes from information retrieval TF IDF.",
                    "label": 1
                },
                {
                    "sent": "Now that's a representation of this query image.",
                    "label": 1
                },
                {
                    "sent": "Now before hand for all the images in the data set for the BBC data set, this same description has been built and also what is being built is what called an inverted index, so that for each visual word we know all the images it appears in and that's where we get the speed from.",
                    "label": 1
                },
                {
                    "sent": "So now we have a visual word coming from the query.",
                    "label": 0
                },
                {
                    "sent": "We can quickly find using this inverted index all the images it appears in.",
                    "label": 0
                },
                {
                    "sent": "So for the entire visual words which appear in the query we get.",
                    "label": 0
                },
                {
                    "sent": "A list of images which have something in common and they're ranked initially by this TF.",
                    "label": 0
                },
                {
                    "sent": "IDF ranking now.",
                    "label": 0
                },
                {
                    "sent": "So far so good, we get to here and then we're going to do some re ranking based on spatial consistency between the query and these target images.",
                    "label": 0
                },
                {
                    "sent": "In this short list, or say something about that now just to explain it.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So imagine we start with this image.",
                    "label": 0
                },
                {
                    "sent": "This is our query.",
                    "label": 0
                },
                {
                    "sent": "Want to find this bridge?",
                    "label": 0
                },
                {
                    "sent": "Here we search for our database and.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Get things like this so you're finding the bridge of different sizes and the yellow region here indicates the matching region and how that's got and how we rank this is.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Using this spatial consistency.",
                    "label": 0
                },
                {
                    "sent": "So here is our query.",
                    "label": 0
                },
                {
                    "sent": "Here are the regions within that for each image in our short list.",
                    "label": 1
                },
                {
                    "sent": "This might be 2 or 300 images.",
                    "label": 0
                },
                {
                    "sent": "We compute an affine transformation between this region and and corresponding region Maps what's found in the target image that's done using RANSAC, something or some sort of sampling algorithm, and the number of matches here, which would be like 100 or so here.",
                    "label": 1
                },
                {
                    "sent": "That's going to determine the ranking of this image.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's what goes on.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here we have a.",
                    "label": 0
                },
                {
                    "sent": "We have this short list, a few 100 images for each one.",
                    "label": 0
                },
                {
                    "sent": "We compute this transformation.",
                    "label": 0
                },
                {
                    "sent": "The number of matches.",
                    "label": 0
                },
                {
                    "sent": "A number of inliers.",
                    "label": 0
                },
                {
                    "sent": "And gives the ranking of that image, so that's all done at runtime.",
                    "label": 0
                },
                {
                    "sent": "That's what you saw in the painting example.",
                    "label": 0
                },
                {
                    "sent": "So in the painting example we downloaded the images and then they use those to match and rerank using the spatial consistency.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so now I'm going to that.",
                    "label": 0
                },
                {
                    "sent": "The background that's a sketch of the pipeline.",
                    "label": 0
                },
                {
                    "sent": "Now I'm going to move on to something which has been done in the last few years, which is improved the performance.",
                    "label": 0
                },
                {
                    "sent": "Now we're going to talk about performance.",
                    "label": 0
                },
                {
                    "sent": "We have some way of measuring performance, so here's a standard data set that's used in this area for measuring performance, which is Boxford building data set.",
                    "label": 0
                },
                {
                    "sent": "So this consists of a number of queries.",
                    "label": 0
                },
                {
                    "sent": "So we call these landmarks and for each landmark we have 5 queries, 'cause 11 landmarks, 5 queries that, 55 queries in total, and for a data set of about 5000 images we know where all these buildings occur.",
                    "label": 0
                },
                {
                    "sent": "So we know the ground truth.",
                    "label": 0
                },
                {
                    "sent": "So the idea is we take one of these queries, do a search and then we can see how well we're doing, well, how many of the ground truth instances we retrieve?",
                    "label": 0
                },
                {
                    "sent": "OK, that's how we're going to measure performance.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is a typical results.",
                    "label": 0
                },
                {
                    "sent": "We're getting five years ago, not ours, but people working in this area.",
                    "label": 0
                },
                {
                    "sent": "So here are five queries for the same building, and here are the five corresponding precision recall curves so.",
                    "label": 0
                },
                {
                    "sent": "Just explain precision recall curves.",
                    "label": 1
                },
                {
                    "sent": "Precision is the proportion of the returns we get, which are correct, and recall is the proportion of the true instances in the data set that we get.",
                    "label": 0
                },
                {
                    "sent": "So what proportion of all the instances do we get?",
                    "label": 0
                },
                {
                    "sent": "These are typical curves.",
                    "label": 0
                },
                {
                    "sent": "You get what we'd like to get is this.",
                    "label": 0
                },
                {
                    "sent": "This would be a perfect precision recall curve.",
                    "label": 0
                },
                {
                    "sent": "Would like to get that where we get back all the instances that will be the total recall and we don't make any mistakes, but we get.",
                    "label": 0
                },
                {
                    "sent": "Is this OK so you can see this is typical retrieval systems at low recall.",
                    "label": 0
                },
                {
                    "sent": "We're getting good precision.",
                    "label": 0
                },
                {
                    "sent": "We're not making a mistake, so this is like the first few pages of Google.",
                    "label": 0
                },
                {
                    "sent": "Everything is correct.",
                    "label": 0
                },
                {
                    "sent": "And then higher recall depending on the query the position falls off and this would be recalled one where we retrieve all the instances.",
                    "label": 0
                },
                {
                    "sent": "None of the queries in this case return all the instances, and I say we'd like.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To be here and a lot of research in the last five or six years has been trying to get to hear.",
                    "label": 0
                },
                {
                    "sent": "OK, I want to mention one thing which has happened in the last couple of years which is beneficial everywhere.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's a way of improving Sift.",
                    "label": 0
                },
                {
                    "sent": "OK, so we start with Sift.",
                    "label": 0
                },
                {
                    "sent": "If we can improve safety then everything will improve and this isn't just true in these.",
                    "label": 0
                },
                {
                    "sent": "Instant Search this is true for our computer vision because of ubiquity of sift and you think yourself help sift it's done by David Lowe.",
                    "label": 0
                },
                {
                    "sent": "How could you improve on Sift?",
                    "label": 0
                },
                {
                    "sent": "How could that be possible?",
                    "label": 0
                },
                {
                    "sent": "And here's a thought process.",
                    "label": 0
                },
                {
                    "sent": "We know from other work that if you're matching histograms, it's not a good idea to use Euclidean distance.",
                    "label": 0
                },
                {
                    "sent": "Often you get better performance if you're comparing histograms by using something designed for histograms like the Earth movers, distance Chi Squared, Hellinger Colonel Intersection kernel, lots of histogram comparisons.",
                    "label": 0
                },
                {
                    "sent": "We know that works best with histograms, then using Euclidean distance now and then.",
                    "label": 0
                },
                {
                    "sent": "You think Sift is a histogram because it's a spatial binning of gradients.",
                    "label": 0
                },
                {
                    "sent": "So maybe if we use the histogram better instead of Euclidean distance, which is normally used for sift, we could get better performance that thought process and the answer is you can go bit of mass to go through that now.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's the this is a histogram kernel that we're going to use.",
                    "label": 1
                },
                {
                    "sent": "So Helen Journal and it just what this is your your your histogram to L1 normalized so they add up to one and the kernel between them is simply take found by taking the square root of each element of this rummage bin value and then summing it.",
                    "label": 0
                },
                {
                    "sent": "OK, that's how does your kernel.",
                    "label": 0
                },
                {
                    "sent": "Now we need to go between distances and kernels, and there's a standard relationship for doing that.",
                    "label": 0
                },
                {
                    "sent": "So that's done here.",
                    "label": 0
                },
                {
                    "sent": "So now just imagine for the moment now the X&Y L2 normalized, so they have recreated normal one like Sift.",
                    "label": 0
                },
                {
                    "sent": "Sift is L2 normalized.",
                    "label": 0
                },
                {
                    "sent": "And usually we're sift.",
                    "label": 0
                },
                {
                    "sent": "You would look at the distance between 2 subvectors X&Y by looking very clean distance, maybe squaring it or not, and then using low secondary neighbor tests or something like that.",
                    "label": 0
                },
                {
                    "sent": "If we expand this and straightforward way because these are unit norm, each one of these once we get to hear and then minus two times a scalar product and I've written it out here and that was the Colonel in this linear case.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is a relationship between distance between two vectors in the kernel.",
                    "label": 0
                },
                {
                    "sent": "Now if you compare this formula to this formula.",
                    "label": 0
                },
                {
                    "sent": "You can see that to go from here to here we have two L1 normalize these vectors instead of to normalize them and then take the square root element wise and then that will make this equal to this.",
                    "label": 0
                },
                {
                    "sent": "So now if we do that on both sides of the equation 'cause of course we must do that.",
                    "label": 0
                },
                {
                    "sent": "So we replace X&Y, which were's effectors L2 normalized by first of all L1, normalizing them and then taking the square root element wise.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we get to this.",
                    "label": 0
                },
                {
                    "sent": "And it's like a trick, but what we've done is on the right here.",
                    "label": 0
                },
                {
                    "sent": "Now we have the header kernel and on the left it says that if instead of using SIFT which is L2 normalized U L1 normalize it and take the elementwise square root, then instead of using and what you end up using to compare safety is going to be headed kernel which is a histogram measure.",
                    "label": 0
                },
                {
                    "sent": "It feels to completely true.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's very simple, so here's the summarize of the steps.",
                    "label": 0
                },
                {
                    "sent": "We want to get the internal to compare, Sift, Uiowa, normalize and then take the elementwise square root.",
                    "label": 0
                },
                {
                    "sent": "We call that routes.",
                    "label": 0
                },
                {
                    "sent": "If so, we dignify this with the name and the great thing here, and This is why we chose the header kernel is, it doesn't change the dimension of SIFT.",
                    "label": 0
                },
                {
                    "sent": "OK, so we've done is taking the square root.",
                    "label": 0
                },
                {
                    "sent": "So let's see.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This working so say we plug this into this pipeline.",
                    "label": 0
                },
                {
                    "sent": "What do we have to change?",
                    "label": 0
                },
                {
                    "sent": "We've already computed loss if so.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To do is take my square root so.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We go from using Sift.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using root sift that also is done in K means or any other step.",
                    "label": 0
                },
                {
                    "sent": "Wherever sifted used, we now replace it by root, sift and now we can look at the performance improvement.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we start off with numbers so we have this 5000 dimensional sorry 5000 image dataset, 55 queries.",
                    "label": 0
                },
                {
                    "sent": "We could get a single number by taking the mean of the average position over all those queries.",
                    "label": 0
                },
                {
                    "sent": "OK, that's what measure is going to be.",
                    "label": 0
                },
                {
                    "sent": "We're going to test that on the 5000 data set and also adding 100,000 other images, which actors distractors which don't have those queries in.",
                    "label": 0
                },
                {
                    "sent": "So then that will be 105.",
                    "label": 0
                },
                {
                    "sent": "1000 day set.",
                    "label": 0
                },
                {
                    "sent": "So we have two datasets we're going to test on each case using the average precision averaged over all the mean of that overall 55 queries so you can look at the two lines where the arrows are.",
                    "label": 0
                },
                {
                    "sent": "Here you can see that the changes in performance, so this is going from Sifter using roots if throughout.",
                    "label": 0
                },
                {
                    "sent": "And just by doing that change, so just by taking these square root you get a 5% improvement for the smaller datasets and for the larger data set where the performance is generally lower.",
                    "label": 0
                },
                {
                    "sent": "The 6% improvement simply by making that change.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's the idea or message.",
                    "label": 0
                },
                {
                    "sent": "You can make everything work better if you use root sift.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you're using safety issues.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Shift and things work better.",
                    "label": 0
                },
                {
                    "sent": "And it's depressing recall curves.",
                    "label": 0
                },
                {
                    "sent": "You concentrate on the red curve.",
                    "label": 0
                },
                {
                    "sent": "That's where shift is being replaced by root shift.",
                    "label": 0
                },
                {
                    "sent": "And as you see, they move towards the right, which is the way we want to go to get to our total recall.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's all you have to do.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now why it works?",
                    "label": 0
                },
                {
                    "sent": "The intuition is that with.",
                    "label": 0
                },
                {
                    "sent": "You created distance.",
                    "label": 0
                },
                {
                    "sent": "It tends to be dominated by the large components by the large bins of a histogram that tends to dominate, and what the root does is to change the relative proportion.",
                    "label": 0
                },
                {
                    "sent": "So if you look on the bottom, we start with X.",
                    "label": 0
                },
                {
                    "sent": "If you have two bins, one we've sized .8 and one 5.2.",
                    "label": 0
                },
                {
                    "sent": "We then plug it or pump it through the root and starts off by being four times as big .8 times point to .2 after it goes through the square root.",
                    "label": 0
                },
                {
                    "sent": "Then it's only about twice as big, so we've changed the relative proportions and so that.",
                    "label": 0
                },
                {
                    "sent": "Reduces dominance or that of these large bins.",
                    "label": 0
                },
                {
                    "sent": "That's why it works.",
                    "label": 0
                },
                {
                    "sent": "Is also a kernel.",
                    "label": 0
                },
                {
                    "sent": "This is an explicit feature map.",
                    "label": 0
                },
                {
                    "sent": "That's what's going on here.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is the take home message for this part of the talk, OK?",
                    "label": 0
                },
                {
                    "sent": "All you have to do to make your system work better is this line of Matlab.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have to.",
                    "label": 0
                },
                {
                    "sent": "And you'll find that anywhere you sift it will now start working better.",
                    "label": 0
                },
                {
                    "sent": "So this is true in image classification.",
                    "label": 0
                },
                {
                    "sent": "In some vector quantization anyway you sift it will work better.",
                    "label": 0
                },
                {
                    "sent": "Right, so I'll call this magic bullet.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now I've got time to talk about this, but there are many other pieces of work by other researchers which have been trying to improve the precision recall of these systems and you can look at this on the slides afterwards.",
                    "label": 0
                },
                {
                    "sent": "These other methods.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now back to on the fly.",
                    "label": 0
                },
                {
                    "sent": "So now we know the engine behind it.",
                    "label": 0
                },
                {
                    "sent": "I just leave a message with the on the fly system so offline we have our 3 million images from this PC data set and of course I'm showing it on the BBC data set, but it could be your own images or any datasets very general and offline.",
                    "label": 0
                },
                {
                    "sent": "For each image we compute these often covariant regions sift through SIFT, in this case of course now vector quantized into visual words and store.",
                    "label": 0
                },
                {
                    "sent": "Is 1,000,000 dimensional sparse vector for each of these, we compute an inverted index, which so for each word we know all the images it occurs in, so that's all done offline, then online.",
                    "label": 0
                },
                {
                    "sent": "So we're looking for the Earth.",
                    "label": 0
                },
                {
                    "sent": "We use Google Image search to get images of the Earth, compute the same visual word representation, use the inverted index to find all the images.",
                    "label": 0
                },
                {
                    "sent": "In the data set and get rank lists of these.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This one issue then and how we plug this in, which is how do we combine the results from all these images we downloaded so we got say 20 images which describe our specific object.",
                    "label": 0
                },
                {
                    "sent": "How do we combine the results?",
                    "label": 0
                },
                {
                    "sent": "So here are two choices I present to you.",
                    "label": 0
                },
                {
                    "sent": "One is we take all the.",
                    "label": 0
                },
                {
                    "sent": "Vectors describing these images we downloaded.",
                    "label": 0
                },
                {
                    "sent": "We average them.",
                    "label": 0
                },
                {
                    "sent": "Now why would you do that?",
                    "label": 0
                },
                {
                    "sent": "So it could be that in some.",
                    "label": 0
                },
                {
                    "sent": "Some images of the detectors don't fire properly, so you miss visual words if you average them together then you're going to get a very good descriptor in which will catch some of the ones that we missed.",
                    "label": 0
                },
                {
                    "sent": "In one image.",
                    "label": 0
                },
                {
                    "sent": "You'll catch them.",
                    "label": 0
                },
                {
                    "sent": "You have a super vector 4.",
                    "label": 0
                },
                {
                    "sent": "Retreating from, you have to do one search.",
                    "label": 0
                },
                {
                    "sent": "OK if you have to get the vantages.",
                    "label": 0
                },
                {
                    "sent": "That's number 1 #2 is you query of each one, even though it might be missing some visual words and then you combine the results now.",
                    "label": 0
                },
                {
                    "sent": "You can't really.",
                    "label": 0
                },
                {
                    "sent": "You can't read use a wireless network at the moment, so I can actually ask you a question.",
                    "label": 0
                },
                {
                    "sent": "So let's have a have a think about it.",
                    "label": 0
                },
                {
                    "sent": "Which one do you think of these two is better?",
                    "label": 0
                },
                {
                    "sent": "I'm going to.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to ask her to vote an informed vote.",
                    "label": 0
                },
                {
                    "sent": "Who would like who thinks one is the best?",
                    "label": 0
                },
                {
                    "sent": "Thank you for being an 2.",
                    "label": 0
                },
                {
                    "sent": "OK, so it turns out two is better.",
                    "label": 0
                },
                {
                    "sent": "One for for these sort of obvious this is the reasoning.",
                    "label": 0
                },
                {
                    "sent": "OK, so the reasoning is if we have images like this, is the White House.",
                    "label": 0
                },
                {
                    "sent": "We have two images like this.",
                    "label": 0
                },
                {
                    "sent": "Then taking the average of these two makes a lot of sense because there's different elimination conditions you could imagine that you'll get different descriptors in these two cases.",
                    "label": 0
                },
                {
                    "sent": "So taking average of these two is a good idea.",
                    "label": 0
                },
                {
                    "sent": "But if you have a building you also get different viewpoints.",
                    "label": 0
                },
                {
                    "sent": "So this is front and back of the White House and inside the White House.",
                    "label": 0
                },
                {
                    "sent": "You don't have these together, so in general it seems to be better to do a search for each each one individually, and then the combined the results.",
                    "label": 0
                },
                {
                    "sent": "And that's simple just by the number of matches to the query image.",
                    "label": 0
                },
                {
                    "sent": "OK, so that seems to be the better way to do it.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, I think I have another demo now.",
                    "label": 0
                },
                {
                    "sent": "Demo, see.",
                    "label": 0
                },
                {
                    "sent": "Let's go.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Go back so I would look for a building and I've chosen to look for Buckingham Palace.",
                    "label": 0
                },
                {
                    "sent": "So we're getting the training images Buckingham Palace and we're going to get different viewpoints of it.",
                    "label": 0
                },
                {
                    "sent": "And then for each of these, as you know now we do a query.",
                    "label": 0
                },
                {
                    "sent": "We do a query.",
                    "label": 0
                },
                {
                    "sent": "OK, so that was that was searching for shoes million datasets.",
                    "label": 0
                },
                {
                    "sent": "And we can see many, many programs or bucking Palace.",
                    "label": 0
                },
                {
                    "sent": "So expect a Royal theme here.",
                    "label": 0
                },
                {
                    "sent": "So we get to hear some programs, Diamond Queen Royal Bodyguard News at 10.",
                    "label": 0
                },
                {
                    "sent": "And we can also organize this by programs to actually look at the programs that we are.",
                    "label": 0
                },
                {
                    "sent": "This is a funny choice festival.",
                    "label": 0
                },
                {
                    "sent": "Harris Bike is best to push in the Diamond Queen Diamond Queen, so these are various programs on the Queen, but also general news programs as well.",
                    "label": 0
                },
                {
                    "sent": "OK and of course I said we can play these things.",
                    "label": 0
                },
                {
                    "sent": "OK. At the end of the talk, at this time I'll let you shout out some queries we can do, but let me go through 1st.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now I want to move on to the second part of the talk, which is category search so.",
                    "label": 0
                },
                {
                    "sent": "Now we're going to go from specific objects.",
                    "label": 0
                },
                {
                    "sent": "Two classes of objects.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we need some generalization, so we're looking for airplanes, not particularly subject to class of objects, and the technology here changes slightly because we're going to have generalization.",
                    "label": 0
                },
                {
                    "sent": "We're going to now learn a classifier, and so we need training images positive negative.",
                    "label": 0
                },
                {
                    "sent": "It's been some discriminative learning.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now the technology we're using is shown here.",
                    "label": 0
                },
                {
                    "sent": "We're going to have positive images, which is the ones we're going to get from the web, and we also have some negative images, so this is actually a set of images here.",
                    "label": 0
                },
                {
                    "sent": "There will be some image encoding which will take us from the images to feature vector for each image and then learning which is going to be in our case be a classifier and we're going to use a linear SVM.",
                    "label": 0
                },
                {
                    "sent": "Why do we use a linear SVM?",
                    "label": 0
                },
                {
                    "sent": "We do a linear SVM becausw linear SVM is fast at runtime.",
                    "label": 0
                },
                {
                    "sent": "OK, linear SVM, you have a single vector wallet W and we classify the image have a image represented by X.",
                    "label": 0
                },
                {
                    "sent": "We classify the images by taking the scalar product dot X.",
                    "label": 0
                },
                {
                    "sent": "That's very very fast.",
                    "label": 0
                },
                {
                    "sent": "Just depends on the size of these vectors.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's why we're using a linear SVM, so we learn discriminate W vector.",
                    "label": 0
                },
                {
                    "sent": "Then for testing.",
                    "label": 0
                },
                {
                    "sent": "So this is now for the data set for each image we encode it in the same way.",
                    "label": 0
                },
                {
                    "sent": "Have a feature vector, then we score it WX and that's the thing that is being the distance from the discriminative plane.",
                    "label": 0
                },
                {
                    "sent": "That's a scoring that gives us a score for each image.",
                    "label": 0
                },
                {
                    "sent": "Then we sort based on that and that gives us our ranking.",
                    "label": 0
                },
                {
                    "sent": "And here's the examples for airplanes.",
                    "label": 0
                },
                {
                    "sent": "So the first images will be airplanes.",
                    "label": 0
                },
                {
                    "sent": "OK, that's that's the mechanism that underpins it.",
                    "label": 0
                },
                {
                    "sent": "What I want to do is before we get onto the on the fly aspect, I'm going to talk about.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The encoding part.",
                    "label": 0
                },
                {
                    "sent": "How do we go from the image to the feature vector?",
                    "label": 0
                },
                {
                    "sent": "'cause that's where there's been a lot of development over the last five or six years, and then after that I'll talk about how this Maps onto the on the fly and the issues there are.",
                    "label": 0
                },
                {
                    "sent": "If you're going to make this thing work in real time, you have to be aware of memory footprint, how much memory you are using because you have to get everything into memory and think about size in order to get the right speed and performance.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Now back to image encoding and sift again.",
                    "label": 1
                },
                {
                    "sent": "So here's the standard pipeline that was developed maybe 10 years ago now.",
                    "label": 0
                },
                {
                    "sent": "Certainly helped 2006 of how to classify an image.",
                    "label": 0
                },
                {
                    "sent": "In other words, how to decide how to decide an image has a particular object inside it.",
                    "label": 0
                },
                {
                    "sent": "That's what we're looking at here.",
                    "label": 0
                },
                {
                    "sent": "So we start with the image and the descriptor we get.",
                    "label": 0
                },
                {
                    "sent": "We start with is again using Sift.",
                    "label": 0
                },
                {
                    "sent": "So now instead of using these affine covariant region detectors, what was done is what called dense.",
                    "label": 0
                },
                {
                    "sent": "If so, you cover the image with these sift footprints.",
                    "label": 0
                },
                {
                    "sent": "And you do that multiple scales.",
                    "label": 0
                },
                {
                    "sent": "That's now the representation you're starting with the image.",
                    "label": 0
                },
                {
                    "sent": "OK, so again, again again, Sift might be 10s of thousands of these would be measured with 128 dimensional vectors.",
                    "label": 0
                },
                {
                    "sent": "His shifts there, then vector quantized in the same way.",
                    "label": 0
                },
                {
                    "sent": "So we use a bag of visual words representation.",
                    "label": 0
                },
                {
                    "sent": "Now these are the shifts assigned to their various code words assigned to the visual words.",
                    "label": 0
                },
                {
                    "sent": "That's what the color coding means here.",
                    "label": 0
                },
                {
                    "sent": "Then you might bring some spatial information in, so you have this idea from Kristen Grauman traffic barrel of having a pyramid that was brought down to spatial by lacnic at all.",
                    "label": 0
                },
                {
                    "sent": "So now you have spatial information by tiles.",
                    "label": 0
                },
                {
                    "sent": "And then in each of these tiles you represent the histogram of visual words.",
                    "label": 0
                },
                {
                    "sent": "Again, that's what's being shown here.",
                    "label": 0
                },
                {
                    "sent": "Now if you're.",
                    "label": 0
                },
                {
                    "sent": "Yo Kai in yo Kai means number of words.",
                    "label": 0
                },
                {
                    "sent": "Your vocabulary, say 1000 dimensional.",
                    "label": 0
                },
                {
                    "sent": "Then the size of this vector is 1000 times a number of spatial tiles you're using.",
                    "label": 0
                },
                {
                    "sent": "So if you get, this can start getting large.",
                    "label": 1
                },
                {
                    "sent": "OK, you do this for all your training images.",
                    "label": 1
                },
                {
                    "sent": "That gives you positive training examples you have negative training examples which don't complain.",
                    "label": 0
                },
                {
                    "sent": "Dogs in this case you learn the linear SVM and that's then your classifier, which we're going to use ranking OK, so that's the pipeline.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Has been a lot of improvement in this area over the last since 2000.",
                    "label": 0
                },
                {
                    "sent": "Six 2007.",
                    "label": 0
                },
                {
                    "sent": "I'm I'm I want to briefly mention some of the improvements.",
                    "label": 0
                },
                {
                    "sent": "Typically I want talk about Vlad and Fisher Vector to the improvements that happened.",
                    "label": 0
                },
                {
                    "sent": "OK, again you can look at this list on the PDF afterwards.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the intuition here thing you have to keep in mind as you got these say 20,000 shifts you're measuring from image and what you'd like to represent.",
                    "label": 0
                },
                {
                    "sent": "The distribution of these shifts in safe space.",
                    "label": 0
                },
                {
                    "sent": "You have 128 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "You have all these vectors which have come from the image and you want to represent distribution and what the bag of words does is represented in a very coarse way.",
                    "label": 0
                },
                {
                    "sent": "So here are the voran ourselves.",
                    "label": 0
                },
                {
                    "sent": "This comes from the K means, so these are cells in the safe space.",
                    "label": 0
                },
                {
                    "sent": "And these are the centers that they mean have been found.",
                    "label": 0
                },
                {
                    "sent": "These these visual words W and we start with the shifts measured in the image there map to view in ourselves and then the representation is to count how many shifts are assigned to this cell.",
                    "label": 0
                },
                {
                    "sent": "OK, so the representation of the sift, this distribution of shifts is a histogram of how many shifts are mapped to each of these visual words, OK?",
                    "label": 0
                },
                {
                    "sent": "So this is very it's quite coarse representation of that distribution and what's happened over the last few years is try make this better.",
                    "label": 0
                },
                {
                    "sent": "How can we?",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This better and one thing is rather than just storing the count, you can store something about the distribution within a cell and one natural.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To do is maybe store the mean of vectors, so what's being shown here is here.",
                    "label": 0
                },
                {
                    "sent": "All the vectors assigned to this cell and also with across were showing the mean of those, which won't necessarily correspond to the visual word the came in center OK. That's when he's starting to represent a bit more about the distribution, and we can go even further and represent the mean and the covariance, or at least the variance of the descriptors.",
                    "label": 0
                },
                {
                    "sent": "OK, so this one where we represent the mean is what Vlad Dazzle supervector stars.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We represent the mean and the covariance or diagonal covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "In fact, is the Fisher vector OK, so that's what it's trying to do.",
                    "label": 0
                },
                {
                    "sent": "It's trying to represent more about the distribution.",
                    "label": 0
                },
                {
                    "sent": "And go into the flat one in.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "More detail.",
                    "label": 0
                },
                {
                    "sent": "So we have this is there on my cell.",
                    "label": 0
                },
                {
                    "sent": "Here are the shifts assigned that role in myself, and what Vlad actually sources is the sum of the residuals.",
                    "label": 0
                },
                {
                    "sent": "So it stores the difference between the center here that came in Victor and the system should be assigned.",
                    "label": 0
                },
                {
                    "sent": "It sums up these and that's what's stored.",
                    "label": 0
                },
                {
                    "sent": "Now to go into that, imagine your K the number of ways 1000.",
                    "label": 1
                },
                {
                    "sent": "If you're going to store these visuals.",
                    "label": 0
                },
                {
                    "sent": "Each one of these, if you're using shift, is 128 dimensional.",
                    "label": 0
                },
                {
                    "sent": "So your descriptor, which previously would say 1000 times the just account for each of those, now becomes 1000 times the size of 50,000 * 28, so it becomes bigger.",
                    "label": 0
                },
                {
                    "sent": "You also have to multiply that by if you're doing Spata spatial encoding by the number of tiles you got in your spatial encoding, so that it becomes bigger still.",
                    "label": 0
                },
                {
                    "sent": "So now you're getting these huge.",
                    "label": 0
                },
                {
                    "sent": "Vectors, So what happens in Vlad and Fisher vectors?",
                    "label": 0
                },
                {
                    "sent": "You reduce the size of recovery to start with.",
                    "label": 0
                },
                {
                    "sent": "Down to 256 you also reduce the size of shift by using PCA before it goes in, so you get these example manageable sizes, but they can still be large.",
                    "label": 0
                },
                {
                    "sent": "OK now this idea of reps anymore about the distribution does.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Pay off then So what you're seeing here.",
                    "label": 0
                },
                {
                    "sent": "Is how the performance has changed as we go at these different encodings, so we're going from the left.",
                    "label": 0
                },
                {
                    "sent": "This is someone logical says 2007 up to say 2013 and I'll go through.",
                    "label": 0
                },
                {
                    "sent": "What changed in the second?",
                    "label": 0
                },
                {
                    "sent": "We're measuring this on the Pascal 2007 test set, so it's this is if you're not familiar with its twenty object classes, cars, airplanes, cats, dogs.",
                    "label": 0
                },
                {
                    "sent": "Say horses, cats etc.",
                    "label": 0
                },
                {
                    "sent": "About 5000 images that are being assessed OK, but it's not so important.",
                    "label": 0
                },
                {
                    "sent": "We're testing on more the trend we're seeing.",
                    "label": 0
                },
                {
                    "sent": "So let's go through it off with original bag of word descriptor with 4000 vocabulary you get some improvement if you increase the vocabulary size.",
                    "label": 0
                },
                {
                    "sent": "So increase the K from 4 to 25 K. OK, so in fact that some improvement next improvement comes by not doing hard assignment, but by doing some form of soft assignment.",
                    "label": 0
                },
                {
                    "sent": "This was the LLC assignment, so that gives an improvement and then the big improvement comes by sort by representing not just the count but now the residuals are mean of vectors, shifts assigned to the cells.",
                    "label": 1
                },
                {
                    "sent": "We see the boost you get there, you get a further boost by representing the diagonal covariance as well.",
                    "label": 0
                },
                {
                    "sent": "It's official vector and with various things we've learned in the last few years about Normalization, Square rooting, etc we can get.",
                    "label": 0
                },
                {
                    "sent": "A larger boost still, so we're up here.",
                    "label": 0
                },
                {
                    "sent": "So you can see that in these six years the performance is increased by 10%.",
                    "label": 0
                },
                {
                    "sent": "All starting from the same descriptor, so we start with dense sift.",
                    "label": 0
                },
                {
                    "sent": "In all cases this is.",
                    "label": 0
                },
                {
                    "sent": "We start with dense.",
                    "label": 0
                },
                {
                    "sent": "If computed across the image, just changing the encoding method is given this 10% improvement.",
                    "label": 0
                },
                {
                    "sent": "So no extra cost over the original.",
                    "label": 0
                },
                {
                    "sent": "An measurements in the image so it's impressive.",
                    "label": 0
                },
                {
                    "sent": "Element over the field.",
                    "label": 0
                },
                {
                    "sent": "I think now also need to pick.",
                    "label": 0
                },
                {
                    "sent": "Start paying attention to the size of these scriptures to remember the size.",
                    "label": 0
                },
                {
                    "sent": "Depends on the vocabulary size, how much of the safety represent and also the number of spatial tiles.",
                    "label": 1
                },
                {
                    "sent": "I'll come back the second what we're going to use for the on the fly system is Vlad.",
                    "label": 0
                },
                {
                    "sent": "Explain why after I show you.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me again go through the anatomy of how these, how this is mapped onto the online system so.",
                    "label": 0
                },
                {
                    "sent": "Offline we have our data set assuming images and we compute the.",
                    "label": 0
                },
                {
                    "sent": "In our case of lab descriptors of all of these are computed.",
                    "label": 0
                },
                {
                    "sent": "In advance and they need to be stored in memory so that they can be accessed quickly.",
                    "label": 0
                },
                {
                    "sent": "Now at runtime we're going to get training examples using Google Image Search, so we're looking for motorbike.",
                    "label": 0
                },
                {
                    "sent": "These would be the positive examples were doing discriminative learning now.",
                    "label": 0
                },
                {
                    "sent": "So unlike the search I showed you before, we need negative images and we have a Bank of negative images that we precompute.",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter what they are, so much is just a general set of images which can be hopefully different from the ones we download.",
                    "label": 0
                },
                {
                    "sent": "So again we've precomputed those descriptors.",
                    "label": 0
                },
                {
                    "sent": "So at runtime we get these positives, encode them.",
                    "label": 0
                },
                {
                    "sent": "Using Glad learner linear SVM.",
                    "label": 0
                },
                {
                    "sent": "And then rank all the images in the data set using it OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do an example.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now.",
                    "label": 0
                },
                {
                    "sent": "Search categories so the most boring category in the world I look for for cars.",
                    "label": 0
                },
                {
                    "sent": "So now we're getting training examples of cars research, doing research.",
                    "label": 0
                },
                {
                    "sent": "OK, some interesting cars mode ones.",
                    "label": 0
                },
                {
                    "sent": "So viewpoints of cars.",
                    "label": 0
                },
                {
                    "sent": "And now it's encoding all these, encoding them using black descriptors.",
                    "label": 0
                },
                {
                    "sent": "And it's trained the SVM now its ranking.",
                    "label": 0
                },
                {
                    "sent": "And this retreat.",
                    "label": 0
                },
                {
                    "sent": "All these programs which have caused in it for some delay on the network.",
                    "label": 0
                },
                {
                    "sent": "I can take.",
                    "label": 0
                },
                {
                    "sent": "So let's see what programs have cars in it, some sex and suspicion Top Gear.",
                    "label": 0
                },
                {
                    "sent": "That's no no prize.",
                    "label": 0
                },
                {
                    "sent": "Hot like us, ridiculous.",
                    "label": 0
                },
                {
                    "sent": "But also some dramas like Casualty.",
                    "label": 0
                },
                {
                    "sent": "Top Gear one show the news.",
                    "label": 0
                },
                {
                    "sent": "OK, so you see all these programs with cause.",
                    "label": 0
                },
                {
                    "sent": "They will so we can see the generalization we got as well, so this is good.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right now.",
                    "label": 0
                },
                {
                    "sent": "That's that's.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It was behind it.",
                    "label": 0
                },
                {
                    "sent": "Now let's go to how do we?",
                    "label": 0
                },
                {
                    "sent": "Why do we choose Vlad and how do we map it onto the real time system?",
                    "label": 0
                },
                {
                    "sent": "So look then at the dimension of the codes is the dimension of the feature vectors, and we're choosing Vlad because it's got not too bad performance compared to Fisher vectors.",
                    "label": 0
                },
                {
                    "sent": "Some not the best, but not so bad, but it's also smaller because Fisher vectors you have to store the diagonal covariance as well as the mean or the residuals, which twice as large also Fisher vectors.",
                    "label": 0
                },
                {
                    "sent": "It's based on Gaussian mixture models, so the actual.",
                    "label": 0
                },
                {
                    "sent": "Building the descriptor takes longer, so This is why we're using Vlad.",
                    "label": 0
                },
                {
                    "sent": "But now I just go through.",
                    "label": 0
                },
                {
                    "sent": "I want to go through sort of the size implications of this.",
                    "label": 0
                },
                {
                    "sent": "So Vlad here you can see is 28,000 dimensional vector and it's dense.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's large.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have just going to work through the size here we got 3 million keyframes.",
                    "label": 0
                },
                {
                    "sent": "OK, now we have a 328 K dimensional vector, so 3 million keyframes.",
                    "label": 0
                },
                {
                    "sent": "Here's the mass.",
                    "label": 0
                },
                {
                    "sent": "Each 4 bytes for each float.",
                    "label": 0
                },
                {
                    "sent": "So this is the size which is like 4 terabytes.",
                    "label": 0
                },
                {
                    "sent": "If we store these descriptors.",
                    "label": 0
                },
                {
                    "sent": "And we have to get this into memory to be fast.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's a problem there.",
                    "label": 0
                },
                {
                    "sent": "If you're going to use these labs.",
                    "label": 0
                },
                {
                    "sent": "OK, So what do you do?",
                    "label": 0
                },
                {
                    "sent": "At the moment we can't do that.",
                    "label": 0
                },
                {
                    "sent": "Also, we take a long time to load.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we reduced and there are various ways to downsize reduction, and the simplest possible one is to use PCA, and this is what we do.",
                    "label": 0
                },
                {
                    "sent": "You can do better than this and you'll hear more about that later today, I think.",
                    "label": 0
                },
                {
                    "sent": "But if we reduce using PCA, so this is really low dimension.",
                    "label": 0
                },
                {
                    "sent": "328 down to 8 K is in PCA, we get some performance loss, but we get a manageable 96 gigabytes.",
                    "label": 0
                },
                {
                    "sent": "Now nowadays it's not so hard to get massive bikes into into memory, so we could do that.",
                    "label": 0
                },
                {
                    "sent": "But this is almost like the second take home message of this talk.",
                    "label": 0
                },
                {
                    "sent": "There is a very very good vector compression method called product quantization.",
                    "label": 0
                },
                {
                    "sent": "And if we use product quantization, we can produce by factor of 16 with virtually no performance loss at all, and that gets us down to 6 gig, which is nothing.",
                    "label": 0
                },
                {
                    "sent": "Nothing we need to get this into memory.",
                    "label": 0
                },
                {
                    "sent": "OK now product quantization is due to Jogu and Co.",
                    "label": 0
                },
                {
                    "sent": "Authors, and the idea there is it's a vector quantization method.",
                    "label": 0
                },
                {
                    "sent": "Is this compression method?",
                    "label": 0
                },
                {
                    "sent": "What K means you take the whole vector and you then find close by vectors and vector quantizers together.",
                    "label": 0
                },
                {
                    "sent": "The idea in product quantization is you break the vector into blocks.",
                    "label": 0
                },
                {
                    "sent": "In our case we're breaking into blocks of four components, and you vector quantized those separately, and it gives a much finer representation for the same total storage.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a very clever idea, and everyone should know about it.",
                    "label": 0
                },
                {
                    "sent": "I sort of take second part of it is that at runtime you can pre because using this product you can precompute a lot of the scalar products.",
                    "label": 0
                },
                {
                    "sent": "So actually when we are going through our 3 million vectors, we can use a look up table to make the scalar products we have to compute very, very quick.",
                    "label": 0
                },
                {
                    "sent": "You should know about this, so as you can see, we get down to a manageable amount and schedule products quick.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's how we do it.",
                    "label": 0
                },
                {
                    "sent": "I was going to do one more demo here.",
                    "label": 0
                },
                {
                    "sent": "On this.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "System now this is one of the program chairs of BBC is Penguin obsessed so this is too low so I thought I would look for Penguins for him.",
                    "label": 0
                },
                {
                    "sent": "So the question is, does the BBC ever do programs Penguins?",
                    "label": 0
                },
                {
                    "sent": "That's the question.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we're getting the training examples of Penguins from Google Image Search and let's find out.",
                    "label": 0
                },
                {
                    "sent": "Tonight happy Penguins.",
                    "label": 0
                },
                {
                    "sent": "Will there be any programs on Penguins?",
                    "label": 0
                },
                {
                    "sent": "Hello.",
                    "label": 0
                },
                {
                    "sent": "Some warmed up.",
                    "label": 0
                },
                {
                    "sent": "Now it's going to be faster.",
                    "label": 0
                },
                {
                    "sent": "There are programs about Penguins that's good.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. OK.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to 3rd part of the talk which is face search and look for people by their faces.",
                    "label": 0
                },
                {
                    "sent": "So this is a particular class.",
                    "label": 0
                },
                {
                    "sent": "It's going to be discriminative learning again, and the difference is going to be how we represent the image.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Instead of representing the whole image, now we're going to represent just the face region of the image, so we're targeting the special object class.",
                    "label": 0
                },
                {
                    "sent": "We know how to do this.",
                    "label": 0
                },
                {
                    "sent": "This is very straightforward.",
                    "label": 0
                },
                {
                    "sent": "We detect faces standard pipeline.",
                    "label": 0
                },
                {
                    "sent": "For doing this we detect faces using violent Jones or whatever, then detect facial features like corners of the eyes, corner of the mouth.",
                    "label": 0
                },
                {
                    "sent": "We use that to normalize the face, give some alignment to the face and we can also use those facial features to build the descriptor.",
                    "label": 0
                },
                {
                    "sent": "So that's what's being shown here.",
                    "label": 0
                },
                {
                    "sent": "We've got these facial features around the eyes, mouth, nose.",
                    "label": 0
                },
                {
                    "sent": "We around those we form patches.",
                    "label": 0
                },
                {
                    "sent": "We can customize patches and that gives us our feature vector, which in this case is about four K dimensional size.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's what you do for a face in a single image, but we're looking at video database here and we don't need to do is to represent every single image or every single keyframe independently.",
                    "label": 0
                },
                {
                    "sent": "What we can do is within a shot we can detect faces and we can group them together in some way.",
                    "label": 0
                },
                {
                    "sent": "So you think of it as tracking or clustering so that that's what's being shown here.",
                    "label": 0
                },
                {
                    "sent": "This is time going here.",
                    "label": 0
                },
                {
                    "sent": "All these faces are the same person are being grouped together.",
                    "label": 0
                },
                {
                    "sent": "This is automatic and very robust, doesn't fail, so now the ground reality reduces from the number of keyframes to the number of tracks, the number of clusters of a person.",
                    "label": 0
                },
                {
                    "sent": "So now say this person here.",
                    "label": 0
                },
                {
                    "sent": "You've got maybe 100 faces clustered together, each one represented by the feature vector 4000 natural feature vector, and you can do some form of aggregation across this track.",
                    "label": 0
                },
                {
                    "sent": "You could take the mean.",
                    "label": 0
                },
                {
                    "sent": "What we do is we take the single feature vector corresponding to the facial features which have facial feature which has the highest score.",
                    "label": 0
                },
                {
                    "sent": "So when we detect the eyes, nose, mouth, the one which has the highest score, we use the feature vector of that frame.",
                    "label": 0
                },
                {
                    "sent": "Give that person.",
                    "label": 0
                },
                {
                    "sent": "But it means this whole track.",
                    "label": 0
                },
                {
                    "sent": "This whole set of detections represented by a single feature vector.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then apart from that.",
                    "label": 0
                },
                {
                    "sent": "It's a very similar method.",
                    "label": 0
                },
                {
                    "sent": "We gonna fire system to what you just saw for categories.",
                    "label": 0
                },
                {
                    "sent": "So offline we have the video corpus.",
                    "label": 0
                },
                {
                    "sent": "We detect.",
                    "label": 0
                },
                {
                    "sent": "These face tracks represent each face tracked by this feature vector.",
                    "label": 0
                },
                {
                    "sent": "We also have some negative images.",
                    "label": 0
                },
                {
                    "sent": "Can discriminative learning OK, then, for the runtime system online we have a text stream to find a person.",
                    "label": 0
                },
                {
                    "sent": "The different series.",
                    "label": 0
                },
                {
                    "sent": "We don't just download general images, we download images which Google thinks his face is in them.",
                    "label": 0
                },
                {
                    "sent": "So this specialization their compute their descriptors runtime train a linear classifier and then rank all the face tracks by by this classifier.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, let's do a demo.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sick.",
                    "label": 0
                },
                {
                    "sent": "I'm doing a role theme here, so I'm going to look for Queen Elizabeth.",
                    "label": 0
                },
                {
                    "sent": "That's my plan.",
                    "label": 0
                },
                {
                    "sent": "This is our Queen.",
                    "label": 0
                },
                {
                    "sent": "OK, this is what she looks like.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's downloading the image.",
                    "label": 0
                },
                {
                    "sent": "It's training a linear classifier.",
                    "label": 1
                },
                {
                    "sent": "Ranking with images and they were so turns out the Queen appears on power notes.",
                    "label": 0
                },
                {
                    "sent": "She appears in TV programs.",
                    "label": 0
                },
                {
                    "sent": "She wears different site, different hats or else.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is good that works.",
                    "label": 0
                },
                {
                    "sent": "Let's go back now.",
                    "label": 0
                },
                {
                    "sent": "So you've got the system.",
                    "label": 0
                },
                {
                    "sent": "Now we can find particular people.",
                    "label": 0
                },
                {
                    "sent": "That's good.",
                    "label": 0
                },
                {
                    "sent": "Now we can play with this idea of it.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Say something in coding for we do that.",
                    "label": 0
                },
                {
                    "sent": "So here because we're using these face tracks were not talking about Keyframes anymore.",
                    "label": 0
                },
                {
                    "sent": "And what matters is how many face tracks we get.",
                    "label": 0
                },
                {
                    "sent": "It turns out first of all, the only .68 million of the shots have faces in them, so it's not all about people.",
                    "label": 0
                },
                {
                    "sent": "And after we've found all the face tracks, we get .8 million because it could be several people in the same shot.",
                    "label": 0
                },
                {
                    "sent": "The science of descriptors is about 4000 times the number of tracks it's this will fit into memory, but we can use this product quantization again to reduce by factor of 16.",
                    "label": 0
                },
                {
                    "sent": "So we get down to a tiny .8 gig.",
                    "label": 0
                },
                {
                    "sent": "OK, so for 3000 hours of video, .8 giga's expenses are very scalable representation.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now back to playing with the idea.",
                    "label": 0
                },
                {
                    "sent": "So few years ago there was work came out of Columbia.",
                    "label": 0
                },
                {
                    "sent": "Very nice work.",
                    "label": 0
                },
                {
                    "sent": "By Kuma Belama Nya called face Tracer and the idea here was to be able to search for people based on facial attributes so their gender, their race, whether they're smiling weather there.",
                    "label": 0
                },
                {
                    "sent": "I have a massage where they were in glasses.",
                    "label": 0
                },
                {
                    "sent": "The idea here is the idea there was that it was a person independent search.",
                    "label": 0
                },
                {
                    "sent": "You could train person using person independent information that's way easier.",
                    "label": 0
                },
                {
                    "sent": "They got many, many students to annotate.",
                    "label": 0
                },
                {
                    "sent": "Hundreds of thousands of images.",
                    "label": 0
                },
                {
                    "sent": "Independent way because you can you can look for whether somebody is wearing glasses irrespective of who they are, OK. Now what we can do here is use the same idea but avoid all the student training, so we can.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Training information on the fly from Google Image Search.",
                    "label": 0
                },
                {
                    "sent": "That's what I'm going to do now.",
                    "label": 0
                },
                {
                    "sent": "I'm going to look for people who have mustaches.",
                    "label": 0
                },
                {
                    "sent": "That's the plan.",
                    "label": 0
                },
                {
                    "sent": "I'm looking for people.",
                    "label": 0
                },
                {
                    "sent": "For now.",
                    "label": 0
                },
                {
                    "sent": "We got it.",
                    "label": 0
                },
                {
                    "sent": "We got network problem so maybe even the wide internets gone down.",
                    "label": 0
                },
                {
                    "sent": "So you want to get this demo.",
                    "label": 0
                },
                {
                    "sent": "Think about this.",
                    "label": 0
                },
                {
                    "sent": "OK, nice.",
                    "label": 0
                },
                {
                    "sent": "It's alive again, your Internet life.",
                    "label": 0
                },
                {
                    "sent": "So here are people have massages from Google Maps.",
                    "label": 0
                },
                {
                    "sent": "You get some funny ones actually.",
                    "label": 0
                },
                {
                    "sent": "Look for stars like only just saw this one post.",
                    "label": 0
                },
                {
                    "sent": "And now what you've retrieved here, irrespective of the person here are people have massage in.",
                    "label": 0
                },
                {
                    "sent": "So can you can you can do the same thing for babies if you're into babies you can get pictures of babies if you're into people wearing glasses, anything, any facial attribute you can search for using exactly the same representation.",
                    "label": 0
                },
                {
                    "sent": "I want to do one more demo and come towards the end actually.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's now explore the scalability of these ideas.",
                    "label": 0
                },
                {
                    "sent": "So, So what we've been looking at so far is a data set based on five months of BBC broadcasts, which is about 3000 hours of video.",
                    "label": 0
                },
                {
                    "sent": "Now let's up the amount of data for years, and that corresponds to about 40,000 hours of video, and in terms of key frames, that corresponds about 34 million keyframes.",
                    "label": 0
                },
                {
                    "sent": "Now, I've been expressing this idea of making scalable representations that fit into main memory, so now we look at going between these two datasets.",
                    "label": 0
                },
                {
                    "sent": "So we start.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If I've got something precomputed, so we look, let's see.",
                    "label": 0
                },
                {
                    "sent": "Look for Obama.",
                    "label": 0
                },
                {
                    "sent": "Actually, we're going to run live, so this is President Obama being downloaded.",
                    "label": 0
                },
                {
                    "sent": "Research, so we're going to look for him in a data set of.",
                    "label": 0
                },
                {
                    "sent": "3000 hours and of course, because he appears in the news a lot, we expect to find a fair number, OK?",
                    "label": 0
                },
                {
                    "sent": "Now there's news programs of Obama in them, so that's pretty good.",
                    "label": 0
                },
                {
                    "sent": "And we can maybe go on the second page, just see how many we get to him.",
                    "label": 0
                },
                {
                    "sent": "This is on the second page now.",
                    "label": 0
                },
                {
                    "sent": "And I'm getting a couple of mistakes, but he's still there a bit.",
                    "label": 0
                },
                {
                    "sent": "Now I'm going to go to the 10 times larger data set and look for Obama.",
                    "label": 0
                },
                {
                    "sent": "So we don't have.",
                    "label": 0
                },
                {
                    "sent": "We don't have to be quick.",
                    "label": 0
                },
                {
                    "sent": "We don't have to recompute everything this cache.",
                    "label": 0
                },
                {
                    "sent": "Actually, I think that was so fast.",
                    "label": 0
                },
                {
                    "sent": "Say on this data set.",
                    "label": 0
                },
                {
                    "sent": "We can go page after page.",
                    "label": 0
                },
                {
                    "sent": "Looking for Obama purely because of the size of the data set.",
                    "label": 0
                },
                {
                    "sent": "We're just finding many more examples with the same classifier.",
                    "label": 0
                },
                {
                    "sent": "That's the point here.",
                    "label": 0
                },
                {
                    "sent": "We haven't changed the classifier.",
                    "label": 0
                },
                {
                    "sent": "It is getting pages and pages of him without any mistakes actually.",
                    "label": 0
                },
                {
                    "sent": "I'm still looking for mistake.",
                    "label": 0
                },
                {
                    "sent": "Put.",
                    "label": 0
                },
                {
                    "sent": "This number OK, we will give up looking for mistakes, so that's the benefit of having large datasets.",
                    "label": 0
                },
                {
                    "sent": "Customers do very well even that strong, but the point is in the other talks I talked about how to make the technology work better in this one, I'm going to leave you with.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A paper that Karen Stimulans presenting this afternoon, which is how to make these face descriptors work better.",
                    "label": 0
                },
                {
                    "sent": "So there is improvement in this area as well and you'll hear about that later later today, actually.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Come to the end of the talk now.",
                    "label": 0
                },
                {
                    "sent": "So I started off by saying he was.",
                    "label": 0
                },
                {
                    "sent": "The vision is what we wanted to do and.",
                    "label": 1
                },
                {
                    "sent": "You've seen that we can search for people who wish to see that patient attributes as well.",
                    "label": 0
                },
                {
                    "sent": "We can search specific objects, some Pacific objects, and we can search for certain classes.",
                    "label": 0
                },
                {
                    "sent": "We've seen that, so we're sort of up to here on what we want to do, sculptures we haven't.",
                    "label": 0
                },
                {
                    "sent": "We haven't seen.",
                    "label": 0
                },
                {
                    "sent": "Maybe we can do some of them.",
                    "label": 0
                },
                {
                    "sent": "This bottom row.",
                    "label": 0
                },
                {
                    "sent": "We have seen examples of that, and that's because in terms in computer vision were not up to the same standard there.",
                    "label": 0
                },
                {
                    "sent": "Yet this is looking for actions like phoning, playing an instrument.",
                    "label": 0
                },
                {
                    "sent": "Coses activities events birthday parties.",
                    "label": 0
                },
                {
                    "sent": "We can't do that yet, but I think over the next 4 five years will get there and this is really a challenge to you.",
                    "label": 0
                },
                {
                    "sent": "This is this is where we should be.",
                    "label": 1
                },
                {
                    "sent": "In five years we should be able to search for 10 years.",
                    "label": 0
                },
                {
                    "sent": "We should be able to search for all of these things as effortlessly.",
                    "label": 0
                },
                {
                    "sent": "You just saw today Becausw with our work together we can make these things.",
                    "label": 0
                },
                {
                    "sent": "Before as well, so I'm stopping there.",
                    "label": 0
                }
            ]
        }
    }
}