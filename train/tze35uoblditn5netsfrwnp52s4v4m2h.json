{
    "id": "tze35uoblditn5netsfrwnp52s4v4m2h",
    "title": "Dirichlet Processes: Tutorial and Practical Course",
    "info": {
        "author": [
            "Yee Whye Teh, University College London"
        ],
        "published": "Aug. 27, 2007",
        "recorded": "August 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning->Dirichlet Processes"
        ]
    },
    "url": "http://videolectures.net/mlss07_teh_dp/",
    "segmentation": [
        [
            "In the second week of the Machine Learning Summer School.",
            "And today we are going to have a new speaker which is uit from the Gatsby Computational Unit.",
            "He's going to give a talk today, perhaps got talked today and then two practical sessions today and tomorrow, so ET did his undergraduate in mathematics in Waterloo, and then he did his graduate student service jump in 10 minute, where he's now in 2003.",
            "He finished his PhD and immediately after that he did his postdoc with Michael Jordan.",
            "UC Berkeley and with Lee Kuan Yew I don't know Singapore.",
            "OK, OK and since 2007, he's electrified the Gatsby computational unit.",
            "So here it is, an expert in basin Bayesian techniques and approximate inference.",
            "And unsupervised learning.",
            "And today he's going to talk about Dirichlet process is and I'm looking forward to it selection.",
            "OK, thank you.",
            "Is this working now?",
            "OK.",
            "So really honored to be here to tell you about that richly processes.",
            "I'm very excited about this whole area of Bayesian nonparametrics.",
            "Ann.",
            "Really happy to be telling you about this here so.",
            "I guess what I'll be doing today and tomorrow I guess is."
        ],
        [
            "Let them.",
            "Is that I have a tutorial right now and then the practical cost today in the afternoon and tomorrow as well.",
            "So let's start with kind of just a description.",
            "Under each day processes, they're basically a class of what's called Bayesian nonparametric models.",
            "So, as Carl Rasmussen has told you about Gaussian processes, there's another class of base is not permitted models, and this models, which are nonparametric in the sense that they generally have a very large or infinite number of parameters in them an to in order to prevent overfitting.",
            "In such models, you.",
            "You basically try to be based in and integrate out all this infinitely many parameters.",
            "And briefly, processes have become pretty popular nowadays in both statistics and machine learning and.",
            "Some, I'll highlight three general applications that has been used in both in both communities.",
            "That's the estimation.",
            "Yeah, that's the estimation.",
            "So that's basically estimating.",
            "Densities of distributions.",
            "Basically an is also being used a lot in what's called semi parametric modeling and is used a lot in Biostatistics.",
            "And then finally in machine learning, it seems that the richly processes are mostly used as a way of sites that site stepping, model selection and averaging.",
            "So I'll give it to her laundry today.",
            "Processes followed by a practical costs on implementing a directly process mixture will try to cluster NIPS papers if if we do manage.",
            "So get to that quite interesting.",
            "I presume that everybody here is somewhat familiar with the Bayesian paradigm.",
            "Ann, you should know this from zoo banana cost costs as well as some of the previous practical cost.",
            "Something like walking and Matias.",
            "So there's of course been quite a number of other tutorials on directly processes both in machine learning and statistics.",
            "Um so.",
            "You can, I think all this tutorial slides are online as well, so you can take a look at them if you want to kind of see other people presenting so with the different viewpoints I guess.",
            "So let's start."
        ],
        [
            "So the outline of the talk is going to be as follows.",
            "So in the first half of the tutorial or talk about.",
            "Describe to you that free applications that I just talked about and then kind of give you tell you what are the original processes and then we'll take a break and then we will go into a bit more details into the how rich they process can be represented or are represented.",
            "And then we'll finally have link back to the applications by talking about how.",
            "There is a process then used in terms of modeling data and then.",
            "I wouldn't be talking about the practical classes now, just in different costs.",
            "Right?"
        ],
        [
            "So.",
            "Applique"
        ],
        [
            "Asians so?",
            "Let's start with one function estimation who count like link to the talk on Gaussian processes.",
            "That count has told you about.",
            "So let's start with kind of a parametric approach to functional estimation.",
            "So this is used in things like regression and classification, and we assume that our data consists of two sets of points, the axis which correspond to predict this or covert covariance briskly, things which you use to predict your outputs Y.",
            "So from excellent to predict what what is the value of Y?",
            "An being parametric, let's assume that our model.",
            "Is as follows.",
            "We say that Yi is some function of X.",
            "An parameterized by W plus maybe some noise.",
            "In this case, we assume that Gaussian noise.",
            "OK. With a 0 mean and variance Sigma squared right?",
            "And being Bayesian will place the prior over W. And then as you saw in cult talk, you can then.",
            "You can then compute the posterior over.",
            "Over W by basically taking the prior multiply in the likelihood and then normalizing with the basically the marginal likelihood of Y given X integrating out W. And that gives us a posterior overview.",
            "And then if we are given a new test points they X star, then our prediction for why star would be basically the distribution.",
            "Why start given X star as well as our training data would be the integral.",
            "Of probability of wise that given extra NW.",
            "So this is just.",
            "That thing up there.",
            "Integrated over W with respect to the posterior W. So that's kind of the usual Bayesian approach to parametric function estimation.",
            "Right now.",
            "Moving onto the nonparametric way of doing function estimation in the Bayesian approach."
        ],
        [
            "We use Gaussian processes and the idea of Gaussian processes is that instead of having a parametric family of functions, we simply say that we place our prior directly over the class of all possible functions.",
            "Well, not quite almost functions, but basically a really large class of a class of functions.",
            "So now we say that.",
            "I output is some function of X plus Gaussian noise.",
            "And our prior over functions is this thing called Gaussian process, which I'll come to again later.",
            "But you should be familiar with from cows talk and then we'll apply the exactly the same based in machinery in which we can compute both the posterior over functions as well as do predictions integrating out all possible functions within the posterior.",
            "OK, so that's kind of straightforward for function estimation, right?",
            "So that's the estimation is really quite similar, so we'll."
        ],
        [
            "Right, yeah, yeah OK, here's a little slight which I got from cows talkin which.",
            "So basically these are samples of our function from a prior and we observe some input output points given by the cross is here and then we have our posterior which is kind of described in terms of the Gray area and as well as samples from the function.",
            "So coming through density estimation."
        ],
        [
            "So it's basically exactly the same way.",
            "I'll tell you a little bit about density estimation in the parametric framework, and then there's the estimation in a nonparametric framework.",
            "OK, so in the in the for density estimation, our data is simply consists of the axis.",
            "OK, so this is basically unsupervised learning, so in the previous case it's supervised learning, in which you are given the labels or the output which you want to predict right?",
            "And in the case of density estimation you are simply given a set of data points an.",
            "Want to model their density?",
            "There's no output for you to predict.",
            "So again, we will model our access.",
            "Using a parametric family, so again parameterized by W. So what this thing is saying is that the axis are distributed according to some function parameterized by W, some distribution parameterized by W. So this distribution F of.",
            "F here being a distribution and it has parameters W, so again we will place priors over parameters.",
            "Do the computer posterior and then we can do prediction again in exactly the same way.",
            "Of course the prediction here is slightly different in that we are not given the wise.",
            "We simply want to figure out what's the probability of some X star.",
            "Some new test points extra what is probability given our training data and we want to integrate our W in the process so?",
            "How do we move from here?",
            "Which is the primary approach to a nonparametric approach so the?",
            "Basic idea is exactly the same.",
            "Instead of assuming a parametric.",
            "Class or function.",
            "We simply off distributions.",
            "We simply say that since we don't know what our distribution is, will simply place our prior directly on the distribution itself.",
            "So this prior is a distribution on distributions an of course.",
            "This prior is that.",
            "We will be using additional process for this."
        ],
        [
            "So.",
            "The basic parametric approaches we saved our axis is distributed according to some distribution F which we don't know.",
            "So being based in replace a prior on F itself.",
            "So this being that there is a process prior.",
            "And then again we do exactly the same thing in which we we can then compute the posterior distribution.",
            "Over distribution, right?",
            "So this is kind of a weird concept.",
            "And this posterior is again given by the prior multiplied by the likelihood, normalized by the marginal likelihood of the data.",
            "And again, we can do prediction.",
            "In this case, we simply want to calculate the probability of X star given our training data X and again we integrate out F here.",
            "Um?",
            "Of course."
        ],
        [
            "Since F itself is the distribution we are interested in, right, so the probability of extra given F is simply.",
            "Well, actually that's the balance.",
            "Diego is simply the density of F at X to.",
            "OK. Um, actually this is not quite correct."
        ],
        [
            "So we'll come back to this point later.",
            "OK, but but that's the general idea, so the basic non permit approaches.",
            "But to density estimation is we want to.",
            "We don't know our density or we don't know the underlying distribution from which our data comes from.",
            "We simply place a prior directly on the class of all possible distributions and then again we apply the Bayesian machinery.",
            "OK.",
            "Right, so that's that's the estimation.",
            "Actually, is there any yes.",
            "OK, so that's kind of the tricky bit which is actually related to what I'm talking about.",
            "Actually, the question I forgot to repeat or is this FA distribution or density?",
            "Well, in this case, the way I'm writing this, it is a distribution.",
            "Anne.",
            "If the distribution is smooth, then it has a density and it's kind of equivalent, basically yes.",
            "I'm not quite sure what the question is.",
            "The question was that you are saying that that usually the distribution function is easy to estimate but not the density, but presumably that's true only in like one dimension or low dimensions.",
            "That's probably not true.",
            "For like high dimensional.",
            "Cases.",
            "In which you know being in machine learning, that's.",
            "What what our interests are.",
            "Um, I'll come through the.",
            "Difference between density and distribution later?",
            "Right?"
        ],
        [
            "So that's that's the estimation, so here's a little.",
            "A snapshot of what's what happens OK?",
            "So in our prior, we may say that.",
            "The rate so we can draw samples from our distribution from the prior and this things the little.",
            "Each of these lines here plot actually the density of the.",
            "Of the distributions we drew from a prior.",
            "An you know they all kind of look so this might be 1.",
            "Sample from the prior.",
            "It looks like that and another sample is this magenta line which count goes down.",
            "Is picked around this area and another one is this blue line which has a little bumps.",
            "You can see the bumps here so.",
            "And the Gray area is again kind of like the.",
            "Leave 5th to 95th quantile of percentile of the.",
            "Of the prior over the over the densities.",
            "The red line here is the mean density.",
            "And the blue is the median.",
            "Over over the plastic.",
            "So and then we observe some."
        ],
        [
            "Data so this data are given by black points up here and once we've observed data, the posterior overdensities now has been has changed.",
            "Where it's basically.",
            "It is basically figure out that there's actually two clumps of data one over here and one over there, right?",
            "So there's higher density here and higher density than lower density in the middle.",
            "An the red light here is is the mean density in the posterior, while again the Gray area is not the region over which we have high posterior probability over densities.",
            "So this is quite similar to the Gaussian process case."
        ],
        [
            "Right, so coming to the second application, which is parametric modeling.",
            "So here the idea is that.",
            "Really.",
            "So sometimes what we care about is some parametric.",
            "Function, so in this case it's linear regression.",
            "We're interested really in the coefficients of our linear regression, but there are certain parts of the model which we're not really interested in.",
            "An.",
            "As a result, we want to be as flexible as as possible in terms of modeling them.",
            "So here's a little example.",
            "So let's say that I corresponds to a subjects or patients.",
            "Anne.",
            "Each patient I have multiple trials indexed by J, so why IJ is the outcome of the Jade trial on the ice patient?",
            "Anne.",
            "We want to predict YIJ the outcome even predict this X IGN Zach IJ.",
            "Exige could be things like you know the.",
            "The the medical treatment which we give our patients.",
            "And that idea could be things like the H health or the Heights or the.",
            "Blood pressure of the patient.",
            "Um and what we're interested in is weather.",
            "Whether the.",
            "Whether the treatment which we gave the patient is effective or not, so given X, Zack and Y we want to.",
            "Infer what's the value of beta or what's the posterior over beta, and if the posterior is positive, then we know that our treatment is somehow effective.",
            "And that's the power model which we care about.",
            "Right?",
            "But there's also other parts of the model which has.",
            "For example, this part is.",
            "Count the effects.",
            "That effects on the.",
            "On the efficient on the effectiveness of the treatment, which depends on things like the general health of the patient.",
            "And of course this part.",
            "We don't really care as much about what we are, what we want to know is, is our new medicine, which is the new treatment that we are giving this patient effective or not?",
            "So."
        ],
        [
            "Since we don't, since we don't really care about the patient specific effects or kind of the noise.",
            "Or the noise?",
            "That on the on the outcome of the fitment.",
            "OK, so we instead of assuming like Gaussian prior over the noise or over the random effects BI, we simply say that the we should model the noise in the nonparametric fashion.",
            "So, and the way we model it is.",
            "Again, we say that noise.",
            "Terms are drawn from some noise distribution F. And since we don't know what F is, we say that F should be drawn from some directly process.",
            "OK, so we model the noise.",
            "Uh.",
            "Distribution in a nonparametric fashion and this is able to handle things like Overdispersion, Orsk illness, which is often observed in clinical trials.",
            "It's also possible to model subjects specific random effects.",
            "Nonparametric, nonparametric nonparametrically.",
            "By saying that the eyes are drawn from some distribution G and we don't know G. So again we place the prior over G AED richly process priority.",
            "So this is called semi parametric because.",
            "These two parts of the model nonparametric.",
            "But what we care about is that part, which is parametric, so it's kind of half half.",
            "So we simply use the nonparametric model as a way of.",
            "Up bring inflexibility towards our model but we kept the parametric part because that's more interpretable.",
            "Yes, the inferences we make are more more interpretable.",
            "Um?"
        ],
        [
            "So in machine learning, the rich data processes are used mostly in model selection or averaging, so I'll kind of go through that again a little bit.",
            "So here our data.",
            "Consist of sequence XX1X2 and so forth.",
            "And our model we have a prior.",
            "So for model MK we have parameters data K and this is our prior over parameters in the model plus K. And we also have a likelihood term, which is the probability of the data given our parameters and model plus K. An for model selection.",
            "So if we have a sequence of.",
            "The M1 and M2 and so forth.",
            "And we want to figure out which one is the best OK so.",
            "To do that, we should.",
            "Calculate the marginal likelihood of the data X given our model.",
            "Integrating out the parameters within the model.",
            "And then we can do model selection by saying that we should choose the model which maximizes the marginal likelihood of the data.",
            "If we're being more base and then of course we don't want to do model selection, we want to do model averaging.",
            "In this case we could.",
            "In the predictive case, we could say that the probability of some new data test point X star given training.",
            "Set X.",
            "Is sum over all possible model classes?",
            "Of the probability of extra given this model class.",
            "Actually there's a. X missing in there sorry.",
            "And we average the likelihood of extra under model plus K. Over the posterior over model classes.",
            "By the post."
        ],
        [
            "Weather posteriors given by this thing.",
            "Prior times likelihood normalized.",
            "But the problem with this."
        ],
        [
            "So the problem is this is the major question to ask is.",
            "Is this marginal likelihood OK?",
            "This term here?",
            "Which is the most important term in terms of evaluating?",
            "Both model selection and model averaging.",
            "Can we actually compute this marginal likelihood?"
        ],
        [
            "It turns out that.",
            "The marginal is extremely hard to compute, or virtually all models which we're interested in.",
            "So so that.",
            "It makes it hard to do model selection and model averaging, right?",
            "And of course.",
            "Both model selection and model averaging are very important in terms of preventing overfitting, and underfitting of our models.",
            "We want to find a model of the right complexity to fit to our data.",
            "But check the idea is that if we actually have reasonable and proper Bayesian methods.",
            "OK, well basically we put reasonable prior over parameters and we integrate our all possible parameters.",
            "Then the model.",
            "The method should not be overfitting to our data.",
            "Um?",
            "So what does that mean?",
            "So if a model doesn't overfit the data, then we should simply use as large model as possible.",
            "So we use a really large model, say Infinity OK. And then somehow let the data speak for itself.",
            "OK, so I'll come.",
            "I'll give you an example of what happens here.",
            "In the case of a in the case of a mixture model.",
            "So when you're trying to model data with a mixture model, you want to figure out the number of components in there.",
            "OK, you could actually take M Infinity, so MK would be a finite mixture model with K components.",
            "Ann we the.",
            "The approach which I've kept refering to here involving an Infinity is a mixture model with an infinite number of mixture components.",
            "And that turns out to be a direct process mixture model.",
            "Hit.",
            "So, um.",
            "Give you some eggs."
        ],
        [
            "Both of.",
            "Cases of model selection and averaging, which yes."
        ],
        [
            "Right?",
            "Right, I see.",
            "OK, so.",
            "Sorry was the last word.",
            "Oh OK, so the question was.",
            "So that's kind of two ways in which you could get at a really large model and Infinity so.",
            "The first way is you take.",
            "A mixture over your mixture models where the mixture is over.",
            "The number of components within that mixture model, so you can take mixture model involving one component involving two component involving three component and then you can.",
            "Average over all possible.",
            "Ah.",
            "Models, and that's of course.",
            "That gives you a infinite mission model, but it's actually different.",
            "So what we're saying here is that we actually have.",
            "A mixture model in which the mixture just one mixture model in which the mixture number of mixture components is infinite rather than an infinite number of finite mixture models, and they are somewhat different, yes.",
            "So.",
            "The question was you have a. Um?",
            "OK, right?",
            "Which I used to OK, so the question was, is it kind of like you have a mixture model in which you have mixture components everywhere in your space?",
            "Right, possibly everywhere in your space, and then there's, of course, if your space is really big, then you need an infinite number of major components and then given data you would the model.",
            "When you do posterior inference, you would.",
            "Automatically.",
            "Select out the mixture components which are kind of useful to explain the data and that is the correct way of viewing.",
            "Listing question.",
            "Right?",
            "Well.",
            "Not what I mean.",
            "You could always play some.",
            "Yes, but you could always place the prior distribution on the mean, which is not a uniform distribution.",
            "Oh, you're using a uniform product.",
            "OK. Fuck.",
            "Dirt.",
            "Wonder why?",
            "My God, my God.",
            "So the question is, why is the Gaussian prior not appropriate for this sort of model?",
            "In fact, there are people who look into density estimation where the prior is where there's a Gaussian process in the prior.",
            "But you need to make sure that your densities are integrable, Ann.",
            "Discount other to make to design your kernel of your Gaussian process so that you know the density.",
            "Is actually integrable.",
            "You kind of have a uniform distribution over things.",
            "An yeah that's global, but it turns out to be computationally a lot harder.",
            "OK.",
            "I should get up here."
        ],
        [
            "So the 1st.",
            "The first model selection averaging thing is a clustering right.",
            "So in the case of a mixture model, if our data looks like that, then the question is does it consists of three clusters or maybe one or maybe 4 right?",
            "We need to figure out how many clusters they are, so that's the model selection or averaging.",
            "Questions to ask in this case?",
            "Um?"
        ],
        [
            "This solved.",
            "This of problem occurs all over the place, so another case in which it occurs is in spike sorting.",
            "So the idea is.",
            "That that you've plucked you put some electrode into the brain of a monkey or or a mouse.",
            "And let's say that you have four electrodes and these are the signals which are coming from the four electrodes, so.",
            "That's kind of 1 observation, and that's another observation, and each line here corresponds to one electrode.",
            "And if you notice the first tree.",
            "Spikes right so this account, the little spikes that you see.",
            "Kind of has similar shape, right?",
            "And the idea is if they have similar shape then probably the spikes came from the same neuron.",
            "An if you plug in an electrode into the brain of a mouse, then you actually receive spikes from multiple neurons and you need to figure out which spike belongs to which neuron.",
            "In this case, maybe you might think that the first tree spikes belong to one neuron, and then maybe the next 4 belong to a.",
            "To the second neuron and then the next tree belongs to 1/3 neuron.",
            "So again the question here is how many neurons are there in the vicinity of your electrode that you can receive spikes mode right?",
            "So this is basically like clustering, except that the data is much more high dimensional.",
            "So in fact, that's what people do.",
            "People just take.",
            "The high dimensional data and they just cluster them and then each cluster they say belongs to correspond to the spikes corresponding to one.",
            "One unique neuron.",
            "Another example is."
        ],
        [
            "Like modeling, so here the idea is that you are given a set of documents.",
            "Ann you.",
            "Decompose the.",
            "Each document into a mixture over topics.",
            "So you describe each document as a mixture of the topics an.",
            "And then there would be a set of topics which describes all documents within your corpus, so.",
            "Again, the question there is, how many topics are exhibited within a particular corpus of documents.",
            "And again we need to answer that question and we need to do model selection or averaging.",
            "Is it working?",
            "I like this OK. Bye.",
            "OK, so another example is in grammar induction.",
            "So here the idea is you want to learn a context probabilistic context.",
            "Free grammar for language they say English, so there's two parts."
        ],
        [
            "Questions you can.",
            "Ask here, the first one is even lots and lots of sentences of English from English.",
            "Can you?",
            "Figure out a grammar for English and at the same time parse each sentence in.",
            "In the grammar.",
            "Up to people familiar with past reason, context free grammar and stuff should be.",
            "OK, I guess I. I don't think I'll have time to go into what the context free grammars, but basically they look like this.",
            "So in this case.",
            "The sentence is she heard the noise and what you want to do is to parse the sentence in terms of so.",
            "In this case, you have a sentence and that there might be a noun phrase here.",
            "There's a verb phrase, and the verb phrases heard the noise.",
            "And then the phrases in turn parsed into two components.",
            "I think this VBD is a verb, but a particular form of verb, and I'm not a natural language processing expert, so I don't know what it stands for.",
            "And then this is again a noun phrase.",
            "So once you pass the sentence you have, you could kind of say that you have somehow understood the sentences to some extent.",
            "So the question now is, given lots of English sentences, can you learn a grammar for English and parse the sentences in terms of the grammar?",
            "The second question you could ask this.",
            "Let's say that we have a grammar, so this is our past three in the grammar.",
            "Um?",
            "Maybe you might think that OK, there's actually multiple types of verb phrases that goes.",
            "Along with multiple types of noun phrases, so, but of course the grammar you're given you only told is it a verb phrase or noun phrase, and what you want to learn is how many types of verb phrases are there.",
            "How many types of noun phrases are there?",
            "And you can learn all this again from sentences of English.",
            "So again, this is a model selection question because.",
            "You know you don't really know how many types of noun phrases are there to begin with, you need to figure that out right."
        ],
        [
            "So another example is from Visual Studio analysis, in which given things like pictures you want to decompose the scene into objects and object into parts and parts into features.",
            "Again, you you don't really know how many objects are there, how many object types are there?",
            "How many parts are there in within each object, and so forth.",
            "Ann, this of questions is again model selection questions because.",
            "If you.",
            "Have a really rich model and you.",
            "Don't do the Bayesian thing and you just maximize likelihood.",
            "Then of course it would say that.",
            "You know you have.",
            "You have many, many different objects, and each object corresponds to.",
            "Maybe a whole image or something like that?",
            "Um?",
            "So."
        ],
        [
            "Those are kind of the three applications of richly processes and turns out that they process are pretty reasonable.",
            "Ways to solve all three problem?",
            "So it's almost 12, so I kind of better be quick in terms of definition of addition."
        ],
        [
            "Process, so let's start off with a finite mixture model.",
            "Are people familiar with mixture models?",
            "Yes.",
            "OK so I can go through this pretty quickly.",
            "Um?",
            "OK, so here's the definition of a finite mixture model.",
            "We set our data points XI right?",
            "For each data point XI, we first.",
            "Draw that I wish corresponds to which mixture component XI came from an that I the cluster indicated variable like I is drawn from some discrete distribution.",
            "Pot given by pie.",
            "So this is high is the mixing proportions.",
            "Being Bayesian also and then given like I.",
            "Our data X is drawn from some distribution parameterized by by AVS.",
            "I OK, so that's kind of it.",
            "Being Bayesian would face place priors over both high the mixing proportions as well as a cluster parameters.",
            "Spiky so that's a graphical model.",
            "So the model selection.",
            "Or averaging problem here is.",
            "We want to figure out what the hyperparameters in H. What are the directional parameters Alpha, and most importantly, what's the number of components in our finite mixture model."
        ],
        [
            "Um?",
            "Since.",
            "If we take the Bayesian approach, we're not worried about overfitting, right?",
            "So then we can imagine taking K the number of clusters in our mixture model to be really large.",
            "So notice that if we can integrate out both the parameters by Ann mixing proportions high.",
            "Then we integrate those two and those are the only latent variables left in our model, right?",
            "And the number of latent variables here is just N, where N is the number of data points.",
            "So if we take K to be really large.",
            "The number of latent variables that we have does not actually grow in terms of K. Once we've integrated out the cluster component.",
            "The mixture component parameter.",
            "So what this means is that that would not be overfitting in this model.",
            "Notice that.",
            "There can be at most N components which are active in the sense that they are associated with data, and of course the reason for that is you have any data points each being associated with one cluster, right?",
            "Um?",
            "But it turns out that usually the number of such active components is much less than N, even the, even though the total number of mixture components K is could be infinite.",
            "So this actually gives us an infinite mixture model, so I'll kind of give you a little demo to show that this thing actually works.",
            "I need to do something.",
            "Wait?",
            "Yeah, that would be.",
            "I'm good.",
            "Ah.",
            "Thing.",
            "Well, I just looked now.",
            "OK so this is our.",
            "That points to our data points, and we're sampling from the posterior of our richly infinite mixture model.",
            "Anne.",
            "The... here corresponds to the mixture components which are actually active or associated with data.",
            "An.",
            "In fact, there's actually an infinite number of mixture components within our model, but most of the time we only see a small number of major components which are actually associated with data.",
            "OK, so that's kind of two issues which we would like to address here.",
            "At.",
            "The first issue is can we take this infinite limit.",
            "This limit where K goes to Infinity and the second issue is what is the corresponding limiting model.",
            "So what we'd like to do is instead of starting with a finite model and then taking the Internet limit, we actually want to define our.",
            "Infinite model directly and then from there we can see the relationship to the finite model."
        ],
        [
            "Um?",
            "Running out of time so.",
            "I guess everybody here is familiar with Gaussian processes by now, so it defines a distribution over functions.",
            "Um?",
            "So.",
            "Prior over functions, is a Gaussian process.",
            "If for any finite set of input points X one to XN.",
            "F evaluated at X1 until F evaluated at X and this thing is a multivariate Gaussian.",
            "So this defines a Gaussian process."
        ],
        [
            "So Gaussian process has.",
            "I mean function and covariance function.",
            "An important property of this marginal distributions is that they are consistent.",
            "So what meant by consistent here was meant is that if you take.",
            "Um?",
            "This multivariate Gaussian and you integrate out F of XN.",
            "Then we're just left with a distribution over F of X12 F of X N -- 1, and what?",
            "Is important here.",
            "Is that that distribution is again given by a Gaussian.",
            "It was exactly the same form, except that.",
            "Is only vengeance from X1 to XN minus one.",
            "Um?",
            "Right so I."
        ],
        [
            "We've seen a sequence of input points this.",
            "Visualizing about so, you could visualize a Gaussian process by saying that we first draw F of X1 and then we can draw F of X2 given X1 and X3 given X1 and X2 and so forth.",
            "Then I'll skip the demo because I'm already running out of time.",
            "Um?",
            "So we can actually do this because each of the conditional distributions is is Gaussian.",
            "Since F of X12, F of X is Gaussian."
        ],
        [
            "So moving onto a direction they process now.",
            "So just as a Gaussian process has Gaussian.",
            "Distributed marginal distributions directly.",
            "Process has richly distributed marginal distribution, so are people familiar with the rich distributions here.",
            "Good great.",
            "OK so I can speed up again so I guess.",
            "OK, OK. Um?",
            "Richland Distribution is distribution over the K dimensional probability simplex an the K dimensional probability simplex is simply a set of vectors high one 2\u03c0 K. Such that each entry is positive and they sum to one.",
            "So you can almost think of this as a distribution itself, right?",
            "So this is, think of how I want high K as a distribution as a discrete distribution.",
            "Over K outcomes right Pi K being the probability of choosing.",
            "Outcome K. We say that the original distribution is the distribution over the K dimensional probability simplex, and we say that I want high K is richly distributed.",
            "If it is a random vector where.",
            "The densities look like has this form.",
            "So that's the density of operational distribution.",
            "Um?",
            "Um?",
            "So I'll just show you that stuff quite a lot of nice properties of rich data distributions, but I'll just show you."
        ],
        [
            "You can plots of how the density looks like.",
            "So what I forgot to say?"
        ],
        [
            "So the rich they have parameters A1 and 2A, K and this parameters here has to be positive, but they need not sum to one, they just have to be positive.",
            "OK."
        ],
        [
            "The alphas are all one we that richly distribution with all with the parameters of 1.",
            "Simply give you a uniform distribution over the probability simplex.",
            "So this is the case.",
            "Where K equals the tree.",
            "And you have a probability simplex.",
            "Each vertex here corresponds to one outcome.",
            "And if you have a pie.",
            "So each each point within this triangle corresponds to a.",
            "A pipe vector and this high vector would have value which is large.",
            "So if this is Vertex 1, two and three, then apply vector here would mean that Taiwan is pretty big, but Pi 2 and \u03c0 three are pretty small.",
            "When the parameters of the directly go above 1.",
            "Then we kind of get a little bump in the center.",
            "And the larger the so this is, this is a richly with parameters of two.",
            "This is a directly with the parameter of five and you can see that the bump gets more concentrated around.",
            "Around the center.",
            "So the size of the parameter corresponds to how concentrated is the.",
            "Is the distribution.",
            "When you have parameters, in this case it's 5, five and two, then the bump have shifts around this simplex.",
            "Um?",
            "And if the parameter is less than one.",
            "It turns out that instead of kind of concentrating around a little bump, all the mathcounts spreads out and they concentrate around the vertices.",
            "Yeah.",
            "So that's what that original distributions look like.",
            "OK, so that's."
        ],
        [
            "2 interesting properties of richly distributions which is useful in terms of directionally process.",
            "The first one is what I call an elementary.",
            "Property of the original distributions.",
            "Here, let's say that I want you Paikea is drawn from a dish day with premise A1 to Alpha K. And you take 1 high too and you add them together.",
            "Then this vector.",
            "Which belongs to the K -- 1 dimensional probability simplex.",
            "This thing is richly is also directly distributed.",
            "But with the parameters.",
            "A1 and A2 added together.",
            "And generally, if we have a. I want to IJ if this thing is the partition of the Special K is the partition of 1 until K. Then we some.",
            "It each subset here L I-1 the sum of all the entries of Pi within that subset and so forth.",
            "This thing belongs to the.",
            "J dimensional probability simplex.",
            "And this thing turns out to be also richly distributed with the corresponding parameters summed up together.",
            "So this is an important property of attrition rate distributions, which we'll be using later."
        ],
        [
            "So the conference of the Agglomerative property is what?",
            "Is decimated property.",
            "So in the case of the agglomerative property you take 2 components of our.",
            "Of our vector \u03c0, and we added them together, right?",
            "So in the case of decimated, we take one component of this vector and we split into two and the way we split into 2.",
            "Is as follows, so let's again we assume that I want a \u03c0 case richly distributed with this parameter.",
            "In dependently let's draw.",
            "How one how to from Additionally from a 2 dimensional digital distribution?",
            "With parameters given by A1, beta one and A1 beta two and here is important that beta one and beta 2 sum to one.",
            "Then we take high one and we split it into pie.",
            "One power, one anti, one come to we know that this thing comes to one.",
            "So this thing also has to sum to one.",
            "Right, so this thing is also directly if both of these are drawn independently and it's richly where this parameter A1 is split into two parameters.",
            "In exactly the same fashion here.",
            "So that's the estimated property of authority distribution.",
            "Um?"
        ],
        [
            "So now the idea is the following.",
            "So richly process is basically an infinitely estimated duration distribution.",
            "So we start off with.",
            "One is Additionally of Alpha, so this is.",
            "A1 dimensional directly distribution is simply a pointer one.",
            "We take this thing and we split into Pi one and Pi 2.",
            "By and the way we do it is basically we draw.",
            "In dependently withdraw how one and two and that gives us Taiwan?",
            "And Pi 2\u03c0 one is 1 times tell one Pi 2 is 1 * 2.",
            "And because of the estimated, we know that we can split Alpha here into Alpha, develop into an Alpha divided by two.",
            "Then we can take Taiwan and tie two.",
            "We slipped by one and two PI11 and PI12.",
            "So that this.",
            "Who sings some 2\u03c0 one and we split high 2 into \u03c0 two one and two 2 so that these two entries, some 2\u03c0 two?",
            "And again, this thing would still be directly distributed with those parameters, and we basically do this infinitely often.",
            "Um?",
            "And let's see how the how does this look like.",
            "So.",
            "Yeah I have a little rectangle here.",
            "And the area of this rectangle corresponds to the.",
            "The value of one of the pies.",
            "OK, so in this case.",
            "We have one because basically 10 * 0.1 is 1, so that's the total probability mass which we're going to assign to the whole space.",
            "We take this thing I was split into 2.",
            "And that's high one.",
            "And that's by two.",
            "So the area within this rectangle is \u03c0.",
            "One area within that rectangle, Pi 2.",
            "OK, again we take this rectangle with split into PI11 and PI12.",
            "And we take this thing and we spin it into high 21 and Pi 2 two right?",
            "So if we repeat this an we may get something which looks like that.",
            "And if we actually do this often, like.",
            "Lots of times.",
            "Here's what we get.",
            "We keep on splitting each rectangle into two parts.",
            "And we keep on splitting it and splitting it OK. And that's basically a draw from our drizly process.",
            "Notice that it almost looks like a bunch of.",
            "Points methods.",
            "OK, and in fact.",
            "Um?",
            "So what this means is that a draw for moderation they process.",
            "Is basically a discrete distribution.",
            "And that's an important property of reprocessing.",
            "Again."
        ],
        [
            "OK, so.",
            "Properly, what is the derivative process?",
            "They reach their processes basically.",
            "A distribution over probability measures.",
            "What probability measures we can think of probability measures as simply?",
            "Functions as well.",
            "So it's a probability measure is a function from subsets of some SpaceX 201 which satisfies certain properties, which makes them probability measures.",
            "So these are properties which things like.",
            "OK.",
            "So.",
            "G is the probability measure if G evaluated on the whole SpaceX.",
            "Has to be one.",
            "G evaluated on the empty set.",
            "Has to be 0.",
            "And if you have a sequence of subsets.",
            "Page evaluated at.",
            "A1.",
            "Disjoint.",
            "82 AN.",
            "This thing is simply some.",
            "From one to end of G of AI.",
            "And this subsets has to be what's called measurable subsets, but we don't have to worry about that very much really.",
            "Um?",
            "So basically that defines a probability measure is simply a function from subsets of some space 201 OK, so remember the Gaussian processes a prior over functions.",
            "Additional processes also apply over functions, except that these are special functions.",
            "These are probability measures.",
            "An the definition of Additionally process is as follows.",
            "Basically.",
            "G is richly processed distributed.",
            "If it's a random probability measure with the following property for any finite set of partitions A1 until 8K.",
            "So this thing partitions into X, So what that means is that they are disjoint subsets and their union is the whole SpaceX.",
            "If we have such a partition.",
            "Then G evaluated A1 until G evaluated at 8K.",
            "So this thing is a random vector now right?",
            "And this is a random vector in which each entry is non negative and each and the entries sum to one.",
            "Because G is a probability measure, so it has exactly.",
            "Those properties right?",
            "And we say that G is richly processed distributed if each for each finite set of partitions.",
            "This vector here is directly distributed.",
            "OK, that's the definition of a tree."
        ],
        [
            "The process.",
            "Additionally, just as in the case of a Gaussian process, additional process has.",
            "Two parameters, it has a base distribution which basically corresponds to the mean of the process.",
            "And the strength parameter, which basically corresponds to the inverse variance of the process.",
            "OK.",
            "So we write.",
            "G is the richly process with strength parameter Alpha and base distribution H. Like this?",
            "If for any partition.",
            "Of XG of a one until Geo.",
            "AKA this thing is a K that is a vector in the K dimensional probability simplex.",
            "This thing is Dirichlet distributed with parameters given by this.",
            "And that's the definition of a three day process.",
            "So why do we call the base distribution aminin strength parameter, inverse variance?",
            "Cancel that."
        ],
        [
            "It's pretty straightforward to workout.",
            "That if we have some subset a.",
            "Then the expected value of G evaluated at a is simply H of a right?",
            "So this is quite similar to the case of a Gaussian process, in which the mean of a Gaussian process evaluated at some point is the mean.",
            "Of the Gaussian process itself evaluated at that point.",
            "So.",
            "Yeah, in the on the other hand, the variance.",
            "At of G, evaluated evaluated at a.",
            "This thing has the following form is age of 8 * 1 -- 8 / A plus 1A plus one.",
            "So what this means is that the larger Alpha is, the smaller the variance.",
            "Right, so which is why we say that Alpha is like an inverse variance to the judicial process."
        ],
        [
            "Right so.",
            "Basically.",
            "The best definition of additional process.",
            "The main question you need to ask here is that how do we know that such an object exists?",
            "It turns out that there's multiple ways of proving that such an object this.",
            "Um?"
        ],
        [
            "The first way is by Ferguson in 1973, so this paper is the one that gave the definition of a derivative process an.",
            "The first proof of existence he uses what's called the use Kolmogorov consistency theorem, but it turns out that this theorem.",
            "Doesn't quite work, Ann.",
            "The reason is because.",
            "Um?",
            "So this thing only.",
            "Um?",
            "Guarantees the existence of a function.",
            "From measurable subsets.",
            "201 but it doesn't actually guarantee that this function.",
            "This random function is in fact a draw from.",
            "This function is.",
            "A probability measure.",
            "And then other people came along and."
        ],
        [
            "Actually showed that in fact these things do exist.",
            "So that's some people who Blackwell McQueen they use definitely theorem to show that that is in fact such a random.",
            "Our probability measure.",
            "Which satisfies the properties of additional process.",
            "And then the."
        ],
        [
            "The best proof of existence is actually what's called stick breaking construction, and this thing basically.",
            "Constructs that originally process directly, and because you can actually construct it, you know that it exists basically.",
            "That's also a gamma processing machine."
        ],
        [
            "When are we going to?",
            "Um?",
            "I thought"
        ],
        [
            "We can take a 5 minute break.",
            "Yep."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the second week of the Machine Learning Summer School.",
                    "label": 0
                },
                {
                    "sent": "And today we are going to have a new speaker which is uit from the Gatsby Computational Unit.",
                    "label": 0
                },
                {
                    "sent": "He's going to give a talk today, perhaps got talked today and then two practical sessions today and tomorrow, so ET did his undergraduate in mathematics in Waterloo, and then he did his graduate student service jump in 10 minute, where he's now in 2003.",
                    "label": 0
                },
                {
                    "sent": "He finished his PhD and immediately after that he did his postdoc with Michael Jordan.",
                    "label": 0
                },
                {
                    "sent": "UC Berkeley and with Lee Kuan Yew I don't know Singapore.",
                    "label": 0
                },
                {
                    "sent": "OK, OK and since 2007, he's electrified the Gatsby computational unit.",
                    "label": 1
                },
                {
                    "sent": "So here it is, an expert in basin Bayesian techniques and approximate inference.",
                    "label": 0
                },
                {
                    "sent": "And unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "And today he's going to talk about Dirichlet process is and I'm looking forward to it selection.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "Is this working now?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So really honored to be here to tell you about that richly processes.",
                    "label": 0
                },
                {
                    "sent": "I'm very excited about this whole area of Bayesian nonparametrics.",
                    "label": 0
                },
                {
                    "sent": "Ann.",
                    "label": 0
                },
                {
                    "sent": "Really happy to be telling you about this here so.",
                    "label": 0
                },
                {
                    "sent": "I guess what I'll be doing today and tomorrow I guess is.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let them.",
                    "label": 0
                },
                {
                    "sent": "Is that I have a tutorial right now and then the practical cost today in the afternoon and tomorrow as well.",
                    "label": 0
                },
                {
                    "sent": "So let's start with kind of just a description.",
                    "label": 0
                },
                {
                    "sent": "Under each day processes, they're basically a class of what's called Bayesian nonparametric models.",
                    "label": 1
                },
                {
                    "sent": "So, as Carl Rasmussen has told you about Gaussian processes, there's another class of base is not permitted models, and this models, which are nonparametric in the sense that they generally have a very large or infinite number of parameters in them an to in order to prevent overfitting.",
                    "label": 0
                },
                {
                    "sent": "In such models, you.",
                    "label": 0
                },
                {
                    "sent": "You basically try to be based in and integrate out all this infinitely many parameters.",
                    "label": 0
                },
                {
                    "sent": "And briefly, processes have become pretty popular nowadays in both statistics and machine learning and.",
                    "label": 0
                },
                {
                    "sent": "Some, I'll highlight three general applications that has been used in both in both communities.",
                    "label": 0
                },
                {
                    "sent": "That's the estimation.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's the estimation.",
                    "label": 0
                },
                {
                    "sent": "So that's basically estimating.",
                    "label": 0
                },
                {
                    "sent": "Densities of distributions.",
                    "label": 0
                },
                {
                    "sent": "Basically an is also being used a lot in what's called semi parametric modeling and is used a lot in Biostatistics.",
                    "label": 0
                },
                {
                    "sent": "And then finally in machine learning, it seems that the richly processes are mostly used as a way of sites that site stepping, model selection and averaging.",
                    "label": 0
                },
                {
                    "sent": "So I'll give it to her laundry today.",
                    "label": 1
                },
                {
                    "sent": "Processes followed by a practical costs on implementing a directly process mixture will try to cluster NIPS papers if if we do manage.",
                    "label": 1
                },
                {
                    "sent": "So get to that quite interesting.",
                    "label": 0
                },
                {
                    "sent": "I presume that everybody here is somewhat familiar with the Bayesian paradigm.",
                    "label": 0
                },
                {
                    "sent": "Ann, you should know this from zoo banana cost costs as well as some of the previous practical cost.",
                    "label": 0
                },
                {
                    "sent": "Something like walking and Matias.",
                    "label": 0
                },
                {
                    "sent": "So there's of course been quite a number of other tutorials on directly processes both in machine learning and statistics.",
                    "label": 0
                },
                {
                    "sent": "Um so.",
                    "label": 0
                },
                {
                    "sent": "You can, I think all this tutorial slides are online as well, so you can take a look at them if you want to kind of see other people presenting so with the different viewpoints I guess.",
                    "label": 0
                },
                {
                    "sent": "So let's start.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the outline of the talk is going to be as follows.",
                    "label": 0
                },
                {
                    "sent": "So in the first half of the tutorial or talk about.",
                    "label": 0
                },
                {
                    "sent": "Describe to you that free applications that I just talked about and then kind of give you tell you what are the original processes and then we'll take a break and then we will go into a bit more details into the how rich they process can be represented or are represented.",
                    "label": 0
                },
                {
                    "sent": "And then we'll finally have link back to the applications by talking about how.",
                    "label": 0
                },
                {
                    "sent": "There is a process then used in terms of modeling data and then.",
                    "label": 0
                },
                {
                    "sent": "I wouldn't be talking about the practical classes now, just in different costs.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Applique",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Asians so?",
                    "label": 0
                },
                {
                    "sent": "Let's start with one function estimation who count like link to the talk on Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "That count has told you about.",
                    "label": 0
                },
                {
                    "sent": "So let's start with kind of a parametric approach to functional estimation.",
                    "label": 0
                },
                {
                    "sent": "So this is used in things like regression and classification, and we assume that our data consists of two sets of points, the axis which correspond to predict this or covert covariance briskly, things which you use to predict your outputs Y.",
                    "label": 0
                },
                {
                    "sent": "So from excellent to predict what what is the value of Y?",
                    "label": 0
                },
                {
                    "sent": "An being parametric, let's assume that our model.",
                    "label": 0
                },
                {
                    "sent": "Is as follows.",
                    "label": 0
                },
                {
                    "sent": "We say that Yi is some function of X.",
                    "label": 0
                },
                {
                    "sent": "An parameterized by W plus maybe some noise.",
                    "label": 0
                },
                {
                    "sent": "In this case, we assume that Gaussian noise.",
                    "label": 0
                },
                {
                    "sent": "OK. With a 0 mean and variance Sigma squared right?",
                    "label": 0
                },
                {
                    "sent": "And being Bayesian will place the prior over W. And then as you saw in cult talk, you can then.",
                    "label": 1
                },
                {
                    "sent": "You can then compute the posterior over.",
                    "label": 1
                },
                {
                    "sent": "Over W by basically taking the prior multiply in the likelihood and then normalizing with the basically the marginal likelihood of Y given X integrating out W. And that gives us a posterior overview.",
                    "label": 0
                },
                {
                    "sent": "And then if we are given a new test points they X star, then our prediction for why star would be basically the distribution.",
                    "label": 0
                },
                {
                    "sent": "Why start given X star as well as our training data would be the integral.",
                    "label": 0
                },
                {
                    "sent": "Of probability of wise that given extra NW.",
                    "label": 0
                },
                {
                    "sent": "So this is just.",
                    "label": 0
                },
                {
                    "sent": "That thing up there.",
                    "label": 0
                },
                {
                    "sent": "Integrated over W with respect to the posterior W. So that's kind of the usual Bayesian approach to parametric function estimation.",
                    "label": 1
                },
                {
                    "sent": "Right now.",
                    "label": 0
                },
                {
                    "sent": "Moving onto the nonparametric way of doing function estimation in the Bayesian approach.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We use Gaussian processes and the idea of Gaussian processes is that instead of having a parametric family of functions, we simply say that we place our prior directly over the class of all possible functions.",
                    "label": 0
                },
                {
                    "sent": "Well, not quite almost functions, but basically a really large class of a class of functions.",
                    "label": 0
                },
                {
                    "sent": "So now we say that.",
                    "label": 0
                },
                {
                    "sent": "I output is some function of X plus Gaussian noise.",
                    "label": 0
                },
                {
                    "sent": "And our prior over functions is this thing called Gaussian process, which I'll come to again later.",
                    "label": 1
                },
                {
                    "sent": "But you should be familiar with from cows talk and then we'll apply the exactly the same based in machinery in which we can compute both the posterior over functions as well as do predictions integrating out all possible functions within the posterior.",
                    "label": 1
                },
                {
                    "sent": "OK, so that's kind of straightforward for function estimation, right?",
                    "label": 0
                },
                {
                    "sent": "So that's the estimation is really quite similar, so we'll.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, yeah, yeah OK, here's a little slight which I got from cows talkin which.",
                    "label": 0
                },
                {
                    "sent": "So basically these are samples of our function from a prior and we observe some input output points given by the cross is here and then we have our posterior which is kind of described in terms of the Gray area and as well as samples from the function.",
                    "label": 0
                },
                {
                    "sent": "So coming through density estimation.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it's basically exactly the same way.",
                    "label": 0
                },
                {
                    "sent": "I'll tell you a little bit about density estimation in the parametric framework, and then there's the estimation in a nonparametric framework.",
                    "label": 0
                },
                {
                    "sent": "OK, so in the in the for density estimation, our data is simply consists of the axis.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is basically unsupervised learning, so in the previous case it's supervised learning, in which you are given the labels or the output which you want to predict right?",
                    "label": 1
                },
                {
                    "sent": "And in the case of density estimation you are simply given a set of data points an.",
                    "label": 0
                },
                {
                    "sent": "Want to model their density?",
                    "label": 0
                },
                {
                    "sent": "There's no output for you to predict.",
                    "label": 0
                },
                {
                    "sent": "So again, we will model our access.",
                    "label": 1
                },
                {
                    "sent": "Using a parametric family, so again parameterized by W. So what this thing is saying is that the axis are distributed according to some function parameterized by W, some distribution parameterized by W. So this distribution F of.",
                    "label": 0
                },
                {
                    "sent": "F here being a distribution and it has parameters W, so again we will place priors over parameters.",
                    "label": 0
                },
                {
                    "sent": "Do the computer posterior and then we can do prediction again in exactly the same way.",
                    "label": 0
                },
                {
                    "sent": "Of course the prediction here is slightly different in that we are not given the wise.",
                    "label": 0
                },
                {
                    "sent": "We simply want to figure out what's the probability of some X star.",
                    "label": 0
                },
                {
                    "sent": "Some new test points extra what is probability given our training data and we want to integrate our W in the process so?",
                    "label": 0
                },
                {
                    "sent": "How do we move from here?",
                    "label": 0
                },
                {
                    "sent": "Which is the primary approach to a nonparametric approach so the?",
                    "label": 0
                },
                {
                    "sent": "Basic idea is exactly the same.",
                    "label": 0
                },
                {
                    "sent": "Instead of assuming a parametric.",
                    "label": 0
                },
                {
                    "sent": "Class or function.",
                    "label": 0
                },
                {
                    "sent": "We simply off distributions.",
                    "label": 0
                },
                {
                    "sent": "We simply say that since we don't know what our distribution is, will simply place our prior directly on the distribution itself.",
                    "label": 0
                },
                {
                    "sent": "So this prior is a distribution on distributions an of course.",
                    "label": 0
                },
                {
                    "sent": "This prior is that.",
                    "label": 0
                },
                {
                    "sent": "We will be using additional process for this.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The basic parametric approaches we saved our axis is distributed according to some distribution F which we don't know.",
                    "label": 0
                },
                {
                    "sent": "So being based in replace a prior on F itself.",
                    "label": 0
                },
                {
                    "sent": "So this being that there is a process prior.",
                    "label": 0
                },
                {
                    "sent": "And then again we do exactly the same thing in which we we can then compute the posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "Over distribution, right?",
                    "label": 0
                },
                {
                    "sent": "So this is kind of a weird concept.",
                    "label": 0
                },
                {
                    "sent": "And this posterior is again given by the prior multiplied by the likelihood, normalized by the marginal likelihood of the data.",
                    "label": 0
                },
                {
                    "sent": "And again, we can do prediction.",
                    "label": 0
                },
                {
                    "sent": "In this case, we simply want to calculate the probability of X star given our training data X and again we integrate out F here.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Of course.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Since F itself is the distribution we are interested in, right, so the probability of extra given F is simply.",
                    "label": 0
                },
                {
                    "sent": "Well, actually that's the balance.",
                    "label": 0
                },
                {
                    "sent": "Diego is simply the density of F at X to.",
                    "label": 0
                },
                {
                    "sent": "OK. Um, actually this is not quite correct.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we'll come back to this point later.",
                    "label": 0
                },
                {
                    "sent": "OK, but but that's the general idea, so the basic non permit approaches.",
                    "label": 0
                },
                {
                    "sent": "But to density estimation is we want to.",
                    "label": 1
                },
                {
                    "sent": "We don't know our density or we don't know the underlying distribution from which our data comes from.",
                    "label": 0
                },
                {
                    "sent": "We simply place a prior directly on the class of all possible distributions and then again we apply the Bayesian machinery.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Right, so that's that's the estimation.",
                    "label": 0
                },
                {
                    "sent": "Actually, is there any yes.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's kind of the tricky bit which is actually related to what I'm talking about.",
                    "label": 0
                },
                {
                    "sent": "Actually, the question I forgot to repeat or is this FA distribution or density?",
                    "label": 0
                },
                {
                    "sent": "Well, in this case, the way I'm writing this, it is a distribution.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "If the distribution is smooth, then it has a density and it's kind of equivalent, basically yes.",
                    "label": 1
                },
                {
                    "sent": "I'm not quite sure what the question is.",
                    "label": 0
                },
                {
                    "sent": "The question was that you are saying that that usually the distribution function is easy to estimate but not the density, but presumably that's true only in like one dimension or low dimensions.",
                    "label": 0
                },
                {
                    "sent": "That's probably not true.",
                    "label": 0
                },
                {
                    "sent": "For like high dimensional.",
                    "label": 0
                },
                {
                    "sent": "Cases.",
                    "label": 0
                },
                {
                    "sent": "In which you know being in machine learning, that's.",
                    "label": 0
                },
                {
                    "sent": "What what our interests are.",
                    "label": 0
                },
                {
                    "sent": "Um, I'll come through the.",
                    "label": 0
                },
                {
                    "sent": "Difference between density and distribution later?",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's that's the estimation, so here's a little.",
                    "label": 0
                },
                {
                    "sent": "A snapshot of what's what happens OK?",
                    "label": 0
                },
                {
                    "sent": "So in our prior, we may say that.",
                    "label": 0
                },
                {
                    "sent": "The rate so we can draw samples from our distribution from the prior and this things the little.",
                    "label": 0
                },
                {
                    "sent": "Each of these lines here plot actually the density of the.",
                    "label": 0
                },
                {
                    "sent": "Of the distributions we drew from a prior.",
                    "label": 0
                },
                {
                    "sent": "An you know they all kind of look so this might be 1.",
                    "label": 0
                },
                {
                    "sent": "Sample from the prior.",
                    "label": 0
                },
                {
                    "sent": "It looks like that and another sample is this magenta line which count goes down.",
                    "label": 0
                },
                {
                    "sent": "Is picked around this area and another one is this blue line which has a little bumps.",
                    "label": 0
                },
                {
                    "sent": "You can see the bumps here so.",
                    "label": 0
                },
                {
                    "sent": "And the Gray area is again kind of like the.",
                    "label": 0
                },
                {
                    "sent": "Leave 5th to 95th quantile of percentile of the.",
                    "label": 0
                },
                {
                    "sent": "Of the prior over the over the densities.",
                    "label": 0
                },
                {
                    "sent": "The red line here is the mean density.",
                    "label": 0
                },
                {
                    "sent": "And the blue is the median.",
                    "label": 0
                },
                {
                    "sent": "Over over the plastic.",
                    "label": 0
                },
                {
                    "sent": "So and then we observe some.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Data so this data are given by black points up here and once we've observed data, the posterior overdensities now has been has changed.",
                    "label": 0
                },
                {
                    "sent": "Where it's basically.",
                    "label": 0
                },
                {
                    "sent": "It is basically figure out that there's actually two clumps of data one over here and one over there, right?",
                    "label": 0
                },
                {
                    "sent": "So there's higher density here and higher density than lower density in the middle.",
                    "label": 0
                },
                {
                    "sent": "An the red light here is is the mean density in the posterior, while again the Gray area is not the region over which we have high posterior probability over densities.",
                    "label": 0
                },
                {
                    "sent": "So this is quite similar to the Gaussian process case.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so coming to the second application, which is parametric modeling.",
                    "label": 0
                },
                {
                    "sent": "So here the idea is that.",
                    "label": 0
                },
                {
                    "sent": "Really.",
                    "label": 0
                },
                {
                    "sent": "So sometimes what we care about is some parametric.",
                    "label": 1
                },
                {
                    "sent": "Function, so in this case it's linear regression.",
                    "label": 1
                },
                {
                    "sent": "We're interested really in the coefficients of our linear regression, but there are certain parts of the model which we're not really interested in.",
                    "label": 1
                },
                {
                    "sent": "An.",
                    "label": 0
                },
                {
                    "sent": "As a result, we want to be as flexible as as possible in terms of modeling them.",
                    "label": 0
                },
                {
                    "sent": "So here's a little example.",
                    "label": 0
                },
                {
                    "sent": "So let's say that I corresponds to a subjects or patients.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Each patient I have multiple trials indexed by J, so why IJ is the outcome of the Jade trial on the ice patient?",
                    "label": 1
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "We want to predict YIJ the outcome even predict this X IGN Zach IJ.",
                    "label": 1
                },
                {
                    "sent": "Exige could be things like you know the.",
                    "label": 0
                },
                {
                    "sent": "The the medical treatment which we give our patients.",
                    "label": 0
                },
                {
                    "sent": "And that idea could be things like the H health or the Heights or the.",
                    "label": 0
                },
                {
                    "sent": "Blood pressure of the patient.",
                    "label": 0
                },
                {
                    "sent": "Um and what we're interested in is weather.",
                    "label": 0
                },
                {
                    "sent": "Whether the.",
                    "label": 0
                },
                {
                    "sent": "Whether the treatment which we gave the patient is effective or not, so given X, Zack and Y we want to.",
                    "label": 0
                },
                {
                    "sent": "Infer what's the value of beta or what's the posterior over beta, and if the posterior is positive, then we know that our treatment is somehow effective.",
                    "label": 0
                },
                {
                    "sent": "And that's the power model which we care about.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 1
                },
                {
                    "sent": "But there's also other parts of the model which has.",
                    "label": 0
                },
                {
                    "sent": "For example, this part is.",
                    "label": 0
                },
                {
                    "sent": "Count the effects.",
                    "label": 0
                },
                {
                    "sent": "That effects on the.",
                    "label": 0
                },
                {
                    "sent": "On the efficient on the effectiveness of the treatment, which depends on things like the general health of the patient.",
                    "label": 0
                },
                {
                    "sent": "And of course this part.",
                    "label": 0
                },
                {
                    "sent": "We don't really care as much about what we are, what we want to know is, is our new medicine, which is the new treatment that we are giving this patient effective or not?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Since we don't, since we don't really care about the patient specific effects or kind of the noise.",
                    "label": 0
                },
                {
                    "sent": "Or the noise?",
                    "label": 0
                },
                {
                    "sent": "That on the on the outcome of the fitment.",
                    "label": 0
                },
                {
                    "sent": "OK, so we instead of assuming like Gaussian prior over the noise or over the random effects BI, we simply say that the we should model the noise in the nonparametric fashion.",
                    "label": 0
                },
                {
                    "sent": "So, and the way we model it is.",
                    "label": 0
                },
                {
                    "sent": "Again, we say that noise.",
                    "label": 0
                },
                {
                    "sent": "Terms are drawn from some noise distribution F. And since we don't know what F is, we say that F should be drawn from some directly process.",
                    "label": 0
                },
                {
                    "sent": "OK, so we model the noise.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "Distribution in a nonparametric fashion and this is able to handle things like Overdispersion, Orsk illness, which is often observed in clinical trials.",
                    "label": 0
                },
                {
                    "sent": "It's also possible to model subjects specific random effects.",
                    "label": 1
                },
                {
                    "sent": "Nonparametric, nonparametric nonparametrically.",
                    "label": 0
                },
                {
                    "sent": "By saying that the eyes are drawn from some distribution G and we don't know G. So again we place the prior over G AED richly process priority.",
                    "label": 0
                },
                {
                    "sent": "So this is called semi parametric because.",
                    "label": 0
                },
                {
                    "sent": "These two parts of the model nonparametric.",
                    "label": 0
                },
                {
                    "sent": "But what we care about is that part, which is parametric, so it's kind of half half.",
                    "label": 0
                },
                {
                    "sent": "So we simply use the nonparametric model as a way of.",
                    "label": 0
                },
                {
                    "sent": "Up bring inflexibility towards our model but we kept the parametric part because that's more interpretable.",
                    "label": 0
                },
                {
                    "sent": "Yes, the inferences we make are more more interpretable.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in machine learning, the rich data processes are used mostly in model selection or averaging, so I'll kind of go through that again a little bit.",
                    "label": 0
                },
                {
                    "sent": "So here our data.",
                    "label": 0
                },
                {
                    "sent": "Consist of sequence XX1X2 and so forth.",
                    "label": 0
                },
                {
                    "sent": "And our model we have a prior.",
                    "label": 0
                },
                {
                    "sent": "So for model MK we have parameters data K and this is our prior over parameters in the model plus K. And we also have a likelihood term, which is the probability of the data given our parameters and model plus K. An for model selection.",
                    "label": 0
                },
                {
                    "sent": "So if we have a sequence of.",
                    "label": 0
                },
                {
                    "sent": "The M1 and M2 and so forth.",
                    "label": 0
                },
                {
                    "sent": "And we want to figure out which one is the best OK so.",
                    "label": 0
                },
                {
                    "sent": "To do that, we should.",
                    "label": 0
                },
                {
                    "sent": "Calculate the marginal likelihood of the data X given our model.",
                    "label": 1
                },
                {
                    "sent": "Integrating out the parameters within the model.",
                    "label": 0
                },
                {
                    "sent": "And then we can do model selection by saying that we should choose the model which maximizes the marginal likelihood of the data.",
                    "label": 1
                },
                {
                    "sent": "If we're being more base and then of course we don't want to do model selection, we want to do model averaging.",
                    "label": 0
                },
                {
                    "sent": "In this case we could.",
                    "label": 0
                },
                {
                    "sent": "In the predictive case, we could say that the probability of some new data test point X star given training.",
                    "label": 0
                },
                {
                    "sent": "Set X.",
                    "label": 0
                },
                {
                    "sent": "Is sum over all possible model classes?",
                    "label": 0
                },
                {
                    "sent": "Of the probability of extra given this model class.",
                    "label": 0
                },
                {
                    "sent": "Actually there's a. X missing in there sorry.",
                    "label": 0
                },
                {
                    "sent": "And we average the likelihood of extra under model plus K. Over the posterior over model classes.",
                    "label": 0
                },
                {
                    "sent": "By the post.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Weather posteriors given by this thing.",
                    "label": 0
                },
                {
                    "sent": "Prior times likelihood normalized.",
                    "label": 0
                },
                {
                    "sent": "But the problem with this.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the problem is this is the major question to ask is.",
                    "label": 1
                },
                {
                    "sent": "Is this marginal likelihood OK?",
                    "label": 1
                },
                {
                    "sent": "This term here?",
                    "label": 0
                },
                {
                    "sent": "Which is the most important term in terms of evaluating?",
                    "label": 1
                },
                {
                    "sent": "Both model selection and model averaging.",
                    "label": 0
                },
                {
                    "sent": "Can we actually compute this marginal likelihood?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It turns out that.",
                    "label": 0
                },
                {
                    "sent": "The marginal is extremely hard to compute, or virtually all models which we're interested in.",
                    "label": 0
                },
                {
                    "sent": "So so that.",
                    "label": 0
                },
                {
                    "sent": "It makes it hard to do model selection and model averaging, right?",
                    "label": 0
                },
                {
                    "sent": "And of course.",
                    "label": 0
                },
                {
                    "sent": "Both model selection and model averaging are very important in terms of preventing overfitting, and underfitting of our models.",
                    "label": 0
                },
                {
                    "sent": "We want to find a model of the right complexity to fit to our data.",
                    "label": 0
                },
                {
                    "sent": "But check the idea is that if we actually have reasonable and proper Bayesian methods.",
                    "label": 1
                },
                {
                    "sent": "OK, well basically we put reasonable prior over parameters and we integrate our all possible parameters.",
                    "label": 0
                },
                {
                    "sent": "Then the model.",
                    "label": 0
                },
                {
                    "sent": "The method should not be overfitting to our data.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So what does that mean?",
                    "label": 0
                },
                {
                    "sent": "So if a model doesn't overfit the data, then we should simply use as large model as possible.",
                    "label": 0
                },
                {
                    "sent": "So we use a really large model, say Infinity OK. And then somehow let the data speak for itself.",
                    "label": 1
                },
                {
                    "sent": "OK, so I'll come.",
                    "label": 0
                },
                {
                    "sent": "I'll give you an example of what happens here.",
                    "label": 0
                },
                {
                    "sent": "In the case of a in the case of a mixture model.",
                    "label": 0
                },
                {
                    "sent": "So when you're trying to model data with a mixture model, you want to figure out the number of components in there.",
                    "label": 0
                },
                {
                    "sent": "OK, you could actually take M Infinity, so MK would be a finite mixture model with K components.",
                    "label": 0
                },
                {
                    "sent": "Ann we the.",
                    "label": 0
                },
                {
                    "sent": "The approach which I've kept refering to here involving an Infinity is a mixture model with an infinite number of mixture components.",
                    "label": 0
                },
                {
                    "sent": "And that turns out to be a direct process mixture model.",
                    "label": 0
                },
                {
                    "sent": "Hit.",
                    "label": 0
                },
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "Give you some eggs.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Both of.",
                    "label": 0
                },
                {
                    "sent": "Cases of model selection and averaging, which yes.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Right, I see.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Sorry was the last word.",
                    "label": 0
                },
                {
                    "sent": "Oh OK, so the question was.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of two ways in which you could get at a really large model and Infinity so.",
                    "label": 1
                },
                {
                    "sent": "The first way is you take.",
                    "label": 0
                },
                {
                    "sent": "A mixture over your mixture models where the mixture is over.",
                    "label": 0
                },
                {
                    "sent": "The number of components within that mixture model, so you can take mixture model involving one component involving two component involving three component and then you can.",
                    "label": 0
                },
                {
                    "sent": "Average over all possible.",
                    "label": 0
                },
                {
                    "sent": "Ah.",
                    "label": 0
                },
                {
                    "sent": "Models, and that's of course.",
                    "label": 0
                },
                {
                    "sent": "That gives you a infinite mission model, but it's actually different.",
                    "label": 0
                },
                {
                    "sent": "So what we're saying here is that we actually have.",
                    "label": 0
                },
                {
                    "sent": "A mixture model in which the mixture just one mixture model in which the mixture number of mixture components is infinite rather than an infinite number of finite mixture models, and they are somewhat different, yes.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The question was you have a. Um?",
                    "label": 0
                },
                {
                    "sent": "OK, right?",
                    "label": 0
                },
                {
                    "sent": "Which I used to OK, so the question was, is it kind of like you have a mixture model in which you have mixture components everywhere in your space?",
                    "label": 0
                },
                {
                    "sent": "Right, possibly everywhere in your space, and then there's, of course, if your space is really big, then you need an infinite number of major components and then given data you would the model.",
                    "label": 0
                },
                {
                    "sent": "When you do posterior inference, you would.",
                    "label": 0
                },
                {
                    "sent": "Automatically.",
                    "label": 1
                },
                {
                    "sent": "Select out the mixture components which are kind of useful to explain the data and that is the correct way of viewing.",
                    "label": 0
                },
                {
                    "sent": "Listing question.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "Not what I mean.",
                    "label": 0
                },
                {
                    "sent": "You could always play some.",
                    "label": 0
                },
                {
                    "sent": "Yes, but you could always place the prior distribution on the mean, which is not a uniform distribution.",
                    "label": 0
                },
                {
                    "sent": "Oh, you're using a uniform product.",
                    "label": 0
                },
                {
                    "sent": "OK. Fuck.",
                    "label": 0
                },
                {
                    "sent": "Dirt.",
                    "label": 0
                },
                {
                    "sent": "Wonder why?",
                    "label": 0
                },
                {
                    "sent": "My God, my God.",
                    "label": 0
                },
                {
                    "sent": "So the question is, why is the Gaussian prior not appropriate for this sort of model?",
                    "label": 0
                },
                {
                    "sent": "In fact, there are people who look into density estimation where the prior is where there's a Gaussian process in the prior.",
                    "label": 0
                },
                {
                    "sent": "But you need to make sure that your densities are integrable, Ann.",
                    "label": 0
                },
                {
                    "sent": "Discount other to make to design your kernel of your Gaussian process so that you know the density.",
                    "label": 0
                },
                {
                    "sent": "Is actually integrable.",
                    "label": 0
                },
                {
                    "sent": "You kind of have a uniform distribution over things.",
                    "label": 0
                },
                {
                    "sent": "An yeah that's global, but it turns out to be computationally a lot harder.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I should get up here.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the 1st.",
                    "label": 0
                },
                {
                    "sent": "The first model selection averaging thing is a clustering right.",
                    "label": 0
                },
                {
                    "sent": "So in the case of a mixture model, if our data looks like that, then the question is does it consists of three clusters or maybe one or maybe 4 right?",
                    "label": 0
                },
                {
                    "sent": "We need to figure out how many clusters they are, so that's the model selection or averaging.",
                    "label": 1
                },
                {
                    "sent": "Questions to ask in this case?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This solved.",
                    "label": 0
                },
                {
                    "sent": "This of problem occurs all over the place, so another case in which it occurs is in spike sorting.",
                    "label": 0
                },
                {
                    "sent": "So the idea is.",
                    "label": 0
                },
                {
                    "sent": "That that you've plucked you put some electrode into the brain of a monkey or or a mouse.",
                    "label": 0
                },
                {
                    "sent": "And let's say that you have four electrodes and these are the signals which are coming from the four electrodes, so.",
                    "label": 0
                },
                {
                    "sent": "That's kind of 1 observation, and that's another observation, and each line here corresponds to one electrode.",
                    "label": 0
                },
                {
                    "sent": "And if you notice the first tree.",
                    "label": 0
                },
                {
                    "sent": "Spikes right so this account, the little spikes that you see.",
                    "label": 0
                },
                {
                    "sent": "Kind of has similar shape, right?",
                    "label": 0
                },
                {
                    "sent": "And the idea is if they have similar shape then probably the spikes came from the same neuron.",
                    "label": 0
                },
                {
                    "sent": "An if you plug in an electrode into the brain of a mouse, then you actually receive spikes from multiple neurons and you need to figure out which spike belongs to which neuron.",
                    "label": 0
                },
                {
                    "sent": "In this case, maybe you might think that the first tree spikes belong to one neuron, and then maybe the next 4 belong to a.",
                    "label": 0
                },
                {
                    "sent": "To the second neuron and then the next tree belongs to 1/3 neuron.",
                    "label": 0
                },
                {
                    "sent": "So again the question here is how many neurons are there in the vicinity of your electrode that you can receive spikes mode right?",
                    "label": 1
                },
                {
                    "sent": "So this is basically like clustering, except that the data is much more high dimensional.",
                    "label": 0
                },
                {
                    "sent": "So in fact, that's what people do.",
                    "label": 0
                },
                {
                    "sent": "People just take.",
                    "label": 0
                },
                {
                    "sent": "The high dimensional data and they just cluster them and then each cluster they say belongs to correspond to the spikes corresponding to one.",
                    "label": 0
                },
                {
                    "sent": "One unique neuron.",
                    "label": 0
                },
                {
                    "sent": "Another example is.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Like modeling, so here the idea is that you are given a set of documents.",
                    "label": 0
                },
                {
                    "sent": "Ann you.",
                    "label": 0
                },
                {
                    "sent": "Decompose the.",
                    "label": 0
                },
                {
                    "sent": "Each document into a mixture over topics.",
                    "label": 0
                },
                {
                    "sent": "So you describe each document as a mixture of the topics an.",
                    "label": 0
                },
                {
                    "sent": "And then there would be a set of topics which describes all documents within your corpus, so.",
                    "label": 0
                },
                {
                    "sent": "Again, the question there is, how many topics are exhibited within a particular corpus of documents.",
                    "label": 1
                },
                {
                    "sent": "And again we need to answer that question and we need to do model selection or averaging.",
                    "label": 0
                },
                {
                    "sent": "Is it working?",
                    "label": 0
                },
                {
                    "sent": "I like this OK. Bye.",
                    "label": 0
                },
                {
                    "sent": "OK, so another example is in grammar induction.",
                    "label": 0
                },
                {
                    "sent": "So here the idea is you want to learn a context probabilistic context.",
                    "label": 0
                },
                {
                    "sent": "Free grammar for language they say English, so there's two parts.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Questions you can.",
                    "label": 0
                },
                {
                    "sent": "Ask here, the first one is even lots and lots of sentences of English from English.",
                    "label": 0
                },
                {
                    "sent": "Can you?",
                    "label": 0
                },
                {
                    "sent": "Figure out a grammar for English and at the same time parse each sentence in.",
                    "label": 0
                },
                {
                    "sent": "In the grammar.",
                    "label": 0
                },
                {
                    "sent": "Up to people familiar with past reason, context free grammar and stuff should be.",
                    "label": 0
                },
                {
                    "sent": "OK, I guess I. I don't think I'll have time to go into what the context free grammars, but basically they look like this.",
                    "label": 0
                },
                {
                    "sent": "So in this case.",
                    "label": 0
                },
                {
                    "sent": "The sentence is she heard the noise and what you want to do is to parse the sentence in terms of so.",
                    "label": 0
                },
                {
                    "sent": "In this case, you have a sentence and that there might be a noun phrase here.",
                    "label": 0
                },
                {
                    "sent": "There's a verb phrase, and the verb phrases heard the noise.",
                    "label": 0
                },
                {
                    "sent": "And then the phrases in turn parsed into two components.",
                    "label": 0
                },
                {
                    "sent": "I think this VBD is a verb, but a particular form of verb, and I'm not a natural language processing expert, so I don't know what it stands for.",
                    "label": 0
                },
                {
                    "sent": "And then this is again a noun phrase.",
                    "label": 0
                },
                {
                    "sent": "So once you pass the sentence you have, you could kind of say that you have somehow understood the sentences to some extent.",
                    "label": 0
                },
                {
                    "sent": "So the question now is, given lots of English sentences, can you learn a grammar for English and parse the sentences in terms of the grammar?",
                    "label": 0
                },
                {
                    "sent": "The second question you could ask this.",
                    "label": 0
                },
                {
                    "sent": "Let's say that we have a grammar, so this is our past three in the grammar.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Maybe you might think that OK, there's actually multiple types of verb phrases that goes.",
                    "label": 0
                },
                {
                    "sent": "Along with multiple types of noun phrases, so, but of course the grammar you're given you only told is it a verb phrase or noun phrase, and what you want to learn is how many types of verb phrases are there.",
                    "label": 0
                },
                {
                    "sent": "How many types of noun phrases are there?",
                    "label": 1
                },
                {
                    "sent": "And you can learn all this again from sentences of English.",
                    "label": 0
                },
                {
                    "sent": "So again, this is a model selection question because.",
                    "label": 0
                },
                {
                    "sent": "You know you don't really know how many types of noun phrases are there to begin with, you need to figure that out right.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So another example is from Visual Studio analysis, in which given things like pictures you want to decompose the scene into objects and object into parts and parts into features.",
                    "label": 0
                },
                {
                    "sent": "Again, you you don't really know how many objects are there, how many object types are there?",
                    "label": 1
                },
                {
                    "sent": "How many parts are there in within each object, and so forth.",
                    "label": 0
                },
                {
                    "sent": "Ann, this of questions is again model selection questions because.",
                    "label": 0
                },
                {
                    "sent": "If you.",
                    "label": 0
                },
                {
                    "sent": "Have a really rich model and you.",
                    "label": 0
                },
                {
                    "sent": "Don't do the Bayesian thing and you just maximize likelihood.",
                    "label": 0
                },
                {
                    "sent": "Then of course it would say that.",
                    "label": 0
                },
                {
                    "sent": "You know you have.",
                    "label": 0
                },
                {
                    "sent": "You have many, many different objects, and each object corresponds to.",
                    "label": 0
                },
                {
                    "sent": "Maybe a whole image or something like that?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Those are kind of the three applications of richly processes and turns out that they process are pretty reasonable.",
                    "label": 0
                },
                {
                    "sent": "Ways to solve all three problem?",
                    "label": 0
                },
                {
                    "sent": "So it's almost 12, so I kind of better be quick in terms of definition of addition.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Process, so let's start off with a finite mixture model.",
                    "label": 1
                },
                {
                    "sent": "Are people familiar with mixture models?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "OK so I can go through this pretty quickly.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, so here's the definition of a finite mixture model.",
                    "label": 0
                },
                {
                    "sent": "We set our data points XI right?",
                    "label": 0
                },
                {
                    "sent": "For each data point XI, we first.",
                    "label": 0
                },
                {
                    "sent": "Draw that I wish corresponds to which mixture component XI came from an that I the cluster indicated variable like I is drawn from some discrete distribution.",
                    "label": 0
                },
                {
                    "sent": "Pot given by pie.",
                    "label": 0
                },
                {
                    "sent": "So this is high is the mixing proportions.",
                    "label": 0
                },
                {
                    "sent": "Being Bayesian also and then given like I.",
                    "label": 0
                },
                {
                    "sent": "Our data X is drawn from some distribution parameterized by by AVS.",
                    "label": 0
                },
                {
                    "sent": "I OK, so that's kind of it.",
                    "label": 0
                },
                {
                    "sent": "Being Bayesian would face place priors over both high the mixing proportions as well as a cluster parameters.",
                    "label": 0
                },
                {
                    "sent": "Spiky so that's a graphical model.",
                    "label": 0
                },
                {
                    "sent": "So the model selection.",
                    "label": 0
                },
                {
                    "sent": "Or averaging problem here is.",
                    "label": 0
                },
                {
                    "sent": "We want to figure out what the hyperparameters in H. What are the directional parameters Alpha, and most importantly, what's the number of components in our finite mixture model.",
                    "label": 1
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Since.",
                    "label": 0
                },
                {
                    "sent": "If we take the Bayesian approach, we're not worried about overfitting, right?",
                    "label": 0
                },
                {
                    "sent": "So then we can imagine taking K the number of clusters in our mixture model to be really large.",
                    "label": 1
                },
                {
                    "sent": "So notice that if we can integrate out both the parameters by Ann mixing proportions high.",
                    "label": 0
                },
                {
                    "sent": "Then we integrate those two and those are the only latent variables left in our model, right?",
                    "label": 0
                },
                {
                    "sent": "And the number of latent variables here is just N, where N is the number of data points.",
                    "label": 0
                },
                {
                    "sent": "So if we take K to be really large.",
                    "label": 0
                },
                {
                    "sent": "The number of latent variables that we have does not actually grow in terms of K. Once we've integrated out the cluster component.",
                    "label": 1
                },
                {
                    "sent": "The mixture component parameter.",
                    "label": 0
                },
                {
                    "sent": "So what this means is that that would not be overfitting in this model.",
                    "label": 1
                },
                {
                    "sent": "Notice that.",
                    "label": 0
                },
                {
                    "sent": "There can be at most N components which are active in the sense that they are associated with data, and of course the reason for that is you have any data points each being associated with one cluster, right?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "But it turns out that usually the number of such active components is much less than N, even the, even though the total number of mixture components K is could be infinite.",
                    "label": 1
                },
                {
                    "sent": "So this actually gives us an infinite mixture model, so I'll kind of give you a little demo to show that this thing actually works.",
                    "label": 0
                },
                {
                    "sent": "I need to do something.",
                    "label": 0
                },
                {
                    "sent": "Wait?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that would be.",
                    "label": 0
                },
                {
                    "sent": "I'm good.",
                    "label": 0
                },
                {
                    "sent": "Ah.",
                    "label": 0
                },
                {
                    "sent": "Thing.",
                    "label": 0
                },
                {
                    "sent": "Well, I just looked now.",
                    "label": 0
                },
                {
                    "sent": "OK so this is our.",
                    "label": 0
                },
                {
                    "sent": "That points to our data points, and we're sampling from the posterior of our richly infinite mixture model.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "The... here corresponds to the mixture components which are actually active or associated with data.",
                    "label": 0
                },
                {
                    "sent": "An.",
                    "label": 0
                },
                {
                    "sent": "In fact, there's actually an infinite number of mixture components within our model, but most of the time we only see a small number of major components which are actually associated with data.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's kind of two issues which we would like to address here.",
                    "label": 1
                },
                {
                    "sent": "At.",
                    "label": 0
                },
                {
                    "sent": "The first issue is can we take this infinite limit.",
                    "label": 1
                },
                {
                    "sent": "This limit where K goes to Infinity and the second issue is what is the corresponding limiting model.",
                    "label": 0
                },
                {
                    "sent": "So what we'd like to do is instead of starting with a finite model and then taking the Internet limit, we actually want to define our.",
                    "label": 0
                },
                {
                    "sent": "Infinite model directly and then from there we can see the relationship to the finite model.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Running out of time so.",
                    "label": 0
                },
                {
                    "sent": "I guess everybody here is familiar with Gaussian processes by now, so it defines a distribution over functions.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Prior over functions, is a Gaussian process.",
                    "label": 1
                },
                {
                    "sent": "If for any finite set of input points X one to XN.",
                    "label": 1
                },
                {
                    "sent": "F evaluated at X1 until F evaluated at X and this thing is a multivariate Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So this defines a Gaussian process.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So Gaussian process has.",
                    "label": 0
                },
                {
                    "sent": "I mean function and covariance function.",
                    "label": 1
                },
                {
                    "sent": "An important property of this marginal distributions is that they are consistent.",
                    "label": 1
                },
                {
                    "sent": "So what meant by consistent here was meant is that if you take.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "This multivariate Gaussian and you integrate out F of XN.",
                    "label": 0
                },
                {
                    "sent": "Then we're just left with a distribution over F of X12 F of X N -- 1, and what?",
                    "label": 0
                },
                {
                    "sent": "Is important here.",
                    "label": 0
                },
                {
                    "sent": "Is that that distribution is again given by a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "It was exactly the same form, except that.",
                    "label": 0
                },
                {
                    "sent": "Is only vengeance from X1 to XN minus one.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Right so I.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We've seen a sequence of input points this.",
                    "label": 1
                },
                {
                    "sent": "Visualizing about so, you could visualize a Gaussian process by saying that we first draw F of X1 and then we can draw F of X2 given X1 and X3 given X1 and X2 and so forth.",
                    "label": 0
                },
                {
                    "sent": "Then I'll skip the demo because I'm already running out of time.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So we can actually do this because each of the conditional distributions is is Gaussian.",
                    "label": 1
                },
                {
                    "sent": "Since F of X12, F of X is Gaussian.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So moving onto a direction they process now.",
                    "label": 0
                },
                {
                    "sent": "So just as a Gaussian process has Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Distributed marginal distributions directly.",
                    "label": 0
                },
                {
                    "sent": "Process has richly distributed marginal distribution, so are people familiar with the rich distributions here.",
                    "label": 0
                },
                {
                    "sent": "Good great.",
                    "label": 0
                },
                {
                    "sent": "OK so I can speed up again so I guess.",
                    "label": 0
                },
                {
                    "sent": "OK, OK. Um?",
                    "label": 0
                },
                {
                    "sent": "Richland Distribution is distribution over the K dimensional probability simplex an the K dimensional probability simplex is simply a set of vectors high one 2\u03c0 K. Such that each entry is positive and they sum to one.",
                    "label": 0
                },
                {
                    "sent": "So you can almost think of this as a distribution itself, right?",
                    "label": 1
                },
                {
                    "sent": "So this is, think of how I want high K as a distribution as a discrete distribution.",
                    "label": 0
                },
                {
                    "sent": "Over K outcomes right Pi K being the probability of choosing.",
                    "label": 0
                },
                {
                    "sent": "Outcome K. We say that the original distribution is the distribution over the K dimensional probability simplex, and we say that I want high K is richly distributed.",
                    "label": 1
                },
                {
                    "sent": "If it is a random vector where.",
                    "label": 0
                },
                {
                    "sent": "The densities look like has this form.",
                    "label": 0
                },
                {
                    "sent": "So that's the density of operational distribution.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So I'll just show you that stuff quite a lot of nice properties of rich data distributions, but I'll just show you.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can plots of how the density looks like.",
                    "label": 0
                },
                {
                    "sent": "So what I forgot to say?",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the rich they have parameters A1 and 2A, K and this parameters here has to be positive, but they need not sum to one, they just have to be positive.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The alphas are all one we that richly distribution with all with the parameters of 1.",
                    "label": 0
                },
                {
                    "sent": "Simply give you a uniform distribution over the probability simplex.",
                    "label": 0
                },
                {
                    "sent": "So this is the case.",
                    "label": 0
                },
                {
                    "sent": "Where K equals the tree.",
                    "label": 0
                },
                {
                    "sent": "And you have a probability simplex.",
                    "label": 0
                },
                {
                    "sent": "Each vertex here corresponds to one outcome.",
                    "label": 0
                },
                {
                    "sent": "And if you have a pie.",
                    "label": 0
                },
                {
                    "sent": "So each each point within this triangle corresponds to a.",
                    "label": 0
                },
                {
                    "sent": "A pipe vector and this high vector would have value which is large.",
                    "label": 0
                },
                {
                    "sent": "So if this is Vertex 1, two and three, then apply vector here would mean that Taiwan is pretty big, but Pi 2 and \u03c0 three are pretty small.",
                    "label": 0
                },
                {
                    "sent": "When the parameters of the directly go above 1.",
                    "label": 0
                },
                {
                    "sent": "Then we kind of get a little bump in the center.",
                    "label": 0
                },
                {
                    "sent": "And the larger the so this is, this is a richly with parameters of two.",
                    "label": 0
                },
                {
                    "sent": "This is a directly with the parameter of five and you can see that the bump gets more concentrated around.",
                    "label": 0
                },
                {
                    "sent": "Around the center.",
                    "label": 0
                },
                {
                    "sent": "So the size of the parameter corresponds to how concentrated is the.",
                    "label": 0
                },
                {
                    "sent": "Is the distribution.",
                    "label": 0
                },
                {
                    "sent": "When you have parameters, in this case it's 5, five and two, then the bump have shifts around this simplex.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And if the parameter is less than one.",
                    "label": 0
                },
                {
                    "sent": "It turns out that instead of kind of concentrating around a little bump, all the mathcounts spreads out and they concentrate around the vertices.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So that's what that original distributions look like.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "2 interesting properties of richly distributions which is useful in terms of directionally process.",
                    "label": 0
                },
                {
                    "sent": "The first one is what I call an elementary.",
                    "label": 0
                },
                {
                    "sent": "Property of the original distributions.",
                    "label": 1
                },
                {
                    "sent": "Here, let's say that I want you Paikea is drawn from a dish day with premise A1 to Alpha K. And you take 1 high too and you add them together.",
                    "label": 0
                },
                {
                    "sent": "Then this vector.",
                    "label": 0
                },
                {
                    "sent": "Which belongs to the K -- 1 dimensional probability simplex.",
                    "label": 0
                },
                {
                    "sent": "This thing is richly is also directly distributed.",
                    "label": 0
                },
                {
                    "sent": "But with the parameters.",
                    "label": 0
                },
                {
                    "sent": "A1 and A2 added together.",
                    "label": 0
                },
                {
                    "sent": "And generally, if we have a. I want to IJ if this thing is the partition of the Special K is the partition of 1 until K. Then we some.",
                    "label": 1
                },
                {
                    "sent": "It each subset here L I-1 the sum of all the entries of Pi within that subset and so forth.",
                    "label": 0
                },
                {
                    "sent": "This thing belongs to the.",
                    "label": 0
                },
                {
                    "sent": "J dimensional probability simplex.",
                    "label": 0
                },
                {
                    "sent": "And this thing turns out to be also richly distributed with the corresponding parameters summed up together.",
                    "label": 0
                },
                {
                    "sent": "So this is an important property of attrition rate distributions, which we'll be using later.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the conference of the Agglomerative property is what?",
                    "label": 1
                },
                {
                    "sent": "Is decimated property.",
                    "label": 0
                },
                {
                    "sent": "So in the case of the agglomerative property you take 2 components of our.",
                    "label": 0
                },
                {
                    "sent": "Of our vector \u03c0, and we added them together, right?",
                    "label": 0
                },
                {
                    "sent": "So in the case of decimated, we take one component of this vector and we split into two and the way we split into 2.",
                    "label": 0
                },
                {
                    "sent": "Is as follows, so let's again we assume that I want a \u03c0 case richly distributed with this parameter.",
                    "label": 0
                },
                {
                    "sent": "In dependently let's draw.",
                    "label": 0
                },
                {
                    "sent": "How one how to from Additionally from a 2 dimensional digital distribution?",
                    "label": 0
                },
                {
                    "sent": "With parameters given by A1, beta one and A1 beta two and here is important that beta one and beta 2 sum to one.",
                    "label": 0
                },
                {
                    "sent": "Then we take high one and we split it into pie.",
                    "label": 0
                },
                {
                    "sent": "One power, one anti, one come to we know that this thing comes to one.",
                    "label": 0
                },
                {
                    "sent": "So this thing also has to sum to one.",
                    "label": 0
                },
                {
                    "sent": "Right, so this thing is also directly if both of these are drawn independently and it's richly where this parameter A1 is split into two parameters.",
                    "label": 0
                },
                {
                    "sent": "In exactly the same fashion here.",
                    "label": 0
                },
                {
                    "sent": "So that's the estimated property of authority distribution.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now the idea is the following.",
                    "label": 0
                },
                {
                    "sent": "So richly process is basically an infinitely estimated duration distribution.",
                    "label": 0
                },
                {
                    "sent": "So we start off with.",
                    "label": 0
                },
                {
                    "sent": "One is Additionally of Alpha, so this is.",
                    "label": 0
                },
                {
                    "sent": "A1 dimensional directly distribution is simply a pointer one.",
                    "label": 0
                },
                {
                    "sent": "We take this thing and we split into Pi one and Pi 2.",
                    "label": 0
                },
                {
                    "sent": "By and the way we do it is basically we draw.",
                    "label": 0
                },
                {
                    "sent": "In dependently withdraw how one and two and that gives us Taiwan?",
                    "label": 0
                },
                {
                    "sent": "And Pi 2\u03c0 one is 1 times tell one Pi 2 is 1 * 2.",
                    "label": 0
                },
                {
                    "sent": "And because of the estimated, we know that we can split Alpha here into Alpha, develop into an Alpha divided by two.",
                    "label": 0
                },
                {
                    "sent": "Then we can take Taiwan and tie two.",
                    "label": 0
                },
                {
                    "sent": "We slipped by one and two PI11 and PI12.",
                    "label": 0
                },
                {
                    "sent": "So that this.",
                    "label": 0
                },
                {
                    "sent": "Who sings some 2\u03c0 one and we split high 2 into \u03c0 two one and two 2 so that these two entries, some 2\u03c0 two?",
                    "label": 0
                },
                {
                    "sent": "And again, this thing would still be directly distributed with those parameters, and we basically do this infinitely often.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And let's see how the how does this look like.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Yeah I have a little rectangle here.",
                    "label": 0
                },
                {
                    "sent": "And the area of this rectangle corresponds to the.",
                    "label": 0
                },
                {
                    "sent": "The value of one of the pies.",
                    "label": 0
                },
                {
                    "sent": "OK, so in this case.",
                    "label": 0
                },
                {
                    "sent": "We have one because basically 10 * 0.1 is 1, so that's the total probability mass which we're going to assign to the whole space.",
                    "label": 0
                },
                {
                    "sent": "We take this thing I was split into 2.",
                    "label": 0
                },
                {
                    "sent": "And that's high one.",
                    "label": 0
                },
                {
                    "sent": "And that's by two.",
                    "label": 0
                },
                {
                    "sent": "So the area within this rectangle is \u03c0.",
                    "label": 0
                },
                {
                    "sent": "One area within that rectangle, Pi 2.",
                    "label": 0
                },
                {
                    "sent": "OK, again we take this rectangle with split into PI11 and PI12.",
                    "label": 0
                },
                {
                    "sent": "And we take this thing and we spin it into high 21 and Pi 2 two right?",
                    "label": 0
                },
                {
                    "sent": "So if we repeat this an we may get something which looks like that.",
                    "label": 0
                },
                {
                    "sent": "And if we actually do this often, like.",
                    "label": 0
                },
                {
                    "sent": "Lots of times.",
                    "label": 0
                },
                {
                    "sent": "Here's what we get.",
                    "label": 0
                },
                {
                    "sent": "We keep on splitting each rectangle into two parts.",
                    "label": 0
                },
                {
                    "sent": "And we keep on splitting it and splitting it OK. And that's basically a draw from our drizly process.",
                    "label": 0
                },
                {
                    "sent": "Notice that it almost looks like a bunch of.",
                    "label": 0
                },
                {
                    "sent": "Points methods.",
                    "label": 0
                },
                {
                    "sent": "OK, and in fact.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So what this means is that a draw for moderation they process.",
                    "label": 0
                },
                {
                    "sent": "Is basically a discrete distribution.",
                    "label": 0
                },
                {
                    "sent": "And that's an important property of reprocessing.",
                    "label": 0
                },
                {
                    "sent": "Again.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Properly, what is the derivative process?",
                    "label": 0
                },
                {
                    "sent": "They reach their processes basically.",
                    "label": 0
                },
                {
                    "sent": "A distribution over probability measures.",
                    "label": 1
                },
                {
                    "sent": "What probability measures we can think of probability measures as simply?",
                    "label": 0
                },
                {
                    "sent": "Functions as well.",
                    "label": 0
                },
                {
                    "sent": "So it's a probability measure is a function from subsets of some SpaceX 201 which satisfies certain properties, which makes them probability measures.",
                    "label": 1
                },
                {
                    "sent": "So these are properties which things like.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "G is the probability measure if G evaluated on the whole SpaceX.",
                    "label": 0
                },
                {
                    "sent": "Has to be one.",
                    "label": 0
                },
                {
                    "sent": "G evaluated on the empty set.",
                    "label": 0
                },
                {
                    "sent": "Has to be 0.",
                    "label": 0
                },
                {
                    "sent": "And if you have a sequence of subsets.",
                    "label": 0
                },
                {
                    "sent": "Page evaluated at.",
                    "label": 0
                },
                {
                    "sent": "A1.",
                    "label": 0
                },
                {
                    "sent": "Disjoint.",
                    "label": 0
                },
                {
                    "sent": "82 AN.",
                    "label": 0
                },
                {
                    "sent": "This thing is simply some.",
                    "label": 0
                },
                {
                    "sent": "From one to end of G of AI.",
                    "label": 0
                },
                {
                    "sent": "And this subsets has to be what's called measurable subsets, but we don't have to worry about that very much really.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So basically that defines a probability measure is simply a function from subsets of some space 201 OK, so remember the Gaussian processes a prior over functions.",
                    "label": 0
                },
                {
                    "sent": "Additional processes also apply over functions, except that these are special functions.",
                    "label": 0
                },
                {
                    "sent": "These are probability measures.",
                    "label": 0
                },
                {
                    "sent": "An the definition of Additionally process is as follows.",
                    "label": 0
                },
                {
                    "sent": "Basically.",
                    "label": 0
                },
                {
                    "sent": "G is richly processed distributed.",
                    "label": 1
                },
                {
                    "sent": "If it's a random probability measure with the following property for any finite set of partitions A1 until 8K.",
                    "label": 0
                },
                {
                    "sent": "So this thing partitions into X, So what that means is that they are disjoint subsets and their union is the whole SpaceX.",
                    "label": 0
                },
                {
                    "sent": "If we have such a partition.",
                    "label": 0
                },
                {
                    "sent": "Then G evaluated A1 until G evaluated at 8K.",
                    "label": 0
                },
                {
                    "sent": "So this thing is a random vector now right?",
                    "label": 0
                },
                {
                    "sent": "And this is a random vector in which each entry is non negative and each and the entries sum to one.",
                    "label": 0
                },
                {
                    "sent": "Because G is a probability measure, so it has exactly.",
                    "label": 0
                },
                {
                    "sent": "Those properties right?",
                    "label": 0
                },
                {
                    "sent": "And we say that G is richly processed distributed if each for each finite set of partitions.",
                    "label": 0
                },
                {
                    "sent": "This vector here is directly distributed.",
                    "label": 0
                },
                {
                    "sent": "OK, that's the definition of a tree.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The process.",
                    "label": 0
                },
                {
                    "sent": "Additionally, just as in the case of a Gaussian process, additional process has.",
                    "label": 0
                },
                {
                    "sent": "Two parameters, it has a base distribution which basically corresponds to the mean of the process.",
                    "label": 1
                },
                {
                    "sent": "And the strength parameter, which basically corresponds to the inverse variance of the process.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So we write.",
                    "label": 0
                },
                {
                    "sent": "G is the richly process with strength parameter Alpha and base distribution H. Like this?",
                    "label": 0
                },
                {
                    "sent": "If for any partition.",
                    "label": 0
                },
                {
                    "sent": "Of XG of a one until Geo.",
                    "label": 0
                },
                {
                    "sent": "AKA this thing is a K that is a vector in the K dimensional probability simplex.",
                    "label": 0
                },
                {
                    "sent": "This thing is Dirichlet distributed with parameters given by this.",
                    "label": 0
                },
                {
                    "sent": "And that's the definition of a three day process.",
                    "label": 0
                },
                {
                    "sent": "So why do we call the base distribution aminin strength parameter, inverse variance?",
                    "label": 0
                },
                {
                    "sent": "Cancel that.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's pretty straightforward to workout.",
                    "label": 0
                },
                {
                    "sent": "That if we have some subset a.",
                    "label": 0
                },
                {
                    "sent": "Then the expected value of G evaluated at a is simply H of a right?",
                    "label": 0
                },
                {
                    "sent": "So this is quite similar to the case of a Gaussian process, in which the mean of a Gaussian process evaluated at some point is the mean.",
                    "label": 1
                },
                {
                    "sent": "Of the Gaussian process itself evaluated at that point.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Yeah, in the on the other hand, the variance.",
                    "label": 0
                },
                {
                    "sent": "At of G, evaluated evaluated at a.",
                    "label": 0
                },
                {
                    "sent": "This thing has the following form is age of 8 * 1 -- 8 / A plus 1A plus one.",
                    "label": 0
                },
                {
                    "sent": "So what this means is that the larger Alpha is, the smaller the variance.",
                    "label": 0
                },
                {
                    "sent": "Right, so which is why we say that Alpha is like an inverse variance to the judicial process.",
                    "label": 1
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "Basically.",
                    "label": 0
                },
                {
                    "sent": "The best definition of additional process.",
                    "label": 0
                },
                {
                    "sent": "The main question you need to ask here is that how do we know that such an object exists?",
                    "label": 1
                },
                {
                    "sent": "It turns out that there's multiple ways of proving that such an object this.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The first way is by Ferguson in 1973, so this paper is the one that gave the definition of a derivative process an.",
                    "label": 0
                },
                {
                    "sent": "The first proof of existence he uses what's called the use Kolmogorov consistency theorem, but it turns out that this theorem.",
                    "label": 1
                },
                {
                    "sent": "Doesn't quite work, Ann.",
                    "label": 0
                },
                {
                    "sent": "The reason is because.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So this thing only.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Guarantees the existence of a function.",
                    "label": 1
                },
                {
                    "sent": "From measurable subsets.",
                    "label": 0
                },
                {
                    "sent": "201 but it doesn't actually guarantee that this function.",
                    "label": 0
                },
                {
                    "sent": "This random function is in fact a draw from.",
                    "label": 0
                },
                {
                    "sent": "This function is.",
                    "label": 0
                },
                {
                    "sent": "A probability measure.",
                    "label": 0
                },
                {
                    "sent": "And then other people came along and.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually showed that in fact these things do exist.",
                    "label": 0
                },
                {
                    "sent": "So that's some people who Blackwell McQueen they use definitely theorem to show that that is in fact such a random.",
                    "label": 0
                },
                {
                    "sent": "Our probability measure.",
                    "label": 0
                },
                {
                    "sent": "Which satisfies the properties of additional process.",
                    "label": 0
                },
                {
                    "sent": "And then the.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The best proof of existence is actually what's called stick breaking construction, and this thing basically.",
                    "label": 0
                },
                {
                    "sent": "Constructs that originally process directly, and because you can actually construct it, you know that it exists basically.",
                    "label": 0
                },
                {
                    "sent": "That's also a gamma processing machine.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When are we going to?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "I thought",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can take a 5 minute break.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                }
            ]
        }
    }
}