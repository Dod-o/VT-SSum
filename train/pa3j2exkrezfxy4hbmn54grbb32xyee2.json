{
    "id": "pa3j2exkrezfxy4hbmn54grbb32xyee2",
    "title": "Gaussian Processes",
    "info": {
        "author": [
            "John Cunningham, Department of Electrical Engineering, Stanford University"
        ],
        "published": "Jan. 15, 2013",
        "recorded": "April 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Gaussian Processes"
        ]
    },
    "url": "http://videolectures.net/mlss2012_cunningham_gaussian_processes/",
    "segmentation": [
        [
            "Alright, will talk about Gaussian processes for the next couple hours, so I wanted to start by saying that Gaussian processes are a mathematical object with with a great history in theory and also have are something that's been used to good effect and quite a number of applications.",
            "We're not going to focus on either of those two pieces specifically, but rather talk about the piece in the middle, which is which is from a usability context.",
            "How as machine learners, can we use Gaussian processes?"
        ],
        [
            "So here's how we're going to go about introducing Gaussian processes.",
            "I want to start talking about Gaussians in general, both in words and pictures.",
            "This is just going to be sort of an easy introduction into this, so we can think about we can think about what a Gaussian process actually is from an intuitive perspective.",
            "Then we'll go in and build out some of the equations will talk about using Gaussian processes in a basic regression setting.",
            "That'll be, that'll get us about through the first hour, and then we'll take.",
            "We'll take a quick break and then we'll come back and we'll think about moving beyond the basics of Gaussian processes.",
            "So what kind of things can we change?",
            "Will connect that to some of the different technologies and machine learning that we've seen.",
            "And that'll just about do it."
        ],
        [
            "Alright, So what is a Gaussian as far as machine learning is concerned, this should tie into some of the notions of Bayesian inference that you've seen in the last couple of days.",
            "So a Gaussian a Gaussian distribution is essentially a handy tool for Bayesian inference on real valued variables.",
            "So here's a specific example that we're going to talk about.",
            "Throughout the course of this, I'm interested in measuring my heart rate.",
            "So how might I do this from a modeling modeling perspective?",
            "And I'm here I'm going to measure my heart rate at 7:00 AM and the fact that I do that index by time is going to be important and we're going to see that in a moment.",
            "OK, so."
        ],
        [
            "I'm reasonably healthy guys, so you might think OK. Apriori I have some belief about what my heart rate is going to be when I measured at 77 AM.",
            "Maybe it's somewhere between 50 and 60 beats per minute, so I put some Gaussian some Gaussian prior on that, and there's a density."
        ],
        [
            "Now I can go in on a particular morning and I can measure my heart rate.",
            "I measured at 61."
        ],
        [
            "I can go in on a couple other days and I can measure that 3 three more times.",
            "So now I've got these four observations.",
            "These four noisy observations data measured on four different days, my heart rate and what the Gaussian allows me to do, and what the notion of Bayesian inference allows me to do is that I can then take my prior this Gray distribution and those four draws from that Gaussian."
        ],
        [
            "And I can do posterior inference so I can come up with a posterior P of this of my underlying heart rate given the noisy observations that I've seen.",
            "You see that that's again a Gaussian.",
            "I now have more confidence about where that is an.",
            "I see that in fact it's centered around, say, 6062.",
            "OK.",
            "So."
        ],
        [
            "So.",
            "Let's take that univariate Gaussian case and move that up to multivariate Gaussian.",
            "So we talked about measuring my heart rate at 7:00 AM."
        ],
        [
            "I could also want to measure my heart rate at 8:00 AM an let's think about how how those observations have not just a single real valued variable, but rather a pair of numbers at 7:00 and 8:00 AM.",
            "How that would change.",
            "So to do that we can't use the same University."
        ],
        [
            "Gaussian we want to move up to a multivariate Gaussian.",
            "This should be an object that we're all familiar with."
        ],
        [
            "Now the multivariate Gaussian we're going to conventionally look at as these.",
            "On a flat surface is ellipsoids of ice, so probability, and So what is this distribution telling us?",
            "This distribution is telling us?",
            "Now we've got some prior belief, not on a single heart rate measurement, but on a pair of heart rate measurements.",
            "It shows that there's some positive correlation, which is to say, if I measure if I've got a higher heart rate at 7:00 AM, I imagine it will be higher at 8:00 AM.",
            "And so then we can do the same thing."
        ],
        [
            "So here's our prior.",
            "I can go in and take four measurements on four different days.",
            "Now remember these measurements now, or a pair of numbers.",
            "7:00 AM and 8:00 AM so I can get that data.",
            "I can use Bayes rule in the."
        ],
        [
            "Anyway, and I can come up with some posterior inference.",
            "And now I have.",
            "I have a refined belief of what of what my heart rate is at 78 AM.",
            "So now I'm going to take."
        ],
        [
            "Those two measurements, the measurement that happened 7 in the measurement happened at 8 and I'm going to represent that in a slightly different way.",
            "So this is the same data.",
            "I've got four data for four pairs of numbers is the same data I've just indexed it by time now?",
            "So you see that the one the one red point we were looking at before, which is a pair of numbers at 7:00 AM and 8:00 AM, is now is now put there on that axis at 7:00 AM, four data points still.",
            "So then the natural thing you would want."
        ],
        [
            "Do next if you want to say OK. What if I measure at 9:00 AM?"
        ],
        [
            "What if I measure?",
            "What if I measure my heart rate at 10A?"
        ],
        [
            "Him.",
            "And so on."
        ],
        [
            "So what this is getting at is if we wanted to measure at 7:00 AM, we used a bivariate Gaussian.",
            "Want to do that again at 9:00 AM?",
            "Maybe we would use a 3 dimensional Gaussian 4 dimensional Gaussian, 5 dimensional Gaussian, when really what we're getting at what we care about.",
            "Or we might be interested in is inferring that entire function, overtime, and that that's how we get to a Gaussian process intuitively.",
            "So, rather than having some finite set of Gaussians that we have to measure strictly from.",
            "We have if you will, an infinite set of Gaussians, and that's sort of function is."
        ],
        [
            "So here's how we're going to represent that throughout throughout the course of this.",
            "So each of these.",
            "Each of these function curves in color is going to be a single draw from a Gaussian process.",
            "That's a function.",
            "Well, the way we're going to represent the way we're going to represent the prior distribution is with this.",
            "This mean, which is this Gray line here, and this envelope that sits around it, which is 2 standard deviations around that.",
            "So what this is saying is that we imagine that our average draw is going to be something like that, and we have some distribution that wiggles around that inside that envelope."
        ],
        [
            "What this allows us to do, and this is this, is really one of the key features of the Gaussian processes that remember before we were talking about measuring rigidly at those hourly times.",
            "But now if we've got this nice infinite dimensional object, we can measure really at."
        ],
        [
            "Anytime we want and we'll get into all the mathematical reasons for why we're able to do this so I can measure at this particular time, and you can see what's happened.",
            "Is that I've measured a data point at about 10:30.",
            "And what that's done is that's taken my prior in the same way that we did in that fixed dimensional case that's taken my prior and it said it's refined into posterior.",
            "And it says, actually, I believe that my function is not quite flat anymore, but rather is closer to that data point and you can see that around the data point that I've measured I now have.",
            "I now have increased confidence."
        ],
        [
            "We can then."
        ],
        [
            "Take another measurement, more measurements still and you see what's happening as we go through this process."
        ],
        [
            "As we get more and more data.",
            "We're scribing out this regression function.",
            "This nice smooth underlying function and we're getting more and more confident about the envelope around it."
        ],
        [
            "OK, so that intuitive summary is as follows.",
            "When we were taking when we were taking measurements, single measurements at 7:00 AM, we were getting real value variables.",
            "So the univariate Gaussian was a nice distribution over real valued variables.",
            "When we move that to pairs or triplets or what have you that the multivariate Gaussian allowed us to do that.",
            "And now when we want infinite numbers of real valued variables, in other words, functions of real value variables that sort of Gaussian process allows us to do so so.",
            "If you take nothing else away from this lecture, take take that away that this is something that allows us the Gaussian processes.",
            "Effectively something that allows us to have a Gaussian distribution over infinite numbers of variables.",
            "And what that drives us to is this notion of regression."
        ],
        [
            "So let's look at regression real quick and talk about a few reminders of what regression does for us.",
            "So I'll put this up again.",
            "So here we've got all these blue data points that we've that we've observed, and we believe that there's some smooth underlying function.",
            "That is really what the description of what the data is doing.",
            "So one of the things that regression is quite good for is just that which is denoising and smoothing.",
            "So we want we don't want to follow every little wiggle of these data points, but rather rather come up with some good description of what is noise and what is true signal."
        ],
        [
            "We also want to do prediction and forecasting, so I've collected all this data.",
            "That's great, but now I want to know what my heart rate might be.",
            "A couple of minutes after 9 in the morning well.",
            "To do that you can.",
            "You can have some."
        ],
        [
            "Query at that time and you can say OK.",
            "I believe that my heart rate should be centered centered at this point, with some with some variance envelope."
        ],
        [
            "Furthermore, one of the one of the things about regression and this you've heard you've heard a bit from Peter Boughton will hear, will hear more is the dangers of parametric models.",
            "So what I've done here is I've taken this data and I fit a quadratic to it.",
            "So you can see OK, this quadratic has a reasonably good, reasonably good fit to the data, but it seems to miss some of the features and that of course is because it's a fixed parametric model an it can't.",
            "It can't respond to a lot of these features.",
            "Furthermore, parametric models get us into some dangerous places like because of this because of the way this data was fit, there seems to be some magical point around 11:00 AM where my heart rate peaks for the day and then it sort of falls off after that.",
            "And Furthermore, if you if you really take this too seriously and you.",
            "Can you extrapolate from this?",
            "This has my heart stopping around around dinner time, so there's some dangers to parametric MoD."
        ],
        [
            "Pause.",
            "Furthermore, overfitting underfitting is always going to be a concern with regression and will talk about how Gaussian process deal nicely with that.",
            "So you can see here.",
            "We've got this model, that is that is overfit.",
            "In other words, chasing all the little wiggles, all the little noise in the data."
        ],
        [
            "Conversely, you can have a data that you can have a model that's under fit, and so you see here, we're still we're still fitting the data, so to speak, but we seem to have missed a lot of the interesting structure."
        ],
        [
            "Alright.",
            "So that is that's basically one of what I wanted to go through in this first section of Gaussians in words and pictures.",
            "So now we're going to.",
            "We're going to fill that intuition in with some equations."
        ],
        [
            "I should say, please interrupt me throughout the course of this.",
            "If you have questions about this, I suppose everyone's been doing them.",
            "OK, the multivariate Gaussian.",
            "This should be a review.",
            "I hope we say that F. Which is an N vector is normally distributed if it has the following distribution.",
            "And that following distribution is parameterized by some mean vector M, which is, which is an arbitrary an vector and some covariance matrix K. And the only constraint on that covariance matrix is that it's positive semidefinite.",
            "The shorthand that will use throughout this is.",
            "We say that F is distributed normal with mean M and covariance K."
        ],
        [
            "So as we said before, the loose definition of what a Gaussian processes is a multivariate Gaussian of uncountably infinite length.",
            "In other words, take that multivariate Gaussian vector and just make it longer and longer and longer an.",
            "What does that get to?",
            "That gets to a function?",
            "That's a very loose definition indeed.",
            "Here's a slightly more rigorous definition.",
            "We say that F is a Gaussian process if.",
            "For any subset index, a set of indexes T if F of T which is which is, which is the function F evaluated at those index point has a multivariate distribution according to normal MFT.",
            "An CFT.",
            "Alright, so I'll say that I'm using T here as real numbers for familiarity with regression in time, but the domain can be can be any dimension, any dimension X in Rd, and we will show an example of that later.",
            "Alright, so I kind of breezed by this fact in this definition.",
            "What are those functions M and what are those functions K?",
            "So let's talk about that now, 'cause that's that's an interesting part of what makes a Gaussian process."
        ],
        [
            "So the mean function by analogy to that mean vector in the multivariate Gaussian case where we said the mean vector can be just about anything.",
            "The mean function can be can be any function that Maps.",
            "Index points T onto real values.",
            "Often in Gaussian in the Gaussian process literature, because you can mean subtract your data and because it makes notation easier, we'll just we'll just.",
            "Set set the mean function to 0.",
            "Because really.",
            "In the modeling context, what often makes things most interesting is modeling that kernel or covariance function so.",
            "That that covariance function is is again a function that Maps your input space onto a real value, except it takes a pair of arguments, so it's any valid Mercer kernel.",
            "So this connects to all the stuff that you've talked about in kernels already.",
            "And what this is is just any function.",
            "Any function that has two arguments and that function has to be a positive semidefinite positive semidefinite function.",
            "In other words, it needs to.",
            "It needs to obey Mercer's theorem.",
            "Now Mercer's theorem again is a very rich mathematical theorem from functional analysis, but when it's whittled down to what we care about in this particular case, what it says is if you give any finite or any any.",
            "Any subset index of T and you evaluate that function into a matrix KTT.",
            "In other words, take all your time points, evaluate it and build it into this end by end matrix that that matrix K will be positive semidefinite."
        ],
        [
            "So to summarize that the GP is fully defined by amine function, Anna kernel function and this requirement that every finite subset of the domain has this multivariate normal distribution, this consistent multivariate normal distribution F according to evaluate the mean function evaluate at those points T and the kernel function evaluated those points T. So a couple notes.",
            "One, this is something that we can conceptualize pretty easily and say, OK, I've got these two functions, the meenan and the kernel function, and I can evaluate that and that will give me that will give me a mean and covariance matrix.",
            "Great, and I can stipulate that I want that I want those to always be this to define this Gaussian, but the fact that that should exist as a valid mathematical object is not at all trivial, and Furthermore the fact that this is a full specification.",
            "Or in other words that you give me up, you give me one M and 1K and that defines uniquely a Gaussian process.",
            "Is not at all trivial.",
            "One of the things that's also nontrivial and is quite nice is that most of the interesting properties that were used to in dealing with Gaussian variables and will get into those in a moment that those are all inherited.",
            "OK, so this kernel function is the only is the only really interesting thing that doesn't sort of slot in seamlessly into what we were talking about with Gaussian, so let's."
        ],
        [
            "Let's unpack that a bit more.",
            "So the Canonical example for a kernel function is probably the squared exponential kernel, so I know that looks like a Gaussian, but ignore that for the time being.",
            "Just consider that just consider that a kernel function of two arguments.",
            "So what I want to do just to just to make a very, very explicit connection between a kernel function, Anna covariance matrix is evaluate, evaluate this kernel function at a handful of points.",
            "So to do that, we're going to choose some hyperparameters.",
            "So you'll notice that I've slipped in.",
            "I've slipped in a couple new parameters here.",
            "We call these hyperparameters because they live, they live in the kernel.",
            "This will use the clicker here.",
            "So we've got two.",
            "We've got two hyperparameters here.",
            "We've got L the characteristic characteristic length scale and we've got Sigma squared F which is, which is the variance of the power of this kernel.",
            "And So what we're going to do is we're just going to.",
            "We're just going to evaluate this.",
            "So let's say I take.",
            "Three index points in T at 7 AM, 8:00 AM and 10:30.",
            "These are the three measurement times that we care about now.",
            "What are we asking?",
            "We're asking what is the correlation with covariance between?",
            "Between random variables between my heart rate variable at these times.",
            "So how do I go about doing that?",
            "OK, I take this function here.",
            "I take these pairs of points.",
            "I evaluated all pairs and I build this into a matrix.",
            "So what we can then do is we can then change these kernel hyperparameters and see how this covariance matrix changes.",
            "So what I've done here, I'll just flashback in between that we had a length scale of 100."
        ],
        [
            "Now we're going to go to a length scale of 500 and what we see is that the diagonal we've still got the same values, but what's happening is as we move away from the diagonal.",
            "The correlation is falling off much less quickly, so what's this saying?",
            "With a higher length scale value.",
            "Between 7:00 and 8:00 AM.",
            "These variables are highly highly correlated.",
            "As you get further away to 10:30, this variable is not quite as correlated, but still it's still quite highly correlated."
        ],
        [
            "On the other hand, if we make this a smaller number, you see that the correlation drops off very quickly, so this is nearly scaled identity matrix.",
            "In other words, in other words, my heart rate is 7:00 AM is nearly independent from my heart rate at 8:00 AM."
        ],
        [
            "We can also change Sigma F and see how that changes things, so this remember we were putting up that envelope around around the Gaussian mean around the GP mean that Gray envelope.",
            "So what we're doing here is changing that so.",
            "We double that the envelope doubles."
        ],
        [
            "OK.",
            "So that was just a tie in something something where a lot of people when they're learning Gaussian processes get tripped up, is connecting the kernel function to covariance matrices.",
            "So, so I'm going to I'm going to repeat that a couple of times, but I think, but I think it's valuable to make that connection.",
            "Alright, so an intuitive summary of GP so far GP offer distributions over functions.",
            "And for any finite subset vector we've got this normal distribution, and you see here.",
            "As promised, I've dropped, I've dropped the mean function.",
            "And the covariance, the covariance matrix K is calculated by just plugging the T the index points into this kernel function.",
            "So to introduce some new notation before we were saying F is normally distributed with mean zero and covariance K, you'll often see written F is distributed as a GP with mean function M and covariance funk."
        ],
        [
            "OK. Alright, so I mentioned that.",
            "I mentioned that most of the important Gaussian properties that we care about are inherited by by Gaussian processes, and So what I want to do is walk through a couple of properties of the Gaussian that are going to be interesting for today's purposes that are very useful to in the GP context.",
            "So one is is activity.",
            "In other words, adding two Gaussians together gives you a Gaussian.",
            "Again.",
            "That'll be nice to us in forming a joint.",
            "Conditioning so conditioning on Gaussian random variables.",
            "This is important for inference.",
            "The ability to calculate expectations which is going to be interesting for calculating posterior predictive moments.",
            "And finally, our ability to marginalise out variables that we don't care about.",
            "So there are many other.",
            "There are many other nice properties of Gaussian of course, but those are the ones that we want that we really want to care about today."
        ],
        [
            "So let's first talk about forming a joint jointly Gaussian distribution so.",
            "I've got some prior.",
            "I've got some prior on F, so F I'm going to use throughout as our prior.",
            "I've got some Gaussian prior with MF and covariance KFF.",
            "I've got some IID noise that I add to that an.",
            "And then I want to let Y equal F + N. So what's this saying this is saying the underlying function that I care about?",
            "Regressor that I care about is F An.",
            "I measured some noisy data observations of that why?",
            "Which is why.",
            "Plus this independent noise N. So what's nice about this is that this allows us then to form a joint distribution P of Y&F, and you see that this is again a Gaussian.",
            "So a couple objects that we haven't seen before.",
            "So KFF we gave to you MF we gave to you and so we say OK what's KFY and what's kyy?",
            "OK you can just evaluate that out.",
            "KF why is this expectation?",
            "And in this case that equals KFF kyy in this case equals KF plus the noise.",
            "So the nice thing that the nice feature now that we have to connect this back to the regression problem is that the latent F which we care about and the noisy observation Y are jointly Gaussian."
        ],
        [
            "OK, wait a second.",
            "We just did this all with regular multivariate Gaussians.",
            "So where did the GP go?",
            "'cause we've just been talking bout GP.",
            "So the point I want to make here is that if F&Y are indexed by some some input points T. In other words, if MF is actually just some mean function evaluated at these at these endpoints T, these end index points and KFF is of some kernel evaluation.",
            "Then it could have just as easily written this as a GP prior F and some add an additive noise GPN and use this same additivity property, but here.",
            "When I wrote this this specific Gaussian, I would have just indexed this Y at T and index this F at T. So this is.",
            "This is your starting to see one of the really."
        ],
        [
            "Nice features of the GP is that is that all we need to do is bring in a finite set of index points and then we're working with multivariate Gaussians.",
            "So as a warning because of this is that there is some overloaded notation here.",
            "So F. People, people are generally pretty loose about that notation.",
            "It can either be infinite.",
            "In other words, GP right F is GP.",
            "So now F is this infinite dimensional object or a finite multivariate Gaussian and that can.",
            "That's generally put pretty clear depending on the context."
        ],
        [
            "Alright, the next property that we care about is conditioning or doing doing Bayesian inference.",
            "So here we've got our latent F and our noisy observation why, and we know that those are jointly Gaussian and we've got F. And why are distributed according to this distribution.",
            "So then.",
            "We can do inference and we can say that the posterior of F given Y is again a normal distribution, so this is an important fact of Gaussian distributions and this is just.",
            "Stock and trade manipulation of a Gaussian distribution.",
            "This is actually proving that this is.",
            "This is the case is something that I think that everybody should do once and only once.",
            "Once once you've done it, just just just forget about it, 'cause it's rather tedious, but it's a cool fact to know.",
            "So a couple things to point out here.",
            "You see that we've got this mean function.",
            "Sorry this, this mean an this covariance.",
            "We can impact this a little bit.",
            "We should all be pretty familiar with this one.",
            "Interesting thing here is that you see that this is just a linear function of our observations Y, so that's nice to know.",
            "And further, this term here KFF is our prior is our prior covariance, so that's the uncertainty that we had about that latent, and you see that we've.",
            "Subtracted here this other term, and that's essentially how much our data explains about what we know about about our prior.",
            "So if our data tells us nothing about our prior uncertainty, then this will be a very small term and in other words, our uncertainty is still just around KFF.",
            "If instead our data tells us a whole bunch, then.",
            "This this approach is KFF and our uncertainty decreases considerably.",
            "So the main point of this and you can.",
            "Not worry about parsing this too much, but rather inference of the latent given the data is simple linear algebra, so we've reduced all of all of the complexity of Bayesian inference and all of the problems that are sometimes associated that associated with that to a simple to a simple set of linear equations."
        ],
        [
            "Alright, the next feature so we talked about forming a joint.",
            "We've talked about doing inference so now we can talk about calculating expectations.",
            "So again this simple term.",
            "This simple conditioning term gave us this fact and what this allows us to see is that I mean this is repetitive I suppose, but the expectation?",
            "The expectation of F given Y is simply this mean term, and so that's what is that.",
            "That's the map estimate.",
            "That's the posterior mean.",
            "There are a number of other moments that would be interested in one another.",
            "You want to bring up.",
            "So we looked at the posterior moments.",
            "F and why we've been talking about this joint gaussianity between the latent F and the noisy observation Y.",
            "Instead, we can look at.",
            "We can look at why, which is data that we've collected an Y star, which is data that we haven't collected.",
            "In other words, when I said I want to query on and see what my heart rate is going to be at nine couple minutes after 9:00 AM.",
            "This is convention in the literature that YY star is a data that you want to predict, so those are also jointly Gaussian.",
            "And so this is no different, we just use the same.",
            "We just use the same conditioning property to get to get to this fact and you can see that this is our predictive predictive mean."
        ],
        [
            "The final property that I want that I want to explore is marginalization.",
            "So again, we have these jointly Gaussian variables.",
            "We can marginalized out the latent 'cause you say, maybe I don't care about that.",
            "Maybe I don't care about the latent function at all.",
            "I just want to know how well this Gaussian process model describes my data.",
            "So to do that, you just P of Y, you integrate out F and another another nice property of Gaussians is that you can just read that right off of here that why is distributed MY with covariance kyy.",
            "So this is nice because it gives us the data log likelihood, which then can be useful for model selection, model comparison and things like this.",
            "Oh, right, so 111 note because we'll come back to this.",
            "You notice here that when I introduced those hyperparameters that lived up in the kernel.",
            "Those have been suppressed here, but actually the data is P of Y.",
            "Given those those kernel hyperparameters and this will be the basis of model selection because we want to tune our data marginal likelihood based on what those hyperparameters settings."
        ],
        [
            "Things are OK at this point.",
            "You might be complaining because.",
            "Because you might say OK, I'm bored.",
            "All we've done so far is is messed around with Gaussians and I'm familiar with Gaussians and you know, I thought we were coming to talk about infinite dimensional probability distributions and interesting stuff, and so if that's your complaint, you're correct and I'm sorry about that.",
            "But in fact this is the whole point right?",
            "The whole point of this is that we take this this beautiful mathematical theory.",
            "And when it comes down to actually dealing with these objects, it's simple linear algebra.",
            "It's simple inference on Gaussian distributions and all this, and So what I want I want to convince you of is that even with that sort of banal setup, we can do some really quite remarkable things."
        ],
        [
            "OK, so now let's look at some of the.",
            "Remarkable things.",
            "I suppose that GP can do.",
            "We've talked about Gaussians in words and pictures.",
            "We've talked about some of the equations, so let's talk about using using GPS in a regression context."
        ],
        [
            "So.",
            "Our example model, which we've introduced throughout the course of the equation section an.",
            "Now we'll see what it can do.",
            "So say the F is our is our latent.",
            "It's a GP with zero mean and some some kernel KFF, and the kernel has this form.",
            "This is the squared exponential.",
            "When we get to talking about talking about kernels will mess with that, but for now just just just let that be.",
            "We say that Y given F, this is our noise term, right?",
            "In other words, if I give you if I give you the latent function value F, the data that I observe is distributed some with some independent noise.",
            "On top of that, that's got some kernel.",
            "That's that's this is the.",
            "This is the white noise kernel and all this is saying is that two different is that two different.",
            "The noise that I observe the measurement noise that I observe on two different time points is independent.",
            "What this allows us to do?",
            "Again because of this additivity property is we can.",
            "We can add this and see again that why is distributed as a GP with some kernel kyy and you see that these kernel functions add.",
            "So alright, so now we've got the probabilistic model.",
            "The distribution fully specified, so let's just fill that in with some of these hyperparameters.",
            "So I'm going to choose Sigma F = 10, so that's the standard deviation envelope I'm going to choose a characteristic length scale of 50 an A noise, uh, noise power of 1.",
            "So alright, so let's let's let's look at again of our visual representation of that.",
            "So this is the prior on F. What this is saying to connect this to the equations is that we've got a mean function of 0, so that mean continues along a zero at all times.",
            "And we've got Sigma Squared Sigma F of 10.",
            "And so this is the two standard deviation envelope.",
            "So now we can go ahead and we can take draws from this Gaussian."
        ],
        [
            "Process.",
            "So this is a single draw from that prior F the GP.",
            "And so now you see, hopefully connected to this notion of how we can draw function from from."
        ],
        [
            "BP.",
            "So the steps of this should be clear.",
            "This is the only.",
            "This is the only code snippet I will give you throughout the course of it, but it's but it's, but it's only.",
            "It's only one line, and Dylan will unpack that more.",
            "OK, so how do I get?",
            "How do I actually get this draw?",
            "So to do that I take a whole bunch of index points, a finite number.",
            "So here I took the integer index points between 0 and 500.",
            "I evaluate that kernel function just like we did with those three index points.",
            "I evaluate the kernel function and build that into some into some 500 by 500 matrix KFF.",
            "And then I can take a draw from a Gaussian with zero meenan this covariance?",
            "So how that's actually done right?",
            "And this would be your MATLAB code.",
            "How that's actually done is according to this.",
            "So that will give you this procedure will give you this nice will give you this nice draw."
        ],
        [
            "OK, so there was one draw in green.",
            "Now we've taken 4 draws of that and I'm belaboring this point just so that just so that we remind ourselves that a draw from a Gaussian process gives you a function.",
            "So four draws from that will give you these nice four."
        ],
        [
            "Functions.",
            "Before, when we were evaluating the kernel matrix, we mess around with the hyperparameters a little bit to see how that changed.",
            "To see how that changed the covariance matrix, let's do that here in pictures.",
            "So here is Sigma F of 10 and the length scale of 50."
        ],
        [
            "If I change I'm going to leave that lengthscale of 50, but now I've changed the power of that.",
            "The envelope of that from 10 down to four and you see what's happened is that the.",
            "Envelope has shrunk and the draws have shrunk."
        ],
        [
            "Not surprising.",
            "If I change, if I change the length scale of 50 to a length scale of 10, right?",
            "So what this is saying is?",
            "As two points get further apart, they fall off.",
            "Their correlation falls off more quickly, so accordingly you get more wiggly draws.",
            "One final point on this is this.",
            "This should feel a whole lot like when we were evaluating those those covariance matrices.",
            "In other words, this is a a closer to the identity matrix."
        ],
        [
            "Variance matrix is close to the identity matrix.",
            "Then for example this which has much longer range correlations."
        ],
        [
            "OK, so I mentioned that we've been looking at regression in time and taking draws that are nice temporal functions.",
            "Those are easy to look at and familiar to us.",
            "You can also have multidimensional input.",
            "So one of the one of the histories of the application areas where Gaussian process even used a lot is in geostatistics, and there they are often interested in spatial Gaussian processes.",
            "So for example, latitude in Lanja tude instead of regression.",
            "In time, you want to regress on latitude and longitude.",
            "So to do that, we can make.",
            "We can make each input instead of instead of a single real real valued number of pair of numbers.",
            "So now we've got some Gaussian process.",
            "Here F is the same GP and the kernel is almost exactly the same, except here there's just an extra term.",
            "There's a.",
            "There's a squared term for each dimension of that Gaussian process.",
            "So what might draw from that look like instead of one single function?",
            "What you get now is a field over over Latitude in lanja tude, so this is your random function that's drawn over that field.",
            "So, um.",
            "I suppose shameless plug if you are staying for AI stats and you want to see a multidimensional GP in action we've got.",
            "We've got a paper on that and I'm sure there will be other papers."
        ],
        [
            "GPS as well.",
            "OK, so we've got the same model that we've just been dealing with, and now let's gather some data.",
            "So this is this is our model the GP model?"
        ],
        [
            "We've got our prior."
        ],
        [
            "And we can go in and take.",
            "We can go in and take a data point.",
            "So I want to say I gather a data point at time to 204.",
            "So what I know is that I can evaluate Y of 204 and I know that this is according to the model.",
            "This is Gaussian distributed according to mean Zero and Kyy evaluated at 204, 204.",
            "That's a simple univariate Gaussian.",
            "This is all just pulling this right out of right out of the definition of of the GP Y and I'm just evaluating the kernel matrix."
        ],
        [
            "So then we can use conditioning to update the posterior.",
            "So here I've still got the prior P of F. But now I've got this data observation so I can use.",
            "I can use the inference rule that we talked about and evaluate why to take this data point Y at 204 and run this through the equation."
        ],
        [
            "And what does this give me?",
            "Well, now this is refined.",
            "My posterior estimate of F. So this is no longer a flat mean function.",
            "But I think the main function comes down here and goes through F. And Furthermore, because of our choice of noise parameters, you can see that what's happened here is right around 204.",
            "I'm awfully sure my.",
            "Covariance envelope has collapsed, and I'm awfully sure that that the measurement is around there, but as soon as I get further away, well, I forget because because I don't think that what happens at 400 is particularly related to what happens in two or four.",
            "So by the time I get over here, I'm basically back to the prior."
        ],
        [
            "So a small change is that."
        ],
        [
            "Here we were looking at the posterior this this I think is important is important to hang on just for a second because often when you look at GP work, it's not clear whether people are talking about whether they're showing the posterior or they're showing a predictive model, so.",
            "This is on the."
        ],
        [
            "Steri are and this here is the predictive the predictive distribution.",
            "So you'll just notice there's a very small change here, but this variance envelope has just increased slightly ever so slightly, because we think that on top of that, on top of that, posterior is actually some measurement noise.",
            "So this is our belief about the predictive distribution."
        ],
        [
            "OK, so then I had.",
            "I had .204 let's say I also gather, uh, another data point at night."
        ],
        [
            "Andy.",
            "Well then I can add that in the same way to this distribution.",
            "So and you see what's happened here is I've added this data point at 90 and the same thing has happened.",
            "The variance envelope has decreased.",
            "The mean function is chain."
        ],
        [
            "Changed I can do this an add more and more."
        ],
        [
            "ETA points as I go.",
            "And this is what we've got to.",
            "And this is, again, the predictive.",
            "This is again the predictive distribution.",
            "It's Gaussian and all I'm doing is adding more and more more.",
            "I'm making this vector here.",
            "What I've observed longer and longer."
        ],
        [
            "OK, so this gets us to this guy."
        ],
        [
            "This to a question which is alright, so I get more and more data.",
            "But when am I getting to my actual regression function 'cause I can see that we're doing regression here.",
            "But when am I going to produce the parameters of my model?",
            "But of course this is a non pair."
        ],
        [
            "Metric regression model.",
            "So this is one of the virtues of Gaussian processes that we're not going to just spit out a couple a couple parameters of a quadratic function.",
            "But rather as we, as we gather all our data, the GP regression gets more and more refined.",
            "So this is one of the benefits of GP regression, which is it?",
            "Let's let's the data speak for itself.",
            "I suppose the downside of that is that all the data, all the data must speak.",
            "So you see that as this Y grows and we collect more and more data we're doing, we're doing a larger and larger problem here with inverting kyy doing this, this nice piece of simple linear algebra.",
            "So you'll often hear you'll often hear this this comment, which is nonparametric models, have an infinite number of parameters."
        ],
        [
            "Anne.",
            "I'd like to refine this slightly in our minds, the way we think about this and not say that nonparametric models have an infinite number of parameters, but rather nonparametric models have a finite but unbounded number of parameters, and that number of parameters grows with the data.",
            "So the way you can think about it here is that as this gets larger and larger and tends towards infinite, yes, we have.",
            "We have an unbounded number of parameters that can describe what our prediction is going to be, but that's still just growing finitely with the amount of data that we."
        ],
        [
            "OK.",
            "So we are almost were almost through the basics.",
            "There's one more piece of the basics in Gaussian processes and using Gaussian processes for regression that I want to talk about."
        ],
        [
            "And that is model selection or hyperparameter learning.",
            "So all throughout the course of this, we've been adding these data points looking at the predictive distribution.",
            "Looking at posterior inference.",
            "And we've been doing this with a fixed with a fixed model F. This Gaussian process with KFF according to this function, so.",
            "I want to talk about these hyperparameters now L and Sigma squared F. So here we've got L E = 50 and we've seen how changing that can change the fit that we get.",
            "So if I make L quite a bit smaller, you can see that now L is.",
            "Sorry that this GP is overfitting the data.",
            "In other words, it's chasing.",
            "It's chasing each individual wiggle of these of these data points.",
            "It's recurring.",
            "It's forgetting very quickly.",
            "Probably too quickly, such that really it says here that we know effectively nothing about about about about our inference on this data point, when in fact, given this is probably a better description, we believe that the function should be around here.",
            "Again, if we have a length scale that's too high, then we're underfitting the data.",
            "So here we've got some very confident prediction, but it's probably missing some of the interesting structure that exists in this data.",
            "So the question that we want to talk about right now and what we want to address is how can we?",
            "How can we tune or integrate over these hyperparameters L and Sigma squared F so that we get so that we take our data and we get to this model which is just right?"
        ],
        [
            "So there's two popular ways to do that.",
            "The first of which is to use the marginal likelihood.",
            "We can marginalized out the latent function F, and we can just look at the marginal likelihood of the data Y.",
            "And again, I said that this is actually hiding these extra parameters, which are now the parameters that we care about these hyperparameters.",
            "We want to model selection on these hyperparameters.",
            "So just looking at this, and particularly when you talk to people that are outside of Bayesian machine learning, it's quite common that people say it's not obvious why this should do model selection automatically, and that this shouldn't overfit or underfit the data, but it's right in the math, so let's unpack that for a moment.",
            "So here is the log marginal likelihood of the data.",
            "And you can see that this has three terms.",
            "OK, let's ignore this one.",
            "This is just a normalizing constant.",
            "So what are these these two terms do?",
            "This term typically is called a data fit term, and this term is often called a complexity penalty.",
            "So what you can see here.",
            "Is that if we just consider Sigma F right?",
            "So that's the envelope?",
            "That's the envelope of the Gaussian process.",
            "So as Sigma F gets larger.",
            "This will fit the data better.",
            "In other words, more data gets more data gets inside that that envelope.",
            "But as that happens, as Sigma F gets larger, this term also scales up.",
            "So you pay a penalty here.",
            "So these two terms these two terms, your complexity in your data fit, are at odds with one another, and that gives you this, this this.",
            "Automatic determination of overfitting versus underfitting, which happens in a Bayesian model.",
            "Unpacking this for the length scale is just a little bit trickier, but again, is simple linear algebra and comes to the volume of this volume of this ellipsoid, which gets larger.",
            "This gets larger as you get closer to White.",
            "In other words, you get a shorter, shorter length scale and that opposes in the data fit term as well, so that's.",
            "That's worth spending some time unpacking on your own.",
            "This is why you hear the term Bayesian Occam's Razor or Occam's razors is implemented via this regularization, or this Bayesian model selection, because there is this.",
            "At this data fit term here, and you've got this automatic regularizer here, which discourages overcomplex models.",
            "So the details of this dealing with model selection will be will be fleshed out some in the practical."
        ],
        [
            "Another way to do this, and this falls outside of the this falls outside of the Bayesian context I supposed, but one can also use a cross validation cross validation approach, which is quite popular machine learning.",
            "So instead of considering the marginal likelihood, we can consider the predictive.",
            "Predictive distribution for some held out data.",
            "So here I'll call this the predictive log likelihood.",
            "So here I've hold out some, hold out some test data, and as we know this is again a Gaussian.",
            "P of Y test given why train?",
            "So it's again a Gaussian, so you can use this.",
            "You can take derivatives on this and you can tune model hyperparameters in the same way that you do in any cross validation approach.",
            "And again, that will be.",
            "Model selection will be dealt with in the practical."
        ],
        [
            "Alright, so that gets through the basics of what I want to talk about with GPS and words and pictures, GPS and equations and then using GPS in a basic regression context.",
            "We're just just a few minutes ahead of time, so why don't we take a few questions?",
            "Nice code with the heart rate example.",
            "Is it possible to?",
            "Is there a nice cattle will give you kind of periodic.",
            "Yeah so we will get to that in kernel and kernel choices will talk about a periodic, the periodic function exactly.",
            "Yeah so so that kernel that we've chosen the squared exponential kernel is just a nice and Canonical choice that gives you smooth functions overtime.",
            "We can and will mess with that as soon as as soon as we go to the next section.",
            "After no other questions, why don't we take one?",
            "We take a five minute, 5 minute breather and then we'll come back and we'll get into some more interesting details."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, will talk about Gaussian processes for the next couple hours, so I wanted to start by saying that Gaussian processes are a mathematical object with with a great history in theory and also have are something that's been used to good effect and quite a number of applications.",
                    "label": 1
                },
                {
                    "sent": "We're not going to focus on either of those two pieces specifically, but rather talk about the piece in the middle, which is which is from a usability context.",
                    "label": 0
                },
                {
                    "sent": "How as machine learners, can we use Gaussian processes?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's how we're going to go about introducing Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "I want to start talking about Gaussians in general, both in words and pictures.",
                    "label": 1
                },
                {
                    "sent": "This is just going to be sort of an easy introduction into this, so we can think about we can think about what a Gaussian process actually is from an intuitive perspective.",
                    "label": 0
                },
                {
                    "sent": "Then we'll go in and build out some of the equations will talk about using Gaussian processes in a basic regression setting.",
                    "label": 0
                },
                {
                    "sent": "That'll be, that'll get us about through the first hour, and then we'll take.",
                    "label": 0
                },
                {
                    "sent": "We'll take a quick break and then we'll come back and we'll think about moving beyond the basics of Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "So what kind of things can we change?",
                    "label": 0
                },
                {
                    "sent": "Will connect that to some of the different technologies and machine learning that we've seen.",
                    "label": 0
                },
                {
                    "sent": "And that'll just about do it.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, So what is a Gaussian as far as machine learning is concerned, this should tie into some of the notions of Bayesian inference that you've seen in the last couple of days.",
                    "label": 0
                },
                {
                    "sent": "So a Gaussian a Gaussian distribution is essentially a handy tool for Bayesian inference on real valued variables.",
                    "label": 1
                },
                {
                    "sent": "So here's a specific example that we're going to talk about.",
                    "label": 0
                },
                {
                    "sent": "Throughout the course of this, I'm interested in measuring my heart rate.",
                    "label": 0
                },
                {
                    "sent": "So how might I do this from a modeling modeling perspective?",
                    "label": 0
                },
                {
                    "sent": "And I'm here I'm going to measure my heart rate at 7:00 AM and the fact that I do that index by time is going to be important and we're going to see that in a moment.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm reasonably healthy guys, so you might think OK. Apriori I have some belief about what my heart rate is going to be when I measured at 77 AM.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's somewhere between 50 and 60 beats per minute, so I put some Gaussian some Gaussian prior on that, and there's a density.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now I can go in on a particular morning and I can measure my heart rate.",
                    "label": 0
                },
                {
                    "sent": "I measured at 61.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I can go in on a couple other days and I can measure that 3 three more times.",
                    "label": 0
                },
                {
                    "sent": "So now I've got these four observations.",
                    "label": 0
                },
                {
                    "sent": "These four noisy observations data measured on four different days, my heart rate and what the Gaussian allows me to do, and what the notion of Bayesian inference allows me to do is that I can then take my prior this Gray distribution and those four draws from that Gaussian.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I can do posterior inference so I can come up with a posterior P of this of my underlying heart rate given the noisy observations that I've seen.",
                    "label": 0
                },
                {
                    "sent": "You see that that's again a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "I now have more confidence about where that is an.",
                    "label": 0
                },
                {
                    "sent": "I see that in fact it's centered around, say, 6062.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let's take that univariate Gaussian case and move that up to multivariate Gaussian.",
                    "label": 1
                },
                {
                    "sent": "So we talked about measuring my heart rate at 7:00 AM.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I could also want to measure my heart rate at 8:00 AM an let's think about how how those observations have not just a single real valued variable, but rather a pair of numbers at 7:00 and 8:00 AM.",
                    "label": 0
                },
                {
                    "sent": "How that would change.",
                    "label": 0
                },
                {
                    "sent": "So to do that we can't use the same University.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Gaussian we want to move up to a multivariate Gaussian.",
                    "label": 0
                },
                {
                    "sent": "This should be an object that we're all familiar with.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the multivariate Gaussian we're going to conventionally look at as these.",
                    "label": 0
                },
                {
                    "sent": "On a flat surface is ellipsoids of ice, so probability, and So what is this distribution telling us?",
                    "label": 0
                },
                {
                    "sent": "This distribution is telling us?",
                    "label": 0
                },
                {
                    "sent": "Now we've got some prior belief, not on a single heart rate measurement, but on a pair of heart rate measurements.",
                    "label": 1
                },
                {
                    "sent": "It shows that there's some positive correlation, which is to say, if I measure if I've got a higher heart rate at 7:00 AM, I imagine it will be higher at 8:00 AM.",
                    "label": 0
                },
                {
                    "sent": "And so then we can do the same thing.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's our prior.",
                    "label": 0
                },
                {
                    "sent": "I can go in and take four measurements on four different days.",
                    "label": 0
                },
                {
                    "sent": "Now remember these measurements now, or a pair of numbers.",
                    "label": 0
                },
                {
                    "sent": "7:00 AM and 8:00 AM so I can get that data.",
                    "label": 0
                },
                {
                    "sent": "I can use Bayes rule in the.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anyway, and I can come up with some posterior inference.",
                    "label": 0
                },
                {
                    "sent": "And now I have.",
                    "label": 0
                },
                {
                    "sent": "I have a refined belief of what of what my heart rate is at 78 AM.",
                    "label": 0
                },
                {
                    "sent": "So now I'm going to take.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Those two measurements, the measurement that happened 7 in the measurement happened at 8 and I'm going to represent that in a slightly different way.",
                    "label": 0
                },
                {
                    "sent": "So this is the same data.",
                    "label": 0
                },
                {
                    "sent": "I've got four data for four pairs of numbers is the same data I've just indexed it by time now?",
                    "label": 0
                },
                {
                    "sent": "So you see that the one the one red point we were looking at before, which is a pair of numbers at 7:00 AM and 8:00 AM, is now is now put there on that axis at 7:00 AM, four data points still.",
                    "label": 0
                },
                {
                    "sent": "So then the natural thing you would want.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do next if you want to say OK. What if I measure at 9:00 AM?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What if I measure?",
                    "label": 0
                },
                {
                    "sent": "What if I measure my heart rate at 10A?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Him.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what this is getting at is if we wanted to measure at 7:00 AM, we used a bivariate Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Want to do that again at 9:00 AM?",
                    "label": 0
                },
                {
                    "sent": "Maybe we would use a 3 dimensional Gaussian 4 dimensional Gaussian, 5 dimensional Gaussian, when really what we're getting at what we care about.",
                    "label": 0
                },
                {
                    "sent": "Or we might be interested in is inferring that entire function, overtime, and that that's how we get to a Gaussian process intuitively.",
                    "label": 0
                },
                {
                    "sent": "So, rather than having some finite set of Gaussians that we have to measure strictly from.",
                    "label": 0
                },
                {
                    "sent": "We have if you will, an infinite set of Gaussians, and that's sort of function is.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's how we're going to represent that throughout throughout the course of this.",
                    "label": 0
                },
                {
                    "sent": "So each of these.",
                    "label": 0
                },
                {
                    "sent": "Each of these function curves in color is going to be a single draw from a Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "That's a function.",
                    "label": 0
                },
                {
                    "sent": "Well, the way we're going to represent the way we're going to represent the prior distribution is with this.",
                    "label": 0
                },
                {
                    "sent": "This mean, which is this Gray line here, and this envelope that sits around it, which is 2 standard deviations around that.",
                    "label": 0
                },
                {
                    "sent": "So what this is saying is that we imagine that our average draw is going to be something like that, and we have some distribution that wiggles around that inside that envelope.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What this allows us to do, and this is this, is really one of the key features of the Gaussian processes that remember before we were talking about measuring rigidly at those hourly times.",
                    "label": 0
                },
                {
                    "sent": "But now if we've got this nice infinite dimensional object, we can measure really at.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anytime we want and we'll get into all the mathematical reasons for why we're able to do this so I can measure at this particular time, and you can see what's happened.",
                    "label": 0
                },
                {
                    "sent": "Is that I've measured a data point at about 10:30.",
                    "label": 0
                },
                {
                    "sent": "And what that's done is that's taken my prior in the same way that we did in that fixed dimensional case that's taken my prior and it said it's refined into posterior.",
                    "label": 0
                },
                {
                    "sent": "And it says, actually, I believe that my function is not quite flat anymore, but rather is closer to that data point and you can see that around the data point that I've measured I now have.",
                    "label": 0
                },
                {
                    "sent": "I now have increased confidence.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can then.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Take another measurement, more measurements still and you see what's happening as we go through this process.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As we get more and more data.",
                    "label": 0
                },
                {
                    "sent": "We're scribing out this regression function.",
                    "label": 0
                },
                {
                    "sent": "This nice smooth underlying function and we're getting more and more confident about the envelope around it.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so that intuitive summary is as follows.",
                    "label": 1
                },
                {
                    "sent": "When we were taking when we were taking measurements, single measurements at 7:00 AM, we were getting real value variables.",
                    "label": 0
                },
                {
                    "sent": "So the univariate Gaussian was a nice distribution over real valued variables.",
                    "label": 1
                },
                {
                    "sent": "When we move that to pairs or triplets or what have you that the multivariate Gaussian allowed us to do that.",
                    "label": 0
                },
                {
                    "sent": "And now when we want infinite numbers of real valued variables, in other words, functions of real value variables that sort of Gaussian process allows us to do so so.",
                    "label": 1
                },
                {
                    "sent": "If you take nothing else away from this lecture, take take that away that this is something that allows us the Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "Effectively something that allows us to have a Gaussian distribution over infinite numbers of variables.",
                    "label": 0
                },
                {
                    "sent": "And what that drives us to is this notion of regression.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's look at regression real quick and talk about a few reminders of what regression does for us.",
                    "label": 1
                },
                {
                    "sent": "So I'll put this up again.",
                    "label": 0
                },
                {
                    "sent": "So here we've got all these blue data points that we've that we've observed, and we believe that there's some smooth underlying function.",
                    "label": 0
                },
                {
                    "sent": "That is really what the description of what the data is doing.",
                    "label": 0
                },
                {
                    "sent": "So one of the things that regression is quite good for is just that which is denoising and smoothing.",
                    "label": 0
                },
                {
                    "sent": "So we want we don't want to follow every little wiggle of these data points, but rather rather come up with some good description of what is noise and what is true signal.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also want to do prediction and forecasting, so I've collected all this data.",
                    "label": 0
                },
                {
                    "sent": "That's great, but now I want to know what my heart rate might be.",
                    "label": 0
                },
                {
                    "sent": "A couple of minutes after 9 in the morning well.",
                    "label": 0
                },
                {
                    "sent": "To do that you can.",
                    "label": 0
                },
                {
                    "sent": "You can have some.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Query at that time and you can say OK.",
                    "label": 0
                },
                {
                    "sent": "I believe that my heart rate should be centered centered at this point, with some with some variance envelope.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Furthermore, one of the one of the things about regression and this you've heard you've heard a bit from Peter Boughton will hear, will hear more is the dangers of parametric models.",
                    "label": 1
                },
                {
                    "sent": "So what I've done here is I've taken this data and I fit a quadratic to it.",
                    "label": 0
                },
                {
                    "sent": "So you can see OK, this quadratic has a reasonably good, reasonably good fit to the data, but it seems to miss some of the features and that of course is because it's a fixed parametric model an it can't.",
                    "label": 0
                },
                {
                    "sent": "It can't respond to a lot of these features.",
                    "label": 0
                },
                {
                    "sent": "Furthermore, parametric models get us into some dangerous places like because of this because of the way this data was fit, there seems to be some magical point around 11:00 AM where my heart rate peaks for the day and then it sort of falls off after that.",
                    "label": 0
                },
                {
                    "sent": "And Furthermore, if you if you really take this too seriously and you.",
                    "label": 0
                },
                {
                    "sent": "Can you extrapolate from this?",
                    "label": 0
                },
                {
                    "sent": "This has my heart stopping around around dinner time, so there's some dangers to parametric MoD.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pause.",
                    "label": 0
                },
                {
                    "sent": "Furthermore, overfitting underfitting is always going to be a concern with regression and will talk about how Gaussian process deal nicely with that.",
                    "label": 0
                },
                {
                    "sent": "So you can see here.",
                    "label": 0
                },
                {
                    "sent": "We've got this model, that is that is overfit.",
                    "label": 0
                },
                {
                    "sent": "In other words, chasing all the little wiggles, all the little noise in the data.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Conversely, you can have a data that you can have a model that's under fit, and so you see here, we're still we're still fitting the data, so to speak, but we seem to have missed a lot of the interesting structure.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So that is that's basically one of what I wanted to go through in this first section of Gaussians in words and pictures.",
                    "label": 1
                },
                {
                    "sent": "So now we're going to.",
                    "label": 0
                },
                {
                    "sent": "We're going to fill that intuition in with some equations.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I should say, please interrupt me throughout the course of this.",
                    "label": 0
                },
                {
                    "sent": "If you have questions about this, I suppose everyone's been doing them.",
                    "label": 0
                },
                {
                    "sent": "OK, the multivariate Gaussian.",
                    "label": 0
                },
                {
                    "sent": "This should be a review.",
                    "label": 0
                },
                {
                    "sent": "I hope we say that F. Which is an N vector is normally distributed if it has the following distribution.",
                    "label": 0
                },
                {
                    "sent": "And that following distribution is parameterized by some mean vector M, which is, which is an arbitrary an vector and some covariance matrix K. And the only constraint on that covariance matrix is that it's positive semidefinite.",
                    "label": 1
                },
                {
                    "sent": "The shorthand that will use throughout this is.",
                    "label": 0
                },
                {
                    "sent": "We say that F is distributed normal with mean M and covariance K.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as we said before, the loose definition of what a Gaussian processes is a multivariate Gaussian of uncountably infinite length.",
                    "label": 1
                },
                {
                    "sent": "In other words, take that multivariate Gaussian vector and just make it longer and longer and longer an.",
                    "label": 0
                },
                {
                    "sent": "What does that get to?",
                    "label": 0
                },
                {
                    "sent": "That gets to a function?",
                    "label": 0
                },
                {
                    "sent": "That's a very loose definition indeed.",
                    "label": 0
                },
                {
                    "sent": "Here's a slightly more rigorous definition.",
                    "label": 0
                },
                {
                    "sent": "We say that F is a Gaussian process if.",
                    "label": 0
                },
                {
                    "sent": "For any subset index, a set of indexes T if F of T which is which is, which is the function F evaluated at those index point has a multivariate distribution according to normal MFT.",
                    "label": 0
                },
                {
                    "sent": "An CFT.",
                    "label": 0
                },
                {
                    "sent": "Alright, so I'll say that I'm using T here as real numbers for familiarity with regression in time, but the domain can be can be any dimension, any dimension X in Rd, and we will show an example of that later.",
                    "label": 0
                },
                {
                    "sent": "Alright, so I kind of breezed by this fact in this definition.",
                    "label": 0
                },
                {
                    "sent": "What are those functions M and what are those functions K?",
                    "label": 0
                },
                {
                    "sent": "So let's talk about that now, 'cause that's that's an interesting part of what makes a Gaussian process.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the mean function by analogy to that mean vector in the multivariate Gaussian case where we said the mean vector can be just about anything.",
                    "label": 0
                },
                {
                    "sent": "The mean function can be can be any function that Maps.",
                    "label": 1
                },
                {
                    "sent": "Index points T onto real values.",
                    "label": 0
                },
                {
                    "sent": "Often in Gaussian in the Gaussian process literature, because you can mean subtract your data and because it makes notation easier, we'll just we'll just.",
                    "label": 1
                },
                {
                    "sent": "Set set the mean function to 0.",
                    "label": 0
                },
                {
                    "sent": "Because really.",
                    "label": 0
                },
                {
                    "sent": "In the modeling context, what often makes things most interesting is modeling that kernel or covariance function so.",
                    "label": 1
                },
                {
                    "sent": "That that covariance function is is again a function that Maps your input space onto a real value, except it takes a pair of arguments, so it's any valid Mercer kernel.",
                    "label": 0
                },
                {
                    "sent": "So this connects to all the stuff that you've talked about in kernels already.",
                    "label": 1
                },
                {
                    "sent": "And what this is is just any function.",
                    "label": 0
                },
                {
                    "sent": "Any function that has two arguments and that function has to be a positive semidefinite positive semidefinite function.",
                    "label": 0
                },
                {
                    "sent": "In other words, it needs to.",
                    "label": 0
                },
                {
                    "sent": "It needs to obey Mercer's theorem.",
                    "label": 0
                },
                {
                    "sent": "Now Mercer's theorem again is a very rich mathematical theorem from functional analysis, but when it's whittled down to what we care about in this particular case, what it says is if you give any finite or any any.",
                    "label": 0
                },
                {
                    "sent": "Any subset index of T and you evaluate that function into a matrix KTT.",
                    "label": 0
                },
                {
                    "sent": "In other words, take all your time points, evaluate it and build it into this end by end matrix that that matrix K will be positive semidefinite.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to summarize that the GP is fully defined by amine function, Anna kernel function and this requirement that every finite subset of the domain has this multivariate normal distribution, this consistent multivariate normal distribution F according to evaluate the mean function evaluate at those points T and the kernel function evaluated those points T. So a couple notes.",
                    "label": 1
                },
                {
                    "sent": "One, this is something that we can conceptualize pretty easily and say, OK, I've got these two functions, the meenan and the kernel function, and I can evaluate that and that will give me that will give me a mean and covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "Great, and I can stipulate that I want that I want those to always be this to define this Gaussian, but the fact that that should exist as a valid mathematical object is not at all trivial, and Furthermore the fact that this is a full specification.",
                    "label": 0
                },
                {
                    "sent": "Or in other words that you give me up, you give me one M and 1K and that defines uniquely a Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "Is not at all trivial.",
                    "label": 0
                },
                {
                    "sent": "One of the things that's also nontrivial and is quite nice is that most of the interesting properties that were used to in dealing with Gaussian variables and will get into those in a moment that those are all inherited.",
                    "label": 0
                },
                {
                    "sent": "OK, so this kernel function is the only is the only really interesting thing that doesn't sort of slot in seamlessly into what we were talking about with Gaussian, so let's.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's unpack that a bit more.",
                    "label": 0
                },
                {
                    "sent": "So the Canonical example for a kernel function is probably the squared exponential kernel, so I know that looks like a Gaussian, but ignore that for the time being.",
                    "label": 1
                },
                {
                    "sent": "Just consider that just consider that a kernel function of two arguments.",
                    "label": 0
                },
                {
                    "sent": "So what I want to do just to just to make a very, very explicit connection between a kernel function, Anna covariance matrix is evaluate, evaluate this kernel function at a handful of points.",
                    "label": 0
                },
                {
                    "sent": "So to do that, we're going to choose some hyperparameters.",
                    "label": 1
                },
                {
                    "sent": "So you'll notice that I've slipped in.",
                    "label": 0
                },
                {
                    "sent": "I've slipped in a couple new parameters here.",
                    "label": 0
                },
                {
                    "sent": "We call these hyperparameters because they live, they live in the kernel.",
                    "label": 0
                },
                {
                    "sent": "This will use the clicker here.",
                    "label": 0
                },
                {
                    "sent": "So we've got two.",
                    "label": 0
                },
                {
                    "sent": "We've got two hyperparameters here.",
                    "label": 0
                },
                {
                    "sent": "We've got L the characteristic characteristic length scale and we've got Sigma squared F which is, which is the variance of the power of this kernel.",
                    "label": 0
                },
                {
                    "sent": "And So what we're going to do is we're just going to.",
                    "label": 0
                },
                {
                    "sent": "We're just going to evaluate this.",
                    "label": 0
                },
                {
                    "sent": "So let's say I take.",
                    "label": 0
                },
                {
                    "sent": "Three index points in T at 7 AM, 8:00 AM and 10:30.",
                    "label": 0
                },
                {
                    "sent": "These are the three measurement times that we care about now.",
                    "label": 0
                },
                {
                    "sent": "What are we asking?",
                    "label": 0
                },
                {
                    "sent": "We're asking what is the correlation with covariance between?",
                    "label": 0
                },
                {
                    "sent": "Between random variables between my heart rate variable at these times.",
                    "label": 0
                },
                {
                    "sent": "So how do I go about doing that?",
                    "label": 0
                },
                {
                    "sent": "OK, I take this function here.",
                    "label": 0
                },
                {
                    "sent": "I take these pairs of points.",
                    "label": 1
                },
                {
                    "sent": "I evaluated all pairs and I build this into a matrix.",
                    "label": 0
                },
                {
                    "sent": "So what we can then do is we can then change these kernel hyperparameters and see how this covariance matrix changes.",
                    "label": 0
                },
                {
                    "sent": "So what I've done here, I'll just flashback in between that we had a length scale of 100.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we're going to go to a length scale of 500 and what we see is that the diagonal we've still got the same values, but what's happening is as we move away from the diagonal.",
                    "label": 0
                },
                {
                    "sent": "The correlation is falling off much less quickly, so what's this saying?",
                    "label": 0
                },
                {
                    "sent": "With a higher length scale value.",
                    "label": 0
                },
                {
                    "sent": "Between 7:00 and 8:00 AM.",
                    "label": 0
                },
                {
                    "sent": "These variables are highly highly correlated.",
                    "label": 0
                },
                {
                    "sent": "As you get further away to 10:30, this variable is not quite as correlated, but still it's still quite highly correlated.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the other hand, if we make this a smaller number, you see that the correlation drops off very quickly, so this is nearly scaled identity matrix.",
                    "label": 0
                },
                {
                    "sent": "In other words, in other words, my heart rate is 7:00 AM is nearly independent from my heart rate at 8:00 AM.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can also change Sigma F and see how that changes things, so this remember we were putting up that envelope around around the Gaussian mean around the GP mean that Gray envelope.",
                    "label": 0
                },
                {
                    "sent": "So what we're doing here is changing that so.",
                    "label": 0
                },
                {
                    "sent": "We double that the envelope doubles.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So that was just a tie in something something where a lot of people when they're learning Gaussian processes get tripped up, is connecting the kernel function to covariance matrices.",
                    "label": 0
                },
                {
                    "sent": "So, so I'm going to I'm going to repeat that a couple of times, but I think, but I think it's valuable to make that connection.",
                    "label": 0
                },
                {
                    "sent": "Alright, so an intuitive summary of GP so far GP offer distributions over functions.",
                    "label": 1
                },
                {
                    "sent": "And for any finite subset vector we've got this normal distribution, and you see here.",
                    "label": 0
                },
                {
                    "sent": "As promised, I've dropped, I've dropped the mean function.",
                    "label": 0
                },
                {
                    "sent": "And the covariance, the covariance matrix K is calculated by just plugging the T the index points into this kernel function.",
                    "label": 0
                },
                {
                    "sent": "So to introduce some new notation before we were saying F is normally distributed with mean zero and covariance K, you'll often see written F is distributed as a GP with mean function M and covariance funk.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. Alright, so I mentioned that.",
                    "label": 0
                },
                {
                    "sent": "I mentioned that most of the important Gaussian properties that we care about are inherited by by Gaussian processes, and So what I want to do is walk through a couple of properties of the Gaussian that are going to be interesting for today's purposes that are very useful to in the GP context.",
                    "label": 1
                },
                {
                    "sent": "So one is is activity.",
                    "label": 0
                },
                {
                    "sent": "In other words, adding two Gaussians together gives you a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Again.",
                    "label": 0
                },
                {
                    "sent": "That'll be nice to us in forming a joint.",
                    "label": 0
                },
                {
                    "sent": "Conditioning so conditioning on Gaussian random variables.",
                    "label": 0
                },
                {
                    "sent": "This is important for inference.",
                    "label": 0
                },
                {
                    "sent": "The ability to calculate expectations which is going to be interesting for calculating posterior predictive moments.",
                    "label": 0
                },
                {
                    "sent": "And finally, our ability to marginalise out variables that we don't care about.",
                    "label": 0
                },
                {
                    "sent": "So there are many other.",
                    "label": 0
                },
                {
                    "sent": "There are many other nice properties of Gaussian of course, but those are the ones that we want that we really want to care about today.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's first talk about forming a joint jointly Gaussian distribution so.",
                    "label": 0
                },
                {
                    "sent": "I've got some prior.",
                    "label": 0
                },
                {
                    "sent": "I've got some prior on F, so F I'm going to use throughout as our prior.",
                    "label": 0
                },
                {
                    "sent": "I've got some Gaussian prior with MF and covariance KFF.",
                    "label": 0
                },
                {
                    "sent": "I've got some IID noise that I add to that an.",
                    "label": 0
                },
                {
                    "sent": "And then I want to let Y equal F + N. So what's this saying this is saying the underlying function that I care about?",
                    "label": 0
                },
                {
                    "sent": "Regressor that I care about is F An.",
                    "label": 0
                },
                {
                    "sent": "I measured some noisy data observations of that why?",
                    "label": 0
                },
                {
                    "sent": "Which is why.",
                    "label": 0
                },
                {
                    "sent": "Plus this independent noise N. So what's nice about this is that this allows us then to form a joint distribution P of Y&F, and you see that this is again a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So a couple objects that we haven't seen before.",
                    "label": 0
                },
                {
                    "sent": "So KFF we gave to you MF we gave to you and so we say OK what's KFY and what's kyy?",
                    "label": 0
                },
                {
                    "sent": "OK you can just evaluate that out.",
                    "label": 0
                },
                {
                    "sent": "KF why is this expectation?",
                    "label": 0
                },
                {
                    "sent": "And in this case that equals KFF kyy in this case equals KF plus the noise.",
                    "label": 0
                },
                {
                    "sent": "So the nice thing that the nice feature now that we have to connect this back to the regression problem is that the latent F which we care about and the noisy observation Y are jointly Gaussian.",
                    "label": 1
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, wait a second.",
                    "label": 0
                },
                {
                    "sent": "We just did this all with regular multivariate Gaussians.",
                    "label": 0
                },
                {
                    "sent": "So where did the GP go?",
                    "label": 1
                },
                {
                    "sent": "'cause we've just been talking bout GP.",
                    "label": 1
                },
                {
                    "sent": "So the point I want to make here is that if F&Y are indexed by some some input points T. In other words, if MF is actually just some mean function evaluated at these at these endpoints T, these end index points and KFF is of some kernel evaluation.",
                    "label": 0
                },
                {
                    "sent": "Then it could have just as easily written this as a GP prior F and some add an additive noise GPN and use this same additivity property, but here.",
                    "label": 0
                },
                {
                    "sent": "When I wrote this this specific Gaussian, I would have just indexed this Y at T and index this F at T. So this is.",
                    "label": 0
                },
                {
                    "sent": "This is your starting to see one of the really.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nice features of the GP is that is that all we need to do is bring in a finite set of index points and then we're working with multivariate Gaussians.",
                    "label": 1
                },
                {
                    "sent": "So as a warning because of this is that there is some overloaded notation here.",
                    "label": 1
                },
                {
                    "sent": "So F. People, people are generally pretty loose about that notation.",
                    "label": 0
                },
                {
                    "sent": "It can either be infinite.",
                    "label": 1
                },
                {
                    "sent": "In other words, GP right F is GP.",
                    "label": 0
                },
                {
                    "sent": "So now F is this infinite dimensional object or a finite multivariate Gaussian and that can.",
                    "label": 1
                },
                {
                    "sent": "That's generally put pretty clear depending on the context.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, the next property that we care about is conditioning or doing doing Bayesian inference.",
                    "label": 0
                },
                {
                    "sent": "So here we've got our latent F and our noisy observation why, and we know that those are jointly Gaussian and we've got F. And why are distributed according to this distribution.",
                    "label": 1
                },
                {
                    "sent": "So then.",
                    "label": 0
                },
                {
                    "sent": "We can do inference and we can say that the posterior of F given Y is again a normal distribution, so this is an important fact of Gaussian distributions and this is just.",
                    "label": 0
                },
                {
                    "sent": "Stock and trade manipulation of a Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "This is actually proving that this is.",
                    "label": 0
                },
                {
                    "sent": "This is the case is something that I think that everybody should do once and only once.",
                    "label": 0
                },
                {
                    "sent": "Once once you've done it, just just just forget about it, 'cause it's rather tedious, but it's a cool fact to know.",
                    "label": 0
                },
                {
                    "sent": "So a couple things to point out here.",
                    "label": 0
                },
                {
                    "sent": "You see that we've got this mean function.",
                    "label": 0
                },
                {
                    "sent": "Sorry this, this mean an this covariance.",
                    "label": 0
                },
                {
                    "sent": "We can impact this a little bit.",
                    "label": 0
                },
                {
                    "sent": "We should all be pretty familiar with this one.",
                    "label": 0
                },
                {
                    "sent": "Interesting thing here is that you see that this is just a linear function of our observations Y, so that's nice to know.",
                    "label": 0
                },
                {
                    "sent": "And further, this term here KFF is our prior is our prior covariance, so that's the uncertainty that we had about that latent, and you see that we've.",
                    "label": 0
                },
                {
                    "sent": "Subtracted here this other term, and that's essentially how much our data explains about what we know about about our prior.",
                    "label": 0
                },
                {
                    "sent": "So if our data tells us nothing about our prior uncertainty, then this will be a very small term and in other words, our uncertainty is still just around KFF.",
                    "label": 0
                },
                {
                    "sent": "If instead our data tells us a whole bunch, then.",
                    "label": 0
                },
                {
                    "sent": "This this approach is KFF and our uncertainty decreases considerably.",
                    "label": 0
                },
                {
                    "sent": "So the main point of this and you can.",
                    "label": 0
                },
                {
                    "sent": "Not worry about parsing this too much, but rather inference of the latent given the data is simple linear algebra, so we've reduced all of all of the complexity of Bayesian inference and all of the problems that are sometimes associated that associated with that to a simple to a simple set of linear equations.",
                    "label": 1
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, the next feature so we talked about forming a joint.",
                    "label": 0
                },
                {
                    "sent": "We've talked about doing inference so now we can talk about calculating expectations.",
                    "label": 0
                },
                {
                    "sent": "So again this simple term.",
                    "label": 0
                },
                {
                    "sent": "This simple conditioning term gave us this fact and what this allows us to see is that I mean this is repetitive I suppose, but the expectation?",
                    "label": 0
                },
                {
                    "sent": "The expectation of F given Y is simply this mean term, and so that's what is that.",
                    "label": 0
                },
                {
                    "sent": "That's the map estimate.",
                    "label": 0
                },
                {
                    "sent": "That's the posterior mean.",
                    "label": 0
                },
                {
                    "sent": "There are a number of other moments that would be interested in one another.",
                    "label": 0
                },
                {
                    "sent": "You want to bring up.",
                    "label": 0
                },
                {
                    "sent": "So we looked at the posterior moments.",
                    "label": 0
                },
                {
                    "sent": "F and why we've been talking about this joint gaussianity between the latent F and the noisy observation Y.",
                    "label": 0
                },
                {
                    "sent": "Instead, we can look at.",
                    "label": 0
                },
                {
                    "sent": "We can look at why, which is data that we've collected an Y star, which is data that we haven't collected.",
                    "label": 0
                },
                {
                    "sent": "In other words, when I said I want to query on and see what my heart rate is going to be at nine couple minutes after 9:00 AM.",
                    "label": 0
                },
                {
                    "sent": "This is convention in the literature that YY star is a data that you want to predict, so those are also jointly Gaussian.",
                    "label": 0
                },
                {
                    "sent": "And so this is no different, we just use the same.",
                    "label": 0
                },
                {
                    "sent": "We just use the same conditioning property to get to get to this fact and you can see that this is our predictive predictive mean.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The final property that I want that I want to explore is marginalization.",
                    "label": 0
                },
                {
                    "sent": "So again, we have these jointly Gaussian variables.",
                    "label": 0
                },
                {
                    "sent": "We can marginalized out the latent 'cause you say, maybe I don't care about that.",
                    "label": 1
                },
                {
                    "sent": "Maybe I don't care about the latent function at all.",
                    "label": 0
                },
                {
                    "sent": "I just want to know how well this Gaussian process model describes my data.",
                    "label": 0
                },
                {
                    "sent": "So to do that, you just P of Y, you integrate out F and another another nice property of Gaussians is that you can just read that right off of here that why is distributed MY with covariance kyy.",
                    "label": 0
                },
                {
                    "sent": "So this is nice because it gives us the data log likelihood, which then can be useful for model selection, model comparison and things like this.",
                    "label": 0
                },
                {
                    "sent": "Oh, right, so 111 note because we'll come back to this.",
                    "label": 0
                },
                {
                    "sent": "You notice here that when I introduced those hyperparameters that lived up in the kernel.",
                    "label": 0
                },
                {
                    "sent": "Those have been suppressed here, but actually the data is P of Y.",
                    "label": 0
                },
                {
                    "sent": "Given those those kernel hyperparameters and this will be the basis of model selection because we want to tune our data marginal likelihood based on what those hyperparameters settings.",
                    "label": 1
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Things are OK at this point.",
                    "label": 0
                },
                {
                    "sent": "You might be complaining because.",
                    "label": 0
                },
                {
                    "sent": "Because you might say OK, I'm bored.",
                    "label": 1
                },
                {
                    "sent": "All we've done so far is is messed around with Gaussians and I'm familiar with Gaussians and you know, I thought we were coming to talk about infinite dimensional probability distributions and interesting stuff, and so if that's your complaint, you're correct and I'm sorry about that.",
                    "label": 0
                },
                {
                    "sent": "But in fact this is the whole point right?",
                    "label": 1
                },
                {
                    "sent": "The whole point of this is that we take this this beautiful mathematical theory.",
                    "label": 0
                },
                {
                    "sent": "And when it comes down to actually dealing with these objects, it's simple linear algebra.",
                    "label": 1
                },
                {
                    "sent": "It's simple inference on Gaussian distributions and all this, and So what I want I want to convince you of is that even with that sort of banal setup, we can do some really quite remarkable things.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now let's look at some of the.",
                    "label": 0
                },
                {
                    "sent": "Remarkable things.",
                    "label": 0
                },
                {
                    "sent": "I suppose that GP can do.",
                    "label": 0
                },
                {
                    "sent": "We've talked about Gaussians in words and pictures.",
                    "label": 1
                },
                {
                    "sent": "We've talked about some of the equations, so let's talk about using using GPS in a regression context.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Our example model, which we've introduced throughout the course of the equation section an.",
                    "label": 1
                },
                {
                    "sent": "Now we'll see what it can do.",
                    "label": 0
                },
                {
                    "sent": "So say the F is our is our latent.",
                    "label": 0
                },
                {
                    "sent": "It's a GP with zero mean and some some kernel KFF, and the kernel has this form.",
                    "label": 0
                },
                {
                    "sent": "This is the squared exponential.",
                    "label": 0
                },
                {
                    "sent": "When we get to talking about talking about kernels will mess with that, but for now just just just let that be.",
                    "label": 0
                },
                {
                    "sent": "We say that Y given F, this is our noise term, right?",
                    "label": 0
                },
                {
                    "sent": "In other words, if I give you if I give you the latent function value F, the data that I observe is distributed some with some independent noise.",
                    "label": 0
                },
                {
                    "sent": "On top of that, that's got some kernel.",
                    "label": 0
                },
                {
                    "sent": "That's that's this is the.",
                    "label": 0
                },
                {
                    "sent": "This is the white noise kernel and all this is saying is that two different is that two different.",
                    "label": 0
                },
                {
                    "sent": "The noise that I observe the measurement noise that I observe on two different time points is independent.",
                    "label": 0
                },
                {
                    "sent": "What this allows us to do?",
                    "label": 0
                },
                {
                    "sent": "Again because of this additivity property is we can.",
                    "label": 0
                },
                {
                    "sent": "We can add this and see again that why is distributed as a GP with some kernel kyy and you see that these kernel functions add.",
                    "label": 0
                },
                {
                    "sent": "So alright, so now we've got the probabilistic model.",
                    "label": 0
                },
                {
                    "sent": "The distribution fully specified, so let's just fill that in with some of these hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to choose Sigma F = 10, so that's the standard deviation envelope I'm going to choose a characteristic length scale of 50 an A noise, uh, noise power of 1.",
                    "label": 0
                },
                {
                    "sent": "So alright, so let's let's let's look at again of our visual representation of that.",
                    "label": 0
                },
                {
                    "sent": "So this is the prior on F. What this is saying to connect this to the equations is that we've got a mean function of 0, so that mean continues along a zero at all times.",
                    "label": 1
                },
                {
                    "sent": "And we've got Sigma Squared Sigma F of 10.",
                    "label": 0
                },
                {
                    "sent": "And so this is the two standard deviation envelope.",
                    "label": 0
                },
                {
                    "sent": "So now we can go ahead and we can take draws from this Gaussian.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Process.",
                    "label": 0
                },
                {
                    "sent": "So this is a single draw from that prior F the GP.",
                    "label": 0
                },
                {
                    "sent": "And so now you see, hopefully connected to this notion of how we can draw function from from.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "BP.",
                    "label": 0
                },
                {
                    "sent": "So the steps of this should be clear.",
                    "label": 1
                },
                {
                    "sent": "This is the only.",
                    "label": 0
                },
                {
                    "sent": "This is the only code snippet I will give you throughout the course of it, but it's but it's, but it's only.",
                    "label": 0
                },
                {
                    "sent": "It's only one line, and Dylan will unpack that more.",
                    "label": 0
                },
                {
                    "sent": "OK, so how do I get?",
                    "label": 0
                },
                {
                    "sent": "How do I actually get this draw?",
                    "label": 0
                },
                {
                    "sent": "So to do that I take a whole bunch of index points, a finite number.",
                    "label": 0
                },
                {
                    "sent": "So here I took the integer index points between 0 and 500.",
                    "label": 0
                },
                {
                    "sent": "I evaluate that kernel function just like we did with those three index points.",
                    "label": 0
                },
                {
                    "sent": "I evaluate the kernel function and build that into some into some 500 by 500 matrix KFF.",
                    "label": 1
                },
                {
                    "sent": "And then I can take a draw from a Gaussian with zero meenan this covariance?",
                    "label": 0
                },
                {
                    "sent": "So how that's actually done right?",
                    "label": 0
                },
                {
                    "sent": "And this would be your MATLAB code.",
                    "label": 0
                },
                {
                    "sent": "How that's actually done is according to this.",
                    "label": 0
                },
                {
                    "sent": "So that will give you this procedure will give you this nice will give you this nice draw.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so there was one draw in green.",
                    "label": 0
                },
                {
                    "sent": "Now we've taken 4 draws of that and I'm belaboring this point just so that just so that we remind ourselves that a draw from a Gaussian process gives you a function.",
                    "label": 0
                },
                {
                    "sent": "So four draws from that will give you these nice four.",
                    "label": 1
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Functions.",
                    "label": 0
                },
                {
                    "sent": "Before, when we were evaluating the kernel matrix, we mess around with the hyperparameters a little bit to see how that changed.",
                    "label": 0
                },
                {
                    "sent": "To see how that changed the covariance matrix, let's do that here in pictures.",
                    "label": 0
                },
                {
                    "sent": "So here is Sigma F of 10 and the length scale of 50.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If I change I'm going to leave that lengthscale of 50, but now I've changed the power of that.",
                    "label": 0
                },
                {
                    "sent": "The envelope of that from 10 down to four and you see what's happened is that the.",
                    "label": 0
                },
                {
                    "sent": "Envelope has shrunk and the draws have shrunk.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not surprising.",
                    "label": 0
                },
                {
                    "sent": "If I change, if I change the length scale of 50 to a length scale of 10, right?",
                    "label": 0
                },
                {
                    "sent": "So what this is saying is?",
                    "label": 0
                },
                {
                    "sent": "As two points get further apart, they fall off.",
                    "label": 0
                },
                {
                    "sent": "Their correlation falls off more quickly, so accordingly you get more wiggly draws.",
                    "label": 0
                },
                {
                    "sent": "One final point on this is this.",
                    "label": 0
                },
                {
                    "sent": "This should feel a whole lot like when we were evaluating those those covariance matrices.",
                    "label": 0
                },
                {
                    "sent": "In other words, this is a a closer to the identity matrix.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Variance matrix is close to the identity matrix.",
                    "label": 0
                },
                {
                    "sent": "Then for example this which has much longer range correlations.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I mentioned that we've been looking at regression in time and taking draws that are nice temporal functions.",
                    "label": 0
                },
                {
                    "sent": "Those are easy to look at and familiar to us.",
                    "label": 0
                },
                {
                    "sent": "You can also have multidimensional input.",
                    "label": 0
                },
                {
                    "sent": "So one of the one of the histories of the application areas where Gaussian process even used a lot is in geostatistics, and there they are often interested in spatial Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "So for example, latitude in Lanja tude instead of regression.",
                    "label": 0
                },
                {
                    "sent": "In time, you want to regress on latitude and longitude.",
                    "label": 0
                },
                {
                    "sent": "So to do that, we can make.",
                    "label": 0
                },
                {
                    "sent": "We can make each input instead of instead of a single real real valued number of pair of numbers.",
                    "label": 0
                },
                {
                    "sent": "So now we've got some Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "Here F is the same GP and the kernel is almost exactly the same, except here there's just an extra term.",
                    "label": 0
                },
                {
                    "sent": "There's a.",
                    "label": 0
                },
                {
                    "sent": "There's a squared term for each dimension of that Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "So what might draw from that look like instead of one single function?",
                    "label": 0
                },
                {
                    "sent": "What you get now is a field over over Latitude in lanja tude, so this is your random function that's drawn over that field.",
                    "label": 0
                },
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "I suppose shameless plug if you are staying for AI stats and you want to see a multidimensional GP in action we've got.",
                    "label": 1
                },
                {
                    "sent": "We've got a paper on that and I'm sure there will be other papers.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "GPS as well.",
                    "label": 0
                },
                {
                    "sent": "OK, so we've got the same model that we've just been dealing with, and now let's gather some data.",
                    "label": 0
                },
                {
                    "sent": "So this is this is our model the GP model?",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We've got our prior.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we can go in and take.",
                    "label": 0
                },
                {
                    "sent": "We can go in and take a data point.",
                    "label": 0
                },
                {
                    "sent": "So I want to say I gather a data point at time to 204.",
                    "label": 0
                },
                {
                    "sent": "So what I know is that I can evaluate Y of 204 and I know that this is according to the model.",
                    "label": 0
                },
                {
                    "sent": "This is Gaussian distributed according to mean Zero and Kyy evaluated at 204, 204.",
                    "label": 0
                },
                {
                    "sent": "That's a simple univariate Gaussian.",
                    "label": 0
                },
                {
                    "sent": "This is all just pulling this right out of right out of the definition of of the GP Y and I'm just evaluating the kernel matrix.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So then we can use conditioning to update the posterior.",
                    "label": 1
                },
                {
                    "sent": "So here I've still got the prior P of F. But now I've got this data observation so I can use.",
                    "label": 0
                },
                {
                    "sent": "I can use the inference rule that we talked about and evaluate why to take this data point Y at 204 and run this through the equation.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what does this give me?",
                    "label": 0
                },
                {
                    "sent": "Well, now this is refined.",
                    "label": 0
                },
                {
                    "sent": "My posterior estimate of F. So this is no longer a flat mean function.",
                    "label": 0
                },
                {
                    "sent": "But I think the main function comes down here and goes through F. And Furthermore, because of our choice of noise parameters, you can see that what's happened here is right around 204.",
                    "label": 0
                },
                {
                    "sent": "I'm awfully sure my.",
                    "label": 0
                },
                {
                    "sent": "Covariance envelope has collapsed, and I'm awfully sure that that the measurement is around there, but as soon as I get further away, well, I forget because because I don't think that what happens at 400 is particularly related to what happens in two or four.",
                    "label": 0
                },
                {
                    "sent": "So by the time I get over here, I'm basically back to the prior.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So a small change is that.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here we were looking at the posterior this this I think is important is important to hang on just for a second because often when you look at GP work, it's not clear whether people are talking about whether they're showing the posterior or they're showing a predictive model, so.",
                    "label": 0
                },
                {
                    "sent": "This is on the.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Steri are and this here is the predictive the predictive distribution.",
                    "label": 0
                },
                {
                    "sent": "So you'll just notice there's a very small change here, but this variance envelope has just increased slightly ever so slightly, because we think that on top of that, on top of that, posterior is actually some measurement noise.",
                    "label": 0
                },
                {
                    "sent": "So this is our belief about the predictive distribution.",
                    "label": 1
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so then I had.",
                    "label": 0
                },
                {
                    "sent": "I had .204 let's say I also gather, uh, another data point at night.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Andy.",
                    "label": 0
                },
                {
                    "sent": "Well then I can add that in the same way to this distribution.",
                    "label": 0
                },
                {
                    "sent": "So and you see what's happened here is I've added this data point at 90 and the same thing has happened.",
                    "label": 0
                },
                {
                    "sent": "The variance envelope has decreased.",
                    "label": 0
                },
                {
                    "sent": "The mean function is chain.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Changed I can do this an add more and more.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "ETA points as I go.",
                    "label": 0
                },
                {
                    "sent": "And this is what we've got to.",
                    "label": 0
                },
                {
                    "sent": "And this is, again, the predictive.",
                    "label": 0
                },
                {
                    "sent": "This is again the predictive distribution.",
                    "label": 0
                },
                {
                    "sent": "It's Gaussian and all I'm doing is adding more and more more.",
                    "label": 0
                },
                {
                    "sent": "I'm making this vector here.",
                    "label": 0
                },
                {
                    "sent": "What I've observed longer and longer.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this gets us to this guy.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This to a question which is alright, so I get more and more data.",
                    "label": 0
                },
                {
                    "sent": "But when am I getting to my actual regression function 'cause I can see that we're doing regression here.",
                    "label": 0
                },
                {
                    "sent": "But when am I going to produce the parameters of my model?",
                    "label": 0
                },
                {
                    "sent": "But of course this is a non pair.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Metric regression model.",
                    "label": 0
                },
                {
                    "sent": "So this is one of the virtues of Gaussian processes that we're not going to just spit out a couple a couple parameters of a quadratic function.",
                    "label": 0
                },
                {
                    "sent": "But rather as we, as we gather all our data, the GP regression gets more and more refined.",
                    "label": 0
                },
                {
                    "sent": "So this is one of the benefits of GP regression, which is it?",
                    "label": 0
                },
                {
                    "sent": "Let's let's the data speak for itself.",
                    "label": 1
                },
                {
                    "sent": "I suppose the downside of that is that all the data, all the data must speak.",
                    "label": 0
                },
                {
                    "sent": "So you see that as this Y grows and we collect more and more data we're doing, we're doing a larger and larger problem here with inverting kyy doing this, this nice piece of simple linear algebra.",
                    "label": 0
                },
                {
                    "sent": "So you'll often hear you'll often hear this this comment, which is nonparametric models, have an infinite number of parameters.",
                    "label": 1
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "I'd like to refine this slightly in our minds, the way we think about this and not say that nonparametric models have an infinite number of parameters, but rather nonparametric models have a finite but unbounded number of parameters, and that number of parameters grows with the data.",
                    "label": 1
                },
                {
                    "sent": "So the way you can think about it here is that as this gets larger and larger and tends towards infinite, yes, we have.",
                    "label": 0
                },
                {
                    "sent": "We have an unbounded number of parameters that can describe what our prediction is going to be, but that's still just growing finitely with the amount of data that we.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So we are almost were almost through the basics.",
                    "label": 0
                },
                {
                    "sent": "There's one more piece of the basics in Gaussian processes and using Gaussian processes for regression that I want to talk about.",
                    "label": 1
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And that is model selection or hyperparameter learning.",
                    "label": 1
                },
                {
                    "sent": "So all throughout the course of this, we've been adding these data points looking at the predictive distribution.",
                    "label": 0
                },
                {
                    "sent": "Looking at posterior inference.",
                    "label": 1
                },
                {
                    "sent": "And we've been doing this with a fixed with a fixed model F. This Gaussian process with KFF according to this function, so.",
                    "label": 0
                },
                {
                    "sent": "I want to talk about these hyperparameters now L and Sigma squared F. So here we've got L E = 50 and we've seen how changing that can change the fit that we get.",
                    "label": 0
                },
                {
                    "sent": "So if I make L quite a bit smaller, you can see that now L is.",
                    "label": 0
                },
                {
                    "sent": "Sorry that this GP is overfitting the data.",
                    "label": 0
                },
                {
                    "sent": "In other words, it's chasing.",
                    "label": 0
                },
                {
                    "sent": "It's chasing each individual wiggle of these of these data points.",
                    "label": 0
                },
                {
                    "sent": "It's recurring.",
                    "label": 0
                },
                {
                    "sent": "It's forgetting very quickly.",
                    "label": 0
                },
                {
                    "sent": "Probably too quickly, such that really it says here that we know effectively nothing about about about about our inference on this data point, when in fact, given this is probably a better description, we believe that the function should be around here.",
                    "label": 0
                },
                {
                    "sent": "Again, if we have a length scale that's too high, then we're underfitting the data.",
                    "label": 0
                },
                {
                    "sent": "So here we've got some very confident prediction, but it's probably missing some of the interesting structure that exists in this data.",
                    "label": 0
                },
                {
                    "sent": "So the question that we want to talk about right now and what we want to address is how can we?",
                    "label": 0
                },
                {
                    "sent": "How can we tune or integrate over these hyperparameters L and Sigma squared F so that we get so that we take our data and we get to this model which is just right?",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's two popular ways to do that.",
                    "label": 0
                },
                {
                    "sent": "The first of which is to use the marginal likelihood.",
                    "label": 0
                },
                {
                    "sent": "We can marginalized out the latent function F, and we can just look at the marginal likelihood of the data Y.",
                    "label": 0
                },
                {
                    "sent": "And again, I said that this is actually hiding these extra parameters, which are now the parameters that we care about these hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "We want to model selection on these hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "So just looking at this, and particularly when you talk to people that are outside of Bayesian machine learning, it's quite common that people say it's not obvious why this should do model selection automatically, and that this shouldn't overfit or underfit the data, but it's right in the math, so let's unpack that for a moment.",
                    "label": 1
                },
                {
                    "sent": "So here is the log marginal likelihood of the data.",
                    "label": 0
                },
                {
                    "sent": "And you can see that this has three terms.",
                    "label": 0
                },
                {
                    "sent": "OK, let's ignore this one.",
                    "label": 0
                },
                {
                    "sent": "This is just a normalizing constant.",
                    "label": 0
                },
                {
                    "sent": "So what are these these two terms do?",
                    "label": 0
                },
                {
                    "sent": "This term typically is called a data fit term, and this term is often called a complexity penalty.",
                    "label": 0
                },
                {
                    "sent": "So what you can see here.",
                    "label": 0
                },
                {
                    "sent": "Is that if we just consider Sigma F right?",
                    "label": 0
                },
                {
                    "sent": "So that's the envelope?",
                    "label": 1
                },
                {
                    "sent": "That's the envelope of the Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "So as Sigma F gets larger.",
                    "label": 0
                },
                {
                    "sent": "This will fit the data better.",
                    "label": 0
                },
                {
                    "sent": "In other words, more data gets more data gets inside that that envelope.",
                    "label": 0
                },
                {
                    "sent": "But as that happens, as Sigma F gets larger, this term also scales up.",
                    "label": 0
                },
                {
                    "sent": "So you pay a penalty here.",
                    "label": 0
                },
                {
                    "sent": "So these two terms these two terms, your complexity in your data fit, are at odds with one another, and that gives you this, this this.",
                    "label": 0
                },
                {
                    "sent": "Automatic determination of overfitting versus underfitting, which happens in a Bayesian model.",
                    "label": 0
                },
                {
                    "sent": "Unpacking this for the length scale is just a little bit trickier, but again, is simple linear algebra and comes to the volume of this volume of this ellipsoid, which gets larger.",
                    "label": 0
                },
                {
                    "sent": "This gets larger as you get closer to White.",
                    "label": 0
                },
                {
                    "sent": "In other words, you get a shorter, shorter length scale and that opposes in the data fit term as well, so that's.",
                    "label": 0
                },
                {
                    "sent": "That's worth spending some time unpacking on your own.",
                    "label": 0
                },
                {
                    "sent": "This is why you hear the term Bayesian Occam's Razor or Occam's razors is implemented via this regularization, or this Bayesian model selection, because there is this.",
                    "label": 0
                },
                {
                    "sent": "At this data fit term here, and you've got this automatic regularizer here, which discourages overcomplex models.",
                    "label": 1
                },
                {
                    "sent": "So the details of this dealing with model selection will be will be fleshed out some in the practical.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another way to do this, and this falls outside of the this falls outside of the Bayesian context I supposed, but one can also use a cross validation cross validation approach, which is quite popular machine learning.",
                    "label": 0
                },
                {
                    "sent": "So instead of considering the marginal likelihood, we can consider the predictive.",
                    "label": 0
                },
                {
                    "sent": "Predictive distribution for some held out data.",
                    "label": 1
                },
                {
                    "sent": "So here I'll call this the predictive log likelihood.",
                    "label": 0
                },
                {
                    "sent": "So here I've hold out some, hold out some test data, and as we know this is again a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "P of Y test given why train?",
                    "label": 1
                },
                {
                    "sent": "So it's again a Gaussian, so you can use this.",
                    "label": 1
                },
                {
                    "sent": "You can take derivatives on this and you can tune model hyperparameters in the same way that you do in any cross validation approach.",
                    "label": 0
                },
                {
                    "sent": "And again, that will be.",
                    "label": 0
                },
                {
                    "sent": "Model selection will be dealt with in the practical.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so that gets through the basics of what I want to talk about with GPS and words and pictures, GPS and equations and then using GPS in a basic regression context.",
                    "label": 1
                },
                {
                    "sent": "We're just just a few minutes ahead of time, so why don't we take a few questions?",
                    "label": 0
                },
                {
                    "sent": "Nice code with the heart rate example.",
                    "label": 0
                },
                {
                    "sent": "Is it possible to?",
                    "label": 0
                },
                {
                    "sent": "Is there a nice cattle will give you kind of periodic.",
                    "label": 1
                },
                {
                    "sent": "Yeah so we will get to that in kernel and kernel choices will talk about a periodic, the periodic function exactly.",
                    "label": 0
                },
                {
                    "sent": "Yeah so so that kernel that we've chosen the squared exponential kernel is just a nice and Canonical choice that gives you smooth functions overtime.",
                    "label": 0
                },
                {
                    "sent": "We can and will mess with that as soon as as soon as we go to the next section.",
                    "label": 0
                },
                {
                    "sent": "After no other questions, why don't we take one?",
                    "label": 0
                },
                {
                    "sent": "We take a five minute, 5 minute breather and then we'll come back and we'll get into some more interesting details.",
                    "label": 0
                }
            ]
        }
    }
}