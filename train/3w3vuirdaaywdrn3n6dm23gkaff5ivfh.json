{
    "id": "3w3vuirdaaywdrn3n6dm23gkaff5ivfh",
    "title": "Large-Scale Object Recognition Systems",
    "info": {
        "author": [
            "Cordelia Schmid, INRIA - The French National Institute for Research in Computer Science and Control"
        ],
        "published": "Dec. 5, 2008",
        "recorded": "November 2008",
        "category": [
            "Top->Computer Science->Computer Vision"
        ]
    },
    "url": "http://videolectures.net/etvc08_schmid_lsors/",
    "segmentation": [
        [
            "I will present my talk on large scale image search.",
            "This is trying to work with me.",
            "She Guimet his shoes."
        ],
        [
            "So what's the goal?",
            "The goal is given the query image and large set of images, for example 1,000,000 images, you want to find similar images in this set and this as fast as possible and so what's the problem?",
            "Each image here is described by approximately the top 2000 descriptors and this makes it 2 billion descriptors to index.",
            "And obviously that's much, much larger.",
            "Too large numbers to store it directly in rum, and so basically we have to find a solution to reduce the storage and the."
        ],
        [
            "And make this search efficient.",
            "And so our approach builds up on the bag of words approach, which has been proposed by civilian scissorman.",
            "So the idea is the following.",
            "Given the query image, you first extract a set of invariant F An invariant descriptors.",
            "So bitter based on a set of FN invariant regions described by the SIFT descriptor.",
            "So then you have each image described by, say, between 1000 and 2000 local descriptors.",
            "So local index vectors of dimension 100.",
            "So you have a set of vectors of dimension 100.",
            "Around 1000, then you vector quantized them.",
            "So build the visual vocabulary based on.",
            "For example, K means clustering, which gives you a vocabulary you sign each of the descriptors in and in a given image to one of the visual words and this gives you the bag of features representation.",
            "So forgiven image you have one bag of features for each feature.",
            "Each feature in the image is signed to one visual words histogram over visual words which we call bag of features in the following and then we can use this structure for querying.",
            "Can use an inverted file system.",
            "So for each entry in the bag of features you have a list of images to which this word corresponds, which allows you to search quickly.",
            "This gives you rank shortlist on top of this we can add an algorithm for geometric verification.",
            "So this is the overall structure and our contribution comes in and two places.",
            "The first one is to improve the bag of features representation to make the search of the descriptors more approximate, more precise, and the second one comes in.",
            "The Jeremy created verification.",
            "So instead of verifying only a short list of images, we propose an approach which allows us to verify each of the images with the geometric consistency."
        ],
        [
            "Constraint."
        ],
        [
            "So, briefly, what is the bag of features approach?",
            "It's just another yet another approximate nearest neighbors search.",
            "So basically, if you want to search for the closest descriptors in the data set, we could just search for the K nearest neighbor of a descriptor.",
            "Of course, if you have two billion of descriptors, this is prohibitively slow and even approximate nearest neighbor searches such as LSH is too slow and too memory consuming, and so if we use the bag of features approach, what we do is we vector quantized the descriptors right?",
            "And so the matching is described by the comparison of the vector quantized features.",
            "So you find each descriptor to visual word, little bird, one little word.",
            "One it's the same visual word.",
            "The descriptors are said to match.",
            "Of course, this is an approximation, and if you look."
        ],
        [
            "If you evaluate this approximation, so how do we can evaluate it?",
            "We look at the accuracy.",
            "So basically, which shows us if the matching descriptor is among the nearest neighbors and we can look at how many achieved vectors are in."
        ],
        [
            "A short list OK, and So what does this show show us?",
            "We can see that depending on the size of the vocabulary, right?",
            "If we have 100 words now visual vocabulary, we can see that we retrieve a large proportion of the descriptors and our accuracy.",
            "Our recall is very good, so we find most of the matching descriptors how.",
            "However, we also find a lot of chunk right and ideally what we would want is to be very low here.",
            "And very high up there.",
            "So basically, but we can see here is a trade off.",
            "If we increase the size of the visual vocabulary you can see that we've retrieve less chunk points, so the rate of points, overall rate of points achieved is lower, but we also eliminate good matches, right?",
            "So we can really see this is a trade off if you have a large vocabulary, retrieve many points and most of the points which you want to retrieve are among the points.",
            "If we have a large vocabulary.",
            "Then we retrieve notice not many chunk points, but there are a lot of."
        ],
        [
            "Scriptures which we made."
        ],
        [
            "OK, so it's just here for small visual dictionary you have too many false matches for lateral discrete.",
            "Two matches are missed OK and there is no no tradeoff between large and small either.",
            "This devoid cells are too big and or this descriptive cells can't descriptor descriptor noise, so we have a descriptor it will just fall in the neighboring cell."
        ],
        [
            "And so there OK just here in the stration.",
            "So we have 20 K visual words.",
            "You can see that there are quite a lot of false matches.",
            "So here are the matching is just done based on the assignment with the visual closest visual words, we can see that there are many false matches here."
        ],
        [
            "If we go to two 200,000 visual words, we can see that many good matches are missed here."
        ],
        [
            "And there are several recent approaches who fight against this description, or this quantization noise while approaches multiple or soft assignment of the descriptors or other approaches compare distribution of the descriptors based on, for example, Fisher kernels in the context of Image Cat."
        ],
        [
            "For isation OK, and so the approach.",
            "The proposed here is based.",
            "On be take the cells right, but in the sales we just.",
            "We also have a representation, a quantization within the cells.",
            "So we assign refers to script, assign the description, the cell and then we have a binary of short vector which quantizes the position over.",
            "This describes the position of the descriptor within the cell.",
            "Right until basically perform vector quantization as in the standard bag of features, and then we have a short binary vector which which allows an additional localization within the cell.",
            "So we have the advantage of this course quantization, but at the same time we search more precisely within a cell, and then we have two 2 descriptors match if they fall in the same cell, and if they look if they fall into the same region within the cell.",
            "So basically.",
            "This binary vectors divide the cell in different Subs subparts and we declare that two descriptors match if they fall into the same subregion up to some some thresholding, and so the prescript is out.",
            "His binary descriptors are compared with the Hamming distance.",
            "It's approximation for the Euclidean distance, and it allows us to reduce the dimension curse effect.",
            "And what's actually interesting, it's very efficient.",
            "This search plus the Hamming distance.",
            "Can be quoted in very few bits and it loads of very few operations for comparison and it reduces also the the memory access.",
            "So actually in the end is 3 times faster than the."
        ],
        [
            "About the features approach and so it's interesting.",
            "How do we obtain?",
            "This hamming embedding, so first of all we have to.",
            "We have to draw.",
            "Son.",
            "You have to define the projection directions.",
            "So here what we use.",
            "We can either learn them from the data or we can just use.",
            "Randomly chosen orthogonal projections.",
            "We then use this matrices to project our descriptor vectors down into some in this representative space.",
            "We determine the median of the descriptors which lie in each cell.",
            "Use this as the point for comparison and then depending of if the point lies on the left or the right, is bigger or smaller than the median value decided to zero or one.",
            "OK, so this allows us to obtain binary coding of the vectors within the cell, and these vectors can then be compared with the Hamming distance.",
            "We can again use a threshold.",
            "So basically you can say they're up to some threshold similar so they don't have to follow it.",
            "You have to find to exactly the same cell, but they can fall into similar.",
            "I mean parts which are similar right?",
            "And then for online computation computes the binary signature of given descriptor vector by projecting into.",
            "Our code is coordinate system and assigning one if it's above the median value.",
            "Another way zero?",
            "OK, so now we have representation within this space."
        ],
        [
            "And we can look at the tradeoff between.",
            "Memory use it to Nick Sicura.",
            "See and hear what we do is like.",
            "We vary the number of bits.",
            "Write the number of projection directions so the maximum would be a 228 which is the maximum size of our descriptor which is the size of our descriptor and date would be very very very small representation right?",
            "And what we can see here.",
            "Is the rate of the cell points retrieved and hear the rate of the five nearest neighbors retrieved and we can actually see that.",
            "Obviously, the more the more dimensions you used, the better results get.",
            "They can also see that even if their presentation is pretty small, it's much better than arbitrary filtering, so we do actually really improve the matching accuracy of ourselves.",
            "So basically this would be just the mixture accuracy of the cells and then by adding this Hamming distance on top of it really improved.",
            "The accuracy of our matching so you know what we're doing now.",
            "Be getting much closer to a matching were really retrieve the closest descriptors directly from the set of descriptors and so depending on if you want to favor memory or accuracy, you can.",
            "We can either use lower or higher dimensions where we used for our experiments.",
            "We use forty 6864 bits."
        ],
        [
            "As the representation, and if you compare it here with the code that you have initially seen for the K nearest neighbor for the K. For the vocabulary based on K means, we can see here.",
            "So basically in red it's the curve obtained by the bag of words and the blue ones are by adding the Hamming Hamming embedding right and what's changed on this blue curves is the distance.",
            "So if the distance is the full number of differences, we obtained the same results just obtained the results.",
            "Of the points which fall into the cell without filtering and then then by making the distance smaller and smaller, we filter out more and more points.",
            "As we can see here.",
            "And if you look at the curves, for example, if you look at the curve K equals at the point K = 100.",
            "If we look at the blue curve, we can see that for for the same level of recall.",
            "You can retrieve 10 times less points, right?",
            "So we really improve the performance of our system.",
            "So it really performs provides a much better tradeoff between recall an ambiguity approval, and what's really interesting is that it's for a very low cost, so all you have to do is to store this additional binary final signatures for descriptor and compare them based on the bit comparison.",
            "Basic was each of the Mr."
        ],
        [
            "Sorta steel ones.",
            "And if you look at an example you can see here.",
            "OK, this is this is the result obtained with the Hamming embedding for a vocabulary of 20,000."
        ],
        [
            "We're all words, and here is out there having embedding and you can really see that."
        ],
        [
            "By adding the Hamming embedding.",
            "It allows to remove false script false matches without removing correct?"
        ],
        [
            "That's right, so these ones are the ones obtained just by signing the Scriptures to the visual words and these ones."
        ],
        [
            "The ones obtained if you add the distance comparison based on the heading distance."
        ],
        [
            "And another example here again, that's the same exam."
        ],
        [
            "If you look at the much larger visual vocabulary, so here 200,000 visual words, you can see that actually."
        ],
        [
            "Paired to the one which you obtained with Hemming and bedding, you don't remove it."
        ],
        [
            "May descriptors right if you increase the size of the vocabulary, remove more and more good match."
        ],
        [
            "OK, and so the second contribution is a week to metric consistency."
        ],
        [
            "Constrained so state of the art systems they are based on.",
            "Re ranking with a full geometric verification.",
            "So what you do once the system has returned a short list of images is take for example say the 100 best retrieved images on the verify the creature magic consistency between the query image and the returned images, for example by estimating a homography based on the matches between the two images, right?",
            "And this this is pretty costly so you could definitely not do it for a million images people.",
            "Add internal to it 400, but you could also go say for up to 1000.",
            "However, if you look at the graph here, so when you look at what you see here on the X axis, you see the size of the data set right?",
            "And the more the data set increases, the more the more the rate of relevant images in the shortlist decreases, right?",
            "So you can see here if you have 1 million of images, the images which are in the shortlist say here.",
            "If you take a short list of 100 images there, only 20% of the correct return images in the short list.",
            "So basically here.",
            "Even if you use the geometric verification, there will be many miss images missing.",
            "So basically the ones which are in here we can move them up in the in the short list ranking, but all the others you won't find them.",
            "And even if you go up to 1000 images you can still see that only 50% of the images have been returned, so many of them are missing, right?",
            "So you have to know what you thought about how can you improve this without going up to the full dramatic verification.",
            "So the idea."
        ],
        [
            "Is to introduce some very simple Victor metric consistency check, which can be implemented for all images and not transfer.",
            "Shortlist all images.",
            "So implemented really for the millions of images.",
            "And So what is our approach based on?",
            "So you have seen initially that we extract a set of invariant image regions, right?",
            "So for each region we have for example we have an F and transformation a scale in the rotation angle associated.",
            "So here we can see.",
            "Actually, the size of the region is the scale and the rotation angle is.",
            "There is some some angle which gives the rotation of the image Patch here what we use is the characteristic scale, which is the scale peak in the scale domain and dominant gradient orientation.",
            "The different ways of estimating actually the scale and rotation characteristics of a Patch.",
            "And then if you have a matching pair right.",
            "So for example, you can see these two image patches are similar, so they have similar descriptors.",
            "They will match.",
            "And for the ones for the matching descriptor, you also have a size of the region and a dominant rotation angle, right?",
            "So once you have this pair, what you can do, you can determine the scale change between the two patches and the rotation change between the two 2 patches.",
            "So here will be a scale change of murder.",
            "Two half and rotation angle of 20 degrees.",
            "So each matching pair descriptor pair results in a scale and angle difference.",
            "And the assumption we make here, which is not.",
            "Which is not exact, but it's an approximation.",
            "Is that the global image rotation changes are roughly consistent, so it holds exactly if you have a scale change between the images, so the image patches they will have the same scale change between them.",
            "The matching image patches will have the scale same triskel times between them.",
            "It doesn't hold if you have, for example, a viewpoint change, then it will be only approximately correct.",
            "But in case of image rotation, image scale scanned is exactly correct.",
            "In case of perspective, viewpoint changes is rough."
        ],
        [
            "Exclamation.",
            "OK, and here an example for the orientation consistency so on.",
            "On the left you have the query image on the right image which has been stored in the database and you can see for each point we show the rotation the angle in the rotation difference between the patches right?",
            "And you can see here most of them are consistent, but if you look closely there are a few outliers for example.",
            "This one up here in the top right, which is much larger, is not the same orientation.",
            "And if you quantize this orientation differences, we can see here, there is a maximum, and this is actually the rotation angle between the two images.",
            "So if you see a look here, this is the maximum for these pair of images.",
            "You look here, it's pay \u03c0 / 4, so we have approximately rotation angle of 45 degrees between the two images."
        ],
        [
            "Is found correctly.",
            "Another example for the scale consistency we can see here 2 images to different images of the Eiffel Tower.",
            "There's approximately scale change of 1/2 between the two of them, and you can see here we show the matching matching regions.",
            "Again, take the difference of the characteristic scales and you can see here that you find the peak here at approximately 1/2 right?",
            "So this allows us to filter the matches not only on their description, but also on their geometric consistency.",
            "And so if you keep this."
        ],
        [
            "This histograms in mind.",
            "These are the histograms used for the voting, so voting is not only performed per image, but it's also performed post angle and scale difference.",
            "We have seen their hopes up that this steps substances are independent, so their rotation and scale, which is occurs between 2 images independent and we then obtain a score for all quantized quite contest angles and scale difference for each image and the final score is then filtering for each of those two parameters and we take them in between them.",
            "This allows us to obtain matches that agrees with the main difference in orientation and scale.",
            "This will be taken into account for the final score, so if we have many matches but none of them agrees in the scale of the orientation, there will be all filtered out.",
            "On the other hand, if they agree, they give a peak which is much higher, so in the end we have voting score match, which is much higher.",
            "And what an interesting question, which we then can ask is, do we still?",
            "Getting further improvement by using the full geometric ranking or not so ideally I would say yes, will show this in our expandable results.",
            "Ideally the answer would be yes, because for now we just have done a very weak traumatic verification.",
            "However, if you put the full geometric model on top of that, we will obtain an additional verification."
        ],
        [
            "And then something which is interesting as well is actually if you look at the the angle differences for our images, they can observe that what would you would think anyways, for natural collection of natural images, most of the images are taken at the same angle.",
            "Then there's a peak at \u03c0 and \u03c0 / 2, and so we can use this as a prior for waiting our angle difference, which shows it has been shown to further."
        ],
        [
            "The results and for our experimental evaluation we have created the data set for evaluation which contains the 1000 images, 500 query images 991 annotated true positives.",
            "We have a 1,000,000 discount tractor images which we have obtained from Flickr.",
            "We have constructed vocabulary on an independent data set also obtained from Flickr, and our search system is almost real time as I'll show in the demo in a minute and so how do we allocate our results?",
            "There is outside evaluated based on mean average precision, which means the bigger the better, right?",
            "Zero will be no, no, no precision and one will be a very higher precision.",
            "So basically average precision is the precision recall curve.",
            "So if you have a recall one precision, one that's the best curve we can obtain."
        ],
        [
            "Average or precision recall curve.",
            "If you samples of our."
        ],
        [
            "Dataset.",
            "And then a few images, image sequences which are in the data set.",
            "So on the left it's the query images and on the right that's all the images which have which we have stored in this million images and which would like to retrieve."
        ],
        [
            "Another example boasting."
        ],
        [
            "Venice and a few examples of the distractor images from Flickr."
        ],
        [
            "And here is the evaluation for our holiday data set.",
            "And in blue we show the baseline, so just obtained with a bag of visual words.",
            "And then we can see the different variations, so only using the geometry improved results only using the Hamming embedding with improved results.",
            "And if you combine two of them together, we further improve the results and.",
            "And you can see that the improvement is actually substantial.",
            "So if you look at a data set of 1,000,000 images here we have a mean average precision of 0, three and then by just adding Victor American Hamming insistence he double.",
            "I mean average precision.",
            "And then what I said before.",
            "Actually if we add this re ranking step we can see that it allows us to further improve the results, right?",
            "So actually what this week too much, even the Hamming embedding does.",
            "It gives us back much more true positives in the short list.",
            "And if you apply the re ranking, we can still further improve the results.",
            "So really the two of them being geometry in full geometry, their complementary when is really weak filter which improves the results in the second one really verifies the results and so based on this re ranking most of the results are actually correct.",
            "The ones which have been found their correct and what we can see here as well, which is interesting if you look at the CPU which is which is consumed.",
            "So there is this is for computation of the transcripts and quantization and then if you look at the search.",
            "And for searching, we can see that the overall search time for Hemming and adding and geometry consistency is compareable to the baseline and so actually I had before the Hamming embedding allows to find much less descriptors and therefore less memory accesses.",
            "So it really reduces the search time.",
            "The geometry victory constrained increases it, but both in together allow us to have the same same search time as before.",
            "So it's really it's feasible to use this."
        ],
        [
            "Rota so time consuming.",
            "And here a real example.",
            "So this is the query and this is what the first 5 results which we system returns.",
            "And this is actually nice.",
            "I like this example because it doesn't only give back the images of which we have stored explicitly in the data set, but also find similar images in Flickr, so figured as extras images which are similar to the query."
        ],
        [
            "And it finds it automatically.",
            "And if you compare our approach to the bag of features, we can see.",
            "So in green.",
            "The ranking we obtained for the images with our approach.",
            "So the first, 4th and 5th.",
            "So these are the ones which we have in our data set.",
            "And then if you use just the standard bag of features, we can actually see that there really far behind.",
            "So this is precision 5800.",
            "So basically even with geometric Re ranking would be never moved to that."
        ],
        [
            "Up and similar for the last time, Marco so we don't find all of the images here.",
            "It's a formal freak alright.",
            "We don't find all of the images which you have put into our data set, but only a subset of them.",
            "So here for in the first 6 retrieved images and it's nice.",
            "Here again, we find also images from Flickr will show which correspond to the query.",
            "And so ideally would now we want to demo, because because I've just switched laptops, I'll just conclude, so we've seen that we."
        ],
        [
            "OK, excellent results for large scale image search.",
            "Our data set is online and the 1,000,000 active speaker data set can be obtained on request and we're currently extending with currently extended this work to videos.",
            "And here I show just one result from that record competition so the traffic competition is 200 hours of videos and the task is to find a deformed video in the data set so you can see here the upper the upper left, upper right corner here.",
            "Corresponds to the image in the tracker data set and what you want is to retrieve the corresponding image correctly from a collection of 202 hundred hours of videos.",
            "So you can really see that our system performs extremely well, because here the image is really scaled soon, so it's much smaller.",
            "It's inserted in another image and there's some noise on the image, so this is the track field task, right?",
            "That record is task is we have a data set, large data set of videos and you want to find deformed copies within this data set.",
            "Many of the applications is like for example.",
            "Video producers want to see where their copy skill.",
            "So basically there's a large market."
        ],
        [
            "And if you just compare one of the results we have obtained for the traffic competition, so the most difficult difficult transformation was a combined type of information.",
            "So having this scale trend of video noise and insertion and you can see here that our three approaches perform significantly better than the other best ones, right?",
            "So again, here you have recall and precision, and we have submitted three different runs.",
            "This one is just based on the keyframes to prove an interest based on the keyframes in the red and green ones, we store more frames per per short right?",
            "And you can just say we just wanted to give an idea you can really see that our system outperforms the systems of the other people."
        ],
        [
            "You know?",
            "Maybe you can take questions well, so now the second part is the demo, but then we can quit.",
            "Take questions while Frankie's.",
            "Hi.",
            "Any questions?",
            "So we have to reboot.",
            "Accordingly, I would like to ask a question in the demos that you sold with a video where you.",
            "Do you know if you're primarily looking at the face?",
            "Too much the woman.",
            "That you were showing on the top right with the with the rest of the video.",
            "What where do you know?",
            "Where were you matching?",
            "No, we don't.",
            "What we all we have is a set of like a data set of videos and then we have this query video with the woman and the background and we don't know if all what we're looking for.",
            "So we just have the whole the whole video stream and we don't know if you're looking for the woman.",
            "We don't know if you're looking for the background one approach.",
            "Actually what we tried is to separate the two parts automatically, yes, but actually that the performance of that was worse than if you just match all the descriptors in the whole sequence and then use the spatial temporal constraints to filter out right?",
            "So what we do is we store set of frames and then.",
            "Use a spatial temporal constraints to do something additional filtering and that really gives the best results.",
            "I see because because I should probably the faces to me as a human as the differentiation.",
            "I mean as as the way I would I would differentiate if it's that person in the video, somebody else now impress you don't even know if there's a face or not in the image or all you know is like you want, I know, but because you saw in the optimas alotta face looks very prominent, I'm saying be interesting to see if the face makes a difference.",
            "No, no, the I mean the descriptors.",
            "They don't really fire on the face, it just fire and texted regions in the image and so the face, I mean, the fact that it's a face or not, this doesn't make it boils down to see if the face not is tilted in the video.",
            "Would you have the same correspondence?",
            "So I was trying to see how I mean.",
            "If you just had.",
            "And you have maybe the grass right would be saying if you just had the grass.",
            "So have you.",
            "This is the robustness basically have you tested?",
            "If I if you saw this woman there little bit different polls.",
            "In a video, would you have recognized let's her know you're OK?",
            "Your question is is do we have an algorithm for face detection?",
            "Now what we have is an algorithm.",
            "You can compare it with other programs that do it on faces.",
            "Yes, we have worked on faces, but I mean basically it does.",
            "It does work if there's post changes, but up to some point and then if you if you want to do something for their faces, we add some learning structure to learn the differences and things like that.",
            "But here in this video copy detection task you always know that it's just the copy of the initial video and not no changes right?",
            "For that maybe for the image search demo you can say what happens if you have faith in the face?",
            "Seem seem from different viewpoint changes and that works up to some point.",
            "Some extent, but if you really want to detect faces then I think in the long run there's other approaches than this.",
            "Exactly this one, which gives the best I know as well, was only faint.",
            "No, it has nothing to do with the face.",
            "You could show the same thing with the logo inserted and it would work.",
            "And what you're doing?",
            "Any other question?",
            "No.",
            "OK, and so.",
            "OK, the demo.",
            "So here what we have online on our computer at home.",
            "We have this data set with 1 million a figure emerges here.",
            "You have selected a few images.",
            "For demonstration, what I do now, I submit this image to our data set of images where we have the Flickr images and some of our own datasets, and it now starts it's loading the images up to the cluster and then it'll start computing first descriptors, then searching in the data set and you can actually see it's pretty quickly to his already found the results and here you can see all the results it has returned so you can see that you have found a lot of similar images which show the same feelings.",
            "And which are actually all in Flickr?",
            "Because it's one of the famous monuments and you can see that many people have stored images of this monument in the data set in the approach has allowed to find all of them, or many of them correctly.",
            "And another example is for logos.",
            "So it works well for logos as well.",
            "So here we have this Coca Cola logo and we would like to see we find other advertisements which contain Coca Cola and again so I'm submitting the images computing descriptors.",
            "Then it will.",
            "Find the vectors to the visual words and then search in the data set to hear the results.",
            "You can actually see that we have found many of the Coca Cola publicity's in the data set right?",
            "So you can see here even if the color changes, so we don't use color even if the color changes.",
            "Find the logo even if there is there.",
            "Clutter occlusion incorrectly find it.",
            "And.",
            "OK, and here you can see these are few false matches, right?",
            "So it doesn't work always perfectly, but there's also some of the metrics which are incorrect, but the overall performance is very good and you find a lot of these Coca Cola logos correctly and maybe a last result.",
            "So you could also search on the moon and this goes more towards update categories.",
            "So you would like to find not only exactly the same image of the moon, but also different images of the moon and you can see here.",
            "Correcting finds different images.",
            "The moon and one thing you can do is, well, you can actually look at the short list.",
            "Before the spatial re ranking.",
            "OK, the charges before this page will be ranking and you can see here actually.",
            "But this also includes like wrong ones, right?",
            "So what it's for this is finding here.",
            "It's where we do the round glasses and hear the round shoes.",
            "So basically this is before the spatial ranking and conceal.",
            "Yeah, that's pretty quite a lot of them are wrong, right?",
            "And you can, some of them explain about here.",
            "For example the light bulb.",
            "OK, and this is actually this actually shows that the spatial ranking is important, right?",
            "This is before the spatial ranking, and if you add a special ranking on top you can really see that you can further remove wrong matches.",
            "OK, OK so.",
            "I just I just had a question about the Hamming embedding.",
            "Is there some kind of an assumption on the geometry of the descriptor in the Hamming embedding?",
            "Or like, what is the intuition?",
            "Why does it make it better?",
            "It's just there's no geometry whatsoever.",
            "All you have is like this.",
            "Gift descriptors, writer descriptor of your image Patch which describes the Patch and what you want is to find the most similar descriptor vectors right all you want is to find in your space of descriptor vectors the closest ones, and if you assigned to the visual words, there are many many of them are similar, but the similarity is pretty pretty large, and the Hamming embedding.",
            "What it helps it approximates a much, much more the matching.",
            "So actually what you do?",
            "What you would do in a perfect world, you just would compare all the descriptors and see their distances and take the best ones right and order them based on this score.",
            "But this is of course not feasible.",
            "As I said in the beginning, and so the Hamming embedding just approximates better this matching distance, right?",
            "And as you said, the increase is really significant.",
            "So you mentioned early on that things like LSH don't work very well for what you want to do.",
            "Or maybe I was mistaken.",
            "Did you mention you say yeah, I'm curious.",
            "Also because LSH at its core is a kind of a Hamming embedding.",
            "A randomized Hamming embedding, but you're having a billing works actually look better, and I'm just wanting any reason or any information at this age is like if you if you apply it with the image descriptors, right?",
            "So there it doesn't work very well at all.",
            "It doesn't work.",
            "The approximation is not valid and B.",
            "You have to store all the descriptors in yours in your in your memory, right?",
            "So it's not feasible.",
            "The two things.",
            "And yes, I think in the cell it's similar, right?",
            "It's similar to LSH, but it's it's more precise.",
            "AIDS more precise what we're doing and being.",
            "We have this quantization based on the results, right?",
            "So that's that's important difference, and actually we were thinking of doing this Hamming embedding for the whole space, but then still you would need some quantization really.",
            "Argument conversation and you can cope with the large number of descriptors and behavior.",
            "Results will be less precise.",
            "Any other last question perhaps?",
            "I think it's finished.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I will present my talk on large scale image search.",
                    "label": 1
                },
                {
                    "sent": "This is trying to work with me.",
                    "label": 0
                },
                {
                    "sent": "She Guimet his shoes.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what's the goal?",
                    "label": 0
                },
                {
                    "sent": "The goal is given the query image and large set of images, for example 1,000,000 images, you want to find similar images in this set and this as fast as possible and so what's the problem?",
                    "label": 0
                },
                {
                    "sent": "Each image here is described by approximately the top 2000 descriptors and this makes it 2 billion descriptors to index.",
                    "label": 1
                },
                {
                    "sent": "And obviously that's much, much larger.",
                    "label": 0
                },
                {
                    "sent": "Too large numbers to store it directly in rum, and so basically we have to find a solution to reduce the storage and the.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And make this search efficient.",
                    "label": 0
                },
                {
                    "sent": "And so our approach builds up on the bag of words approach, which has been proposed by civilian scissorman.",
                    "label": 0
                },
                {
                    "sent": "So the idea is the following.",
                    "label": 0
                },
                {
                    "sent": "Given the query image, you first extract a set of invariant F An invariant descriptors.",
                    "label": 1
                },
                {
                    "sent": "So bitter based on a set of FN invariant regions described by the SIFT descriptor.",
                    "label": 0
                },
                {
                    "sent": "So then you have each image described by, say, between 1000 and 2000 local descriptors.",
                    "label": 0
                },
                {
                    "sent": "So local index vectors of dimension 100.",
                    "label": 0
                },
                {
                    "sent": "So you have a set of vectors of dimension 100.",
                    "label": 0
                },
                {
                    "sent": "Around 1000, then you vector quantized them.",
                    "label": 0
                },
                {
                    "sent": "So build the visual vocabulary based on.",
                    "label": 0
                },
                {
                    "sent": "For example, K means clustering, which gives you a vocabulary you sign each of the descriptors in and in a given image to one of the visual words and this gives you the bag of features representation.",
                    "label": 0
                },
                {
                    "sent": "So forgiven image you have one bag of features for each feature.",
                    "label": 1
                },
                {
                    "sent": "Each feature in the image is signed to one visual words histogram over visual words which we call bag of features in the following and then we can use this structure for querying.",
                    "label": 1
                },
                {
                    "sent": "Can use an inverted file system.",
                    "label": 0
                },
                {
                    "sent": "So for each entry in the bag of features you have a list of images to which this word corresponds, which allows you to search quickly.",
                    "label": 0
                },
                {
                    "sent": "This gives you rank shortlist on top of this we can add an algorithm for geometric verification.",
                    "label": 0
                },
                {
                    "sent": "So this is the overall structure and our contribution comes in and two places.",
                    "label": 0
                },
                {
                    "sent": "The first one is to improve the bag of features representation to make the search of the descriptors more approximate, more precise, and the second one comes in.",
                    "label": 0
                },
                {
                    "sent": "The Jeremy created verification.",
                    "label": 0
                },
                {
                    "sent": "So instead of verifying only a short list of images, we propose an approach which allows us to verify each of the images with the geometric consistency.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Constraint.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, briefly, what is the bag of features approach?",
                    "label": 1
                },
                {
                    "sent": "It's just another yet another approximate nearest neighbors search.",
                    "label": 0
                },
                {
                    "sent": "So basically, if you want to search for the closest descriptors in the data set, we could just search for the K nearest neighbor of a descriptor.",
                    "label": 0
                },
                {
                    "sent": "Of course, if you have two billion of descriptors, this is prohibitively slow and even approximate nearest neighbor searches such as LSH is too slow and too memory consuming, and so if we use the bag of features approach, what we do is we vector quantized the descriptors right?",
                    "label": 0
                },
                {
                    "sent": "And so the matching is described by the comparison of the vector quantized features.",
                    "label": 0
                },
                {
                    "sent": "So you find each descriptor to visual word, little bird, one little word.",
                    "label": 1
                },
                {
                    "sent": "One it's the same visual word.",
                    "label": 0
                },
                {
                    "sent": "The descriptors are said to match.",
                    "label": 0
                },
                {
                    "sent": "Of course, this is an approximation, and if you look.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you evaluate this approximation, so how do we can evaluate it?",
                    "label": 0
                },
                {
                    "sent": "We look at the accuracy.",
                    "label": 0
                },
                {
                    "sent": "So basically, which shows us if the matching descriptor is among the nearest neighbors and we can look at how many achieved vectors are in.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A short list OK, and So what does this show show us?",
                    "label": 0
                },
                {
                    "sent": "We can see that depending on the size of the vocabulary, right?",
                    "label": 0
                },
                {
                    "sent": "If we have 100 words now visual vocabulary, we can see that we retrieve a large proportion of the descriptors and our accuracy.",
                    "label": 0
                },
                {
                    "sent": "Our recall is very good, so we find most of the matching descriptors how.",
                    "label": 0
                },
                {
                    "sent": "However, we also find a lot of chunk right and ideally what we would want is to be very low here.",
                    "label": 0
                },
                {
                    "sent": "And very high up there.",
                    "label": 0
                },
                {
                    "sent": "So basically, but we can see here is a trade off.",
                    "label": 0
                },
                {
                    "sent": "If we increase the size of the visual vocabulary you can see that we've retrieve less chunk points, so the rate of points, overall rate of points achieved is lower, but we also eliminate good matches, right?",
                    "label": 0
                },
                {
                    "sent": "So we can really see this is a trade off if you have a large vocabulary, retrieve many points and most of the points which you want to retrieve are among the points.",
                    "label": 0
                },
                {
                    "sent": "If we have a large vocabulary.",
                    "label": 0
                },
                {
                    "sent": "Then we retrieve notice not many chunk points, but there are a lot of.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Scriptures which we made.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so it's just here for small visual dictionary you have too many false matches for lateral discrete.",
                    "label": 1
                },
                {
                    "sent": "Two matches are missed OK and there is no no tradeoff between large and small either.",
                    "label": 1
                },
                {
                    "sent": "This devoid cells are too big and or this descriptive cells can't descriptor descriptor noise, so we have a descriptor it will just fall in the neighboring cell.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so there OK just here in the stration.",
                    "label": 0
                },
                {
                    "sent": "So we have 20 K visual words.",
                    "label": 0
                },
                {
                    "sent": "You can see that there are quite a lot of false matches.",
                    "label": 0
                },
                {
                    "sent": "So here are the matching is just done based on the assignment with the visual closest visual words, we can see that there are many false matches here.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we go to two 200,000 visual words, we can see that many good matches are missed here.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there are several recent approaches who fight against this description, or this quantization noise while approaches multiple or soft assignment of the descriptors or other approaches compare distribution of the descriptors based on, for example, Fisher kernels in the context of Image Cat.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For isation OK, and so the approach.",
                    "label": 0
                },
                {
                    "sent": "The proposed here is based.",
                    "label": 0
                },
                {
                    "sent": "On be take the cells right, but in the sales we just.",
                    "label": 0
                },
                {
                    "sent": "We also have a representation, a quantization within the cells.",
                    "label": 0
                },
                {
                    "sent": "So we assign refers to script, assign the description, the cell and then we have a binary of short vector which quantizes the position over.",
                    "label": 0
                },
                {
                    "sent": "This describes the position of the descriptor within the cell.",
                    "label": 0
                },
                {
                    "sent": "Right until basically perform vector quantization as in the standard bag of features, and then we have a short binary vector which which allows an additional localization within the cell.",
                    "label": 1
                },
                {
                    "sent": "So we have the advantage of this course quantization, but at the same time we search more precisely within a cell, and then we have two 2 descriptors match if they fall in the same cell, and if they look if they fall into the same region within the cell.",
                    "label": 0
                },
                {
                    "sent": "So basically.",
                    "label": 0
                },
                {
                    "sent": "This binary vectors divide the cell in different Subs subparts and we declare that two descriptors match if they fall into the same subregion up to some some thresholding, and so the prescript is out.",
                    "label": 0
                },
                {
                    "sent": "His binary descriptors are compared with the Hamming distance.",
                    "label": 0
                },
                {
                    "sent": "It's approximation for the Euclidean distance, and it allows us to reduce the dimension curse effect.",
                    "label": 0
                },
                {
                    "sent": "And what's actually interesting, it's very efficient.",
                    "label": 1
                },
                {
                    "sent": "This search plus the Hamming distance.",
                    "label": 0
                },
                {
                    "sent": "Can be quoted in very few bits and it loads of very few operations for comparison and it reduces also the the memory access.",
                    "label": 0
                },
                {
                    "sent": "So actually in the end is 3 times faster than the.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "About the features approach and so it's interesting.",
                    "label": 0
                },
                {
                    "sent": "How do we obtain?",
                    "label": 0
                },
                {
                    "sent": "This hamming embedding, so first of all we have to.",
                    "label": 1
                },
                {
                    "sent": "We have to draw.",
                    "label": 0
                },
                {
                    "sent": "Son.",
                    "label": 0
                },
                {
                    "sent": "You have to define the projection directions.",
                    "label": 1
                },
                {
                    "sent": "So here what we use.",
                    "label": 0
                },
                {
                    "sent": "We can either learn them from the data or we can just use.",
                    "label": 0
                },
                {
                    "sent": "Randomly chosen orthogonal projections.",
                    "label": 0
                },
                {
                    "sent": "We then use this matrices to project our descriptor vectors down into some in this representative space.",
                    "label": 0
                },
                {
                    "sent": "We determine the median of the descriptors which lie in each cell.",
                    "label": 0
                },
                {
                    "sent": "Use this as the point for comparison and then depending of if the point lies on the left or the right, is bigger or smaller than the median value decided to zero or one.",
                    "label": 0
                },
                {
                    "sent": "OK, so this allows us to obtain binary coding of the vectors within the cell, and these vectors can then be compared with the Hamming distance.",
                    "label": 0
                },
                {
                    "sent": "We can again use a threshold.",
                    "label": 0
                },
                {
                    "sent": "So basically you can say they're up to some threshold similar so they don't have to follow it.",
                    "label": 0
                },
                {
                    "sent": "You have to find to exactly the same cell, but they can fall into similar.",
                    "label": 0
                },
                {
                    "sent": "I mean parts which are similar right?",
                    "label": 0
                },
                {
                    "sent": "And then for online computation computes the binary signature of given descriptor vector by projecting into.",
                    "label": 1
                },
                {
                    "sent": "Our code is coordinate system and assigning one if it's above the median value.",
                    "label": 1
                },
                {
                    "sent": "Another way zero?",
                    "label": 0
                },
                {
                    "sent": "OK, so now we have representation within this space.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we can look at the tradeoff between.",
                    "label": 1
                },
                {
                    "sent": "Memory use it to Nick Sicura.",
                    "label": 0
                },
                {
                    "sent": "See and hear what we do is like.",
                    "label": 0
                },
                {
                    "sent": "We vary the number of bits.",
                    "label": 0
                },
                {
                    "sent": "Write the number of projection directions so the maximum would be a 228 which is the maximum size of our descriptor which is the size of our descriptor and date would be very very very small representation right?",
                    "label": 0
                },
                {
                    "sent": "And what we can see here.",
                    "label": 0
                },
                {
                    "sent": "Is the rate of the cell points retrieved and hear the rate of the five nearest neighbors retrieved and we can actually see that.",
                    "label": 1
                },
                {
                    "sent": "Obviously, the more the more dimensions you used, the better results get.",
                    "label": 0
                },
                {
                    "sent": "They can also see that even if their presentation is pretty small, it's much better than arbitrary filtering, so we do actually really improve the matching accuracy of ourselves.",
                    "label": 0
                },
                {
                    "sent": "So basically this would be just the mixture accuracy of the cells and then by adding this Hamming distance on top of it really improved.",
                    "label": 0
                },
                {
                    "sent": "The accuracy of our matching so you know what we're doing now.",
                    "label": 0
                },
                {
                    "sent": "Be getting much closer to a matching were really retrieve the closest descriptors directly from the set of descriptors and so depending on if you want to favor memory or accuracy, you can.",
                    "label": 0
                },
                {
                    "sent": "We can either use lower or higher dimensions where we used for our experiments.",
                    "label": 0
                },
                {
                    "sent": "We use forty 6864 bits.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As the representation, and if you compare it here with the code that you have initially seen for the K nearest neighbor for the K. For the vocabulary based on K means, we can see here.",
                    "label": 0
                },
                {
                    "sent": "So basically in red it's the curve obtained by the bag of words and the blue ones are by adding the Hamming Hamming embedding right and what's changed on this blue curves is the distance.",
                    "label": 0
                },
                {
                    "sent": "So if the distance is the full number of differences, we obtained the same results just obtained the results.",
                    "label": 0
                },
                {
                    "sent": "Of the points which fall into the cell without filtering and then then by making the distance smaller and smaller, we filter out more and more points.",
                    "label": 0
                },
                {
                    "sent": "As we can see here.",
                    "label": 0
                },
                {
                    "sent": "And if you look at the curves, for example, if you look at the curve K equals at the point K = 100.",
                    "label": 0
                },
                {
                    "sent": "If we look at the blue curve, we can see that for for the same level of recall.",
                    "label": 1
                },
                {
                    "sent": "You can retrieve 10 times less points, right?",
                    "label": 0
                },
                {
                    "sent": "So we really improve the performance of our system.",
                    "label": 0
                },
                {
                    "sent": "So it really performs provides a much better tradeoff between recall an ambiguity approval, and what's really interesting is that it's for a very low cost, so all you have to do is to store this additional binary final signatures for descriptor and compare them based on the bit comparison.",
                    "label": 1
                },
                {
                    "sent": "Basic was each of the Mr.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sorta steel ones.",
                    "label": 0
                },
                {
                    "sent": "And if you look at an example you can see here.",
                    "label": 0
                },
                {
                    "sent": "OK, this is this is the result obtained with the Hamming embedding for a vocabulary of 20,000.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're all words, and here is out there having embedding and you can really see that.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By adding the Hamming embedding.",
                    "label": 0
                },
                {
                    "sent": "It allows to remove false script false matches without removing correct?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's right, so these ones are the ones obtained just by signing the Scriptures to the visual words and these ones.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The ones obtained if you add the distance comparison based on the heading distance.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And another example here again, that's the same exam.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you look at the much larger visual vocabulary, so here 200,000 visual words, you can see that actually.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Paired to the one which you obtained with Hemming and bedding, you don't remove it.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "May descriptors right if you increase the size of the vocabulary, remove more and more good match.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and so the second contribution is a week to metric consistency.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Constrained so state of the art systems they are based on.",
                    "label": 1
                },
                {
                    "sent": "Re ranking with a full geometric verification.",
                    "label": 1
                },
                {
                    "sent": "So what you do once the system has returned a short list of images is take for example say the 100 best retrieved images on the verify the creature magic consistency between the query image and the returned images, for example by estimating a homography based on the matches between the two images, right?",
                    "label": 0
                },
                {
                    "sent": "And this this is pretty costly so you could definitely not do it for a million images people.",
                    "label": 0
                },
                {
                    "sent": "Add internal to it 400, but you could also go say for up to 1000.",
                    "label": 0
                },
                {
                    "sent": "However, if you look at the graph here, so when you look at what you see here on the X axis, you see the size of the data set right?",
                    "label": 0
                },
                {
                    "sent": "And the more the data set increases, the more the more the rate of relevant images in the shortlist decreases, right?",
                    "label": 1
                },
                {
                    "sent": "So you can see here if you have 1 million of images, the images which are in the shortlist say here.",
                    "label": 0
                },
                {
                    "sent": "If you take a short list of 100 images there, only 20% of the correct return images in the short list.",
                    "label": 0
                },
                {
                    "sent": "So basically here.",
                    "label": 0
                },
                {
                    "sent": "Even if you use the geometric verification, there will be many miss images missing.",
                    "label": 0
                },
                {
                    "sent": "So basically the ones which are in here we can move them up in the in the short list ranking, but all the others you won't find them.",
                    "label": 0
                },
                {
                    "sent": "And even if you go up to 1000 images you can still see that only 50% of the images have been returned, so many of them are missing, right?",
                    "label": 0
                },
                {
                    "sent": "So you have to know what you thought about how can you improve this without going up to the full dramatic verification.",
                    "label": 0
                },
                {
                    "sent": "So the idea.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is to introduce some very simple Victor metric consistency check, which can be implemented for all images and not transfer.",
                    "label": 0
                },
                {
                    "sent": "Shortlist all images.",
                    "label": 0
                },
                {
                    "sent": "So implemented really for the millions of images.",
                    "label": 0
                },
                {
                    "sent": "And So what is our approach based on?",
                    "label": 0
                },
                {
                    "sent": "So you have seen initially that we extract a set of invariant image regions, right?",
                    "label": 0
                },
                {
                    "sent": "So for each region we have for example we have an F and transformation a scale in the rotation angle associated.",
                    "label": 0
                },
                {
                    "sent": "So here we can see.",
                    "label": 0
                },
                {
                    "sent": "Actually, the size of the region is the scale and the rotation angle is.",
                    "label": 0
                },
                {
                    "sent": "There is some some angle which gives the rotation of the image Patch here what we use is the characteristic scale, which is the scale peak in the scale domain and dominant gradient orientation.",
                    "label": 0
                },
                {
                    "sent": "The different ways of estimating actually the scale and rotation characteristics of a Patch.",
                    "label": 0
                },
                {
                    "sent": "And then if you have a matching pair right.",
                    "label": 0
                },
                {
                    "sent": "So for example, you can see these two image patches are similar, so they have similar descriptors.",
                    "label": 0
                },
                {
                    "sent": "They will match.",
                    "label": 0
                },
                {
                    "sent": "And for the ones for the matching descriptor, you also have a size of the region and a dominant rotation angle, right?",
                    "label": 0
                },
                {
                    "sent": "So once you have this pair, what you can do, you can determine the scale change between the two patches and the rotation change between the two 2 patches.",
                    "label": 0
                },
                {
                    "sent": "So here will be a scale change of murder.",
                    "label": 0
                },
                {
                    "sent": "Two half and rotation angle of 20 degrees.",
                    "label": 1
                },
                {
                    "sent": "So each matching pair descriptor pair results in a scale and angle difference.",
                    "label": 1
                },
                {
                    "sent": "And the assumption we make here, which is not.",
                    "label": 0
                },
                {
                    "sent": "Which is not exact, but it's an approximation.",
                    "label": 1
                },
                {
                    "sent": "Is that the global image rotation changes are roughly consistent, so it holds exactly if you have a scale change between the images, so the image patches they will have the same scale change between them.",
                    "label": 0
                },
                {
                    "sent": "The matching image patches will have the scale same triskel times between them.",
                    "label": 0
                },
                {
                    "sent": "It doesn't hold if you have, for example, a viewpoint change, then it will be only approximately correct.",
                    "label": 0
                },
                {
                    "sent": "But in case of image rotation, image scale scanned is exactly correct.",
                    "label": 0
                },
                {
                    "sent": "In case of perspective, viewpoint changes is rough.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Exclamation.",
                    "label": 0
                },
                {
                    "sent": "OK, and here an example for the orientation consistency so on.",
                    "label": 1
                },
                {
                    "sent": "On the left you have the query image on the right image which has been stored in the database and you can see for each point we show the rotation the angle in the rotation difference between the patches right?",
                    "label": 0
                },
                {
                    "sent": "And you can see here most of them are consistent, but if you look closely there are a few outliers for example.",
                    "label": 0
                },
                {
                    "sent": "This one up here in the top right, which is much larger, is not the same orientation.",
                    "label": 0
                },
                {
                    "sent": "And if you quantize this orientation differences, we can see here, there is a maximum, and this is actually the rotation angle between the two images.",
                    "label": 1
                },
                {
                    "sent": "So if you see a look here, this is the maximum for these pair of images.",
                    "label": 0
                },
                {
                    "sent": "You look here, it's pay \u03c0 / 4, so we have approximately rotation angle of 45 degrees between the two images.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is found correctly.",
                    "label": 0
                },
                {
                    "sent": "Another example for the scale consistency we can see here 2 images to different images of the Eiffel Tower.",
                    "label": 0
                },
                {
                    "sent": "There's approximately scale change of 1/2 between the two of them, and you can see here we show the matching matching regions.",
                    "label": 0
                },
                {
                    "sent": "Again, take the difference of the characteristic scales and you can see here that you find the peak here at approximately 1/2 right?",
                    "label": 0
                },
                {
                    "sent": "So this allows us to filter the matches not only on their description, but also on their geometric consistency.",
                    "label": 0
                },
                {
                    "sent": "And so if you keep this.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This histograms in mind.",
                    "label": 0
                },
                {
                    "sent": "These are the histograms used for the voting, so voting is not only performed per image, but it's also performed post angle and scale difference.",
                    "label": 0
                },
                {
                    "sent": "We have seen their hopes up that this steps substances are independent, so their rotation and scale, which is occurs between 2 images independent and we then obtain a score for all quantized quite contest angles and scale difference for each image and the final score is then filtering for each of those two parameters and we take them in between them.",
                    "label": 1
                },
                {
                    "sent": "This allows us to obtain matches that agrees with the main difference in orientation and scale.",
                    "label": 0
                },
                {
                    "sent": "This will be taken into account for the final score, so if we have many matches but none of them agrees in the scale of the orientation, there will be all filtered out.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if they agree, they give a peak which is much higher, so in the end we have voting score match, which is much higher.",
                    "label": 0
                },
                {
                    "sent": "And what an interesting question, which we then can ask is, do we still?",
                    "label": 0
                },
                {
                    "sent": "Getting further improvement by using the full geometric ranking or not so ideally I would say yes, will show this in our expandable results.",
                    "label": 0
                },
                {
                    "sent": "Ideally the answer would be yes, because for now we just have done a very weak traumatic verification.",
                    "label": 0
                },
                {
                    "sent": "However, if you put the full geometric model on top of that, we will obtain an additional verification.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then something which is interesting as well is actually if you look at the the angle differences for our images, they can observe that what would you would think anyways, for natural collection of natural images, most of the images are taken at the same angle.",
                    "label": 0
                },
                {
                    "sent": "Then there's a peak at \u03c0 and \u03c0 / 2, and so we can use this as a prior for waiting our angle difference, which shows it has been shown to further.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The results and for our experimental evaluation we have created the data set for evaluation which contains the 1000 images, 500 query images 991 annotated true positives.",
                    "label": 1
                },
                {
                    "sent": "We have a 1,000,000 discount tractor images which we have obtained from Flickr.",
                    "label": 0
                },
                {
                    "sent": "We have constructed vocabulary on an independent data set also obtained from Flickr, and our search system is almost real time as I'll show in the demo in a minute and so how do we allocate our results?",
                    "label": 1
                },
                {
                    "sent": "There is outside evaluated based on mean average precision, which means the bigger the better, right?",
                    "label": 0
                },
                {
                    "sent": "Zero will be no, no, no precision and one will be a very higher precision.",
                    "label": 0
                },
                {
                    "sent": "So basically average precision is the precision recall curve.",
                    "label": 0
                },
                {
                    "sent": "So if you have a recall one precision, one that's the best curve we can obtain.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Average or precision recall curve.",
                    "label": 0
                },
                {
                    "sent": "If you samples of our.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dataset.",
                    "label": 0
                },
                {
                    "sent": "And then a few images, image sequences which are in the data set.",
                    "label": 0
                },
                {
                    "sent": "So on the left it's the query images and on the right that's all the images which have which we have stored in this million images and which would like to retrieve.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another example boasting.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Venice and a few examples of the distractor images from Flickr.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here is the evaluation for our holiday data set.",
                    "label": 0
                },
                {
                    "sent": "And in blue we show the baseline, so just obtained with a bag of visual words.",
                    "label": 0
                },
                {
                    "sent": "And then we can see the different variations, so only using the geometry improved results only using the Hamming embedding with improved results.",
                    "label": 0
                },
                {
                    "sent": "And if you combine two of them together, we further improve the results and.",
                    "label": 0
                },
                {
                    "sent": "And you can see that the improvement is actually substantial.",
                    "label": 0
                },
                {
                    "sent": "So if you look at a data set of 1,000,000 images here we have a mean average precision of 0, three and then by just adding Victor American Hamming insistence he double.",
                    "label": 0
                },
                {
                    "sent": "I mean average precision.",
                    "label": 0
                },
                {
                    "sent": "And then what I said before.",
                    "label": 0
                },
                {
                    "sent": "Actually if we add this re ranking step we can see that it allows us to further improve the results, right?",
                    "label": 0
                },
                {
                    "sent": "So actually what this week too much, even the Hamming embedding does.",
                    "label": 0
                },
                {
                    "sent": "It gives us back much more true positives in the short list.",
                    "label": 0
                },
                {
                    "sent": "And if you apply the re ranking, we can still further improve the results.",
                    "label": 0
                },
                {
                    "sent": "So really the two of them being geometry in full geometry, their complementary when is really weak filter which improves the results in the second one really verifies the results and so based on this re ranking most of the results are actually correct.",
                    "label": 0
                },
                {
                    "sent": "The ones which have been found their correct and what we can see here as well, which is interesting if you look at the CPU which is which is consumed.",
                    "label": 0
                },
                {
                    "sent": "So there is this is for computation of the transcripts and quantization and then if you look at the search.",
                    "label": 0
                },
                {
                    "sent": "And for searching, we can see that the overall search time for Hemming and adding and geometry consistency is compareable to the baseline and so actually I had before the Hamming embedding allows to find much less descriptors and therefore less memory accesses.",
                    "label": 0
                },
                {
                    "sent": "So it really reduces the search time.",
                    "label": 0
                },
                {
                    "sent": "The geometry victory constrained increases it, but both in together allow us to have the same same search time as before.",
                    "label": 0
                },
                {
                    "sent": "So it's really it's feasible to use this.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rota so time consuming.",
                    "label": 0
                },
                {
                    "sent": "And here a real example.",
                    "label": 0
                },
                {
                    "sent": "So this is the query and this is what the first 5 results which we system returns.",
                    "label": 0
                },
                {
                    "sent": "And this is actually nice.",
                    "label": 0
                },
                {
                    "sent": "I like this example because it doesn't only give back the images of which we have stored explicitly in the data set, but also find similar images in Flickr, so figured as extras images which are similar to the query.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it finds it automatically.",
                    "label": 0
                },
                {
                    "sent": "And if you compare our approach to the bag of features, we can see.",
                    "label": 0
                },
                {
                    "sent": "So in green.",
                    "label": 0
                },
                {
                    "sent": "The ranking we obtained for the images with our approach.",
                    "label": 0
                },
                {
                    "sent": "So the first, 4th and 5th.",
                    "label": 0
                },
                {
                    "sent": "So these are the ones which we have in our data set.",
                    "label": 0
                },
                {
                    "sent": "And then if you use just the standard bag of features, we can actually see that there really far behind.",
                    "label": 0
                },
                {
                    "sent": "So this is precision 5800.",
                    "label": 0
                },
                {
                    "sent": "So basically even with geometric Re ranking would be never moved to that.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Up and similar for the last time, Marco so we don't find all of the images here.",
                    "label": 0
                },
                {
                    "sent": "It's a formal freak alright.",
                    "label": 0
                },
                {
                    "sent": "We don't find all of the images which you have put into our data set, but only a subset of them.",
                    "label": 0
                },
                {
                    "sent": "So here for in the first 6 retrieved images and it's nice.",
                    "label": 0
                },
                {
                    "sent": "Here again, we find also images from Flickr will show which correspond to the query.",
                    "label": 0
                },
                {
                    "sent": "And so ideally would now we want to demo, because because I've just switched laptops, I'll just conclude, so we've seen that we.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, excellent results for large scale image search.",
                    "label": 1
                },
                {
                    "sent": "Our data set is online and the 1,000,000 active speaker data set can be obtained on request and we're currently extending with currently extended this work to videos.",
                    "label": 0
                },
                {
                    "sent": "And here I show just one result from that record competition so the traffic competition is 200 hours of videos and the task is to find a deformed video in the data set so you can see here the upper the upper left, upper right corner here.",
                    "label": 0
                },
                {
                    "sent": "Corresponds to the image in the tracker data set and what you want is to retrieve the corresponding image correctly from a collection of 202 hundred hours of videos.",
                    "label": 0
                },
                {
                    "sent": "So you can really see that our system performs extremely well, because here the image is really scaled soon, so it's much smaller.",
                    "label": 0
                },
                {
                    "sent": "It's inserted in another image and there's some noise on the image, so this is the track field task, right?",
                    "label": 0
                },
                {
                    "sent": "That record is task is we have a data set, large data set of videos and you want to find deformed copies within this data set.",
                    "label": 0
                },
                {
                    "sent": "Many of the applications is like for example.",
                    "label": 0
                },
                {
                    "sent": "Video producers want to see where their copy skill.",
                    "label": 0
                },
                {
                    "sent": "So basically there's a large market.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if you just compare one of the results we have obtained for the traffic competition, so the most difficult difficult transformation was a combined type of information.",
                    "label": 0
                },
                {
                    "sent": "So having this scale trend of video noise and insertion and you can see here that our three approaches perform significantly better than the other best ones, right?",
                    "label": 0
                },
                {
                    "sent": "So again, here you have recall and precision, and we have submitted three different runs.",
                    "label": 0
                },
                {
                    "sent": "This one is just based on the keyframes to prove an interest based on the keyframes in the red and green ones, we store more frames per per short right?",
                    "label": 0
                },
                {
                    "sent": "And you can just say we just wanted to give an idea you can really see that our system outperforms the systems of the other people.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You know?",
                    "label": 0
                },
                {
                    "sent": "Maybe you can take questions well, so now the second part is the demo, but then we can quit.",
                    "label": 0
                },
                {
                    "sent": "Take questions while Frankie's.",
                    "label": 0
                },
                {
                    "sent": "Hi.",
                    "label": 0
                },
                {
                    "sent": "Any questions?",
                    "label": 0
                },
                {
                    "sent": "So we have to reboot.",
                    "label": 0
                },
                {
                    "sent": "Accordingly, I would like to ask a question in the demos that you sold with a video where you.",
                    "label": 0
                },
                {
                    "sent": "Do you know if you're primarily looking at the face?",
                    "label": 0
                },
                {
                    "sent": "Too much the woman.",
                    "label": 0
                },
                {
                    "sent": "That you were showing on the top right with the with the rest of the video.",
                    "label": 0
                },
                {
                    "sent": "What where do you know?",
                    "label": 0
                },
                {
                    "sent": "Where were you matching?",
                    "label": 0
                },
                {
                    "sent": "No, we don't.",
                    "label": 0
                },
                {
                    "sent": "What we all we have is a set of like a data set of videos and then we have this query video with the woman and the background and we don't know if all what we're looking for.",
                    "label": 0
                },
                {
                    "sent": "So we just have the whole the whole video stream and we don't know if you're looking for the woman.",
                    "label": 0
                },
                {
                    "sent": "We don't know if you're looking for the background one approach.",
                    "label": 0
                },
                {
                    "sent": "Actually what we tried is to separate the two parts automatically, yes, but actually that the performance of that was worse than if you just match all the descriptors in the whole sequence and then use the spatial temporal constraints to filter out right?",
                    "label": 0
                },
                {
                    "sent": "So what we do is we store set of frames and then.",
                    "label": 0
                },
                {
                    "sent": "Use a spatial temporal constraints to do something additional filtering and that really gives the best results.",
                    "label": 0
                },
                {
                    "sent": "I see because because I should probably the faces to me as a human as the differentiation.",
                    "label": 0
                },
                {
                    "sent": "I mean as as the way I would I would differentiate if it's that person in the video, somebody else now impress you don't even know if there's a face or not in the image or all you know is like you want, I know, but because you saw in the optimas alotta face looks very prominent, I'm saying be interesting to see if the face makes a difference.",
                    "label": 0
                },
                {
                    "sent": "No, no, the I mean the descriptors.",
                    "label": 0
                },
                {
                    "sent": "They don't really fire on the face, it just fire and texted regions in the image and so the face, I mean, the fact that it's a face or not, this doesn't make it boils down to see if the face not is tilted in the video.",
                    "label": 0
                },
                {
                    "sent": "Would you have the same correspondence?",
                    "label": 0
                },
                {
                    "sent": "So I was trying to see how I mean.",
                    "label": 0
                },
                {
                    "sent": "If you just had.",
                    "label": 0
                },
                {
                    "sent": "And you have maybe the grass right would be saying if you just had the grass.",
                    "label": 0
                },
                {
                    "sent": "So have you.",
                    "label": 0
                },
                {
                    "sent": "This is the robustness basically have you tested?",
                    "label": 0
                },
                {
                    "sent": "If I if you saw this woman there little bit different polls.",
                    "label": 0
                },
                {
                    "sent": "In a video, would you have recognized let's her know you're OK?",
                    "label": 0
                },
                {
                    "sent": "Your question is is do we have an algorithm for face detection?",
                    "label": 0
                },
                {
                    "sent": "Now what we have is an algorithm.",
                    "label": 0
                },
                {
                    "sent": "You can compare it with other programs that do it on faces.",
                    "label": 0
                },
                {
                    "sent": "Yes, we have worked on faces, but I mean basically it does.",
                    "label": 0
                },
                {
                    "sent": "It does work if there's post changes, but up to some point and then if you if you want to do something for their faces, we add some learning structure to learn the differences and things like that.",
                    "label": 0
                },
                {
                    "sent": "But here in this video copy detection task you always know that it's just the copy of the initial video and not no changes right?",
                    "label": 0
                },
                {
                    "sent": "For that maybe for the image search demo you can say what happens if you have faith in the face?",
                    "label": 0
                },
                {
                    "sent": "Seem seem from different viewpoint changes and that works up to some point.",
                    "label": 0
                },
                {
                    "sent": "Some extent, but if you really want to detect faces then I think in the long run there's other approaches than this.",
                    "label": 0
                },
                {
                    "sent": "Exactly this one, which gives the best I know as well, was only faint.",
                    "label": 0
                },
                {
                    "sent": "No, it has nothing to do with the face.",
                    "label": 0
                },
                {
                    "sent": "You could show the same thing with the logo inserted and it would work.",
                    "label": 0
                },
                {
                    "sent": "And what you're doing?",
                    "label": 0
                },
                {
                    "sent": "Any other question?",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "OK, and so.",
                    "label": 0
                },
                {
                    "sent": "OK, the demo.",
                    "label": 0
                },
                {
                    "sent": "So here what we have online on our computer at home.",
                    "label": 0
                },
                {
                    "sent": "We have this data set with 1 million a figure emerges here.",
                    "label": 0
                },
                {
                    "sent": "You have selected a few images.",
                    "label": 0
                },
                {
                    "sent": "For demonstration, what I do now, I submit this image to our data set of images where we have the Flickr images and some of our own datasets, and it now starts it's loading the images up to the cluster and then it'll start computing first descriptors, then searching in the data set and you can actually see it's pretty quickly to his already found the results and here you can see all the results it has returned so you can see that you have found a lot of similar images which show the same feelings.",
                    "label": 0
                },
                {
                    "sent": "And which are actually all in Flickr?",
                    "label": 0
                },
                {
                    "sent": "Because it's one of the famous monuments and you can see that many people have stored images of this monument in the data set in the approach has allowed to find all of them, or many of them correctly.",
                    "label": 0
                },
                {
                    "sent": "And another example is for logos.",
                    "label": 0
                },
                {
                    "sent": "So it works well for logos as well.",
                    "label": 0
                },
                {
                    "sent": "So here we have this Coca Cola logo and we would like to see we find other advertisements which contain Coca Cola and again so I'm submitting the images computing descriptors.",
                    "label": 0
                },
                {
                    "sent": "Then it will.",
                    "label": 0
                },
                {
                    "sent": "Find the vectors to the visual words and then search in the data set to hear the results.",
                    "label": 0
                },
                {
                    "sent": "You can actually see that we have found many of the Coca Cola publicity's in the data set right?",
                    "label": 0
                },
                {
                    "sent": "So you can see here even if the color changes, so we don't use color even if the color changes.",
                    "label": 0
                },
                {
                    "sent": "Find the logo even if there is there.",
                    "label": 0
                },
                {
                    "sent": "Clutter occlusion incorrectly find it.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "OK, and here you can see these are few false matches, right?",
                    "label": 0
                },
                {
                    "sent": "So it doesn't work always perfectly, but there's also some of the metrics which are incorrect, but the overall performance is very good and you find a lot of these Coca Cola logos correctly and maybe a last result.",
                    "label": 0
                },
                {
                    "sent": "So you could also search on the moon and this goes more towards update categories.",
                    "label": 0
                },
                {
                    "sent": "So you would like to find not only exactly the same image of the moon, but also different images of the moon and you can see here.",
                    "label": 0
                },
                {
                    "sent": "Correcting finds different images.",
                    "label": 0
                },
                {
                    "sent": "The moon and one thing you can do is, well, you can actually look at the short list.",
                    "label": 0
                },
                {
                    "sent": "Before the spatial re ranking.",
                    "label": 0
                },
                {
                    "sent": "OK, the charges before this page will be ranking and you can see here actually.",
                    "label": 0
                },
                {
                    "sent": "But this also includes like wrong ones, right?",
                    "label": 0
                },
                {
                    "sent": "So what it's for this is finding here.",
                    "label": 0
                },
                {
                    "sent": "It's where we do the round glasses and hear the round shoes.",
                    "label": 0
                },
                {
                    "sent": "So basically this is before the spatial ranking and conceal.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's pretty quite a lot of them are wrong, right?",
                    "label": 0
                },
                {
                    "sent": "And you can, some of them explain about here.",
                    "label": 0
                },
                {
                    "sent": "For example the light bulb.",
                    "label": 0
                },
                {
                    "sent": "OK, and this is actually this actually shows that the spatial ranking is important, right?",
                    "label": 0
                },
                {
                    "sent": "This is before the spatial ranking, and if you add a special ranking on top you can really see that you can further remove wrong matches.",
                    "label": 0
                },
                {
                    "sent": "OK, OK so.",
                    "label": 0
                },
                {
                    "sent": "I just I just had a question about the Hamming embedding.",
                    "label": 0
                },
                {
                    "sent": "Is there some kind of an assumption on the geometry of the descriptor in the Hamming embedding?",
                    "label": 1
                },
                {
                    "sent": "Or like, what is the intuition?",
                    "label": 0
                },
                {
                    "sent": "Why does it make it better?",
                    "label": 0
                },
                {
                    "sent": "It's just there's no geometry whatsoever.",
                    "label": 0
                },
                {
                    "sent": "All you have is like this.",
                    "label": 0
                },
                {
                    "sent": "Gift descriptors, writer descriptor of your image Patch which describes the Patch and what you want is to find the most similar descriptor vectors right all you want is to find in your space of descriptor vectors the closest ones, and if you assigned to the visual words, there are many many of them are similar, but the similarity is pretty pretty large, and the Hamming embedding.",
                    "label": 0
                },
                {
                    "sent": "What it helps it approximates a much, much more the matching.",
                    "label": 0
                },
                {
                    "sent": "So actually what you do?",
                    "label": 0
                },
                {
                    "sent": "What you would do in a perfect world, you just would compare all the descriptors and see their distances and take the best ones right and order them based on this score.",
                    "label": 0
                },
                {
                    "sent": "But this is of course not feasible.",
                    "label": 0
                },
                {
                    "sent": "As I said in the beginning, and so the Hamming embedding just approximates better this matching distance, right?",
                    "label": 0
                },
                {
                    "sent": "And as you said, the increase is really significant.",
                    "label": 0
                },
                {
                    "sent": "So you mentioned early on that things like LSH don't work very well for what you want to do.",
                    "label": 0
                },
                {
                    "sent": "Or maybe I was mistaken.",
                    "label": 0
                },
                {
                    "sent": "Did you mention you say yeah, I'm curious.",
                    "label": 0
                },
                {
                    "sent": "Also because LSH at its core is a kind of a Hamming embedding.",
                    "label": 0
                },
                {
                    "sent": "A randomized Hamming embedding, but you're having a billing works actually look better, and I'm just wanting any reason or any information at this age is like if you if you apply it with the image descriptors, right?",
                    "label": 0
                },
                {
                    "sent": "So there it doesn't work very well at all.",
                    "label": 0
                },
                {
                    "sent": "It doesn't work.",
                    "label": 0
                },
                {
                    "sent": "The approximation is not valid and B.",
                    "label": 0
                },
                {
                    "sent": "You have to store all the descriptors in yours in your in your memory, right?",
                    "label": 0
                },
                {
                    "sent": "So it's not feasible.",
                    "label": 0
                },
                {
                    "sent": "The two things.",
                    "label": 0
                },
                {
                    "sent": "And yes, I think in the cell it's similar, right?",
                    "label": 0
                },
                {
                    "sent": "It's similar to LSH, but it's it's more precise.",
                    "label": 0
                },
                {
                    "sent": "AIDS more precise what we're doing and being.",
                    "label": 0
                },
                {
                    "sent": "We have this quantization based on the results, right?",
                    "label": 1
                },
                {
                    "sent": "So that's that's important difference, and actually we were thinking of doing this Hamming embedding for the whole space, but then still you would need some quantization really.",
                    "label": 0
                },
                {
                    "sent": "Argument conversation and you can cope with the large number of descriptors and behavior.",
                    "label": 0
                },
                {
                    "sent": "Results will be less precise.",
                    "label": 0
                },
                {
                    "sent": "Any other last question perhaps?",
                    "label": 0
                },
                {
                    "sent": "I think it's finished.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}