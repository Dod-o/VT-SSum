{
    "id": "zrv77niynlwjbnr757y7anv7b6dhql52",
    "title": "Support Vector Machines and Kernel Methods",
    "info": {
        "author": [
            "Colin Campbell, Intelligent Systems Laboratory, University of Bristol"
        ],
        "published": "Dec. 14, 2007",
        "recorded": "October 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/aop07_campbell_svm/",
    "segmentation": [
        [
            "Right, so this is tutorial on support vector machines and kernel methods.",
            "Now the first talk is just about this is just a straightforward introduction to support vector machines.",
            "I had a second optional talk.",
            "I just see what the interest is at the end, which I talk about other types of kernel based methods, But this is just the standard scenario."
        ],
        [
            "And first of all I deal with support vector machines for binary classification and then soft margins and multiclass classification support vector machines for regression and a little bit about support vector machines in practice."
        ],
        [
            "The second talk if I get get to do it, I do a linear programming alternative approach to kernel methods, then some comments about actually how you train support vector machines.",
            "The important issue of model selection.",
            "OK, if we get onto this second talk, different types of kernels."
        ],
        [
            "And so on.",
            "OK, right?",
            "So support vector machines are very well known.",
            "Indeed, a very old subject Nella just pointed out to me on this fresco at the back.",
            "There is apparently SVM somewhere, so obviously back in the Middle Ages on about them right?",
            "So support vector machines principled approach to classification regression, novelty detection problems.",
            "As if it good generalization in practice and also the hypothesis or decision function you may construct has explicit dependence on a day to the support vectors.",
            "Hence this enables more readily the actual interpretation of the model OK, and because they have a principled approach, they've somewhat taken over neural networks as a sort of general approach to classify."
        ],
        [
            "Kacian and regression.",
            "Importantly, learning involves optimization of a convex function.",
            "OK, now this makes it quite different from a neural network and network ascentia Lee it's not a convex problem, there are many minima or Maxima in my decision function and I can get caught up in a poor local minimum.",
            "OK, whereas with a sport vector machine the actual underlying function is a quadratic and we know what a quadratic looks like.",
            "It has one solution at one minimum.",
            "And indeed, that's a nice thick feature of the model.",
            "OK, so a convex approach to learning relatively few parameters required to train a learning machine.",
            "OK, if I take a neural network for example, then I have sort of a number of unknowns.",
            "One is the architecture.",
            "How many hidden nodes are required or acquire hidden layer and other parameters in the neural network?",
            "For example, if a tench updating function, there's a parameter in the updating function.",
            "And various things like that.",
            "They're not so elegant as the support vector machine as we'll see.",
            "Indeed, you could actually say with a support vector machine there different linear using a linear kernel.",
            "I don't have any parameters to mess around with altogether.",
            "Other types of kernels.",
            "I do have a kernel parameter and have various ways of approaching the problem, but just have one kernel parameter.",
            "There's another type of parameter soft margin, which I will talk about later on, but relatively few parameters and I do have a strategy.",
            "About how to actually handle the relatively few sets of parameters that I do have, you can implement implement confidence measures, I'll talk.",
            "Think I'll talk about that little bit in this talk later on."
        ],
        [
            "So what I want to do is just set up the basic scenario for support vector machines for binary classification and the subject of support vector machines goes back a long way.",
            "It was actually anticipated quite early on in the 1960s by Russian mathematicians, and there's also a book in Polish which anticipates the subject quite a lot, but they took off from around about 98 and being popular ever since.",
            "So what I want to do is just sort of introduce binary classification.",
            "The standard scenario for support vector machine and I have an input vector which I'll call XI&YI for binary classification, will only have two labels, plus or minus one OK and I call these labels the.",
            "At Target or labels the index I labels pattern pairs OK, so this thing is actually a vector with a number of attributes and I labels the pattern and why eyes its corresponding target or label.",
            "And so this range is overall the pattern pairs I = 1 to M and this XI therefore defines a sort of space of labeled points, which I call the input space."
        ],
        [
            "So this is my starting point now.",
            "The justification for system for support vector machines actually comes from statistical learning theory, and indeed had an introduction to this I guess, certainly with Joshua Taylor it was certainly this area and the actual justification for the subject comes from certain theoretical bounds on the generalization error, which come from this subject, and indeed I'm not actually going to define.",
            "The theorem I'm just going to say the consequences of of these generalization bounds.",
            "In fact, the generalization bounds are therefore derived from this school learning theory, and it actually got 2 features to them."
        ],
        [
            "One is the upper bound on generalization error does not depend on the dimension of the space.",
            "Now we will note this now, but it's a point I'm going to come back to later on.",
            "OK, so the generalization bound doesn't depend on whether the two dimensional or 10 dimensional space, etc.",
            "The other point about the theorem, which I don't state is the bound, is minimized by maximizing an object called the marginal OK, and that margin is the minimal distance between the hyperplane separating.",
            "The two classes and the closest data points of each class."
        ],
        [
            "In fact, this is all best illustrated using a picture.",
            "So here I have a very nice separable data set.",
            "Two types of data copy back to my last talk that could be relapse, non relapse OK and the data separable.",
            "Now later on I'll consider the point where the data is meshed together but as it stands that is that's my data.",
            "OK now my classifier or decision function can be formulated as a. Hyperplane directed hyperplane so that on this side everything is positive.",
            "On this side everything is negative.",
            "Indeed I might be able to.",
            "Hopefully in my next."
        ],
        [
            "My justify why that separators are hyperplane.",
            "Yes, in this space.",
            "OK a hyperplane will be actually described by this.",
            "This is a W will be my normal to the hyperplane, so it's a bit like N X = C. That would be the standard for formulation of hyperplane that I'd get in my mass.",
            "OK. And indeed this object.",
            "Therefore when the equals zero, is the separatrix.",
            "I have in my decision function."
        ],
        [
            "But coming back to the basic SVM scenario, what the the theorem from Cisco learning Theory says is the best generalization is achieved by maximizing the distance between the hyperplane here and the closest points on both sides.",
            "In fact, that seems intuitively fairly clear, because if I had a separator which was like this OK, where does separate the two classes of data?",
            "But it doesn't look quite as good a solution, and indeed if my separator was so OK, then it's making a mistake.",
            "That would be mistaken, and these two points would be in error as well.",
            "OK, so intuitively that hyperplane which is maximally distant from the closest points on both sides seems pretty clear solution, and indeed these points which are closest to the separating hyperplane.",
            "They are the support vectors and those points which are non support vectors actually.",
            "Not there, not support vectors, but later on RC I can actually remove these data points and my decision function will remain the same.",
            "Now you actually notice one thing immediately, but if I do have rather unusual outliers, perhaps a negative here it has rather a big influence on that separating hyperplane.",
            "I'll come back to that point later on when I consider the soft margin, but considering a fairly separable, nicely distributed data set, no outliers, then intuitively that seems a good choice.",
            "OK, the."
        ],
        [
            "Axle separating hyperplane indeed.",
            "I've just explained.",
            "Therefore this hyperplane separating these two types of data can be written like so and this object here is in here and if that object is positive then I have a plus one out and if it's negative then I have a -- 1 out OK and I call this the ways and B is the bias."
        ],
        [
            "Switch.",
            "OK, now one thing."
        ],
        [
            "Notice actually about that decision function, come back to it.",
            "I can positively re scale the W and the B and it has no influence on the decision function because that sign only depends on the sign function and it depends on the sign of the argument, not on its magnitude.",
            "So in other words, is invariant under."
        ],
        [
            "Scaling of the W in the be OK, and that is a sort of awkward point that I can re scale the W in the B and why it's awkward is because just now I talked about maximizing the margin, the distance between the separating hyperplane and the closest points on each side where there's a distance measuring there OK, and this gives me some sort of.",
            "Scale invariants going on.",
            "So indeed what I have to do with the SVM is I'm explicitly fixed the scale."
        ],
        [
            "OK, in other words, I dictate that W X + B is plus one on one side W X + B is minus one on the other side.",
            "If I do that then I talk about Canonical hyperplanes.",
            "OK, what I'm going saying is let me go back to the picture."
        ],
        [
            "I have this invariants affixed invariants implicitly, and that's implicitly to fixing an actual measure for the distance here between the margin and the support vectors.",
            "OK.",
            "In other words, these support vectors, the WX plus B -- 1.",
            "These will be greater than minus one.",
            "The support vectors.",
            "Here we WX plus B we plus one.",
            "These will be greater than plus one OK, and there is my setting up of the basics in our."
        ],
        [
            "Yeah, so I've dictated that for the support vectors on both sides, the plus one and negative on the other side as well.",
            "With these two equations are true.",
            "Then if I take the difference between the two then."
        ],
        [
            "I get the following, so let X1."
        ],
        [
            "So let X1 perhaps be this point here X2 must be a point on the other side.",
            "Then I have the."
        ],
        [
            "Going, if I subtract the two, I subtract 1 of the equation for the other, get rid of the bees that have W is dot X 1 -- X two equals two OK?"
        ],
        [
            "Just by subtracting top equation."
        ],
        [
            "Bottom equation from the top equation, right?",
            "So this is the case and if X1 and X2 on."
        ],
        [
            "Both sides are support vectors.",
            "Now I can also see that the margin is given by the projection of the vector X 1 -- X two onto the normal vector.",
            "The hyperplane which actually is W over model W. OK, let me just go back to that."
        ],
        [
            "Basic picture, yeah, here it was.",
            "I'm saying the recording system learning theory.",
            "I should maximize the margin this distance here, OK?",
            "And I had a X1 here and X2 here.",
            "The vector which connects or two is X 1 -- X two and that's the vector there in red.",
            "OK here is my separating hyperplane Witcher said just now was W dot X = B OK?",
            "Well obviously the normal to that hyperplane is WW over MoD W. If I make it a normalized vector.",
            "So indeed, the margin.",
            "This thing is the projection of this vector onto the normal.",
            "OK, so that's it.",
            "I want to maximize that thing.",
            "In other words, I want to maximize the projection of this onto W over MoD W. So if I do that.",
            "I get the following."
        ],
        [
            "Yeah, so I want to project this onto this OK and just from my."
        ],
        [
            "This equation.",
            "I note that if I divide this by MoD W. Then this quantity, provided this divided by MoD W, is actually the projection of X 1 -- X two onto the normal to hyperplane.",
            "What I'm saying therefore, if you follow this argument is that to maximize the margin I must maximize two or MoD W OK."
        ],
        [
            "That will do it for me, so there it is to maximize the margin I must maximize this quantity here.",
            "Well, I actually just know how to to hear, but naturally just dropping the two anything which maximizes this will maximize the margin.",
            "OK, so that's my first step.",
            "I must maximize the object in order to maximize the margin.",
            "And maximizing one or mug W. Sorry yeah, maximizing one or more W is equivalent to minimizing MoD W. OK, maximizing 1X is just the same thing as minimizing X OK, so to maximize my margin, my argument is that I must minimize MoD W. Now I must minimum."
        ],
        [
            "Eyes model you.",
            "Subject to constraints.",
            "OK, so this is a thing that she maximized my margin minimize MoD W or WW.",
            "OK, so anybody getting questions at this point?",
            "You also happy, yeah?",
            "OK, now that is that expressions do with maximizing the margin, but I had those constraints in there.",
            "I had W dot X + B is plus one on one side minus one, the other OK.",
            "It was plus one wise plus one.",
            "In other words, the product that is positive and the other side I had W dot X + B is minus one winner.",
            "Hand labeled points wise minus one.",
            "So indeed minus one times a negative here.",
            "Is also positive, and indeed this term here encapsulates those two equations I had for the Canonical hyperplane.",
            "So in the day when I was must do for my SVM is maximizing margin subject to constraints.",
            "Now you know from optimization theory."
        ],
        [
            "Therefore, that this is formulated as a Lagrangian and have a primal objective function, that's a Max margin bit.",
            "That's my constraints, and these are my La Grange multipliers, Alpha I and requirement in LaGrange multipliers.",
            "Therefore, is the Alpha I must be greater than or equal to zero?",
            "OK, now I this is 1 formulation.",
            "My problem I called it the primal formulation, but it is not the formulation I work with.",
            "In a support vector machine, because it's actually the dual formulation that I really is more convenient, it is possible to solve a certain type of learning machine from this stage here, but it's actually a later stage that we're going to use so to arrive at the jewel, so this is a primal.",
            "I wish to arrive at the jewel.",
            "I take the circle point equations which are DL by D, B = 0 and that deal by DB on.",
            "This will give me.",
            "This requirement here OK, what am I actually doing with the duality here well?",
            "I don't know whether a lot of how many people whose familiarity with optimization, but briefly, there's a primal problem.",
            "OK, which perhaps my primal problem is a quadratic OK, and the dual problem.",
            "This is a mint type problem.",
            "The jewel problems are Max type problem an at the solution, the primal and the jewel have those.",
            "Actually the same solution.",
            "You can drive one from the other.",
            "OK, so that's what I'm trying to do I.",
            "A jewel version of this which gives me which when solved will give me the same solution.",
            "So deal by DB OK gives me this.",
            "And.",
            "DL by DW.",
            "Let me actually do that here.",
            "DL by DW.",
            "That was going to give me a W here because I have W square is going to be double here.",
            "It's going to attract a term and."
        ],
        [
            "Indeed, deal by DW gives me this expression here, OK, which must be satisfied at the solution of my primal problem.",
            "OK, when DL by DW the solution, this must be the case.",
            "Now I can take this W expression so."
        ],
        [
            "Shoot back in in here.",
            "Here and here and here and get the dual solution.",
            "OK, I'll just do it."
        ],
        [
            "And then explain the sort of philosophy of this proof so."
        ],
        [
            "W goes in there and if I place the W in there, naturally the B&AW now gone.",
            "I get this jewel formulation here.",
            "OK, so yes, that's my argument, therefore is from Cisco learning theory.",
            "I know it must maximize the margin, but it had constraints in it that set me up.",
            "The primal LaGrange am and the problem with religion looks roughly sort of like this.",
            "There's always a dual formulation of any problem in optimization theory, and actually when I solve the primal problem.",
            "The solution of the dual problem is also done, and indeed the solution of the primal problem was dealt by DB equals zero deal by W = 0.",
            "They gave me this requirements.",
            "So if I'm able to actually minimize this expression here, then I will arrive at exactly the same solution now.",
            "I say here, so maximize that function, maximize the jewel is actually called the Wolf jewel, which is this expression here.",
            "OK, this actually is the main equation for support vector machines and I want to just pause and have a look at it before I want drawn to do other things.",
            "First of all, what you notice is the data is coming in here.",
            "OK, that's a data X is an their corresponding labels, which are the wise and indeed my Max task is in terms of the alphas, the LaGrange multipliers.",
            "It is actually constrained optimization because those alphas must be positive."
        ],
        [
            "And indeed, as the second constraint which we don't overlook, which is this constraint?"
        ],
        [
            "Here, so it is constrained optimization, but you'll notice important point is my Max task over the alphas involves a quadratic programming type task.",
            "Alpha and Alpha squared types pressure, so it's a quadratic, just like here actually here, and the Max is going to have some unique solution.",
            "OK indeed all this sort of ties together because if we consider that maximum maximum separating hyperplane.",
            "That I started with.",
            "You can probably see there's only sort of 1 solution really, and hence I get only one solution to this problem here.",
            "OK, so indeed that is my ascential.",
            "SVM problem Max.",
            "This object subject to constraints given with respect to Alpha and it's it's convex only one solution OK and my data just simply plunk my data in here and do the quadratic programming get my solution.",
            "Of course another feature is a very efficient algorithms for quadratic programming so I can handle that."
        ],
        [
            "Large datasets.",
            "Right, So what I would do is I would solve that quadratic programming problem having got the alphas I put the alphas in here.",
            "OK now initially this was W. This would have been W zed plus be OK so this would have the decision function bit like a neural network would be W zed plus B. I've put in the W. The equation I got just now, namely this thing here.",
            "This expression I put that in where W is over there and that gave me as my decision function here.",
            "OK, so my general SVM task for binary classification is I put in the data into that expression just gave you.",
            "I solve the quadratic programming problem.",
            "My solution is put in here.",
            "My data is here, that's just data and this is my new test point.",
            "I wish to evaluate.",
            "I haven't said how to get the B, I'm just going to stay to formula for it in a second but.",
            "Not really comment on it beyond that now certain things one notices a comment I made earlier.",
            "The function has an explicit dependence on the data, unlike a neural network.",
            "For example.",
            "The other point is if you do this in reality, you'll actually find a lot of these alphas are zero.",
            "OK, Alpha equals zero, it means the corresponding sample has no influence on the decision function.",
            "Indeed there all hearts back to what I."
        ],
        [
            "Started with earlier where if you notice these support vectors have influence on where that hyperplane should be.",
            "But if I remove this guy, the half plane would be exactly where it is similar here, here and he's done.",
            "Select support vectors on the other side, so the non spec support vectors don't influence where the hyperplane should be and equivalently when I look at the."
        ],
        [
            "Decision function which I had here, they'll appear as Alpha equals zero.",
            "They will not appear in the decision function, have no influence on where the hyperlink should be OK. OK, so let's see a central story for the basic SVM.",
            "OK, any questions at this point you're happy.",
            "OK, probably some of you are familiar with this anyway, yeah?",
            "Yeah."
        ],
        [
            "Well, which one this one?",
            "Oh, that shouldn't be there.",
            "Yeah, actually this has been borrowed on the latex regression where it should be there.",
            "So scrub that out.",
            "Yeah, it should go.",
            "There should be actually social BWB Alpha as a mistake.",
            "OK there might be other bugs in here, yeah?",
            "I think I was moving some late."
        ],
        [
            "Check around and lose yourself, right?",
            "So far we don't know how how to handle non separable datasets.",
            "So the picture I've just given you is OK, but it is a very nice sort of well behaved data set where is quite separable.",
            "What happens when the data is intermeshed?",
            "You'll know where something like Neil Networks for things like X or problem and things like that.",
            "I must have hidden nodes and So what do I do in this case?",
            "Well, actually I have to fall back on the second bit of the theorem I mentioned at the beginning, which I just state its consequences, not what the theorem is.",
            "The second part of the theorem I just remind you is that.",
            "The theoretical generalization error or the bound does not depend on the dimension of the space.",
            "In other words, I get the same if I maximize the margin.",
            "I guess I should get the same sort of general performance independently of whether men attend a mental space or 100 dimensions or 1000."
        ],
        [
            "Pensions OK. Now another thing you will notice is that the objective function the data appears in the form of."
        ],
        [
            "The product I'm trying to just skip back here if you notice, here's my data.",
            "OK now the X is are in the form of an inner product.",
            "Indeed, I can refine, define, redefine my X my YIXI as a new object X prime, and essentially there for my data only appears in the form of a scalar product."
        ],
        [
            "OK.",
            "So I've noted the following.",
            "My generalization, Brown does not depend on dimension of the space, and also that my data only appears in my objective function in the form of an inner product or scalar product.",
            "So indeed this brings me to the elaboration of my support vector machine, namely what I can do if I have a data set which is highly intermeshed, not separable, then I can mark my data to a higher dimensional space by replacement.",
            "OK, by mapping function an, if I map data from a low dimension to high dimension, practically I think it always if I go high enough I will be able to separate the data.",
            "OK, so just to illustrate the point, if I was just in two dimensions, OK an I had.",
            "Plus minus minus plus in two dimensions.",
            "I can't find hyperplane which separates the two types of data, but if there is a 3 dimensions OK and these are sort of in the board and these are sort of like a minus here, I can't really do it, but let me circle it.",
            "OK so I have pluses here and the two minus is here where I can easily put a hyperplane that separates the two.",
            "So in two dimensions I can't find a hyperplane that separates them but in a 3 dimensional world where these minuses are outside the board I can easily find hyperplane that will do it.",
            "Indeed, therefore, if I map into high enough dimensional space, then I'll be able to separate my data and I won't have a problem.",
            "OK now I will do that therefore and my data is in the form of a dot product or inner product and this is I must have a mapping function that Maps me for my input space into this higher dimensional space which."
        ],
        [
            "Well, cool feature space.",
            "Now there's this mapping function and I call it 5 and I called the higher dimensional space are mapped to a feature space, and one thing which must be true about the spacer map into is that there must be a Hilbert space and textbooks site got it wrong here, it can be a pre Hilbert space or inner product space.",
            "OK, but what it all boils down to is of course and mapping and inner product.",
            "So the spacer map into an inner product must be defined.",
            "So it must be an inner product space or he pre Hill."
        ],
        [
            "Space.",
            "Now, one thing."
        ],
        [
            "It may be bothering, yeah, one thing that's important, of course, is if I'm going to map into this high dimensional space, I must have some mapping function.",
            "OK, now one of the in very simple cases that polynomial kernels I can actually figure out what the mapping function is, but one of the curious things about kernel map makes methods is I can map into high dimensional space without actually knowing what the mapping function is.",
            "Because if I define.",
            "A functional form for my inner product in that high dimensional space.",
            "It implicitly defines a mapping function, but I don't need to know what the mapping function is."
        ],
        [
            "OK.",
            "Words what I'm saying here is the following.",
            "I had an inner product and this is now my Maps in a product and if I define certain types of kernels, they implicitly define certain types of embedding functions, but I don't need to know what the bedding function is.",
            "OK, so just define a kernel, provide it's OK and implicitly define a mapping function, implicitly define a high dimensional feature space?",
            "Well, I don't need to bother with that file.",
            "X is just doesn't matter, so don't don't bother and the cut this object here is there for the kernel and it is there for inner product in a Mac.",
            "Pairs of points in feature space, yeah.",
            "As you say, we don't care about the future space.",
            "You don't care about the mapping function, yes.",
            "He quit.",
            "How do?",
            "True.",
            "Yeah, it does mean one thing is apparent in that the kernel function.",
            "I can't just choose any kernel function because the kernel function must actually represent an inner product in some higher dimensional space, so that's going to put a restriction, yeah?",
            "You have about another problem because you have some problem with your data, yeah?",
            "Four girl thing yeah.",
            "Well separable yeah.",
            "Yeah.",
            "This is what I do with girls myself, gushing camera yeah.",
            "Phone number for Danville?",
            "Understand why, what why?",
            "Yeah, unfortunately it's a lot of the reply that is buried in the mass papers in that they defined.",
            "They drive theorems which restrict the kernel function and which restrict you to a inner product space OK. And that bit is a little bit buried in mass.",
            "I'll show you what the end end effect of this is is a requirement in the kernel and if you satisfy that requirement then it's a legitimate kernel.",
            "Are you busy this morning with brother?",
            "Yeah yeah.",
            "But I will answer that point in the second.",
            "I haven't actually defined what are the strictures on Ki.",
            "Haven't defined that?",
            "OK, I've just.",
            "I'm just saying it's a slightly strange point that I don't need to know what the mapping function is because I provide it satisfies these theorems and it's OK."
        ],
        [
            "Right, So what is the mapping relation?",
            "In fact, we do not need to know the formulas mapping since it's implicitly defined by the functional form within the pro."
        ],
        [
            "Not OK mapped in a product now.",
            "Some kernels which are commonly used to talk about them, then I'll talk about the theorems, which was strictly kernels.",
            "Different types of curls to find different types of Hilbert spaces.",
            "OK, very popular.",
            "Kernel is an RBF or Gaussian kernel which I have here.",
            "These are my data points and have a kernel parameter Sigma here which alters the shape of the space, but I've used.",
            "I've had vase.",
            "Non separable datasets axle problem in Paraty sort of.",
            "The data is hugely into and this is excellent eye problem.",
            "But if I went behind, had a hypercube and every time I move along an edge I flip the sign heavily into meshed.",
            "Well I've done em Paraty using a RBF kernel in a work fine too spirals problem.",
            "Another classic sort of highly intermesh data set.",
            "I've done it also with an RBF kernel.",
            "I've not actually found.",
            "Any problem with I couldn't do with an RBF kernel OK because in fact it is indeed actually defining sort of infinite dimensional space.",
            "An another choice is a polynomial kernel relatively infrequently used RBF kernel Gauss."
        ],
        [
            "Kernels very, very common.",
            "There's in fact the whole family of these kernels, a huge number, and mellows written a book which defines lots of kernels.",
            "Legitimate curls are not restricted to simple functions like the Gaussian kernel.",
            "OK, indeed, there's a huge class of kernels which are possible.",
            "Just show you two types of kernels without really much explanation, but which you could use.",
            "OK, you maybe wanted to compare strings, and indeed this morning.",
            "With the aquila's talk he was talking about strings and very common in many bioinformatics applications, and you notice these strings karkat cart chart have some certain similarity.",
            "OK, and you can define little algorithm algorithms, edit codes Anson, which actually will enable you to score the similarity of strings.",
            "OK, well, that means you can drive string kernels.",
            "The string kernels not.",
            "Therefore, function it is actually defined by now with."
        ],
        [
            "But it's a legitimate kernel to use OK, and I'm just saying here, such kernels are very important in Barn formatics text process."
        ],
        [
            "Sing, you have text as well, etc.",
            "Another type unusual kernel is of course you might have a graph.",
            "OK, this could be a graph to do with Inter relationships of jeans.",
            "Are they functionally related or not?",
            "And you can consider the similarity of certain graphs.",
            "OK, if I drew another set of nodes as many as here and then just wiped out one or two links, I would easily see that they're very similar graphs.",
            "Then I could have other grass which are very dissimilar so you know.",
            "Even by looking at the Mike realize I can define some sort of similarity measure.",
            "Overlap graphs.",
            "OK was a thing called the diffusion kernel, visually defined by Condor Lafferty, which enables you to drive a kernel over graphs and networks.",
            "OK, So what I'm saying is that the kernels are very big, broad class of possibilities, which essentially means that.",
            "Huge range of models to consider.",
            "Indeed, that's why we talk about kernel based methods rather than SVM's 'cause the whole subject, the kernel based methods is broader than a support vector machine and encompass is all the various different types of kernels I can consider.",
            "Indeed, if I had chosen that polynomial kernel, I could just now I can re derive splines from statistics statistics if I choose attenti type kernel I can drive neural networks.",
            "If I choose account Gaussian RBF type kernel.",
            "I can get an RBF.",
            "Network OK.",
            "In other words, a lot of things are previously considered as independent models are just subcases of kernel based methods were picked at a particular type of kernel.",
            "OK and importantly I can if I had a new network I wouldn't really know what to do with about a graph and if I had."
        ],
        [
            "Strings of unequal lengths and a new network.",
            "I wouldn't really know what to do.",
            "A new network typically would have 100 inputs, say.",
            "Where is this thing is requiring three 510 inputs, etc.",
            "So you know I have a real problem with the neural network trying to do do some classification."
        ],
        [
            "Strings say so I'm able to define a very broad class of models.",
            "That's what makes them powerful.",
            "Convexity make some powerful, relatively few parameters, and."
        ],
        [
            "The possibility of a broad range of kernels.",
            "Now how said there are restrictions on the kernels, I can't just choose anything, only certain things of valid.",
            "And indeed the initial sort of restrictions you come across as."
        ],
        [
            "Some textbooks is the Mercer conditions and these were approved by mathematicians and provided your kernel satisfies these two equations, then it's a legitimate kernel.",
            "An inner product is defined and there's a legitimate kernel, but in fact the simplest criterion could consider is that the kernel is any object which is positive semidefinite.",
            "OK, provided that's the case of bottom criterion is true for your kernel then."
        ],
        [
            "So legitimate kernel to use.",
            "OK, so if a kernel is positive semidefinite here we are where these are real numbers then, is this a function 5X defining inner product of possibly higher dimension?",
            "OK which my kernel is equal to that mapped in a product.",
            "OK, so that's it, it's very broad classes."
        ],
        [
            "Objects that I can use.",
            "So now summarize then the main stacks.",
            "Depending on my particular problem text graphs.",
            "All men are different data.",
            "I would choose the kernel may have a kernel parameter in it, but I don't know if I consider this or in the next lecture.",
            "I don't maximize this function over the Alpha squared subject constraints, so it's constrained optimization constrained quadratic programming."
        ],
        [
            "OK, and I find the bars which I haven't talked about til up to now, but this is an equation which you can drive for the bias."
        ],
        [
            "OK, and how we got the bias, the alphas and my choice of kernel.",
            "I would now put a new zed in here and it would give me a plus or minus one and that would be my binary."
        ],
        [
            "Classifier that I'd use OK now so.",
            "That is my what I've done therefore is introduce a.",
            "A support vector machine.",
            "My basic very basic type of kernel based method.",
            "I've showed that you can map to high dimensional space, show there's a bunch of a broad class of kernel based methods, but I still have a few other problems to sort out.",
            "Indeed, one is I pointed out that outliers have a big influence on that separating hyperplane at that motivates the topic of soft margins.",
            "Another issue is that the SVM is well set up for binary classification.",
            "What do I do about multiclass classification?",
            "Many problems of old multi class, and so I must have some scheme to handle that.",
            "Indeed, the scheme are going to give you now the directed acyclic graph way of doing it is was invented by Nella Cristianini and John Platt and published in NIPS, but."
        ],
        [
            "But the idea is relatively straightforward, so enable me to do multi class using the framework I've just given you.",
            "Namely, suppose I had a three class problem, 1, two and three.",
            "Then I can do a directed acyclic graph where my first task is to say is it class one or class 3.",
            "If it's one, is it one or two?",
            "If a phone is 2 ago here, fine, it's when I go here, similar if I had found its three and make a choice to three, and I eventually get here.",
            "And when I'm just saying is for relatively small number of classes.",
            "Such a scheme is OK, and in particular what you notice is that all these steps here are binary classification, so they're done exactly using the method which I've given you just now.",
            "OK, this becomes rather inadequate.",
            "If I had some like 100 classes, OK, wouldn't be the way to do it.",
            "Now will say a lot of people trying to look at multi class classification with SVM's.",
            "They're not the binary classification cases, not well set up for multi class.",
            "There's a number of schemes and they all sort of have faced similar performance OK."
        ],
        [
            "So that is one way of doing it.",
            "That's the end of my story for multiclass.",
            "The other story was what happens when I have data points which actually have an influence, are outliers and really pushed the separating hyperplane around because I try and maximize the margin but one data point can mess things up for me.",
            "Now most real life datasets contain noise and the SVM would give me poor generalization as it fits the noise.",
            "I will say something about this before plowing to this point here.",
            "If I actually do the story of the SVM, which I've just given you.",
            "Let me."
        ],
        [
            "Actually flip back to here, OK?",
            "I said just now if I took a separable data set I would find sometimes the alphas are zero and not support vectors.",
            "Sometimes the alphas are non zero.",
            "There are support vectors OK and they are the closest current happens suffering hyperplane if you run it in practice.",
            "You also might find Alphas, VO very large.",
            "If you have Alpha very large then it could be a correct point which is very very unusual.",
            "Alternatively it usually indicates an outlier.",
            "And actually give you one good illustration of this.",
            "In my previous lecture I talked about leukemia and I used the SVM with.",
            "Cancer data sets in the very early study done at MIT in Tagalog's Group.",
            "It was leukemia trying to distinguish lymphoblastic leukemia from myeloid leukemia.",
            "OK, fairly similar types of leukemias and when they ran it, they SVM to try and do this binary classification tasks.",
            "There was one patient with a very large Alpha value.",
            "Now they knew there for this indicates either rather unusual but correct date data point or possibly an outlier.",
            "So they went back to the medics and asked what this patient seems to be unusual.",
            "Very large Alpha value.",
            "There is only the patient and the patient had been misclassified OK, so a large Alpha value can indeed."
        ],
        [
            "Dayton outlier OK, so it's good idea to be aware of that, but the conventional way to lessen the influence of noise in the data is to introduce a soft margin.",
            "Now in textbooks you'll find usually one of these two, but in fact there's two types of Softmart."
        ],
        [
            "Oceans that one would consider the one you get in textbooks is the L1 error norm, and previously I said that because Alpha is a LaGrange multiplier must be positive.",
            "So I had this bit.",
            "But in fact I don't think I'd give the proof here, but you can show that I can choose.",
            "There are normal by actually putting upper limit on the A1.",
            "Sort of easy way to see this is I said just now a large Alpha value usually indicates an outlier.",
            "So far cap the limit for the Alpha is going to lessen its influence.",
            "OK, but in fact you can prove formally prove that this is a good adaptation.",
            "So your scenario for the SVM is just like before.",
            "But whereas before I head out for greater angle to 0, now I've got this criterium.",
            "And that is one way to induce the soft margin, and that is the soft margin parameter.",
            "The alternative, which is not often in textbooks, which is pretty robust, is to take the situation for the SVM which I've described up to now.",
            "Look at the diagonal components of the kernel.",
            "In other words, K, XI, XI and add a small positive constant to those diagonal components of the kernel.",
            "OK, only that.",
            "That's all you do, so it's a normal SVM story.",
            "But I had to."
        ],
        [
            "Full positive Mount to the.",
            "Colonel Diagonal now this is actually the first of those using the soft margin.",
            "Now I'm not quite sure if I've inverted one on C or whether it's enough to quickly think about that and get bit tired, but it may be one on.",
            "See down here or maybe see but important point, I want to know is this is actually also ionosphere data set from the UCI repository, quite a noisy data set, and when it boils down to this must be season finity.",
            "This must be one.",
            "See down below sees infinite.",
            "Or not know structurally A1 CSF was zero and.",
            "If I have no soft margin, my test error is 7%.",
            "Another hand as I introduce to see this actually one on, see below as I might see goes from Infinity down to some finite value.",
            "Then my test error falls OK, falls down to about 5.4, so it's improvement if I keep pushing my C smaller and smaller.",
            "OK, so my alphas lying on a range like so.",
            "And I'm decreasing the C, making it smaller smaller course.",
            "In the end my SCM just can't learn the problem.",
            "I'm trying to give it.",
            "OK so my error starts climbing on the other side, but there's a point in there where if I introduce a soft margin, I definitely dropped the test error.",
            "This is the L1."
        ],
        [
            "Error norm not so well, sort of described in textbooks is the L2 error norm, which is just the it's quite stable, robust positive quantity added to the kernel diagonal.",
            "Again, this is the Lambda OK. Again, Lambda equals zero.",
            "No L2 error norm.",
            "7% error introduce diagonal component as addition to my kernel diagonal.",
            "My test error falls.",
            "Then if I start introducing too much I will.",
            "Addition to my Colonel diagonal then course after a time my album, just my math, my SPM."
        ],
        [
            "Work, my error starts climbing OK, right?",
            "Indeed I think I maybe I prove some of these now.",
            "Justification for the Xarelto.",
            "Yeah, the L1 error norm its justification is that without the hard margin, that means no soft margin.",
            "That was the case I had before I actually had that equation stated for the soft margin.",
            "The L1 error norm I allow for points actually in what's called the margin band.",
            "If you remember, I had the separating hyperplane had the support vectors here support vectors here.",
            "This is equivalent to allowing some points in the margin ban or even totally wrong.",
            "OK, if that is now greater than one, it would mean that that data point can actually be wrong.",
            "So my training error is not zero, right?",
            "So that is how I started."
        ],
        [
            "Out justifying the L1 error norm and there's variance on this picture I've just mentioned.",
            "At alternative approach called the new SVM Solutions for less L1 error normal same as obtained from maximizing this function.",
            "Here there's very."
        ],
        [
            "This on it different ways, of which I can produce a soft margin, OK?"
        ],
        [
            "Very similar type problem.",
            "The soft margin parameter if I view it as so or as an addition to my kernel diagonal.",
            "Intuitively it's not quite to clear what you're doing in this alternative form.",
            "Formulation of the soft margin.",
            "The new SVM.",
            "The soft margin has a very clear and transparent meaning.",
            "It's a fracture of training errors.",
            "Upper bounded by Mu, which provides a lower bound on the fractional points which are support vectors.",
            "I think I will."
        ],
        [
            "At this point, but just point out that there's different ways of doing a soft margin.",
            "Right, so up to now, I've done binary classification.",
            "I've done multiclass classification.",
            "I want to do regression.",
            "Now.",
            "It was somebody else question, yeah?",
            "Intuitive explanation for the second language.",
            "I don't think I've got.",
            "If I go back to this way."
        ],
        [
            "One which is here.",
            "Not too intuitive, that's why Alex Smolen burner shook off to the new SVM, which sort of makes it more intuitive.",
            "With this method, I'll tell you it's starting point for the derivation is not that difficult to drive it."
        ],
        [
            "The L2 error norm this is the L1 error norm.",
            "OK, the L2 error norm is simply the same thing, but it's squared up here.",
            "That's just squared up some sort of doubly penalizing the error in a different way.",
            "OK, you don't have to do the equations you square up here, fail to an error norm.",
            "You set up a Lagrangian just like before, but you got additional term here and they just do some mass and you find actually all boils down to addition to the kernel parameter.",
            "At least that is an exercise for you to do."
        ],
        [
            "So like lecture right?",
            "So solving regression tasks, I've done the classification, including multitask, whilst the story when you have a continuous valued output.",
            "OK, don't get bogged down in the detail because I'll just show you the story very similar.",
            "Now my function OK doesn't have assigned function in here.",
            "It's not a classifier, my function is continuous valued and is simply going to be W dot X + B. OK, now I'm just."
        ],
        [
            "I'm going to say about regression.",
            "Is like with the SVM you try and minimized model W squared that I did exactly for binary classification.",
            "I have two types of errors now because I have two types of training errors depending on which side."
        ],
        [
            "Of this, functionally am OK.",
            "So if you like I could be an error decrementing this or incrementing it.",
            "So I gotta plus type of error or minus Typeerror added to this function here taking me away from the correct value.",
            "I might want .2 here and that's .1 and I have put plus .1 as my error or it might be point 3.4 and I have a minus.",
            "OK so I have."
        ],
        [
            "Two times errors.",
            "And that is accounted for by these two types of water, called slack variables, very similar to the soft margins slack variables and use earlier."
        ],
        [
            "Well, now you just plow through."
        ],
        [
            "The mass OK, and what it all boils down to is I have a.",
            "A function which I must minimize, which looks like the following.",
            "This is very similar to the maximizing the margin term.",
            "This is now to do with the errors, and since I'm minimizing I want to minimize the errors both ways that I may make, and this is the tradeoff between these two OK. Now I'll just say if you start with that."
        ],
        [
            "You put in the constraints you have, which are indeed this things here.",
            "These things here if you like.",
            "If this was zero and this was zero, then why I will be W dot X + B?",
            "Exactly the function I want to learn, and similarly with the other type of error.",
            "If it was zero and this was zero, then I have Y equals WX plus B again.",
            "So in fact if they are set to zero, I can easily see that I'm forcing this to equal Y.",
            "Now these two terms, this is an error you may puzzle about this epsilon that isn't allowed.",
            "A bit of error.",
            "OK, now typically if I've got noise, I don't want to have epsilon precisely zero 'cause I totally fit to noise.",
            "OK, so this is actually a equivalent to a soft margin.",
            "OK, but I don't can't have an endless number of errors, so these other errors which are going to have take me away from my function, correct functional mapping?",
            "I just don't want."
        ],
        [
            "So I want to minimize that zyan."
        ],
        [
            "So I had I what all boils down to is the objective function.",
            "I will have to do for aggression very similar to the story for classification.",
            "What you'll notice that previously I had Alpha and Alpha squared.",
            "Well now it's the same sort of thing is linear in Alpha and Alpha hat, and it's a quadratic in Alpha and Alpha hat, so it's a convex type problem.",
            "A quadratic problem, just like before 1 solution and that makes it quite nice.",
            "OK, one last point, I should mention about this is if you ever encountered having to solve this thing, one must say you must not do is the following.",
            "Don't define this as Alpha minus and this is Alpha plus and do the optimization in terms of those re define variables because that is a different problem.",
            "It is actually if you think about it, no longer a quadratic programming problem.",
            "OK, so to remain as a quadratic programming problem you've got to do it in terms of this formulation here never.",
            "Some people have tried that to redefine.",
            "You actually get a semi working thing is actually not a convex problem.",
            "OK, so you must strictly do."
        ],
        [
            "As stated.",
            "So."
        ],
        [
            "This quadratic type."
        ],
        [
            "That is, solve subject to constraints.",
            "Just like previously, and those constraints are.",
            "This is equivalent of my summon Iyi Alpha I type condition, and this is like that soft margin condition I had previously OK."
        ],
        [
            "And they the Alpha and Alpha Hatala growth multipliers."
        ],
        [
            "So what I would do if I just run back is out solve this task, give you a present the data data coming in here the corresponding labels coming in here I make a choice for epsilon.",
            "Typically I would mess around with that actually defined.",
            "There is a psychological thing.",
            "There's certain suggested values for certain types of problems, but I would.",
            "Could you could if you wish, set that to zero, OK if you wish to start with, but I would put my data in in my kernel and so."
        ],
        [
            "Solve the quadratic programming task."
        ],
        [
            "Subject constraints having got my Alpha, my Alpha hat, and my choice of kernel, I put in a new data point said and I should get out a continuous valued number like so.",
            "I'm actually tried out regression using this method works quite well.",
            "I'm not quite sure whether it's totally competitive with some other ways of doing it, so there may be other ways of doing aggression which are similar."
        ],
        [
            "Most maybe slightly better, right?",
            "We can define many types of loss function, so there's different ways to do the regression.",
            "We can define a linear programming approach to regression.",
            "I did it just now in terms of quadratic programming and we can use other types of approaches to regression, one which had been pushed a lot by Johan circles in Belgium is a kernelized least squares approach.",
            "OK, So what I'm saying is the bigger.",
            "Field in this.",
            "Cancel.",
            "Maybe?",
            "Collaboration means yeah, OK, I think it may have come to the end here I know applications.",
            "Yeah.",
            "Ascentia Lee."
        ],
        [
            "It sort of operates a bit like the following.",
            "That slack variables the Alpha of the Zion's I hacked so come in here.",
            "This is exactly what I want to do is find out why here that is my function I wish to set up I have a slack variables on both sides.",
            "OK now my objective function tries to force those slack variables to zero which forces me towards this function here.",
            "OK now the epsilon bid which was actually."
        ],
        [
            "And there is actually this type of loss function.",
            "You have other types of loss functions as well.",
            "Basically there's no penalization from the loss function if you're in a certain band around the function.",
            "Outside that bound, it begins to penalize you so it looks a bit like the following.",
            "Let me do the sign function which afterwards regression OK.",
            "Here's my.",
            "Sine function, The actual data give you is a little bit corrupted by noise.",
            "OK, and looks a bit like this and this sort of band.",
            "OK looks a bit like the following.",
            "OK, so if I minimize my objective function OK and I force those slack variables."
        ],
        [
            "Zero, so I had the objective function here.",
            "If I did, it got away with that force.",
            "These to zero OK, but had an epsilon which is not zero.",
            "OK then actually the solution I end up with at the end of the day is a function which is quite smooth, OK, and which sort of goes through this space and we roughly map the sine wave.",
            "OK, if I tell an epsilon which was precisely zero, then obviously what I'm doing here is shrinking this band to 0.",
            "And unfortunately I don't have different colors here, but what I would do would fit exactly to these data points.",
            "It fits the noise and this gives me a sub optimal solution.",
            "OK, so that's roughly how it works.",
            "Not quite as clear as the binder classifica."
        ],
        [
            "Right applications vast number applications.",
            "Too many summarize here."
        ],
        [
            "Two applications share our repeat one."
        ],
        [
            "Leave earlier."
        ],
        [
            "So."
        ],
        [
            "This is."
        ],
        [
            "Am I crazy?"
        ],
        [
            "What is seen this?"
        ],
        [
            "So I'll skip that.",
            "Was sick one recognition of ZIP codes very important real life standard benchmarking data set is NIST and.",
            "This slides are wrote down a year or two ago, so this is probably not accurate any longer, but state of the art for recognition of Postal codes was a neural network."
        ],
        [
            "The net five for some time, whereas Burner shook off Dennis to cost.",
            "Using SVM with virtual training and they get a improvement.",
            ".15 which is good in this context in terms of the test error reduction for recognition of Postal codes.",
            "So you know this is a huge important application and one way of the SVM comes out top.",
            "Actually, that's the end of my talk.",
            "Now there is a second talk.",
            "Who wants to the second or probably be a break when I get going to drink 2nd?",
            "The talks about sort of elaborating this picture how to do it not using quadratic programming but linear programming model complexity.",
            "I can't remember what's in the second or actually, but does anybody want to do the second talk?",
            "OK, if there's a critical mass of, there's just one or two individuals."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so this is tutorial on support vector machines and kernel methods.",
                    "label": 1
                },
                {
                    "sent": "Now the first talk is just about this is just a straightforward introduction to support vector machines.",
                    "label": 0
                },
                {
                    "sent": "I had a second optional talk.",
                    "label": 0
                },
                {
                    "sent": "I just see what the interest is at the end, which I talk about other types of kernel based methods, But this is just the standard scenario.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And first of all I deal with support vector machines for binary classification and then soft margins and multiclass classification support vector machines for regression and a little bit about support vector machines in practice.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The second talk if I get get to do it, I do a linear programming alternative approach to kernel methods, then some comments about actually how you train support vector machines.",
                    "label": 0
                },
                {
                    "sent": "The important issue of model selection.",
                    "label": 1
                },
                {
                    "sent": "OK, if we get onto this second talk, different types of kernels.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "OK, right?",
                    "label": 0
                },
                {
                    "sent": "So support vector machines are very well known.",
                    "label": 0
                },
                {
                    "sent": "Indeed, a very old subject Nella just pointed out to me on this fresco at the back.",
                    "label": 0
                },
                {
                    "sent": "There is apparently SVM somewhere, so obviously back in the Middle Ages on about them right?",
                    "label": 0
                },
                {
                    "sent": "So support vector machines principled approach to classification regression, novelty detection problems.",
                    "label": 1
                },
                {
                    "sent": "As if it good generalization in practice and also the hypothesis or decision function you may construct has explicit dependence on a day to the support vectors.",
                    "label": 0
                },
                {
                    "sent": "Hence this enables more readily the actual interpretation of the model OK, and because they have a principled approach, they've somewhat taken over neural networks as a sort of general approach to classify.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Kacian and regression.",
                    "label": 0
                },
                {
                    "sent": "Importantly, learning involves optimization of a convex function.",
                    "label": 1
                },
                {
                    "sent": "OK, now this makes it quite different from a neural network and network ascentia Lee it's not a convex problem, there are many minima or Maxima in my decision function and I can get caught up in a poor local minimum.",
                    "label": 0
                },
                {
                    "sent": "OK, whereas with a sport vector machine the actual underlying function is a quadratic and we know what a quadratic looks like.",
                    "label": 0
                },
                {
                    "sent": "It has one solution at one minimum.",
                    "label": 0
                },
                {
                    "sent": "And indeed, that's a nice thick feature of the model.",
                    "label": 1
                },
                {
                    "sent": "OK, so a convex approach to learning relatively few parameters required to train a learning machine.",
                    "label": 0
                },
                {
                    "sent": "OK, if I take a neural network for example, then I have sort of a number of unknowns.",
                    "label": 0
                },
                {
                    "sent": "One is the architecture.",
                    "label": 0
                },
                {
                    "sent": "How many hidden nodes are required or acquire hidden layer and other parameters in the neural network?",
                    "label": 1
                },
                {
                    "sent": "For example, if a tench updating function, there's a parameter in the updating function.",
                    "label": 0
                },
                {
                    "sent": "And various things like that.",
                    "label": 0
                },
                {
                    "sent": "They're not so elegant as the support vector machine as we'll see.",
                    "label": 0
                },
                {
                    "sent": "Indeed, you could actually say with a support vector machine there different linear using a linear kernel.",
                    "label": 0
                },
                {
                    "sent": "I don't have any parameters to mess around with altogether.",
                    "label": 0
                },
                {
                    "sent": "Other types of kernels.",
                    "label": 0
                },
                {
                    "sent": "I do have a kernel parameter and have various ways of approaching the problem, but just have one kernel parameter.",
                    "label": 0
                },
                {
                    "sent": "There's another type of parameter soft margin, which I will talk about later on, but relatively few parameters and I do have a strategy.",
                    "label": 0
                },
                {
                    "sent": "About how to actually handle the relatively few sets of parameters that I do have, you can implement implement confidence measures, I'll talk.",
                    "label": 0
                },
                {
                    "sent": "Think I'll talk about that little bit in this talk later on.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what I want to do is just set up the basic scenario for support vector machines for binary classification and the subject of support vector machines goes back a long way.",
                    "label": 0
                },
                {
                    "sent": "It was actually anticipated quite early on in the 1960s by Russian mathematicians, and there's also a book in Polish which anticipates the subject quite a lot, but they took off from around about 98 and being popular ever since.",
                    "label": 0
                },
                {
                    "sent": "So what I want to do is just sort of introduce binary classification.",
                    "label": 0
                },
                {
                    "sent": "The standard scenario for support vector machine and I have an input vector which I'll call XI&YI for binary classification, will only have two labels, plus or minus one OK and I call these labels the.",
                    "label": 0
                },
                {
                    "sent": "At Target or labels the index I labels pattern pairs OK, so this thing is actually a vector with a number of attributes and I labels the pattern and why eyes its corresponding target or label.",
                    "label": 1
                },
                {
                    "sent": "And so this range is overall the pattern pairs I = 1 to M and this XI therefore defines a sort of space of labeled points, which I call the input space.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is my starting point now.",
                    "label": 0
                },
                {
                    "sent": "The justification for system for support vector machines actually comes from statistical learning theory, and indeed had an introduction to this I guess, certainly with Joshua Taylor it was certainly this area and the actual justification for the subject comes from certain theoretical bounds on the generalization error, which come from this subject, and indeed I'm not actually going to define.",
                    "label": 1
                },
                {
                    "sent": "The theorem I'm just going to say the consequences of of these generalization bounds.",
                    "label": 0
                },
                {
                    "sent": "In fact, the generalization bounds are therefore derived from this school learning theory, and it actually got 2 features to them.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One is the upper bound on generalization error does not depend on the dimension of the space.",
                    "label": 1
                },
                {
                    "sent": "Now we will note this now, but it's a point I'm going to come back to later on.",
                    "label": 0
                },
                {
                    "sent": "OK, so the generalization bound doesn't depend on whether the two dimensional or 10 dimensional space, etc.",
                    "label": 1
                },
                {
                    "sent": "The other point about the theorem, which I don't state is the bound, is minimized by maximizing an object called the marginal OK, and that margin is the minimal distance between the hyperplane separating.",
                    "label": 1
                },
                {
                    "sent": "The two classes and the closest data points of each class.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In fact, this is all best illustrated using a picture.",
                    "label": 0
                },
                {
                    "sent": "So here I have a very nice separable data set.",
                    "label": 0
                },
                {
                    "sent": "Two types of data copy back to my last talk that could be relapse, non relapse OK and the data separable.",
                    "label": 0
                },
                {
                    "sent": "Now later on I'll consider the point where the data is meshed together but as it stands that is that's my data.",
                    "label": 0
                },
                {
                    "sent": "OK now my classifier or decision function can be formulated as a. Hyperplane directed hyperplane so that on this side everything is positive.",
                    "label": 0
                },
                {
                    "sent": "On this side everything is negative.",
                    "label": 0
                },
                {
                    "sent": "Indeed I might be able to.",
                    "label": 0
                },
                {
                    "sent": "Hopefully in my next.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My justify why that separators are hyperplane.",
                    "label": 0
                },
                {
                    "sent": "Yes, in this space.",
                    "label": 0
                },
                {
                    "sent": "OK a hyperplane will be actually described by this.",
                    "label": 0
                },
                {
                    "sent": "This is a W will be my normal to the hyperplane, so it's a bit like N X = C. That would be the standard for formulation of hyperplane that I'd get in my mass.",
                    "label": 0
                },
                {
                    "sent": "OK. And indeed this object.",
                    "label": 0
                },
                {
                    "sent": "Therefore when the equals zero, is the separatrix.",
                    "label": 1
                },
                {
                    "sent": "I have in my decision function.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But coming back to the basic SVM scenario, what the the theorem from Cisco learning Theory says is the best generalization is achieved by maximizing the distance between the hyperplane here and the closest points on both sides.",
                    "label": 0
                },
                {
                    "sent": "In fact, that seems intuitively fairly clear, because if I had a separator which was like this OK, where does separate the two classes of data?",
                    "label": 0
                },
                {
                    "sent": "But it doesn't look quite as good a solution, and indeed if my separator was so OK, then it's making a mistake.",
                    "label": 0
                },
                {
                    "sent": "That would be mistaken, and these two points would be in error as well.",
                    "label": 0
                },
                {
                    "sent": "OK, so intuitively that hyperplane which is maximally distant from the closest points on both sides seems pretty clear solution, and indeed these points which are closest to the separating hyperplane.",
                    "label": 0
                },
                {
                    "sent": "They are the support vectors and those points which are non support vectors actually.",
                    "label": 0
                },
                {
                    "sent": "Not there, not support vectors, but later on RC I can actually remove these data points and my decision function will remain the same.",
                    "label": 0
                },
                {
                    "sent": "Now you actually notice one thing immediately, but if I do have rather unusual outliers, perhaps a negative here it has rather a big influence on that separating hyperplane.",
                    "label": 1
                },
                {
                    "sent": "I'll come back to that point later on when I consider the soft margin, but considering a fairly separable, nicely distributed data set, no outliers, then intuitively that seems a good choice.",
                    "label": 0
                },
                {
                    "sent": "OK, the.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Axle separating hyperplane indeed.",
                    "label": 0
                },
                {
                    "sent": "I've just explained.",
                    "label": 0
                },
                {
                    "sent": "Therefore this hyperplane separating these two types of data can be written like so and this object here is in here and if that object is positive then I have a plus one out and if it's negative then I have a -- 1 out OK and I call this the ways and B is the bias.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Switch.",
                    "label": 0
                },
                {
                    "sent": "OK, now one thing.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Notice actually about that decision function, come back to it.",
                    "label": 0
                },
                {
                    "sent": "I can positively re scale the W and the B and it has no influence on the decision function because that sign only depends on the sign function and it depends on the sign of the argument, not on its magnitude.",
                    "label": 0
                },
                {
                    "sent": "So in other words, is invariant under.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Scaling of the W in the be OK, and that is a sort of awkward point that I can re scale the W in the B and why it's awkward is because just now I talked about maximizing the margin, the distance between the separating hyperplane and the closest points on each side where there's a distance measuring there OK, and this gives me some sort of.",
                    "label": 0
                },
                {
                    "sent": "Scale invariants going on.",
                    "label": 0
                },
                {
                    "sent": "So indeed what I have to do with the SVM is I'm explicitly fixed the scale.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, in other words, I dictate that W X + B is plus one on one side W X + B is minus one on the other side.",
                    "label": 1
                },
                {
                    "sent": "If I do that then I talk about Canonical hyperplanes.",
                    "label": 0
                },
                {
                    "sent": "OK, what I'm going saying is let me go back to the picture.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I have this invariants affixed invariants implicitly, and that's implicitly to fixing an actual measure for the distance here between the margin and the support vectors.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "In other words, these support vectors, the WX plus B -- 1.",
                    "label": 0
                },
                {
                    "sent": "These will be greater than minus one.",
                    "label": 0
                },
                {
                    "sent": "The support vectors.",
                    "label": 0
                },
                {
                    "sent": "Here we WX plus B we plus one.",
                    "label": 0
                },
                {
                    "sent": "These will be greater than plus one OK, and there is my setting up of the basics in our.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, so I've dictated that for the support vectors on both sides, the plus one and negative on the other side as well.",
                    "label": 1
                },
                {
                    "sent": "With these two equations are true.",
                    "label": 0
                },
                {
                    "sent": "Then if I take the difference between the two then.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I get the following, so let X1.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let X1 perhaps be this point here X2 must be a point on the other side.",
                    "label": 0
                },
                {
                    "sent": "Then I have the.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going, if I subtract the two, I subtract 1 of the equation for the other, get rid of the bees that have W is dot X 1 -- X two equals two OK?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just by subtracting top equation.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bottom equation from the top equation, right?",
                    "label": 0
                },
                {
                    "sent": "So this is the case and if X1 and X2 on.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Both sides are support vectors.",
                    "label": 0
                },
                {
                    "sent": "Now I can also see that the margin is given by the projection of the vector X 1 -- X two onto the normal vector.",
                    "label": 1
                },
                {
                    "sent": "The hyperplane which actually is W over model W. OK, let me just go back to that.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Basic picture, yeah, here it was.",
                    "label": 0
                },
                {
                    "sent": "I'm saying the recording system learning theory.",
                    "label": 0
                },
                {
                    "sent": "I should maximize the margin this distance here, OK?",
                    "label": 0
                },
                {
                    "sent": "And I had a X1 here and X2 here.",
                    "label": 0
                },
                {
                    "sent": "The vector which connects or two is X 1 -- X two and that's the vector there in red.",
                    "label": 0
                },
                {
                    "sent": "OK here is my separating hyperplane Witcher said just now was W dot X = B OK?",
                    "label": 1
                },
                {
                    "sent": "Well obviously the normal to that hyperplane is WW over MoD W. If I make it a normalized vector.",
                    "label": 0
                },
                {
                    "sent": "So indeed, the margin.",
                    "label": 0
                },
                {
                    "sent": "This thing is the projection of this vector onto the normal.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's it.",
                    "label": 0
                },
                {
                    "sent": "I want to maximize that thing.",
                    "label": 0
                },
                {
                    "sent": "In other words, I want to maximize the projection of this onto W over MoD W. So if I do that.",
                    "label": 0
                },
                {
                    "sent": "I get the following.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, so I want to project this onto this OK and just from my.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This equation.",
                    "label": 0
                },
                {
                    "sent": "I note that if I divide this by MoD W. Then this quantity, provided this divided by MoD W, is actually the projection of X 1 -- X two onto the normal to hyperplane.",
                    "label": 1
                },
                {
                    "sent": "What I'm saying therefore, if you follow this argument is that to maximize the margin I must maximize two or MoD W OK.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That will do it for me, so there it is to maximize the margin I must maximize this quantity here.",
                    "label": 1
                },
                {
                    "sent": "Well, I actually just know how to to hear, but naturally just dropping the two anything which maximizes this will maximize the margin.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's my first step.",
                    "label": 0
                },
                {
                    "sent": "I must maximize the object in order to maximize the margin.",
                    "label": 1
                },
                {
                    "sent": "And maximizing one or mug W. Sorry yeah, maximizing one or more W is equivalent to minimizing MoD W. OK, maximizing 1X is just the same thing as minimizing X OK, so to maximize my margin, my argument is that I must minimize MoD W. Now I must minimum.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Eyes model you.",
                    "label": 0
                },
                {
                    "sent": "Subject to constraints.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a thing that she maximized my margin minimize MoD W or WW.",
                    "label": 0
                },
                {
                    "sent": "OK, so anybody getting questions at this point?",
                    "label": 0
                },
                {
                    "sent": "You also happy, yeah?",
                    "label": 0
                },
                {
                    "sent": "OK, now that is that expressions do with maximizing the margin, but I had those constraints in there.",
                    "label": 1
                },
                {
                    "sent": "I had W dot X + B is plus one on one side minus one, the other OK.",
                    "label": 0
                },
                {
                    "sent": "It was plus one wise plus one.",
                    "label": 0
                },
                {
                    "sent": "In other words, the product that is positive and the other side I had W dot X + B is minus one winner.",
                    "label": 0
                },
                {
                    "sent": "Hand labeled points wise minus one.",
                    "label": 0
                },
                {
                    "sent": "So indeed minus one times a negative here.",
                    "label": 0
                },
                {
                    "sent": "Is also positive, and indeed this term here encapsulates those two equations I had for the Canonical hyperplane.",
                    "label": 1
                },
                {
                    "sent": "So in the day when I was must do for my SVM is maximizing margin subject to constraints.",
                    "label": 0
                },
                {
                    "sent": "Now you know from optimization theory.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Therefore, that this is formulated as a Lagrangian and have a primal objective function, that's a Max margin bit.",
                    "label": 1
                },
                {
                    "sent": "That's my constraints, and these are my La Grange multipliers, Alpha I and requirement in LaGrange multipliers.",
                    "label": 0
                },
                {
                    "sent": "Therefore, is the Alpha I must be greater than or equal to zero?",
                    "label": 0
                },
                {
                    "sent": "OK, now I this is 1 formulation.",
                    "label": 0
                },
                {
                    "sent": "My problem I called it the primal formulation, but it is not the formulation I work with.",
                    "label": 0
                },
                {
                    "sent": "In a support vector machine, because it's actually the dual formulation that I really is more convenient, it is possible to solve a certain type of learning machine from this stage here, but it's actually a later stage that we're going to use so to arrive at the jewel, so this is a primal.",
                    "label": 0
                },
                {
                    "sent": "I wish to arrive at the jewel.",
                    "label": 1
                },
                {
                    "sent": "I take the circle point equations which are DL by D, B = 0 and that deal by DB on.",
                    "label": 0
                },
                {
                    "sent": "This will give me.",
                    "label": 0
                },
                {
                    "sent": "This requirement here OK, what am I actually doing with the duality here well?",
                    "label": 0
                },
                {
                    "sent": "I don't know whether a lot of how many people whose familiarity with optimization, but briefly, there's a primal problem.",
                    "label": 0
                },
                {
                    "sent": "OK, which perhaps my primal problem is a quadratic OK, and the dual problem.",
                    "label": 0
                },
                {
                    "sent": "This is a mint type problem.",
                    "label": 0
                },
                {
                    "sent": "The jewel problems are Max type problem an at the solution, the primal and the jewel have those.",
                    "label": 0
                },
                {
                    "sent": "Actually the same solution.",
                    "label": 0
                },
                {
                    "sent": "You can drive one from the other.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's what I'm trying to do I.",
                    "label": 0
                },
                {
                    "sent": "A jewel version of this which gives me which when solved will give me the same solution.",
                    "label": 0
                },
                {
                    "sent": "So deal by DB OK gives me this.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "DL by DW.",
                    "label": 0
                },
                {
                    "sent": "Let me actually do that here.",
                    "label": 0
                },
                {
                    "sent": "DL by DW.",
                    "label": 0
                },
                {
                    "sent": "That was going to give me a W here because I have W square is going to be double here.",
                    "label": 0
                },
                {
                    "sent": "It's going to attract a term and.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Indeed, deal by DW gives me this expression here, OK, which must be satisfied at the solution of my primal problem.",
                    "label": 0
                },
                {
                    "sent": "OK, when DL by DW the solution, this must be the case.",
                    "label": 0
                },
                {
                    "sent": "Now I can take this W expression so.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Shoot back in in here.",
                    "label": 0
                },
                {
                    "sent": "Here and here and here and get the dual solution.",
                    "label": 0
                },
                {
                    "sent": "OK, I'll just do it.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then explain the sort of philosophy of this proof so.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "W goes in there and if I place the W in there, naturally the B&AW now gone.",
                    "label": 0
                },
                {
                    "sent": "I get this jewel formulation here.",
                    "label": 0
                },
                {
                    "sent": "OK, so yes, that's my argument, therefore is from Cisco learning theory.",
                    "label": 0
                },
                {
                    "sent": "I know it must maximize the margin, but it had constraints in it that set me up.",
                    "label": 0
                },
                {
                    "sent": "The primal LaGrange am and the problem with religion looks roughly sort of like this.",
                    "label": 0
                },
                {
                    "sent": "There's always a dual formulation of any problem in optimization theory, and actually when I solve the primal problem.",
                    "label": 0
                },
                {
                    "sent": "The solution of the dual problem is also done, and indeed the solution of the primal problem was dealt by DB equals zero deal by W = 0.",
                    "label": 0
                },
                {
                    "sent": "They gave me this requirements.",
                    "label": 0
                },
                {
                    "sent": "So if I'm able to actually minimize this expression here, then I will arrive at exactly the same solution now.",
                    "label": 0
                },
                {
                    "sent": "I say here, so maximize that function, maximize the jewel is actually called the Wolf jewel, which is this expression here.",
                    "label": 0
                },
                {
                    "sent": "OK, this actually is the main equation for support vector machines and I want to just pause and have a look at it before I want drawn to do other things.",
                    "label": 0
                },
                {
                    "sent": "First of all, what you notice is the data is coming in here.",
                    "label": 0
                },
                {
                    "sent": "OK, that's a data X is an their corresponding labels, which are the wise and indeed my Max task is in terms of the alphas, the LaGrange multipliers.",
                    "label": 0
                },
                {
                    "sent": "It is actually constrained optimization because those alphas must be positive.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And indeed, as the second constraint which we don't overlook, which is this constraint?",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here, so it is constrained optimization, but you'll notice important point is my Max task over the alphas involves a quadratic programming type task.",
                    "label": 0
                },
                {
                    "sent": "Alpha and Alpha squared types pressure, so it's a quadratic, just like here actually here, and the Max is going to have some unique solution.",
                    "label": 0
                },
                {
                    "sent": "OK indeed all this sort of ties together because if we consider that maximum maximum separating hyperplane.",
                    "label": 0
                },
                {
                    "sent": "That I started with.",
                    "label": 0
                },
                {
                    "sent": "You can probably see there's only sort of 1 solution really, and hence I get only one solution to this problem here.",
                    "label": 0
                },
                {
                    "sent": "OK, so indeed that is my ascential.",
                    "label": 0
                },
                {
                    "sent": "SVM problem Max.",
                    "label": 0
                },
                {
                    "sent": "This object subject to constraints given with respect to Alpha and it's it's convex only one solution OK and my data just simply plunk my data in here and do the quadratic programming get my solution.",
                    "label": 0
                },
                {
                    "sent": "Of course another feature is a very efficient algorithms for quadratic programming so I can handle that.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Large datasets.",
                    "label": 0
                },
                {
                    "sent": "Right, So what I would do is I would solve that quadratic programming problem having got the alphas I put the alphas in here.",
                    "label": 0
                },
                {
                    "sent": "OK now initially this was W. This would have been W zed plus be OK so this would have the decision function bit like a neural network would be W zed plus B. I've put in the W. The equation I got just now, namely this thing here.",
                    "label": 0
                },
                {
                    "sent": "This expression I put that in where W is over there and that gave me as my decision function here.",
                    "label": 0
                },
                {
                    "sent": "OK, so my general SVM task for binary classification is I put in the data into that expression just gave you.",
                    "label": 0
                },
                {
                    "sent": "I solve the quadratic programming problem.",
                    "label": 0
                },
                {
                    "sent": "My solution is put in here.",
                    "label": 0
                },
                {
                    "sent": "My data is here, that's just data and this is my new test point.",
                    "label": 0
                },
                {
                    "sent": "I wish to evaluate.",
                    "label": 0
                },
                {
                    "sent": "I haven't said how to get the B, I'm just going to stay to formula for it in a second but.",
                    "label": 0
                },
                {
                    "sent": "Not really comment on it beyond that now certain things one notices a comment I made earlier.",
                    "label": 0
                },
                {
                    "sent": "The function has an explicit dependence on the data, unlike a neural network.",
                    "label": 0
                },
                {
                    "sent": "For example.",
                    "label": 0
                },
                {
                    "sent": "The other point is if you do this in reality, you'll actually find a lot of these alphas are zero.",
                    "label": 0
                },
                {
                    "sent": "OK, Alpha equals zero, it means the corresponding sample has no influence on the decision function.",
                    "label": 1
                },
                {
                    "sent": "Indeed there all hearts back to what I.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Started with earlier where if you notice these support vectors have influence on where that hyperplane should be.",
                    "label": 0
                },
                {
                    "sent": "But if I remove this guy, the half plane would be exactly where it is similar here, here and he's done.",
                    "label": 0
                },
                {
                    "sent": "Select support vectors on the other side, so the non spec support vectors don't influence where the hyperplane should be and equivalently when I look at the.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Decision function which I had here, they'll appear as Alpha equals zero.",
                    "label": 0
                },
                {
                    "sent": "They will not appear in the decision function, have no influence on where the hyperlink should be OK. OK, so let's see a central story for the basic SVM.",
                    "label": 1
                },
                {
                    "sent": "OK, any questions at this point you're happy.",
                    "label": 0
                },
                {
                    "sent": "OK, probably some of you are familiar with this anyway, yeah?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, which one this one?",
                    "label": 0
                },
                {
                    "sent": "Oh, that shouldn't be there.",
                    "label": 0
                },
                {
                    "sent": "Yeah, actually this has been borrowed on the latex regression where it should be there.",
                    "label": 0
                },
                {
                    "sent": "So scrub that out.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it should go.",
                    "label": 0
                },
                {
                    "sent": "There should be actually social BWB Alpha as a mistake.",
                    "label": 0
                },
                {
                    "sent": "OK there might be other bugs in here, yeah?",
                    "label": 0
                },
                {
                    "sent": "I think I was moving some late.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Check around and lose yourself, right?",
                    "label": 0
                },
                {
                    "sent": "So far we don't know how how to handle non separable datasets.",
                    "label": 1
                },
                {
                    "sent": "So the picture I've just given you is OK, but it is a very nice sort of well behaved data set where is quite separable.",
                    "label": 0
                },
                {
                    "sent": "What happens when the data is intermeshed?",
                    "label": 0
                },
                {
                    "sent": "You'll know where something like Neil Networks for things like X or problem and things like that.",
                    "label": 0
                },
                {
                    "sent": "I must have hidden nodes and So what do I do in this case?",
                    "label": 0
                },
                {
                    "sent": "Well, actually I have to fall back on the second bit of the theorem I mentioned at the beginning, which I just state its consequences, not what the theorem is.",
                    "label": 0
                },
                {
                    "sent": "The second part of the theorem I just remind you is that.",
                    "label": 1
                },
                {
                    "sent": "The theoretical generalization error or the bound does not depend on the dimension of the space.",
                    "label": 0
                },
                {
                    "sent": "In other words, I get the same if I maximize the margin.",
                    "label": 0
                },
                {
                    "sent": "I guess I should get the same sort of general performance independently of whether men attend a mental space or 100 dimensions or 1000.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pensions OK. Now another thing you will notice is that the objective function the data appears in the form of.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The product I'm trying to just skip back here if you notice, here's my data.",
                    "label": 0
                },
                {
                    "sent": "OK now the X is are in the form of an inner product.",
                    "label": 0
                },
                {
                    "sent": "Indeed, I can refine, define, redefine my X my YIXI as a new object X prime, and essentially there for my data only appears in the form of a scalar product.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So I've noted the following.",
                    "label": 0
                },
                {
                    "sent": "My generalization, Brown does not depend on dimension of the space, and also that my data only appears in my objective function in the form of an inner product or scalar product.",
                    "label": 1
                },
                {
                    "sent": "So indeed this brings me to the elaboration of my support vector machine, namely what I can do if I have a data set which is highly intermeshed, not separable, then I can mark my data to a higher dimensional space by replacement.",
                    "label": 0
                },
                {
                    "sent": "OK, by mapping function an, if I map data from a low dimension to high dimension, practically I think it always if I go high enough I will be able to separate the data.",
                    "label": 0
                },
                {
                    "sent": "OK, so just to illustrate the point, if I was just in two dimensions, OK an I had.",
                    "label": 0
                },
                {
                    "sent": "Plus minus minus plus in two dimensions.",
                    "label": 0
                },
                {
                    "sent": "I can't find hyperplane which separates the two types of data, but if there is a 3 dimensions OK and these are sort of in the board and these are sort of like a minus here, I can't really do it, but let me circle it.",
                    "label": 0
                },
                {
                    "sent": "OK so I have pluses here and the two minus is here where I can easily put a hyperplane that separates the two.",
                    "label": 0
                },
                {
                    "sent": "So in two dimensions I can't find a hyperplane that separates them but in a 3 dimensional world where these minuses are outside the board I can easily find hyperplane that will do it.",
                    "label": 0
                },
                {
                    "sent": "Indeed, therefore, if I map into high enough dimensional space, then I'll be able to separate my data and I won't have a problem.",
                    "label": 0
                },
                {
                    "sent": "OK now I will do that therefore and my data is in the form of a dot product or inner product and this is I must have a mapping function that Maps me for my input space into this higher dimensional space which.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, cool feature space.",
                    "label": 0
                },
                {
                    "sent": "Now there's this mapping function and I call it 5 and I called the higher dimensional space are mapped to a feature space, and one thing which must be true about the spacer map into is that there must be a Hilbert space and textbooks site got it wrong here, it can be a pre Hilbert space or inner product space.",
                    "label": 1
                },
                {
                    "sent": "OK, but what it all boils down to is of course and mapping and inner product.",
                    "label": 0
                },
                {
                    "sent": "So the spacer map into an inner product must be defined.",
                    "label": 0
                },
                {
                    "sent": "So it must be an inner product space or he pre Hill.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Space.",
                    "label": 0
                },
                {
                    "sent": "Now, one thing.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It may be bothering, yeah, one thing that's important, of course, is if I'm going to map into this high dimensional space, I must have some mapping function.",
                    "label": 0
                },
                {
                    "sent": "OK, now one of the in very simple cases that polynomial kernels I can actually figure out what the mapping function is, but one of the curious things about kernel map makes methods is I can map into high dimensional space without actually knowing what the mapping function is.",
                    "label": 0
                },
                {
                    "sent": "Because if I define.",
                    "label": 0
                },
                {
                    "sent": "A functional form for my inner product in that high dimensional space.",
                    "label": 1
                },
                {
                    "sent": "It implicitly defines a mapping function, but I don't need to know what the mapping function is.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Words what I'm saying here is the following.",
                    "label": 0
                },
                {
                    "sent": "I had an inner product and this is now my Maps in a product and if I define certain types of kernels, they implicitly define certain types of embedding functions, but I don't need to know what the bedding function is.",
                    "label": 0
                },
                {
                    "sent": "OK, so just define a kernel, provide it's OK and implicitly define a mapping function, implicitly define a high dimensional feature space?",
                    "label": 0
                },
                {
                    "sent": "Well, I don't need to bother with that file.",
                    "label": 0
                },
                {
                    "sent": "X is just doesn't matter, so don't don't bother and the cut this object here is there for the kernel and it is there for inner product in a Mac.",
                    "label": 0
                },
                {
                    "sent": "Pairs of points in feature space, yeah.",
                    "label": 0
                },
                {
                    "sent": "As you say, we don't care about the future space.",
                    "label": 0
                },
                {
                    "sent": "You don't care about the mapping function, yes.",
                    "label": 0
                },
                {
                    "sent": "He quit.",
                    "label": 0
                },
                {
                    "sent": "How do?",
                    "label": 0
                },
                {
                    "sent": "True.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it does mean one thing is apparent in that the kernel function.",
                    "label": 0
                },
                {
                    "sent": "I can't just choose any kernel function because the kernel function must actually represent an inner product in some higher dimensional space, so that's going to put a restriction, yeah?",
                    "label": 0
                },
                {
                    "sent": "You have about another problem because you have some problem with your data, yeah?",
                    "label": 0
                },
                {
                    "sent": "Four girl thing yeah.",
                    "label": 0
                },
                {
                    "sent": "Well separable yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "This is what I do with girls myself, gushing camera yeah.",
                    "label": 0
                },
                {
                    "sent": "Phone number for Danville?",
                    "label": 0
                },
                {
                    "sent": "Understand why, what why?",
                    "label": 0
                },
                {
                    "sent": "Yeah, unfortunately it's a lot of the reply that is buried in the mass papers in that they defined.",
                    "label": 0
                },
                {
                    "sent": "They drive theorems which restrict the kernel function and which restrict you to a inner product space OK. And that bit is a little bit buried in mass.",
                    "label": 0
                },
                {
                    "sent": "I'll show you what the end end effect of this is is a requirement in the kernel and if you satisfy that requirement then it's a legitimate kernel.",
                    "label": 0
                },
                {
                    "sent": "Are you busy this morning with brother?",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "But I will answer that point in the second.",
                    "label": 0
                },
                {
                    "sent": "I haven't actually defined what are the strictures on Ki.",
                    "label": 0
                },
                {
                    "sent": "Haven't defined that?",
                    "label": 0
                },
                {
                    "sent": "OK, I've just.",
                    "label": 0
                },
                {
                    "sent": "I'm just saying it's a slightly strange point that I don't need to know what the mapping function is because I provide it satisfies these theorems and it's OK.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, So what is the mapping relation?",
                    "label": 0
                },
                {
                    "sent": "In fact, we do not need to know the formulas mapping since it's implicitly defined by the functional form within the pro.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Not OK mapped in a product now.",
                    "label": 0
                },
                {
                    "sent": "Some kernels which are commonly used to talk about them, then I'll talk about the theorems, which was strictly kernels.",
                    "label": 0
                },
                {
                    "sent": "Different types of curls to find different types of Hilbert spaces.",
                    "label": 1
                },
                {
                    "sent": "OK, very popular.",
                    "label": 0
                },
                {
                    "sent": "Kernel is an RBF or Gaussian kernel which I have here.",
                    "label": 0
                },
                {
                    "sent": "These are my data points and have a kernel parameter Sigma here which alters the shape of the space, but I've used.",
                    "label": 0
                },
                {
                    "sent": "I've had vase.",
                    "label": 0
                },
                {
                    "sent": "Non separable datasets axle problem in Paraty sort of.",
                    "label": 0
                },
                {
                    "sent": "The data is hugely into and this is excellent eye problem.",
                    "label": 0
                },
                {
                    "sent": "But if I went behind, had a hypercube and every time I move along an edge I flip the sign heavily into meshed.",
                    "label": 0
                },
                {
                    "sent": "Well I've done em Paraty using a RBF kernel in a work fine too spirals problem.",
                    "label": 0
                },
                {
                    "sent": "Another classic sort of highly intermesh data set.",
                    "label": 0
                },
                {
                    "sent": "I've done it also with an RBF kernel.",
                    "label": 0
                },
                {
                    "sent": "I've not actually found.",
                    "label": 0
                },
                {
                    "sent": "Any problem with I couldn't do with an RBF kernel OK because in fact it is indeed actually defining sort of infinite dimensional space.",
                    "label": 0
                },
                {
                    "sent": "An another choice is a polynomial kernel relatively infrequently used RBF kernel Gauss.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Kernels very, very common.",
                    "label": 0
                },
                {
                    "sent": "There's in fact the whole family of these kernels, a huge number, and mellows written a book which defines lots of kernels.",
                    "label": 0
                },
                {
                    "sent": "Legitimate curls are not restricted to simple functions like the Gaussian kernel.",
                    "label": 1
                },
                {
                    "sent": "OK, indeed, there's a huge class of kernels which are possible.",
                    "label": 0
                },
                {
                    "sent": "Just show you two types of kernels without really much explanation, but which you could use.",
                    "label": 0
                },
                {
                    "sent": "OK, you maybe wanted to compare strings, and indeed this morning.",
                    "label": 0
                },
                {
                    "sent": "With the aquila's talk he was talking about strings and very common in many bioinformatics applications, and you notice these strings karkat cart chart have some certain similarity.",
                    "label": 0
                },
                {
                    "sent": "OK, and you can define little algorithm algorithms, edit codes Anson, which actually will enable you to score the similarity of strings.",
                    "label": 1
                },
                {
                    "sent": "OK, well, that means you can drive string kernels.",
                    "label": 1
                },
                {
                    "sent": "The string kernels not.",
                    "label": 0
                },
                {
                    "sent": "Therefore, function it is actually defined by now with.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But it's a legitimate kernel to use OK, and I'm just saying here, such kernels are very important in Barn formatics text process.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sing, you have text as well, etc.",
                    "label": 0
                },
                {
                    "sent": "Another type unusual kernel is of course you might have a graph.",
                    "label": 0
                },
                {
                    "sent": "OK, this could be a graph to do with Inter relationships of jeans.",
                    "label": 0
                },
                {
                    "sent": "Are they functionally related or not?",
                    "label": 0
                },
                {
                    "sent": "And you can consider the similarity of certain graphs.",
                    "label": 0
                },
                {
                    "sent": "OK, if I drew another set of nodes as many as here and then just wiped out one or two links, I would easily see that they're very similar graphs.",
                    "label": 0
                },
                {
                    "sent": "Then I could have other grass which are very dissimilar so you know.",
                    "label": 0
                },
                {
                    "sent": "Even by looking at the Mike realize I can define some sort of similarity measure.",
                    "label": 0
                },
                {
                    "sent": "Overlap graphs.",
                    "label": 0
                },
                {
                    "sent": "OK was a thing called the diffusion kernel, visually defined by Condor Lafferty, which enables you to drive a kernel over graphs and networks.",
                    "label": 1
                },
                {
                    "sent": "OK, So what I'm saying is that the kernels are very big, broad class of possibilities, which essentially means that.",
                    "label": 0
                },
                {
                    "sent": "Huge range of models to consider.",
                    "label": 0
                },
                {
                    "sent": "Indeed, that's why we talk about kernel based methods rather than SVM's 'cause the whole subject, the kernel based methods is broader than a support vector machine and encompass is all the various different types of kernels I can consider.",
                    "label": 0
                },
                {
                    "sent": "Indeed, if I had chosen that polynomial kernel, I could just now I can re derive splines from statistics statistics if I choose attenti type kernel I can drive neural networks.",
                    "label": 0
                },
                {
                    "sent": "If I choose account Gaussian RBF type kernel.",
                    "label": 0
                },
                {
                    "sent": "I can get an RBF.",
                    "label": 0
                },
                {
                    "sent": "Network OK.",
                    "label": 0
                },
                {
                    "sent": "In other words, a lot of things are previously considered as independent models are just subcases of kernel based methods were picked at a particular type of kernel.",
                    "label": 0
                },
                {
                    "sent": "OK and importantly I can if I had a new network I wouldn't really know what to do with about a graph and if I had.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Strings of unequal lengths and a new network.",
                    "label": 0
                },
                {
                    "sent": "I wouldn't really know what to do.",
                    "label": 0
                },
                {
                    "sent": "A new network typically would have 100 inputs, say.",
                    "label": 0
                },
                {
                    "sent": "Where is this thing is requiring three 510 inputs, etc.",
                    "label": 0
                },
                {
                    "sent": "So you know I have a real problem with the neural network trying to do do some classification.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Strings say so I'm able to define a very broad class of models.",
                    "label": 0
                },
                {
                    "sent": "That's what makes them powerful.",
                    "label": 0
                },
                {
                    "sent": "Convexity make some powerful, relatively few parameters, and.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The possibility of a broad range of kernels.",
                    "label": 0
                },
                {
                    "sent": "Now how said there are restrictions on the kernels, I can't just choose anything, only certain things of valid.",
                    "label": 0
                },
                {
                    "sent": "And indeed the initial sort of restrictions you come across as.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some textbooks is the Mercer conditions and these were approved by mathematicians and provided your kernel satisfies these two equations, then it's a legitimate kernel.",
                    "label": 0
                },
                {
                    "sent": "An inner product is defined and there's a legitimate kernel, but in fact the simplest criterion could consider is that the kernel is any object which is positive semidefinite.",
                    "label": 1
                },
                {
                    "sent": "OK, provided that's the case of bottom criterion is true for your kernel then.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So legitimate kernel to use.",
                    "label": 0
                },
                {
                    "sent": "OK, so if a kernel is positive semidefinite here we are where these are real numbers then, is this a function 5X defining inner product of possibly higher dimension?",
                    "label": 1
                },
                {
                    "sent": "OK which my kernel is equal to that mapped in a product.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's it, it's very broad classes.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Objects that I can use.",
                    "label": 0
                },
                {
                    "sent": "So now summarize then the main stacks.",
                    "label": 0
                },
                {
                    "sent": "Depending on my particular problem text graphs.",
                    "label": 0
                },
                {
                    "sent": "All men are different data.",
                    "label": 0
                },
                {
                    "sent": "I would choose the kernel may have a kernel parameter in it, but I don't know if I consider this or in the next lecture.",
                    "label": 0
                },
                {
                    "sent": "I don't maximize this function over the Alpha squared subject constraints, so it's constrained optimization constrained quadratic programming.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and I find the bars which I haven't talked about til up to now, but this is an equation which you can drive for the bias.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and how we got the bias, the alphas and my choice of kernel.",
                    "label": 0
                },
                {
                    "sent": "I would now put a new zed in here and it would give me a plus or minus one and that would be my binary.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Classifier that I'd use OK now so.",
                    "label": 0
                },
                {
                    "sent": "That is my what I've done therefore is introduce a.",
                    "label": 0
                },
                {
                    "sent": "A support vector machine.",
                    "label": 0
                },
                {
                    "sent": "My basic very basic type of kernel based method.",
                    "label": 0
                },
                {
                    "sent": "I've showed that you can map to high dimensional space, show there's a bunch of a broad class of kernel based methods, but I still have a few other problems to sort out.",
                    "label": 0
                },
                {
                    "sent": "Indeed, one is I pointed out that outliers have a big influence on that separating hyperplane at that motivates the topic of soft margins.",
                    "label": 0
                },
                {
                    "sent": "Another issue is that the SVM is well set up for binary classification.",
                    "label": 0
                },
                {
                    "sent": "What do I do about multiclass classification?",
                    "label": 1
                },
                {
                    "sent": "Many problems of old multi class, and so I must have some scheme to handle that.",
                    "label": 0
                },
                {
                    "sent": "Indeed, the scheme are going to give you now the directed acyclic graph way of doing it is was invented by Nella Cristianini and John Platt and published in NIPS, but.",
                    "label": 1
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But the idea is relatively straightforward, so enable me to do multi class using the framework I've just given you.",
                    "label": 0
                },
                {
                    "sent": "Namely, suppose I had a three class problem, 1, two and three.",
                    "label": 0
                },
                {
                    "sent": "Then I can do a directed acyclic graph where my first task is to say is it class one or class 3.",
                    "label": 0
                },
                {
                    "sent": "If it's one, is it one or two?",
                    "label": 0
                },
                {
                    "sent": "If a phone is 2 ago here, fine, it's when I go here, similar if I had found its three and make a choice to three, and I eventually get here.",
                    "label": 0
                },
                {
                    "sent": "And when I'm just saying is for relatively small number of classes.",
                    "label": 0
                },
                {
                    "sent": "Such a scheme is OK, and in particular what you notice is that all these steps here are binary classification, so they're done exactly using the method which I've given you just now.",
                    "label": 0
                },
                {
                    "sent": "OK, this becomes rather inadequate.",
                    "label": 0
                },
                {
                    "sent": "If I had some like 100 classes, OK, wouldn't be the way to do it.",
                    "label": 0
                },
                {
                    "sent": "Now will say a lot of people trying to look at multi class classification with SVM's.",
                    "label": 0
                },
                {
                    "sent": "They're not the binary classification cases, not well set up for multi class.",
                    "label": 0
                },
                {
                    "sent": "There's a number of schemes and they all sort of have faced similar performance OK.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that is one way of doing it.",
                    "label": 0
                },
                {
                    "sent": "That's the end of my story for multiclass.",
                    "label": 0
                },
                {
                    "sent": "The other story was what happens when I have data points which actually have an influence, are outliers and really pushed the separating hyperplane around because I try and maximize the margin but one data point can mess things up for me.",
                    "label": 0
                },
                {
                    "sent": "Now most real life datasets contain noise and the SVM would give me poor generalization as it fits the noise.",
                    "label": 1
                },
                {
                    "sent": "I will say something about this before plowing to this point here.",
                    "label": 0
                },
                {
                    "sent": "If I actually do the story of the SVM, which I've just given you.",
                    "label": 0
                },
                {
                    "sent": "Let me.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually flip back to here, OK?",
                    "label": 0
                },
                {
                    "sent": "I said just now if I took a separable data set I would find sometimes the alphas are zero and not support vectors.",
                    "label": 0
                },
                {
                    "sent": "Sometimes the alphas are non zero.",
                    "label": 0
                },
                {
                    "sent": "There are support vectors OK and they are the closest current happens suffering hyperplane if you run it in practice.",
                    "label": 0
                },
                {
                    "sent": "You also might find Alphas, VO very large.",
                    "label": 0
                },
                {
                    "sent": "If you have Alpha very large then it could be a correct point which is very very unusual.",
                    "label": 0
                },
                {
                    "sent": "Alternatively it usually indicates an outlier.",
                    "label": 0
                },
                {
                    "sent": "And actually give you one good illustration of this.",
                    "label": 0
                },
                {
                    "sent": "In my previous lecture I talked about leukemia and I used the SVM with.",
                    "label": 0
                },
                {
                    "sent": "Cancer data sets in the very early study done at MIT in Tagalog's Group.",
                    "label": 0
                },
                {
                    "sent": "It was leukemia trying to distinguish lymphoblastic leukemia from myeloid leukemia.",
                    "label": 0
                },
                {
                    "sent": "OK, fairly similar types of leukemias and when they ran it, they SVM to try and do this binary classification tasks.",
                    "label": 0
                },
                {
                    "sent": "There was one patient with a very large Alpha value.",
                    "label": 0
                },
                {
                    "sent": "Now they knew there for this indicates either rather unusual but correct date data point or possibly an outlier.",
                    "label": 0
                },
                {
                    "sent": "So they went back to the medics and asked what this patient seems to be unusual.",
                    "label": 0
                },
                {
                    "sent": "Very large Alpha value.",
                    "label": 0
                },
                {
                    "sent": "There is only the patient and the patient had been misclassified OK, so a large Alpha value can indeed.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dayton outlier OK, so it's good idea to be aware of that, but the conventional way to lessen the influence of noise in the data is to introduce a soft margin.",
                    "label": 0
                },
                {
                    "sent": "Now in textbooks you'll find usually one of these two, but in fact there's two types of Softmart.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oceans that one would consider the one you get in textbooks is the L1 error norm, and previously I said that because Alpha is a LaGrange multiplier must be positive.",
                    "label": 0
                },
                {
                    "sent": "So I had this bit.",
                    "label": 0
                },
                {
                    "sent": "But in fact I don't think I'd give the proof here, but you can show that I can choose.",
                    "label": 0
                },
                {
                    "sent": "There are normal by actually putting upper limit on the A1.",
                    "label": 0
                },
                {
                    "sent": "Sort of easy way to see this is I said just now a large Alpha value usually indicates an outlier.",
                    "label": 0
                },
                {
                    "sent": "So far cap the limit for the Alpha is going to lessen its influence.",
                    "label": 0
                },
                {
                    "sent": "OK, but in fact you can prove formally prove that this is a good adaptation.",
                    "label": 0
                },
                {
                    "sent": "So your scenario for the SVM is just like before.",
                    "label": 0
                },
                {
                    "sent": "But whereas before I head out for greater angle to 0, now I've got this criterium.",
                    "label": 0
                },
                {
                    "sent": "And that is one way to induce the soft margin, and that is the soft margin parameter.",
                    "label": 0
                },
                {
                    "sent": "The alternative, which is not often in textbooks, which is pretty robust, is to take the situation for the SVM which I've described up to now.",
                    "label": 0
                },
                {
                    "sent": "Look at the diagonal components of the kernel.",
                    "label": 0
                },
                {
                    "sent": "In other words, K, XI, XI and add a small positive constant to those diagonal components of the kernel.",
                    "label": 0
                },
                {
                    "sent": "OK, only that.",
                    "label": 0
                },
                {
                    "sent": "That's all you do, so it's a normal SVM story.",
                    "label": 0
                },
                {
                    "sent": "But I had to.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Full positive Mount to the.",
                    "label": 0
                },
                {
                    "sent": "Colonel Diagonal now this is actually the first of those using the soft margin.",
                    "label": 0
                },
                {
                    "sent": "Now I'm not quite sure if I've inverted one on C or whether it's enough to quickly think about that and get bit tired, but it may be one on.",
                    "label": 0
                },
                {
                    "sent": "See down here or maybe see but important point, I want to know is this is actually also ionosphere data set from the UCI repository, quite a noisy data set, and when it boils down to this must be season finity.",
                    "label": 0
                },
                {
                    "sent": "This must be one.",
                    "label": 0
                },
                {
                    "sent": "See down below sees infinite.",
                    "label": 0
                },
                {
                    "sent": "Or not know structurally A1 CSF was zero and.",
                    "label": 0
                },
                {
                    "sent": "If I have no soft margin, my test error is 7%.",
                    "label": 0
                },
                {
                    "sent": "Another hand as I introduce to see this actually one on, see below as I might see goes from Infinity down to some finite value.",
                    "label": 0
                },
                {
                    "sent": "Then my test error falls OK, falls down to about 5.4, so it's improvement if I keep pushing my C smaller and smaller.",
                    "label": 0
                },
                {
                    "sent": "OK, so my alphas lying on a range like so.",
                    "label": 0
                },
                {
                    "sent": "And I'm decreasing the C, making it smaller smaller course.",
                    "label": 0
                },
                {
                    "sent": "In the end my SCM just can't learn the problem.",
                    "label": 0
                },
                {
                    "sent": "I'm trying to give it.",
                    "label": 0
                },
                {
                    "sent": "OK so my error starts climbing on the other side, but there's a point in there where if I introduce a soft margin, I definitely dropped the test error.",
                    "label": 0
                },
                {
                    "sent": "This is the L1.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Error norm not so well, sort of described in textbooks is the L2 error norm, which is just the it's quite stable, robust positive quantity added to the kernel diagonal.",
                    "label": 0
                },
                {
                    "sent": "Again, this is the Lambda OK. Again, Lambda equals zero.",
                    "label": 0
                },
                {
                    "sent": "No L2 error norm.",
                    "label": 0
                },
                {
                    "sent": "7% error introduce diagonal component as addition to my kernel diagonal.",
                    "label": 0
                },
                {
                    "sent": "My test error falls.",
                    "label": 0
                },
                {
                    "sent": "Then if I start introducing too much I will.",
                    "label": 0
                },
                {
                    "sent": "Addition to my Colonel diagonal then course after a time my album, just my math, my SPM.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Work, my error starts climbing OK, right?",
                    "label": 0
                },
                {
                    "sent": "Indeed I think I maybe I prove some of these now.",
                    "label": 0
                },
                {
                    "sent": "Justification for the Xarelto.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the L1 error norm its justification is that without the hard margin, that means no soft margin.",
                    "label": 1
                },
                {
                    "sent": "That was the case I had before I actually had that equation stated for the soft margin.",
                    "label": 0
                },
                {
                    "sent": "The L1 error norm I allow for points actually in what's called the margin band.",
                    "label": 0
                },
                {
                    "sent": "If you remember, I had the separating hyperplane had the support vectors here support vectors here.",
                    "label": 0
                },
                {
                    "sent": "This is equivalent to allowing some points in the margin ban or even totally wrong.",
                    "label": 0
                },
                {
                    "sent": "OK, if that is now greater than one, it would mean that that data point can actually be wrong.",
                    "label": 0
                },
                {
                    "sent": "So my training error is not zero, right?",
                    "label": 0
                },
                {
                    "sent": "So that is how I started.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Out justifying the L1 error norm and there's variance on this picture I've just mentioned.",
                    "label": 0
                },
                {
                    "sent": "At alternative approach called the new SVM Solutions for less L1 error normal same as obtained from maximizing this function.",
                    "label": 1
                },
                {
                    "sent": "Here there's very.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This on it different ways, of which I can produce a soft margin, OK?",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very similar type problem.",
                    "label": 0
                },
                {
                    "sent": "The soft margin parameter if I view it as so or as an addition to my kernel diagonal.",
                    "label": 0
                },
                {
                    "sent": "Intuitively it's not quite to clear what you're doing in this alternative form.",
                    "label": 0
                },
                {
                    "sent": "Formulation of the soft margin.",
                    "label": 1
                },
                {
                    "sent": "The new SVM.",
                    "label": 0
                },
                {
                    "sent": "The soft margin has a very clear and transparent meaning.",
                    "label": 1
                },
                {
                    "sent": "It's a fracture of training errors.",
                    "label": 0
                },
                {
                    "sent": "Upper bounded by Mu, which provides a lower bound on the fractional points which are support vectors.",
                    "label": 1
                },
                {
                    "sent": "I think I will.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At this point, but just point out that there's different ways of doing a soft margin.",
                    "label": 0
                },
                {
                    "sent": "Right, so up to now, I've done binary classification.",
                    "label": 0
                },
                {
                    "sent": "I've done multiclass classification.",
                    "label": 0
                },
                {
                    "sent": "I want to do regression.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "It was somebody else question, yeah?",
                    "label": 0
                },
                {
                    "sent": "Intuitive explanation for the second language.",
                    "label": 0
                },
                {
                    "sent": "I don't think I've got.",
                    "label": 0
                },
                {
                    "sent": "If I go back to this way.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One which is here.",
                    "label": 0
                },
                {
                    "sent": "Not too intuitive, that's why Alex Smolen burner shook off to the new SVM, which sort of makes it more intuitive.",
                    "label": 0
                },
                {
                    "sent": "With this method, I'll tell you it's starting point for the derivation is not that difficult to drive it.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The L2 error norm this is the L1 error norm.",
                    "label": 0
                },
                {
                    "sent": "OK, the L2 error norm is simply the same thing, but it's squared up here.",
                    "label": 0
                },
                {
                    "sent": "That's just squared up some sort of doubly penalizing the error in a different way.",
                    "label": 0
                },
                {
                    "sent": "OK, you don't have to do the equations you square up here, fail to an error norm.",
                    "label": 0
                },
                {
                    "sent": "You set up a Lagrangian just like before, but you got additional term here and they just do some mass and you find actually all boils down to addition to the kernel parameter.",
                    "label": 0
                },
                {
                    "sent": "At least that is an exercise for you to do.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So like lecture right?",
                    "label": 0
                },
                {
                    "sent": "So solving regression tasks, I've done the classification, including multitask, whilst the story when you have a continuous valued output.",
                    "label": 1
                },
                {
                    "sent": "OK, don't get bogged down in the detail because I'll just show you the story very similar.",
                    "label": 0
                },
                {
                    "sent": "Now my function OK doesn't have assigned function in here.",
                    "label": 1
                },
                {
                    "sent": "It's not a classifier, my function is continuous valued and is simply going to be W dot X + B. OK, now I'm just.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm going to say about regression.",
                    "label": 0
                },
                {
                    "sent": "Is like with the SVM you try and minimized model W squared that I did exactly for binary classification.",
                    "label": 0
                },
                {
                    "sent": "I have two types of errors now because I have two types of training errors depending on which side.",
                    "label": 1
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of this, functionally am OK.",
                    "label": 0
                },
                {
                    "sent": "So if you like I could be an error decrementing this or incrementing it.",
                    "label": 0
                },
                {
                    "sent": "So I gotta plus type of error or minus Typeerror added to this function here taking me away from the correct value.",
                    "label": 0
                },
                {
                    "sent": "I might want .2 here and that's .1 and I have put plus .1 as my error or it might be point 3.4 and I have a minus.",
                    "label": 0
                },
                {
                    "sent": "OK so I have.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two times errors.",
                    "label": 0
                },
                {
                    "sent": "And that is accounted for by these two types of water, called slack variables, very similar to the soft margins slack variables and use earlier.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, now you just plow through.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The mass OK, and what it all boils down to is I have a.",
                    "label": 0
                },
                {
                    "sent": "A function which I must minimize, which looks like the following.",
                    "label": 0
                },
                {
                    "sent": "This is very similar to the maximizing the margin term.",
                    "label": 0
                },
                {
                    "sent": "This is now to do with the errors, and since I'm minimizing I want to minimize the errors both ways that I may make, and this is the tradeoff between these two OK. Now I'll just say if you start with that.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You put in the constraints you have, which are indeed this things here.",
                    "label": 0
                },
                {
                    "sent": "These things here if you like.",
                    "label": 0
                },
                {
                    "sent": "If this was zero and this was zero, then why I will be W dot X + B?",
                    "label": 0
                },
                {
                    "sent": "Exactly the function I want to learn, and similarly with the other type of error.",
                    "label": 0
                },
                {
                    "sent": "If it was zero and this was zero, then I have Y equals WX plus B again.",
                    "label": 0
                },
                {
                    "sent": "So in fact if they are set to zero, I can easily see that I'm forcing this to equal Y.",
                    "label": 0
                },
                {
                    "sent": "Now these two terms, this is an error you may puzzle about this epsilon that isn't allowed.",
                    "label": 0
                },
                {
                    "sent": "A bit of error.",
                    "label": 0
                },
                {
                    "sent": "OK, now typically if I've got noise, I don't want to have epsilon precisely zero 'cause I totally fit to noise.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is actually a equivalent to a soft margin.",
                    "label": 0
                },
                {
                    "sent": "OK, but I don't can't have an endless number of errors, so these other errors which are going to have take me away from my function, correct functional mapping?",
                    "label": 0
                },
                {
                    "sent": "I just don't want.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I want to minimize that zyan.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I had I what all boils down to is the objective function.",
                    "label": 0
                },
                {
                    "sent": "I will have to do for aggression very similar to the story for classification.",
                    "label": 0
                },
                {
                    "sent": "What you'll notice that previously I had Alpha and Alpha squared.",
                    "label": 0
                },
                {
                    "sent": "Well now it's the same sort of thing is linear in Alpha and Alpha hat, and it's a quadratic in Alpha and Alpha hat, so it's a convex type problem.",
                    "label": 0
                },
                {
                    "sent": "A quadratic problem, just like before 1 solution and that makes it quite nice.",
                    "label": 0
                },
                {
                    "sent": "OK, one last point, I should mention about this is if you ever encountered having to solve this thing, one must say you must not do is the following.",
                    "label": 0
                },
                {
                    "sent": "Don't define this as Alpha minus and this is Alpha plus and do the optimization in terms of those re define variables because that is a different problem.",
                    "label": 0
                },
                {
                    "sent": "It is actually if you think about it, no longer a quadratic programming problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so to remain as a quadratic programming problem you've got to do it in terms of this formulation here never.",
                    "label": 0
                },
                {
                    "sent": "Some people have tried that to redefine.",
                    "label": 0
                },
                {
                    "sent": "You actually get a semi working thing is actually not a convex problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so you must strictly do.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As stated.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This quadratic type.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That is, solve subject to constraints.",
                    "label": 1
                },
                {
                    "sent": "Just like previously, and those constraints are.",
                    "label": 0
                },
                {
                    "sent": "This is equivalent of my summon Iyi Alpha I type condition, and this is like that soft margin condition I had previously OK.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And they the Alpha and Alpha Hatala growth multipliers.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what I would do if I just run back is out solve this task, give you a present the data data coming in here the corresponding labels coming in here I make a choice for epsilon.",
                    "label": 0
                },
                {
                    "sent": "Typically I would mess around with that actually defined.",
                    "label": 0
                },
                {
                    "sent": "There is a psychological thing.",
                    "label": 0
                },
                {
                    "sent": "There's certain suggested values for certain types of problems, but I would.",
                    "label": 0
                },
                {
                    "sent": "Could you could if you wish, set that to zero, OK if you wish to start with, but I would put my data in in my kernel and so.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Solve the quadratic programming task.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Subject constraints having got my Alpha, my Alpha hat, and my choice of kernel, I put in a new data point said and I should get out a continuous valued number like so.",
                    "label": 0
                },
                {
                    "sent": "I'm actually tried out regression using this method works quite well.",
                    "label": 0
                },
                {
                    "sent": "I'm not quite sure whether it's totally competitive with some other ways of doing it, so there may be other ways of doing aggression which are similar.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Most maybe slightly better, right?",
                    "label": 0
                },
                {
                    "sent": "We can define many types of loss function, so there's different ways to do the regression.",
                    "label": 1
                },
                {
                    "sent": "We can define a linear programming approach to regression.",
                    "label": 1
                },
                {
                    "sent": "I did it just now in terms of quadratic programming and we can use other types of approaches to regression, one which had been pushed a lot by Johan circles in Belgium is a kernelized least squares approach.",
                    "label": 0
                },
                {
                    "sent": "OK, So what I'm saying is the bigger.",
                    "label": 0
                },
                {
                    "sent": "Field in this.",
                    "label": 0
                },
                {
                    "sent": "Cancel.",
                    "label": 0
                },
                {
                    "sent": "Maybe?",
                    "label": 0
                },
                {
                    "sent": "Collaboration means yeah, OK, I think it may have come to the end here I know applications.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Ascentia Lee.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It sort of operates a bit like the following.",
                    "label": 0
                },
                {
                    "sent": "That slack variables the Alpha of the Zion's I hacked so come in here.",
                    "label": 0
                },
                {
                    "sent": "This is exactly what I want to do is find out why here that is my function I wish to set up I have a slack variables on both sides.",
                    "label": 0
                },
                {
                    "sent": "OK now my objective function tries to force those slack variables to zero which forces me towards this function here.",
                    "label": 0
                },
                {
                    "sent": "OK now the epsilon bid which was actually.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there is actually this type of loss function.",
                    "label": 1
                },
                {
                    "sent": "You have other types of loss functions as well.",
                    "label": 0
                },
                {
                    "sent": "Basically there's no penalization from the loss function if you're in a certain band around the function.",
                    "label": 0
                },
                {
                    "sent": "Outside that bound, it begins to penalize you so it looks a bit like the following.",
                    "label": 0
                },
                {
                    "sent": "Let me do the sign function which afterwards regression OK.",
                    "label": 0
                },
                {
                    "sent": "Here's my.",
                    "label": 1
                },
                {
                    "sent": "Sine function, The actual data give you is a little bit corrupted by noise.",
                    "label": 0
                },
                {
                    "sent": "OK, and looks a bit like this and this sort of band.",
                    "label": 0
                },
                {
                    "sent": "OK looks a bit like the following.",
                    "label": 0
                },
                {
                    "sent": "OK, so if I minimize my objective function OK and I force those slack variables.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Zero, so I had the objective function here.",
                    "label": 0
                },
                {
                    "sent": "If I did, it got away with that force.",
                    "label": 0
                },
                {
                    "sent": "These to zero OK, but had an epsilon which is not zero.",
                    "label": 0
                },
                {
                    "sent": "OK then actually the solution I end up with at the end of the day is a function which is quite smooth, OK, and which sort of goes through this space and we roughly map the sine wave.",
                    "label": 0
                },
                {
                    "sent": "OK, if I tell an epsilon which was precisely zero, then obviously what I'm doing here is shrinking this band to 0.",
                    "label": 0
                },
                {
                    "sent": "And unfortunately I don't have different colors here, but what I would do would fit exactly to these data points.",
                    "label": 0
                },
                {
                    "sent": "It fits the noise and this gives me a sub optimal solution.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's roughly how it works.",
                    "label": 0
                },
                {
                    "sent": "Not quite as clear as the binder classifica.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right applications vast number applications.",
                    "label": 0
                },
                {
                    "sent": "Too many summarize here.",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two applications share our repeat one.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Leave earlier.",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is.",
                    "label": 0
                }
            ]
        },
        "clip_101": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Am I crazy?",
                    "label": 0
                }
            ]
        },
        "clip_102": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What is seen this?",
                    "label": 0
                }
            ]
        },
        "clip_103": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'll skip that.",
                    "label": 0
                },
                {
                    "sent": "Was sick one recognition of ZIP codes very important real life standard benchmarking data set is NIST and.",
                    "label": 1
                },
                {
                    "sent": "This slides are wrote down a year or two ago, so this is probably not accurate any longer, but state of the art for recognition of Postal codes was a neural network.",
                    "label": 0
                }
            ]
        },
        "clip_104": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The net five for some time, whereas Burner shook off Dennis to cost.",
                    "label": 0
                },
                {
                    "sent": "Using SVM with virtual training and they get a improvement.",
                    "label": 1
                },
                {
                    "sent": ".15 which is good in this context in terms of the test error reduction for recognition of Postal codes.",
                    "label": 0
                },
                {
                    "sent": "So you know this is a huge important application and one way of the SVM comes out top.",
                    "label": 0
                },
                {
                    "sent": "Actually, that's the end of my talk.",
                    "label": 0
                },
                {
                    "sent": "Now there is a second talk.",
                    "label": 0
                },
                {
                    "sent": "Who wants to the second or probably be a break when I get going to drink 2nd?",
                    "label": 0
                },
                {
                    "sent": "The talks about sort of elaborating this picture how to do it not using quadratic programming but linear programming model complexity.",
                    "label": 0
                },
                {
                    "sent": "I can't remember what's in the second or actually, but does anybody want to do the second talk?",
                    "label": 0
                },
                {
                    "sent": "OK, if there's a critical mass of, there's just one or two individuals.",
                    "label": 0
                }
            ]
        }
    }
}