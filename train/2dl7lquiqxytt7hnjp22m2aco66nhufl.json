{
    "id": "2dl7lquiqxytt7hnjp22m2aco66nhufl",
    "title": "Lectures on Clustering",
    "info": {
        "author": [
            "Ulrike von Luxburg, Max Planck Institute for Biological Cybernetics, Max Planck Institute"
        ],
        "published": "July 9, 2007",
        "recorded": "July 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Clustering"
        ]
    },
    "url": "http://videolectures.net/bootcamp07_luxburg_clu/",
    "segmentation": [
        [
            "Can start.",
            "OK so I'm gonna talk about clustering.",
            "And I figured out that many people, if you already know a bit of clustering, so many people already know what he means is many of you have already implemented it and so on.",
            "But some of you don't, so I really had to struggle the last week to figure out where I should start with.",
            "I should start with the boring stuff like he means and single linkage in half a few days off, or whether I should start with something more interesting.",
            "So I decided for the interesting bit, which means that the boring bit comes later, but it comes a bit shorter.",
            "Hopefully when the interesting bit, in which also means that the order of my slides is not exactly the one I designed them, so maybe.",
            "We'll see where."
        ],
        [
            "Everything works out.",
            "So I mean, this plan is not really up to date, but essentially what I want to do is now I want to start with the advanced algorithms section and because I think so in the algorithm I want to present to you in detail is spectral clustering because I think on the one hand it's a very interesting algorithm and it also shows very many things one has to take care of when one tries to design A clustering algorithms and there are many steps involved and we will go through all of them in detail, then OK. We also have some other algorithms which are partly in this first section which I now will.",
            "Present maybe in the later today or even tomorrow.",
            "And then this is the third and 4th lecture for tomorrow.",
            "Mainly will concentrate on issues which are more from a theoretical side so.",
            "We want to ask the question what how can we define what clustering is?",
            "Or can we define it or does it help anything to try to define what clustering is?",
            "Does it help to look at algorithms from a theoretical point of view?",
            "What are the questions we would like to look at and so on and that's what I want to talk about tomorrow.",
            "OK."
        ],
        [
            "So before we start.",
            "OK, clustering, most of you know what it is, but the intuition is we're given some objects.",
            "They can be whatever we want.",
            "They can be digits.",
            "There can be images that can be.",
            "I don't know, Gene expression, microarray data or whatever.",
            "So we have those objects and what we also have some relations between those objects.",
            "So what we want to do in the end is we want to say which objects are similar to each other and which objects sort of belong to the same group of objects of which objects belong to different groups somehow.",
            "And in order to do that we always need some information about the relations between objects.",
            "And this information can either be we have something like a similarity function or distance function, or it could also be something more abstract like a neighborhood to relationship.",
            "Certain points are neighbors of other points, but we don't have like a way of quantifying that something like that.",
            "And then OK, the goal is to find groups in those data points which have something to do with each other and intuitively somehow it's clear what we want to achieve with it.",
            "But practically, it's often very difficult to really get a grip on what we exactly want to do.",
            "And one of the reasons is that actually there there might be very many different reasons why you would like to do clustering.",
            "So the first reason which is I think maybe the older reason is you want just to explore your data.",
            "So somebody gives you a data set like in Isabel setting.",
            "Maybe you have a customer and he comes up with this data set and gives it to you.",
            "You have no clue about the data.",
            "You've never seen such data before and before you know, start going off with designing the most complicated model to fit some data.",
            "You might just try to play with your data a bit and try to find whether they're structures in the data.",
            "Which you should need to know before you go off and do something more complicated.",
            "And.",
            "OK, and one way you can do this clustering.",
            "So you just apply clustering algorithms to try to figure out whether their groups in the data which might have something in common and maybe in later analysis you would like to treat those groups differently.",
            "That might be a goal so, but here it's really more.",
            "It's really playing around, it's not, you don't have a.",
            "Like an objective goal, like minimizing the squared error loss, whatever.",
            "It's just, it's more fuzzy somehow.",
            "The second goal, which is very, very important and becomes even more important, is the data amount.",
            "Gross is that often you have so many data points that you simply can't run an algorithm on your 5 million data points, so you need to do something about it and what people often do is they first of all cluster the data points.",
            "To some groups and then they try to run the algorithm on the clusters instead.",
            "So instead of running it on 5 million data points, we maybe say we generate it on 1000 classes and then then we try to analysis to analyze those clusters.",
            "OK, so and you can already see those two reasons might be very different from each other, so you really want to achieve different things with both both goals.",
            "But somehow both is called clustering and that's already part of the confusion."
        ],
        [
            "So here I just want to start off with some pictures before we go into an algorithm.",
            "So this is gene expression data and maybe many of you have already seen it.",
            "So here and there.",
            "So on the left side you see something like.",
            "So it's a gene expression.",
            "Data is something from bioinformatics where you want to measure whether certain genes are expressed in a cell or not.",
            "And you do this because you want to find out.",
            "I don't know which genetic reasons there might be that a certain cell does something or whatever.",
            "I'm not a biologist, so.",
            "So, so the raw data.",
            "You get something or it's actually already pre processed.",
            "The process that we get is like a matrix which contains the genes in the call in the rows in different conditions in the columns.",
            "And now you might want to figure out at a certain type of cell always that there are certain genes which are active in the cell.",
            "If you cluster the data ideally and if you believe that the clustering result in something useful, you should get something like this where you I don't have all the express jeans on top and all the non express jeans on.",
            "Bottom and then you can give this to a biologist and he can tell you whether this might have some meaning or not."
        ],
        [
            "OK.",
            "So another area which I find quite fascinating is Network analysis.",
            "So there very many datasets which are not just numbers like in teen expression.",
            "In the end you have numbers.",
            "It's a vector, but very many datasets are in form of a graph directly.",
            "So if you want to, I don't know, you look at the Internet into 12222 data mining on some web pages.",
            "They always have the structure of a graph because you have the links between the web pages so.",
            "And maybe now you want to figure out.",
            "I don't know which pages are related to which other pages.",
            "And here is.",
            "This is an example for a social network where you it's from a company they collected who writes email to which other person in this company and you just make a graph out of it and maybe you want to know.",
            "I don't know which are the groups of people who communicate with each other which are groups which should more communicate with each other but actually they don't.",
            "So you might want to find classes in such a graph."
        ],
        [
            "OK, an example which I think many people if you are working on this image segmentation where the goal is a bit.",
            "I mean what you want to do essentially is you are given an image and you want to do a very rough segmentation, which just I don't know segments background from foreground or something detects very very large structure in your data set and then you might go and analyze it a bit deeper."
        ],
        [
            "And the last example I want to show.",
            "Is hierarchical clustering, so the ones we show I showed before it, you always have a data set and then you say you want to find.",
            "I don't know five clusters.",
            "Here we have something where we not only want to find like a certain number of clusters, but we want to find a hierarchy of clusters, and the example here is told from some paper here is you want to you want to classify mammals in different groups and what you then get to such an evolutionary tree which maybe shows that I don't know.",
            "Red and mouse are very much related to each other there or cluster on their small cluster.",
            "But then on a very high scale there also related to primates.",
            "But sort of the the distance between the primates to the Reds is pretty large, so it's or it's a, it's a hierarchy, so so.",
            "All of them are mammals and then you can split them into different groups, and that's called hierarchical clustering."
        ],
        [
            "OK um.",
            "So, and that's not the part I want to skip.",
            "Because I just want to start with something else."
        ],
        [
            "OK, so the algorithm I want to start with now is called spectral clustering.",
            "It's an algorithm.",
            "Which does not do hierarchical clustering.",
            "So you tell the algorithm, give me 5 clusters, and then hopefully it gives you 5 or it will give you 5 classes and whether they're useful."
        ],
        [
            "Not we will see.",
            "So I want to go into detail to this algorithm, and before we do this I want to give a rough overview that you know where in which step you are so.",
            "Very rough idea of spectral clustering is the following.",
            "So OK, we start with data points as usual and we assume that we have similarities, so similarity means so that slows as IJ we have over there.",
            "So similarity means a similarity between two points is high.",
            "If the objects are very close to each other.",
            "If they were related to each other, so they hire.",
            "This value is the more the more.",
            "I believe those points should be in the same cluster.",
            "Now what I do is I want to represent the state in a in a in a convenient form, but I don't have coordinates, so I don't assume that those points are in Rd or something like this.",
            "It can be anything and I just have any similarity function and then an abstract way of representing your data will be to build some similarity graph and we will see different ways of doing that.",
            "Um?",
            "So what we do is we just say each vertex of this graph represents one data point and then we connect the vertices which are similar to each other.",
            "So we could say.",
            "And then we put an edge between between the two vertices.",
            "So here we have.",
            "I don't know point 1.2 and they are similar to each other, so we put an edge.",
            "And Additionally what we do is we wait the edge by the similarity.",
            "So if points are very similar to each other, we put a like I always what I plot here is always a shortage, so they're very related to each other.",
            "They are close.",
            "And it carries the weight as I J.",
            "And here they are.",
            "They maybe have a positive similarity, but it's so here.",
            "Maybe we have similarity 0.9 and here we have 0.1 so it just is a graphic way of showing they're not as similar as the other tools.",
            "And then OK, now we have this graph and the goal of clustering is then sort of to find the cut in this graph such that.",
            "Such that the resulting things are clusters and we will define how this can be done.",
            "And so that's the basic scheme, just to keep in back of your mind."
        ],
        [
            "What?",
            "OK, because now we need to start with some notation.",
            "So OK, I already said this, so we need a similarity function and we will always put the value.",
            "The similarity values between the data points in a matrix, which is called capital S. So that's a similarity matrix.",
            "Then we need to know what the degree of a vertex is.",
            "So in a graph, the degree of a vertex is just the sum of the edge weights of the edges which are attached to this vertex.",
            "So it simply shows you.",
            "So you sum the similarities of all data points which are close to this vertex which are attached to this vertex.",
            "That's the degree of a vertex.",
            "And then we will put those into a matrix which we call degree matrix.",
            "It's a diagonal matrix and we just put the different degrees on the diagonal.",
            "Um?",
            "OK, and then what?",
            "We will also need later is we need to measure how large different parts of of of a graph are.",
            "So if we say later on, for example, we would assume this guy should be a cluster.",
            "We will need to know how large this cluster is.",
            "And there are two different, generally two different ways how you are too.",
            "I don't know if you think about how you would measure the size of a cluster.",
            "There are two ways you could come up with, so either you simply count how many vertices are in this cluster.",
            "That's what I do note by absolute value of a.",
            "So if this is our group, the absolute value is just five.",
            "Or we can do which is called the volume of a instead of counting the vertices, we simply count the edges.",
            "So what we do is forward vertices which are in the subset A.",
            "We sum up the weights of all edges which are attached to those vertices.",
            "Those aren't connected to each other, so it means that we don't know the similarities in those points.",
            "No, it means it means, well, OK.",
            "It means that the similarity zero essentially so and do and I will.",
            "I will tell later later on how we achieve that the similarity is here.",
            "So it's essentially what I plot.",
            "Here is a sparse similarity matrix, so you know that certain points, for example, what you can do is you just computer Gaussian kernel matrix on your data, But then you Only Connect points where the similarity is higher than 0.9 and the other ones you set to 0.",
            "So it's essentially if there's no edge, we really assume that there is a similarity 0.",
            "However, we achieve that later on.",
            "OK. Anymore questions so far about the notation.",
            "OK, so you need to.",
            "You need to have the intuition what this means, because if you don't know it then you will get lost from the next slide.",
            "Um?",
            "OK, and one more thing, which you probably can't read because it's just on the bottom of the screen.",
            "So assume you have such a graph, and later on we want I mean clustering.",
            "In the end will be we want to give labels to data points, whether they're in cluster one or in cluster two and Cluster 5.",
            "So we want to attach vertices to those data points more or less in each vertex get the get the label of its corresponding cluster.",
            "And the way you can express this now, as you can just write the vertices as a vector, so iffy, meaning the clustering the index of the clustering of the East data point, and we will often right.",
            "So that's why vectors in my notation often called F, because essentially they are the function values.",
            "So if we have a vector of N data points in this setting, we usually mean this vector contains the entries which would show you in which cluster the data point should be.",
            "OK."
        ],
        [
            "OK, so now the standard object we will always use in spectral clustering is the graph Laplacian.",
            "And actually, I just think.",
            "OK, let's leave it.",
            "So the graph Laplacian, it's a matrix and.",
            "So given a graph, you can define this matrix which is called graph Laplacian and it is simply the difference between the degree matrix and the similarity matrix.",
            "OK, just take it as a definition for now.",
            "We will see why why this makes sense later on.",
            "Um?",
            "And the most important property of this matrix is the one which I have on this slide, so everything else is what we will do is is a consequence of this property, and the property is that if you take some vector F in our end and two multiplied from left and rights to your matrix, what you will get is the expression you see down here.",
            "So it is essentially you some overall data points, so all pairs of data points the sun runs over inj.",
            "So for all pairs of data points you some.",
            "The similarity times the difference of the function value squared.",
            "Sorry.",
            "Yes.",
            "Yeah.",
            "What's the question?",
            "Here.",
            "So give me oh there.",
            "Oh this actually here sorry.",
            "Let's attack was done when it happened.",
            "Sorry, so forget about this L here.",
            "Yeah, so maybe I write the definitions again that you don't lose it.",
            "So the is the matrix which contains the degrees on the diagonal and otherwise is zero and S is just a similarity matrix which contains the entries of your similarity function.",
            "And OK, I'll write it again.",
            "The degree which you have in this matrix DI is some over JSIJ.",
            "It's the sum of.",
            "The sum of the similarities to other data points.",
            "Yes, so if I so F is now that's what I said on my previous slide.",
            "So F is A is a vector and then."
        ],
        [
            "Really, the notation can be confusing when you come from linear algebra, so F is really this vector.",
            "Um?"
        ],
        [
            "Oh OK, but but for this equation it doesn't need to be discreet, so this holds for any arbitrary vector in R in and it's actually very simple to see, so you just.",
            "It's really, it's not more than then there is on the slide.",
            "So you start by putting so Ellis just D -- S. So if you forget about this L here, you just multiply it out.",
            "Then you plug in the definition, so decide diagonal matrix.",
            "So if you multiply is a vector to the diagonal matrix, you will get this expression here, which is just a sum over DFI squared.",
            "And this is just a quadratic form corresponding to S and now essentially you just do some clever.",
            "I mean you observe that the I is actually the sum over SJ.",
            "You sort of rearrange the formula in fewer, more less than.",
            "I mean it takes 10 minutes to think about it, but essentially it's really not more than it's on this slide.",
            "OK, so why is this property actually helpful?"
        ],
        [
            "I'm.",
            "We already see this if we look, and that's actually also explains why we call this in graph Laplacian.",
            "So this property in some way measures the deviation of a function or how.",
            "How much variation a certain function is.",
            "So if you just take some graph.",
            "And now we look at what this expression does.",
            "So assume we attach some function values so we have equal say plus one here, minus one here, minus 10 here.",
            "Well, actually we're doing clustering, so call it 2 here.",
            "Two here, and maybe this is in cluster 10, so you will have all those cluster labels.",
            "And if you now look at this expression.",
            "You look at all two pairs of points at all pairs of points and you sum the difference between the function values weighted by the similarity.",
            "So this means that.",
            "Somehow and later on we will minimize this quantity.",
            "So if you minimize this quantity, this will mean that either this essay is a very small number and then it doesn't matter what the function does.",
            "So if points are very far apart or are not similar at all to each other, we don't punish if the function values are different from each other.",
            "But if if the function values have side is very large, which means that points are very close to each other, then.",
            "We then if we want this whole expression to be smaller, we need that the function values are very very close to each other in a certain sense.",
            "And so intuitively, what this expression does is it takes something like a computing variation or a second derivative.",
            "So if you know that's very handwaving, it's just to give you an intuition why we call it this way.",
            "So often people think so.",
            "Similarities is something like an inverse distance.",
            "So distance is small if points are very similar to each other.",
            "So if you want to get from a similarity to a distance, you can just say you take the inverse.",
            "And now if you replace this AJ here by the inverse distance and conveniently we squared OK. Then this is something like a difference quotient, so you have the.",
            "We do have it so you have the the distance between FIFJ and then you you divide by DJ so it looks like like if you take a derivative so in the derivative you always have something like F of X I -- F of X J / X and Y -- X Ray and then you take this the limit to Infinity and that's what is essentially the expression here and now you can if you want to make this formula.",
            "Then somehow if you sum sum squares of derivatives it looks like so if you take the sum is an integral.",
            "A derivative is this, not the operator.",
            "Then it's somehow like to Laplace operator on argue.",
            "So if you don't know the Laplace operator forgetting about, but it's just.",
            "This this quadratic form measures the variation of a graph of a function of the graph.",
            "Contact three clusters so we have like 3 discrete values or F right so?",
            "Assume that the labels have some particular order.",
            "OK.",
            "But type of thing that sits here it doesn't matter.",
            "Yeah, I agree so, OK, so let's come to this point later.",
            "So when I introduce this algorithm in the following, I will.",
            "I will always talk about the case, we just look at two clusters and there doesn't matter because you have either it's the same or it's not the same if you do something with more classes you actually don't only look at one vector, you do some similar.",
            "Trick is a better I think.",
            "Already told you you discretize by just like you introduce a vector which has three components and you set it to one of the clusters in this.",
            "One setting, but let's worry about that later, but it's true, it's yeah so."
        ],
        [
            "OK, let's continue.",
            "So now we've seen.",
            "So the definition of Li write it again here.",
            "Um?",
            "We've seen the key property I go back maybe.",
            "But if we multiply a vector to L then we get this outcome.",
            "Now what is the first thing which which tells us is?",
            "So if we assume that the similarities are non negative and that's something one usually always assumes, like if you have a Gaussian kernel whatever the case will never be nonnegative, will never be negative.",
            "So then this expression is always non negative because we have something which is not negative here and here we are something.",
            "So this whole thing will never be negative, which means and we do this for all vectors F that the matrix L is positive semidefinite.",
            "That's just the definition.",
            "OK, so that's already a very important observation.",
            "The second observation is OK. Now we know it's positive semidefinite, so so we know all eigenvalues will be either 0 or larger.",
            "And now it turns out actually 0 isn't eigenvalue of this matrix.",
            "And the corresponding eigenvectors are constant one vector.",
            "And this.",
            "I mean once you know it, it's obvious because somehow if you.",
            "Oops.",
            "Because if you multiply the constant one vector to the Laplacian, it's just D 1 -- S one and S1.",
            "I mean D contains the row sums of the matrix S. So here by take multiplying the one vector could take the row sums and you already have their awesome.",
            "So if you take the difference, it will be 0.",
            "So think about it for a few minutes, but it's pretty obvious that this thing has to be 0.",
            "So what we know is we know all eigenvalues are non negative.",
            "So if we have a smaller so the smallest angle we can have zero and actually we have it.",
            "So we have this angle is zero and the corresponding eigenvector is a constant one vector.",
            "Um?",
            "Now that's actually a very very nice relation already between the eigenvectors of the graph.",
            "Which of angle is zero, and the number of clusters, and the relation is as follows.",
            "So of course the matrix equative the eigenvalue zero several times, so it could be that the spaces just does not only contain one vector, but contains several vectors.",
            "And what time is out now?",
            "Is that this actually only happens in the situation where your graph contains of different disconnected parts.",
            "So if our graph looks like this, so we have two parts and there are no connections between each other.",
            "Then one can prove that the multiplicity of the eigenvalue zero will be exactly 2.",
            "Because we have two connected components and the eigenvectors which correspond to this to those two parts are essentially like the indicator vectors of the parts.",
            "So we have one of the vertices are ordered.",
            "This is the first group in this the second group with one in Vector which looks like this and the second one looks like this.",
            "And those two eigenvectors spend the space of angle is 0.",
            "And actually, that already shows that the small eigenvectors of image of the Laplace matrix might have something to do with the number with the clustering, because we see that if we would look at the first 2 eigenvectors of this matrix, it is they really contain the cluster indicator vectors of the two groups in our data.",
            "And of course there's only holds if there is no noise, so if you have an edge here, it gets more complicated, and that's what spectral clustering is about.",
            "But that's really the intuition.",
            "Or that's a very important observation.",
            "The nice thing about this statement here is that the proof is actually very simple and very elegant, and I like it a lot and we will do it in the exercise."
        ],
        [
            "OK so um.",
            "The Matrix LV IF looked at so far, is what is called the unnormalized Laplacian.",
            "So it's one of the word unnormalized so far because we didn't have any normalized ones.",
            "But now we introduced to other versions of Laplacians which are normalized graph laplacians so this will be the unnormalized version and essentially we will see that later we will need mainly this one.",
            "So the normalization what it essentially does is it normalizes the matrix such that the eigenvectors are the eigenvalues are always in the same range, which is not the case for the for this unnormalized application and we will mainly work with this matrix.",
            "Well, we normalized by multiplying the inverse of death to the front of L. And if you multiply it all out so you have L and now you put the to the minus one in front, then they hear it cancels, so that gets the identity matrix.",
            "Here and here we have the data, the minus 1 * S, so that's the part here.",
            "And the reason why I put this index are we?",
            "It's because it's so it stands for random Walk.",
            "So for those of you who know what the random walk on a graph is, this is the transition matrix of the random walk on the graph.",
            "So who actually knows what the random walk is?",
            "OK, I'll go into that later.",
            "So for those who know that's the relation for the others will see.",
            "OK, there's also another normalization, but I don't want to talk about this.",
            "Actually, in the lecture, just take it.",
            "There's another normalization so we don't.",
            "Put the D -- 1 in front but we multiply from both sides we with due to the minus 1/2 and the advantages of this matrix is still symmetric while the matrix here is not symmetric but it forget it, it's not important.",
            "So what is important is that the properties of this random of both matrices are actually very close to the other one, so also both are positive semidefinite in both of the properties that like this property about the number of clusters corresponds to the to the multiplicity of the first eigenvalue value."
        ],
        [
            "OK. OK, now I want to introduce.",
            "I want to show oh sorry.",
            "Definition of.",
            "The rationality.",
            "OK, so I think the that's a very good question so.",
            "Maybe we'll see that later.",
            "I mean, I will show later on why this I mean.",
            "Right now.",
            "It's a mystery, I know, so I will try to solve the mystery later on.",
            "I think the most important point is really this property we had.",
            "So this one here, F transpose LF equals sum SIJ if I -- F J squared because somehow it is a.",
            "It is so in machine learning.",
            "It is often seen as a regularizer, big because it simply punishes if a function on a graph varies a lot on parts where the graph is closely connected.",
            "I mean, there are very many.",
            "I mean there's a whole field called spectral graph theory which does nothing but look at different matrices and how those properties of the matrices are related to the graph.",
            "And often this helps you, because then you can use linear algebra.",
            "Things to for example, compute how many comic connected component your graph has, or it's just a different view of looking at graphs.",
            "I think, and this one is just a matrix which turns out somehow to be very convenient, but it's nothing you can see on the.",
            "It's not obvious on the first glance, it's really if you go deeper into it.",
            "More questions so far.",
            "OK, so.",
            "Then I will introduce the algorithm and it's nothing you don't need to understand why it works right now.",
            "You just need to understand how it works right the way it comes later.",
            "So what we do is so we're given our data.",
            "And the and we are given the similarity matrix, so OK, so we give our that we built the similarity graph in.",
            "I mean I didn't really go into detail how to do that, but essentially we connect points which are similar to each other.",
            "And now the the whole miracle of spectral clustering is stepping.",
            "Only look at eigenvectors.",
            "So there are two versions of spectral clustering, which I call unnormalized and normalized spectral clustering is just different flavors so.",
            "Let's just talk about the anomalous version.",
            "What we do is we compute the graph Laplacian.",
            "We compute the first OK. Then we compute the iron vectors of this graph Laplacian.",
            "We stuck it in the Matrix V, so you have the matrix V which just contains the 1st.",
            "So say all eigenvalues as columns, right?",
            "Oh, I'm vector sorry.",
            "And now.",
            "Actually what we do is if you want to find K clusters and say for the simplicity case equal to two, we only look at the first 2 eigenvectors here, so.",
            "What we're going to do is we just look at the first 2 eigenvectors, stick them in this matrix so it's A N * 2 matrix right now.",
            "That's this matrix V. And now we're going to interpret the role.",
            "So it's it's a bit mysterious.",
            "We're going to interpret the Rose of this matrix.",
            "Is new data points, so we say.",
            "So here we have, say, entry ABC DEG whatever.",
            "So the first data point we just take the rule.",
            "But just as a D. So we say we map the first of all of our data point now to a 2 dimensional representation which contains those two points.",
            "So we say the first data point.",
            "Which is just some abstract object or image or whatever is now mapped into a vector in R2 in this case.",
            "So it's just a way of getting a completely new representation of your data, and it's you don't need to understand why.",
            "You just need to understand how.",
            "So we interpret the Rose as new data points at Eve.",
            "And now we have those new data points.",
            "There are no points in R2, so now we say OK, great.",
            "Now we're not who we can cluster them and we just use K means to do that.",
            "And of course, I mean it's not obvious at all why this should buy you something.",
            "I mean you.",
            "You just do it right, but that's how spectral clustering works.",
            "So it's really a very, very simple algorithm to compute your graph Laplacian.",
            "You compute the eigenvectors you map them into this real space, and then you K means or any other simple clustering algorithms on this representation.",
            "And now you can do this in two ways.",
            "You can do it for the with the normalized or the unnormalized Laplacian.",
            "And depending on which one you take a call, it normalized run normalized spectral clustering.",
            "That doesn't mean yes.",
            "Actually, it does matter later on, because if you don't normalize them.",
            "Camins might get to do.",
            "It might have different scales on different coordinate somehow.",
            "I mean usually we here like this.",
            "We take the normalized eigenvectors here.",
            "OK, so maybe I just show how it works."
        ],
        [
            "Write on an example.",
            "So here we have a toilet I sent this week.",
            "About this week Gaussians in are two very simple data set.",
            "What we do is we take the similarity function.",
            "And for convenience, that's something.",
            "I mean most people do is they simply use the Gaussian kernel so as to compute the similarity between two of your data points.",
            "You just use this expression and OK, there's a parameter Sigma to set.",
            "We don't talk about how to do that now.",
            "Just set it in some way.",
            "And then we simply take the graph.",
            "The similarity matrix of the graph is like the similarity matrix as the adjacency matrix of our graph.",
            "So essentially it's a completely connected graph, but we will have, like the weights will be very different on the edges, so there will be some edges like the edge which connects this in this point will have weighed approximately 0 because it causing Kernel says they are not similar at all and the weights here will be pretty large.",
            "OK, and now we say we don't know how many clusters there are, we say."
        ],
        [
            "Explore a few number of clusters.",
            "So what we do is we compute the eigenvectors and that's this matrix V here which we have here.",
            "And because I said we want to compute, I don't know for a five class to say we compute the first 5 eigenvectors.",
            "Now they're just a columns of numbers, right?",
            "What we can see here.",
            "The first one?",
            "That's the one, the constant one eigenvector we always had, so I said the eigenvector of eigenvalue zero is always the constant one vector.",
            "And now as we normalize it to have norm one, it's not.",
            "It doesn't, interest doesn't have entries one, but just in this case some like 0.1 something, but it's constant.",
            "Then if we look at the second one.",
            "We actually see that there are quite a lot of numbers which are around.",
            "Minus Zero point 14.",
            "So here we have three.",
            "Here we have one for you.",
            "Probably can't read it, but believe me, so there are some numbers which are more or less look very similar and the other numbers are always something 0.0 tiny positive thing.",
            "OK, so this means that this vector more or less contains I mean more or less two different numbers.",
            "And that's why it's why it's going to be helpful later on.",
            "And OK, it gets more complicated the more eigenvectors you take."
        ],
        [
            "I'm.",
            "So now here I have a plot of those eigenvectors in a different interpretation, so.",
            "OK, ignore the first row.",
            "Look at the 2nd row.",
            "What I plot here is so the 1st.",
            "No, look at the first draw.",
            "Ignore the 2nd row.",
            "I plot the.",
            "First Eigenvector is a function on the data points in this function is color coded, and probably you can see the yellow very well.",
            "So the first eigenvector is so.",
            "The first eigenvector was this one which was just constant, so I so on each point I just saw what I plot.",
            "Here is the data point and then the like a color corresponding to the entry of the eigenvector.",
            "And here it's constant.",
            "It has all the same colors.",
            "Here if we look at the second eigenvector that was the one where I said we have two different entries, so we see if we color code where those different entries are that most of the.",
            "So I think those were the zero point.",
            "14 entries are on one thing we would consider a cluster and the other ones are.",
            "More or less the same.",
            "Now if you go ahead it gets more and more complicated.",
            "So here we can see the colors discriminate between this cluster and those two.",
            "Maybe here they discriminate between this cluster in those two.",
            "And here it already gets more fuzzy.",
            "OK, but that's more or less how they investors look like if you."
        ],
        [
            "Look at them in a color code.",
            "And now I'm.",
            "That's a spectral embedding.",
            "So what I plot here is we now have.",
            "In the three dimensional case, so I plot the case where we have.",
            "So say we want to discover three clusters.",
            "We have those three eigenvectors.",
            "And we map each data point to a row in this.",
            "Vector, and that's what I've got here.",
            "So each of the of the Blue Cross is actually corresponds to one of those points in R3.",
            "I'm and maybe I'll show a lifetime later on.",
            "You can.",
            "It's better to see so.",
            "So we can see that forget about the red one.",
            "So far the blue points are more or less.",
            "Here is a very large cluster of points.",
            "Here is a very large clustered here and then.",
            "OK, we have some outliers and this is another presentation on which we run K means.",
            "So let's after embedding the data points in R3 you have this.",
            "And now K means.",
            "I mean, even if it's a very dump algorithm.",
            "But that's a very simple case, so we have like I don't know, 20 points here.",
            "20 points here, 20 points here.",
            "So K means if you're not completely stupid.",
            "I mean it will be able to detect the clusters, right?",
            "And that's actually what's going to happen.",
            "So if you look at three clusters, OK. Then came into work on the three dimensional representation and will perfectly identify the three clusters.",
            "I mean, of course it's a toy example, and now if you look at what happens if you take a spectral clustering to just find 2 classes, it OK, discovers two of them for four and five, it starts to split clusters.",
            "OK, so I think that's a bit so.",
            "How many people have I still with me?",
            "So yeah.",
            "Oh hang on, I need to close the door.",
            "It's just so disturbing.",
            "Sorry.",
            "Yes.",
            "In the reference, yes.",
            "In this representation, it's true.",
            "They're all the same.",
            "The reason why I included them is that if you are actually in the ideal case where your data.",
            "But your graph contains disconnected components, the eigenspace.",
            "Has dimension which is higher than one.",
            "So for example two if you have two components and then the eigenspaces spent by those components 1000011 and it might be.",
            "And of course the constant one vector is also part in this eigenspace, but depending on which I can follow you use.",
            "It might happen that you the first one you output is not just a constant one vector, so in most real cases the first vector doesn't play any role, but in extreme cases it's important and it doesn't hurt to solve, at least if it's constant.",
            "Simply doesn't hurt.",
            "OK, yeah.",
            "What are the mapping of data point to the eigenvectors?",
            "I mean intuitively, what does it mean?",
            "Well, what it means is.",
            "Each point is math.",
            "Yeah, yeah.",
            "Well actually the problem is it's nothing where.",
            "It's simply to get an intuition on it.",
            "I still don't really have a clear intuition, but what is going on is.",
            "Let's take again this ideal case where we have two separate clusters and then our matrix.",
            "We will look like this, so that's a first, and that's the second eigenvector.",
            "So what happens is and say this is data .123456.",
            "So what will happen is we will map the first data point.",
            "To the .10 and we will map the second data point.",
            "Also to 1 zero and the same for the third one.",
            "So those three points will actually be mapped on the identical point, which is just on the coordinate axis, like 00 to the left one to the top.",
            "So and for the other clusters the other way.",
            "So we will map it like if you're looking for two classes, we are in 2D, so all those points will be mapped here.",
            "And now look at the other cluster.",
            "We map them.",
            "To those columns which is actually 01, so they will all end up here.",
            "So in this ideal case.",
            "We have like after the after the embedding, this point will appear three times.",
            "This point will appear 3 times and that's actually the blue points which which we have here, more or less.",
            "Yep.",
            "Of doing killings within this state.",
            "Right, and that's what I'm going to show in the next slide.",
            "So the point the whole point about spectral clustering is that this change of representations that are really very clever one, and if your data is not a wild, it is a means of separating your data points.",
            "So classes which might be still a little bit overlapping here are perfectly.",
            "I mean, in the ideal case, at least, uh, perfectly separated in this part.",
            "Here it simply means to.",
            "To improve the OR to workout the structure in the clustering structure.",
            "But it's it's not.",
            "I mean we will see how this works in the next slides, but it's not obvious."
        ],
        [
            "OK. Um?",
            "One thing which I might already mention here, we will see that later on a bit more closely is that in this case, like the example we had, it contains 3 very clear clusters.",
            "You can now plot the eigenvalues of your of this graph Laplacian.",
            "What will happen is So what I plot here is like I had hundred data points, probably so, and so my matrix contains 100 eigenvalues and I've got all of them next to each other.",
            "So what you probably can't see it very well, but.",
            "It is in fact, what you see is we have three eigenvalues which are very very close to 0.",
            "Then there's a huge gap, and then there are all the other ones and actually one can use that.",
            "At least in theory, to predict the number of clusters.",
            "So you just look at the eigen values and look at how many small angles do you have and when do we have a gap.",
            "And then you say OK, we have three clusters, but that's just I mean people do it but."
        ],
        [
            "Doesn't work that well."
        ],
        [
            "OK. Um?",
            "OK, so I tried to before the break.",
            "I tried to get the first interpretation by spectral clustering works.",
            "That's my favorite one.",
            "So actually their spectral clustering.",
            "It's actually not new.",
            "I mean people have done it already in the math in the 70s actually, but nobody has really noticed it and it became really a very well spread.",
            "Like many people use it since like 2001 2002.",
            "There are lots of having lots of papers, but it's nothing new and so there are very many different ways of how you could derive why spectral clustering works.",
            "But I think this is the one which I find most intuitive.",
            "So let's step back a bit.",
            "We have our data graph and we set clustering is something which makes sure that points return the same cluster similar to each other in points which are in different clusters are not so similar to each other.",
            "So now OK we need to.",
            "I mean if you want to have an algorithm to do that, you somehow need to direct or.",
            "The easiest way is to derive a mathematical criterion which tells you what you can optimize.",
            "I mean, OK, that's what people do usually.",
            "Um?",
            "So the idea, what spectral clustering or what we want to look at is we say OK, what we want is we forget about the distance within clusters.",
            "We just look at distances between clusters and we say we want that two clusters are things where the connections between the two clusters are very very low weight somehow.",
            "So we want to end and we define the things we say the cut between two groups of points A&B is just so we sum over the points in the different clusters.",
            "The similarities.",
            "So we say we define what the cut between those two groups is.",
            "It's actually the sum of this in this edge, like the weights of those two edges and then we say great we already done.",
            "We minimize cut and we have an objective criterion.",
            "We use that for clustering.",
            "OK, and actually for I mean for minimizing cut it can even be done in a more or less efficient way.",
            "It's surprisingly not NP hard, but it's there's this min cut Max flow theorem.",
            "I don't know many, maybe some people know it, so you can really do that.",
            "Um?",
            "But now there is a little problem and this problem is shows up if we look at this little vertex here.",
            "So what happens very often is you have one outlier data point.",
            "And then you just say minimize cut and I say, OK, great if I want to have two groups in this graph I just got here because here are just cut one edge in here.",
            "I have to cut two edges.",
            "So OK, the problem is now of course that this is not what we want to do in clustering.",
            "So I mean somehow we don't want to identify outliers.",
            "I mean, that's also something we would like to do, but that's not what we want to do.",
            "We want to find groups in the data in those groups should be more or less have reasonable size, right?",
            "And that's why people came up with different objective functions which tried to incorporate the size of this data of the different clusters.",
            "And essentially there are two different ways.",
            "Which are do something very similar, so ratio cut.",
            "What it does is we say OK, so in general what we do is we want to define a balanced cut.",
            "That is, we say we we have two objectives.",
            "We want to minimize the cut, but we also want to make sure that the groups are more or less have the same size.",
            "And that's what they should cut that.",
            "So we're going to minimize this later on, so we have a term which incorporates the cut.",
            "But we have also term which incorporates the.",
            "The balance between the two clusters and what's actually going on there.",
            "It's very simple to see.",
            "So this term 1 / A + 1 / B.",
            "So remember absolute value of A is just a number of points.",
            "The number of vertices in Group A and if we now so just look at like look at it sharply.",
            "So if we say for example if A&B if they're both, have the same size, then this term will be just one over and half plus one over in half is.",
            "4 / 10 which is pretty small, if any sludge, and if you have another case where, for example, lesson outline just contains one point and B contains the other N -- 1 points.",
            "We have something like 1 / 1 + 1 / N -- 1, which is I mean roughly one.",
            "So here we have something in the order of 1 / N and here we have something in the order of One South.",
            "It shows that if you manage to get groups more or less, having the equal size in this term will be much smaller.",
            "And so that's what we.",
            "So now what we do is we just take the product of those two terms.",
            "It's a pretty arbitrary way of doing it, but that's how we start, OK?",
            "There's another way of doing this, so that's called ratio cut.",
            "This is called normalized cut.",
            "So very simply, instead of taking the number of points in each group, you take the volume which was the weights of the edges in each group but with the same reasoning somehow.",
            "How do you choose?",
            "Sorry, how are you?",
            "Possible subsets in or.",
            "Yeah, OK, I mean, that's where we're going to talk about that.",
            "So that's a problem.",
            "Of course, I mean a. OK, so so far we just defined our objective function.",
            "We want to minimize it and of course it's a very difficult problem.",
            "That's exactly the point.",
            "So we can just look at all different subsets of the graph.",
            "It's it's hard and actually it's an NP hard problem.",
            "Both of them are NP hard, actually.",
            "I mean more or less.",
            "It turns out any graph partitioning problem where you try to balance the clusters.",
            "You can do whatever you want to turns out to be NP hard always.",
            "Um?",
            "OK, so there is something we have to do about that to solve it right?",
            "And that's now where spectral clustering comes into."
        ],
        [
            "OK, two more slides for the before the break.",
            "So what I'm going to do now is I want to derive spectral clustering as a way of solving this optimization problem, which we've just seen before.",
            "What I do in the slightest bit cheating, I look at an optimization problem which is slightly simpler than solving ratio cut.",
            "Just because the formulas are similar.",
            "We will do the ratio cut version in the exercises, so the principle is exactly the same, but so we just so K ratio cut.",
            "So the first problem is we want to minimize.",
            "If you want to minimize ratio cut, we look at cut.",
            "I'll be team times 1 / A + 1 / B.",
            "And now I say I take a slightly simpler problem where I say I want to minimize cut AB and I enforced at A&B have the same number of vertices, so it's the same principle.",
            "This is a bit more general.",
            "But you will see it, it will more or less go in the same way.",
            "OK, so we start with that optimization problem.",
            "Now what we do is OK, we observe it's NP hard or we can't.",
            "It's a discrete optimization problem.",
            "We don't know what to do.",
            "We start to rewrite it.",
            "So what we do is we introduce this OK and we say we want to find 2 clusters.",
            "That's the simplest case.",
            "So we introduce this cluster indicator vector, which is just a function which tells us whether the data point XI is instead a or in set B.",
            "So we have this vector plus one or minus one depending on whether the point is instead a or be now.",
            "And now that's more or less already, all we have to do now comes just very nice tricks.",
            "So the first or observations.",
            "The first observation is if you now right cut.",
            "What happens?",
            "So cut the abuse defined as the sum over SIJ where we sum over I in A and Jane B.",
            "And now there's a tricky way of writing this.",
            "Is this some here now?",
            "So including which holds for this vector here.",
            "So if I enter in the same cluster, they have the same vector here.",
            "So this term here will be 0.",
            "So this expression will have like will be 0.",
            "And if I understand different classes, we will have, say plus one here and minus one here.",
            "So this term here will be plus 1 -- -- 1 which is 2B squared.",
            "It gives 4.",
            "So if there are two points which are different classes, it's we have this factor floor here.",
            "That's why I divide by 104.",
            "And So what did what?",
            "This allows us now is so here we only summed over the pairs of points which are in different clusters.",
            "And now we sum over all pairs.",
            "So OK.",
            "Does everybody see that or OK?",
            "And now is a very elegant nice thing is, of course, we've seen that this is this key property of the graph Laplacian, so this is just F, transpose, LF, some factor in front.",
            "So what we managed is we rewrote the objective function in terms of the graph Laplacian.",
            "Now then, the second nice thing is.",
            "We have this condition now that it should be in be should have the same size Now if you just look at what this means.",
            "If we look at this vector F, it just means that if we sum over all elements of FD, some has to be 0.",
            "And if you write this in a linear algebra way now, this means that the sum of a vector is just the product of the vector with the constant one vector.",
            "This would be 0 and so.",
            "Intuitively, this means that F should be orthogonal to the constant one vector.",
            "OK, so now we can.",
            "We haven't done anything, we just have made observations and we know who write this optimization problem.",
            "So instead of minimizing cut Abby, we simply say we minimize F, transpose LF.",
            "The condition A = B is translated into F is perpendicular to one.",
            "And then we have OK slightly nasty condition here.",
            "Of course this only is the same as above if we if you take the vector.",
            "If I if we did so if you have discrete values here.",
            "Um?",
            "OK, and the editor constraint here, which we will see in the next step.",
            "So I mean in the here above we always have that F is the norm of F is constant is just square root in, so it doesn't hurt us so far.",
            "It's just an additional thing.",
            "OK, now so far nothing has happened.",
            "We just rewrote it and still have a discrete optimization problem because we have these annoying if I hear you know, that's the heuristic approach.",
            "You say OK, too difficult to me.",
            "I can solve a discrete optimization problem, I just relax.",
            "So relaxing always means throwing away conditions, making life very much easier.",
            "So we simply throw away this condition that if I has to be plus or minus one, we say we ignore it.",
            "So what we say is we solve the optimization problem ignoring this condition.",
            "And then we see what happens.",
            "And what actually happens then is now the third nice fact.",
            "So now we have the optimization problem.",
            "Minimize essentially F, transpose, LF, subject to F orthogonal to one, and we fix the norm of F to some value.",
            "It doesn't matter.",
            "And now the third nursing is that the constant one vector actually was the first eigenvector of the graph Laplacian, the smallest one.",
            "So if you wouldn't have this condition if you just minimize F transpose LF, the solution is always the smallest eigenvector.",
            "I mean, that's a throwaway.",
            "I never know how to pronounce it, really.",
            "It's whatever serum.",
            "And we can look at this in the exercises if you don't know it, so it, but it's essentially very simple to see, because if you.",
            "Well, actually I don't go into the proof, but.",
            "Somehow, if you multiply a vector like if you want to minimize a quadratic form, it always the minimum the vector which minimizes it is always the smallest eigenvector.",
            "Now this smallest eigenvector can be because we have this condition that it should be also going out to one and now then this theorem tells you that in this case you need to take the second smallest eigenvector.",
            "So what we see is without doing any more math, we simply see the solution of this problem.",
            "If we forget about this, discreteness condition is just we take F as a second eigenvector of the graph Laplacian.",
            "And OK, then we now have a real valued vector and we but we wanted to have a clustering.",
            "Now we need let's so that we need to sort of inverse this relaxation together.",
            "Discrete solution again.",
            "And what we do is simply we say OK, we just take the sign of the vector.",
            "So maybe the vector the entries here are not plus plus or minus one, but something positive in something negative.",
            "So we just take the sinus cluster indicator vector.",
            "And that's what we then call the clustering.",
            "OK, Yep.",
            "First home.",
            "Actually OK, good question.",
            "We we don't know it at all.",
            "So OK, so that's now the OK. All sounds very nice.",
            "The downside is this relaxation.",
            "You don't have any control what's going to happen.",
            "So this is really a heuristic.",
            "And we will see I don't know whether I have a slide on this, but their examples we can show that this can be completely wrong.",
            "I mean, essentially you don't even know that.",
            "I mean, apart from this condition here, but you don't even know whether the solution after you do this rounding step again, whether it's minimal in for cut a be in any way, it could just be that this rounding that's arbitrary things and turns out to be very bad, so that's really.",
            "So that's the bad part of of this relaxation, so it helps you to solve a problem which is very complicated.",
            "You make a linear problem out of it, but of course there's some price to pay in the prices that you don't know what's going on here.",
            "Science seems like the only works for two.",
            "OK, I will tell you after the break how it works.",
            "For more cluster, OK, let's take a break now."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can start.",
                    "label": 0
                },
                {
                    "sent": "OK so I'm gonna talk about clustering.",
                    "label": 0
                },
                {
                    "sent": "And I figured out that many people, if you already know a bit of clustering, so many people already know what he means is many of you have already implemented it and so on.",
                    "label": 0
                },
                {
                    "sent": "But some of you don't, so I really had to struggle the last week to figure out where I should start with.",
                    "label": 0
                },
                {
                    "sent": "I should start with the boring stuff like he means and single linkage in half a few days off, or whether I should start with something more interesting.",
                    "label": 0
                },
                {
                    "sent": "So I decided for the interesting bit, which means that the boring bit comes later, but it comes a bit shorter.",
                    "label": 0
                },
                {
                    "sent": "Hopefully when the interesting bit, in which also means that the order of my slides is not exactly the one I designed them, so maybe.",
                    "label": 0
                },
                {
                    "sent": "We'll see where.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Everything works out.",
                    "label": 0
                },
                {
                    "sent": "So I mean, this plan is not really up to date, but essentially what I want to do is now I want to start with the advanced algorithms section and because I think so in the algorithm I want to present to you in detail is spectral clustering because I think on the one hand it's a very interesting algorithm and it also shows very many things one has to take care of when one tries to design A clustering algorithms and there are many steps involved and we will go through all of them in detail, then OK. We also have some other algorithms which are partly in this first section which I now will.",
                    "label": 0
                },
                {
                    "sent": "Present maybe in the later today or even tomorrow.",
                    "label": 0
                },
                {
                    "sent": "And then this is the third and 4th lecture for tomorrow.",
                    "label": 0
                },
                {
                    "sent": "Mainly will concentrate on issues which are more from a theoretical side so.",
                    "label": 0
                },
                {
                    "sent": "We want to ask the question what how can we define what clustering is?",
                    "label": 1
                },
                {
                    "sent": "Or can we define it or does it help anything to try to define what clustering is?",
                    "label": 0
                },
                {
                    "sent": "Does it help to look at algorithms from a theoretical point of view?",
                    "label": 0
                },
                {
                    "sent": "What are the questions we would like to look at and so on and that's what I want to talk about tomorrow.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So before we start.",
                    "label": 0
                },
                {
                    "sent": "OK, clustering, most of you know what it is, but the intuition is we're given some objects.",
                    "label": 0
                },
                {
                    "sent": "They can be whatever we want.",
                    "label": 0
                },
                {
                    "sent": "They can be digits.",
                    "label": 0
                },
                {
                    "sent": "There can be images that can be.",
                    "label": 0
                },
                {
                    "sent": "I don't know, Gene expression, microarray data or whatever.",
                    "label": 0
                },
                {
                    "sent": "So we have those objects and what we also have some relations between those objects.",
                    "label": 1
                },
                {
                    "sent": "So what we want to do in the end is we want to say which objects are similar to each other and which objects sort of belong to the same group of objects of which objects belong to different groups somehow.",
                    "label": 1
                },
                {
                    "sent": "And in order to do that we always need some information about the relations between objects.",
                    "label": 0
                },
                {
                    "sent": "And this information can either be we have something like a similarity function or distance function, or it could also be something more abstract like a neighborhood to relationship.",
                    "label": 0
                },
                {
                    "sent": "Certain points are neighbors of other points, but we don't have like a way of quantifying that something like that.",
                    "label": 0
                },
                {
                    "sent": "And then OK, the goal is to find groups in those data points which have something to do with each other and intuitively somehow it's clear what we want to achieve with it.",
                    "label": 0
                },
                {
                    "sent": "But practically, it's often very difficult to really get a grip on what we exactly want to do.",
                    "label": 0
                },
                {
                    "sent": "And one of the reasons is that actually there there might be very many different reasons why you would like to do clustering.",
                    "label": 0
                },
                {
                    "sent": "So the first reason which is I think maybe the older reason is you want just to explore your data.",
                    "label": 0
                },
                {
                    "sent": "So somebody gives you a data set like in Isabel setting.",
                    "label": 1
                },
                {
                    "sent": "Maybe you have a customer and he comes up with this data set and gives it to you.",
                    "label": 0
                },
                {
                    "sent": "You have no clue about the data.",
                    "label": 0
                },
                {
                    "sent": "You've never seen such data before and before you know, start going off with designing the most complicated model to fit some data.",
                    "label": 0
                },
                {
                    "sent": "You might just try to play with your data a bit and try to find whether they're structures in the data.",
                    "label": 1
                },
                {
                    "sent": "Which you should need to know before you go off and do something more complicated.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "OK, and one way you can do this clustering.",
                    "label": 0
                },
                {
                    "sent": "So you just apply clustering algorithms to try to figure out whether their groups in the data which might have something in common and maybe in later analysis you would like to treat those groups differently.",
                    "label": 0
                },
                {
                    "sent": "That might be a goal so, but here it's really more.",
                    "label": 0
                },
                {
                    "sent": "It's really playing around, it's not, you don't have a.",
                    "label": 0
                },
                {
                    "sent": "Like an objective goal, like minimizing the squared error loss, whatever.",
                    "label": 0
                },
                {
                    "sent": "It's just, it's more fuzzy somehow.",
                    "label": 0
                },
                {
                    "sent": "The second goal, which is very, very important and becomes even more important, is the data amount.",
                    "label": 0
                },
                {
                    "sent": "Gross is that often you have so many data points that you simply can't run an algorithm on your 5 million data points, so you need to do something about it and what people often do is they first of all cluster the data points.",
                    "label": 0
                },
                {
                    "sent": "To some groups and then they try to run the algorithm on the clusters instead.",
                    "label": 0
                },
                {
                    "sent": "So instead of running it on 5 million data points, we maybe say we generate it on 1000 classes and then then we try to analysis to analyze those clusters.",
                    "label": 0
                },
                {
                    "sent": "OK, so and you can already see those two reasons might be very different from each other, so you really want to achieve different things with both both goals.",
                    "label": 0
                },
                {
                    "sent": "But somehow both is called clustering and that's already part of the confusion.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here I just want to start off with some pictures before we go into an algorithm.",
                    "label": 0
                },
                {
                    "sent": "So this is gene expression data and maybe many of you have already seen it.",
                    "label": 1
                },
                {
                    "sent": "So here and there.",
                    "label": 0
                },
                {
                    "sent": "So on the left side you see something like.",
                    "label": 0
                },
                {
                    "sent": "So it's a gene expression.",
                    "label": 0
                },
                {
                    "sent": "Data is something from bioinformatics where you want to measure whether certain genes are expressed in a cell or not.",
                    "label": 0
                },
                {
                    "sent": "And you do this because you want to find out.",
                    "label": 0
                },
                {
                    "sent": "I don't know which genetic reasons there might be that a certain cell does something or whatever.",
                    "label": 0
                },
                {
                    "sent": "I'm not a biologist, so.",
                    "label": 0
                },
                {
                    "sent": "So, so the raw data.",
                    "label": 0
                },
                {
                    "sent": "You get something or it's actually already pre processed.",
                    "label": 0
                },
                {
                    "sent": "The process that we get is like a matrix which contains the genes in the call in the rows in different conditions in the columns.",
                    "label": 0
                },
                {
                    "sent": "And now you might want to figure out at a certain type of cell always that there are certain genes which are active in the cell.",
                    "label": 0
                },
                {
                    "sent": "If you cluster the data ideally and if you believe that the clustering result in something useful, you should get something like this where you I don't have all the express jeans on top and all the non express jeans on.",
                    "label": 0
                },
                {
                    "sent": "Bottom and then you can give this to a biologist and he can tell you whether this might have some meaning or not.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So another area which I find quite fascinating is Network analysis.",
                    "label": 0
                },
                {
                    "sent": "So there very many datasets which are not just numbers like in teen expression.",
                    "label": 0
                },
                {
                    "sent": "In the end you have numbers.",
                    "label": 0
                },
                {
                    "sent": "It's a vector, but very many datasets are in form of a graph directly.",
                    "label": 0
                },
                {
                    "sent": "So if you want to, I don't know, you look at the Internet into 12222 data mining on some web pages.",
                    "label": 0
                },
                {
                    "sent": "They always have the structure of a graph because you have the links between the web pages so.",
                    "label": 0
                },
                {
                    "sent": "And maybe now you want to figure out.",
                    "label": 0
                },
                {
                    "sent": "I don't know which pages are related to which other pages.",
                    "label": 0
                },
                {
                    "sent": "And here is.",
                    "label": 0
                },
                {
                    "sent": "This is an example for a social network where you it's from a company they collected who writes email to which other person in this company and you just make a graph out of it and maybe you want to know.",
                    "label": 0
                },
                {
                    "sent": "I don't know which are the groups of people who communicate with each other which are groups which should more communicate with each other but actually they don't.",
                    "label": 0
                },
                {
                    "sent": "So you might want to find classes in such a graph.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, an example which I think many people if you are working on this image segmentation where the goal is a bit.",
                    "label": 0
                },
                {
                    "sent": "I mean what you want to do essentially is you are given an image and you want to do a very rough segmentation, which just I don't know segments background from foreground or something detects very very large structure in your data set and then you might go and analyze it a bit deeper.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the last example I want to show.",
                    "label": 0
                },
                {
                    "sent": "Is hierarchical clustering, so the ones we show I showed before it, you always have a data set and then you say you want to find.",
                    "label": 0
                },
                {
                    "sent": "I don't know five clusters.",
                    "label": 0
                },
                {
                    "sent": "Here we have something where we not only want to find like a certain number of clusters, but we want to find a hierarchy of clusters, and the example here is told from some paper here is you want to you want to classify mammals in different groups and what you then get to such an evolutionary tree which maybe shows that I don't know.",
                    "label": 0
                },
                {
                    "sent": "Red and mouse are very much related to each other there or cluster on their small cluster.",
                    "label": 0
                },
                {
                    "sent": "But then on a very high scale there also related to primates.",
                    "label": 0
                },
                {
                    "sent": "But sort of the the distance between the primates to the Reds is pretty large, so it's or it's a, it's a hierarchy, so so.",
                    "label": 0
                },
                {
                    "sent": "All of them are mammals and then you can split them into different groups, and that's called hierarchical clustering.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK um.",
                    "label": 0
                },
                {
                    "sent": "So, and that's not the part I want to skip.",
                    "label": 0
                },
                {
                    "sent": "Because I just want to start with something else.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the algorithm I want to start with now is called spectral clustering.",
                    "label": 1
                },
                {
                    "sent": "It's an algorithm.",
                    "label": 0
                },
                {
                    "sent": "Which does not do hierarchical clustering.",
                    "label": 0
                },
                {
                    "sent": "So you tell the algorithm, give me 5 clusters, and then hopefully it gives you 5 or it will give you 5 classes and whether they're useful.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Not we will see.",
                    "label": 0
                },
                {
                    "sent": "So I want to go into detail to this algorithm, and before we do this I want to give a rough overview that you know where in which step you are so.",
                    "label": 0
                },
                {
                    "sent": "Very rough idea of spectral clustering is the following.",
                    "label": 1
                },
                {
                    "sent": "So OK, we start with data points as usual and we assume that we have similarities, so similarity means so that slows as IJ we have over there.",
                    "label": 0
                },
                {
                    "sent": "So similarity means a similarity between two points is high.",
                    "label": 0
                },
                {
                    "sent": "If the objects are very close to each other.",
                    "label": 0
                },
                {
                    "sent": "If they were related to each other, so they hire.",
                    "label": 0
                },
                {
                    "sent": "This value is the more the more.",
                    "label": 0
                },
                {
                    "sent": "I believe those points should be in the same cluster.",
                    "label": 1
                },
                {
                    "sent": "Now what I do is I want to represent the state in a in a in a convenient form, but I don't have coordinates, so I don't assume that those points are in Rd or something like this.",
                    "label": 0
                },
                {
                    "sent": "It can be anything and I just have any similarity function and then an abstract way of representing your data will be to build some similarity graph and we will see different ways of doing that.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So what we do is we just say each vertex of this graph represents one data point and then we connect the vertices which are similar to each other.",
                    "label": 0
                },
                {
                    "sent": "So we could say.",
                    "label": 0
                },
                {
                    "sent": "And then we put an edge between between the two vertices.",
                    "label": 0
                },
                {
                    "sent": "So here we have.",
                    "label": 0
                },
                {
                    "sent": "I don't know point 1.2 and they are similar to each other, so we put an edge.",
                    "label": 0
                },
                {
                    "sent": "And Additionally what we do is we wait the edge by the similarity.",
                    "label": 0
                },
                {
                    "sent": "So if points are very similar to each other, we put a like I always what I plot here is always a shortage, so they're very related to each other.",
                    "label": 0
                },
                {
                    "sent": "They are close.",
                    "label": 0
                },
                {
                    "sent": "And it carries the weight as I J.",
                    "label": 0
                },
                {
                    "sent": "And here they are.",
                    "label": 0
                },
                {
                    "sent": "They maybe have a positive similarity, but it's so here.",
                    "label": 0
                },
                {
                    "sent": "Maybe we have similarity 0.9 and here we have 0.1 so it just is a graphic way of showing they're not as similar as the other tools.",
                    "label": 0
                },
                {
                    "sent": "And then OK, now we have this graph and the goal of clustering is then sort of to find the cut in this graph such that.",
                    "label": 0
                },
                {
                    "sent": "Such that the resulting things are clusters and we will define how this can be done.",
                    "label": 0
                },
                {
                    "sent": "And so that's the basic scheme, just to keep in back of your mind.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "OK, because now we need to start with some notation.",
                    "label": 0
                },
                {
                    "sent": "So OK, I already said this, so we need a similarity function and we will always put the value.",
                    "label": 0
                },
                {
                    "sent": "The similarity values between the data points in a matrix, which is called capital S. So that's a similarity matrix.",
                    "label": 0
                },
                {
                    "sent": "Then we need to know what the degree of a vertex is.",
                    "label": 0
                },
                {
                    "sent": "So in a graph, the degree of a vertex is just the sum of the edge weights of the edges which are attached to this vertex.",
                    "label": 1
                },
                {
                    "sent": "So it simply shows you.",
                    "label": 0
                },
                {
                    "sent": "So you sum the similarities of all data points which are close to this vertex which are attached to this vertex.",
                    "label": 0
                },
                {
                    "sent": "That's the degree of a vertex.",
                    "label": 1
                },
                {
                    "sent": "And then we will put those into a matrix which we call degree matrix.",
                    "label": 0
                },
                {
                    "sent": "It's a diagonal matrix and we just put the different degrees on the diagonal.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, and then what?",
                    "label": 0
                },
                {
                    "sent": "We will also need later is we need to measure how large different parts of of of a graph are.",
                    "label": 0
                },
                {
                    "sent": "So if we say later on, for example, we would assume this guy should be a cluster.",
                    "label": 0
                },
                {
                    "sent": "We will need to know how large this cluster is.",
                    "label": 0
                },
                {
                    "sent": "And there are two different, generally two different ways how you are too.",
                    "label": 0
                },
                {
                    "sent": "I don't know if you think about how you would measure the size of a cluster.",
                    "label": 0
                },
                {
                    "sent": "There are two ways you could come up with, so either you simply count how many vertices are in this cluster.",
                    "label": 0
                },
                {
                    "sent": "That's what I do note by absolute value of a.",
                    "label": 0
                },
                {
                    "sent": "So if this is our group, the absolute value is just five.",
                    "label": 0
                },
                {
                    "sent": "Or we can do which is called the volume of a instead of counting the vertices, we simply count the edges.",
                    "label": 1
                },
                {
                    "sent": "So what we do is forward vertices which are in the subset A.",
                    "label": 0
                },
                {
                    "sent": "We sum up the weights of all edges which are attached to those vertices.",
                    "label": 0
                },
                {
                    "sent": "Those aren't connected to each other, so it means that we don't know the similarities in those points.",
                    "label": 0
                },
                {
                    "sent": "No, it means it means, well, OK.",
                    "label": 0
                },
                {
                    "sent": "It means that the similarity zero essentially so and do and I will.",
                    "label": 0
                },
                {
                    "sent": "I will tell later later on how we achieve that the similarity is here.",
                    "label": 1
                },
                {
                    "sent": "So it's essentially what I plot.",
                    "label": 0
                },
                {
                    "sent": "Here is a sparse similarity matrix, so you know that certain points, for example, what you can do is you just computer Gaussian kernel matrix on your data, But then you Only Connect points where the similarity is higher than 0.9 and the other ones you set to 0.",
                    "label": 0
                },
                {
                    "sent": "So it's essentially if there's no edge, we really assume that there is a similarity 0.",
                    "label": 0
                },
                {
                    "sent": "However, we achieve that later on.",
                    "label": 0
                },
                {
                    "sent": "OK. Anymore questions so far about the notation.",
                    "label": 0
                },
                {
                    "sent": "OK, so you need to.",
                    "label": 0
                },
                {
                    "sent": "You need to have the intuition what this means, because if you don't know it then you will get lost from the next slide.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, and one more thing, which you probably can't read because it's just on the bottom of the screen.",
                    "label": 0
                },
                {
                    "sent": "So assume you have such a graph, and later on we want I mean clustering.",
                    "label": 0
                },
                {
                    "sent": "In the end will be we want to give labels to data points, whether they're in cluster one or in cluster two and Cluster 5.",
                    "label": 0
                },
                {
                    "sent": "So we want to attach vertices to those data points more or less in each vertex get the get the label of its corresponding cluster.",
                    "label": 0
                },
                {
                    "sent": "And the way you can express this now, as you can just write the vertices as a vector, so iffy, meaning the clustering the index of the clustering of the East data point, and we will often right.",
                    "label": 0
                },
                {
                    "sent": "So that's why vectors in my notation often called F, because essentially they are the function values.",
                    "label": 0
                },
                {
                    "sent": "So if we have a vector of N data points in this setting, we usually mean this vector contains the entries which would show you in which cluster the data point should be.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now the standard object we will always use in spectral clustering is the graph Laplacian.",
                    "label": 0
                },
                {
                    "sent": "And actually, I just think.",
                    "label": 0
                },
                {
                    "sent": "OK, let's leave it.",
                    "label": 0
                },
                {
                    "sent": "So the graph Laplacian, it's a matrix and.",
                    "label": 0
                },
                {
                    "sent": "So given a graph, you can define this matrix which is called graph Laplacian and it is simply the difference between the degree matrix and the similarity matrix.",
                    "label": 0
                },
                {
                    "sent": "OK, just take it as a definition for now.",
                    "label": 0
                },
                {
                    "sent": "We will see why why this makes sense later on.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And the most important property of this matrix is the one which I have on this slide, so everything else is what we will do is is a consequence of this property, and the property is that if you take some vector F in our end and two multiplied from left and rights to your matrix, what you will get is the expression you see down here.",
                    "label": 0
                },
                {
                    "sent": "So it is essentially you some overall data points, so all pairs of data points the sun runs over inj.",
                    "label": 0
                },
                {
                    "sent": "So for all pairs of data points you some.",
                    "label": 0
                },
                {
                    "sent": "The similarity times the difference of the function value squared.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "What's the question?",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "So give me oh there.",
                    "label": 0
                },
                {
                    "sent": "Oh this actually here sorry.",
                    "label": 0
                },
                {
                    "sent": "Let's attack was done when it happened.",
                    "label": 0
                },
                {
                    "sent": "Sorry, so forget about this L here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so maybe I write the definitions again that you don't lose it.",
                    "label": 0
                },
                {
                    "sent": "So the is the matrix which contains the degrees on the diagonal and otherwise is zero and S is just a similarity matrix which contains the entries of your similarity function.",
                    "label": 0
                },
                {
                    "sent": "And OK, I'll write it again.",
                    "label": 0
                },
                {
                    "sent": "The degree which you have in this matrix DI is some over JSIJ.",
                    "label": 0
                },
                {
                    "sent": "It's the sum of.",
                    "label": 0
                },
                {
                    "sent": "The sum of the similarities to other data points.",
                    "label": 0
                },
                {
                    "sent": "Yes, so if I so F is now that's what I said on my previous slide.",
                    "label": 0
                },
                {
                    "sent": "So F is A is a vector and then.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Really, the notation can be confusing when you come from linear algebra, so F is really this vector.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh OK, but but for this equation it doesn't need to be discreet, so this holds for any arbitrary vector in R in and it's actually very simple to see, so you just.",
                    "label": 0
                },
                {
                    "sent": "It's really, it's not more than then there is on the slide.",
                    "label": 0
                },
                {
                    "sent": "So you start by putting so Ellis just D -- S. So if you forget about this L here, you just multiply it out.",
                    "label": 0
                },
                {
                    "sent": "Then you plug in the definition, so decide diagonal matrix.",
                    "label": 0
                },
                {
                    "sent": "So if you multiply is a vector to the diagonal matrix, you will get this expression here, which is just a sum over DFI squared.",
                    "label": 0
                },
                {
                    "sent": "And this is just a quadratic form corresponding to S and now essentially you just do some clever.",
                    "label": 0
                },
                {
                    "sent": "I mean you observe that the I is actually the sum over SJ.",
                    "label": 0
                },
                {
                    "sent": "You sort of rearrange the formula in fewer, more less than.",
                    "label": 0
                },
                {
                    "sent": "I mean it takes 10 minutes to think about it, but essentially it's really not more than it's on this slide.",
                    "label": 0
                },
                {
                    "sent": "OK, so why is this property actually helpful?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "We already see this if we look, and that's actually also explains why we call this in graph Laplacian.",
                    "label": 0
                },
                {
                    "sent": "So this property in some way measures the deviation of a function or how.",
                    "label": 0
                },
                {
                    "sent": "How much variation a certain function is.",
                    "label": 0
                },
                {
                    "sent": "So if you just take some graph.",
                    "label": 0
                },
                {
                    "sent": "And now we look at what this expression does.",
                    "label": 0
                },
                {
                    "sent": "So assume we attach some function values so we have equal say plus one here, minus one here, minus 10 here.",
                    "label": 0
                },
                {
                    "sent": "Well, actually we're doing clustering, so call it 2 here.",
                    "label": 0
                },
                {
                    "sent": "Two here, and maybe this is in cluster 10, so you will have all those cluster labels.",
                    "label": 0
                },
                {
                    "sent": "And if you now look at this expression.",
                    "label": 0
                },
                {
                    "sent": "You look at all two pairs of points at all pairs of points and you sum the difference between the function values weighted by the similarity.",
                    "label": 0
                },
                {
                    "sent": "So this means that.",
                    "label": 0
                },
                {
                    "sent": "Somehow and later on we will minimize this quantity.",
                    "label": 0
                },
                {
                    "sent": "So if you minimize this quantity, this will mean that either this essay is a very small number and then it doesn't matter what the function does.",
                    "label": 0
                },
                {
                    "sent": "So if points are very far apart or are not similar at all to each other, we don't punish if the function values are different from each other.",
                    "label": 0
                },
                {
                    "sent": "But if if the function values have side is very large, which means that points are very close to each other, then.",
                    "label": 1
                },
                {
                    "sent": "We then if we want this whole expression to be smaller, we need that the function values are very very close to each other in a certain sense.",
                    "label": 0
                },
                {
                    "sent": "And so intuitively, what this expression does is it takes something like a computing variation or a second derivative.",
                    "label": 0
                },
                {
                    "sent": "So if you know that's very handwaving, it's just to give you an intuition why we call it this way.",
                    "label": 0
                },
                {
                    "sent": "So often people think so.",
                    "label": 0
                },
                {
                    "sent": "Similarities is something like an inverse distance.",
                    "label": 0
                },
                {
                    "sent": "So distance is small if points are very similar to each other.",
                    "label": 0
                },
                {
                    "sent": "So if you want to get from a similarity to a distance, you can just say you take the inverse.",
                    "label": 0
                },
                {
                    "sent": "And now if you replace this AJ here by the inverse distance and conveniently we squared OK. Then this is something like a difference quotient, so you have the.",
                    "label": 0
                },
                {
                    "sent": "We do have it so you have the the distance between FIFJ and then you you divide by DJ so it looks like like if you take a derivative so in the derivative you always have something like F of X I -- F of X J / X and Y -- X Ray and then you take this the limit to Infinity and that's what is essentially the expression here and now you can if you want to make this formula.",
                    "label": 0
                },
                {
                    "sent": "Then somehow if you sum sum squares of derivatives it looks like so if you take the sum is an integral.",
                    "label": 0
                },
                {
                    "sent": "A derivative is this, not the operator.",
                    "label": 1
                },
                {
                    "sent": "Then it's somehow like to Laplace operator on argue.",
                    "label": 0
                },
                {
                    "sent": "So if you don't know the Laplace operator forgetting about, but it's just.",
                    "label": 0
                },
                {
                    "sent": "This this quadratic form measures the variation of a graph of a function of the graph.",
                    "label": 1
                },
                {
                    "sent": "Contact three clusters so we have like 3 discrete values or F right so?",
                    "label": 0
                },
                {
                    "sent": "Assume that the labels have some particular order.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "But type of thing that sits here it doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I agree so, OK, so let's come to this point later.",
                    "label": 0
                },
                {
                    "sent": "So when I introduce this algorithm in the following, I will.",
                    "label": 0
                },
                {
                    "sent": "I will always talk about the case, we just look at two clusters and there doesn't matter because you have either it's the same or it's not the same if you do something with more classes you actually don't only look at one vector, you do some similar.",
                    "label": 0
                },
                {
                    "sent": "Trick is a better I think.",
                    "label": 0
                },
                {
                    "sent": "Already told you you discretize by just like you introduce a vector which has three components and you set it to one of the clusters in this.",
                    "label": 0
                },
                {
                    "sent": "One setting, but let's worry about that later, but it's true, it's yeah so.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, let's continue.",
                    "label": 0
                },
                {
                    "sent": "So now we've seen.",
                    "label": 0
                },
                {
                    "sent": "So the definition of Li write it again here.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "We've seen the key property I go back maybe.",
                    "label": 0
                },
                {
                    "sent": "But if we multiply a vector to L then we get this outcome.",
                    "label": 0
                },
                {
                    "sent": "Now what is the first thing which which tells us is?",
                    "label": 0
                },
                {
                    "sent": "So if we assume that the similarities are non negative and that's something one usually always assumes, like if you have a Gaussian kernel whatever the case will never be nonnegative, will never be negative.",
                    "label": 0
                },
                {
                    "sent": "So then this expression is always non negative because we have something which is not negative here and here we are something.",
                    "label": 0
                },
                {
                    "sent": "So this whole thing will never be negative, which means and we do this for all vectors F that the matrix L is positive semidefinite.",
                    "label": 1
                },
                {
                    "sent": "That's just the definition.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's already a very important observation.",
                    "label": 0
                },
                {
                    "sent": "The second observation is OK. Now we know it's positive semidefinite, so so we know all eigenvalues will be either 0 or larger.",
                    "label": 0
                },
                {
                    "sent": "And now it turns out actually 0 isn't eigenvalue of this matrix.",
                    "label": 0
                },
                {
                    "sent": "And the corresponding eigenvectors are constant one vector.",
                    "label": 0
                },
                {
                    "sent": "And this.",
                    "label": 0
                },
                {
                    "sent": "I mean once you know it, it's obvious because somehow if you.",
                    "label": 0
                },
                {
                    "sent": "Oops.",
                    "label": 0
                },
                {
                    "sent": "Because if you multiply the constant one vector to the Laplacian, it's just D 1 -- S one and S1.",
                    "label": 0
                },
                {
                    "sent": "I mean D contains the row sums of the matrix S. So here by take multiplying the one vector could take the row sums and you already have their awesome.",
                    "label": 0
                },
                {
                    "sent": "So if you take the difference, it will be 0.",
                    "label": 0
                },
                {
                    "sent": "So think about it for a few minutes, but it's pretty obvious that this thing has to be 0.",
                    "label": 0
                },
                {
                    "sent": "So what we know is we know all eigenvalues are non negative.",
                    "label": 0
                },
                {
                    "sent": "So if we have a smaller so the smallest angle we can have zero and actually we have it.",
                    "label": 0
                },
                {
                    "sent": "So we have this angle is zero and the corresponding eigenvector is a constant one vector.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Now that's actually a very very nice relation already between the eigenvectors of the graph.",
                    "label": 1
                },
                {
                    "sent": "Which of angle is zero, and the number of clusters, and the relation is as follows.",
                    "label": 0
                },
                {
                    "sent": "So of course the matrix equative the eigenvalue zero several times, so it could be that the spaces just does not only contain one vector, but contains several vectors.",
                    "label": 0
                },
                {
                    "sent": "And what time is out now?",
                    "label": 0
                },
                {
                    "sent": "Is that this actually only happens in the situation where your graph contains of different disconnected parts.",
                    "label": 0
                },
                {
                    "sent": "So if our graph looks like this, so we have two parts and there are no connections between each other.",
                    "label": 0
                },
                {
                    "sent": "Then one can prove that the multiplicity of the eigenvalue zero will be exactly 2.",
                    "label": 0
                },
                {
                    "sent": "Because we have two connected components and the eigenvectors which correspond to this to those two parts are essentially like the indicator vectors of the parts.",
                    "label": 0
                },
                {
                    "sent": "So we have one of the vertices are ordered.",
                    "label": 0
                },
                {
                    "sent": "This is the first group in this the second group with one in Vector which looks like this and the second one looks like this.",
                    "label": 0
                },
                {
                    "sent": "And those two eigenvectors spend the space of angle is 0.",
                    "label": 0
                },
                {
                    "sent": "And actually, that already shows that the small eigenvectors of image of the Laplace matrix might have something to do with the number with the clustering, because we see that if we would look at the first 2 eigenvectors of this matrix, it is they really contain the cluster indicator vectors of the two groups in our data.",
                    "label": 0
                },
                {
                    "sent": "And of course there's only holds if there is no noise, so if you have an edge here, it gets more complicated, and that's what spectral clustering is about.",
                    "label": 0
                },
                {
                    "sent": "But that's really the intuition.",
                    "label": 0
                },
                {
                    "sent": "Or that's a very important observation.",
                    "label": 0
                },
                {
                    "sent": "The nice thing about this statement here is that the proof is actually very simple and very elegant, and I like it a lot and we will do it in the exercise.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so um.",
                    "label": 0
                },
                {
                    "sent": "The Matrix LV IF looked at so far, is what is called the unnormalized Laplacian.",
                    "label": 0
                },
                {
                    "sent": "So it's one of the word unnormalized so far because we didn't have any normalized ones.",
                    "label": 0
                },
                {
                    "sent": "But now we introduced to other versions of Laplacians which are normalized graph laplacians so this will be the unnormalized version and essentially we will see that later we will need mainly this one.",
                    "label": 1
                },
                {
                    "sent": "So the normalization what it essentially does is it normalizes the matrix such that the eigenvectors are the eigenvalues are always in the same range, which is not the case for the for this unnormalized application and we will mainly work with this matrix.",
                    "label": 0
                },
                {
                    "sent": "Well, we normalized by multiplying the inverse of death to the front of L. And if you multiply it all out so you have L and now you put the to the minus one in front, then they hear it cancels, so that gets the identity matrix.",
                    "label": 0
                },
                {
                    "sent": "Here and here we have the data, the minus 1 * S, so that's the part here.",
                    "label": 0
                },
                {
                    "sent": "And the reason why I put this index are we?",
                    "label": 1
                },
                {
                    "sent": "It's because it's so it stands for random Walk.",
                    "label": 0
                },
                {
                    "sent": "So for those of you who know what the random walk on a graph is, this is the transition matrix of the random walk on the graph.",
                    "label": 0
                },
                {
                    "sent": "So who actually knows what the random walk is?",
                    "label": 0
                },
                {
                    "sent": "OK, I'll go into that later.",
                    "label": 0
                },
                {
                    "sent": "So for those who know that's the relation for the others will see.",
                    "label": 0
                },
                {
                    "sent": "OK, there's also another normalization, but I don't want to talk about this.",
                    "label": 0
                },
                {
                    "sent": "Actually, in the lecture, just take it.",
                    "label": 0
                },
                {
                    "sent": "There's another normalization so we don't.",
                    "label": 0
                },
                {
                    "sent": "Put the D -- 1 in front but we multiply from both sides we with due to the minus 1/2 and the advantages of this matrix is still symmetric while the matrix here is not symmetric but it forget it, it's not important.",
                    "label": 0
                },
                {
                    "sent": "So what is important is that the properties of this random of both matrices are actually very close to the other one, so also both are positive semidefinite in both of the properties that like this property about the number of clusters corresponds to the to the multiplicity of the first eigenvalue value.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. OK, now I want to introduce.",
                    "label": 0
                },
                {
                    "sent": "I want to show oh sorry.",
                    "label": 0
                },
                {
                    "sent": "Definition of.",
                    "label": 0
                },
                {
                    "sent": "The rationality.",
                    "label": 0
                },
                {
                    "sent": "OK, so I think the that's a very good question so.",
                    "label": 0
                },
                {
                    "sent": "Maybe we'll see that later.",
                    "label": 0
                },
                {
                    "sent": "I mean, I will show later on why this I mean.",
                    "label": 0
                },
                {
                    "sent": "Right now.",
                    "label": 0
                },
                {
                    "sent": "It's a mystery, I know, so I will try to solve the mystery later on.",
                    "label": 0
                },
                {
                    "sent": "I think the most important point is really this property we had.",
                    "label": 0
                },
                {
                    "sent": "So this one here, F transpose LF equals sum SIJ if I -- F J squared because somehow it is a.",
                    "label": 0
                },
                {
                    "sent": "It is so in machine learning.",
                    "label": 0
                },
                {
                    "sent": "It is often seen as a regularizer, big because it simply punishes if a function on a graph varies a lot on parts where the graph is closely connected.",
                    "label": 0
                },
                {
                    "sent": "I mean, there are very many.",
                    "label": 0
                },
                {
                    "sent": "I mean there's a whole field called spectral graph theory which does nothing but look at different matrices and how those properties of the matrices are related to the graph.",
                    "label": 0
                },
                {
                    "sent": "And often this helps you, because then you can use linear algebra.",
                    "label": 0
                },
                {
                    "sent": "Things to for example, compute how many comic connected component your graph has, or it's just a different view of looking at graphs.",
                    "label": 0
                },
                {
                    "sent": "I think, and this one is just a matrix which turns out somehow to be very convenient, but it's nothing you can see on the.",
                    "label": 0
                },
                {
                    "sent": "It's not obvious on the first glance, it's really if you go deeper into it.",
                    "label": 0
                },
                {
                    "sent": "More questions so far.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Then I will introduce the algorithm and it's nothing you don't need to understand why it works right now.",
                    "label": 0
                },
                {
                    "sent": "You just need to understand how it works right the way it comes later.",
                    "label": 0
                },
                {
                    "sent": "So what we do is so we're given our data.",
                    "label": 0
                },
                {
                    "sent": "And the and we are given the similarity matrix, so OK, so we give our that we built the similarity graph in.",
                    "label": 0
                },
                {
                    "sent": "I mean I didn't really go into detail how to do that, but essentially we connect points which are similar to each other.",
                    "label": 0
                },
                {
                    "sent": "And now the the whole miracle of spectral clustering is stepping.",
                    "label": 0
                },
                {
                    "sent": "Only look at eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "So there are two versions of spectral clustering, which I call unnormalized and normalized spectral clustering is just different flavors so.",
                    "label": 1
                },
                {
                    "sent": "Let's just talk about the anomalous version.",
                    "label": 0
                },
                {
                    "sent": "What we do is we compute the graph Laplacian.",
                    "label": 1
                },
                {
                    "sent": "We compute the first OK. Then we compute the iron vectors of this graph Laplacian.",
                    "label": 0
                },
                {
                    "sent": "We stuck it in the Matrix V, so you have the matrix V which just contains the 1st.",
                    "label": 0
                },
                {
                    "sent": "So say all eigenvalues as columns, right?",
                    "label": 0
                },
                {
                    "sent": "Oh, I'm vector sorry.",
                    "label": 0
                },
                {
                    "sent": "And now.",
                    "label": 0
                },
                {
                    "sent": "Actually what we do is if you want to find K clusters and say for the simplicity case equal to two, we only look at the first 2 eigenvectors here, so.",
                    "label": 0
                },
                {
                    "sent": "What we're going to do is we just look at the first 2 eigenvectors, stick them in this matrix so it's A N * 2 matrix right now.",
                    "label": 0
                },
                {
                    "sent": "That's this matrix V. And now we're going to interpret the role.",
                    "label": 0
                },
                {
                    "sent": "So it's it's a bit mysterious.",
                    "label": 0
                },
                {
                    "sent": "We're going to interpret the Rose of this matrix.",
                    "label": 0
                },
                {
                    "sent": "Is new data points, so we say.",
                    "label": 0
                },
                {
                    "sent": "So here we have, say, entry ABC DEG whatever.",
                    "label": 0
                },
                {
                    "sent": "So the first data point we just take the rule.",
                    "label": 0
                },
                {
                    "sent": "But just as a D. So we say we map the first of all of our data point now to a 2 dimensional representation which contains those two points.",
                    "label": 0
                },
                {
                    "sent": "So we say the first data point.",
                    "label": 0
                },
                {
                    "sent": "Which is just some abstract object or image or whatever is now mapped into a vector in R2 in this case.",
                    "label": 0
                },
                {
                    "sent": "So it's just a way of getting a completely new representation of your data, and it's you don't need to understand why.",
                    "label": 0
                },
                {
                    "sent": "You just need to understand how.",
                    "label": 0
                },
                {
                    "sent": "So we interpret the Rose as new data points at Eve.",
                    "label": 1
                },
                {
                    "sent": "And now we have those new data points.",
                    "label": 0
                },
                {
                    "sent": "There are no points in R2, so now we say OK, great.",
                    "label": 0
                },
                {
                    "sent": "Now we're not who we can cluster them and we just use K means to do that.",
                    "label": 0
                },
                {
                    "sent": "And of course, I mean it's not obvious at all why this should buy you something.",
                    "label": 0
                },
                {
                    "sent": "I mean you.",
                    "label": 0
                },
                {
                    "sent": "You just do it right, but that's how spectral clustering works.",
                    "label": 0
                },
                {
                    "sent": "So it's really a very, very simple algorithm to compute your graph Laplacian.",
                    "label": 0
                },
                {
                    "sent": "You compute the eigenvectors you map them into this real space, and then you K means or any other simple clustering algorithms on this representation.",
                    "label": 0
                },
                {
                    "sent": "And now you can do this in two ways.",
                    "label": 0
                },
                {
                    "sent": "You can do it for the with the normalized or the unnormalized Laplacian.",
                    "label": 0
                },
                {
                    "sent": "And depending on which one you take a call, it normalized run normalized spectral clustering.",
                    "label": 0
                },
                {
                    "sent": "That doesn't mean yes.",
                    "label": 0
                },
                {
                    "sent": "Actually, it does matter later on, because if you don't normalize them.",
                    "label": 0
                },
                {
                    "sent": "Camins might get to do.",
                    "label": 0
                },
                {
                    "sent": "It might have different scales on different coordinate somehow.",
                    "label": 0
                },
                {
                    "sent": "I mean usually we here like this.",
                    "label": 0
                },
                {
                    "sent": "We take the normalized eigenvectors here.",
                    "label": 0
                },
                {
                    "sent": "OK, so maybe I just show how it works.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Write on an example.",
                    "label": 0
                },
                {
                    "sent": "So here we have a toilet I sent this week.",
                    "label": 0
                },
                {
                    "sent": "About this week Gaussians in are two very simple data set.",
                    "label": 1
                },
                {
                    "sent": "What we do is we take the similarity function.",
                    "label": 1
                },
                {
                    "sent": "And for convenience, that's something.",
                    "label": 0
                },
                {
                    "sent": "I mean most people do is they simply use the Gaussian kernel so as to compute the similarity between two of your data points.",
                    "label": 0
                },
                {
                    "sent": "You just use this expression and OK, there's a parameter Sigma to set.",
                    "label": 0
                },
                {
                    "sent": "We don't talk about how to do that now.",
                    "label": 0
                },
                {
                    "sent": "Just set it in some way.",
                    "label": 0
                },
                {
                    "sent": "And then we simply take the graph.",
                    "label": 0
                },
                {
                    "sent": "The similarity matrix of the graph is like the similarity matrix as the adjacency matrix of our graph.",
                    "label": 0
                },
                {
                    "sent": "So essentially it's a completely connected graph, but we will have, like the weights will be very different on the edges, so there will be some edges like the edge which connects this in this point will have weighed approximately 0 because it causing Kernel says they are not similar at all and the weights here will be pretty large.",
                    "label": 0
                },
                {
                    "sent": "OK, and now we say we don't know how many clusters there are, we say.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Explore a few number of clusters.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we compute the eigenvectors and that's this matrix V here which we have here.",
                    "label": 0
                },
                {
                    "sent": "And because I said we want to compute, I don't know for a five class to say we compute the first 5 eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "Now they're just a columns of numbers, right?",
                    "label": 0
                },
                {
                    "sent": "What we can see here.",
                    "label": 0
                },
                {
                    "sent": "The first one?",
                    "label": 0
                },
                {
                    "sent": "That's the one, the constant one eigenvector we always had, so I said the eigenvector of eigenvalue zero is always the constant one vector.",
                    "label": 0
                },
                {
                    "sent": "And now as we normalize it to have norm one, it's not.",
                    "label": 0
                },
                {
                    "sent": "It doesn't, interest doesn't have entries one, but just in this case some like 0.1 something, but it's constant.",
                    "label": 0
                },
                {
                    "sent": "Then if we look at the second one.",
                    "label": 0
                },
                {
                    "sent": "We actually see that there are quite a lot of numbers which are around.",
                    "label": 0
                },
                {
                    "sent": "Minus Zero point 14.",
                    "label": 0
                },
                {
                    "sent": "So here we have three.",
                    "label": 0
                },
                {
                    "sent": "Here we have one for you.",
                    "label": 0
                },
                {
                    "sent": "Probably can't read it, but believe me, so there are some numbers which are more or less look very similar and the other numbers are always something 0.0 tiny positive thing.",
                    "label": 0
                },
                {
                    "sent": "OK, so this means that this vector more or less contains I mean more or less two different numbers.",
                    "label": 0
                },
                {
                    "sent": "And that's why it's why it's going to be helpful later on.",
                    "label": 0
                },
                {
                    "sent": "And OK, it gets more complicated the more eigenvectors you take.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "So now here I have a plot of those eigenvectors in a different interpretation, so.",
                    "label": 1
                },
                {
                    "sent": "OK, ignore the first row.",
                    "label": 0
                },
                {
                    "sent": "Look at the 2nd row.",
                    "label": 0
                },
                {
                    "sent": "What I plot here is so the 1st.",
                    "label": 0
                },
                {
                    "sent": "No, look at the first draw.",
                    "label": 0
                },
                {
                    "sent": "Ignore the 2nd row.",
                    "label": 0
                },
                {
                    "sent": "I plot the.",
                    "label": 0
                },
                {
                    "sent": "First Eigenvector is a function on the data points in this function is color coded, and probably you can see the yellow very well.",
                    "label": 1
                },
                {
                    "sent": "So the first eigenvector is so.",
                    "label": 0
                },
                {
                    "sent": "The first eigenvector was this one which was just constant, so I so on each point I just saw what I plot.",
                    "label": 1
                },
                {
                    "sent": "Here is the data point and then the like a color corresponding to the entry of the eigenvector.",
                    "label": 0
                },
                {
                    "sent": "And here it's constant.",
                    "label": 0
                },
                {
                    "sent": "It has all the same colors.",
                    "label": 0
                },
                {
                    "sent": "Here if we look at the second eigenvector that was the one where I said we have two different entries, so we see if we color code where those different entries are that most of the.",
                    "label": 0
                },
                {
                    "sent": "So I think those were the zero point.",
                    "label": 0
                },
                {
                    "sent": "14 entries are on one thing we would consider a cluster and the other ones are.",
                    "label": 0
                },
                {
                    "sent": "More or less the same.",
                    "label": 0
                },
                {
                    "sent": "Now if you go ahead it gets more and more complicated.",
                    "label": 0
                },
                {
                    "sent": "So here we can see the colors discriminate between this cluster and those two.",
                    "label": 0
                },
                {
                    "sent": "Maybe here they discriminate between this cluster in those two.",
                    "label": 0
                },
                {
                    "sent": "And here it already gets more fuzzy.",
                    "label": 0
                },
                {
                    "sent": "OK, but that's more or less how they investors look like if you.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look at them in a color code.",
                    "label": 0
                },
                {
                    "sent": "And now I'm.",
                    "label": 0
                },
                {
                    "sent": "That's a spectral embedding.",
                    "label": 0
                },
                {
                    "sent": "So what I plot here is we now have.",
                    "label": 0
                },
                {
                    "sent": "In the three dimensional case, so I plot the case where we have.",
                    "label": 0
                },
                {
                    "sent": "So say we want to discover three clusters.",
                    "label": 0
                },
                {
                    "sent": "We have those three eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "And we map each data point to a row in this.",
                    "label": 0
                },
                {
                    "sent": "Vector, and that's what I've got here.",
                    "label": 0
                },
                {
                    "sent": "So each of the of the Blue Cross is actually corresponds to one of those points in R3.",
                    "label": 0
                },
                {
                    "sent": "I'm and maybe I'll show a lifetime later on.",
                    "label": 0
                },
                {
                    "sent": "You can.",
                    "label": 0
                },
                {
                    "sent": "It's better to see so.",
                    "label": 0
                },
                {
                    "sent": "So we can see that forget about the red one.",
                    "label": 0
                },
                {
                    "sent": "So far the blue points are more or less.",
                    "label": 0
                },
                {
                    "sent": "Here is a very large cluster of points.",
                    "label": 0
                },
                {
                    "sent": "Here is a very large clustered here and then.",
                    "label": 0
                },
                {
                    "sent": "OK, we have some outliers and this is another presentation on which we run K means.",
                    "label": 0
                },
                {
                    "sent": "So let's after embedding the data points in R3 you have this.",
                    "label": 0
                },
                {
                    "sent": "And now K means.",
                    "label": 0
                },
                {
                    "sent": "I mean, even if it's a very dump algorithm.",
                    "label": 0
                },
                {
                    "sent": "But that's a very simple case, so we have like I don't know, 20 points here.",
                    "label": 0
                },
                {
                    "sent": "20 points here, 20 points here.",
                    "label": 0
                },
                {
                    "sent": "So K means if you're not completely stupid.",
                    "label": 0
                },
                {
                    "sent": "I mean it will be able to detect the clusters, right?",
                    "label": 0
                },
                {
                    "sent": "And that's actually what's going to happen.",
                    "label": 0
                },
                {
                    "sent": "So if you look at three clusters, OK. Then came into work on the three dimensional representation and will perfectly identify the three clusters.",
                    "label": 0
                },
                {
                    "sent": "I mean, of course it's a toy example, and now if you look at what happens if you take a spectral clustering to just find 2 classes, it OK, discovers two of them for four and five, it starts to split clusters.",
                    "label": 0
                },
                {
                    "sent": "OK, so I think that's a bit so.",
                    "label": 0
                },
                {
                    "sent": "How many people have I still with me?",
                    "label": 0
                },
                {
                    "sent": "So yeah.",
                    "label": 0
                },
                {
                    "sent": "Oh hang on, I need to close the door.",
                    "label": 0
                },
                {
                    "sent": "It's just so disturbing.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "In the reference, yes.",
                    "label": 0
                },
                {
                    "sent": "In this representation, it's true.",
                    "label": 0
                },
                {
                    "sent": "They're all the same.",
                    "label": 0
                },
                {
                    "sent": "The reason why I included them is that if you are actually in the ideal case where your data.",
                    "label": 0
                },
                {
                    "sent": "But your graph contains disconnected components, the eigenspace.",
                    "label": 0
                },
                {
                    "sent": "Has dimension which is higher than one.",
                    "label": 0
                },
                {
                    "sent": "So for example two if you have two components and then the eigenspaces spent by those components 1000011 and it might be.",
                    "label": 0
                },
                {
                    "sent": "And of course the constant one vector is also part in this eigenspace, but depending on which I can follow you use.",
                    "label": 0
                },
                {
                    "sent": "It might happen that you the first one you output is not just a constant one vector, so in most real cases the first vector doesn't play any role, but in extreme cases it's important and it doesn't hurt to solve, at least if it's constant.",
                    "label": 0
                },
                {
                    "sent": "Simply doesn't hurt.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah.",
                    "label": 0
                },
                {
                    "sent": "What are the mapping of data point to the eigenvectors?",
                    "label": 0
                },
                {
                    "sent": "I mean intuitively, what does it mean?",
                    "label": 0
                },
                {
                    "sent": "Well, what it means is.",
                    "label": 0
                },
                {
                    "sent": "Each point is math.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Well actually the problem is it's nothing where.",
                    "label": 0
                },
                {
                    "sent": "It's simply to get an intuition on it.",
                    "label": 0
                },
                {
                    "sent": "I still don't really have a clear intuition, but what is going on is.",
                    "label": 0
                },
                {
                    "sent": "Let's take again this ideal case where we have two separate clusters and then our matrix.",
                    "label": 0
                },
                {
                    "sent": "We will look like this, so that's a first, and that's the second eigenvector.",
                    "label": 0
                },
                {
                    "sent": "So what happens is and say this is data .123456.",
                    "label": 0
                },
                {
                    "sent": "So what will happen is we will map the first data point.",
                    "label": 0
                },
                {
                    "sent": "To the .10 and we will map the second data point.",
                    "label": 0
                },
                {
                    "sent": "Also to 1 zero and the same for the third one.",
                    "label": 0
                },
                {
                    "sent": "So those three points will actually be mapped on the identical point, which is just on the coordinate axis, like 00 to the left one to the top.",
                    "label": 0
                },
                {
                    "sent": "So and for the other clusters the other way.",
                    "label": 0
                },
                {
                    "sent": "So we will map it like if you're looking for two classes, we are in 2D, so all those points will be mapped here.",
                    "label": 0
                },
                {
                    "sent": "And now look at the other cluster.",
                    "label": 0
                },
                {
                    "sent": "We map them.",
                    "label": 0
                },
                {
                    "sent": "To those columns which is actually 01, so they will all end up here.",
                    "label": 0
                },
                {
                    "sent": "So in this ideal case.",
                    "label": 0
                },
                {
                    "sent": "We have like after the after the embedding, this point will appear three times.",
                    "label": 0
                },
                {
                    "sent": "This point will appear 3 times and that's actually the blue points which which we have here, more or less.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Of doing killings within this state.",
                    "label": 0
                },
                {
                    "sent": "Right, and that's what I'm going to show in the next slide.",
                    "label": 0
                },
                {
                    "sent": "So the point the whole point about spectral clustering is that this change of representations that are really very clever one, and if your data is not a wild, it is a means of separating your data points.",
                    "label": 0
                },
                {
                    "sent": "So classes which might be still a little bit overlapping here are perfectly.",
                    "label": 0
                },
                {
                    "sent": "I mean, in the ideal case, at least, uh, perfectly separated in this part.",
                    "label": 0
                },
                {
                    "sent": "Here it simply means to.",
                    "label": 0
                },
                {
                    "sent": "To improve the OR to workout the structure in the clustering structure.",
                    "label": 0
                },
                {
                    "sent": "But it's it's not.",
                    "label": 0
                },
                {
                    "sent": "I mean we will see how this works in the next slides, but it's not obvious.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. Um?",
                    "label": 0
                },
                {
                    "sent": "One thing which I might already mention here, we will see that later on a bit more closely is that in this case, like the example we had, it contains 3 very clear clusters.",
                    "label": 0
                },
                {
                    "sent": "You can now plot the eigenvalues of your of this graph Laplacian.",
                    "label": 1
                },
                {
                    "sent": "What will happen is So what I plot here is like I had hundred data points, probably so, and so my matrix contains 100 eigenvalues and I've got all of them next to each other.",
                    "label": 0
                },
                {
                    "sent": "So what you probably can't see it very well, but.",
                    "label": 0
                },
                {
                    "sent": "It is in fact, what you see is we have three eigenvalues which are very very close to 0.",
                    "label": 0
                },
                {
                    "sent": "Then there's a huge gap, and then there are all the other ones and actually one can use that.",
                    "label": 0
                },
                {
                    "sent": "At least in theory, to predict the number of clusters.",
                    "label": 0
                },
                {
                    "sent": "So you just look at the eigen values and look at how many small angles do you have and when do we have a gap.",
                    "label": 1
                },
                {
                    "sent": "And then you say OK, we have three clusters, but that's just I mean people do it but.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Doesn't work that well.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. Um?",
                    "label": 0
                },
                {
                    "sent": "OK, so I tried to before the break.",
                    "label": 0
                },
                {
                    "sent": "I tried to get the first interpretation by spectral clustering works.",
                    "label": 0
                },
                {
                    "sent": "That's my favorite one.",
                    "label": 0
                },
                {
                    "sent": "So actually their spectral clustering.",
                    "label": 0
                },
                {
                    "sent": "It's actually not new.",
                    "label": 0
                },
                {
                    "sent": "I mean people have done it already in the math in the 70s actually, but nobody has really noticed it and it became really a very well spread.",
                    "label": 0
                },
                {
                    "sent": "Like many people use it since like 2001 2002.",
                    "label": 0
                },
                {
                    "sent": "There are lots of having lots of papers, but it's nothing new and so there are very many different ways of how you could derive why spectral clustering works.",
                    "label": 0
                },
                {
                    "sent": "But I think this is the one which I find most intuitive.",
                    "label": 0
                },
                {
                    "sent": "So let's step back a bit.",
                    "label": 0
                },
                {
                    "sent": "We have our data graph and we set clustering is something which makes sure that points return the same cluster similar to each other in points which are in different clusters are not so similar to each other.",
                    "label": 0
                },
                {
                    "sent": "So now OK we need to.",
                    "label": 0
                },
                {
                    "sent": "I mean if you want to have an algorithm to do that, you somehow need to direct or.",
                    "label": 0
                },
                {
                    "sent": "The easiest way is to derive a mathematical criterion which tells you what you can optimize.",
                    "label": 0
                },
                {
                    "sent": "I mean, OK, that's what people do usually.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So the idea, what spectral clustering or what we want to look at is we say OK, what we want is we forget about the distance within clusters.",
                    "label": 0
                },
                {
                    "sent": "We just look at distances between clusters and we say we want that two clusters are things where the connections between the two clusters are very very low weight somehow.",
                    "label": 0
                },
                {
                    "sent": "So we want to end and we define the things we say the cut between two groups of points A&B is just so we sum over the points in the different clusters.",
                    "label": 0
                },
                {
                    "sent": "The similarities.",
                    "label": 0
                },
                {
                    "sent": "So we say we define what the cut between those two groups is.",
                    "label": 0
                },
                {
                    "sent": "It's actually the sum of this in this edge, like the weights of those two edges and then we say great we already done.",
                    "label": 0
                },
                {
                    "sent": "We minimize cut and we have an objective criterion.",
                    "label": 0
                },
                {
                    "sent": "We use that for clustering.",
                    "label": 0
                },
                {
                    "sent": "OK, and actually for I mean for minimizing cut it can even be done in a more or less efficient way.",
                    "label": 0
                },
                {
                    "sent": "It's surprisingly not NP hard, but it's there's this min cut Max flow theorem.",
                    "label": 0
                },
                {
                    "sent": "I don't know many, maybe some people know it, so you can really do that.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "But now there is a little problem and this problem is shows up if we look at this little vertex here.",
                    "label": 0
                },
                {
                    "sent": "So what happens very often is you have one outlier data point.",
                    "label": 0
                },
                {
                    "sent": "And then you just say minimize cut and I say, OK, great if I want to have two groups in this graph I just got here because here are just cut one edge in here.",
                    "label": 0
                },
                {
                    "sent": "I have to cut two edges.",
                    "label": 0
                },
                {
                    "sent": "So OK, the problem is now of course that this is not what we want to do in clustering.",
                    "label": 0
                },
                {
                    "sent": "So I mean somehow we don't want to identify outliers.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's also something we would like to do, but that's not what we want to do.",
                    "label": 0
                },
                {
                    "sent": "We want to find groups in the data in those groups should be more or less have reasonable size, right?",
                    "label": 0
                },
                {
                    "sent": "And that's why people came up with different objective functions which tried to incorporate the size of this data of the different clusters.",
                    "label": 0
                },
                {
                    "sent": "And essentially there are two different ways.",
                    "label": 0
                },
                {
                    "sent": "Which are do something very similar, so ratio cut.",
                    "label": 0
                },
                {
                    "sent": "What it does is we say OK, so in general what we do is we want to define a balanced cut.",
                    "label": 0
                },
                {
                    "sent": "That is, we say we we have two objectives.",
                    "label": 0
                },
                {
                    "sent": "We want to minimize the cut, but we also want to make sure that the groups are more or less have the same size.",
                    "label": 0
                },
                {
                    "sent": "And that's what they should cut that.",
                    "label": 0
                },
                {
                    "sent": "So we're going to minimize this later on, so we have a term which incorporates the cut.",
                    "label": 0
                },
                {
                    "sent": "But we have also term which incorporates the.",
                    "label": 0
                },
                {
                    "sent": "The balance between the two clusters and what's actually going on there.",
                    "label": 0
                },
                {
                    "sent": "It's very simple to see.",
                    "label": 0
                },
                {
                    "sent": "So this term 1 / A + 1 / B.",
                    "label": 1
                },
                {
                    "sent": "So remember absolute value of A is just a number of points.",
                    "label": 0
                },
                {
                    "sent": "The number of vertices in Group A and if we now so just look at like look at it sharply.",
                    "label": 0
                },
                {
                    "sent": "So if we say for example if A&B if they're both, have the same size, then this term will be just one over and half plus one over in half is.",
                    "label": 0
                },
                {
                    "sent": "4 / 10 which is pretty small, if any sludge, and if you have another case where, for example, lesson outline just contains one point and B contains the other N -- 1 points.",
                    "label": 0
                },
                {
                    "sent": "We have something like 1 / 1 + 1 / N -- 1, which is I mean roughly one.",
                    "label": 0
                },
                {
                    "sent": "So here we have something in the order of 1 / N and here we have something in the order of One South.",
                    "label": 0
                },
                {
                    "sent": "It shows that if you manage to get groups more or less, having the equal size in this term will be much smaller.",
                    "label": 0
                },
                {
                    "sent": "And so that's what we.",
                    "label": 0
                },
                {
                    "sent": "So now what we do is we just take the product of those two terms.",
                    "label": 0
                },
                {
                    "sent": "It's a pretty arbitrary way of doing it, but that's how we start, OK?",
                    "label": 0
                },
                {
                    "sent": "There's another way of doing this, so that's called ratio cut.",
                    "label": 0
                },
                {
                    "sent": "This is called normalized cut.",
                    "label": 0
                },
                {
                    "sent": "So very simply, instead of taking the number of points in each group, you take the volume which was the weights of the edges in each group but with the same reasoning somehow.",
                    "label": 0
                },
                {
                    "sent": "How do you choose?",
                    "label": 0
                },
                {
                    "sent": "Sorry, how are you?",
                    "label": 0
                },
                {
                    "sent": "Possible subsets in or.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, I mean, that's where we're going to talk about that.",
                    "label": 0
                },
                {
                    "sent": "So that's a problem.",
                    "label": 0
                },
                {
                    "sent": "Of course, I mean a. OK, so so far we just defined our objective function.",
                    "label": 0
                },
                {
                    "sent": "We want to minimize it and of course it's a very difficult problem.",
                    "label": 0
                },
                {
                    "sent": "That's exactly the point.",
                    "label": 0
                },
                {
                    "sent": "So we can just look at all different subsets of the graph.",
                    "label": 0
                },
                {
                    "sent": "It's it's hard and actually it's an NP hard problem.",
                    "label": 0
                },
                {
                    "sent": "Both of them are NP hard, actually.",
                    "label": 0
                },
                {
                    "sent": "I mean more or less.",
                    "label": 0
                },
                {
                    "sent": "It turns out any graph partitioning problem where you try to balance the clusters.",
                    "label": 0
                },
                {
                    "sent": "You can do whatever you want to turns out to be NP hard always.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, so there is something we have to do about that to solve it right?",
                    "label": 0
                },
                {
                    "sent": "And that's now where spectral clustering comes into.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, two more slides for the before the break.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to do now is I want to derive spectral clustering as a way of solving this optimization problem, which we've just seen before.",
                    "label": 0
                },
                {
                    "sent": "What I do in the slightest bit cheating, I look at an optimization problem which is slightly simpler than solving ratio cut.",
                    "label": 0
                },
                {
                    "sent": "Just because the formulas are similar.",
                    "label": 0
                },
                {
                    "sent": "We will do the ratio cut version in the exercises, so the principle is exactly the same, but so we just so K ratio cut.",
                    "label": 0
                },
                {
                    "sent": "So the first problem is we want to minimize.",
                    "label": 0
                },
                {
                    "sent": "If you want to minimize ratio cut, we look at cut.",
                    "label": 0
                },
                {
                    "sent": "I'll be team times 1 / A + 1 / B.",
                    "label": 0
                },
                {
                    "sent": "And now I say I take a slightly simpler problem where I say I want to minimize cut AB and I enforced at A&B have the same number of vertices, so it's the same principle.",
                    "label": 0
                },
                {
                    "sent": "This is a bit more general.",
                    "label": 0
                },
                {
                    "sent": "But you will see it, it will more or less go in the same way.",
                    "label": 0
                },
                {
                    "sent": "OK, so we start with that optimization problem.",
                    "label": 0
                },
                {
                    "sent": "Now what we do is OK, we observe it's NP hard or we can't.",
                    "label": 0
                },
                {
                    "sent": "It's a discrete optimization problem.",
                    "label": 0
                },
                {
                    "sent": "We don't know what to do.",
                    "label": 0
                },
                {
                    "sent": "We start to rewrite it.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we introduce this OK and we say we want to find 2 clusters.",
                    "label": 0
                },
                {
                    "sent": "That's the simplest case.",
                    "label": 0
                },
                {
                    "sent": "So we introduce this cluster indicator vector, which is just a function which tells us whether the data point XI is instead a or in set B.",
                    "label": 0
                },
                {
                    "sent": "So we have this vector plus one or minus one depending on whether the point is instead a or be now.",
                    "label": 0
                },
                {
                    "sent": "And now that's more or less already, all we have to do now comes just very nice tricks.",
                    "label": 0
                },
                {
                    "sent": "So the first or observations.",
                    "label": 0
                },
                {
                    "sent": "The first observation is if you now right cut.",
                    "label": 0
                },
                {
                    "sent": "What happens?",
                    "label": 0
                },
                {
                    "sent": "So cut the abuse defined as the sum over SIJ where we sum over I in A and Jane B.",
                    "label": 0
                },
                {
                    "sent": "And now there's a tricky way of writing this.",
                    "label": 0
                },
                {
                    "sent": "Is this some here now?",
                    "label": 0
                },
                {
                    "sent": "So including which holds for this vector here.",
                    "label": 0
                },
                {
                    "sent": "So if I enter in the same cluster, they have the same vector here.",
                    "label": 0
                },
                {
                    "sent": "So this term here will be 0.",
                    "label": 0
                },
                {
                    "sent": "So this expression will have like will be 0.",
                    "label": 0
                },
                {
                    "sent": "And if I understand different classes, we will have, say plus one here and minus one here.",
                    "label": 0
                },
                {
                    "sent": "So this term here will be plus 1 -- -- 1 which is 2B squared.",
                    "label": 0
                },
                {
                    "sent": "It gives 4.",
                    "label": 0
                },
                {
                    "sent": "So if there are two points which are different classes, it's we have this factor floor here.",
                    "label": 0
                },
                {
                    "sent": "That's why I divide by 104.",
                    "label": 0
                },
                {
                    "sent": "And So what did what?",
                    "label": 0
                },
                {
                    "sent": "This allows us now is so here we only summed over the pairs of points which are in different clusters.",
                    "label": 0
                },
                {
                    "sent": "And now we sum over all pairs.",
                    "label": 0
                },
                {
                    "sent": "So OK.",
                    "label": 0
                },
                {
                    "sent": "Does everybody see that or OK?",
                    "label": 0
                },
                {
                    "sent": "And now is a very elegant nice thing is, of course, we've seen that this is this key property of the graph Laplacian, so this is just F, transpose, LF, some factor in front.",
                    "label": 0
                },
                {
                    "sent": "So what we managed is we rewrote the objective function in terms of the graph Laplacian.",
                    "label": 0
                },
                {
                    "sent": "Now then, the second nice thing is.",
                    "label": 0
                },
                {
                    "sent": "We have this condition now that it should be in be should have the same size Now if you just look at what this means.",
                    "label": 0
                },
                {
                    "sent": "If we look at this vector F, it just means that if we sum over all elements of FD, some has to be 0.",
                    "label": 0
                },
                {
                    "sent": "And if you write this in a linear algebra way now, this means that the sum of a vector is just the product of the vector with the constant one vector.",
                    "label": 0
                },
                {
                    "sent": "This would be 0 and so.",
                    "label": 0
                },
                {
                    "sent": "Intuitively, this means that F should be orthogonal to the constant one vector.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we can.",
                    "label": 0
                },
                {
                    "sent": "We haven't done anything, we just have made observations and we know who write this optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So instead of minimizing cut Abby, we simply say we minimize F, transpose LF.",
                    "label": 0
                },
                {
                    "sent": "The condition A = B is translated into F is perpendicular to one.",
                    "label": 0
                },
                {
                    "sent": "And then we have OK slightly nasty condition here.",
                    "label": 0
                },
                {
                    "sent": "Of course this only is the same as above if we if you take the vector.",
                    "label": 0
                },
                {
                    "sent": "If I if we did so if you have discrete values here.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, and the editor constraint here, which we will see in the next step.",
                    "label": 0
                },
                {
                    "sent": "So I mean in the here above we always have that F is the norm of F is constant is just square root in, so it doesn't hurt us so far.",
                    "label": 0
                },
                {
                    "sent": "It's just an additional thing.",
                    "label": 0
                },
                {
                    "sent": "OK, now so far nothing has happened.",
                    "label": 0
                },
                {
                    "sent": "We just rewrote it and still have a discrete optimization problem because we have these annoying if I hear you know, that's the heuristic approach.",
                    "label": 0
                },
                {
                    "sent": "You say OK, too difficult to me.",
                    "label": 0
                },
                {
                    "sent": "I can solve a discrete optimization problem, I just relax.",
                    "label": 0
                },
                {
                    "sent": "So relaxing always means throwing away conditions, making life very much easier.",
                    "label": 0
                },
                {
                    "sent": "So we simply throw away this condition that if I has to be plus or minus one, we say we ignore it.",
                    "label": 0
                },
                {
                    "sent": "So what we say is we solve the optimization problem ignoring this condition.",
                    "label": 0
                },
                {
                    "sent": "And then we see what happens.",
                    "label": 0
                },
                {
                    "sent": "And what actually happens then is now the third nice fact.",
                    "label": 0
                },
                {
                    "sent": "So now we have the optimization problem.",
                    "label": 0
                },
                {
                    "sent": "Minimize essentially F, transpose, LF, subject to F orthogonal to one, and we fix the norm of F to some value.",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "And now the third nursing is that the constant one vector actually was the first eigenvector of the graph Laplacian, the smallest one.",
                    "label": 0
                },
                {
                    "sent": "So if you wouldn't have this condition if you just minimize F transpose LF, the solution is always the smallest eigenvector.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's a throwaway.",
                    "label": 0
                },
                {
                    "sent": "I never know how to pronounce it, really.",
                    "label": 0
                },
                {
                    "sent": "It's whatever serum.",
                    "label": 0
                },
                {
                    "sent": "And we can look at this in the exercises if you don't know it, so it, but it's essentially very simple to see, because if you.",
                    "label": 0
                },
                {
                    "sent": "Well, actually I don't go into the proof, but.",
                    "label": 0
                },
                {
                    "sent": "Somehow, if you multiply a vector like if you want to minimize a quadratic form, it always the minimum the vector which minimizes it is always the smallest eigenvector.",
                    "label": 0
                },
                {
                    "sent": "Now this smallest eigenvector can be because we have this condition that it should be also going out to one and now then this theorem tells you that in this case you need to take the second smallest eigenvector.",
                    "label": 0
                },
                {
                    "sent": "So what we see is without doing any more math, we simply see the solution of this problem.",
                    "label": 0
                },
                {
                    "sent": "If we forget about this, discreteness condition is just we take F as a second eigenvector of the graph Laplacian.",
                    "label": 0
                },
                {
                    "sent": "And OK, then we now have a real valued vector and we but we wanted to have a clustering.",
                    "label": 0
                },
                {
                    "sent": "Now we need let's so that we need to sort of inverse this relaxation together.",
                    "label": 0
                },
                {
                    "sent": "Discrete solution again.",
                    "label": 0
                },
                {
                    "sent": "And what we do is simply we say OK, we just take the sign of the vector.",
                    "label": 0
                },
                {
                    "sent": "So maybe the vector the entries here are not plus plus or minus one, but something positive in something negative.",
                    "label": 0
                },
                {
                    "sent": "So we just take the sinus cluster indicator vector.",
                    "label": 0
                },
                {
                    "sent": "And that's what we then call the clustering.",
                    "label": 0
                },
                {
                    "sent": "OK, Yep.",
                    "label": 0
                },
                {
                    "sent": "First home.",
                    "label": 0
                },
                {
                    "sent": "Actually OK, good question.",
                    "label": 0
                },
                {
                    "sent": "We we don't know it at all.",
                    "label": 0
                },
                {
                    "sent": "So OK, so that's now the OK. All sounds very nice.",
                    "label": 0
                },
                {
                    "sent": "The downside is this relaxation.",
                    "label": 0
                },
                {
                    "sent": "You don't have any control what's going to happen.",
                    "label": 0
                },
                {
                    "sent": "So this is really a heuristic.",
                    "label": 0
                },
                {
                    "sent": "And we will see I don't know whether I have a slide on this, but their examples we can show that this can be completely wrong.",
                    "label": 0
                },
                {
                    "sent": "I mean, essentially you don't even know that.",
                    "label": 0
                },
                {
                    "sent": "I mean, apart from this condition here, but you don't even know whether the solution after you do this rounding step again, whether it's minimal in for cut a be in any way, it could just be that this rounding that's arbitrary things and turns out to be very bad, so that's really.",
                    "label": 0
                },
                {
                    "sent": "So that's the bad part of of this relaxation, so it helps you to solve a problem which is very complicated.",
                    "label": 0
                },
                {
                    "sent": "You make a linear problem out of it, but of course there's some price to pay in the prices that you don't know what's going on here.",
                    "label": 0
                },
                {
                    "sent": "Science seems like the only works for two.",
                    "label": 0
                },
                {
                    "sent": "OK, I will tell you after the break how it works.",
                    "label": 0
                },
                {
                    "sent": "For more cluster, OK, let's take a break now.",
                    "label": 0
                }
            ]
        }
    }
}