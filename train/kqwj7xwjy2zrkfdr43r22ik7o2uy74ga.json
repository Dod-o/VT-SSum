{
    "id": "kqwj7xwjy2zrkfdr43r22ik7o2uy74ga",
    "title": "Exploiting feature covariance in high-dimensional online learning",
    "info": {
        "author": [
            "Justin Ma, Department of Computer Science and Engineering, UC San Diego"
        ],
        "published": "June 3, 2010",
        "recorded": "May 2010",
        "category": [
            "Top->Computer Science->Machine Learning->On-line Learning"
        ]
    },
    "url": "http://videolectures.net/aistats2010_ma_efcih/",
    "segmentation": [
        [
            "So first I want to say that this work would have been possible without the help from.",
            "My collaborators, we are from all over the place.",
            "Kobe lamented the fact that each of us didn't come from a separate institution, so I thought I'd mention that I'm starting a postdoc in Berkeley this summer.",
            "So six people, six different places.",
            "I think it'll work.",
            "Sound so today I'm going to talk about how you know what the benefits are of leveraging 2nd order feature interactions for online learning.",
            "And with that I'll start off with a brief overview of online learning of linear classifiers.",
            "So in this case, the model is characterized by a weight vector W. An learning takes place as follows, so you have an input."
        ],
        [
            "Vector X at time T you make a prediction which is taking the sign of the inner product between your weight vector.",
            "An example.",
            "So if it's positive that is, it lies above the hyperplane decision boundary.",
            "That's that'll be prediction an.",
            "Likewise, if it's negative will be lying below the hyperplane decision boundary.",
            "After that you receive a label, the true label of the example, whether it's negative or positive, and record an error if your prediction and the actual label disagree.",
            "And with the true label and the example in hand, you can then modify the weight vector you know to kind of improve your model."
        ],
        [
            "And this kind of learning has world real world motivations behind it.",
            "So in industrial scale applications you're dealing with large datasets with on the order of millions to billions of examples, and typically the data evolves overtime.",
            "That is, it's nonstationary annual.",
            "Be dealing with drifting concepts and so to that end, online learning is appealing because it has the following attributes, right?",
            "If you can make a single pass.",
            "Over the data for training the update that you make is incremental.",
            "And as a result, you will have potentially low overhead for storage and computation.",
            "And this is important when we're."
        ],
        [
            "Dealing with particularly high dimensional applications such as sentiment classification, trying to classify malicious URLs, or detecting web spam.",
            "In these applications, you're typically dealing with.",
            "There's a bag of words, style of feature generation, right?",
            "So you're dealing with as a result, millions of features and new features are being introduced all the time as a result.",
            "So so in the case of sentiment classification, you might want to take a look at Amazon reviews for Star Wars Five versus reviews for Stormers, one.",
            "And you'll use the bag of words representation of the reviews to determine whether the review is positive for the review is negative, or in the case of detecting malicious websites, you want to take all the features related to those and come to a decision based on that and these features, whether enumerating which ISP they came from will be a lot of those.",
            "Lot of those features that get generated."
        ],
        [
            "So online learning is promising, but to that end, which online algorithms should be used now?",
            "It's not.",
            "It's not a new concept.",
            "I mean it's it all started with the perception way back in the late 50s, but there's been a resurgence in interest within the last decade for online learning.",
            "And.",
            "The.",
            "The general trend that I want to point out is that the updates tend to get more complex overtime, especially with the use of 2nd order feature information so.",
            "I want to mention briefly that yesterday on the presenters for yesterday's talk, Shiva Swami Angavar they had they use 2nd order information in the context of training in SVM to great success in their work with the relative margin machines.",
            "But the algorithms that I want to focus on.",
            "Are these three, the perceptron, the passive aggressive algorithm?"
        ],
        [
            "And confidence weighted learning confidence.",
            "Weighted learning in particular.",
            "Well, I want to consider those three because they kind of represent a evolution of online learning from something that's very simple that doesn't take into account 2nd order information too.",
            "Confidence weighted learning, which actually keeps a covariance matrix.",
            "You know uncertainties about the feature weights and there are a number of algorithms on this slide.",
            "It's not exhaustive, but this is a sampling of them, such as Bayesian logistic regression, 2nd order Perceptron, online ellipsoid method, arrow added red and these make use of 2nd order information.",
            "Whether it's going to be called the Hessian or Mahalanobis norm, and this talk will be focusing on that.",
            "Let me go give you a quick overview of this.",
            "These three algorithms that represent this kind of evolution so."
        ],
        [
            "For the perceptron, it's pretty simple.",
            "When you make a mistake, you update the weight vector as follows.",
            "You have the new weight vector taking on the value of the old weight vector, which you add yx.",
            "The example with assign modulated by whether it's positive or negative example.",
            "And as you can see by this update, it's relatively crude, right?",
            "Because you're not.",
            "You know, adjusting the magnitude of the feature updates with this.",
            "And so kind of."
        ],
        [
            "Next step in this line of evolution is the passive aggressive algorithm, which works as follows.",
            "So on each iteration, at each time step, the passive aggressive algorithm solves the following constraint problem, which happens to have a closed form solution.",
            "So the constraint problem is that we want to update the weight vector by choosing a weight vector W that mean that is changes as little as possible from the previous weight vector.",
            "That is, we want to minimize the squared distance from the last wavevector subject to the constraint.",
            "But the classification margin is going to be greater than or equal to 1.",
            "And this can be expressed in closed form as we can see from the update below.",
            "So it looks a lot like the perceptron, but this time you have this little extra scalar in front Alpha which is.",
            "Which differs for each example.",
            "This proportional update is key because it kind of it allows us to assign greater weight to examples that have a higher amount of classification error.",
            "But the problem here is that we're modeling a single scaler, but you're kind of adjusting the weights of all features equally, so.",
            "The next step along this line of evolution."
        ],
        [
            "Can be represented by algorithms such as the confidence way of learning.",
            "So the idea here is that you were just represent a weight vector by distribution.",
            "In this case a Gaussian distribution.",
            "Pardon me if I deviate slightly from the standard Gaussian notation, so I'm we're approximating Sigma inverse for much of this talk, so I thought we just convenient to express it this way.",
            "So we draw weight vector from the distribution characterized by its mean and Sigma inverse the precision matrix.",
            "Which represents the uncertainty of our feature weight estimates.",
            "So this also can be what happens in this algorithm.",
            "Is that at each iteration it tries to solve a constrained optimization problem, which in turn has a closed form update.",
            "So here we update the new meeting code and the mean and precision matrix using the choosing the meaning.",
            "The precision matrix in a way that minimizes the calendar versions with the old model, but subject to.",
            "To the constraint.",
            "The classification margin YWX will be greater than zero with greater than some confidence value ADA, which in this talk we use .95.",
            "It's also mentioned in the papers."
        ],
        [
            "But as it turns out, this has a closed form update.",
            "And if you look at the update from you, which becomes our classification vector, this two also looks a lot like the perception update, with a little bit of a difference.",
            "Right this time you've got this covariance matrix up front, which modulates the rate at which different features are learned.",
            "It modulates each feature differently.",
            "And this structure in turn has an update rule.",
            "As you can see below, which is the precision matrix.",
            "The old Matrix plus another product of the example.",
            "So the key thing that I want to focus on this talk is how do we represent the precision matrix?",
            "So at first glance we could either represent it as a diagonal matrix or a full precision matrix.",
            "So with the diagonal you've got linear storage overhead and lower computation overhead, and contrast with a full covariance matrix.",
            "Pardon me if I interchange precision and covariance every now and then.",
            "If I have that slip, please don't mind that.",
            "So with the full precision matrix, you've got quadratic storage overhead.",
            "And as a result, higher compute time.",
            "So things are not looking good for fool.",
            "You might ask, why bother with using a full covariance matrix for confidence weighted learning?",
            "Well, as it turns out, there are some benefits and."
        ],
        [
            "Going to valuate these first, using a synthetic experiment where we construct a data set with noisily correlated features.",
            "So we construct the data for this set as follows.",
            "For each run, we first generate 1000 examples in a low dimensional space consisting of 10 dimensions.",
            "We generate random weight vector and examples on either side of the weight of the hyperplane will be labeled accordingly, positive and negative.",
            "Then for each example, we replicate the feature vector a number of times, in this case 100 times to yield 1000 features.",
            "But we're not done yet.",
            "Afterwards, with each feature, there's a 5% probability that the bit will be flipped, and so that's how we introduce noise into this data set.",
            "So the source data is correlated but fuzzily so.",
            "So considering the online algorithms that we discussed before, how did they fare under the under this case?"
        ],
        [
            "So what you're seeing here are the classification results for this experiment, averaged over 100 runs.",
            "So the X axis shows the number of rounds, number of examples in the Y axis shows the cumulative number of mistakes made.",
            "The four lines represent the four different online algorithms.",
            "Perceptron, passive, aggressive CW with diagonal covariance, and CLU with a full covariance full precision matrix.",
            "And what you see here clearly is that the full precision matrix helps exploit.",
            "The correlations present in the data.",
            "So going back to the."
        ],
        [
            "To the high level comparison.",
            "We can see here that, well.",
            "I mean, just by its construction, the diagonal covariance is going, or correlations in the data, whereas a full covariance can exploit them.",
            "So to that end, we ask, can we get the benefits of both?",
            "Can we get something in between?"
        ],
        [
            "So with that we introduce a factor approximation of the precision matrix as follows.",
            "So Sigma inverse is approximated by a diagonal matrix D plus the outer product of skinny rectangular matrix.",
            "Are the dimension of our is N, which is the number of features by KK is the number of factors.",
            "You can kind of think of it as like guess on the implicit dimensionality of the data."
        ],
        [
            "So by approximating the full covariance this way, we have a data structure which has order Katie.",
            "In storage I mean order KN storage and we compress the updates to the matrix using factor analysis as follows.",
            "So I'll just give you a high level overview.",
            "What happens is that we have our original distribution for the weight vector characterized by mu and Sigma inverse an we want to approximate it this time.",
            "An approximate distribution that shares the same mean but a different covariance.",
            "the D plus RR representation.",
            "And two, and in this case D&R or the free parameters, and so we use an iterative procedure to minimize the calendar vergence between the exact and approximate distribution.",
            "This procedure has order NK squared overhead because of the matrix multiplications involved in the in the M steps.",
            "So."
        ],
        [
            "How does this do?",
            "Well, here are the results from before.",
            "And with just K."
        ],
        [
            "Equals 4.",
            "Remember this is a data set with 1000 features with just K = 4 We get results that closely.",
            "You know that closely track CW full and actually if you look in the paper we do run with K = 16 for the factor approximation and it's it's even closer to the full result.",
            "So the factor result approximates the full result.",
            "And is using less memory.",
            "At the same time.",
            "So this is promising."
        ],
        [
            "Love.",
            "Next we ask well, what are the benefits of approximating the full covariance matrix in a real world application so?",
            "You know to that you can't store a full covariance matrix for applications that have millions of features, right?",
            "A million by million covariance matrix that just not you can't store that in memory.",
            "So what we want to ask is what happens if.",
            "We approximate it, at least.",
            "What do we lose by sticking with diagonal?",
            "What do we gain by doing an approximation?",
            "So."
        ],
        [
            "I'll show you three experiments.",
            "The first is malicious URL detection.",
            "Each trial consists of 200,000 examples and on the order of 1,000,000 features.",
            "And these are the results we get here.",
            "They actually shows the number of days 20 days over which the experiments conducted at why the X axis shows that the Y axis shows the cumulative number of mistakes and both versions of CW, the diagonal and the factor are clearly outperforming the perceptron and passive aggressive.",
            "So let's zoom in on that and see the difference.",
            "What we see here is that the additional mistakes made by the diagonal covariance diverges overtime.",
            "This results in.",
            "Overall improvement of 5% accuracy for the factored approximation.",
            "Let's take a look at it."
        ],
        [
            "They said so.",
            "Here we use the web spam set from the Pascal large scale learning competition.",
            "We divide it into 35 chunks and subsample from each to produce experimental runs within the order of 170,000 examples, an 700,000 features and these are going to be kind of very correlated.",
            "By the way the features are constructed because their web pages.",
            "But the tokens, but there is broken into 3 gram tokens, so if you have a word that occurs a lot, there are going to be these Co occurring 3 grams.",
            "And these are the results for this.",
            "And if we zoom in.",
            "And look at the relative difference between The CW, diagonal and CW with factory covariance.",
            "We see an overall improvement of 18% /, 18% reduction in error of factor versus diagonal.",
            "And."
        ],
        [
            "Finally, we consider experiments document classification experiments that were also introduced in the ICM 2008 CW paper.",
            "They have, on the order of thousands of examples.",
            "10,000 to 1,000,000 features.",
            "And the improvement of the factor approximation over the diagonal is pretty consistent across the board, anywhere from 3% to 10 to 20% improvement depending on the data set.",
            "And with that, I'll wrap up so."
        ],
        [
            "A full in factor covariance helps when the features are correlated.",
            "But also when the number of features are greater than the number of examples, what I didn't have time to talk about today in which I encourage you to take a look at it in our paper in Section 2 is that there are situations where a full covariance factor covariance will not help.",
            "Especially, is especially the case when the number of examples out numbers that number of features.",
            "But in the case of real world applications, effective approximation helps improve the accuracy and as far as future work is concerned.",
            "You know CW is not the only algorithm that this could be applied to.",
            "As I mentioned earlier in the talk, there are a lot of related algorithms that keep track of 2nd order information, so you might ask, well, do those benefit from doing this kind of approximation?",
            "And also factor analysis isn't the only way to do this approximation.",
            "I imagine that there's a lot of room for, you know, an approximation that somehow took advantage of the sparsity of the data in that can be pretty promising, and there are other ways to approximate data structures like a covariance matrix as well."
        ],
        [
            "To that end, we have code available.",
            "The code we use for the synthetic experiments at this URL, in case you're curious and interested in checking it out and.",
            "A couple of other Co authors were able to make it.",
            "To the conference.",
            "So you can ask me after the talk or you can ask Alex or Kobe if you have any questions.",
            "But with that I'll take questions from the audience."
        ],
        [
            "Hi, I think I missed the update rules for.",
            "Like R&B, RE still analytic?",
            "Or is it very simple or oh what they are?",
            "I have the.",
            "The expressions in the paper I left the, you know the matrix multiplications, those formulas out of this talk.",
            "Are you interested in seeing kind of?",
            "It's similar to the in terms of like the one for the passive, the confidence weighted one or fairly iterative iterative procedure.",
            "Yeah, exactly.",
            "Oh sure, yeah.",
            "You mentioned that it uses.",
            "Yeah, have you ever experienced?",
            ", haven't, I don't do any experiments to distinguish between whether it's.",
            "Sorry, the local locality.",
            "What was the question again?",
            "Local minima, yeah.",
            "I mean, I imagine it's local minima, but it doesn't seem to affect the results too much.",
            "I mean, that's kind of a given with that kind of procedure.",
            "We have two 2 limits as you'll see in the code.",
            "We have a iteration limit of 50 and also a convergence limit, like if the relative change in the parameter value is less than 10 to the six, then we stop.",
            "So we have two limits that we use for the procedure.",
            "Yeah."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first I want to say that this work would have been possible without the help from.",
                    "label": 0
                },
                {
                    "sent": "My collaborators, we are from all over the place.",
                    "label": 0
                },
                {
                    "sent": "Kobe lamented the fact that each of us didn't come from a separate institution, so I thought I'd mention that I'm starting a postdoc in Berkeley this summer.",
                    "label": 0
                },
                {
                    "sent": "So six people, six different places.",
                    "label": 0
                },
                {
                    "sent": "I think it'll work.",
                    "label": 0
                },
                {
                    "sent": "Sound so today I'm going to talk about how you know what the benefits are of leveraging 2nd order feature interactions for online learning.",
                    "label": 0
                },
                {
                    "sent": "And with that I'll start off with a brief overview of online learning of linear classifiers.",
                    "label": 0
                },
                {
                    "sent": "So in this case, the model is characterized by a weight vector W. An learning takes place as follows, so you have an input.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Vector X at time T you make a prediction which is taking the sign of the inner product between your weight vector.",
                    "label": 0
                },
                {
                    "sent": "An example.",
                    "label": 0
                },
                {
                    "sent": "So if it's positive that is, it lies above the hyperplane decision boundary.",
                    "label": 0
                },
                {
                    "sent": "That's that'll be prediction an.",
                    "label": 0
                },
                {
                    "sent": "Likewise, if it's negative will be lying below the hyperplane decision boundary.",
                    "label": 0
                },
                {
                    "sent": "After that you receive a label, the true label of the example, whether it's negative or positive, and record an error if your prediction and the actual label disagree.",
                    "label": 0
                },
                {
                    "sent": "And with the true label and the example in hand, you can then modify the weight vector you know to kind of improve your model.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this kind of learning has world real world motivations behind it.",
                    "label": 0
                },
                {
                    "sent": "So in industrial scale applications you're dealing with large datasets with on the order of millions to billions of examples, and typically the data evolves overtime.",
                    "label": 0
                },
                {
                    "sent": "That is, it's nonstationary annual.",
                    "label": 0
                },
                {
                    "sent": "Be dealing with drifting concepts and so to that end, online learning is appealing because it has the following attributes, right?",
                    "label": 1
                },
                {
                    "sent": "If you can make a single pass.",
                    "label": 0
                },
                {
                    "sent": "Over the data for training the update that you make is incremental.",
                    "label": 0
                },
                {
                    "sent": "And as a result, you will have potentially low overhead for storage and computation.",
                    "label": 0
                },
                {
                    "sent": "And this is important when we're.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Dealing with particularly high dimensional applications such as sentiment classification, trying to classify malicious URLs, or detecting web spam.",
                    "label": 1
                },
                {
                    "sent": "In these applications, you're typically dealing with.",
                    "label": 0
                },
                {
                    "sent": "There's a bag of words, style of feature generation, right?",
                    "label": 0
                },
                {
                    "sent": "So you're dealing with as a result, millions of features and new features are being introduced all the time as a result.",
                    "label": 0
                },
                {
                    "sent": "So so in the case of sentiment classification, you might want to take a look at Amazon reviews for Star Wars Five versus reviews for Stormers, one.",
                    "label": 0
                },
                {
                    "sent": "And you'll use the bag of words representation of the reviews to determine whether the review is positive for the review is negative, or in the case of detecting malicious websites, you want to take all the features related to those and come to a decision based on that and these features, whether enumerating which ISP they came from will be a lot of those.",
                    "label": 0
                },
                {
                    "sent": "Lot of those features that get generated.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So online learning is promising, but to that end, which online algorithms should be used now?",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "It's not a new concept.",
                    "label": 0
                },
                {
                    "sent": "I mean it's it all started with the perception way back in the late 50s, but there's been a resurgence in interest within the last decade for online learning.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "The general trend that I want to point out is that the updates tend to get more complex overtime, especially with the use of 2nd order feature information so.",
                    "label": 0
                },
                {
                    "sent": "I want to mention briefly that yesterday on the presenters for yesterday's talk, Shiva Swami Angavar they had they use 2nd order information in the context of training in SVM to great success in their work with the relative margin machines.",
                    "label": 0
                },
                {
                    "sent": "But the algorithms that I want to focus on.",
                    "label": 0
                },
                {
                    "sent": "Are these three, the perceptron, the passive aggressive algorithm?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And confidence weighted learning confidence.",
                    "label": 0
                },
                {
                    "sent": "Weighted learning in particular.",
                    "label": 0
                },
                {
                    "sent": "Well, I want to consider those three because they kind of represent a evolution of online learning from something that's very simple that doesn't take into account 2nd order information too.",
                    "label": 0
                },
                {
                    "sent": "Confidence weighted learning, which actually keeps a covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "You know uncertainties about the feature weights and there are a number of algorithms on this slide.",
                    "label": 0
                },
                {
                    "sent": "It's not exhaustive, but this is a sampling of them, such as Bayesian logistic regression, 2nd order Perceptron, online ellipsoid method, arrow added red and these make use of 2nd order information.",
                    "label": 1
                },
                {
                    "sent": "Whether it's going to be called the Hessian or Mahalanobis norm, and this talk will be focusing on that.",
                    "label": 0
                },
                {
                    "sent": "Let me go give you a quick overview of this.",
                    "label": 0
                },
                {
                    "sent": "These three algorithms that represent this kind of evolution so.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the perceptron, it's pretty simple.",
                    "label": 0
                },
                {
                    "sent": "When you make a mistake, you update the weight vector as follows.",
                    "label": 0
                },
                {
                    "sent": "You have the new weight vector taking on the value of the old weight vector, which you add yx.",
                    "label": 0
                },
                {
                    "sent": "The example with assign modulated by whether it's positive or negative example.",
                    "label": 0
                },
                {
                    "sent": "And as you can see by this update, it's relatively crude, right?",
                    "label": 0
                },
                {
                    "sent": "Because you're not.",
                    "label": 0
                },
                {
                    "sent": "You know, adjusting the magnitude of the feature updates with this.",
                    "label": 0
                },
                {
                    "sent": "And so kind of.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Next step in this line of evolution is the passive aggressive algorithm, which works as follows.",
                    "label": 0
                },
                {
                    "sent": "So on each iteration, at each time step, the passive aggressive algorithm solves the following constraint problem, which happens to have a closed form solution.",
                    "label": 0
                },
                {
                    "sent": "So the constraint problem is that we want to update the weight vector by choosing a weight vector W that mean that is changes as little as possible from the previous weight vector.",
                    "label": 0
                },
                {
                    "sent": "That is, we want to minimize the squared distance from the last wavevector subject to the constraint.",
                    "label": 0
                },
                {
                    "sent": "But the classification margin is going to be greater than or equal to 1.",
                    "label": 0
                },
                {
                    "sent": "And this can be expressed in closed form as we can see from the update below.",
                    "label": 0
                },
                {
                    "sent": "So it looks a lot like the perceptron, but this time you have this little extra scalar in front Alpha which is.",
                    "label": 0
                },
                {
                    "sent": "Which differs for each example.",
                    "label": 0
                },
                {
                    "sent": "This proportional update is key because it kind of it allows us to assign greater weight to examples that have a higher amount of classification error.",
                    "label": 0
                },
                {
                    "sent": "But the problem here is that we're modeling a single scaler, but you're kind of adjusting the weights of all features equally, so.",
                    "label": 0
                },
                {
                    "sent": "The next step along this line of evolution.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Can be represented by algorithms such as the confidence way of learning.",
                    "label": 0
                },
                {
                    "sent": "So the idea here is that you were just represent a weight vector by distribution.",
                    "label": 0
                },
                {
                    "sent": "In this case a Gaussian distribution.",
                    "label": 1
                },
                {
                    "sent": "Pardon me if I deviate slightly from the standard Gaussian notation, so I'm we're approximating Sigma inverse for much of this talk, so I thought we just convenient to express it this way.",
                    "label": 0
                },
                {
                    "sent": "So we draw weight vector from the distribution characterized by its mean and Sigma inverse the precision matrix.",
                    "label": 0
                },
                {
                    "sent": "Which represents the uncertainty of our feature weight estimates.",
                    "label": 0
                },
                {
                    "sent": "So this also can be what happens in this algorithm.",
                    "label": 0
                },
                {
                    "sent": "Is that at each iteration it tries to solve a constrained optimization problem, which in turn has a closed form update.",
                    "label": 0
                },
                {
                    "sent": "So here we update the new meeting code and the mean and precision matrix using the choosing the meaning.",
                    "label": 0
                },
                {
                    "sent": "The precision matrix in a way that minimizes the calendar versions with the old model, but subject to.",
                    "label": 0
                },
                {
                    "sent": "To the constraint.",
                    "label": 0
                },
                {
                    "sent": "The classification margin YWX will be greater than zero with greater than some confidence value ADA, which in this talk we use .95.",
                    "label": 0
                },
                {
                    "sent": "It's also mentioned in the papers.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But as it turns out, this has a closed form update.",
                    "label": 0
                },
                {
                    "sent": "And if you look at the update from you, which becomes our classification vector, this two also looks a lot like the perception update, with a little bit of a difference.",
                    "label": 0
                },
                {
                    "sent": "Right this time you've got this covariance matrix up front, which modulates the rate at which different features are learned.",
                    "label": 0
                },
                {
                    "sent": "It modulates each feature differently.",
                    "label": 0
                },
                {
                    "sent": "And this structure in turn has an update rule.",
                    "label": 0
                },
                {
                    "sent": "As you can see below, which is the precision matrix.",
                    "label": 0
                },
                {
                    "sent": "The old Matrix plus another product of the example.",
                    "label": 0
                },
                {
                    "sent": "So the key thing that I want to focus on this talk is how do we represent the precision matrix?",
                    "label": 0
                },
                {
                    "sent": "So at first glance we could either represent it as a diagonal matrix or a full precision matrix.",
                    "label": 0
                },
                {
                    "sent": "So with the diagonal you've got linear storage overhead and lower computation overhead, and contrast with a full covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "Pardon me if I interchange precision and covariance every now and then.",
                    "label": 0
                },
                {
                    "sent": "If I have that slip, please don't mind that.",
                    "label": 0
                },
                {
                    "sent": "So with the full precision matrix, you've got quadratic storage overhead.",
                    "label": 0
                },
                {
                    "sent": "And as a result, higher compute time.",
                    "label": 1
                },
                {
                    "sent": "So things are not looking good for fool.",
                    "label": 0
                },
                {
                    "sent": "You might ask, why bother with using a full covariance matrix for confidence weighted learning?",
                    "label": 0
                },
                {
                    "sent": "Well, as it turns out, there are some benefits and.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Going to valuate these first, using a synthetic experiment where we construct a data set with noisily correlated features.",
                    "label": 1
                },
                {
                    "sent": "So we construct the data for this set as follows.",
                    "label": 1
                },
                {
                    "sent": "For each run, we first generate 1000 examples in a low dimensional space consisting of 10 dimensions.",
                    "label": 1
                },
                {
                    "sent": "We generate random weight vector and examples on either side of the weight of the hyperplane will be labeled accordingly, positive and negative.",
                    "label": 0
                },
                {
                    "sent": "Then for each example, we replicate the feature vector a number of times, in this case 100 times to yield 1000 features.",
                    "label": 0
                },
                {
                    "sent": "But we're not done yet.",
                    "label": 0
                },
                {
                    "sent": "Afterwards, with each feature, there's a 5% probability that the bit will be flipped, and so that's how we introduce noise into this data set.",
                    "label": 0
                },
                {
                    "sent": "So the source data is correlated but fuzzily so.",
                    "label": 0
                },
                {
                    "sent": "So considering the online algorithms that we discussed before, how did they fare under the under this case?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what you're seeing here are the classification results for this experiment, averaged over 100 runs.",
                    "label": 0
                },
                {
                    "sent": "So the X axis shows the number of rounds, number of examples in the Y axis shows the cumulative number of mistakes made.",
                    "label": 0
                },
                {
                    "sent": "The four lines represent the four different online algorithms.",
                    "label": 0
                },
                {
                    "sent": "Perceptron, passive, aggressive CW with diagonal covariance, and CLU with a full covariance full precision matrix.",
                    "label": 0
                },
                {
                    "sent": "And what you see here clearly is that the full precision matrix helps exploit.",
                    "label": 0
                },
                {
                    "sent": "The correlations present in the data.",
                    "label": 0
                },
                {
                    "sent": "So going back to the.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To the high level comparison.",
                    "label": 0
                },
                {
                    "sent": "We can see here that, well.",
                    "label": 0
                },
                {
                    "sent": "I mean, just by its construction, the diagonal covariance is going, or correlations in the data, whereas a full covariance can exploit them.",
                    "label": 0
                },
                {
                    "sent": "So to that end, we ask, can we get the benefits of both?",
                    "label": 1
                },
                {
                    "sent": "Can we get something in between?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So with that we introduce a factor approximation of the precision matrix as follows.",
                    "label": 0
                },
                {
                    "sent": "So Sigma inverse is approximated by a diagonal matrix D plus the outer product of skinny rectangular matrix.",
                    "label": 0
                },
                {
                    "sent": "Are the dimension of our is N, which is the number of features by KK is the number of factors.",
                    "label": 1
                },
                {
                    "sent": "You can kind of think of it as like guess on the implicit dimensionality of the data.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So by approximating the full covariance this way, we have a data structure which has order Katie.",
                    "label": 0
                },
                {
                    "sent": "In storage I mean order KN storage and we compress the updates to the matrix using factor analysis as follows.",
                    "label": 1
                },
                {
                    "sent": "So I'll just give you a high level overview.",
                    "label": 0
                },
                {
                    "sent": "What happens is that we have our original distribution for the weight vector characterized by mu and Sigma inverse an we want to approximate it this time.",
                    "label": 0
                },
                {
                    "sent": "An approximate distribution that shares the same mean but a different covariance.",
                    "label": 0
                },
                {
                    "sent": "the D plus RR representation.",
                    "label": 0
                },
                {
                    "sent": "And two, and in this case D&R or the free parameters, and so we use an iterative procedure to minimize the calendar vergence between the exact and approximate distribution.",
                    "label": 1
                },
                {
                    "sent": "This procedure has order NK squared overhead because of the matrix multiplications involved in the in the M steps.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How does this do?",
                    "label": 0
                },
                {
                    "sent": "Well, here are the results from before.",
                    "label": 0
                },
                {
                    "sent": "And with just K.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Equals 4.",
                    "label": 0
                },
                {
                    "sent": "Remember this is a data set with 1000 features with just K = 4 We get results that closely.",
                    "label": 0
                },
                {
                    "sent": "You know that closely track CW full and actually if you look in the paper we do run with K = 16 for the factor approximation and it's it's even closer to the full result.",
                    "label": 0
                },
                {
                    "sent": "So the factor result approximates the full result.",
                    "label": 0
                },
                {
                    "sent": "And is using less memory.",
                    "label": 1
                },
                {
                    "sent": "At the same time.",
                    "label": 0
                },
                {
                    "sent": "So this is promising.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Love.",
                    "label": 0
                },
                {
                    "sent": "Next we ask well, what are the benefits of approximating the full covariance matrix in a real world application so?",
                    "label": 1
                },
                {
                    "sent": "You know to that you can't store a full covariance matrix for applications that have millions of features, right?",
                    "label": 0
                },
                {
                    "sent": "A million by million covariance matrix that just not you can't store that in memory.",
                    "label": 0
                },
                {
                    "sent": "So what we want to ask is what happens if.",
                    "label": 0
                },
                {
                    "sent": "We approximate it, at least.",
                    "label": 0
                },
                {
                    "sent": "What do we lose by sticking with diagonal?",
                    "label": 0
                },
                {
                    "sent": "What do we gain by doing an approximation?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'll show you three experiments.",
                    "label": 0
                },
                {
                    "sent": "The first is malicious URL detection.",
                    "label": 0
                },
                {
                    "sent": "Each trial consists of 200,000 examples and on the order of 1,000,000 features.",
                    "label": 1
                },
                {
                    "sent": "And these are the results we get here.",
                    "label": 0
                },
                {
                    "sent": "They actually shows the number of days 20 days over which the experiments conducted at why the X axis shows that the Y axis shows the cumulative number of mistakes and both versions of CW, the diagonal and the factor are clearly outperforming the perceptron and passive aggressive.",
                    "label": 0
                },
                {
                    "sent": "So let's zoom in on that and see the difference.",
                    "label": 1
                },
                {
                    "sent": "What we see here is that the additional mistakes made by the diagonal covariance diverges overtime.",
                    "label": 0
                },
                {
                    "sent": "This results in.",
                    "label": 0
                },
                {
                    "sent": "Overall improvement of 5% accuracy for the factored approximation.",
                    "label": 0
                },
                {
                    "sent": "Let's take a look at it.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "They said so.",
                    "label": 0
                },
                {
                    "sent": "Here we use the web spam set from the Pascal large scale learning competition.",
                    "label": 1
                },
                {
                    "sent": "We divide it into 35 chunks and subsample from each to produce experimental runs within the order of 170,000 examples, an 700,000 features and these are going to be kind of very correlated.",
                    "label": 1
                },
                {
                    "sent": "By the way the features are constructed because their web pages.",
                    "label": 0
                },
                {
                    "sent": "But the tokens, but there is broken into 3 gram tokens, so if you have a word that occurs a lot, there are going to be these Co occurring 3 grams.",
                    "label": 0
                },
                {
                    "sent": "And these are the results for this.",
                    "label": 0
                },
                {
                    "sent": "And if we zoom in.",
                    "label": 0
                },
                {
                    "sent": "And look at the relative difference between The CW, diagonal and CW with factory covariance.",
                    "label": 0
                },
                {
                    "sent": "We see an overall improvement of 18% /, 18% reduction in error of factor versus diagonal.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Finally, we consider experiments document classification experiments that were also introduced in the ICM 2008 CW paper.",
                    "label": 1
                },
                {
                    "sent": "They have, on the order of thousands of examples.",
                    "label": 0
                },
                {
                    "sent": "10,000 to 1,000,000 features.",
                    "label": 0
                },
                {
                    "sent": "And the improvement of the factor approximation over the diagonal is pretty consistent across the board, anywhere from 3% to 10 to 20% improvement depending on the data set.",
                    "label": 0
                },
                {
                    "sent": "And with that, I'll wrap up so.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A full in factor covariance helps when the features are correlated.",
                    "label": 1
                },
                {
                    "sent": "But also when the number of features are greater than the number of examples, what I didn't have time to talk about today in which I encourage you to take a look at it in our paper in Section 2 is that there are situations where a full covariance factor covariance will not help.",
                    "label": 0
                },
                {
                    "sent": "Especially, is especially the case when the number of examples out numbers that number of features.",
                    "label": 1
                },
                {
                    "sent": "But in the case of real world applications, effective approximation helps improve the accuracy and as far as future work is concerned.",
                    "label": 0
                },
                {
                    "sent": "You know CW is not the only algorithm that this could be applied to.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned earlier in the talk, there are a lot of related algorithms that keep track of 2nd order information, so you might ask, well, do those benefit from doing this kind of approximation?",
                    "label": 0
                },
                {
                    "sent": "And also factor analysis isn't the only way to do this approximation.",
                    "label": 1
                },
                {
                    "sent": "I imagine that there's a lot of room for, you know, an approximation that somehow took advantage of the sparsity of the data in that can be pretty promising, and there are other ways to approximate data structures like a covariance matrix as well.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To that end, we have code available.",
                    "label": 0
                },
                {
                    "sent": "The code we use for the synthetic experiments at this URL, in case you're curious and interested in checking it out and.",
                    "label": 0
                },
                {
                    "sent": "A couple of other Co authors were able to make it.",
                    "label": 0
                },
                {
                    "sent": "To the conference.",
                    "label": 0
                },
                {
                    "sent": "So you can ask me after the talk or you can ask Alex or Kobe if you have any questions.",
                    "label": 0
                },
                {
                    "sent": "But with that I'll take questions from the audience.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi, I think I missed the update rules for.",
                    "label": 0
                },
                {
                    "sent": "Like R&B, RE still analytic?",
                    "label": 0
                },
                {
                    "sent": "Or is it very simple or oh what they are?",
                    "label": 0
                },
                {
                    "sent": "I have the.",
                    "label": 0
                },
                {
                    "sent": "The expressions in the paper I left the, you know the matrix multiplications, those formulas out of this talk.",
                    "label": 0
                },
                {
                    "sent": "Are you interested in seeing kind of?",
                    "label": 0
                },
                {
                    "sent": "It's similar to the in terms of like the one for the passive, the confidence weighted one or fairly iterative iterative procedure.",
                    "label": 0
                },
                {
                    "sent": "Yeah, exactly.",
                    "label": 0
                },
                {
                    "sent": "Oh sure, yeah.",
                    "label": 0
                },
                {
                    "sent": "You mentioned that it uses.",
                    "label": 0
                },
                {
                    "sent": "Yeah, have you ever experienced?",
                    "label": 0
                },
                {
                    "sent": ", haven't, I don't do any experiments to distinguish between whether it's.",
                    "label": 0
                },
                {
                    "sent": "Sorry, the local locality.",
                    "label": 0
                },
                {
                    "sent": "What was the question again?",
                    "label": 0
                },
                {
                    "sent": "Local minima, yeah.",
                    "label": 0
                },
                {
                    "sent": "I mean, I imagine it's local minima, but it doesn't seem to affect the results too much.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's kind of a given with that kind of procedure.",
                    "label": 0
                },
                {
                    "sent": "We have two 2 limits as you'll see in the code.",
                    "label": 0
                },
                {
                    "sent": "We have a iteration limit of 50 and also a convergence limit, like if the relative change in the parameter value is less than 10 to the six, then we stop.",
                    "label": 0
                },
                {
                    "sent": "So we have two limits that we use for the procedure.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        }
    }
}