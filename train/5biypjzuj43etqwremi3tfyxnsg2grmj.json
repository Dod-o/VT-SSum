{
    "id": "5biypjzuj43etqwremi3tfyxnsg2grmj",
    "title": "Estimation of Multiple Transcription Factor Activities using ODEs and Gaussian Processes",
    "info": {
        "presenter": [
            "Neil D. Lawrence, Department of Computer Science, University of Sheffield"
        ],
        "published": "April 16, 2009",
        "recorded": "April 2009",
        "category": [
            "Top->Computer Science->Bioinformatics",
            "Top->Computer Science->Machine Learning->Gaussian Processes"
        ]
    },
    "url": "http://videolectures.net/licsb09_lawrence_emtf/",
    "segmentation": [
        [
            "So next week is new Lawrence stepping in for Michael Theseus and he's going to talk to us about estimation of multiple transcription factor activities using ODS and motion process.",
            "Thanks Martina, so yeah.",
            "As Martino said, this is Nicholas is work.",
            "Unfortunately Carlos has a family illness and is back in Greece so you couldn't make it but he sends his regards but basically everything I'm presenting is it's from a project at Nicholas is on with Magnussen I as investigators but particularly the sampling ideas that are used are all coming from the carlis.",
            "OK, so it's nice."
        ],
        [
            "Previous talk was about Gaussian process.",
            "'cause that's what that will help me.",
            "Well, you'll have a sense of what a Gaussian process function looks like if you've not seen them before.",
            "So review of the talk.",
            "These McCarthy slides that he sent.",
            "So if I want to get surprised about something, it's 'cause I've not read them properly.",
            "So learning a single TF with Gaussian processes is the sort of initial motivation where we're going, but the contribution of this talk is the directions we're moving in with multiple transcription factors.",
            "And we've got some sort of ongoing experiments and conclusions about where you are with this sort of model, and we think there's a lot of promise, but we don't think it's quite there."
        ],
        [
            "Yet.",
            "So the idea is you're interested in transcriptional regulation.",
            "And you've got data, which is why, as in the previous talk of engines, at times, what we're interested in modeling is the dynamics of the expression of the set of genes, and we want to, for a single transcription factor case, we want to infer a single transcription factor that is governing how those genes are expressed.",
            "And the way we're going to do that is we're going to follow these other guys here and use a differential equation model.",
            "So the rate of production of genes concentration is given by some basal transcription rate.",
            "Plus some sensitivity to nonlinear function of the gene expression F of T or in fact will typically be using FT is the log, so the transcription factor concentration, so it will typically be the log transcription factor concentration what follows.",
            "So basically the rate of production is Portugal.",
            "Some nonlinear function of the log, transcription factor concentration, and then the gene itself decays."
        ],
        [
            "OK, So what the idea is in the Gaussian process framework is you put a Gaussian process prior over this log transcription factor concentration.",
            "Now if it wasn't a log and if he was linear, then the estimation problem becomes what that implies is that why is a Gaussian process so you get this joint Gaussian process distribution and we published on that with Magnus and Guido.",
            "At NIPS a few years, and more recently with Pai Gow.",
            "ETCB but this linear model said this is just F here and no G. But this linear model isn't really plausable because Gaussian process can go negative.",
            "So we want to place these priors over log concentrations.",
            "And indeed we believe in some kind of Michaelis Menten kinetics model, so the examples will be using in this talk how this form.",
            "So if you think that that's log concentration in their E to the power of that is just the concentration of the transcription transcription factor, so this is the Michaelis menten type kinetex situation with activation repression now.",
            "The work Nicholas is actually the way he's implemented these things that we're going to show.",
            "You can put any G you like in here.",
            "It's sort of completely generic, but for the examples you'll see that there will be of this type of function.",
            "Now if you're from a machine learning background, you also think of that as like a sigmoidal function in the log transcription factor.",
            "So I'll probably refer to that as a bit of a sigmoid."
        ],
        [
            "So what the idea is, we want to generalize that preliminary work with nonlinearities to a Gaussian process framework, which allows us to do multiple, possibly interacting transcription factors.",
            "So now it's not placing any constraints on G and there's multiple FS we're interested in, the non."
        ],
        [
            "In your response case.",
            "So the general form of the equation is.",
            "We've got several transcription factors.",
            "We're only considering linear decay, although you could extend that to be some sort of nasty function of Y, but it would make some of the inference more complicated.",
            "The solution of the differential equation would be more difficult, but so we're considering forms like this where G is sort of any nonlinear function of all these interacting transcription factors.",
            "Um, well, what we parameterize that by is typically a vector W, so this is following work by Eric Meola, Cnis, who did I think back in the early 90s, so proposed models like the one we're going to show, which were inspired by sort of neural network type models.",
            "So W was the weights applied to each of these transcription factors, and you'll see that in a moment, and then we also tend to use a bias for each gene.",
            "So I go back, Yep."
        ],
        [
            "OK, so the basic idea is for the experiment will for the results will show.",
            "First is that you take all these functions and you sum them linearly plus some offset and then you put that in the argument of the exponential.",
            "This is very neural network ish and that's it.",
            "But it's a model that's even used for example in blaster development in Drosophila, and the idea is that the weights, if their positive, then the transcription factor is activating, and if these weights are negative then the transcription factor is repressing.",
            "And this has been used to sort of this model like this is being used for Gap gene network for deriving Watson Activator and what suppress repress.",
            "But as I say, you could use.",
            "This just happens to be the one that Nicholas is implemented so far, but he's implemented the code generally so you could replace this G with any sort of function."
        ],
        [
            "Like So what we have is we're going to take a Bayesian approach, so the likelihood.",
            "This long thing here has the data an independence across the data.",
            "Given these functions and all the parameters of the differential equation and these weights which indicate activation repression plus a noise which is gene specific.",
            "So each gene has its own noise level.",
            "And the idea is, well, model microarray data with a system like this and will try and infer the concentrations of the transcription factors.",
            "F will also infer distributions over all these kinetic parameters using Bayesian inference, so.",
            "Here, Nicholas is used lognormal distribution over these guys, 'cause they're all positive.",
            "These guys here is the Gaussian process prior, so we place the Gaussian process as we saw in the previous talk right over these guys, these FS.",
            "NWS we take real values.",
            "We just faced Gaussian priors over those or Michalis place, Gaussian priors over those, and then the noise variance in the length scales of the Gaussian process.",
            "So this is the parameter that controls that width we saw in the previous talk and the noisy place gamma priors, inverse gamma priors over those guys there, so it's a full sort of.",
            "Gaussian system now what I like up to this stage, it's all very easy.",
            "You just say we're going to.",
            "This is our model and we're going to place prize over that prize over that prize over that and so and so forth.",
            "And then you find that extremely capable pose."
        ],
        [
            "Stock.",
            "And you ask them to implement a sampling algorithm that will actually work in this situation.",
            "So that's what we have in in Michalis, and indeed even just a sampling of F is very complicated to do because over as you sample these F you sample them continuously across time, not just at the time points we've observed data.",
            "So you get extremely high correlations between the samples.",
            "You have 4F.",
            "So what MIC are listed first when he first arrived and this is published at NIPS is a sampling framework for this situation.",
            "So just to deal with?",
            "So that's a paper in itself.",
            "For the rest, you could ask me how he's done it an I wouldn't know at all.",
            "He's basically sampling, going through sampling F, then sampling the kinetic parameters, sampling the interaction weights, sampling the gene specific noise variance, and sampling the length scale.",
            "So this is like where all the work is, I mean."
        ],
        [
            "The previous stuff.",
            "This is what you get to say as a supervisor.",
            "Why don't we do that?"
        ],
        [
            "And then Michalis spends an enormous amount of time trying to get it all working, so that's all all the work in one's life."
        ],
        [
            "Really.",
            "So learning the real transcription factors that produced the gene expression is not actually that easy because of identifiability problems in both the parameter space and in the transcription factor concentration space.",
            "So we've actually made the identifiability quite a lot worse by.",
            "Putting in all these additional latent factors, I don't know if how many people are familiar with the in's and outs of PCA principal component analysis, but principal component analysis is like a super simplification of this system with no dynamics.",
            "An linear setup so and so forth.",
            "So you've got.",
            "In that case there the transcription factors are like the latent variables in principle component, axis and people should know that there's a complete rotational invariants to what those latent variables should be.",
            "Those sort of rotational invariances also come up here, because, particularly in."
        ],
        [
            "This some here I could rotate all these F's in latent space and just apply rotation on the W. So I get a bunch of different transcription factors and I get a bunch of different WS.",
            "Now you might hope that because the length scales of these apps is slightly different in each case that somehow you would be able to identify separately all these things.",
            "But even if that was so, I'd say it's sort of it.",
            "Definitely what it will cause is strong correlations between F&W.",
            "In the inference over these guys, so even if it is identifiable, you get very strong correlations between F&W."
        ],
        [
            "And because currently the sampling is being done, F&W are being done separately, you have to iterate between these things a lot to get to the right solution.",
            "So that's a difficulty.",
            "I think that's the main difficulty with this sort of approach, that sort of."
        ],
        [
            "Identifiability problem.",
            "So what the solution was in hope in biology.",
            "Very often it's the case that you might have side information, so the side information is in the form of a binary matrix X which indicates for example, some knowledge about which transcription factors bind to which genes.",
            "Basically it says that some of these W some of these weights are zero, and by using that side information, the hope is that you can then identify the system, and that's quite common.",
            "It might be from other biological experiments, and indeed one of the things with Carla suggests here.",
            "Is that you could even consider X being drawn from some probability distribution that expresses prior beliefs about the system, so there's lots of sort of scope there to use.",
            "Chip, chip or chip seek or whatever to try and constrain this matrix.",
            "So that's why."
        ],
        [
            "Way forward, so here's a sort of toy problem that illustrates that so toy problem with two transcription factors and 20 jeans and what we're assuming is we have deterministic side information for eight out of the 20 jeans.",
            "So we know for eight of those jeans we know which of these weights are zero for these jeans, so we don't know the value of the weights that are on, but we know which weights are constrained to be 0.",
            "So we are inferring the weights on, but then some of them were constraining to be 0.",
            "So we're going to assume also that the initial conditions are the differential equation of zero.",
            "We could sample over those at T0 and we have to infer over W 25 parameters which are indicating the activity of each of these genes activation."
        ],
        [
            "Repression.",
            "So here's the inference of the two transcription factors.",
            "I should say the log concentration of the transcription factors.",
            "Well, I hope it's log 'cause they're negative.",
            "So it's coming.",
            "I think they're coming from zero at 0, but he's starting from time one here, so that's why so it must be off the graph here.",
            "But what's interesting is you see this inference.",
            "This is the blue.",
            "This is the in Ferd Gaussian process posterior over the log concentrations and what you see in red is the true actual value that Michalis used to generate the data.",
            "So you can get identifiability problems if the length scale of these things are sort of similar.",
            "So here's the other system here and you see there both towards the lower end of the system."
        ],
        [
            "What's also interesting is looking at the fit to the artificial genes, so this is a series of jeans that were artificially generated is coming from that data with different noise variances.",
            "The idea is trying to make the data as realistic as possible, but you can see the fits are very good.",
            "The fits are quite.",
            "Soap."
        ],
        [
            "Size."
        ],
        [
            "Um?"
        ],
        [
            "Give"
        ],
        [
            "The fact that you're actually often that's part of the issue with the identifiability that you can get very nice, precise fits, but there's lots of even with the side information.",
            "There's lots of flexibility in these latent processes to what can generate them."
        ],
        [
            "I."
        ],
        [
            "Here for example as well here.",
            "So here's another issue with the flexibility.",
            "These are the error bars on the Basel rates, so he's doing sampling over everything in the black is the true value Michalis used to generate the data, and the blue of the estimate, so they're very broad.",
            "I mean, the black always falls pretty much part from one time here, but you would expect it occasionally to fall out within the blue estimates of the error bars, so very large variance."
        ],
        [
            "Um?",
            "Sensitivities you get the same sort of thing.",
            "That's interesting.",
            "I mailed Nicholas about that, but I don't fully understand if that's an issue there.",
            "This sensitivity is the only one that fall significantly outside the infer dare above all the rest fall inside, but look at, you know.",
            "Well, I don't like to say identifiability problem because if you're Bayesian it's an identifiability opportunity.",
            "So if you have identifiability issues then are doing Bayesian approaches.",
            "You can still learn something about the structure of the data."
        ],
        [
            "And that's sort of what comes for his the decays, similar to the story."
        ],
        [
            "But principally, I think you look at this plot.",
            "This is for if we are interested in whether these jeans are activating or repressing what this plot is showing is not the actual ground truth, plus our estimates of the parameters or mcalisters estimates of the parameters with error bars.",
            "But what these error bars still allow you to do is give a probability that a gene is activating or repressing.",
            "So for example in this case here all the mass of distribution is in activation, so you can be pretty confident that that's an activating gene where it's in this example here.",
            "It seems that the information isn't there to determine, even though it thinks it's a repressor.",
            "The variance is quite large, so if you look at their confidence estimates that that's the repressor, it's only 80% likely to be a repressor, so you still get this additional information and it's associated with uncertainties despite the lack of identifiability.",
            "So I wouldn't say that the you know it's a dead issue because of identifiability problems.",
            "I don't see those as a major issue."
        ],
        [
            "What's far more important actually is well, it comes onto it.",
            "After this example is having a system in which your sample does converge."
        ],
        [
            "Reasonable time does mix, so this is Spelman used data, so this is real data.",
            "Similar situation, emailed the call is about these and it seems he's not plotting the noise values for these guys.",
            "He's just probably uncertainty in the function, so that's why you get lots of errors.",
            "Things outside the error bars here, but look it's capturing very nicely some of these.",
            "These jeans from the East Spellman data and then you."
        ],
        [
            "Even sort of.",
            "So I can't remember how many used in this experiment, but it appears that it's least 16."
        ],
        [
            "And then there's the inference over what the transcription factor profiles were.",
            "So there's still a lot of uncertainty.",
            "For example, about this speak about what was going on, and we'll see if we don't have ground truth here to compare with."
        ],
        [
            "And the last example, one of the things, so we're not convinced that the model we showed you first is the right model.",
            "So here Michalis is just doing experiments with a slightly different activation model.",
            "So sorry this is a reformulation of activation model.",
            "In the previous case, but."
        ],
        [
            "The next slide he's suggesting using this activation model where you sum across the exponentials here and here."
        ],
        [
            "Rather than as in the previous case, where effectively we were multiplying across the exponentials raised to the power of these W."
        ],
        [
            "So it's just a."
        ],
        [
            "Current model, but I think the interesting."
        ],
        [
            "About this model and I'm short on time."
        ],
        [
            "So I'll skip to.",
            "The interesting thing is.",
            "Here, for example, you're getting issues.",
            "I think I haven't discussed this in detail with Michalis and this is a recent result.",
            "But that's an identifiability.",
            "That's a problem with the sample are not getting towards the mode in the correct time I believe, and Nicholas might disagree."
        ],
        [
            "With me and the effect it has is you misspeaks like this so you know.",
            "I believe the model could model that data correctly if you gave enough time to sample, But in this."
        ],
        [
            "Case one of the transcription factors seems to be off, so this artificial data again, so we know we know the truth, so this is going to be a problem for modeling of this type is not a problem, that's an identifiability issue in the base case, but it's a problem if you're sampling scheme doesn't explore the space well enough to find this sort of."
        ],
        [
            "The modes and explore them well."
        ],
        [
            "OK, and so in this case if he was doing for that artificial example, you got 24% error on classifying which was active."
        ],
        [
            "Cases in which represses.",
            "OK, so basically that's Nicholas is work he's been working on extending the Zodi models to multiple transcription factors in the latent space and it's ongoing.",
            "I think then the issues are to do with how you make sure that the sampler.",
            "Well, you get around these identifiability issues and you cite information to improve the convergence of the sample.",
            "Thank you very much.",
            "I mean, you see that can be interacting, but you can't model something like a hand getting things because if you look at the surface where your overall activation is equal to half hyperplane Maps pace.",
            "So yeah, the key point is that you can put any nonlinearity in there you like, so there's no need, so that's like so in neural network terms from 10 years ago.",
            "That's like a one layer perceptron.",
            "Multilayer perception allows much more complicated, and it's not, I mean.",
            "The more you put in there, the harder it's going to be to sample, but there's no particular reason that you can't put any function in there you like.",
            "Yeah, but that's these examples.",
            "So Michalis is written it as I said at the beginning to allow any G to be put in that you like, but given the difficulties even with a simple G like this, you know seems sensible not to do very complicated GS yet.",
            "The man behind the pillar.",
            "Think about trying to make the model identifiable as strong stance while the dent ifying music scenes.",
            "Yeah, he wanted that we had kept talking these meetings where he kept saying oh it's not something I said.",
            "I put more site information in and so that was what the idea was to try and put more site information in.",
            "But also we wanted to keep it realistic.",
            "So if you put like full connectivity information in then it would work for sure.",
            "But then we wouldn't have gained game much.",
            "So yeah, that's the sort of thing he's considering.",
            "Yeah, well and there's other potential things you could do.",
            "You might have some observations of some of those.",
            "Those missing guys you know.",
            "You might have some inputs to those missing guides, so with anti using the audience, he's working on ideas where you've got a model of translation to help identify those things.",
            "So there's lots of additional things you can do.",
            "Commercial discussion after mass in two or three years ago, your party instead of using CNC, but would only variation roots and I realize that you can see I have not used MCMC at all.",
            "Car listed it.",
            "I don't touch MCMC.",
            "No.",
            "I think in fact it was a lovely.",
            "It was a good discussion that discussions see that was in Ireland in a pub drinking Guinness.",
            "So on I think that's I've changed my mind on that.",
            "I think for machine learning type models where it's predicted variational methods may get you there, but I mean a lot of some of the work that changed my mind on this was work by Mark.",
            "On these ranking of models I'm sort of now firmly convinced of the need to sample.",
            "In the need for Mccollister sample.",
            "Do you monitor or monitor the mixing of yourself?",
            "Though he does all those sort of plotty things and.",
            "And this is one of the issues.",
            "What is one of the reasons why I claimed to duck that we should use variational methods?",
            "But you definitely couldn't use him in this case, so he does.",
            "He does look at that.",
            "I can't give you a detailed answer on that because I don't remember.",
            "Great."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So next week is new Lawrence stepping in for Michael Theseus and he's going to talk to us about estimation of multiple transcription factor activities using ODS and motion process.",
                    "label": 1
                },
                {
                    "sent": "Thanks Martina, so yeah.",
                    "label": 0
                },
                {
                    "sent": "As Martino said, this is Nicholas is work.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately Carlos has a family illness and is back in Greece so you couldn't make it but he sends his regards but basically everything I'm presenting is it's from a project at Nicholas is on with Magnussen I as investigators but particularly the sampling ideas that are used are all coming from the carlis.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's nice.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Previous talk was about Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "'cause that's what that will help me.",
                    "label": 0
                },
                {
                    "sent": "Well, you'll have a sense of what a Gaussian process function looks like if you've not seen them before.",
                    "label": 0
                },
                {
                    "sent": "So review of the talk.",
                    "label": 0
                },
                {
                    "sent": "These McCarthy slides that he sent.",
                    "label": 0
                },
                {
                    "sent": "So if I want to get surprised about something, it's 'cause I've not read them properly.",
                    "label": 0
                },
                {
                    "sent": "So learning a single TF with Gaussian processes is the sort of initial motivation where we're going, but the contribution of this talk is the directions we're moving in with multiple transcription factors.",
                    "label": 1
                },
                {
                    "sent": "And we've got some sort of ongoing experiments and conclusions about where you are with this sort of model, and we think there's a lot of promise, but we don't think it's quite there.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yet.",
                    "label": 0
                },
                {
                    "sent": "So the idea is you're interested in transcriptional regulation.",
                    "label": 1
                },
                {
                    "sent": "And you've got data, which is why, as in the previous talk of engines, at times, what we're interested in modeling is the dynamics of the expression of the set of genes, and we want to, for a single transcription factor case, we want to infer a single transcription factor that is governing how those genes are expressed.",
                    "label": 1
                },
                {
                    "sent": "And the way we're going to do that is we're going to follow these other guys here and use a differential equation model.",
                    "label": 0
                },
                {
                    "sent": "So the rate of production of genes concentration is given by some basal transcription rate.",
                    "label": 0
                },
                {
                    "sent": "Plus some sensitivity to nonlinear function of the gene expression F of T or in fact will typically be using FT is the log, so the transcription factor concentration, so it will typically be the log transcription factor concentration what follows.",
                    "label": 0
                },
                {
                    "sent": "So basically the rate of production is Portugal.",
                    "label": 0
                },
                {
                    "sent": "Some nonlinear function of the log, transcription factor concentration, and then the gene itself decays.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what the idea is in the Gaussian process framework is you put a Gaussian process prior over this log transcription factor concentration.",
                    "label": 0
                },
                {
                    "sent": "Now if it wasn't a log and if he was linear, then the estimation problem becomes what that implies is that why is a Gaussian process so you get this joint Gaussian process distribution and we published on that with Magnus and Guido.",
                    "label": 1
                },
                {
                    "sent": "At NIPS a few years, and more recently with Pai Gow.",
                    "label": 1
                },
                {
                    "sent": "ETCB but this linear model said this is just F here and no G. But this linear model isn't really plausable because Gaussian process can go negative.",
                    "label": 0
                },
                {
                    "sent": "So we want to place these priors over log concentrations.",
                    "label": 0
                },
                {
                    "sent": "And indeed we believe in some kind of Michaelis Menten kinetics model, so the examples will be using in this talk how this form.",
                    "label": 0
                },
                {
                    "sent": "So if you think that that's log concentration in their E to the power of that is just the concentration of the transcription transcription factor, so this is the Michaelis menten type kinetex situation with activation repression now.",
                    "label": 0
                },
                {
                    "sent": "The work Nicholas is actually the way he's implemented these things that we're going to show.",
                    "label": 0
                },
                {
                    "sent": "You can put any G you like in here.",
                    "label": 0
                },
                {
                    "sent": "It's sort of completely generic, but for the examples you'll see that there will be of this type of function.",
                    "label": 1
                },
                {
                    "sent": "Now if you're from a machine learning background, you also think of that as like a sigmoidal function in the log transcription factor.",
                    "label": 0
                },
                {
                    "sent": "So I'll probably refer to that as a bit of a sigmoid.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what the idea is, we want to generalize that preliminary work with nonlinearities to a Gaussian process framework, which allows us to do multiple, possibly interacting transcription factors.",
                    "label": 0
                },
                {
                    "sent": "So now it's not placing any constraints on G and there's multiple FS we're interested in, the non.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In your response case.",
                    "label": 0
                },
                {
                    "sent": "So the general form of the equation is.",
                    "label": 1
                },
                {
                    "sent": "We've got several transcription factors.",
                    "label": 0
                },
                {
                    "sent": "We're only considering linear decay, although you could extend that to be some sort of nasty function of Y, but it would make some of the inference more complicated.",
                    "label": 0
                },
                {
                    "sent": "The solution of the differential equation would be more difficult, but so we're considering forms like this where G is sort of any nonlinear function of all these interacting transcription factors.",
                    "label": 0
                },
                {
                    "sent": "Um, well, what we parameterize that by is typically a vector W, so this is following work by Eric Meola, Cnis, who did I think back in the early 90s, so proposed models like the one we're going to show, which were inspired by sort of neural network type models.",
                    "label": 1
                },
                {
                    "sent": "So W was the weights applied to each of these transcription factors, and you'll see that in a moment, and then we also tend to use a bias for each gene.",
                    "label": 0
                },
                {
                    "sent": "So I go back, Yep.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the basic idea is for the experiment will for the results will show.",
                    "label": 0
                },
                {
                    "sent": "First is that you take all these functions and you sum them linearly plus some offset and then you put that in the argument of the exponential.",
                    "label": 0
                },
                {
                    "sent": "This is very neural network ish and that's it.",
                    "label": 0
                },
                {
                    "sent": "But it's a model that's even used for example in blaster development in Drosophila, and the idea is that the weights, if their positive, then the transcription factor is activating, and if these weights are negative then the transcription factor is repressing.",
                    "label": 0
                },
                {
                    "sent": "And this has been used to sort of this model like this is being used for Gap gene network for deriving Watson Activator and what suppress repress.",
                    "label": 0
                },
                {
                    "sent": "But as I say, you could use.",
                    "label": 0
                },
                {
                    "sent": "This just happens to be the one that Nicholas is implemented so far, but he's implemented the code generally so you could replace this G with any sort of function.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Like So what we have is we're going to take a Bayesian approach, so the likelihood.",
                    "label": 0
                },
                {
                    "sent": "This long thing here has the data an independence across the data.",
                    "label": 0
                },
                {
                    "sent": "Given these functions and all the parameters of the differential equation and these weights which indicate activation repression plus a noise which is gene specific.",
                    "label": 0
                },
                {
                    "sent": "So each gene has its own noise level.",
                    "label": 0
                },
                {
                    "sent": "And the idea is, well, model microarray data with a system like this and will try and infer the concentrations of the transcription factors.",
                    "label": 0
                },
                {
                    "sent": "F will also infer distributions over all these kinetic parameters using Bayesian inference, so.",
                    "label": 0
                },
                {
                    "sent": "Here, Nicholas is used lognormal distribution over these guys, 'cause they're all positive.",
                    "label": 0
                },
                {
                    "sent": "These guys here is the Gaussian process prior, so we place the Gaussian process as we saw in the previous talk right over these guys, these FS.",
                    "label": 0
                },
                {
                    "sent": "NWS we take real values.",
                    "label": 0
                },
                {
                    "sent": "We just faced Gaussian priors over those or Michalis place, Gaussian priors over those, and then the noise variance in the length scales of the Gaussian process.",
                    "label": 1
                },
                {
                    "sent": "So this is the parameter that controls that width we saw in the previous talk and the noisy place gamma priors, inverse gamma priors over those guys there, so it's a full sort of.",
                    "label": 0
                },
                {
                    "sent": "Gaussian system now what I like up to this stage, it's all very easy.",
                    "label": 0
                },
                {
                    "sent": "You just say we're going to.",
                    "label": 0
                },
                {
                    "sent": "This is our model and we're going to place prize over that prize over that prize over that and so and so forth.",
                    "label": 0
                },
                {
                    "sent": "And then you find that extremely capable pose.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Stock.",
                    "label": 0
                },
                {
                    "sent": "And you ask them to implement a sampling algorithm that will actually work in this situation.",
                    "label": 0
                },
                {
                    "sent": "So that's what we have in in Michalis, and indeed even just a sampling of F is very complicated to do because over as you sample these F you sample them continuously across time, not just at the time points we've observed data.",
                    "label": 0
                },
                {
                    "sent": "So you get extremely high correlations between the samples.",
                    "label": 0
                },
                {
                    "sent": "You have 4F.",
                    "label": 0
                },
                {
                    "sent": "So what MIC are listed first when he first arrived and this is published at NIPS is a sampling framework for this situation.",
                    "label": 0
                },
                {
                    "sent": "So just to deal with?",
                    "label": 0
                },
                {
                    "sent": "So that's a paper in itself.",
                    "label": 0
                },
                {
                    "sent": "For the rest, you could ask me how he's done it an I wouldn't know at all.",
                    "label": 0
                },
                {
                    "sent": "He's basically sampling, going through sampling F, then sampling the kinetic parameters, sampling the interaction weights, sampling the gene specific noise variance, and sampling the length scale.",
                    "label": 1
                },
                {
                    "sent": "So this is like where all the work is, I mean.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The previous stuff.",
                    "label": 0
                },
                {
                    "sent": "This is what you get to say as a supervisor.",
                    "label": 0
                },
                {
                    "sent": "Why don't we do that?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then Michalis spends an enormous amount of time trying to get it all working, so that's all all the work in one's life.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Really.",
                    "label": 0
                },
                {
                    "sent": "So learning the real transcription factors that produced the gene expression is not actually that easy because of identifiability problems in both the parameter space and in the transcription factor concentration space.",
                    "label": 1
                },
                {
                    "sent": "So we've actually made the identifiability quite a lot worse by.",
                    "label": 0
                },
                {
                    "sent": "Putting in all these additional latent factors, I don't know if how many people are familiar with the in's and outs of PCA principal component analysis, but principal component analysis is like a super simplification of this system with no dynamics.",
                    "label": 0
                },
                {
                    "sent": "An linear setup so and so forth.",
                    "label": 0
                },
                {
                    "sent": "So you've got.",
                    "label": 0
                },
                {
                    "sent": "In that case there the transcription factors are like the latent variables in principle component, axis and people should know that there's a complete rotational invariants to what those latent variables should be.",
                    "label": 0
                },
                {
                    "sent": "Those sort of rotational invariances also come up here, because, particularly in.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This some here I could rotate all these F's in latent space and just apply rotation on the W. So I get a bunch of different transcription factors and I get a bunch of different WS.",
                    "label": 0
                },
                {
                    "sent": "Now you might hope that because the length scales of these apps is slightly different in each case that somehow you would be able to identify separately all these things.",
                    "label": 0
                },
                {
                    "sent": "But even if that was so, I'd say it's sort of it.",
                    "label": 0
                },
                {
                    "sent": "Definitely what it will cause is strong correlations between F&W.",
                    "label": 0
                },
                {
                    "sent": "In the inference over these guys, so even if it is identifiable, you get very strong correlations between F&W.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And because currently the sampling is being done, F&W are being done separately, you have to iterate between these things a lot to get to the right solution.",
                    "label": 0
                },
                {
                    "sent": "So that's a difficulty.",
                    "label": 0
                },
                {
                    "sent": "I think that's the main difficulty with this sort of approach, that sort of.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Identifiability problem.",
                    "label": 0
                },
                {
                    "sent": "So what the solution was in hope in biology.",
                    "label": 0
                },
                {
                    "sent": "Very often it's the case that you might have side information, so the side information is in the form of a binary matrix X which indicates for example, some knowledge about which transcription factors bind to which genes.",
                    "label": 1
                },
                {
                    "sent": "Basically it says that some of these W some of these weights are zero, and by using that side information, the hope is that you can then identify the system, and that's quite common.",
                    "label": 0
                },
                {
                    "sent": "It might be from other biological experiments, and indeed one of the things with Carla suggests here.",
                    "label": 1
                },
                {
                    "sent": "Is that you could even consider X being drawn from some probability distribution that expresses prior beliefs about the system, so there's lots of sort of scope there to use.",
                    "label": 0
                },
                {
                    "sent": "Chip, chip or chip seek or whatever to try and constrain this matrix.",
                    "label": 0
                },
                {
                    "sent": "So that's why.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Way forward, so here's a sort of toy problem that illustrates that so toy problem with two transcription factors and 20 jeans and what we're assuming is we have deterministic side information for eight out of the 20 jeans.",
                    "label": 1
                },
                {
                    "sent": "So we know for eight of those jeans we know which of these weights are zero for these jeans, so we don't know the value of the weights that are on, but we know which weights are constrained to be 0.",
                    "label": 1
                },
                {
                    "sent": "So we are inferring the weights on, but then some of them were constraining to be 0.",
                    "label": 0
                },
                {
                    "sent": "So we're going to assume also that the initial conditions are the differential equation of zero.",
                    "label": 0
                },
                {
                    "sent": "We could sample over those at T0 and we have to infer over W 25 parameters which are indicating the activity of each of these genes activation.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Repression.",
                    "label": 0
                },
                {
                    "sent": "So here's the inference of the two transcription factors.",
                    "label": 1
                },
                {
                    "sent": "I should say the log concentration of the transcription factors.",
                    "label": 0
                },
                {
                    "sent": "Well, I hope it's log 'cause they're negative.",
                    "label": 0
                },
                {
                    "sent": "So it's coming.",
                    "label": 0
                },
                {
                    "sent": "I think they're coming from zero at 0, but he's starting from time one here, so that's why so it must be off the graph here.",
                    "label": 0
                },
                {
                    "sent": "But what's interesting is you see this inference.",
                    "label": 0
                },
                {
                    "sent": "This is the blue.",
                    "label": 0
                },
                {
                    "sent": "This is the in Ferd Gaussian process posterior over the log concentrations and what you see in red is the true actual value that Michalis used to generate the data.",
                    "label": 1
                },
                {
                    "sent": "So you can get identifiability problems if the length scale of these things are sort of similar.",
                    "label": 0
                },
                {
                    "sent": "So here's the other system here and you see there both towards the lower end of the system.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What's also interesting is looking at the fit to the artificial genes, so this is a series of jeans that were artificially generated is coming from that data with different noise variances.",
                    "label": 0
                },
                {
                    "sent": "The idea is trying to make the data as realistic as possible, but you can see the fits are very good.",
                    "label": 0
                },
                {
                    "sent": "The fits are quite.",
                    "label": 0
                },
                {
                    "sent": "Soap.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Size.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Give",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The fact that you're actually often that's part of the issue with the identifiability that you can get very nice, precise fits, but there's lots of even with the side information.",
                    "label": 0
                },
                {
                    "sent": "There's lots of flexibility in these latent processes to what can generate them.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here for example as well here.",
                    "label": 0
                },
                {
                    "sent": "So here's another issue with the flexibility.",
                    "label": 0
                },
                {
                    "sent": "These are the error bars on the Basel rates, so he's doing sampling over everything in the black is the true value Michalis used to generate the data, and the blue of the estimate, so they're very broad.",
                    "label": 0
                },
                {
                    "sent": "I mean, the black always falls pretty much part from one time here, but you would expect it occasionally to fall out within the blue estimates of the error bars, so very large variance.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Sensitivities you get the same sort of thing.",
                    "label": 0
                },
                {
                    "sent": "That's interesting.",
                    "label": 0
                },
                {
                    "sent": "I mailed Nicholas about that, but I don't fully understand if that's an issue there.",
                    "label": 0
                },
                {
                    "sent": "This sensitivity is the only one that fall significantly outside the infer dare above all the rest fall inside, but look at, you know.",
                    "label": 0
                },
                {
                    "sent": "Well, I don't like to say identifiability problem because if you're Bayesian it's an identifiability opportunity.",
                    "label": 0
                },
                {
                    "sent": "So if you have identifiability issues then are doing Bayesian approaches.",
                    "label": 0
                },
                {
                    "sent": "You can still learn something about the structure of the data.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's sort of what comes for his the decays, similar to the story.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But principally, I think you look at this plot.",
                    "label": 0
                },
                {
                    "sent": "This is for if we are interested in whether these jeans are activating or repressing what this plot is showing is not the actual ground truth, plus our estimates of the parameters or mcalisters estimates of the parameters with error bars.",
                    "label": 0
                },
                {
                    "sent": "But what these error bars still allow you to do is give a probability that a gene is activating or repressing.",
                    "label": 0
                },
                {
                    "sent": "So for example in this case here all the mass of distribution is in activation, so you can be pretty confident that that's an activating gene where it's in this example here.",
                    "label": 0
                },
                {
                    "sent": "It seems that the information isn't there to determine, even though it thinks it's a repressor.",
                    "label": 0
                },
                {
                    "sent": "The variance is quite large, so if you look at their confidence estimates that that's the repressor, it's only 80% likely to be a repressor, so you still get this additional information and it's associated with uncertainties despite the lack of identifiability.",
                    "label": 0
                },
                {
                    "sent": "So I wouldn't say that the you know it's a dead issue because of identifiability problems.",
                    "label": 0
                },
                {
                    "sent": "I don't see those as a major issue.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What's far more important actually is well, it comes onto it.",
                    "label": 0
                },
                {
                    "sent": "After this example is having a system in which your sample does converge.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Reasonable time does mix, so this is Spelman used data, so this is real data.",
                    "label": 0
                },
                {
                    "sent": "Similar situation, emailed the call is about these and it seems he's not plotting the noise values for these guys.",
                    "label": 0
                },
                {
                    "sent": "He's just probably uncertainty in the function, so that's why you get lots of errors.",
                    "label": 0
                },
                {
                    "sent": "Things outside the error bars here, but look it's capturing very nicely some of these.",
                    "label": 0
                },
                {
                    "sent": "These jeans from the East Spellman data and then you.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Even sort of.",
                    "label": 0
                },
                {
                    "sent": "So I can't remember how many used in this experiment, but it appears that it's least 16.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then there's the inference over what the transcription factor profiles were.",
                    "label": 0
                },
                {
                    "sent": "So there's still a lot of uncertainty.",
                    "label": 0
                },
                {
                    "sent": "For example, about this speak about what was going on, and we'll see if we don't have ground truth here to compare with.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the last example, one of the things, so we're not convinced that the model we showed you first is the right model.",
                    "label": 0
                },
                {
                    "sent": "So here Michalis is just doing experiments with a slightly different activation model.",
                    "label": 0
                },
                {
                    "sent": "So sorry this is a reformulation of activation model.",
                    "label": 0
                },
                {
                    "sent": "In the previous case, but.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The next slide he's suggesting using this activation model where you sum across the exponentials here and here.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rather than as in the previous case, where effectively we were multiplying across the exponentials raised to the power of these W.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's just a.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Current model, but I think the interesting.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About this model and I'm short on time.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'll skip to.",
                    "label": 0
                },
                {
                    "sent": "The interesting thing is.",
                    "label": 0
                },
                {
                    "sent": "Here, for example, you're getting issues.",
                    "label": 0
                },
                {
                    "sent": "I think I haven't discussed this in detail with Michalis and this is a recent result.",
                    "label": 0
                },
                {
                    "sent": "But that's an identifiability.",
                    "label": 0
                },
                {
                    "sent": "That's a problem with the sample are not getting towards the mode in the correct time I believe, and Nicholas might disagree.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With me and the effect it has is you misspeaks like this so you know.",
                    "label": 0
                },
                {
                    "sent": "I believe the model could model that data correctly if you gave enough time to sample, But in this.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Case one of the transcription factors seems to be off, so this artificial data again, so we know we know the truth, so this is going to be a problem for modeling of this type is not a problem, that's an identifiability issue in the base case, but it's a problem if you're sampling scheme doesn't explore the space well enough to find this sort of.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The modes and explore them well.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and so in this case if he was doing for that artificial example, you got 24% error on classifying which was active.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Cases in which represses.",
                    "label": 0
                },
                {
                    "sent": "OK, so basically that's Nicholas is work he's been working on extending the Zodi models to multiple transcription factors in the latent space and it's ongoing.",
                    "label": 1
                },
                {
                    "sent": "I think then the issues are to do with how you make sure that the sampler.",
                    "label": 0
                },
                {
                    "sent": "Well, you get around these identifiability issues and you cite information to improve the convergence of the sample.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "I mean, you see that can be interacting, but you can't model something like a hand getting things because if you look at the surface where your overall activation is equal to half hyperplane Maps pace.",
                    "label": 0
                },
                {
                    "sent": "So yeah, the key point is that you can put any nonlinearity in there you like, so there's no need, so that's like so in neural network terms from 10 years ago.",
                    "label": 0
                },
                {
                    "sent": "That's like a one layer perceptron.",
                    "label": 0
                },
                {
                    "sent": "Multilayer perception allows much more complicated, and it's not, I mean.",
                    "label": 0
                },
                {
                    "sent": "The more you put in there, the harder it's going to be to sample, but there's no particular reason that you can't put any function in there you like.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but that's these examples.",
                    "label": 0
                },
                {
                    "sent": "So Michalis is written it as I said at the beginning to allow any G to be put in that you like, but given the difficulties even with a simple G like this, you know seems sensible not to do very complicated GS yet.",
                    "label": 0
                },
                {
                    "sent": "The man behind the pillar.",
                    "label": 0
                },
                {
                    "sent": "Think about trying to make the model identifiable as strong stance while the dent ifying music scenes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, he wanted that we had kept talking these meetings where he kept saying oh it's not something I said.",
                    "label": 0
                },
                {
                    "sent": "I put more site information in and so that was what the idea was to try and put more site information in.",
                    "label": 0
                },
                {
                    "sent": "But also we wanted to keep it realistic.",
                    "label": 0
                },
                {
                    "sent": "So if you put like full connectivity information in then it would work for sure.",
                    "label": 0
                },
                {
                    "sent": "But then we wouldn't have gained game much.",
                    "label": 0
                },
                {
                    "sent": "So yeah, that's the sort of thing he's considering.",
                    "label": 0
                },
                {
                    "sent": "Yeah, well and there's other potential things you could do.",
                    "label": 0
                },
                {
                    "sent": "You might have some observations of some of those.",
                    "label": 0
                },
                {
                    "sent": "Those missing guys you know.",
                    "label": 0
                },
                {
                    "sent": "You might have some inputs to those missing guides, so with anti using the audience, he's working on ideas where you've got a model of translation to help identify those things.",
                    "label": 0
                },
                {
                    "sent": "So there's lots of additional things you can do.",
                    "label": 0
                },
                {
                    "sent": "Commercial discussion after mass in two or three years ago, your party instead of using CNC, but would only variation roots and I realize that you can see I have not used MCMC at all.",
                    "label": 0
                },
                {
                    "sent": "Car listed it.",
                    "label": 0
                },
                {
                    "sent": "I don't touch MCMC.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "I think in fact it was a lovely.",
                    "label": 0
                },
                {
                    "sent": "It was a good discussion that discussions see that was in Ireland in a pub drinking Guinness.",
                    "label": 0
                },
                {
                    "sent": "So on I think that's I've changed my mind on that.",
                    "label": 0
                },
                {
                    "sent": "I think for machine learning type models where it's predicted variational methods may get you there, but I mean a lot of some of the work that changed my mind on this was work by Mark.",
                    "label": 0
                },
                {
                    "sent": "On these ranking of models I'm sort of now firmly convinced of the need to sample.",
                    "label": 0
                },
                {
                    "sent": "In the need for Mccollister sample.",
                    "label": 0
                },
                {
                    "sent": "Do you monitor or monitor the mixing of yourself?",
                    "label": 0
                },
                {
                    "sent": "Though he does all those sort of plotty things and.",
                    "label": 0
                },
                {
                    "sent": "And this is one of the issues.",
                    "label": 0
                },
                {
                    "sent": "What is one of the reasons why I claimed to duck that we should use variational methods?",
                    "label": 0
                },
                {
                    "sent": "But you definitely couldn't use him in this case, so he does.",
                    "label": 0
                },
                {
                    "sent": "He does look at that.",
                    "label": 0
                },
                {
                    "sent": "I can't give you a detailed answer on that because I don't remember.",
                    "label": 0
                },
                {
                    "sent": "Great.",
                    "label": 0
                }
            ]
        }
    }
}