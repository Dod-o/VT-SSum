{
    "id": "qxyh6pi6ftkhhlnim3zkq54arg3gqcga",
    "title": "Measuring Semantic Coherence of a Conversation",
    "info": {
        "author": [
            "Svitlana Vakulenko, Institute for Information Business, Vienna University of Economics and Business"
        ],
        "published": "Nov. 22, 2018",
        "recorded": "October 2018",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2018_vakulenko_measuring_coherence_conversatio/",
    "segmentation": [
        [
            "Thanks for coming to my presentation."
        ],
        [
            "So let's start with the first question.",
            "What do we mean by semantic coherence?",
            "So imagine at the lunch break you over here the following conversation.",
            "Oh, I think Monterrey is really great location for conference.",
            "Oh yeah, Florida has beautiful coastline.",
            "I really want to see the Eiffel Tower.",
            "So the interesting observation that all three sentences are grammatically correct.",
            "They just don't make sense when they're put in the same sequence, right?",
            "So if it blew up your mind, that was designed like this.",
            "And the intuition here that for the conversation to be interpretable or like comprehensible by humans, it means to have certain relations between the.",
            "Entities in the conversation.",
            "So in order to make to correct this example, only think."
        ],
        [
            "I I can do these away is just to replace the entities with the ones that actually have connections to each other, so it makes sense talking about California in context of Monterrey because Monterrey is located in California and Florida.",
            "So in our reality right?",
            "So that corresponds to our representation of the world that shared some common knowledge we have and then aquarium is in monitoring or de filter right?",
            "So it's some implicit background knowledge that.",
            "Participants of the conversations must share for all of them to accept this conversation to be sensible.",
            "Alright, so the question is how?"
        ],
        [
            "How do we make computers?",
            "Are computers able to understand this kind of intuition?",
            "We have about conversations and sensemaking.",
            "So what we propose here is to introduce the task of measuring semantic coherence by modeling as a classification task.",
            "So basically the output of the classifier is the score, so from zero to one, which we interpret as the coherence score, and this basically the.",
            "So we create two sets that we want to learn the to separate them.",
            "So this sounds making line here.",
            "Right, and the interesting parts of each of the dots is the conversation and the position of the conversation.",
            "The space is obviously conditioned on the background knowledge so nonsense.",
            "In this case, it doesn't mean necessarily.",
            "That is not true or not relevant, so I give you an example.",
            "If you work in another room and there is a chemistry conference going on for you, it might be basically not interpretable.",
            "Doesn't mean that it's not true, right?",
            "So it depends on the knowledge we possess an our goal here was to.",
            "Evaluate the knowledge models we have, whether we are able to use them to basically separate this correct or real conversations from this.",
            "Non nonsense making one.",
            "Anne."
        ],
        [
            "The so the other question is why is it useful?",
            "So this is sort of intermediate that there's just a reality.",
            "Check whether our knowledge models are useful.",
            "Whether we can use them so the applications can be in conversation analysis.",
            "So imagine you record the business meeting where we have maybe several dialogues at the same time, and you want to separate them to to figure out who's actually talking to whom and maybe also to segment it based on the topics that detect, at which point you actually start talking about different things.",
            "And ultimately, we want computers to be able to be part of this conversation.",
            "For this, they need to understand the context of the conversation to be able to generate appropriate response.",
            "Alright."
        ],
        [
            "So this is a set of our contributions reclaiming this paper.",
            "The first one, the task we introduced.",
            "And then we also described how to create the benchmark for scalable experiments for this task and we evaluate a set of initial approaches on this benchmark."
        ],
        [
            "So the benchmark we took the existing data set of, so it's one of the biggest datasets I've used it also because it's actually extracted from public chat on Ubuntu Support Forum, and then I think about it is also because it's full of this computer science terminology.",
            "So boom, two related, all the software tools which have many entities in DPM.",
            "That's why we took Peter Week data converted to AC team, so that's our knowledge graph representation.",
            "We took knowledge graph embeddings as an alternative knowledge model representation service look state of the art, RDF avec Bluff and for comparison reasons with Gordon Beddings, standard ones worth back in glove.",
            "To connect the Knowledge Graph to to dialects, we used entity linking.",
            "OK."
        ],
        [
            "So that's an example.",
            "So just to have a look at that actually means.",
            "So we have the.",
            "The background is just a snippet of the conversation, so there are two users talking on one 2 channel and.",
            "Reporting issues and try and troubleshooting basically so you see they they mentioned some words.",
            "Highlighted words are linked to entities.",
            "Here so this.",
            "Get it as a DB pedia Wikipedia page and then by linking them to DBT we also extract something that wasn't mentioned in the conversation, but it's on the intersection.",
            "So from crawling the graph we also extract the using shortest path.",
            "We extract the subgraphs and retrieve the entities that are implicit.",
            "Um?",
            "OK. Net."
        ],
        [
            "Step is right.",
            "So we have the green ones, so this through dialects that we take from existing corpus and the question is where do we get this stupid dialogue that don't exist or would it break the coherence of the conversation?"
        ],
        [
            "And we propose actually several approaches how we can generate them.",
            "So the first group of approaches we call sampling and the simplest way is just to take vocabulary and randomly sample so uniform probability of sampling one word from the vocabulary, and this would be so the presentation of the dialogue would be a sequence of this entities.",
            "Second approach for sampling is actually account for the vocabulary distribution mean that so all the all the dialects here about the boot means sampling.",
            "Ubuntu is actually more probable.",
            "OK, then second group approaches is a bit more sophisticated and they deal with taking.",
            "Either one conversation and permuting the order of the sentences and.",
            "The other two approaches involve two dialects, and then you try to slice them.",
            "So you take the first part of single conversation and you append the second part from a different conversation.",
            "So you introduce.",
            "Basically this break of coherence and you know exactly the place where it breaks, because you generate and vertical split means taking the same, but basically taking two speakers from different conversations and pretend they're talking to each other, or they are not.",
            "Ways to create the negative exam?"
        ],
        [
            "And the first experiment we run using this subgraph induction approach.",
            "We basically just examine the distance.",
            "So the intuition that coherence somehow.",
            "Corresponds to distance on the graph, right?",
            "The closer the closer entities you have, the more coherent is, and that's what we also observed.",
            "So here you have this plot.",
            "The blue one is the true positive means real dialect, so it's more likely that the entities in real dialects are close to each other.",
            "So one or two hops away.",
            "And then it decreases here.",
            "Then for capital distribution which basically shows the topic.",
            "So for common common entities in the corpus it's already 2 hops away and you are very very likely to reach one of those common words.",
            "And then if you just pick random ones, then add three hops away, you basically can reach any any of the entities and the other bottleneck.",
            "We discovered that we cannot actually get path is longer than four because we ran out of memory.",
            "So commodity server.",
            "OK, so that was it."
        ],
        [
            "The next approach take how about embeddings?",
            "So since they scale better, so we take.",
            "Conditional neural network classifier for this task and in this case we can basically replace in the same architecture.",
            "We can replace knowledge graph, embeddings involved, embeddings, so again, word embeddings we take only for the entasis, not for all the words to make the comparison fair.",
            "Alright, so that's the standard architecture.",
            "We have the input are this entities.",
            "We take embeddings pretraining bindings from.",
            "So we loaded the models.",
            "Then we have convolutional layer, multiple layer hidden layer and the output is sigmoid probability score from zero to one which we interpret as a coherent score and the training on on this dialects.",
            "So we had thousands of dialects we trained on.",
            "Strain through basically separate through dialects from all kinds of.",
            "Generate the dialogues."
        ],
        [
            "And there is also bad news for the Knowledge Graph.",
            "The word embeddings actually won this challenge.",
            "And we also said one of the goals we had is to check different configurations.",
            "So far knowledge graph embedding.",
            "So we found the ones that work best so it's patron tardy after work is actually best performing.",
            "But still, so now the task her glove embeddings outperformed.",
            "Knowledge graph embeddings.",
            "OK, so."
        ],
        [
            "We also took a look basically on the cosine distance, which representation of similarity distance in this space.",
            "So we see the same picture right?",
            "We had on the original graph, which is good for the embeddings.",
            "That means they scale, but they preserve basically the same results, so they approximate well.",
            "The problem with that is not is not good enough to separate the three distribution.",
            "Actually, that's good enough for random one, but for for the other ones is not that well.",
            "So if you compare it to the.",
            "Make sure you have forward embeddings.",
            "This one's already better separated, so that's why we got the results just away."
        ],
        [
            "The embedding some modeling the knowledge.",
            "And."
        ],
        [
            "Not operational, so this is our result.",
            "Tables in the paper and so for these two groups of generative approaches for negative samples.",
            "It is more or less straightforward to separate real dialogue from this random entities, right?",
            "So this is purple column.",
            "So it's um, zero 99's, almost perfect, and this permutations even board embeddings cannot cannot do very well on this task, so it means it's still a hard task.",
            "So basically distinguish when this coherence breaks.",
            "So some summaries."
        ],
        [
            "If you want to share is that we try to have a look at the working of this neural network.",
            "So this is basically what the neural network sees.",
            "So this is visualization of the.",
            "All their embedding layer which is the input to the the classifier, and that's how it perceives the dialogue.",
            "So this is the entities in the dialogue.",
            "The first one is original surreal dialogue and 2nd is a random, and it looks quite different, right?",
            "So what we notice is this column.",
            "So some sort of bars which we which we interpret as coherent."
        ],
        [
            "Right, so some sort of threads that running across the conversation so it's something like a semantic.",
            "Dimension where it finds the entities that are related.",
            "That's why we see this pattern that it highlights.",
            "It activates certain topic.",
            "And this helps to invent, produce the score for the whole dialogue, and for for the random in some sort of random pattern activation which doesn't have the same structure.",
            "And to examine it even even."
        ],
        [
            "Other we took example various split, so remember this this example.",
            "When we take one part from one dialect and 2nd from 2nd and just merge them concatenate so the first one is a true positive."
        ],
        [
            "Then we split it and we add the different."
        ],
        [
            "And exactly at this point we observe what happened.",
            "So here the bars."
        ],
        [
            "The bars we had are disappearing, so they're shifting in some other dimensions.",
            "That's why the aggregated score is lower for this because the bar server.",
            "So that's the way we interpret it, right?",
            "That's only thing we can observe.",
            "Alright, and that's it."
        ],
        [
            "Sessions so.",
            "Um?",
            "One of the limitations is that since we use one to use knowledge graphs, we bound to entities.",
            "If you if you analyze some chitchat dialects, there are not so many entities you're mentioning, so the most one of the directions for the future work in that website right to integrate the Knowledge Graph involved in buildings.",
            "An because why do weddings performed well but we still believe knowledge graph embeddings might add something there.",
            "We don't give up on them and the other.",
            "Intuition we we observed that the sum.",
            "The problem with the entity linking that we propagate errors if we use it in the pipeline fashion, we cannot recover from errors.",
            "So in our example, then boom two is often matched to religion instead of operational system.",
            "And if we use a graph we can recover it because we still have the relations.",
            "But if we use embeddings, it's very often we lose it because that end up in complete different space."
        ],
        [
            "Thank you.",
            "Hi, thanks for the talk so.",
            "Frame the problem is a binary classification problem, right?",
            "So how do you account for topic drifts in conversations for topic groups, like when you are talking about a certain topic and then the topic of attention conversation, it trips and you start talking about something else.",
            "Thank you started talking about Monterrey and then you move to San Francisco right?",
            "Yeah so basically the only thing what I was showing in here.",
            "I know that we interpret this is a topic drift right?",
            "So we can observe it from this patterns so we didn't solve that task OK, but it's sort of going towards.",
            "So we model the simplest way binary classification just to to check whether we have enough knowledge.",
            "So we were not solving all the problems.",
            "Yeah, OK, thank you, thank you.",
            "Yeah man, thanks for the talk.",
            "I think that the work is interesting also in some other fields like argumentation, mining for example.",
            "So my question is if you think that your approach can be extended to.",
            "Understand which is the role of each sentence in the conversation.",
            "If it is a claim, or if as another role is supporting one other sentence because that's what, for example argumentation mining does to try to understand the rhetorical role of each sentence.",
            "Well, I don't really have straightforward idea for that.",
            "I mean, you could try to model the classification task, but in this case you have to label the corpora.",
            "So like we had a nice use case because we could create the corpus.",
            "From dialect threats because just be corrupt him.",
            "That's why it was easier, but for any other task you need to corpus to train a supervised methods.",
            "Hi, thanks for the talk.",
            "I have a question on the application page where you mentioned the digital assistant side with Alexa and everything.",
            "I wonder what what concrete application you were thinking with this measurement in together with either natural language generation or natural language understanding.",
            "So what was the applications page?",
            "So you have a page on the application of this research.",
            "Possible application of this research in the beginning.",
            "No OK, yes.",
            "So I was wondering on the second half, interpreting Contacts and generating response, so how?",
            "What's your thought about applying this into those area?",
            "That's actually the reason why we did this research in the 1st place because we wanted to make a conversational agent, but then it wasn't clear.",
            "What what the agent should say, and then we were starting exploring The Cave, but it makes sense to say in the context, so we need to know how far the how far.",
            "Basically the entities you want to present to the user.",
            "So so that was just the.",
            "Research in progress sort of thing.",
            "OK, so I think it's it's directly.",
            "I mean, that was my goal.",
            "That is why I was doing this.",
            "Thank you.",
            "So yes, thank you also for the talk.",
            "I was wondering in the example you gave at the beginning.",
            "So I am in Monterrey and if the response for in the dialogue would be but I prefer the flock, it does cost.",
            "In a dialogue.",
            "It's perfectly correct and so would you with your system.",
            "Also permitted Florida into California.",
            "I agree, I mean this is a hard task and if.",
            "So it's more or less the best guess, right?",
            "So if you imagine there's this game where you are given a random set of words and they need to make conversation out of it, a sentence out of it is not so easy unless there are some relations between the words.",
            "So it means.",
            "Of course you can always find edges, but you assume there are outliers.",
            "So you basically look for some patterns that are general and frequent.",
            "But obviously there can be any sentence where you can put any words you have a conversation, but I think it's working also to the question on argumentation, because This is why you might get into such cases.",
            "But thank you, thank you, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks for coming to my presentation.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's start with the first question.",
                    "label": 0
                },
                {
                    "sent": "What do we mean by semantic coherence?",
                    "label": 0
                },
                {
                    "sent": "So imagine at the lunch break you over here the following conversation.",
                    "label": 0
                },
                {
                    "sent": "Oh, I think Monterrey is really great location for conference.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, Florida has beautiful coastline.",
                    "label": 0
                },
                {
                    "sent": "I really want to see the Eiffel Tower.",
                    "label": 1
                },
                {
                    "sent": "So the interesting observation that all three sentences are grammatically correct.",
                    "label": 0
                },
                {
                    "sent": "They just don't make sense when they're put in the same sequence, right?",
                    "label": 0
                },
                {
                    "sent": "So if it blew up your mind, that was designed like this.",
                    "label": 0
                },
                {
                    "sent": "And the intuition here that for the conversation to be interpretable or like comprehensible by humans, it means to have certain relations between the.",
                    "label": 0
                },
                {
                    "sent": "Entities in the conversation.",
                    "label": 0
                },
                {
                    "sent": "So in order to make to correct this example, only think.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I I can do these away is just to replace the entities with the ones that actually have connections to each other, so it makes sense talking about California in context of Monterrey because Monterrey is located in California and Florida.",
                    "label": 0
                },
                {
                    "sent": "So in our reality right?",
                    "label": 0
                },
                {
                    "sent": "So that corresponds to our representation of the world that shared some common knowledge we have and then aquarium is in monitoring or de filter right?",
                    "label": 0
                },
                {
                    "sent": "So it's some implicit background knowledge that.",
                    "label": 0
                },
                {
                    "sent": "Participants of the conversations must share for all of them to accept this conversation to be sensible.",
                    "label": 0
                },
                {
                    "sent": "Alright, so the question is how?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How do we make computers?",
                    "label": 0
                },
                {
                    "sent": "Are computers able to understand this kind of intuition?",
                    "label": 0
                },
                {
                    "sent": "We have about conversations and sensemaking.",
                    "label": 0
                },
                {
                    "sent": "So what we propose here is to introduce the task of measuring semantic coherence by modeling as a classification task.",
                    "label": 1
                },
                {
                    "sent": "So basically the output of the classifier is the score, so from zero to one, which we interpret as the coherence score, and this basically the.",
                    "label": 0
                },
                {
                    "sent": "So we create two sets that we want to learn the to separate them.",
                    "label": 0
                },
                {
                    "sent": "So this sounds making line here.",
                    "label": 0
                },
                {
                    "sent": "Right, and the interesting parts of each of the dots is the conversation and the position of the conversation.",
                    "label": 1
                },
                {
                    "sent": "The space is obviously conditioned on the background knowledge so nonsense.",
                    "label": 0
                },
                {
                    "sent": "In this case, it doesn't mean necessarily.",
                    "label": 0
                },
                {
                    "sent": "That is not true or not relevant, so I give you an example.",
                    "label": 0
                },
                {
                    "sent": "If you work in another room and there is a chemistry conference going on for you, it might be basically not interpretable.",
                    "label": 0
                },
                {
                    "sent": "Doesn't mean that it's not true, right?",
                    "label": 0
                },
                {
                    "sent": "So it depends on the knowledge we possess an our goal here was to.",
                    "label": 0
                },
                {
                    "sent": "Evaluate the knowledge models we have, whether we are able to use them to basically separate this correct or real conversations from this.",
                    "label": 0
                },
                {
                    "sent": "Non nonsense making one.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The so the other question is why is it useful?",
                    "label": 0
                },
                {
                    "sent": "So this is sort of intermediate that there's just a reality.",
                    "label": 0
                },
                {
                    "sent": "Check whether our knowledge models are useful.",
                    "label": 0
                },
                {
                    "sent": "Whether we can use them so the applications can be in conversation analysis.",
                    "label": 0
                },
                {
                    "sent": "So imagine you record the business meeting where we have maybe several dialogues at the same time, and you want to separate them to to figure out who's actually talking to whom and maybe also to segment it based on the topics that detect, at which point you actually start talking about different things.",
                    "label": 0
                },
                {
                    "sent": "And ultimately, we want computers to be able to be part of this conversation.",
                    "label": 0
                },
                {
                    "sent": "For this, they need to understand the context of the conversation to be able to generate appropriate response.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is a set of our contributions reclaiming this paper.",
                    "label": 0
                },
                {
                    "sent": "The first one, the task we introduced.",
                    "label": 0
                },
                {
                    "sent": "And then we also described how to create the benchmark for scalable experiments for this task and we evaluate a set of initial approaches on this benchmark.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the benchmark we took the existing data set of, so it's one of the biggest datasets I've used it also because it's actually extracted from public chat on Ubuntu Support Forum, and then I think about it is also because it's full of this computer science terminology.",
                    "label": 0
                },
                {
                    "sent": "So boom, two related, all the software tools which have many entities in DPM.",
                    "label": 0
                },
                {
                    "sent": "That's why we took Peter Week data converted to AC team, so that's our knowledge graph representation.",
                    "label": 0
                },
                {
                    "sent": "We took knowledge graph embeddings as an alternative knowledge model representation service look state of the art, RDF avec Bluff and for comparison reasons with Gordon Beddings, standard ones worth back in glove.",
                    "label": 0
                },
                {
                    "sent": "To connect the Knowledge Graph to to dialects, we used entity linking.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's an example.",
                    "label": 0
                },
                {
                    "sent": "So just to have a look at that actually means.",
                    "label": 0
                },
                {
                    "sent": "So we have the.",
                    "label": 0
                },
                {
                    "sent": "The background is just a snippet of the conversation, so there are two users talking on one 2 channel and.",
                    "label": 0
                },
                {
                    "sent": "Reporting issues and try and troubleshooting basically so you see they they mentioned some words.",
                    "label": 0
                },
                {
                    "sent": "Highlighted words are linked to entities.",
                    "label": 0
                },
                {
                    "sent": "Here so this.",
                    "label": 0
                },
                {
                    "sent": "Get it as a DB pedia Wikipedia page and then by linking them to DBT we also extract something that wasn't mentioned in the conversation, but it's on the intersection.",
                    "label": 0
                },
                {
                    "sent": "So from crawling the graph we also extract the using shortest path.",
                    "label": 0
                },
                {
                    "sent": "We extract the subgraphs and retrieve the entities that are implicit.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK. Net.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Step is right.",
                    "label": 0
                },
                {
                    "sent": "So we have the green ones, so this through dialects that we take from existing corpus and the question is where do we get this stupid dialogue that don't exist or would it break the coherence of the conversation?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we propose actually several approaches how we can generate them.",
                    "label": 0
                },
                {
                    "sent": "So the first group of approaches we call sampling and the simplest way is just to take vocabulary and randomly sample so uniform probability of sampling one word from the vocabulary, and this would be so the presentation of the dialogue would be a sequence of this entities.",
                    "label": 0
                },
                {
                    "sent": "Second approach for sampling is actually account for the vocabulary distribution mean that so all the all the dialects here about the boot means sampling.",
                    "label": 0
                },
                {
                    "sent": "Ubuntu is actually more probable.",
                    "label": 0
                },
                {
                    "sent": "OK, then second group approaches is a bit more sophisticated and they deal with taking.",
                    "label": 0
                },
                {
                    "sent": "Either one conversation and permuting the order of the sentences and.",
                    "label": 0
                },
                {
                    "sent": "The other two approaches involve two dialects, and then you try to slice them.",
                    "label": 0
                },
                {
                    "sent": "So you take the first part of single conversation and you append the second part from a different conversation.",
                    "label": 0
                },
                {
                    "sent": "So you introduce.",
                    "label": 0
                },
                {
                    "sent": "Basically this break of coherence and you know exactly the place where it breaks, because you generate and vertical split means taking the same, but basically taking two speakers from different conversations and pretend they're talking to each other, or they are not.",
                    "label": 0
                },
                {
                    "sent": "Ways to create the negative exam?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the first experiment we run using this subgraph induction approach.",
                    "label": 0
                },
                {
                    "sent": "We basically just examine the distance.",
                    "label": 0
                },
                {
                    "sent": "So the intuition that coherence somehow.",
                    "label": 0
                },
                {
                    "sent": "Corresponds to distance on the graph, right?",
                    "label": 0
                },
                {
                    "sent": "The closer the closer entities you have, the more coherent is, and that's what we also observed.",
                    "label": 0
                },
                {
                    "sent": "So here you have this plot.",
                    "label": 0
                },
                {
                    "sent": "The blue one is the true positive means real dialect, so it's more likely that the entities in real dialects are close to each other.",
                    "label": 0
                },
                {
                    "sent": "So one or two hops away.",
                    "label": 0
                },
                {
                    "sent": "And then it decreases here.",
                    "label": 0
                },
                {
                    "sent": "Then for capital distribution which basically shows the topic.",
                    "label": 0
                },
                {
                    "sent": "So for common common entities in the corpus it's already 2 hops away and you are very very likely to reach one of those common words.",
                    "label": 0
                },
                {
                    "sent": "And then if you just pick random ones, then add three hops away, you basically can reach any any of the entities and the other bottleneck.",
                    "label": 0
                },
                {
                    "sent": "We discovered that we cannot actually get path is longer than four because we ran out of memory.",
                    "label": 0
                },
                {
                    "sent": "So commodity server.",
                    "label": 0
                },
                {
                    "sent": "OK, so that was it.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The next approach take how about embeddings?",
                    "label": 0
                },
                {
                    "sent": "So since they scale better, so we take.",
                    "label": 0
                },
                {
                    "sent": "Conditional neural network classifier for this task and in this case we can basically replace in the same architecture.",
                    "label": 0
                },
                {
                    "sent": "We can replace knowledge graph, embeddings involved, embeddings, so again, word embeddings we take only for the entasis, not for all the words to make the comparison fair.",
                    "label": 0
                },
                {
                    "sent": "Alright, so that's the standard architecture.",
                    "label": 0
                },
                {
                    "sent": "We have the input are this entities.",
                    "label": 0
                },
                {
                    "sent": "We take embeddings pretraining bindings from.",
                    "label": 0
                },
                {
                    "sent": "So we loaded the models.",
                    "label": 0
                },
                {
                    "sent": "Then we have convolutional layer, multiple layer hidden layer and the output is sigmoid probability score from zero to one which we interpret as a coherent score and the training on on this dialects.",
                    "label": 0
                },
                {
                    "sent": "So we had thousands of dialects we trained on.",
                    "label": 0
                },
                {
                    "sent": "Strain through basically separate through dialects from all kinds of.",
                    "label": 0
                },
                {
                    "sent": "Generate the dialogues.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there is also bad news for the Knowledge Graph.",
                    "label": 0
                },
                {
                    "sent": "The word embeddings actually won this challenge.",
                    "label": 0
                },
                {
                    "sent": "And we also said one of the goals we had is to check different configurations.",
                    "label": 0
                },
                {
                    "sent": "So far knowledge graph embedding.",
                    "label": 0
                },
                {
                    "sent": "So we found the ones that work best so it's patron tardy after work is actually best performing.",
                    "label": 0
                },
                {
                    "sent": "But still, so now the task her glove embeddings outperformed.",
                    "label": 0
                },
                {
                    "sent": "Knowledge graph embeddings.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also took a look basically on the cosine distance, which representation of similarity distance in this space.",
                    "label": 0
                },
                {
                    "sent": "So we see the same picture right?",
                    "label": 0
                },
                {
                    "sent": "We had on the original graph, which is good for the embeddings.",
                    "label": 0
                },
                {
                    "sent": "That means they scale, but they preserve basically the same results, so they approximate well.",
                    "label": 0
                },
                {
                    "sent": "The problem with that is not is not good enough to separate the three distribution.",
                    "label": 0
                },
                {
                    "sent": "Actually, that's good enough for random one, but for for the other ones is not that well.",
                    "label": 0
                },
                {
                    "sent": "So if you compare it to the.",
                    "label": 0
                },
                {
                    "sent": "Make sure you have forward embeddings.",
                    "label": 0
                },
                {
                    "sent": "This one's already better separated, so that's why we got the results just away.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The embedding some modeling the knowledge.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not operational, so this is our result.",
                    "label": 0
                },
                {
                    "sent": "Tables in the paper and so for these two groups of generative approaches for negative samples.",
                    "label": 0
                },
                {
                    "sent": "It is more or less straightforward to separate real dialogue from this random entities, right?",
                    "label": 0
                },
                {
                    "sent": "So this is purple column.",
                    "label": 0
                },
                {
                    "sent": "So it's um, zero 99's, almost perfect, and this permutations even board embeddings cannot cannot do very well on this task, so it means it's still a hard task.",
                    "label": 0
                },
                {
                    "sent": "So basically distinguish when this coherence breaks.",
                    "label": 0
                },
                {
                    "sent": "So some summaries.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you want to share is that we try to have a look at the working of this neural network.",
                    "label": 0
                },
                {
                    "sent": "So this is basically what the neural network sees.",
                    "label": 0
                },
                {
                    "sent": "So this is visualization of the.",
                    "label": 0
                },
                {
                    "sent": "All their embedding layer which is the input to the the classifier, and that's how it perceives the dialogue.",
                    "label": 0
                },
                {
                    "sent": "So this is the entities in the dialogue.",
                    "label": 0
                },
                {
                    "sent": "The first one is original surreal dialogue and 2nd is a random, and it looks quite different, right?",
                    "label": 0
                },
                {
                    "sent": "So what we notice is this column.",
                    "label": 0
                },
                {
                    "sent": "So some sort of bars which we which we interpret as coherent.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so some sort of threads that running across the conversation so it's something like a semantic.",
                    "label": 0
                },
                {
                    "sent": "Dimension where it finds the entities that are related.",
                    "label": 0
                },
                {
                    "sent": "That's why we see this pattern that it highlights.",
                    "label": 0
                },
                {
                    "sent": "It activates certain topic.",
                    "label": 0
                },
                {
                    "sent": "And this helps to invent, produce the score for the whole dialogue, and for for the random in some sort of random pattern activation which doesn't have the same structure.",
                    "label": 0
                },
                {
                    "sent": "And to examine it even even.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other we took example various split, so remember this this example.",
                    "label": 0
                },
                {
                    "sent": "When we take one part from one dialect and 2nd from 2nd and just merge them concatenate so the first one is a true positive.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we split it and we add the different.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And exactly at this point we observe what happened.",
                    "label": 0
                },
                {
                    "sent": "So here the bars.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The bars we had are disappearing, so they're shifting in some other dimensions.",
                    "label": 0
                },
                {
                    "sent": "That's why the aggregated score is lower for this because the bar server.",
                    "label": 0
                },
                {
                    "sent": "So that's the way we interpret it, right?",
                    "label": 0
                },
                {
                    "sent": "That's only thing we can observe.",
                    "label": 0
                },
                {
                    "sent": "Alright, and that's it.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sessions so.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "One of the limitations is that since we use one to use knowledge graphs, we bound to entities.",
                    "label": 0
                },
                {
                    "sent": "If you if you analyze some chitchat dialects, there are not so many entities you're mentioning, so the most one of the directions for the future work in that website right to integrate the Knowledge Graph involved in buildings.",
                    "label": 0
                },
                {
                    "sent": "An because why do weddings performed well but we still believe knowledge graph embeddings might add something there.",
                    "label": 0
                },
                {
                    "sent": "We don't give up on them and the other.",
                    "label": 0
                },
                {
                    "sent": "Intuition we we observed that the sum.",
                    "label": 0
                },
                {
                    "sent": "The problem with the entity linking that we propagate errors if we use it in the pipeline fashion, we cannot recover from errors.",
                    "label": 0
                },
                {
                    "sent": "So in our example, then boom two is often matched to religion instead of operational system.",
                    "label": 0
                },
                {
                    "sent": "And if we use a graph we can recover it because we still have the relations.",
                    "label": 0
                },
                {
                    "sent": "But if we use embeddings, it's very often we lose it because that end up in complete different space.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Hi, thanks for the talk so.",
                    "label": 0
                },
                {
                    "sent": "Frame the problem is a binary classification problem, right?",
                    "label": 0
                },
                {
                    "sent": "So how do you account for topic drifts in conversations for topic groups, like when you are talking about a certain topic and then the topic of attention conversation, it trips and you start talking about something else.",
                    "label": 0
                },
                {
                    "sent": "Thank you started talking about Monterrey and then you move to San Francisco right?",
                    "label": 0
                },
                {
                    "sent": "Yeah so basically the only thing what I was showing in here.",
                    "label": 0
                },
                {
                    "sent": "I know that we interpret this is a topic drift right?",
                    "label": 0
                },
                {
                    "sent": "So we can observe it from this patterns so we didn't solve that task OK, but it's sort of going towards.",
                    "label": 0
                },
                {
                    "sent": "So we model the simplest way binary classification just to to check whether we have enough knowledge.",
                    "label": 0
                },
                {
                    "sent": "So we were not solving all the problems.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, thank you, thank you.",
                    "label": 0
                },
                {
                    "sent": "Yeah man, thanks for the talk.",
                    "label": 0
                },
                {
                    "sent": "I think that the work is interesting also in some other fields like argumentation, mining for example.",
                    "label": 0
                },
                {
                    "sent": "So my question is if you think that your approach can be extended to.",
                    "label": 0
                },
                {
                    "sent": "Understand which is the role of each sentence in the conversation.",
                    "label": 0
                },
                {
                    "sent": "If it is a claim, or if as another role is supporting one other sentence because that's what, for example argumentation mining does to try to understand the rhetorical role of each sentence.",
                    "label": 0
                },
                {
                    "sent": "Well, I don't really have straightforward idea for that.",
                    "label": 0
                },
                {
                    "sent": "I mean, you could try to model the classification task, but in this case you have to label the corpora.",
                    "label": 0
                },
                {
                    "sent": "So like we had a nice use case because we could create the corpus.",
                    "label": 0
                },
                {
                    "sent": "From dialect threats because just be corrupt him.",
                    "label": 0
                },
                {
                    "sent": "That's why it was easier, but for any other task you need to corpus to train a supervised methods.",
                    "label": 0
                },
                {
                    "sent": "Hi, thanks for the talk.",
                    "label": 0
                },
                {
                    "sent": "I have a question on the application page where you mentioned the digital assistant side with Alexa and everything.",
                    "label": 0
                },
                {
                    "sent": "I wonder what what concrete application you were thinking with this measurement in together with either natural language generation or natural language understanding.",
                    "label": 0
                },
                {
                    "sent": "So what was the applications page?",
                    "label": 0
                },
                {
                    "sent": "So you have a page on the application of this research.",
                    "label": 0
                },
                {
                    "sent": "Possible application of this research in the beginning.",
                    "label": 0
                },
                {
                    "sent": "No OK, yes.",
                    "label": 0
                },
                {
                    "sent": "So I was wondering on the second half, interpreting Contacts and generating response, so how?",
                    "label": 0
                },
                {
                    "sent": "What's your thought about applying this into those area?",
                    "label": 0
                },
                {
                    "sent": "That's actually the reason why we did this research in the 1st place because we wanted to make a conversational agent, but then it wasn't clear.",
                    "label": 0
                },
                {
                    "sent": "What what the agent should say, and then we were starting exploring The Cave, but it makes sense to say in the context, so we need to know how far the how far.",
                    "label": 0
                },
                {
                    "sent": "Basically the entities you want to present to the user.",
                    "label": 0
                },
                {
                    "sent": "So so that was just the.",
                    "label": 0
                },
                {
                    "sent": "Research in progress sort of thing.",
                    "label": 0
                },
                {
                    "sent": "OK, so I think it's it's directly.",
                    "label": 0
                },
                {
                    "sent": "I mean, that was my goal.",
                    "label": 0
                },
                {
                    "sent": "That is why I was doing this.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "So yes, thank you also for the talk.",
                    "label": 0
                },
                {
                    "sent": "I was wondering in the example you gave at the beginning.",
                    "label": 0
                },
                {
                    "sent": "So I am in Monterrey and if the response for in the dialogue would be but I prefer the flock, it does cost.",
                    "label": 0
                },
                {
                    "sent": "In a dialogue.",
                    "label": 0
                },
                {
                    "sent": "It's perfectly correct and so would you with your system.",
                    "label": 0
                },
                {
                    "sent": "Also permitted Florida into California.",
                    "label": 0
                },
                {
                    "sent": "I agree, I mean this is a hard task and if.",
                    "label": 0
                },
                {
                    "sent": "So it's more or less the best guess, right?",
                    "label": 0
                },
                {
                    "sent": "So if you imagine there's this game where you are given a random set of words and they need to make conversation out of it, a sentence out of it is not so easy unless there are some relations between the words.",
                    "label": 0
                },
                {
                    "sent": "So it means.",
                    "label": 0
                },
                {
                    "sent": "Of course you can always find edges, but you assume there are outliers.",
                    "label": 0
                },
                {
                    "sent": "So you basically look for some patterns that are general and frequent.",
                    "label": 0
                },
                {
                    "sent": "But obviously there can be any sentence where you can put any words you have a conversation, but I think it's working also to the question on argumentation, because This is why you might get into such cases.",
                    "label": 0
                },
                {
                    "sent": "But thank you, thank you, thank you.",
                    "label": 0
                }
            ]
        }
    }
}