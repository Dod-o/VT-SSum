{
    "id": "ec4krkq6z6x2w777k4m2rvcuyoprspxz",
    "title": "Online Learning",
    "info": {
        "author": [
            "Peter L. Bartlett, UC Berkeley"
        ],
        "published": "Feb. 3, 2011",
        "recorded": "October 2010",
        "category": [
            "Top->Computer Science->Machine Learning->On-line Learning"
        ]
    },
    "url": "http://videolectures.net/mlss2010au_bartlett_onlinelearning/",
    "segmentation": [
        [
            "Alright, so.",
            "My name is Steve Bartlett and I'm going to be telling you about online learning online decision problems.",
            "For the next.",
            "Half hours OK, so online learning the setting here is we have a repeated game is there."
        ],
        [
            "So we have we have a repeated game so it's a little different from the funds of formulations of learning models that you've seen that you've seen, or indeed.",
            "That's awesome so far.",
            "The setting here is where we're playing a game, so the decision method gets to choose an action at time T, let's call it 80, and then the environment reveals a loss function.",
            "El Subte from some from some said and this is the tells you the loss that you incur for any action that you could play and the aim is to minimize the cumulative sum of the loss right so?",
            "We look at a whole bunch of examples in a moment, but just think of Lt telling us how good or bad it was to play whatever it was we played and at the end of it all we want some of those losses.",
            "In retrospect, be out too bad.",
            "And it's a deterministic setting so.",
            "We're not imposing.",
            "Probabilistic kinds of assumptions about the process that generates the.",
            "The sequence of losses that we see.",
            "So minimizing costs in an absolute sense is is kind of hopeless, and the aim is to minimize your loss in a relative sense to minimize something that we call regret, and that is to perform almost as well in the loss that we incur as the best we could have done in retrospect when we restrict our actions to some summer class.",
            "Alright, so that's the.",
            "The.",
            "The formulation.",
            "And the key point here is that the data can be adversarially chosen, so we're not thinking about probabilistic sorts of assumptions.",
            "And the."
        ],
        [
            "Regret then, is the value of this game where there's this alternating alternating sequence of Mens and Max is we want to minimize choose our action at the first step to minimize.",
            "The.",
            "The regret the adversary plays a loss at the first step that aims to maximize our regret, and so on.",
            "And at the end of it all, this is our regrets, the loss, the sequence of losses we incurred.",
            "And there's the best that we could have done.",
            "Looking in retrospect, the sequence of losses that the adversary players.",
            "OK, so that's.",
            "That's the."
        ],
        [
            "That's the setting.",
            "That I'm going to be talking about, so why do we think about these adversarial models?",
            "Well, there are a bunch of reasons I want to tell you about.",
            "Several application areas where an adversary model is really quite appropriate.",
            "You really are competing with somebody in making your predictions and will look at a couple of examples from computer security and from computational finance.",
            "But one of the I think maybe even the more appealing aspect to it is that we really have don't have to make many assumptions at all when we look at things in this adverse aerial setting, right?",
            "So it turns out that it's it's really straightforward to convert a successful strategy that can deal with an adverse aerial environment to a method that thing that can deal with the probabilistic environment.",
            "And actually the converse is also true.",
            "It's interesting to study this adversarial model.",
            "In this study, the performance of statistical methods methods have been developed, but from.",
            "Probabilistic kinds of settings.",
            "It's interesting to do that because we get a good understanding of what's at the heart of the statistical problem and will see that there are strong correspondences between the sorts of performance guarantees we can get in the statistical case and the and the adversarial case.",
            "So you know, I think that we get a lot of insight there.",
            "We also understand the robustness of statistical methods to the kinds of assumptions that that.",
            "Windows Phone.",
            "Um?",
            "And you know, there are some curious things that there's a lot of overlap in the design of methods for these adverse aerial problems.",
            "Similarity between the types of tools that are useful there and the types of tools that are useful in statistical statistical settings also.",
            "Alright, so let's let's talk about adverse aerial models and application areas where adverse aerial models are."
        ],
        [
            "Previous computer security is 1 area where.",
            "Where we really can think of the decision problem as playing a repeated game.",
            "Let's say somebody sending you."
        ],
        [
            "Spam messages and you have an algorithm for detecting spam, then you know it's in their interest to have your predictions be wrong.",
            "And they certainly don't want to change the kinds of.",
            "The kinds of features of the email they send in response to the sorts of decisions that you make about.",
            "The best answer.",
            "OK, so putting it in the terminology that we saw before the action that we can take is to choose the classification of alright some function that some features of email messages.",
            "To a decision as to whether a particular email is spam or not, the loss that that we see might be might correspond to a particular email that gets sent to our.",
            "Our account and it might be indicated for that email being corrected.",
            "Basketball, I'd like to spam is allowed through, for instance, or we throw away.",
            "Message.",
            "And so in this setting you know the the the adversary who's who's sending spam by trying to modify the spam in response to our decisions.",
            "So we really do need to to consider the adverse aerial aspect in in modeling this problem, seeing as as a stochastic process that generated the data is missing that aspect.",
            "So you know and.",
            "Talked already about this, this business of minimizing regret.",
            "You know, depending on how we model things, if we're looking at an adverse aerial setting where we have.",
            "Where an adversary has control of the whole sequence then then getting good classification accuracy in an absolute sense is is hopeless.",
            "But we might hope to minimize regret in the sense of having our spam detection accuracy close to the best that we could have done it in perspective.",
            "That's the kind of criterion talking about here.",
            "Suzanne how to put yes like you are going to predict the loss function depending upon what word reveals.",
            "But it though in that information is not available like we need to minimize regret.",
            "And which is based on those El Subte yes, and we want to minimize Elsa peak.",
            "But what if we do not have that information?",
            "Whether our prediction was right or not, right?",
            "That's a good question so.",
            "The question is, what if we don't get to see the functional El Subte at each time step?",
            "So we want to minimize the sum of El Subte, right?",
            "But we may not have complete information about what what would have happened in this example.",
            "You know, for instance, if let's be a little concrete.",
            "So let me."
        ],
        [
            "Get back to that end of this lot, right?",
            "So I think just a little concretely about this example.",
            "We might be deciding on the basis of some features of the of an email message.",
            "Formation about the header words in the message and so on.",
            "Making a decision about whether we would classify that as as as spam or not, or maybe a more refined decision we could.",
            "We could choose an action that's a real number between zero and one.",
            "You can think of that as something like predicting a probability that the message is spam, and at each round the adversary is choosing the message.",
            "And.",
            "Any loss is the squared loss, so YT is the indicator for spam and 80 is our our guess.",
            "It's a function now that that's from features to the Angels are one and the last.",
            "We encourage the square difference.",
            "OK so and then regret is defined as the XSQF squared error over the best that was achievable on that particular sequence.",
            "So in this case.",
            "We see a loss function if we know the.",
            "Value why afterwards, for instance, that might be from the user's decision about whether this is a spam message or not.",
            "If we get.",
            "If it gets passed through or the user rescuing him from the stand Holder, but it may be.",
            "I mean, that's a.",
            "That's an example where we may not get perfect information about about a loss function, right?",
            "So in this case, for instance, we might not see the why every time, and there are other other cases where we get limited information about the loss, and we still want to minimize the the cumulative loss.",
            "So those kinds of restricted information games are also very interesting, and there you know so.",
            "So what happens the way the way we formulate those games is we get a certain piece of information at each step, and we incur a certain loss at each step.",
            "And you know, keeping track of how we can use that information in order to minimize the cumulative losses instead of the game.",
            "I'm not going to be talking so much about the limited information case in this talk.",
            "Maybe a little bit later on, but very little.",
            "Um, I'm mostly focused on the case where you you get to see the last function in its entire T, and so you know what what different actions would have cost you at any point.",
            "No, but that's a that's certainly a very, very important class of problems that partial information games."
        ],
        [
            "OK, there are other examples in computer security.",
            "This is a graph from a website and classification competition.",
            "The black nodes are spam and the white nodes are other websites and other classification problem.",
            "You want to decide should I be if I'm a search engine?",
            "Should I be serving up this web page that claims to be CNN or or you know, sure, I still have some other one and of course the people, the people who have been there.",
            "Their advertisements on the fake CNN page wanna Fool you or your classifier?",
            "Network security problems."
        ],
        [
            "Do not have service sort of attacks and similar similar issues and comments."
        ],
        [
            "Proponents portfolio optimization problems can be viewed in the same sort of way.",
            "They really are repeated interaction between between a number of players that there should be viewed as a as a competitive interaction.",
            "So think of the prob."
        ],
        [
            "Choosing a portfolio so that is we have a certain amount of capital and we like to distribute it across a bunch of financial instruments so as to maximize something, you know, utility.",
            "To distribute in a way that's good for us.",
            "Typically it's more than just worrying about the return that we make.",
            "We might worry without worrying about the risk that we've incurred in making that return.",
            "So there's some notion of utility.",
            "The adverse aerial aspect of it comes from the fact that other market players in profit by making our decisions bad ones.",
            "So in this setting, the action that we that we choose, the ATR decisions, are distributions over the financial instruments, the loss.",
            "Is something the negative of some utility, so it might be the negative logarithm of the portfolios increasing value.",
            "So if we have a bunch of different instruments.",
            "Maybe?",
            "Delete them.",
            "And this vector RT at time T is the vector of relative price increases, so the ratio of the price at time T to the price of terms for T -- 1.",
            "Um?",
            "Then the increase in value portfolio gene product between our distribution 80 and this data RT and wars might be the negative logarithm of that.",
            "If logarithm is the appropriate behavior question.",
            "So we might be interested in comparing our performance to the performance of the best instrument in that set.",
            "Alright, so there the comparison is to distribution over the instruments that that correspond to just the Delta function, like focusing everything on single, a single instrument.",
            "Or we might be interested in.",
            "Competing with some set of indices so.",
            "Articular distribution across a subset of stocks corresponds to the Dow Jones industrial average, and we might be interested in doing well relative to that.",
            "Or we might be interested in doing well relative to any distribution across across the instruments.",
            "So this is a.",
            "At the end about this constantly rebalanced portfolios.",
            "So where we're trying to compete with any distribution across across instruments with low gloss.",
            "So just be a little precise right there."
        ],
        [
            "The decisions that we may go distribution across same instruments and at each round the adversary gets to choose a vector of returns, then negative right the ratio of the price of the previous time at the current time.",
            "For the price the previous time and the losses, the negative loss of the of the negative log of the increase in value of the of the whole portfolio.",
            "So I regret is the difference between the losses that we incurred so that negative loss, negative log of the ratio of the beginning price and ending price.",
            "Of beginning value of any value for the whole portfolio.",
            "This is the smallest loss that that we could have been heard.",
            "That is, for a fixed in this case fixed distribution across across instruments.",
            "Fixed mixture of instruments.",
            "What's the best we could have done?",
            "OK, so we're trying to get the log here.",
            "Is pulling out the exponent we're trying to get the optimal rate of growth, comparing ourselves to the best, the best portfolio that we could have chosen at this time."
        ],
        [
            "OK so um.",
            "I told you a bit about some of these other motivations, so we will see that we can.",
            "We can take effective algorithms for deterministic adversary settings and use them in probabilistic settings, so we'll see some conversions for online to batch.",
            "And that's really very straightforward, so it's a.",
            "It's a more difficult setting in that sense.",
            "I."
        ],
        [
            "And also this this idea of trying to understand understand the.",
            "Robustness of statistical methods that are designed for probabilistic settings robustness to the various various assumptions that were that were made in the design look a little bit later on.",
            "OK, so."
        ],
        [
            "Key points of that formulation.",
            "It's a repeated game that we're playing with, aiming to minimize our regret.",
            "That's the difference between the loss we actually heard and the best loss we could have in curd.",
            "When, in retrospect, we look back and think what was the best single action that we could have played throughout.",
            "And it's again, so the data is adversarially chosen this.",
            "There's an adversary out there.",
            "Alright, so.",
            "Here's what I'm."
        ],
        [
            "Planning to talk about.",
            "For the next.",
            "For the next little over 3 hours, I guess we get some big breaks in there.",
            "First of all, I want to tell you about finite comparison classes, so this is predicting with expert advice.",
            "Is there is the the label that you you may have heard of before.",
            "So here we are thinking of.",
            "Just exactly the setting that I described, but the the class of decisions that we're choosing from is finite, right?",
            "And we're competing with the best best member of that class.",
            "One of the things that I want to draw out is the relationship between adverse aerial and statistical.",
            "Prediction problems and will see you know one link explicitly in this relationship between online and batch will see a a simple couple simple conversions from online online algorithms to back you up.",
            "They don't want to talk about.",
            "I more general formulation online convex optimization.",
            "So here we are thinking about these cost functions as being convex functions.",
            "So the adversary is playing from some from some class of Columbia's loss functions and.",
            "That's important.",
            "An important special case.",
            "We've seen some examples of that already low gloss is actually an example of that, but losses kind of tone for Loblaw's best.",
            "There's a lot that's known about about the optimal decision strategy in various in various settings, and log loss is really rather special, and if we get time will say we'll see a little bit about log loss at the end."
        ],
        [
            "Alright, so let's let's talk about finite comparison classes.",
            "Initially, so this is the setting of prediction with expert advice.",
            "We come back to today at 11, so let's think about this."
        ],
        [
            "In the colors.",
            "Canonical example here is making a decision about whether it's going to rain tomorrow.",
            "We have access to a bunch of experts.",
            "Each of them is going to tell us whether it will rain or not.",
            "They make their forecasts roll one.",
            "It's not going to rain, or it is going to rain, and we want to be able to ensure that we can predict almost as well as the best expert in that step, right?",
            "So we have M experts.",
            "Now set a of actions we can choose to go with expert one through Expert M. And each of those has some sequence of forecasts that they that they come up with.",
            "And at round T. There's an actual outcome, right?",
            "It, it rains or it doesn't, and the loss we incur a loss if we went with expert I.",
            "Right, the loss we would have in curd is weather is the indicator for expert.",
            "I make a mistake right?",
            "That's what I said it wasn't gonna rain.",
            "It did rain incur we incur loss one for going with that expert in O Clock.",
            "OK, so that's the setting.",
            "It's it's a simple binary.",
            "Decision here the minimax regret, the value of this game again."
        ],
        [
            "Just remind you of the notation.",
            "I guess I skipped over this earlier.",
            "We have the loss that we incur this L in half over N rounds of the game is the sum of all these losses for the actions that we chose and the.",
            "LS star the optimal loss, in retrospect, is the best.",
            "Some of the losses that we could have achieved for any fixed fixed choice.",
            "Uh, actually from that class?",
            "OK, so we want to we want to make our decisions 80 so as to minimize the regret.",
            "The difference between LAN happened for some of us as we incur Dan Helen star.",
            "Best in retrospect.",
            "OK, so let's step back from that and think about."
        ],
        [
            "In an easier situation where the adversary is constrained, I think about an adversary that's constrained too.",
            "Always come up with a sequence of these outcomes.",
            "Why?",
            "Whether it rain or not, that's consistent with one of the experts.",
            "Right, so there's always a perfect expert in this set of size in.",
            "Francis somebody incurs no longer selling star here.",
            "The men over all all all of our experts of the cumulative losses in her zero.",
            "So how should we predict in this case?",
            "Alright, so who's seen as a kid the game?",
            "What's it called the?",
            "I love the face guessing game.",
            "Yes, thank you who sings, guess who at least two great.",
            "OK, so what's the strategy there?",
            "At this big sacrifices, for those of you that haven't seen it, but you play with an opponent that you've got a bunch of different faces in front of, he flipped off, and you also have a card which is your hidden face, right?",
            "The opponent has to guess your interface you up against your opponents in place.",
            "So just think from my perspective, I want to get the codes hidden face.",
            "What's my strategy?",
            "But I've got.",
            "I've got a whole array of the potential faces there and I want to quickly.",
            "Home in on the face.",
            "That's the team problem.",
            "So what do I do?",
            "Last question will slip the poppies, not split.",
            "The options in half.",
            "That's exactly right.",
            "I know exactly one of these faces is the right one, and so I look for a question so you asking things like you know, does your person have Brown hair?",
            "Does your person?",
            "Is your person wearing a hat?",
            "These number things right?",
            "So you ask, you ask a set of questions that splits the remaining the faces that have not been illuminated in half, and then you know that involved two of the number of places you're going to get your going to get the right one.",
            "OK, so it's exactly the same sitting here.",
            "We know there's a.",
            "There's a correct expert and we can home in on that expert by elliptic by trying to by making sure that we if we make any mistakes, were eliminating half of the experts from the set that is still consistent so that."
        ],
        [
            "That's a hobby algorithm, right?",
            "We have this set of experts.",
            "Who been correct so far?",
            "Let's go.",
            "Let's see some tea.",
            "So up to time T the loss.",
            "This is the set of experts I there having heard zero loss.",
            "So the truth might be one of these.",
            "And at that time we just choose any element of the set of experts that are in the majority there right there saying the outcome is going to be rain, and the majority of these consistent experts is.",
            "Again, correct experts is predicting that."
        ],
        [
            "So if we vote with the majority, then we'll incur regret no more than log base.",
            "Two of the number of experts.",
            "OK, because anytime we we get it right, we incur zero loss.",
            "Right and the majority.",
            "Gotta right minority women.",
            "And maybe that was only one one and there was nobody.",
            "Anytime we getting wrong.",
            "Then the majority is limited.",
            "Right, so we're reducing the size of the set CT of experts that have been correct so far by a factor of two.",
            "Every time we make a mistake, so we can't make more than log two in the states.",
            "So yeah, is that there's a proof.",
            "OK, so maybe I should.",
            "Maybe I should go through the proof.",
            "It's completely trivial, but the way that the thing I want to point out here is that we use a measure of progress and measure progress.",
            "Here is the size of that set of correct experts, right?",
            "If we make a mistake then that's it goes down by a factor of 2.",
            "And so, and it's something that's that's constraint is.",
            "There's always at least one correct answer.",
            "That question, doesn't this assume that you've always got a question that can divide the sentence?",
            "That's right, that's right.",
            "So in this case you know we're talking about experts that are predicting 01.",
            "So we always have that option.",
            "But that's right, that's essential to the level 2.",
            "Questions.",
            "OK, so this measure progress idea.",
            "Here is the size of the set of correct experts.",
            "That's a pattern."
        ],
        [
            "Will see again several times in.",
            "In the store.",
            "So here is something that changes when we incur some excess loss, right when we make a mistake then this this size of this center correct experts is changing monotonically right?",
            "It's it's going down by factor of two and it's constrained and kind of pulled by one because there's some expert who predict further.",
            "OK, so that's an easy pace.",
            "We know that there's there's one of these guys that predict perfectly.",
            "What if there's no perfect expert?",
            "What can we do there?",
            "Alright, so you know, going with this sexy teammates makes no sense because you know, it's.",
            "If their adversaries is performing well, CT is going to be empty pretty.",
            "Alright, so.",
            "So we have to be a little a little more clever.",
            "In that in that case.",
            "Alright, so we've seen that if we do ever an expert that makes perfect predictions then we can achieve."
        ],
        [
            "Regret that some older log in right side of her trial regret Raiders like a ball game over in for the in trials.",
            "We'll see an example of a strategy that does well in the general case without so, so for a finite comparison classes prediction with expert advisor, so that achieves regret quotes of order square root end over the scaling with the size of the class is the square root of log in.",
            "So let's have a look at let's have a look at that strategy.",
            "First time you've put your bitch.",
            "Example in real life with him have one.",
            "Assumption that we have been exposed.",
            "Headlights with zero loss.",
            "You mean this case would be looking at?",
            "Yeah, you know it's pretty contrived.",
            "Guess who's in kind of that face right?",
            "But it's it's contrived to have to have the zero loss case.",
            "Um?",
            "What I what I want to draw out with some of these.",
            "I mean it's an easy starting point.",
            "Starting point here, right?",
            "To give you a straightforward algorithm and we'll see that when we look at examples that have.",
            "Where we throw away that assumption, throw away that constraint on the atmosphere that they need to have an expert that makes you a good incurs their loss.",
            "Then the algorithms that work there are generalizations of of this simple hopping algorithm.",
            "So yeah, it's I mean, I'm I wouldn't pretend to to come up with a realistic example where that happens.",
            "It's the same as you know, in the in the probabilistic setting will have a look at this a little later on we get.",
            "We get similar sorts of regret rates in a probabilistic setting if we make the assumption that there is a.",
            "A function in a convenient comparison class whose expected losses zero writes the same kind of crazy crazy assumption.",
            "You know, maybe.",
            "Maybe there are such classification problems, but you know there.",
            "You know, it seems wrong, unrealistic.",
            "Resolution."
        ],
        [
            "Is more general case the more interesting case?",
            "Before we do that, I want to say a little word about mixed strategy.",
            "So rather than going with a single so we had our set of experts.",
            "From here I want to I want to look at.",
            "Allowing mixed strategy.",
            "So our actions now, rather than being one of these experts, is a.",
            "It's a distribution across the set of experts.",
            "And so so the notation I'm going to use here is is this Delta M for the simplex on endpoints.",
            "I guess they minus one dimensional simplex, so it's it's the.",
            "Set of vectors that apply.",
            "A way to reach a very mixed fruits and then we can think of you know there are a couple of different ways of viewing.",
            "Viewing mixed strategies like this.",
            "We could think about strategies rather than choosing a single expert.",
            "It chooses one of those randomly right winger distribution across experts and and maybe drawing from that distribution and the loss that we're interested in is the expectation of that of that box under the distribution would chose.",
            "Or we might think of think of the strategy is playing.",
            "An element of this space of distributions across the experts and the loss we define, we're extending the loss for each of the experts to the simplex just linearly right?",
            "So it's the linear the linear function that has the right value at the vertices of the simplex.",
            "OK, so this EI here.",
            "This EI is the.",
            "The symbol for the experts of the loss.",
            "At that time.",
            "At that vertex is the loss incurred by the by the architecture.",
            "OK, so."
        ],
        [
            "Is a strategy pulled out of the air?",
            "Will have a lot more to say about it and about different ways of doing this this strategy, but it's the exponential weights strategy.",
            "So here the idea is to maintain a distribution across our experts and update that distribution of each step.",
            "OK, so we start off with.",
            "I guess we have to maintain a distribution that will keep keep weights, and they're not necessarily normalized.",
            "So we start off with with a constant weight for each expert and then at time T when we observe the loss Lt we update the weight.",
            "For expertise multiplicatively so we have this.",
            "E to minus, Y to LTI.",
            "For expert I OK so if this loss is big, the weight gets decreased by substantial medical.",
            "So these things that they don't normalize or this parameter either, it's just a parameter of the algorithm that we say little more about that later and then in choosing what distribution to play we play the normalized vector.",
            "So we take this vector of weights and we normalize it divided by the sum of all of these words.",
            "So we.",
            "Play the distribution that corresponds to that on normalized vector weights.",
            "OK.",
            "So.",
            "Is the is?"
        ],
        [
            "Theorem that playing this this strategy with the parameter Aida.",
            "So it looks like squaring ball game in the protesters divided by N is the length of the game and number of rounds we get a regret.",
            "That's a border squaring in.",
            "Square Damian is like squared anymore game.",
            "OK, so why is that true?",
            "Well, the argument I'm going to give here is basically."
        ],
        [
            "Then on this idea of a measure of progress.",
            "So a lot of these ideas against manfreds, it was one of The Pioneers in in this area, so you should direct all your questions at Medford.",
            "So the idea here is to think of again measure progress, and in this case it's going to be the sum of the weights across the different.",
            "The different experts and we'll see that if we incur.",
            "A sequence of losses then this is this some WN across the different experts grows as a function of time, at least as either the minus.",
            "Either times the min over the experts of the cumulative loss.",
            "Right?",
            "Um?",
            "And Conversely.",
            "Um, we we get an upper bound on the rate of growth that depends on the loss that we've incurred so you can see you know the difference between these two is the thing that we we care about, right?",
            "The loss that we've incurred cumulative loss and this thing is the loss of the cumulative loss of the best expert.",
            "Alright, so by keeping track of this measure of progress we did.",
            "We can down the difference between those two.",
            "So why is why is the first one the WN grows at least as elisium in of the cumulative loss of of the experts.",
            "Well, we just need to look at the log of the ratio."
        ],
        [
            "Of this sum of the weights.",
            "Um, divided by what it was at the start.",
            "Right, so it's just one of the staff at each breach of these weights, so that sums up to EM, right?",
            "We get along game here and some near across all of the experts depends on this cumulative loss that they have in curd because each one gets multiplied by a factor here minus.",
            "Either times the locks at every step.",
            "OK, so now we've got a log of a sum across the experts.",
            "Well, that's at least as big as the log of the Max.",
            "OK, this Max is pulling out the best expert.",
            "Right, because there's a - in there, it's the it's the cumulative loss.",
            "The smaller schemas of losses that is the relevant one in here, and we can pull that Max outside and we get a min.",
            "I guess.",
            "Pull the Max outside the log in and then take them outside so I don't get 'em in across experts accused of loss and that's just our stuff.",
            "OK. And and the other side, if we think about just one step."
        ],
        [
            "Right, what's the log of the total weight at time T + 1 divided by the total weight of time T?",
            "So that's at time T + 1.",
            "When multiplying your weights by this exponential factor.",
            "Friday, I think spurt at time T had this way and incur this loss, so we multiply by Elis, either Lt. Now that thing we get an inequality here by thinking of this as a probability distribution across across experts and saying this thing inside the log is the expectation under a random choice of the EI.",
            "Right given by these W's.",
            "Expectation of either by the Cedar Lt. Aviar.",
            "OK, so this thing here inside the log is an expectation.",
            "And the inequality there is perfect inequality.",
            "He's heard of her things inequality.",
            "You should have seen this.",
            "It's on the earlier talks, so.",
            "So they think that equality tells us how concentrated it is used to tell us how concentrated a sample averages about about an expectation for abounded random variable, right?",
            "So if anything to equality tells that we've got a random variable that's bound in the range AB.",
            "Then the log of the moment generating function.",
            "So this expectation of either Lambda X.",
            "Is bounded by Lambda times the expectation X plus.",
            "Something involving.",
            "Lambda squared and the range B -- A ^2.",
            "OK, so we get in an alphabet on the on the moment generating function.",
            "Well, that's actually this quantity right?",
            "This thing up here is the expectation.",
            "Under this distribution, the one corresponding to these weights of E to the minus to eat a times random variable.",
            "But this is a random variable with chosen with that probability, so that's the that's the moment generating function were interested in the log of the expectation, and we can bound in terms of victims inequality.",
            "But then once we have that right, this this quantity here WIT over the summer."
        ],
        [
            "Distribution we play a T and that's the way we apply to Lt.",
            "This is exactly the loss incurred by algorithm.",
            "Right, so we have.",
            "We have a lot of our algorithm popping in here.",
            "As the other side of this argument about the measure of progress.",
            "OK, so you know, putting those two together we get a comparison between the loss that we incur a cumulative loss.",
            "This is one step loss that the cumulative loss that we incur and the loss from this other step of the of the best.",
            "The best expert.",
            "Put those two together.",
            "And.",
            "Right, we get we get these upper and lower bounds or no measure of progress that give us regret is bounded by logging over heater plus either end over 8.",
            "An appropriate choice of this of this constant parameter of the algorithm gives us the result right.",
            "The square root log in over in bounded.",
            "The regret for this for this strategy.",
            "OK, so.",
            "I skipped over the proof of her things inequality, so there's a really nice proof base."
        ],
        [
            "Exponential family random variables.",
            "For those of you that know about properties of.",
            "Of the exponential family, that's really out.",
            "One line.",
            "One line proof to get to move teams inequality.",
            "That was something that they apologized recently.",
            "So that's really pretty.",
            "You can go back and look at that yourself."
        ],
        [
            "Alright, so the key points here for the for the expert setting where we had this finite set of actions, we had the.",
            "In this rather contrived case, where one of them incurs zero loss, we get along game over N per round.",
            "Regret right is the number of rounds and is the number of experts, and we saw this particular algorithm.",
            "The exponential weights strategy gives us a per round regret of.",
            "The order of square root log in Mandarin.",
            "OK."
        ],
        [
            "So.",
            "There are a bunch of refinements and twists and turns that we can we can apply here.",
            "You know we might.",
            "We might be interested in in what happens if we're lucky enough to have L Star 0, right?",
            "So if we happen to find ourselves in that setting where the best expert include zero loss, do we get the faster rate?",
            "Do we get the one over end her around?",
            "Regret rail?",
            "Hub the other thing that.",
            "Pops out here.",
            "We're setting either according to end, the number of rounds of the game.",
            "So do we need to set this parameter before the game starts with?",
            "With full knowledge of how long we're going to be playing, that's not very satisfactory.",
            "So it turns out, no, we don't that you can.",
            "You can set things up so that.",
            "You don't need to know the the length of the game in advance.",
            "Also you can you can.",
            "Refine the argument to look at to look at what happens in this case where the.",
            "Optimal.",
            "Expert has zero loss, so here you are."
        ],
        [
            "Really is just replacing.",
            "This is an analogue of the sorts of arguments we make in the in the probabilistic setting.",
            "We're just replacing Perkins inequality with something something like an inequality that's involved in deriving Bernstein inequality, right?",
            "So we get by by doing.",
            "Replacing this sort of inequality with with something else.",
            "When the when the loss is small we get.",
            "We get a when Ellen Star is small.",
            "We get something that can be much, much better.",
            "So for instance, if Alistar is 0 and we choose this.",
            "Fred."
        ],
        [
            "Either to be large, then we get a regret failed.",
            "That's roughly at the order of log in under the end.",
            "Again, regret bounds roughly roughly log in, and so the regret bound is is like this morning over in the per round regret.",
            "But then if you step back and think about what that corresponds to, if we know in advance when star is going to be 0 and we set this parameter needed to be really large.",
            "That's that's multiplying my life even minus.",
            "Either times the loss, right?",
            "If you incur any loss and neither is huge, then you dropping the weight enormously.",
            "Qualitatively, it's just like the having algorithm, right?",
            "We're starting out with uniform weight across the across the experts.",
            "And anytime somebody incurs a mistake, we we drop their weight by an enormous factor in the eater is large, it's it's.",
            "It's qualitatively like.",
            "The approach taken in the Harvey album voting with the majority of the guys that have had not yet include any mistakes.",
            "Are you is your account for hurting better in some languages and abounded in upper middle tier?",
            "Always this one always.",
            "The bound for hurting so.",
            "So yeah it is.",
            "But by Constance I guess I mean this one's a little uglier to work with two.",
            "But I think you do get an improvement in Constance from from the Earth inequality.",
            "Then from this time, because because he had, you know these extra, either multi terms to at least two approximating, overcommitted, physically there.",
            "So the setting of the setting Aida groups.",
            "Um?",
            "So the closest that we get at the end from from Nelson to Civita, I think is is better.",
            "Actually, I'm not.",
            "I'm not entirely sure if it's impossible to.",
            "To get as good as this from the first time.",
            "OK."
        ],
        [
            "This other question about knowing the horizon time.",
            "You can get a similar regret bound without knowing in advance how long the game is going to last.",
            "You can use it I'm hearing.",
            "Parameter.",
            "And the analysis is a little more difficult and involves it produces worse place constants in the result that you get the same the same, right?",
            "And actually, there's a there adaptive approaches.",
            "You know.",
            "We saw that if if Ln star is small, then we can do do better.",
            "In fact, as long as alien star is growing slower than than linearly with them, then we can do better than the than the hurting inequality provides.",
            "But we need to set either as a function of that of that Elaine Star and turns out you can do that in an active way too, by setting it as a function of the best.",
            "Humans have lost so far.",
            "So you know there are these adaptive refinement.",
            "Image.",
            "Um, OK and."
        ],
        [
            "Is also an interpretation of this exponential weight strategy.",
            "Is computing a Bayesian posterior with inappropriate definition of amazing, amazing prior or now our set of of experts and appropriate definition of the of the.",
            "Distribution of the data sequence that we see.",
            "Um?",
            "And one other."
        ],
        [
            "And I guess I deployed the losses as a linear linear extension of the loss at the vertices of the simplex, right?",
            "The loss we have.",
            "Each one of these experts.",
            "Actually we could define with exactly the same argument.",
            "We could deal with any bounded convex function on the simplex, right?",
            "We don't just have to have linear functions and compete with convex losses in that in that sort of way, and the only the only changes that.",
            "Our equality right?",
            "This thing was defined as the loss at the .80 that we played becomes an inequality when this when this losses, dysfunction.",
            "So we get it down for any convex function."
        ],
        [
            "Um, I guess.",
            "The weakness of that that consequences of thinking about convex functions is that we're really only competing with the corners of the.",
            "We really only competing with the corners of the simplex, right?",
            "So we're playing playing these guys, and we have convex losses defined, but where?",
            "Where in Kering regret relative to the corners of the simplex?",
            "The individual experts, rather than two arbitrary points within the simplex, and we'll see later that there are there.",
            "Refinements that this works with.",
            "Compare ourselves with the.",
            "The set of distribution so points in the interior of the simplex support.",
            "OK."
        ],
        [
            "So the last thing that I want to.",
            "Talk about before we take a break.",
            "The brakes at 3:30 for 20 minutes, right?",
            "So the last thing I want to talk about before we take a break here is the analogues statistical setting, right?",
            "So they think about statistical prediction problem, so there's no longer an adversary who's who's competing with this providing a sequence of data.",
            "The data is coming along I ID and we're working with a finite class, so we're trying to.",
            "Predict.",
            "Either way, that compares favorably with the best we could do using a finite family of prediction rules.",
            "So here's the probabilistic formula."
        ],
        [
            "We have a sample of size N drawn IID from a fixed unknown probability distribution on XY.",
            "So Exodus base of patterns and wires, aspects of labels and we see these XY pairs right, any of them?",
            "Their ID, and they have the same distribution as this, this pair XY and we have some statistical method that uses that sequence of data and comes up with in half right.",
            "If pad is a mapping from X to Y.",
            "It's a it's a decision rule, so for some subsequent X user had to decide on the best.",
            "The best action to take and we incur regret that depends on our loss function.",
            "You know this is how bad it is to forecast epad of X when the real outcome is why?",
            "OK, so it's the expectation of that loss that we're hoping to minimize here.",
            "So things are done in the statistical primary and we can define the regret as the expected loss minus the mean over some class is the family of decision rules that we might then we might have used right?",
            "I'll be expected loss.",
            "So this is the best expected loss that we could incur for that distribution.",
            "When we are restricted to making our decisions using the functions from the class after work OK.",
            "So this regret and the previous regret are a little different.",
            "There a factor of an apart in some sense.",
            "I mean, they're totally different assumptions about the weather.",
            "Data is generated, but we were thinking about the expectation of a loss we incur with this single independent XY pair versus cumulative loss over a sequence of LinkedIn."
        ],
        [
            "OK, so this is a.",
            "A probabilistic prediction problem.",
            "And we can think about various strategies in the probabilistic setting.",
            "The strategies can be rather trivial.",
            "Right so so if, for instance, let's let's think about the case where there is a function in the class that incurs zero loss, zero expected, expected loss.",
            "So some F star in our class capital F as the expected loss 0.",
            "And this is, again, it's a very very strong assumption.",
            "Then all we need to do is choose any attack in the class of.",
            "Functions that are consistent.",
            "So he had here because I didn't define the previous slide he had.",
            "Here is the average across the transition, next one through XY and X1Y one through XN, YM, right?",
            "So it's the empirical.",
            "The expectation of the empirical distribution of the loss critical distribution is average across these colors.",
            "OK, So what I mean here?",
            "Well, that's just picking at F that incurs no loss on any of the XY pairs.",
            "OK, so in that case for real.",
            "If we take any function from this set, then we get a regret that some order log of the cardinality of the set divided by hand.",
            "OK, which is just the same as we saw in the.",
            "Same same regret around as we saw in the adverse aerial setting.",
            "It's a little different.",
            "We don't have to be so careful here is to to go with the majority.",
            "We're just choosing.",
            "We're just choosing anything this that's being correct.",
            "So far, alright?",
            "Because when I'm in this cubicle, King of loss with the necessary."
        ],
        [
            "Again, the proof of this is is pretty straightforward.",
            "The.",
            "Yeah, it's it's not so interesting.",
            "We're just we're just worrying about you know, if there is somebody who has.",
            "But the probability that we had last bigger than some small amount we can bound with the Union bound probability that some function in them in the class.",
            "We think in here with the one that we would pick.",
            "Using our our method, FF has empirical risk 0, but the expected loss is bigger than epsilon.",
            "OK, so it's getting N successes.",
            "Ride with the probability of success is is 1 minus epsilon and we're taking union down the probability of some function in their class.",
            "Having these sequence of good outcomes when the loss is big is is no more than the size of the class times one more step, son to the end.",
            "So we get down there and integrate the tail bound and get the expected loss bound by this point.",
            "OK."
        ],
        [
            "And there's an analogous result when we throw away the assumption that they some function in the class that has zero loss choosing.",
            "Seven, so here again, the strategy is very simple.",
            "We just pick out a path to minimize the empirical risk for the sample average of the losses.",
            "And that gives us a regret that sub order square root log of the cardinality because divided by N. And.",
            "I'm just going to."
        ],
        [
            "Now you can go check out the proof.",
            "The bottom line here is that we get the hair round regret exactly the same as the per round regret in the adverse aerial center.",
            "So you know the two are really."
        ],
        [
            "Very closely related and will see there are a bunch of other cases where we get quite similar.",
            "Strong relationship between what goes on in a probabilistic setting and location in the Adversario said.",
            "Um?",
            "So you know that's the that's the thing to keep in mind when we have the case of one element of our class is perfect.",
            "We get a log cardinality.",
            "Their phone renders our per round regret, and in general we can get off a log square root of log cardinality of red.",
            "The slower a slower rate, and that's exactly what we see in the in the adversary face.",
            "Alright, so after the break we'll have a look at at.",
            "What's behind this in the sense to look at how we can?"
        ],
        [
            "Make an online algorithm that successful and converted successful for the adversarial setting and converted to a statistical method that says successful in a probabilistic settings.",
            "Alright, so very simple.",
            "Simple conversion between those two.",
            "Any questions before we?",
            "Alright, so we have 20 minutes every Saturday night.",
            "Dentistry for years phone company got coffee after lunch."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "My name is Steve Bartlett and I'm going to be telling you about online learning online decision problems.",
                    "label": 0
                },
                {
                    "sent": "For the next.",
                    "label": 0
                },
                {
                    "sent": "Half hours OK, so online learning the setting here is we have a repeated game is there.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we have we have a repeated game so it's a little different from the funds of formulations of learning models that you've seen that you've seen, or indeed.",
                    "label": 0
                },
                {
                    "sent": "That's awesome so far.",
                    "label": 0
                },
                {
                    "sent": "The setting here is where we're playing a game, so the decision method gets to choose an action at time T, let's call it 80, and then the environment reveals a loss function.",
                    "label": 0
                },
                {
                    "sent": "El Subte from some from some said and this is the tells you the loss that you incur for any action that you could play and the aim is to minimize the cumulative sum of the loss right so?",
                    "label": 0
                },
                {
                    "sent": "We look at a whole bunch of examples in a moment, but just think of Lt telling us how good or bad it was to play whatever it was we played and at the end of it all we want some of those losses.",
                    "label": 0
                },
                {
                    "sent": "In retrospect, be out too bad.",
                    "label": 1
                },
                {
                    "sent": "And it's a deterministic setting so.",
                    "label": 0
                },
                {
                    "sent": "We're not imposing.",
                    "label": 0
                },
                {
                    "sent": "Probabilistic kinds of assumptions about the process that generates the.",
                    "label": 0
                },
                {
                    "sent": "The sequence of losses that we see.",
                    "label": 0
                },
                {
                    "sent": "So minimizing costs in an absolute sense is is kind of hopeless, and the aim is to minimize your loss in a relative sense to minimize something that we call regret, and that is to perform almost as well in the loss that we incur as the best we could have done in retrospect when we restrict our actions to some summer class.",
                    "label": 0
                },
                {
                    "sent": "Alright, so that's the.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "The formulation.",
                    "label": 0
                },
                {
                    "sent": "And the key point here is that the data can be adversarially chosen, so we're not thinking about probabilistic sorts of assumptions.",
                    "label": 1
                },
                {
                    "sent": "And the.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Regret then, is the value of this game where there's this alternating alternating sequence of Mens and Max is we want to minimize choose our action at the first step to minimize.",
                    "label": 1
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "The regret the adversary plays a loss at the first step that aims to maximize our regret, and so on.",
                    "label": 0
                },
                {
                    "sent": "And at the end of it all, this is our regrets, the loss, the sequence of losses we incurred.",
                    "label": 0
                },
                {
                    "sent": "And there's the best that we could have done.",
                    "label": 0
                },
                {
                    "sent": "Looking in retrospect, the sequence of losses that the adversary players.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's.",
                    "label": 0
                },
                {
                    "sent": "That's the.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's the setting.",
                    "label": 0
                },
                {
                    "sent": "That I'm going to be talking about, so why do we think about these adversarial models?",
                    "label": 0
                },
                {
                    "sent": "Well, there are a bunch of reasons I want to tell you about.",
                    "label": 0
                },
                {
                    "sent": "Several application areas where an adversary model is really quite appropriate.",
                    "label": 0
                },
                {
                    "sent": "You really are competing with somebody in making your predictions and will look at a couple of examples from computer security and from computational finance.",
                    "label": 0
                },
                {
                    "sent": "But one of the I think maybe even the more appealing aspect to it is that we really have don't have to make many assumptions at all when we look at things in this adverse aerial setting, right?",
                    "label": 0
                },
                {
                    "sent": "So it turns out that it's it's really straightforward to convert a successful strategy that can deal with an adverse aerial environment to a method that thing that can deal with the probabilistic environment.",
                    "label": 1
                },
                {
                    "sent": "And actually the converse is also true.",
                    "label": 1
                },
                {
                    "sent": "It's interesting to study this adversarial model.",
                    "label": 0
                },
                {
                    "sent": "In this study, the performance of statistical methods methods have been developed, but from.",
                    "label": 0
                },
                {
                    "sent": "Probabilistic kinds of settings.",
                    "label": 0
                },
                {
                    "sent": "It's interesting to do that because we get a good understanding of what's at the heart of the statistical problem and will see that there are strong correspondences between the sorts of performance guarantees we can get in the statistical case and the and the adversarial case.",
                    "label": 1
                },
                {
                    "sent": "So you know, I think that we get a lot of insight there.",
                    "label": 0
                },
                {
                    "sent": "We also understand the robustness of statistical methods to the kinds of assumptions that that.",
                    "label": 0
                },
                {
                    "sent": "Windows Phone.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And you know, there are some curious things that there's a lot of overlap in the design of methods for these adverse aerial problems.",
                    "label": 0
                },
                {
                    "sent": "Similarity between the types of tools that are useful there and the types of tools that are useful in statistical statistical settings also.",
                    "label": 0
                },
                {
                    "sent": "Alright, so let's let's talk about adverse aerial models and application areas where adverse aerial models are.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Previous computer security is 1 area where.",
                    "label": 0
                },
                {
                    "sent": "Where we really can think of the decision problem as playing a repeated game.",
                    "label": 0
                },
                {
                    "sent": "Let's say somebody sending you.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Spam messages and you have an algorithm for detecting spam, then you know it's in their interest to have your predictions be wrong.",
                    "label": 0
                },
                {
                    "sent": "And they certainly don't want to change the kinds of.",
                    "label": 0
                },
                {
                    "sent": "The kinds of features of the email they send in response to the sorts of decisions that you make about.",
                    "label": 0
                },
                {
                    "sent": "The best answer.",
                    "label": 0
                },
                {
                    "sent": "OK, so putting it in the terminology that we saw before the action that we can take is to choose the classification of alright some function that some features of email messages.",
                    "label": 0
                },
                {
                    "sent": "To a decision as to whether a particular email is spam or not, the loss that that we see might be might correspond to a particular email that gets sent to our.",
                    "label": 1
                },
                {
                    "sent": "Our account and it might be indicated for that email being corrected.",
                    "label": 0
                },
                {
                    "sent": "Basketball, I'd like to spam is allowed through, for instance, or we throw away.",
                    "label": 0
                },
                {
                    "sent": "Message.",
                    "label": 0
                },
                {
                    "sent": "And so in this setting you know the the the adversary who's who's sending spam by trying to modify the spam in response to our decisions.",
                    "label": 0
                },
                {
                    "sent": "So we really do need to to consider the adverse aerial aspect in in modeling this problem, seeing as as a stochastic process that generated the data is missing that aspect.",
                    "label": 0
                },
                {
                    "sent": "So you know and.",
                    "label": 1
                },
                {
                    "sent": "Talked already about this, this business of minimizing regret.",
                    "label": 0
                },
                {
                    "sent": "You know, depending on how we model things, if we're looking at an adverse aerial setting where we have.",
                    "label": 0
                },
                {
                    "sent": "Where an adversary has control of the whole sequence then then getting good classification accuracy in an absolute sense is is hopeless.",
                    "label": 1
                },
                {
                    "sent": "But we might hope to minimize regret in the sense of having our spam detection accuracy close to the best that we could have done it in perspective.",
                    "label": 0
                },
                {
                    "sent": "That's the kind of criterion talking about here.",
                    "label": 0
                },
                {
                    "sent": "Suzanne how to put yes like you are going to predict the loss function depending upon what word reveals.",
                    "label": 0
                },
                {
                    "sent": "But it though in that information is not available like we need to minimize regret.",
                    "label": 0
                },
                {
                    "sent": "And which is based on those El Subte yes, and we want to minimize Elsa peak.",
                    "label": 0
                },
                {
                    "sent": "But what if we do not have that information?",
                    "label": 0
                },
                {
                    "sent": "Whether our prediction was right or not, right?",
                    "label": 0
                },
                {
                    "sent": "That's a good question so.",
                    "label": 1
                },
                {
                    "sent": "The question is, what if we don't get to see the functional El Subte at each time step?",
                    "label": 0
                },
                {
                    "sent": "So we want to minimize the sum of El Subte, right?",
                    "label": 0
                },
                {
                    "sent": "But we may not have complete information about what what would have happened in this example.",
                    "label": 0
                },
                {
                    "sent": "You know, for instance, if let's be a little concrete.",
                    "label": 0
                },
                {
                    "sent": "So let me.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Get back to that end of this lot, right?",
                    "label": 0
                },
                {
                    "sent": "So I think just a little concretely about this example.",
                    "label": 0
                },
                {
                    "sent": "We might be deciding on the basis of some features of the of an email message.",
                    "label": 1
                },
                {
                    "sent": "Formation about the header words in the message and so on.",
                    "label": 1
                },
                {
                    "sent": "Making a decision about whether we would classify that as as as spam or not, or maybe a more refined decision we could.",
                    "label": 0
                },
                {
                    "sent": "We could choose an action that's a real number between zero and one.",
                    "label": 0
                },
                {
                    "sent": "You can think of that as something like predicting a probability that the message is spam, and at each round the adversary is choosing the message.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Any loss is the squared loss, so YT is the indicator for spam and 80 is our our guess.",
                    "label": 0
                },
                {
                    "sent": "It's a function now that that's from features to the Angels are one and the last.",
                    "label": 0
                },
                {
                    "sent": "We encourage the square difference.",
                    "label": 1
                },
                {
                    "sent": "OK so and then regret is defined as the XSQF squared error over the best that was achievable on that particular sequence.",
                    "label": 0
                },
                {
                    "sent": "So in this case.",
                    "label": 0
                },
                {
                    "sent": "We see a loss function if we know the.",
                    "label": 0
                },
                {
                    "sent": "Value why afterwards, for instance, that might be from the user's decision about whether this is a spam message or not.",
                    "label": 0
                },
                {
                    "sent": "If we get.",
                    "label": 0
                },
                {
                    "sent": "If it gets passed through or the user rescuing him from the stand Holder, but it may be.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's a.",
                    "label": 0
                },
                {
                    "sent": "That's an example where we may not get perfect information about about a loss function, right?",
                    "label": 0
                },
                {
                    "sent": "So in this case, for instance, we might not see the why every time, and there are other other cases where we get limited information about the loss, and we still want to minimize the the cumulative loss.",
                    "label": 0
                },
                {
                    "sent": "So those kinds of restricted information games are also very interesting, and there you know so.",
                    "label": 0
                },
                {
                    "sent": "So what happens the way the way we formulate those games is we get a certain piece of information at each step, and we incur a certain loss at each step.",
                    "label": 0
                },
                {
                    "sent": "And you know, keeping track of how we can use that information in order to minimize the cumulative losses instead of the game.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to be talking so much about the limited information case in this talk.",
                    "label": 0
                },
                {
                    "sent": "Maybe a little bit later on, but very little.",
                    "label": 0
                },
                {
                    "sent": "Um, I'm mostly focused on the case where you you get to see the last function in its entire T, and so you know what what different actions would have cost you at any point.",
                    "label": 0
                },
                {
                    "sent": "No, but that's a that's certainly a very, very important class of problems that partial information games.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, there are other examples in computer security.",
                    "label": 1
                },
                {
                    "sent": "This is a graph from a website and classification competition.",
                    "label": 0
                },
                {
                    "sent": "The black nodes are spam and the white nodes are other websites and other classification problem.",
                    "label": 0
                },
                {
                    "sent": "You want to decide should I be if I'm a search engine?",
                    "label": 0
                },
                {
                    "sent": "Should I be serving up this web page that claims to be CNN or or you know, sure, I still have some other one and of course the people, the people who have been there.",
                    "label": 0
                },
                {
                    "sent": "Their advertisements on the fake CNN page wanna Fool you or your classifier?",
                    "label": 0
                },
                {
                    "sent": "Network security problems.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do not have service sort of attacks and similar similar issues and comments.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Proponents portfolio optimization problems can be viewed in the same sort of way.",
                    "label": 0
                },
                {
                    "sent": "They really are repeated interaction between between a number of players that there should be viewed as a as a competitive interaction.",
                    "label": 0
                },
                {
                    "sent": "So think of the prob.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Choosing a portfolio so that is we have a certain amount of capital and we like to distribute it across a bunch of financial instruments so as to maximize something, you know, utility.",
                    "label": 0
                },
                {
                    "sent": "To distribute in a way that's good for us.",
                    "label": 0
                },
                {
                    "sent": "Typically it's more than just worrying about the return that we make.",
                    "label": 0
                },
                {
                    "sent": "We might worry without worrying about the risk that we've incurred in making that return.",
                    "label": 0
                },
                {
                    "sent": "So there's some notion of utility.",
                    "label": 0
                },
                {
                    "sent": "The adverse aerial aspect of it comes from the fact that other market players in profit by making our decisions bad ones.",
                    "label": 1
                },
                {
                    "sent": "So in this setting, the action that we that we choose, the ATR decisions, are distributions over the financial instruments, the loss.",
                    "label": 0
                },
                {
                    "sent": "Is something the negative of some utility, so it might be the negative logarithm of the portfolios increasing value.",
                    "label": 1
                },
                {
                    "sent": "So if we have a bunch of different instruments.",
                    "label": 0
                },
                {
                    "sent": "Maybe?",
                    "label": 0
                },
                {
                    "sent": "Delete them.",
                    "label": 0
                },
                {
                    "sent": "And this vector RT at time T is the vector of relative price increases, so the ratio of the price at time T to the price of terms for T -- 1.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Then the increase in value portfolio gene product between our distribution 80 and this data RT and wars might be the negative logarithm of that.",
                    "label": 0
                },
                {
                    "sent": "If logarithm is the appropriate behavior question.",
                    "label": 0
                },
                {
                    "sent": "So we might be interested in comparing our performance to the performance of the best instrument in that set.",
                    "label": 0
                },
                {
                    "sent": "Alright, so there the comparison is to distribution over the instruments that that correspond to just the Delta function, like focusing everything on single, a single instrument.",
                    "label": 1
                },
                {
                    "sent": "Or we might be interested in.",
                    "label": 0
                },
                {
                    "sent": "Competing with some set of indices so.",
                    "label": 0
                },
                {
                    "sent": "Articular distribution across a subset of stocks corresponds to the Dow Jones industrial average, and we might be interested in doing well relative to that.",
                    "label": 0
                },
                {
                    "sent": "Or we might be interested in doing well relative to any distribution across across the instruments.",
                    "label": 0
                },
                {
                    "sent": "So this is a.",
                    "label": 0
                },
                {
                    "sent": "At the end about this constantly rebalanced portfolios.",
                    "label": 0
                },
                {
                    "sent": "So where we're trying to compete with any distribution across across instruments with low gloss.",
                    "label": 0
                },
                {
                    "sent": "So just be a little precise right there.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The decisions that we may go distribution across same instruments and at each round the adversary gets to choose a vector of returns, then negative right the ratio of the price of the previous time at the current time.",
                    "label": 1
                },
                {
                    "sent": "For the price the previous time and the losses, the negative loss of the of the negative log of the increase in value of the of the whole portfolio.",
                    "label": 1
                },
                {
                    "sent": "So I regret is the difference between the losses that we incurred so that negative loss, negative log of the ratio of the beginning price and ending price.",
                    "label": 0
                },
                {
                    "sent": "Of beginning value of any value for the whole portfolio.",
                    "label": 0
                },
                {
                    "sent": "This is the smallest loss that that we could have been heard.",
                    "label": 0
                },
                {
                    "sent": "That is, for a fixed in this case fixed distribution across across instruments.",
                    "label": 1
                },
                {
                    "sent": "Fixed mixture of instruments.",
                    "label": 0
                },
                {
                    "sent": "What's the best we could have done?",
                    "label": 0
                },
                {
                    "sent": "OK, so we're trying to get the log here.",
                    "label": 0
                },
                {
                    "sent": "Is pulling out the exponent we're trying to get the optimal rate of growth, comparing ourselves to the best, the best portfolio that we could have chosen at this time.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so um.",
                    "label": 0
                },
                {
                    "sent": "I told you a bit about some of these other motivations, so we will see that we can.",
                    "label": 0
                },
                {
                    "sent": "We can take effective algorithms for deterministic adversary settings and use them in probabilistic settings, so we'll see some conversions for online to batch.",
                    "label": 1
                },
                {
                    "sent": "And that's really very straightforward, so it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a more difficult setting in that sense.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And also this this idea of trying to understand understand the.",
                    "label": 0
                },
                {
                    "sent": "Robustness of statistical methods that are designed for probabilistic settings robustness to the various various assumptions that were that were made in the design look a little bit later on.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Key points of that formulation.",
                    "label": 1
                },
                {
                    "sent": "It's a repeated game that we're playing with, aiming to minimize our regret.",
                    "label": 0
                },
                {
                    "sent": "That's the difference between the loss we actually heard and the best loss we could have in curd.",
                    "label": 0
                },
                {
                    "sent": "When, in retrospect, we look back and think what was the best single action that we could have played throughout.",
                    "label": 0
                },
                {
                    "sent": "And it's again, so the data is adversarially chosen this.",
                    "label": 0
                },
                {
                    "sent": "There's an adversary out there.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "Here's what I'm.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Planning to talk about.",
                    "label": 0
                },
                {
                    "sent": "For the next.",
                    "label": 0
                },
                {
                    "sent": "For the next little over 3 hours, I guess we get some big breaks in there.",
                    "label": 0
                },
                {
                    "sent": "First of all, I want to tell you about finite comparison classes, so this is predicting with expert advice.",
                    "label": 0
                },
                {
                    "sent": "Is there is the the label that you you may have heard of before.",
                    "label": 0
                },
                {
                    "sent": "So here we are thinking of.",
                    "label": 0
                },
                {
                    "sent": "Just exactly the setting that I described, but the the class of decisions that we're choosing from is finite, right?",
                    "label": 0
                },
                {
                    "sent": "And we're competing with the best best member of that class.",
                    "label": 0
                },
                {
                    "sent": "One of the things that I want to draw out is the relationship between adverse aerial and statistical.",
                    "label": 0
                },
                {
                    "sent": "Prediction problems and will see you know one link explicitly in this relationship between online and batch will see a a simple couple simple conversions from online online algorithms to back you up.",
                    "label": 0
                },
                {
                    "sent": "They don't want to talk about.",
                    "label": 0
                },
                {
                    "sent": "I more general formulation online convex optimization.",
                    "label": 1
                },
                {
                    "sent": "So here we are thinking about these cost functions as being convex functions.",
                    "label": 0
                },
                {
                    "sent": "So the adversary is playing from some from some class of Columbia's loss functions and.",
                    "label": 0
                },
                {
                    "sent": "That's important.",
                    "label": 0
                },
                {
                    "sent": "An important special case.",
                    "label": 0
                },
                {
                    "sent": "We've seen some examples of that already low gloss is actually an example of that, but losses kind of tone for Loblaw's best.",
                    "label": 0
                },
                {
                    "sent": "There's a lot that's known about about the optimal decision strategy in various in various settings, and log loss is really rather special, and if we get time will say we'll see a little bit about log loss at the end.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so let's let's talk about finite comparison classes.",
                    "label": 1
                },
                {
                    "sent": "Initially, so this is the setting of prediction with expert advice.",
                    "label": 1
                },
                {
                    "sent": "We come back to today at 11, so let's think about this.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the colors.",
                    "label": 0
                },
                {
                    "sent": "Canonical example here is making a decision about whether it's going to rain tomorrow.",
                    "label": 0
                },
                {
                    "sent": "We have access to a bunch of experts.",
                    "label": 1
                },
                {
                    "sent": "Each of them is going to tell us whether it will rain or not.",
                    "label": 0
                },
                {
                    "sent": "They make their forecasts roll one.",
                    "label": 0
                },
                {
                    "sent": "It's not going to rain, or it is going to rain, and we want to be able to ensure that we can predict almost as well as the best expert in that step, right?",
                    "label": 1
                },
                {
                    "sent": "So we have M experts.",
                    "label": 1
                },
                {
                    "sent": "Now set a of actions we can choose to go with expert one through Expert M. And each of those has some sequence of forecasts that they that they come up with.",
                    "label": 0
                },
                {
                    "sent": "And at round T. There's an actual outcome, right?",
                    "label": 0
                },
                {
                    "sent": "It, it rains or it doesn't, and the loss we incur a loss if we went with expert I.",
                    "label": 0
                },
                {
                    "sent": "Right, the loss we would have in curd is weather is the indicator for expert.",
                    "label": 0
                },
                {
                    "sent": "I make a mistake right?",
                    "label": 0
                },
                {
                    "sent": "That's what I said it wasn't gonna rain.",
                    "label": 0
                },
                {
                    "sent": "It did rain incur we incur loss one for going with that expert in O Clock.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the setting.",
                    "label": 0
                },
                {
                    "sent": "It's it's a simple binary.",
                    "label": 0
                },
                {
                    "sent": "Decision here the minimax regret, the value of this game again.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just remind you of the notation.",
                    "label": 0
                },
                {
                    "sent": "I guess I skipped over this earlier.",
                    "label": 0
                },
                {
                    "sent": "We have the loss that we incur this L in half over N rounds of the game is the sum of all these losses for the actions that we chose and the.",
                    "label": 1
                },
                {
                    "sent": "LS star the optimal loss, in retrospect, is the best.",
                    "label": 0
                },
                {
                    "sent": "Some of the losses that we could have achieved for any fixed fixed choice.",
                    "label": 0
                },
                {
                    "sent": "Uh, actually from that class?",
                    "label": 0
                },
                {
                    "sent": "OK, so we want to we want to make our decisions 80 so as to minimize the regret.",
                    "label": 0
                },
                {
                    "sent": "The difference between LAN happened for some of us as we incur Dan Helen star.",
                    "label": 0
                },
                {
                    "sent": "Best in retrospect.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's step back from that and think about.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In an easier situation where the adversary is constrained, I think about an adversary that's constrained too.",
                    "label": 1
                },
                {
                    "sent": "Always come up with a sequence of these outcomes.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Whether it rain or not, that's consistent with one of the experts.",
                    "label": 0
                },
                {
                    "sent": "Right, so there's always a perfect expert in this set of size in.",
                    "label": 1
                },
                {
                    "sent": "Francis somebody incurs no longer selling star here.",
                    "label": 0
                },
                {
                    "sent": "The men over all all all of our experts of the cumulative losses in her zero.",
                    "label": 1
                },
                {
                    "sent": "So how should we predict in this case?",
                    "label": 0
                },
                {
                    "sent": "Alright, so who's seen as a kid the game?",
                    "label": 0
                },
                {
                    "sent": "What's it called the?",
                    "label": 0
                },
                {
                    "sent": "I love the face guessing game.",
                    "label": 0
                },
                {
                    "sent": "Yes, thank you who sings, guess who at least two great.",
                    "label": 0
                },
                {
                    "sent": "OK, so what's the strategy there?",
                    "label": 0
                },
                {
                    "sent": "At this big sacrifices, for those of you that haven't seen it, but you play with an opponent that you've got a bunch of different faces in front of, he flipped off, and you also have a card which is your hidden face, right?",
                    "label": 0
                },
                {
                    "sent": "The opponent has to guess your interface you up against your opponents in place.",
                    "label": 0
                },
                {
                    "sent": "So just think from my perspective, I want to get the codes hidden face.",
                    "label": 0
                },
                {
                    "sent": "What's my strategy?",
                    "label": 0
                },
                {
                    "sent": "But I've got.",
                    "label": 0
                },
                {
                    "sent": "I've got a whole array of the potential faces there and I want to quickly.",
                    "label": 0
                },
                {
                    "sent": "Home in on the face.",
                    "label": 0
                },
                {
                    "sent": "That's the team problem.",
                    "label": 0
                },
                {
                    "sent": "So what do I do?",
                    "label": 0
                },
                {
                    "sent": "Last question will slip the poppies, not split.",
                    "label": 0
                },
                {
                    "sent": "The options in half.",
                    "label": 0
                },
                {
                    "sent": "That's exactly right.",
                    "label": 0
                },
                {
                    "sent": "I know exactly one of these faces is the right one, and so I look for a question so you asking things like you know, does your person have Brown hair?",
                    "label": 0
                },
                {
                    "sent": "Does your person?",
                    "label": 0
                },
                {
                    "sent": "Is your person wearing a hat?",
                    "label": 0
                },
                {
                    "sent": "These number things right?",
                    "label": 0
                },
                {
                    "sent": "So you ask, you ask a set of questions that splits the remaining the faces that have not been illuminated in half, and then you know that involved two of the number of places you're going to get your going to get the right one.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's exactly the same sitting here.",
                    "label": 0
                },
                {
                    "sent": "We know there's a.",
                    "label": 0
                },
                {
                    "sent": "There's a correct expert and we can home in on that expert by elliptic by trying to by making sure that we if we make any mistakes, were eliminating half of the experts from the set that is still consistent so that.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's a hobby algorithm, right?",
                    "label": 0
                },
                {
                    "sent": "We have this set of experts.",
                    "label": 0
                },
                {
                    "sent": "Who been correct so far?",
                    "label": 1
                },
                {
                    "sent": "Let's go.",
                    "label": 0
                },
                {
                    "sent": "Let's see some tea.",
                    "label": 0
                },
                {
                    "sent": "So up to time T the loss.",
                    "label": 1
                },
                {
                    "sent": "This is the set of experts I there having heard zero loss.",
                    "label": 0
                },
                {
                    "sent": "So the truth might be one of these.",
                    "label": 0
                },
                {
                    "sent": "And at that time we just choose any element of the set of experts that are in the majority there right there saying the outcome is going to be rain, and the majority of these consistent experts is.",
                    "label": 0
                },
                {
                    "sent": "Again, correct experts is predicting that.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if we vote with the majority, then we'll incur regret no more than log base.",
                    "label": 1
                },
                {
                    "sent": "Two of the number of experts.",
                    "label": 0
                },
                {
                    "sent": "OK, because anytime we we get it right, we incur zero loss.",
                    "label": 0
                },
                {
                    "sent": "Right and the majority.",
                    "label": 0
                },
                {
                    "sent": "Gotta right minority women.",
                    "label": 0
                },
                {
                    "sent": "And maybe that was only one one and there was nobody.",
                    "label": 0
                },
                {
                    "sent": "Anytime we getting wrong.",
                    "label": 1
                },
                {
                    "sent": "Then the majority is limited.",
                    "label": 1
                },
                {
                    "sent": "Right, so we're reducing the size of the set CT of experts that have been correct so far by a factor of two.",
                    "label": 0
                },
                {
                    "sent": "Every time we make a mistake, so we can't make more than log two in the states.",
                    "label": 0
                },
                {
                    "sent": "So yeah, is that there's a proof.",
                    "label": 0
                },
                {
                    "sent": "OK, so maybe I should.",
                    "label": 0
                },
                {
                    "sent": "Maybe I should go through the proof.",
                    "label": 0
                },
                {
                    "sent": "It's completely trivial, but the way that the thing I want to point out here is that we use a measure of progress and measure progress.",
                    "label": 0
                },
                {
                    "sent": "Here is the size of that set of correct experts, right?",
                    "label": 0
                },
                {
                    "sent": "If we make a mistake then that's it goes down by a factor of 2.",
                    "label": 0
                },
                {
                    "sent": "And so, and it's something that's that's constraint is.",
                    "label": 0
                },
                {
                    "sent": "There's always at least one correct answer.",
                    "label": 0
                },
                {
                    "sent": "That question, doesn't this assume that you've always got a question that can divide the sentence?",
                    "label": 0
                },
                {
                    "sent": "That's right, that's right.",
                    "label": 0
                },
                {
                    "sent": "So in this case you know we're talking about experts that are predicting 01.",
                    "label": 0
                },
                {
                    "sent": "So we always have that option.",
                    "label": 0
                },
                {
                    "sent": "But that's right, that's essential to the level 2.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "OK, so this measure progress idea.",
                    "label": 0
                },
                {
                    "sent": "Here is the size of the set of correct experts.",
                    "label": 0
                },
                {
                    "sent": "That's a pattern.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Will see again several times in.",
                    "label": 1
                },
                {
                    "sent": "In the store.",
                    "label": 1
                },
                {
                    "sent": "So here is something that changes when we incur some excess loss, right when we make a mistake then this this size of this center correct experts is changing monotonically right?",
                    "label": 0
                },
                {
                    "sent": "It's it's going down by factor of two and it's constrained and kind of pulled by one because there's some expert who predict further.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's an easy pace.",
                    "label": 0
                },
                {
                    "sent": "We know that there's there's one of these guys that predict perfectly.",
                    "label": 0
                },
                {
                    "sent": "What if there's no perfect expert?",
                    "label": 1
                },
                {
                    "sent": "What can we do there?",
                    "label": 1
                },
                {
                    "sent": "Alright, so you know, going with this sexy teammates makes no sense because you know, it's.",
                    "label": 0
                },
                {
                    "sent": "If their adversaries is performing well, CT is going to be empty pretty.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "So we have to be a little a little more clever.",
                    "label": 0
                },
                {
                    "sent": "In that in that case.",
                    "label": 0
                },
                {
                    "sent": "Alright, so we've seen that if we do ever an expert that makes perfect predictions then we can achieve.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Regret that some older log in right side of her trial regret Raiders like a ball game over in for the in trials.",
                    "label": 0
                },
                {
                    "sent": "We'll see an example of a strategy that does well in the general case without so, so for a finite comparison classes prediction with expert advisor, so that achieves regret quotes of order square root end over the scaling with the size of the class is the square root of log in.",
                    "label": 1
                },
                {
                    "sent": "So let's have a look at let's have a look at that strategy.",
                    "label": 0
                },
                {
                    "sent": "First time you've put your bitch.",
                    "label": 0
                },
                {
                    "sent": "Example in real life with him have one.",
                    "label": 0
                },
                {
                    "sent": "Assumption that we have been exposed.",
                    "label": 0
                },
                {
                    "sent": "Headlights with zero loss.",
                    "label": 0
                },
                {
                    "sent": "You mean this case would be looking at?",
                    "label": 0
                },
                {
                    "sent": "Yeah, you know it's pretty contrived.",
                    "label": 0
                },
                {
                    "sent": "Guess who's in kind of that face right?",
                    "label": 0
                },
                {
                    "sent": "But it's it's contrived to have to have the zero loss case.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "What I what I want to draw out with some of these.",
                    "label": 0
                },
                {
                    "sent": "I mean it's an easy starting point.",
                    "label": 0
                },
                {
                    "sent": "Starting point here, right?",
                    "label": 0
                },
                {
                    "sent": "To give you a straightforward algorithm and we'll see that when we look at examples that have.",
                    "label": 0
                },
                {
                    "sent": "Where we throw away that assumption, throw away that constraint on the atmosphere that they need to have an expert that makes you a good incurs their loss.",
                    "label": 0
                },
                {
                    "sent": "Then the algorithms that work there are generalizations of of this simple hopping algorithm.",
                    "label": 1
                },
                {
                    "sent": "So yeah, it's I mean, I'm I wouldn't pretend to to come up with a realistic example where that happens.",
                    "label": 0
                },
                {
                    "sent": "It's the same as you know, in the in the probabilistic setting will have a look at this a little later on we get.",
                    "label": 0
                },
                {
                    "sent": "We get similar sorts of regret rates in a probabilistic setting if we make the assumption that there is a.",
                    "label": 1
                },
                {
                    "sent": "A function in a convenient comparison class whose expected losses zero writes the same kind of crazy crazy assumption.",
                    "label": 0
                },
                {
                    "sent": "You know, maybe.",
                    "label": 0
                },
                {
                    "sent": "Maybe there are such classification problems, but you know there.",
                    "label": 0
                },
                {
                    "sent": "You know, it seems wrong, unrealistic.",
                    "label": 0
                },
                {
                    "sent": "Resolution.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is more general case the more interesting case?",
                    "label": 0
                },
                {
                    "sent": "Before we do that, I want to say a little word about mixed strategy.",
                    "label": 0
                },
                {
                    "sent": "So rather than going with a single so we had our set of experts.",
                    "label": 1
                },
                {
                    "sent": "From here I want to I want to look at.",
                    "label": 0
                },
                {
                    "sent": "Allowing mixed strategy.",
                    "label": 0
                },
                {
                    "sent": "So our actions now, rather than being one of these experts, is a.",
                    "label": 1
                },
                {
                    "sent": "It's a distribution across the set of experts.",
                    "label": 0
                },
                {
                    "sent": "And so so the notation I'm going to use here is is this Delta M for the simplex on endpoints.",
                    "label": 0
                },
                {
                    "sent": "I guess they minus one dimensional simplex, so it's it's the.",
                    "label": 0
                },
                {
                    "sent": "Set of vectors that apply.",
                    "label": 0
                },
                {
                    "sent": "A way to reach a very mixed fruits and then we can think of you know there are a couple of different ways of viewing.",
                    "label": 1
                },
                {
                    "sent": "Viewing mixed strategies like this.",
                    "label": 0
                },
                {
                    "sent": "We could think about strategies rather than choosing a single expert.",
                    "label": 0
                },
                {
                    "sent": "It chooses one of those randomly right winger distribution across experts and and maybe drawing from that distribution and the loss that we're interested in is the expectation of that of that box under the distribution would chose.",
                    "label": 0
                },
                {
                    "sent": "Or we might think of think of the strategy is playing.",
                    "label": 1
                },
                {
                    "sent": "An element of this space of distributions across the experts and the loss we define, we're extending the loss for each of the experts to the simplex just linearly right?",
                    "label": 1
                },
                {
                    "sent": "So it's the linear the linear function that has the right value at the vertices of the simplex.",
                    "label": 0
                },
                {
                    "sent": "OK, so this EI here.",
                    "label": 0
                },
                {
                    "sent": "This EI is the.",
                    "label": 0
                },
                {
                    "sent": "The symbol for the experts of the loss.",
                    "label": 1
                },
                {
                    "sent": "At that time.",
                    "label": 0
                },
                {
                    "sent": "At that vertex is the loss incurred by the by the architecture.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is a strategy pulled out of the air?",
                    "label": 1
                },
                {
                    "sent": "Will have a lot more to say about it and about different ways of doing this this strategy, but it's the exponential weights strategy.",
                    "label": 0
                },
                {
                    "sent": "So here the idea is to maintain a distribution across our experts and update that distribution of each step.",
                    "label": 0
                },
                {
                    "sent": "OK, so we start off with.",
                    "label": 0
                },
                {
                    "sent": "I guess we have to maintain a distribution that will keep keep weights, and they're not necessarily normalized.",
                    "label": 0
                },
                {
                    "sent": "So we start off with with a constant weight for each expert and then at time T when we observe the loss Lt we update the weight.",
                    "label": 0
                },
                {
                    "sent": "For expertise multiplicatively so we have this.",
                    "label": 0
                },
                {
                    "sent": "E to minus, Y to LTI.",
                    "label": 0
                },
                {
                    "sent": "For expert I OK so if this loss is big, the weight gets decreased by substantial medical.",
                    "label": 0
                },
                {
                    "sent": "So these things that they don't normalize or this parameter either, it's just a parameter of the algorithm that we say little more about that later and then in choosing what distribution to play we play the normalized vector.",
                    "label": 1
                },
                {
                    "sent": "So we take this vector of weights and we normalize it divided by the sum of all of these words.",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 0
                },
                {
                    "sent": "Play the distribution that corresponds to that on normalized vector weights.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Is the is?",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Theorem that playing this this strategy with the parameter Aida.",
                    "label": 1
                },
                {
                    "sent": "So it looks like squaring ball game in the protesters divided by N is the length of the game and number of rounds we get a regret.",
                    "label": 0
                },
                {
                    "sent": "That's a border squaring in.",
                    "label": 0
                },
                {
                    "sent": "Square Damian is like squared anymore game.",
                    "label": 0
                },
                {
                    "sent": "OK, so why is that true?",
                    "label": 0
                },
                {
                    "sent": "Well, the argument I'm going to give here is basically.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then on this idea of a measure of progress.",
                    "label": 1
                },
                {
                    "sent": "So a lot of these ideas against manfreds, it was one of The Pioneers in in this area, so you should direct all your questions at Medford.",
                    "label": 0
                },
                {
                    "sent": "So the idea here is to think of again measure progress, and in this case it's going to be the sum of the weights across the different.",
                    "label": 0
                },
                {
                    "sent": "The different experts and we'll see that if we incur.",
                    "label": 0
                },
                {
                    "sent": "A sequence of losses then this is this some WN across the different experts grows as a function of time, at least as either the minus.",
                    "label": 0
                },
                {
                    "sent": "Either times the min over the experts of the cumulative loss.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And Conversely.",
                    "label": 0
                },
                {
                    "sent": "Um, we we get an upper bound on the rate of growth that depends on the loss that we've incurred so you can see you know the difference between these two is the thing that we we care about, right?",
                    "label": 0
                },
                {
                    "sent": "The loss that we've incurred cumulative loss and this thing is the loss of the cumulative loss of the best expert.",
                    "label": 0
                },
                {
                    "sent": "Alright, so by keeping track of this measure of progress we did.",
                    "label": 0
                },
                {
                    "sent": "We can down the difference between those two.",
                    "label": 0
                },
                {
                    "sent": "So why is why is the first one the WN grows at least as elisium in of the cumulative loss of of the experts.",
                    "label": 1
                },
                {
                    "sent": "Well, we just need to look at the log of the ratio.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of this sum of the weights.",
                    "label": 0
                },
                {
                    "sent": "Um, divided by what it was at the start.",
                    "label": 0
                },
                {
                    "sent": "Right, so it's just one of the staff at each breach of these weights, so that sums up to EM, right?",
                    "label": 0
                },
                {
                    "sent": "We get along game here and some near across all of the experts depends on this cumulative loss that they have in curd because each one gets multiplied by a factor here minus.",
                    "label": 0
                },
                {
                    "sent": "Either times the locks at every step.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we've got a log of a sum across the experts.",
                    "label": 0
                },
                {
                    "sent": "Well, that's at least as big as the log of the Max.",
                    "label": 0
                },
                {
                    "sent": "OK, this Max is pulling out the best expert.",
                    "label": 0
                },
                {
                    "sent": "Right, because there's a - in there, it's the it's the cumulative loss.",
                    "label": 0
                },
                {
                    "sent": "The smaller schemas of losses that is the relevant one in here, and we can pull that Max outside and we get a min.",
                    "label": 0
                },
                {
                    "sent": "I guess.",
                    "label": 0
                },
                {
                    "sent": "Pull the Max outside the log in and then take them outside so I don't get 'em in across experts accused of loss and that's just our stuff.",
                    "label": 0
                },
                {
                    "sent": "OK. And and the other side, if we think about just one step.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, what's the log of the total weight at time T + 1 divided by the total weight of time T?",
                    "label": 0
                },
                {
                    "sent": "So that's at time T + 1.",
                    "label": 0
                },
                {
                    "sent": "When multiplying your weights by this exponential factor.",
                    "label": 0
                },
                {
                    "sent": "Friday, I think spurt at time T had this way and incur this loss, so we multiply by Elis, either Lt. Now that thing we get an inequality here by thinking of this as a probability distribution across across experts and saying this thing inside the log is the expectation under a random choice of the EI.",
                    "label": 0
                },
                {
                    "sent": "Right given by these W's.",
                    "label": 0
                },
                {
                    "sent": "Expectation of either by the Cedar Lt. Aviar.",
                    "label": 0
                },
                {
                    "sent": "OK, so this thing here inside the log is an expectation.",
                    "label": 0
                },
                {
                    "sent": "And the inequality there is perfect inequality.",
                    "label": 0
                },
                {
                    "sent": "He's heard of her things inequality.",
                    "label": 0
                },
                {
                    "sent": "You should have seen this.",
                    "label": 0
                },
                {
                    "sent": "It's on the earlier talks, so.",
                    "label": 0
                },
                {
                    "sent": "So they think that equality tells us how concentrated it is used to tell us how concentrated a sample averages about about an expectation for abounded random variable, right?",
                    "label": 0
                },
                {
                    "sent": "So if anything to equality tells that we've got a random variable that's bound in the range AB.",
                    "label": 0
                },
                {
                    "sent": "Then the log of the moment generating function.",
                    "label": 0
                },
                {
                    "sent": "So this expectation of either Lambda X.",
                    "label": 0
                },
                {
                    "sent": "Is bounded by Lambda times the expectation X plus.",
                    "label": 0
                },
                {
                    "sent": "Something involving.",
                    "label": 0
                },
                {
                    "sent": "Lambda squared and the range B -- A ^2.",
                    "label": 0
                },
                {
                    "sent": "OK, so we get in an alphabet on the on the moment generating function.",
                    "label": 0
                },
                {
                    "sent": "Well, that's actually this quantity right?",
                    "label": 0
                },
                {
                    "sent": "This thing up here is the expectation.",
                    "label": 0
                },
                {
                    "sent": "Under this distribution, the one corresponding to these weights of E to the minus to eat a times random variable.",
                    "label": 0
                },
                {
                    "sent": "But this is a random variable with chosen with that probability, so that's the that's the moment generating function were interested in the log of the expectation, and we can bound in terms of victims inequality.",
                    "label": 0
                },
                {
                    "sent": "But then once we have that right, this this quantity here WIT over the summer.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Distribution we play a T and that's the way we apply to Lt.",
                    "label": 0
                },
                {
                    "sent": "This is exactly the loss incurred by algorithm.",
                    "label": 0
                },
                {
                    "sent": "Right, so we have.",
                    "label": 0
                },
                {
                    "sent": "We have a lot of our algorithm popping in here.",
                    "label": 0
                },
                {
                    "sent": "As the other side of this argument about the measure of progress.",
                    "label": 0
                },
                {
                    "sent": "OK, so you know, putting those two together we get a comparison between the loss that we incur a cumulative loss.",
                    "label": 0
                },
                {
                    "sent": "This is one step loss that the cumulative loss that we incur and the loss from this other step of the of the best.",
                    "label": 0
                },
                {
                    "sent": "The best expert.",
                    "label": 0
                },
                {
                    "sent": "Put those two together.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Right, we get we get these upper and lower bounds or no measure of progress that give us regret is bounded by logging over heater plus either end over 8.",
                    "label": 0
                },
                {
                    "sent": "An appropriate choice of this of this constant parameter of the algorithm gives us the result right.",
                    "label": 0
                },
                {
                    "sent": "The square root log in over in bounded.",
                    "label": 0
                },
                {
                    "sent": "The regret for this for this strategy.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "I skipped over the proof of her things inequality, so there's a really nice proof base.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Exponential family random variables.",
                    "label": 0
                },
                {
                    "sent": "For those of you that know about properties of.",
                    "label": 0
                },
                {
                    "sent": "Of the exponential family, that's really out.",
                    "label": 1
                },
                {
                    "sent": "One line.",
                    "label": 0
                },
                {
                    "sent": "One line proof to get to move teams inequality.",
                    "label": 0
                },
                {
                    "sent": "That was something that they apologized recently.",
                    "label": 0
                },
                {
                    "sent": "So that's really pretty.",
                    "label": 0
                },
                {
                    "sent": "You can go back and look at that yourself.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so the key points here for the for the expert setting where we had this finite set of actions, we had the.",
                    "label": 1
                },
                {
                    "sent": "In this rather contrived case, where one of them incurs zero loss, we get along game over N per round.",
                    "label": 0
                },
                {
                    "sent": "Regret right is the number of rounds and is the number of experts, and we saw this particular algorithm.",
                    "label": 0
                },
                {
                    "sent": "The exponential weights strategy gives us a per round regret of.",
                    "label": 1
                },
                {
                    "sent": "The order of square root log in Mandarin.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "There are a bunch of refinements and twists and turns that we can we can apply here.",
                    "label": 0
                },
                {
                    "sent": "You know we might.",
                    "label": 0
                },
                {
                    "sent": "We might be interested in in what happens if we're lucky enough to have L Star 0, right?",
                    "label": 0
                },
                {
                    "sent": "So if we happen to find ourselves in that setting where the best expert include zero loss, do we get the faster rate?",
                    "label": 0
                },
                {
                    "sent": "Do we get the one over end her around?",
                    "label": 0
                },
                {
                    "sent": "Regret rail?",
                    "label": 0
                },
                {
                    "sent": "Hub the other thing that.",
                    "label": 0
                },
                {
                    "sent": "Pops out here.",
                    "label": 0
                },
                {
                    "sent": "We're setting either according to end, the number of rounds of the game.",
                    "label": 0
                },
                {
                    "sent": "So do we need to set this parameter before the game starts with?",
                    "label": 1
                },
                {
                    "sent": "With full knowledge of how long we're going to be playing, that's not very satisfactory.",
                    "label": 0
                },
                {
                    "sent": "So it turns out, no, we don't that you can.",
                    "label": 0
                },
                {
                    "sent": "You can set things up so that.",
                    "label": 0
                },
                {
                    "sent": "You don't need to know the the length of the game in advance.",
                    "label": 0
                },
                {
                    "sent": "Also you can you can.",
                    "label": 0
                },
                {
                    "sent": "Refine the argument to look at to look at what happens in this case where the.",
                    "label": 0
                },
                {
                    "sent": "Optimal.",
                    "label": 1
                },
                {
                    "sent": "Expert has zero loss, so here you are.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Really is just replacing.",
                    "label": 0
                },
                {
                    "sent": "This is an analogue of the sorts of arguments we make in the in the probabilistic setting.",
                    "label": 0
                },
                {
                    "sent": "We're just replacing Perkins inequality with something something like an inequality that's involved in deriving Bernstein inequality, right?",
                    "label": 0
                },
                {
                    "sent": "So we get by by doing.",
                    "label": 0
                },
                {
                    "sent": "Replacing this sort of inequality with with something else.",
                    "label": 0
                },
                {
                    "sent": "When the when the loss is small we get.",
                    "label": 0
                },
                {
                    "sent": "We get a when Ellen Star is small.",
                    "label": 0
                },
                {
                    "sent": "We get something that can be much, much better.",
                    "label": 0
                },
                {
                    "sent": "So for instance, if Alistar is 0 and we choose this.",
                    "label": 0
                },
                {
                    "sent": "Fred.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Either to be large, then we get a regret failed.",
                    "label": 1
                },
                {
                    "sent": "That's roughly at the order of log in under the end.",
                    "label": 0
                },
                {
                    "sent": "Again, regret bounds roughly roughly log in, and so the regret bound is is like this morning over in the per round regret.",
                    "label": 1
                },
                {
                    "sent": "But then if you step back and think about what that corresponds to, if we know in advance when star is going to be 0 and we set this parameter needed to be really large.",
                    "label": 0
                },
                {
                    "sent": "That's that's multiplying my life even minus.",
                    "label": 0
                },
                {
                    "sent": "Either times the loss, right?",
                    "label": 0
                },
                {
                    "sent": "If you incur any loss and neither is huge, then you dropping the weight enormously.",
                    "label": 1
                },
                {
                    "sent": "Qualitatively, it's just like the having algorithm, right?",
                    "label": 1
                },
                {
                    "sent": "We're starting out with uniform weight across the across the experts.",
                    "label": 0
                },
                {
                    "sent": "And anytime somebody incurs a mistake, we we drop their weight by an enormous factor in the eater is large, it's it's.",
                    "label": 0
                },
                {
                    "sent": "It's qualitatively like.",
                    "label": 0
                },
                {
                    "sent": "The approach taken in the Harvey album voting with the majority of the guys that have had not yet include any mistakes.",
                    "label": 0
                },
                {
                    "sent": "Are you is your account for hurting better in some languages and abounded in upper middle tier?",
                    "label": 0
                },
                {
                    "sent": "Always this one always.",
                    "label": 0
                },
                {
                    "sent": "The bound for hurting so.",
                    "label": 0
                },
                {
                    "sent": "So yeah it is.",
                    "label": 0
                },
                {
                    "sent": "But by Constance I guess I mean this one's a little uglier to work with two.",
                    "label": 0
                },
                {
                    "sent": "But I think you do get an improvement in Constance from from the Earth inequality.",
                    "label": 0
                },
                {
                    "sent": "Then from this time, because because he had, you know these extra, either multi terms to at least two approximating, overcommitted, physically there.",
                    "label": 0
                },
                {
                    "sent": "So the setting of the setting Aida groups.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So the closest that we get at the end from from Nelson to Civita, I think is is better.",
                    "label": 0
                },
                {
                    "sent": "Actually, I'm not.",
                    "label": 0
                },
                {
                    "sent": "I'm not entirely sure if it's impossible to.",
                    "label": 0
                },
                {
                    "sent": "To get as good as this from the first time.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This other question about knowing the horizon time.",
                    "label": 0
                },
                {
                    "sent": "You can get a similar regret bound without knowing in advance how long the game is going to last.",
                    "label": 0
                },
                {
                    "sent": "You can use it I'm hearing.",
                    "label": 0
                },
                {
                    "sent": "Parameter.",
                    "label": 0
                },
                {
                    "sent": "And the analysis is a little more difficult and involves it produces worse place constants in the result that you get the same the same, right?",
                    "label": 0
                },
                {
                    "sent": "And actually, there's a there adaptive approaches.",
                    "label": 0
                },
                {
                    "sent": "You know.",
                    "label": 0
                },
                {
                    "sent": "We saw that if if Ln star is small, then we can do do better.",
                    "label": 0
                },
                {
                    "sent": "In fact, as long as alien star is growing slower than than linearly with them, then we can do better than the than the hurting inequality provides.",
                    "label": 0
                },
                {
                    "sent": "But we need to set either as a function of that of that Elaine Star and turns out you can do that in an active way too, by setting it as a function of the best.",
                    "label": 1
                },
                {
                    "sent": "Humans have lost so far.",
                    "label": 0
                },
                {
                    "sent": "So you know there are these adaptive refinement.",
                    "label": 0
                },
                {
                    "sent": "Image.",
                    "label": 0
                },
                {
                    "sent": "Um, OK and.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is also an interpretation of this exponential weight strategy.",
                    "label": 0
                },
                {
                    "sent": "Is computing a Bayesian posterior with inappropriate definition of amazing, amazing prior or now our set of of experts and appropriate definition of the of the.",
                    "label": 1
                },
                {
                    "sent": "Distribution of the data sequence that we see.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And one other.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I guess I deployed the losses as a linear linear extension of the loss at the vertices of the simplex, right?",
                    "label": 0
                },
                {
                    "sent": "The loss we have.",
                    "label": 0
                },
                {
                    "sent": "Each one of these experts.",
                    "label": 0
                },
                {
                    "sent": "Actually we could define with exactly the same argument.",
                    "label": 0
                },
                {
                    "sent": "We could deal with any bounded convex function on the simplex, right?",
                    "label": 1
                },
                {
                    "sent": "We don't just have to have linear functions and compete with convex losses in that in that sort of way, and the only the only changes that.",
                    "label": 0
                },
                {
                    "sent": "Our equality right?",
                    "label": 0
                },
                {
                    "sent": "This thing was defined as the loss at the .80 that we played becomes an inequality when this when this losses, dysfunction.",
                    "label": 0
                },
                {
                    "sent": "So we get it down for any convex function.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um, I guess.",
                    "label": 0
                },
                {
                    "sent": "The weakness of that that consequences of thinking about convex functions is that we're really only competing with the corners of the.",
                    "label": 0
                },
                {
                    "sent": "We really only competing with the corners of the simplex, right?",
                    "label": 1
                },
                {
                    "sent": "So we're playing playing these guys, and we have convex losses defined, but where?",
                    "label": 0
                },
                {
                    "sent": "Where in Kering regret relative to the corners of the simplex?",
                    "label": 0
                },
                {
                    "sent": "The individual experts, rather than two arbitrary points within the simplex, and we'll see later that there are there.",
                    "label": 0
                },
                {
                    "sent": "Refinements that this works with.",
                    "label": 0
                },
                {
                    "sent": "Compare ourselves with the.",
                    "label": 0
                },
                {
                    "sent": "The set of distribution so points in the interior of the simplex support.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the last thing that I want to.",
                    "label": 0
                },
                {
                    "sent": "Talk about before we take a break.",
                    "label": 0
                },
                {
                    "sent": "The brakes at 3:30 for 20 minutes, right?",
                    "label": 0
                },
                {
                    "sent": "So the last thing I want to talk about before we take a break here is the analogues statistical setting, right?",
                    "label": 0
                },
                {
                    "sent": "So they think about statistical prediction problem, so there's no longer an adversary who's who's competing with this providing a sequence of data.",
                    "label": 0
                },
                {
                    "sent": "The data is coming along I ID and we're working with a finite class, so we're trying to.",
                    "label": 1
                },
                {
                    "sent": "Predict.",
                    "label": 0
                },
                {
                    "sent": "Either way, that compares favorably with the best we could do using a finite family of prediction rules.",
                    "label": 0
                },
                {
                    "sent": "So here's the probabilistic formula.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have a sample of size N drawn IID from a fixed unknown probability distribution on XY.",
                    "label": 1
                },
                {
                    "sent": "So Exodus base of patterns and wires, aspects of labels and we see these XY pairs right, any of them?",
                    "label": 0
                },
                {
                    "sent": "Their ID, and they have the same distribution as this, this pair XY and we have some statistical method that uses that sequence of data and comes up with in half right.",
                    "label": 0
                },
                {
                    "sent": "If pad is a mapping from X to Y.",
                    "label": 0
                },
                {
                    "sent": "It's a it's a decision rule, so for some subsequent X user had to decide on the best.",
                    "label": 0
                },
                {
                    "sent": "The best action to take and we incur regret that depends on our loss function.",
                    "label": 0
                },
                {
                    "sent": "You know this is how bad it is to forecast epad of X when the real outcome is why?",
                    "label": 0
                },
                {
                    "sent": "OK, so it's the expectation of that loss that we're hoping to minimize here.",
                    "label": 0
                },
                {
                    "sent": "So things are done in the statistical primary and we can define the regret as the expected loss minus the mean over some class is the family of decision rules that we might then we might have used right?",
                    "label": 0
                },
                {
                    "sent": "I'll be expected loss.",
                    "label": 0
                },
                {
                    "sent": "So this is the best expected loss that we could incur for that distribution.",
                    "label": 0
                },
                {
                    "sent": "When we are restricted to making our decisions using the functions from the class after work OK.",
                    "label": 0
                },
                {
                    "sent": "So this regret and the previous regret are a little different.",
                    "label": 0
                },
                {
                    "sent": "There a factor of an apart in some sense.",
                    "label": 0
                },
                {
                    "sent": "I mean, they're totally different assumptions about the weather.",
                    "label": 0
                },
                {
                    "sent": "Data is generated, but we were thinking about the expectation of a loss we incur with this single independent XY pair versus cumulative loss over a sequence of LinkedIn.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is a.",
                    "label": 0
                },
                {
                    "sent": "A probabilistic prediction problem.",
                    "label": 0
                },
                {
                    "sent": "And we can think about various strategies in the probabilistic setting.",
                    "label": 1
                },
                {
                    "sent": "The strategies can be rather trivial.",
                    "label": 0
                },
                {
                    "sent": "Right so so if, for instance, let's let's think about the case where there is a function in the class that incurs zero loss, zero expected, expected loss.",
                    "label": 1
                },
                {
                    "sent": "So some F star in our class capital F as the expected loss 0.",
                    "label": 0
                },
                {
                    "sent": "And this is, again, it's a very very strong assumption.",
                    "label": 0
                },
                {
                    "sent": "Then all we need to do is choose any attack in the class of.",
                    "label": 0
                },
                {
                    "sent": "Functions that are consistent.",
                    "label": 0
                },
                {
                    "sent": "So he had here because I didn't define the previous slide he had.",
                    "label": 0
                },
                {
                    "sent": "Here is the average across the transition, next one through XY and X1Y one through XN, YM, right?",
                    "label": 0
                },
                {
                    "sent": "So it's the empirical.",
                    "label": 0
                },
                {
                    "sent": "The expectation of the empirical distribution of the loss critical distribution is average across these colors.",
                    "label": 0
                },
                {
                    "sent": "OK, So what I mean here?",
                    "label": 0
                },
                {
                    "sent": "Well, that's just picking at F that incurs no loss on any of the XY pairs.",
                    "label": 0
                },
                {
                    "sent": "OK, so in that case for real.",
                    "label": 0
                },
                {
                    "sent": "If we take any function from this set, then we get a regret that some order log of the cardinality of the set divided by hand.",
                    "label": 0
                },
                {
                    "sent": "OK, which is just the same as we saw in the.",
                    "label": 0
                },
                {
                    "sent": "Same same regret around as we saw in the adverse aerial setting.",
                    "label": 0
                },
                {
                    "sent": "It's a little different.",
                    "label": 0
                },
                {
                    "sent": "We don't have to be so careful here is to to go with the majority.",
                    "label": 0
                },
                {
                    "sent": "We're just choosing.",
                    "label": 0
                },
                {
                    "sent": "We're just choosing anything this that's being correct.",
                    "label": 0
                },
                {
                    "sent": "So far, alright?",
                    "label": 0
                },
                {
                    "sent": "Because when I'm in this cubicle, King of loss with the necessary.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Again, the proof of this is is pretty straightforward.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's it's not so interesting.",
                    "label": 0
                },
                {
                    "sent": "We're just we're just worrying about you know, if there is somebody who has.",
                    "label": 0
                },
                {
                    "sent": "But the probability that we had last bigger than some small amount we can bound with the Union bound probability that some function in them in the class.",
                    "label": 0
                },
                {
                    "sent": "We think in here with the one that we would pick.",
                    "label": 0
                },
                {
                    "sent": "Using our our method, FF has empirical risk 0, but the expected loss is bigger than epsilon.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's getting N successes.",
                    "label": 0
                },
                {
                    "sent": "Ride with the probability of success is is 1 minus epsilon and we're taking union down the probability of some function in their class.",
                    "label": 0
                },
                {
                    "sent": "Having these sequence of good outcomes when the loss is big is is no more than the size of the class times one more step, son to the end.",
                    "label": 0
                },
                {
                    "sent": "So we get down there and integrate the tail bound and get the expected loss bound by this point.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there's an analogous result when we throw away the assumption that they some function in the class that has zero loss choosing.",
                    "label": 0
                },
                {
                    "sent": "Seven, so here again, the strategy is very simple.",
                    "label": 0
                },
                {
                    "sent": "We just pick out a path to minimize the empirical risk for the sample average of the losses.",
                    "label": 1
                },
                {
                    "sent": "And that gives us a regret that sub order square root log of the cardinality because divided by N. And.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now you can go check out the proof.",
                    "label": 0
                },
                {
                    "sent": "The bottom line here is that we get the hair round regret exactly the same as the per round regret in the adverse aerial center.",
                    "label": 0
                },
                {
                    "sent": "So you know the two are really.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very closely related and will see there are a bunch of other cases where we get quite similar.",
                    "label": 0
                },
                {
                    "sent": "Strong relationship between what goes on in a probabilistic setting and location in the Adversario said.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So you know that's the that's the thing to keep in mind when we have the case of one element of our class is perfect.",
                    "label": 0
                },
                {
                    "sent": "We get a log cardinality.",
                    "label": 1
                },
                {
                    "sent": "Their phone renders our per round regret, and in general we can get off a log square root of log cardinality of red.",
                    "label": 0
                },
                {
                    "sent": "The slower a slower rate, and that's exactly what we see in the in the adversary face.",
                    "label": 0
                },
                {
                    "sent": "Alright, so after the break we'll have a look at at.",
                    "label": 0
                },
                {
                    "sent": "What's behind this in the sense to look at how we can?",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Make an online algorithm that successful and converted successful for the adversarial setting and converted to a statistical method that says successful in a probabilistic settings.",
                    "label": 0
                },
                {
                    "sent": "Alright, so very simple.",
                    "label": 0
                },
                {
                    "sent": "Simple conversion between those two.",
                    "label": 0
                },
                {
                    "sent": "Any questions before we?",
                    "label": 0
                },
                {
                    "sent": "Alright, so we have 20 minutes every Saturday night.",
                    "label": 0
                },
                {
                    "sent": "Dentistry for years phone company got coffee after lunch.",
                    "label": 0
                }
            ]
        }
    }
}