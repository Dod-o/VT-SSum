{
    "id": "k6ghmie5suyarwofisdu3c5yclpr7bmr",
    "title": "PAC Bayesian Bounds for Spare Regression Estimation with Exponential Weights",
    "info": {
        "author": [
            "Pierre Alquier, Probabilities and Random Models Laboratory, Universit\u00e9 Pierre et Marie Curie (UPMC)"
        ],
        "published": "April 14, 2010",
        "recorded": "March 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/pacbayesian_alquier_pbb/",
    "segmentation": [
        [
            "This exposes talk is about sparse regression estimation, so this is a joint work with Community who defended his PhD last year at University Paris 7, but is now income, regional, postdoctoral position, and.",
            "So there are a lot of papers recently about sparse regression estimation and actually before explaining what we did with Karim, I just like to give a short bibliography about sparse regression estimation and explain what are the problems and what problems we can hope to solve with the Bayesian approach.",
            "So first of all, just some.",
            "Notations, so it's basic regression."
        ],
        [
            "Then I have independent pairs XYYI.",
            "And until regression function FNW is just the noise.",
            "And the objective is to approximate the unknown regression function F uses using functions 5J four from a given Dictionary of function, and I want to perform the best possible estimate estimation approximation of F using functions of this form.",
            "OK, so I insist on the fact that I only have independent pairs and the pairs are not from the moment in the identically distributed, actually the reason.",
            "Why I chose to do that?",
            "Is that well in this work we would like to have IID parents and to give us some background on the provision risk.",
            "But most of work about regression estimation are given in case where the design is deterministic, not random.",
            "So in this case the XY's are deterministic and so the pair are no longer, only the noise is random and so the pair.",
            "I'm no longer I and so I will try to deal with both settings in most of the results of this presentation."
        ],
        [
            "OK, just some results of the risk.",
            "OK, until now we have seen a lot of times but with different notations.",
            "The empirical risk and the provision risk that is actually the expectation of the empirical risk.",
            "A lot of papers about regression estimation in deterministic design case make use of the empirical norm, so I'm just remarked that this is a deterministic object if the design is deterministic, otherwise it is a random object.",
            "But in the case of random design, of course I will try to deal with.",
            "The empirical risk and the provision risk instead of the empirical norm."
        ],
        [
            "OK, and the problem of sparse regression estimation is that well.",
            "There is a big problem if I assume."
        ],
        [
            "But the number of variables of functions in my dictionary is larger than the number of observations, but."
        ],
        [
            "If I assume that there is only a small number of function of interest, so there is a parameter that is a sparse, I mean that has a small number of nonzero coordinates, then this is the case of sparse regression estimation and we have the feeling that we can do something to estimate the regression function.",
            "For example, if we knew what are the nonzero coordinates we could use?",
            "This query estimator and well, for example in the Fit design case so deterministic, is in case we can obtain something like that to control on the expectation of the risk of the square estimator in the best model.",
            "The difference with the best possible parameter is only the number of nonzero coordinates divided by N, so it's something quite good if PO is really smaller than N, but of course we don't.",
            "We don't know what are this zero coordinates, and Moreover we don't even know what is the number of nonzero coordinates peanuts, so we cannot use this this method so.",
            "There have been a lot of papers about what can we do if we do not know."
        ],
        [
            "The nonzero coordinates I will try to give some references.",
            "First ID the LO type panelization.",
            "So as well because I will write 0 norm for the number of nonzero coordinates in data.",
            "So the idea is just to replace the least square estimation by penalization.",
            "We minimize empirical risk plus something that grows with the number of nonzero coordinates.",
            "So a lot of methods can be returning this way, like.",
            "And on criterion like AIC, BICS and the idea is that actually it can give good theoretical results, so I just mentioned just to obtain something that looks."
        ],
        [
            "Like this result.",
            "There is a paper by Brunell and quarters."
        ],
        [
            "Saying that, well, in the fixed design case we have the same kind of results, so the.",
            "Difference between the risk of the ICN, the best possible risk is constant times the number of nonzero coordinates divided by N and multiplied by a log P. But Moreover, it is explained in this paper and references are given to say that we cannot remove this loopy term.",
            "So it seems that using this method we have the best possible result.",
            "This is the end.",
            "The problem is solved and it seems very beautiful except that of course if we want to compute the vics estimator, we do not really have any other solution than to try every possible sub model.",
            "So every possible subset of coordinates for data to compute the list query estimator in every sub model and of course."
        ],
        [
            "12 The two hour people symbols, such models so well we cannot hope to do that except if speed is something like 20 or 30.",
            "I will not be able to compute this estimator so.",
            "We have to find another ID and there's something really.",
            "Popular in the literature recently, that's the idea."
        ],
        [
            "Of L1 type analyzation.",
            "So the most famous example I think is the lasso.",
            "So the name was given by Germany.",
            "The idea is just to replace it the same, actually the same idea, but we just replace the L0 norm of data by the L1 norm.",
            "And if we do so.",
            "This function here that depends on data we are trying to minimize this function with respect to the parameter data.",
            "It becomes a convex function, so as it is convex function there are lots of possible algorithm to minimize efficiently dysfunction.",
            "So there are a lot of known algorithm to do that.",
            "Actually interior point methods and other things.",
            "I just mentioned one of the most famous which is the last last algorithm.",
            "So it allows to compute this for P. Really, really large, you know reasonable time.",
            "So now the question is Vic was has good theoretical properties but cannot be computed when P is large, the lasso.",
            "Can be computed when P is very large, but does it have good theoretical properties?",
            "And the answer is."
        ],
        [
            "Yes, but with some restrictive assumptions.",
            "So actually there are being a lot of papers trying to to give.",
            "Back inequalities.",
            "Oracle inequalities for the lasso under various kind of assumptions.",
            "Here I give two assumptions that are really used.",
            "Mutual course coherence assumption.",
            "It's an assumption and restricted eigenvalue condition.",
            "Both assumptions are on the gram matrix of the of the design.",
            "So for example in this paper, by bigger it off and see back of that is in the.",
            "Fixed design setting the.",
            "The restricted area in value condition is made on the gram matrix on the design, so it's something that we can observe, but if you want to check that the condition is satisfied, you will spend very long time doing it.",
            "Actually, it's far more longer, but in computing the estimator an I do not even know if it can be done in a reasonable time, so you have to apply the term with any purchases that you cannot really check in the paper.",
            "By conscience kits, more inner pack sitting with IID data, so the assumption is made about the.",
            "Expectation of the ground metrics with respect to the unknown distribution, so you cannot even check if the condition is satisfied and what is interesting is that if you look at variance of the lasso, so other methods that can be computed efficiently there.",
            "More or less all use the idea of L1 penalization, for example, then selector basis pursuit and all these methods needs the same kind of assumption if you want to have a theoretical results, for example, known algorithm is learning out leaders, it gives really, really good assumptions.",
            "Sorry, really good theoretical results, but it uses exactly the mutual coherence assumption that was introduced for the less so, so you.",
            "You have to make hypothesis and we would like to avoid such apotheosis like in the previous talk, bye.",
            "Olivia and OK so.",
            "There is another idea that's to use by agent status."
        ],
        [
            "6 Actually, if we look at in this paper by George, there is a very good review of methods of Bayesian statistics using prior distribution that gives large probability to sparse parameters, so that can be used to perform model selection.",
            "So now this paper is a bit old, but there are a lot of new work by people in Bayesian statistics and the idea is that using Monte Carlo methods they are able to compute the estimators.",
            "So this is a good point, but the problem is that they do not give results like the one we had before sparsity.",
            "Oracle inequalities linking the rate of convergence to the unknown sparsity of the unknown parameter, and so this is when we say OK we would like to use by Asian estimator and to give back inequality on it.",
            "So it's probably a good idea to use them back by Asian statistics and so actually."
        ],
        [
            "Someone already did that, so probably it was the paper you are talking about this morning.",
            "But unlike, the young is not a student of the back off.",
            "Actually, this to defer to back office.",
            "Come and see who worked with me on this.",
            "And so first of all I should make a guy should give some references about Bayesian approach, but I don't think I have two because everybody here knows.",
            "So actually this is another cousin to thank the organizers of this conference between because all the references I mentioned in this paper and in my PhD thesis previously are in this room.",
            "So other people I read the paper high.",
            "Now I know you, so it's really important for me and OK Battalion to back off.",
            "They used tool from the last book Biology Catone, to build an estimator that is approximately by using estimator so they can approximate it by Monte Carlo method.",
            "And Moreover, this estimator satisfy a sparsity Oracle inequality, but we are not satisfied with that for tourism.",
            "First reason they worked in the deterministic design setting and we would like to have something more in the pack style, so using IID data and we would like to be able to use if I go back to the.",
            "Definition of the risks.",
            "This means that they are only."
        ],
        [
            "If an Oracle inequality for the empirical norm and you would like to give a result for the provision risk, OK, so This is why I wanted to work on this on this topic while Mikoto Karim."
        ],
        [
            "Was not satisfied with the fact that they have bounded parameter space and we can hope to.",
            "I have a unbounded parameter space like only shows in the.",
            "Previous talk.",
            "Well, so we try to work on.",
            "Removing both hypothesis and the what we have now is that we are not able to remove both epitasis in the same time.",
            "So actually I will give two estimators that are.",
            "Approximately the same, but not exactly the same, and one with gives about the same results than the legend to back off.",
            "But in the random design case and the other one will give the same result but with the unbounded parameter space, so I can."
        ],
        [
            "Now give the overview of our work with Karen.",
            "I represent you, them to aggregation procedures.",
            "The first one, so is for the unbounded parameter space.",
            "The second one is for the random design will give you theoretical results about this estimators.",
            "And finally I will try to explain you why we can use.",
            "Markov chain Monte Carlo methods to approximate these estimators."
        ],
        [
            "Just some additional notations that I will use in this whole talk now.",
            "1st, so the parameters are in RP.",
            "But if I want to bound the parameter space in L1 norm, I will write to K. And detailed J will be the set of parameters that have exactly nonzero coordinates in the set of J. OK and TKO J is the intersection of both, so of course we will use these this set here for both estimation procedures to give the results, but this one will be necessarily necessary only in the case where the parameter space is bounded.",
            "And finally, this will be the uniform probability measure on this set.",
            "And we it will be used as a prior actually.",
            "So to build the priority that it will not be the prior.",
            "And finally I will need that for us here.",
            "This definition implies that for any set data I can find from wealthy to of J, it comes from J.",
            "Of data is the set of nonzero coordinates in data.",
            "So I will use this notation now in the following of."
        ],
        [
            "Of this talk.",
            "So first of all.",
            "The estimator of Theta hat, so it's the first procedure for the unbounded parameter space, but we still are in the deterministic design case, so as we are in the deterministic, this in case the empirical norm here is actually deterministic object and we can normalize the function in the dictionary in this way, so it's not only practice is just.",
            "Renormalization of the function.",
            "And now for any sub model I defined the least square estimator in this given sub model.",
            "OK, and I just have to define a prior on someone then, so we just do something quite intuitive and it's this kind of player were already used in a pack version.",
            "Papers.",
            "We penalize the size of a model and then for every possible size we count how many possible sub models we have.",
            "And finally, we just choose the aggregation of all the possible least square estimators here.",
            "With this prayer, and actually, this term comes from the posterior distribution, so we penalize models with large empirical risk and we give more weight to estimators with very small empirical risk.",
            "OK, so actually I just give the result.",
            "I will spend more time on the second aggregation procedure, so here I jumped to the theoretical results.",
            "So I."
        ],
        [
            "Give two versions.",
            "This is a simple version in the case where we are actually in a parametric setting, so we assume that there is a true parameter Theta star, so the unknown regression function is in the model and in this case with deterministic design, Goshen noise and a choice of parameter that is known for the parameter Lambda in the temperature of the Gibbs distribution.",
            "The empirical distance between our estimator and the real regression function is given by the unknown sparsity of the parameter divided by N, and there's a log term, so it's the same result actually done for the estimator.",
            "What happens if the true regression function is not in the regression model is not in the model?",
            "Yes, they give previous."
        ],
        [
            "Then we have the same results.",
            "But here we have a minimum of all possible data of the approximation of.",
            "F -- F data so of course is F is not exactly the model.",
            "That is quite close to the model and close to a parameter that is passed.",
            "We have the same kind of results and so this is what we wanted to have.",
            "OK, so now I want to move to the random design case.",
            "So from now I assume that the pairs XYXIYIR."
        ],
        [
            "Are IID, and in this case I introduced the following prior distribution.",
            "OK, so it's exactly the same than previously.",
            "We penalize large models, but then I use bounded prior on each model, so it's the uniform probability distribution on.",
            "Remember what T to K is, it's just L1 norm ball OK.",
            "So which means that we can help with this method.",
            "We can.",
            "Sorry, we cannot hope to reach para meters outside this Bolt, so actually will deal with bounded parametric space OK and then we still take the posterior of the Gibbs posterior and the estimator detail is given as the expectation.",
            "Of the parameter with respect to that posterior.",
            "OK, so now here is the."
        ],
        [
            "The reason why we introduce this estimator, it's just a petition inequality.",
            "It's almost the same result that was given in an ancient version of the result dive attorney.",
            "So actually to obtain that we use bench tiny qualities and then use the pack Bayesian tricks to obtain a control of what happens on every possible prior posterior distribution.",
            "And actually it up, it appears that that turns out here.",
            "This posterior is the minimizer of the bound.",
            "On all possible posterior distributions.",
            "We probably could improve the constants now constant are quite ugly here and it will be worse in the next slide, but well, mainly this is a result that is known that since a long time we did not prove a new pack.",
            "Bayesian inequality is to propose this estimator.",
            "We just propose here specific construction of the of the prior and now derived."
        ],
        [
            "A theoretical result from this bound, and here replacing.",
            "Robiah an adequate choice.",
            "We obtain this inequality this packing equality.",
            "So this time this is really an inequality in probability and not in expectation.",
            "So this is really a pack result.",
            "So we are in the case of random or deterministic design.",
            "Both are are OK actually for this result, and we assume that the noise is submission.",
            "Then with probability with large probability, the risk of our estimator is upper bounded by the best possible risk.",
            "Plus same thing done previously, a constant times the unknown sparsity of the parameter divided by N, so it's about the same kind of result.",
            "The only difference here is that, well, what is best is we have the random design case and what is worse is that we have here abundant parameter space.",
            "The two constants C1 and C2 here are ugly as it is why I don't want to write them explicitly, but actually they depend.",
            "On a lot of things that except for this one we are known.",
            "See might say, well, we have to assume that the both quantity here are known in the control of the moments of the noise that is submission.",
            "This is just a power bound on the soup of the function in the dictionary.",
            "So as we choose function in the dictionary, this is known.",
            "So we have to assume that we have a bound on non regression function.",
            "But in this case this bond can be can be written explicitly.",
            "Basically.",
            "End the solve the parameter Lambda is not explicitly except that as I told previously, there's probably some work to improve the bounds, and if you try to use this estimator with this constant here, see one probability one will always be larger than N and you will have problems, but.",
            "Well, from the theoretical point of view we have the the rate of convergence that we wanted to.",
            "Took the."
        ],
        [
            "OK, so now I will just try to convince you that using Monte Carlo methods we are able to explicitly compute both estimators.",
            "Tita Hut, entitled So Things are simpler for Pizza Hut, so I will explain.",
            "How we?"
        ],
        [
            "Need to compute data Hut.",
            "Actually it's easy because once you know all them.",
            "Once all them least square estimators in all possible sub models are given, this is just actually discrete, mean and so using Markov chain Monte Carlo algorithm we just have to simulate a Markov chain G02J 1JN.",
            "Or that's a discrete Markov chain on the set of all possible submodels.",
            "OK, so.",
            "Of course, the idea is that this Markov chain will only live in models that are quite interesting, and so we are not obliged to compute all the possible least square estimators.",
            "Here is just a reminder how fasting Metropolis algorithm.",
            "So we start from an arbitrary J.",
            "0, for example, the empty set.",
            "And then.",
            "Step we tried to move from the current model to another, so we draw I of TI of T from a given distribution that depends on the current hotel we are in and then we have to use this formula to choose to skip to jump from the current model to the new one from JFK to IFT or to stay in the same model using this probability given by.",
            "I think Metro police algorithm and so the only thing that we have to.",
            "To do here is to propose an interesting transition kernel.",
            "So actually we just looked at all the ideas that people had in a backward and forward progression, or even in the last in the last algorithm, it's."
        ],
        [
            "Squads well, first of all, when I'm in the in Model, Ji will not try to jump.",
            "At a random model, everyone in the space because most of the model of no interest.",
            "So actually I will just try to move carefully, only adding one component.",
            "This is Kay Plus or removing one component and.",
            "What does actually done, for example, last algorithm is to add the component that is the most correlated.",
            "OK, we add the new function index dictionary fee of J.",
            "That is the most correlated with the current residual of the regression.",
            "So of course here we cannot do that deterministically.",
            "We have to randomize it.",
            "But well, we can choose to use as a kernel to add 1 component.",
            "The things that gives more weight to model.",
            "That seems to be most correlated with the.",
            "What is not explained in the why I so with the?",
            "Current are residual of the regression.",
            "OK, and if you want to remove an components, the idea is even simpler, because if we have a parameter that's not really sparse, but that almost pass, for example, one component is very small.",
            "I really would like to try to remove that component, so we did actually the same ID.",
            "We have to be careful, of course with the trust of the parameter data, because if data is too small then we try to add a component from a that will be drawn from a uniform measure an if there is.",
            "These very large this will never give something interesting, but on the other case, if we take that are too large, we will.",
            "Actually there will be no difference between this algorithm.",
            "An back forward progression so.",
            "Well, actually we have no theoretical of course way to choose the parameters data, but we tried a lot and I will give them some.",
            "Overall impression on the empirical results just after, but I just want to give a few words for detailed, so actually the idea."
        ],
        [
            "Is about the same we have to move from yes.",
            "So well, the wait is ahead.",
            "Example over the."
        ],
        [
            "What is the weight?",
            "You mean here what are these weights?",
            "Yeah, so you so you sample over the J is yes in investing the Tripolis algorithm you use whatever color you want, but actually the weights are taken into account."
        ],
        [
            "In this step, will you accept or reject the move?"
        ],
        [
            "So if you move in a region that's actually to a model that has a very small weight.",
            "OK, well here you will almost.",
            "Everytime reject the proposal that you make and so you will not reach models with small weights.",
            "I OK OK.",
            "So it was quite easy because we are in the actually simulating a Markov chain in a discrete space, and if I go to the estimator."
        ],
        [
            "He detailed now we have to simulate.",
            "A Markov chain of para meters data.",
            "So we now moving on continuous space and if not more than continuous, it's absolutely continuous with respect to the Lib Egan measure in a lot of possible models with different dimensions.",
            "So actually there was a version.",
            "Two Asian papers only sitting written by green and Green and Richardson where they write the asking Metro police in this setting and this paper was quite useful because the computation are quite heavy too, right?",
            "Explicitly."
        ],
        [
            "This quantity actually in this setting, so they."
        ],
        [
            "Call this method or reversible jump Markov chain Monte Carlo, which just actually a particular case of the Heising Metro police algorithm, and so using this technique, but I do not enjoy to the tiles using this technique, we are able to simulate from the distribution for detailed too, so I do not give numerical results."
        ],
        [
            "You can find them or find them on our paper.",
            "I think it's an active now, but just what we observe I don't know.",
            "I do not want to say this is always better than the other methods or always better than the last one, but actually we compare this method to the lasso and what happens is that almost experiment.",
            "It seems to be better than the lasso actually.",
            "If you fix the number of nonzero components and the noise Sigma and then you make the number of.",
            "Useful, useless components growth.",
            "Then it's our method always become better nonetheless.",
            "Also, it's an interesting observation, but On the contrary, if you fix the number of components and you.",
            "You make the road noise grow, then the lasso becomes always better, so there is a problem of stability.",
            "Now better then I still do not have any explanation for that fact.",
            "The other problem is that it improved DIC because we, as I told you previously, we are only able to compute the icy up to P equals like something like 30 or 40.",
            "Here we are able to go up to P equals to 1000.",
            "Actually, we can do more, but if we want to be able to."
        ],
        [
            "Check that the algorithm is not going into local minimum or something like that.",
            "We wanted to try to compute it with different values for data.",
            "And to check that we have rich something that looks like convergence."
        ],
        [
            "And if we want to make the algorithm run several times."
        ],
        [
            "Actually, the most we can do is P equal to 1000.",
            "After that, it seems that the method becomes bits, less tab stable, and so we cannot really wait until convergence ease is reached.",
            "So another thing that is interesting is that the computation time depends heavily on the number of nonzero coefficients in the best possible parameter.",
            "So the reason is.",
            "And of course, for example, if we.",
            "Come back to the."
        ],
        [
            "This estimator most of the time the models of the Markov chain will be very close to the best possible model so.",
            "This means that all the computation of estimators we have to do our for, if the sparsity is is very small, we only have to compute the square estimators in model of small dimension, which means that actually if the number of non zero coefficient grows."
        ],
        [
            "The computational time for our."
        ],
        [
            "Algorithm for the same step, the same number of step of the Markov chain Monte Carlo algorithm explodes.",
            "So actually it seems to work better when the sparsity is 3 and when P is equal to 1000 than when P is equal to 100.",
            "But the sparsity is something like 50.",
            "So actually the computational time depends more on this quantity then on this quantity.",
            "So.",
            "That's."
        ],
        [
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This exposes talk is about sparse regression estimation, so this is a joint work with Community who defended his PhD last year at University Paris 7, but is now income, regional, postdoctoral position, and.",
                    "label": 1
                },
                {
                    "sent": "So there are a lot of papers recently about sparse regression estimation and actually before explaining what we did with Karim, I just like to give a short bibliography about sparse regression estimation and explain what are the problems and what problems we can hope to solve with the Bayesian approach.",
                    "label": 0
                },
                {
                    "sent": "So first of all, just some.",
                    "label": 0
                },
                {
                    "sent": "Notations, so it's basic regression.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then I have independent pairs XYYI.",
                    "label": 0
                },
                {
                    "sent": "And until regression function FNW is just the noise.",
                    "label": 0
                },
                {
                    "sent": "And the objective is to approximate the unknown regression function F uses using functions 5J four from a given Dictionary of function, and I want to perform the best possible estimate estimation approximation of F using functions of this form.",
                    "label": 0
                },
                {
                    "sent": "OK, so I insist on the fact that I only have independent pairs and the pairs are not from the moment in the identically distributed, actually the reason.",
                    "label": 0
                },
                {
                    "sent": "Why I chose to do that?",
                    "label": 0
                },
                {
                    "sent": "Is that well in this work we would like to have IID parents and to give us some background on the provision risk.",
                    "label": 0
                },
                {
                    "sent": "But most of work about regression estimation are given in case where the design is deterministic, not random.",
                    "label": 0
                },
                {
                    "sent": "So in this case the XY's are deterministic and so the pair are no longer, only the noise is random and so the pair.",
                    "label": 0
                },
                {
                    "sent": "I'm no longer I and so I will try to deal with both settings in most of the results of this presentation.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, just some results of the risk.",
                    "label": 1
                },
                {
                    "sent": "OK, until now we have seen a lot of times but with different notations.",
                    "label": 0
                },
                {
                    "sent": "The empirical risk and the provision risk that is actually the expectation of the empirical risk.",
                    "label": 0
                },
                {
                    "sent": "A lot of papers about regression estimation in deterministic design case make use of the empirical norm, so I'm just remarked that this is a deterministic object if the design is deterministic, otherwise it is a random object.",
                    "label": 0
                },
                {
                    "sent": "But in the case of random design, of course I will try to deal with.",
                    "label": 0
                },
                {
                    "sent": "The empirical risk and the provision risk instead of the empirical norm.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and the problem of sparse regression estimation is that well.",
                    "label": 0
                },
                {
                    "sent": "There is a big problem if I assume.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But the number of variables of functions in my dictionary is larger than the number of observations, but.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If I assume that there is only a small number of function of interest, so there is a parameter that is a sparse, I mean that has a small number of nonzero coordinates, then this is the case of sparse regression estimation and we have the feeling that we can do something to estimate the regression function.",
                    "label": 1
                },
                {
                    "sent": "For example, if we knew what are the nonzero coordinates we could use?",
                    "label": 1
                },
                {
                    "sent": "This query estimator and well, for example in the Fit design case so deterministic, is in case we can obtain something like that to control on the expectation of the risk of the square estimator in the best model.",
                    "label": 0
                },
                {
                    "sent": "The difference with the best possible parameter is only the number of nonzero coordinates divided by N, so it's something quite good if PO is really smaller than N, but of course we don't.",
                    "label": 0
                },
                {
                    "sent": "We don't know what are this zero coordinates, and Moreover we don't even know what is the number of nonzero coordinates peanuts, so we cannot use this this method so.",
                    "label": 0
                },
                {
                    "sent": "There have been a lot of papers about what can we do if we do not know.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The nonzero coordinates I will try to give some references.",
                    "label": 0
                },
                {
                    "sent": "First ID the LO type panelization.",
                    "label": 0
                },
                {
                    "sent": "So as well because I will write 0 norm for the number of nonzero coordinates in data.",
                    "label": 1
                },
                {
                    "sent": "So the idea is just to replace the least square estimation by penalization.",
                    "label": 0
                },
                {
                    "sent": "We minimize empirical risk plus something that grows with the number of nonzero coordinates.",
                    "label": 0
                },
                {
                    "sent": "So a lot of methods can be returning this way, like.",
                    "label": 0
                },
                {
                    "sent": "And on criterion like AIC, BICS and the idea is that actually it can give good theoretical results, so I just mentioned just to obtain something that looks.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like this result.",
                    "label": 0
                },
                {
                    "sent": "There is a paper by Brunell and quarters.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Saying that, well, in the fixed design case we have the same kind of results, so the.",
                    "label": 1
                },
                {
                    "sent": "Difference between the risk of the ICN, the best possible risk is constant times the number of nonzero coordinates divided by N and multiplied by a log P. But Moreover, it is explained in this paper and references are given to say that we cannot remove this loopy term.",
                    "label": 0
                },
                {
                    "sent": "So it seems that using this method we have the best possible result.",
                    "label": 0
                },
                {
                    "sent": "This is the end.",
                    "label": 1
                },
                {
                    "sent": "The problem is solved and it seems very beautiful except that of course if we want to compute the vics estimator, we do not really have any other solution than to try every possible sub model.",
                    "label": 0
                },
                {
                    "sent": "So every possible subset of coordinates for data to compute the list query estimator in every sub model and of course.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "12 The two hour people symbols, such models so well we cannot hope to do that except if speed is something like 20 or 30.",
                    "label": 0
                },
                {
                    "sent": "I will not be able to compute this estimator so.",
                    "label": 0
                },
                {
                    "sent": "We have to find another ID and there's something really.",
                    "label": 0
                },
                {
                    "sent": "Popular in the literature recently, that's the idea.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of L1 type analyzation.",
                    "label": 0
                },
                {
                    "sent": "So the most famous example I think is the lasso.",
                    "label": 0
                },
                {
                    "sent": "So the name was given by Germany.",
                    "label": 0
                },
                {
                    "sent": "The idea is just to replace it the same, actually the same idea, but we just replace the L0 norm of data by the L1 norm.",
                    "label": 0
                },
                {
                    "sent": "And if we do so.",
                    "label": 0
                },
                {
                    "sent": "This function here that depends on data we are trying to minimize this function with respect to the parameter data.",
                    "label": 0
                },
                {
                    "sent": "It becomes a convex function, so as it is convex function there are lots of possible algorithm to minimize efficiently dysfunction.",
                    "label": 0
                },
                {
                    "sent": "So there are a lot of known algorithm to do that.",
                    "label": 0
                },
                {
                    "sent": "Actually interior point methods and other things.",
                    "label": 0
                },
                {
                    "sent": "I just mentioned one of the most famous which is the last last algorithm.",
                    "label": 0
                },
                {
                    "sent": "So it allows to compute this for P. Really, really large, you know reasonable time.",
                    "label": 0
                },
                {
                    "sent": "So now the question is Vic was has good theoretical properties but cannot be computed when P is large, the lasso.",
                    "label": 0
                },
                {
                    "sent": "Can be computed when P is very large, but does it have good theoretical properties?",
                    "label": 1
                },
                {
                    "sent": "And the answer is.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes, but with some restrictive assumptions.",
                    "label": 0
                },
                {
                    "sent": "So actually there are being a lot of papers trying to to give.",
                    "label": 0
                },
                {
                    "sent": "Back inequalities.",
                    "label": 0
                },
                {
                    "sent": "Oracle inequalities for the lasso under various kind of assumptions.",
                    "label": 1
                },
                {
                    "sent": "Here I give two assumptions that are really used.",
                    "label": 0
                },
                {
                    "sent": "Mutual course coherence assumption.",
                    "label": 0
                },
                {
                    "sent": "It's an assumption and restricted eigenvalue condition.",
                    "label": 1
                },
                {
                    "sent": "Both assumptions are on the gram matrix of the of the design.",
                    "label": 1
                },
                {
                    "sent": "So for example in this paper, by bigger it off and see back of that is in the.",
                    "label": 0
                },
                {
                    "sent": "Fixed design setting the.",
                    "label": 0
                },
                {
                    "sent": "The restricted area in value condition is made on the gram matrix on the design, so it's something that we can observe, but if you want to check that the condition is satisfied, you will spend very long time doing it.",
                    "label": 0
                },
                {
                    "sent": "Actually, it's far more longer, but in computing the estimator an I do not even know if it can be done in a reasonable time, so you have to apply the term with any purchases that you cannot really check in the paper.",
                    "label": 0
                },
                {
                    "sent": "By conscience kits, more inner pack sitting with IID data, so the assumption is made about the.",
                    "label": 0
                },
                {
                    "sent": "Expectation of the ground metrics with respect to the unknown distribution, so you cannot even check if the condition is satisfied and what is interesting is that if you look at variance of the lasso, so other methods that can be computed efficiently there.",
                    "label": 0
                },
                {
                    "sent": "More or less all use the idea of L1 penalization, for example, then selector basis pursuit and all these methods needs the same kind of assumption if you want to have a theoretical results, for example, known algorithm is learning out leaders, it gives really, really good assumptions.",
                    "label": 1
                },
                {
                    "sent": "Sorry, really good theoretical results, but it uses exactly the mutual coherence assumption that was introduced for the less so, so you.",
                    "label": 0
                },
                {
                    "sent": "You have to make hypothesis and we would like to avoid such apotheosis like in the previous talk, bye.",
                    "label": 0
                },
                {
                    "sent": "Olivia and OK so.",
                    "label": 0
                },
                {
                    "sent": "There is another idea that's to use by agent status.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "6 Actually, if we look at in this paper by George, there is a very good review of methods of Bayesian statistics using prior distribution that gives large probability to sparse parameters, so that can be used to perform model selection.",
                    "label": 1
                },
                {
                    "sent": "So now this paper is a bit old, but there are a lot of new work by people in Bayesian statistics and the idea is that using Monte Carlo methods they are able to compute the estimators.",
                    "label": 0
                },
                {
                    "sent": "So this is a good point, but the problem is that they do not give results like the one we had before sparsity.",
                    "label": 0
                },
                {
                    "sent": "Oracle inequalities linking the rate of convergence to the unknown sparsity of the unknown parameter, and so this is when we say OK we would like to use by Asian estimator and to give back inequality on it.",
                    "label": 0
                },
                {
                    "sent": "So it's probably a good idea to use them back by Asian statistics and so actually.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Someone already did that, so probably it was the paper you are talking about this morning.",
                    "label": 0
                },
                {
                    "sent": "But unlike, the young is not a student of the back off.",
                    "label": 0
                },
                {
                    "sent": "Actually, this to defer to back office.",
                    "label": 0
                },
                {
                    "sent": "Come and see who worked with me on this.",
                    "label": 0
                },
                {
                    "sent": "And so first of all I should make a guy should give some references about Bayesian approach, but I don't think I have two because everybody here knows.",
                    "label": 0
                },
                {
                    "sent": "So actually this is another cousin to thank the organizers of this conference between because all the references I mentioned in this paper and in my PhD thesis previously are in this room.",
                    "label": 0
                },
                {
                    "sent": "So other people I read the paper high.",
                    "label": 0
                },
                {
                    "sent": "Now I know you, so it's really important for me and OK Battalion to back off.",
                    "label": 0
                },
                {
                    "sent": "They used tool from the last book Biology Catone, to build an estimator that is approximately by using estimator so they can approximate it by Monte Carlo method.",
                    "label": 1
                },
                {
                    "sent": "And Moreover, this estimator satisfy a sparsity Oracle inequality, but we are not satisfied with that for tourism.",
                    "label": 0
                },
                {
                    "sent": "First reason they worked in the deterministic design setting and we would like to have something more in the pack style, so using IID data and we would like to be able to use if I go back to the.",
                    "label": 0
                },
                {
                    "sent": "Definition of the risks.",
                    "label": 0
                },
                {
                    "sent": "This means that they are only.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If an Oracle inequality for the empirical norm and you would like to give a result for the provision risk, OK, so This is why I wanted to work on this on this topic while Mikoto Karim.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Was not satisfied with the fact that they have bounded parameter space and we can hope to.",
                    "label": 0
                },
                {
                    "sent": "I have a unbounded parameter space like only shows in the.",
                    "label": 0
                },
                {
                    "sent": "Previous talk.",
                    "label": 0
                },
                {
                    "sent": "Well, so we try to work on.",
                    "label": 0
                },
                {
                    "sent": "Removing both hypothesis and the what we have now is that we are not able to remove both epitasis in the same time.",
                    "label": 0
                },
                {
                    "sent": "So actually I will give two estimators that are.",
                    "label": 0
                },
                {
                    "sent": "Approximately the same, but not exactly the same, and one with gives about the same results than the legend to back off.",
                    "label": 0
                },
                {
                    "sent": "But in the random design case and the other one will give the same result but with the unbounded parameter space, so I can.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now give the overview of our work with Karen.",
                    "label": 1
                },
                {
                    "sent": "I represent you, them to aggregation procedures.",
                    "label": 0
                },
                {
                    "sent": "The first one, so is for the unbounded parameter space.",
                    "label": 1
                },
                {
                    "sent": "The second one is for the random design will give you theoretical results about this estimators.",
                    "label": 0
                },
                {
                    "sent": "And finally I will try to explain you why we can use.",
                    "label": 0
                },
                {
                    "sent": "Markov chain Monte Carlo methods to approximate these estimators.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just some additional notations that I will use in this whole talk now.",
                    "label": 1
                },
                {
                    "sent": "1st, so the parameters are in RP.",
                    "label": 0
                },
                {
                    "sent": "But if I want to bound the parameter space in L1 norm, I will write to K. And detailed J will be the set of parameters that have exactly nonzero coordinates in the set of J. OK and TKO J is the intersection of both, so of course we will use these this set here for both estimation procedures to give the results, but this one will be necessarily necessary only in the case where the parameter space is bounded.",
                    "label": 0
                },
                {
                    "sent": "And finally, this will be the uniform probability measure on this set.",
                    "label": 1
                },
                {
                    "sent": "And we it will be used as a prior actually.",
                    "label": 0
                },
                {
                    "sent": "So to build the priority that it will not be the prior.",
                    "label": 1
                },
                {
                    "sent": "And finally I will need that for us here.",
                    "label": 0
                },
                {
                    "sent": "This definition implies that for any set data I can find from wealthy to of J, it comes from J.",
                    "label": 0
                },
                {
                    "sent": "Of data is the set of nonzero coordinates in data.",
                    "label": 0
                },
                {
                    "sent": "So I will use this notation now in the following of.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of this talk.",
                    "label": 0
                },
                {
                    "sent": "So first of all.",
                    "label": 0
                },
                {
                    "sent": "The estimator of Theta hat, so it's the first procedure for the unbounded parameter space, but we still are in the deterministic design case, so as we are in the deterministic, this in case the empirical norm here is actually deterministic object and we can normalize the function in the dictionary in this way, so it's not only practice is just.",
                    "label": 1
                },
                {
                    "sent": "Renormalization of the function.",
                    "label": 0
                },
                {
                    "sent": "And now for any sub model I defined the least square estimator in this given sub model.",
                    "label": 0
                },
                {
                    "sent": "OK, and I just have to define a prior on someone then, so we just do something quite intuitive and it's this kind of player were already used in a pack version.",
                    "label": 0
                },
                {
                    "sent": "Papers.",
                    "label": 0
                },
                {
                    "sent": "We penalize the size of a model and then for every possible size we count how many possible sub models we have.",
                    "label": 0
                },
                {
                    "sent": "And finally, we just choose the aggregation of all the possible least square estimators here.",
                    "label": 0
                },
                {
                    "sent": "With this prayer, and actually, this term comes from the posterior distribution, so we penalize models with large empirical risk and we give more weight to estimators with very small empirical risk.",
                    "label": 0
                },
                {
                    "sent": "OK, so actually I just give the result.",
                    "label": 0
                },
                {
                    "sent": "I will spend more time on the second aggregation procedure, so here I jumped to the theoretical results.",
                    "label": 0
                },
                {
                    "sent": "So I.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Give two versions.",
                    "label": 0
                },
                {
                    "sent": "This is a simple version in the case where we are actually in a parametric setting, so we assume that there is a true parameter Theta star, so the unknown regression function is in the model and in this case with deterministic design, Goshen noise and a choice of parameter that is known for the parameter Lambda in the temperature of the Gibbs distribution.",
                    "label": 1
                },
                {
                    "sent": "The empirical distance between our estimator and the real regression function is given by the unknown sparsity of the parameter divided by N, and there's a log term, so it's the same result actually done for the estimator.",
                    "label": 0
                },
                {
                    "sent": "What happens if the true regression function is not in the regression model is not in the model?",
                    "label": 0
                },
                {
                    "sent": "Yes, they give previous.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then we have the same results.",
                    "label": 0
                },
                {
                    "sent": "But here we have a minimum of all possible data of the approximation of.",
                    "label": 1
                },
                {
                    "sent": "F -- F data so of course is F is not exactly the model.",
                    "label": 0
                },
                {
                    "sent": "That is quite close to the model and close to a parameter that is passed.",
                    "label": 0
                },
                {
                    "sent": "We have the same kind of results and so this is what we wanted to have.",
                    "label": 0
                },
                {
                    "sent": "OK, so now I want to move to the random design case.",
                    "label": 1
                },
                {
                    "sent": "So from now I assume that the pairs XYXIYIR.",
                    "label": 1
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are IID, and in this case I introduced the following prior distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's exactly the same than previously.",
                    "label": 0
                },
                {
                    "sent": "We penalize large models, but then I use bounded prior on each model, so it's the uniform probability distribution on.",
                    "label": 0
                },
                {
                    "sent": "Remember what T to K is, it's just L1 norm ball OK.",
                    "label": 0
                },
                {
                    "sent": "So which means that we can help with this method.",
                    "label": 0
                },
                {
                    "sent": "We can.",
                    "label": 0
                },
                {
                    "sent": "Sorry, we cannot hope to reach para meters outside this Bolt, so actually will deal with bounded parametric space OK and then we still take the posterior of the Gibbs posterior and the estimator detail is given as the expectation.",
                    "label": 0
                },
                {
                    "sent": "Of the parameter with respect to that posterior.",
                    "label": 0
                },
                {
                    "sent": "OK, so now here is the.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The reason why we introduce this estimator, it's just a petition inequality.",
                    "label": 0
                },
                {
                    "sent": "It's almost the same result that was given in an ancient version of the result dive attorney.",
                    "label": 0
                },
                {
                    "sent": "So actually to obtain that we use bench tiny qualities and then use the pack Bayesian tricks to obtain a control of what happens on every possible prior posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "And actually it up, it appears that that turns out here.",
                    "label": 0
                },
                {
                    "sent": "This posterior is the minimizer of the bound.",
                    "label": 0
                },
                {
                    "sent": "On all possible posterior distributions.",
                    "label": 0
                },
                {
                    "sent": "We probably could improve the constants now constant are quite ugly here and it will be worse in the next slide, but well, mainly this is a result that is known that since a long time we did not prove a new pack.",
                    "label": 0
                },
                {
                    "sent": "Bayesian inequality is to propose this estimator.",
                    "label": 0
                },
                {
                    "sent": "We just propose here specific construction of the of the prior and now derived.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A theoretical result from this bound, and here replacing.",
                    "label": 0
                },
                {
                    "sent": "Robiah an adequate choice.",
                    "label": 0
                },
                {
                    "sent": "We obtain this inequality this packing equality.",
                    "label": 0
                },
                {
                    "sent": "So this time this is really an inequality in probability and not in expectation.",
                    "label": 0
                },
                {
                    "sent": "So this is really a pack result.",
                    "label": 0
                },
                {
                    "sent": "So we are in the case of random or deterministic design.",
                    "label": 1
                },
                {
                    "sent": "Both are are OK actually for this result, and we assume that the noise is submission.",
                    "label": 1
                },
                {
                    "sent": "Then with probability with large probability, the risk of our estimator is upper bounded by the best possible risk.",
                    "label": 0
                },
                {
                    "sent": "Plus same thing done previously, a constant times the unknown sparsity of the parameter divided by N, so it's about the same kind of result.",
                    "label": 0
                },
                {
                    "sent": "The only difference here is that, well, what is best is we have the random design case and what is worse is that we have here abundant parameter space.",
                    "label": 0
                },
                {
                    "sent": "The two constants C1 and C2 here are ugly as it is why I don't want to write them explicitly, but actually they depend.",
                    "label": 0
                },
                {
                    "sent": "On a lot of things that except for this one we are known.",
                    "label": 0
                },
                {
                    "sent": "See might say, well, we have to assume that the both quantity here are known in the control of the moments of the noise that is submission.",
                    "label": 0
                },
                {
                    "sent": "This is just a power bound on the soup of the function in the dictionary.",
                    "label": 0
                },
                {
                    "sent": "So as we choose function in the dictionary, this is known.",
                    "label": 0
                },
                {
                    "sent": "So we have to assume that we have a bound on non regression function.",
                    "label": 0
                },
                {
                    "sent": "But in this case this bond can be can be written explicitly.",
                    "label": 0
                },
                {
                    "sent": "Basically.",
                    "label": 0
                },
                {
                    "sent": "End the solve the parameter Lambda is not explicitly except that as I told previously, there's probably some work to improve the bounds, and if you try to use this estimator with this constant here, see one probability one will always be larger than N and you will have problems, but.",
                    "label": 0
                },
                {
                    "sent": "Well, from the theoretical point of view we have the the rate of convergence that we wanted to.",
                    "label": 0
                },
                {
                    "sent": "Took the.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now I will just try to convince you that using Monte Carlo methods we are able to explicitly compute both estimators.",
                    "label": 0
                },
                {
                    "sent": "Tita Hut, entitled So Things are simpler for Pizza Hut, so I will explain.",
                    "label": 0
                },
                {
                    "sent": "How we?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Need to compute data Hut.",
                    "label": 0
                },
                {
                    "sent": "Actually it's easy because once you know all them.",
                    "label": 0
                },
                {
                    "sent": "Once all them least square estimators in all possible sub models are given, this is just actually discrete, mean and so using Markov chain Monte Carlo algorithm we just have to simulate a Markov chain G02J 1JN.",
                    "label": 1
                },
                {
                    "sent": "Or that's a discrete Markov chain on the set of all possible submodels.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Of course, the idea is that this Markov chain will only live in models that are quite interesting, and so we are not obliged to compute all the possible least square estimators.",
                    "label": 0
                },
                {
                    "sent": "Here is just a reminder how fasting Metropolis algorithm.",
                    "label": 0
                },
                {
                    "sent": "So we start from an arbitrary J.",
                    "label": 0
                },
                {
                    "sent": "0, for example, the empty set.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "Step we tried to move from the current model to another, so we draw I of TI of T from a given distribution that depends on the current hotel we are in and then we have to use this formula to choose to skip to jump from the current model to the new one from JFK to IFT or to stay in the same model using this probability given by.",
                    "label": 0
                },
                {
                    "sent": "I think Metro police algorithm and so the only thing that we have to.",
                    "label": 0
                },
                {
                    "sent": "To do here is to propose an interesting transition kernel.",
                    "label": 0
                },
                {
                    "sent": "So actually we just looked at all the ideas that people had in a backward and forward progression, or even in the last in the last algorithm, it's.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Squads well, first of all, when I'm in the in Model, Ji will not try to jump.",
                    "label": 0
                },
                {
                    "sent": "At a random model, everyone in the space because most of the model of no interest.",
                    "label": 0
                },
                {
                    "sent": "So actually I will just try to move carefully, only adding one component.",
                    "label": 0
                },
                {
                    "sent": "This is Kay Plus or removing one component and.",
                    "label": 0
                },
                {
                    "sent": "What does actually done, for example, last algorithm is to add the component that is the most correlated.",
                    "label": 0
                },
                {
                    "sent": "OK, we add the new function index dictionary fee of J.",
                    "label": 0
                },
                {
                    "sent": "That is the most correlated with the current residual of the regression.",
                    "label": 0
                },
                {
                    "sent": "So of course here we cannot do that deterministically.",
                    "label": 0
                },
                {
                    "sent": "We have to randomize it.",
                    "label": 0
                },
                {
                    "sent": "But well, we can choose to use as a kernel to add 1 component.",
                    "label": 0
                },
                {
                    "sent": "The things that gives more weight to model.",
                    "label": 0
                },
                {
                    "sent": "That seems to be most correlated with the.",
                    "label": 0
                },
                {
                    "sent": "What is not explained in the why I so with the?",
                    "label": 0
                },
                {
                    "sent": "Current are residual of the regression.",
                    "label": 1
                },
                {
                    "sent": "OK, and if you want to remove an components, the idea is even simpler, because if we have a parameter that's not really sparse, but that almost pass, for example, one component is very small.",
                    "label": 0
                },
                {
                    "sent": "I really would like to try to remove that component, so we did actually the same ID.",
                    "label": 0
                },
                {
                    "sent": "We have to be careful, of course with the trust of the parameter data, because if data is too small then we try to add a component from a that will be drawn from a uniform measure an if there is.",
                    "label": 0
                },
                {
                    "sent": "These very large this will never give something interesting, but on the other case, if we take that are too large, we will.",
                    "label": 0
                },
                {
                    "sent": "Actually there will be no difference between this algorithm.",
                    "label": 0
                },
                {
                    "sent": "An back forward progression so.",
                    "label": 0
                },
                {
                    "sent": "Well, actually we have no theoretical of course way to choose the parameters data, but we tried a lot and I will give them some.",
                    "label": 0
                },
                {
                    "sent": "Overall impression on the empirical results just after, but I just want to give a few words for detailed, so actually the idea.",
                    "label": 1
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is about the same we have to move from yes.",
                    "label": 1
                },
                {
                    "sent": "So well, the wait is ahead.",
                    "label": 0
                },
                {
                    "sent": "Example over the.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What is the weight?",
                    "label": 0
                },
                {
                    "sent": "You mean here what are these weights?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so you so you sample over the J is yes in investing the Tripolis algorithm you use whatever color you want, but actually the weights are taken into account.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this step, will you accept or reject the move?",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if you move in a region that's actually to a model that has a very small weight.",
                    "label": 0
                },
                {
                    "sent": "OK, well here you will almost.",
                    "label": 0
                },
                {
                    "sent": "Everytime reject the proposal that you make and so you will not reach models with small weights.",
                    "label": 0
                },
                {
                    "sent": "I OK OK.",
                    "label": 0
                },
                {
                    "sent": "So it was quite easy because we are in the actually simulating a Markov chain in a discrete space, and if I go to the estimator.",
                    "label": 1
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "He detailed now we have to simulate.",
                    "label": 1
                },
                {
                    "sent": "A Markov chain of para meters data.",
                    "label": 0
                },
                {
                    "sent": "So we now moving on continuous space and if not more than continuous, it's absolutely continuous with respect to the Lib Egan measure in a lot of possible models with different dimensions.",
                    "label": 0
                },
                {
                    "sent": "So actually there was a version.",
                    "label": 0
                },
                {
                    "sent": "Two Asian papers only sitting written by green and Green and Richardson where they write the asking Metro police in this setting and this paper was quite useful because the computation are quite heavy too, right?",
                    "label": 0
                },
                {
                    "sent": "Explicitly.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This quantity actually in this setting, so they.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Call this method or reversible jump Markov chain Monte Carlo, which just actually a particular case of the Heising Metro police algorithm, and so using this technique, but I do not enjoy to the tiles using this technique, we are able to simulate from the distribution for detailed too, so I do not give numerical results.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You can find them or find them on our paper.",
                    "label": 0
                },
                {
                    "sent": "I think it's an active now, but just what we observe I don't know.",
                    "label": 0
                },
                {
                    "sent": "I do not want to say this is always better than the other methods or always better than the last one, but actually we compare this method to the lasso and what happens is that almost experiment.",
                    "label": 0
                },
                {
                    "sent": "It seems to be better than the lasso actually.",
                    "label": 1
                },
                {
                    "sent": "If you fix the number of nonzero components and the noise Sigma and then you make the number of.",
                    "label": 0
                },
                {
                    "sent": "Useful, useless components growth.",
                    "label": 0
                },
                {
                    "sent": "Then it's our method always become better nonetheless.",
                    "label": 0
                },
                {
                    "sent": "Also, it's an interesting observation, but On the contrary, if you fix the number of components and you.",
                    "label": 0
                },
                {
                    "sent": "You make the road noise grow, then the lasso becomes always better, so there is a problem of stability.",
                    "label": 0
                },
                {
                    "sent": "Now better then I still do not have any explanation for that fact.",
                    "label": 0
                },
                {
                    "sent": "The other problem is that it improved DIC because we, as I told you previously, we are only able to compute the icy up to P equals like something like 30 or 40.",
                    "label": 1
                },
                {
                    "sent": "Here we are able to go up to P equals to 1000.",
                    "label": 0
                },
                {
                    "sent": "Actually, we can do more, but if we want to be able to.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Check that the algorithm is not going into local minimum or something like that.",
                    "label": 0
                },
                {
                    "sent": "We wanted to try to compute it with different values for data.",
                    "label": 0
                },
                {
                    "sent": "And to check that we have rich something that looks like convergence.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if we want to make the algorithm run several times.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Actually, the most we can do is P equal to 1000.",
                    "label": 0
                },
                {
                    "sent": "After that, it seems that the method becomes bits, less tab stable, and so we cannot really wait until convergence ease is reached.",
                    "label": 0
                },
                {
                    "sent": "So another thing that is interesting is that the computation time depends heavily on the number of nonzero coefficients in the best possible parameter.",
                    "label": 1
                },
                {
                    "sent": "So the reason is.",
                    "label": 0
                },
                {
                    "sent": "And of course, for example, if we.",
                    "label": 0
                },
                {
                    "sent": "Come back to the.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This estimator most of the time the models of the Markov chain will be very close to the best possible model so.",
                    "label": 0
                },
                {
                    "sent": "This means that all the computation of estimators we have to do our for, if the sparsity is is very small, we only have to compute the square estimators in model of small dimension, which means that actually if the number of non zero coefficient grows.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The computational time for our.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Algorithm for the same step, the same number of step of the Markov chain Monte Carlo algorithm explodes.",
                    "label": 1
                },
                {
                    "sent": "So actually it seems to work better when the sparsity is 3 and when P is equal to 1000 than when P is equal to 100.",
                    "label": 0
                },
                {
                    "sent": "But the sparsity is something like 50.",
                    "label": 0
                },
                {
                    "sent": "So actually the computational time depends more on this quantity then on this quantity.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "That's.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}