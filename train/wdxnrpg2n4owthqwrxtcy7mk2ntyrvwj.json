{
    "id": "wdxnrpg2n4owthqwrxtcy7mk2ntyrvwj",
    "title": "Tightening LP Relaxations for MAP using Message Passing",
    "info": {
        "author": [
            "David Sontag, Computer Science Department, New York University (NYU)"
        ],
        "published": "July 30, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Graphical Models",
            "Top->Computer Science->Artificial Intelligence"
        ]
    },
    "url": "http://videolectures.net/uai08_sontag_tlpr/",
    "segmentation": [
        [
            "So many interesting rowald problems could be posed as.",
            "An inference task of finding the most likely assignment in some undirected graphical model.",
            "The distributions of these models are given by some graph structure where the vertices correspond to the variables and will be considering discrete variables.",
            "And for every edge we have a potential function of real value potential function defined on XI and XJ for edge."
        ],
        [
            "Hi Jay the joint distribution is given as is proportional to the exponential of the sum of these edge potentials and the problem of finding most likely assignment is simply that of maximizing the sum of edge potentials for all possible assignments.",
            "Although map is tractable for some graphical models, in general, the math problem is NP hard.",
            "Nonetheless, we shouldn't give up many real problems are easier than their theoretical worst case, and one of the goals of our work is to design algorithms that can take advantage of the hidden structure in these real world problems.",
            "In this talk, we give an algorithm which allows us to improve our approximation at the cost of additional computation.",
            "One of the key features of our algorithm is that it allows us to.",
            "Improved approximation in a problem specific way, using additional computation for the hard parts of each problem.",
            "We will show that this problem is that this algorithm allows us to solve 2 interesting and hard real world problems given by the protein design and stereo."
        ],
        [
            "No problems.",
            "Our approach will be to.",
            "Formulate the map problem as a linear program.",
            "So we first rewrite the the map problem by including.",
            "Delta distribution for every edge.",
            "Summing this distribution over the assignments to that edge.",
            "So right there that is a equivalent transformation and then we take the expectation of the right hand side with respect to some distribution over assignments, and we get the following linear program.",
            "The variables in this linear program are for every edge and assignment some marginal distribution and the joint.",
            "And each of each feasible vector.",
            "So each each of the points which were optimizing over must be constrained such that the edge marginal can be realized as marginal of some joint distribution.",
            "So an equivalent way of viewing this linear program is as optimizing over the convex Hull of these Delta distributions corresponding to each assignment to the graphical model.",
            "So just because we formulated this linear program doesn't make this problem any easier.",
            "Unfortunately, this polytope this feasible set has very many constraints and is in general very hard to describe."
        ],
        [
            "So instead of optimizing for the marginal polytope, we could try optimizing over relaxation of the marginal polytope given by a small number of."
        ],
        [
            "Lights.",
            "So since we're now optimize over a larger set, the optimal linear program is going to upper bound the value of the map assignment.",
            "One simple outer bound simply enforces that each edge marginal thumbs to one.",
            "And this gives us our first upper bound on the map assignment.",
            "I'm.",
            "Sorry, I in this talk our approach will be to iteratively refine our approximation are of the of the marginal polytope starting with a very loose approximation and adding additional enforcing consistency of larger and larger clusters, until eventually we recover the marginal polytope.",
            "The hopefully we don't go so far."
        ],
        [
            "So we begin with for example some very simple constraints which we know must hold true for any marginal vector.",
            "So the pairwise consistent constraints simply enforce that for any 2 edge marginals that share some node that they give the same single node marginal distribution on that node.",
            "We keep adding these consistency constraints until we're happy with the solution given by."
        ],
        [
            "Linear program.",
            "So for example, here I show a different pairwise consistency."
        ],
        [
            "Taxation and in general we we could add these.",
            "One by one until all pairwise sensitive like constraints are included in the model.",
            "We can then consider enforcing consistency in larger clusters."
        ],
        [
            "For example, we can enforce that for every three nodes where there's a triangle in the graph on those three nodes that the edge marginals for these nodes must be consistent with some joint distribution on those three nodes.",
            "We can implement this in the linear program by adding variables to explicitly represent the joint distribution of these three very."
        ],
        [
            "Both shown here.",
            "We can do the same thing for clusters of four variables and so on.",
            "Note that will need to represent, so every time we add these consistent extremes, the additional cost in the linear program is going to be exponential in the state space of the cluster of the cluster size.",
            "For the cluster that we add, so we could continue adding larger and larger clusters until eventually we use a cluster and all of the nodes and recover the marginal polytope.",
            "But of course this is precisely what we.",
            "Hoped not to do because the marginal polytope is very, very."
        ],
        [
            "Hard to describe.",
            "Luckily for many interesting real world problems, we find that just enforcing a few consistency constraints suffices to find the map assignment, and we can show that adding any additional constraints would not result in a better solution.",
            "Now, this approach sounds good, but it raises several difficult questions.",
            "First, how could we efficiently solve this linear program even though we're working with the relaxation in general for large undirected model, these LP's will be very large and working within the Primal LP might be infeasible.",
            "Second, how do we choose what clusters to add?",
            "If we're going to be iteratively adding clusters to the relaxation and we hope to be able to stop early, then we had better be choosing the right clusters early on.",
            "And third, how can we avoid re solving the linear program after we add each additional cluster consistency constraint?"
        ],
        [
            "In our in our paper, we propose to efficiently solve the linear program by working with the dual with the dual of the primal linear program.",
            "So the dual LP.",
            "Upper bounds the optimum of the Primal LP, and thus, as I mentioned to you before the primal upper bounds the map.",
            "So the dual also upper bounds the map.",
            "We use a particular dual formulation given by globerson yakala from last year, which has the advantage that they give an efficient message passing algorithm to optimize this dual LP.",
            "This message passing algorithm corresponds to doing coordinate descent in the dual, which has the nice property that every step of the algorithm monotonically decreases in upper bound on the map.",
            "So what cluster do we add next?",
            "Or we propose using a greedy bound minimization algorithm where we add clusters which minimize the upper bound on the map as much as possible.",
            "In doing so, we're guaranteed that every cluster we add is going to improve our solution.",
            "We we avoid re solving the LP by warm starting the new messages."
        ],
        [
            "With the old messages, so are now illustrate the dual algorithm.",
            "Which is the main results of this talk.",
            "So we first run message passing using some some initial relaxation, such as that given by pairwise consistency constraints.",
            "Where as I said before, each each message that's being sent out correspondence to optimizing the dual P. Once we converge with the with the current relaxation, we try to decode an assignment from the messages and check to see if there is a large integrality gap.",
            "If the gap is smaller, then we're done, but in this case it isn't, so we try adding in some cluster to the relaxation.",
            "I'll tell you in a little bit how to choose what cluster to add.",
            "So we then warm start by initializing the message from the cluster to zero.",
            "It turns out that this is a new dual feasible solution.",
            "It turns out for this special do LB and this new dual feasible solution has the same dual objective value as.",
            "As we had in the previous relaxation, so we've now saved the effort of optimizing up to the current spot.",
            "So then we continue running message passing until again we converge.",
            "We decode the next assignment, see there still gap, choose a new cluster, worm starts and continue.",
            "So we iterate this process until eventually.",
            "We reached the map assignment and we're done."
        ],
        [
            "So I go straight.",
            "So this is the same diagram, but for one of our real world problems.",
            "The blue line is the dual objective, the red line is the integer solution as we go along."
        ],
        [
            "To the algorithm.",
            "So what cluster do we add to the relaxation well, so we could try to fully optimize LP after adding an additional cluster and try this for every cluster and choose the cluster which optimizes which decreases the upper bound as much as possible.",
            "But this will be quite intense, computationally intensive.",
            "An alternative would be a bit more greedy.",
            "We just tried one message passing iteration with each new cluster and choose the cluster which decreases the bound as much as possible.",
            "What we use is actually even more greedy.",
            "We simply choose we look at for every possible cluster to add.",
            "How much does just sending one outgoing message from that cluster decrease the dual bound?",
            "And we choose the cluster which maximizes the decrease.",
            "So using this coordinate descent algorithm, we get a very nice formula for what the dual decrease will be for any particular cluster.",
            "And this is given."
        ],
        [
            "The bottom so it has an intuitive meaning in terms of the difference between an independent decoding of the edge beliefs.",
            "So these believes B12.",
            "For example, we construct from the messages of the algorithm.",
            "So it's the difference between an independent decoding and a global decoding of the edge beliefs for that cluster."
        ],
        [
            "So as an example, suppose that we have edge beliefs where for edge IJ the belief is 99.",
            "If X is not equal, DXJ and minus 10 effects I = X J.",
            "Well, you can easily, it's easy to show that the left some is going to be 3 * 99 and the right sum.",
            "No matter how you choose, the assignment is going to be 2 * 99 -- 10.",
            "So we see here that the if we were to add in the cluster on nodes 1, two and three, the dual, that is to say that the upper bound on the map will decrease by at least this much.",
            "For those of you familiar with with spin graph models in statistical physics.",
            "The notion of frustration.",
            "You could show that if the dual in decreases.",
            "So if this sum is, this difference is larger than zero, then there was frustration in the model prior to."
        ],
        [
            "In this cluster.",
            "So now move on to some related work before showing you the experimental results.",
            "So the region pursuit algorithm given by Max Welling, an UI of 2004 is qualitatively very similar to our algorithm, and his approach was to try to.",
            "The goal was to approximate marginals well.",
            "Here we're trying to do map, but he adds again just like us.",
            "He adds regions iteratively such that the region he chooses, the regions which most changed, the region free energy approximation.",
            "So because they were using this region free energy approximation and not some bound which we're minimizing like like in our work they found that unfortunately sometimes adding regions gives worse results.",
            "Well in this work, that problem doesn't arise.",
            "Another related work which was by myself and Tommy Apla last year, worked with the Primal LP.",
            "Oh so iteratively tightening the LP relaxation.",
            "In that work we were we used a selection criteria of the consistency constraint, which is most violated.",
            "While in this work we use a bound minimization criteria.",
            "These approaches are possibly complementary, and it would be interesting to consider both primal and dual approaches for optimizing this.",
            "Well, in that earlier work we were able to efficiently find violated constraints.",
            "We found that resolving the primal was very difficult and using this dual formulation gets around that problem.",
            "I'd like to note that there are many other dual formulations for the for the LP that I proposed, and we use the dual formulation given by Globerson Yacula last year, which has the very nice property that the coordinate descent step, which is what happens when you pass the messages.",
            "It gives a very nice and easy to understand.",
            "Guaranteed bound decrease.",
            "That was the formula that I showed you earlier.",
            "I also I'd like to note that concurrently to our work, a similar approach has been proposed by Thomas Warner and CPR this year."
        ],
        [
            "So moving on to the experiments, the protein design problem is we're given a 3D.",
            "We're given a protein is 3D shape, and our goal is to choose amino acids along the backbone of this protein such that we get the most stable structure.",
            "Each state corresponds to the choice of amino acid and sidechain angle.",
            "Because we have to choose not only the amino acids, but we have to figure out what angle does amino acids are in where the angle the angles relate to each other and give some energy function.",
            "The microphone and fields have between 41180 variables, each variable having a very large number of states between 95 and 158.",
            "So these problems are very large.",
            "Have large tree with, have very many small cycles.",
            "In particular 20,000 triangles, about 20,000 and are very frustrated.",
            "For those of you who know, this means, so they're not."
        ],
        [
            "To solve in particular, if you were to try to use the primal LP formulation, you find that for typical protein it has.",
            "the LP formulation has over 10,000,000 variables and this is using the pairwise consistency relaxation, which is sort of the bare minimum you need for these problems and the number of constraints is in order of 300,000.",
            "Using commercial linear programming solver on the Primal LP would only be able to run on three of these proteins before running out of memory.",
            "So this motivates the need to have to solve these problems in other ways.",
            "In particular, we moved to the dual."
        ],
        [
            "Even if we were able to solve them in the primal.",
            "It was shown in earlier work that the pairwise consistency relaxation only solves two of the 97 proteins in this data set.",
            "So we really need to be able to tighten the relaxation efficiently.",
            "With triplets so we start with the pairwise consistency and add triplets iteratively in the method I showed you earlier, we were able to solve 96 of the 97 proteins in this data set.",
            "Now we found these results to be very surprising.",
            "We hadn't known of anyone else previously.",
            "They were able to solve these problems so.",
            "We're pretty excited about this.",
            "We added between 5 and 735 triplets out of the possible 20,000 that you could have added, and notice that because of the large state spaces of these models even just one triplet message requires over a million computations.",
            "So it would be clearly impossible to use all triplets messages.",
            "We really need to be able to accurately select which clusters to use in our relaxation, which is what we do.",
            "On average, these took about 10 hours to solve with the largest proteins taking 11 days.",
            "We used MATLAB and C implementation which was very unoptimized and we have lots of ideas of how to optimize both the code, but also the algorithmic details to get much."
        ],
        [
            "Federer results.",
            "So one question which we looked into was, would it be?",
            "Do we really need to run to convergence prior to choosing a new cluster to add to the relaxation?",
            "So we looked at this and on the left on the red line we show.",
            "The algorithm where we only run 20 message passing iterations before choosing a new cluster to add in and the blue line is if we were to add if we were to run until convergence before adding in the clusters.",
            "So we found that it was much better to stop early.",
            "That is not wait until convergence before adding clusters and we were able to solve these problems much much faster.",
            "Note that the reason that we can do this is because the because precisely because of our use of this greedy bound, this criterion that we have is valid regardless of whether we ran to convergence or not."
        ],
        [
            "Next we looked at a stereo vision problem where roughly in stereo vision were given 2 images corresponding to the view from the left in the right eye, and our goal is to find the depth of the objects in the image.",
            "This again can be posed as an undirected graphical model, where we have one variable for every pixel, and we looked at problems which have about 100, six, 16 by 154 pixels, so 13,000 variables in the model an each variable takes 16 states.",
            "These problems are also somewhat hard to solve.",
            "Have lots of small."
        ],
        [
            "Eagles so we we didn't do that extensive experiment.",
            "We just took 10 images from from the top and it all energy function and the and 10 different settings of parameters that they suggested.",
            "And we tried solving the corresponding map problems.",
            "The pairwise relaxation was able to solve 6 of the 10 problems, but for the remaining ones we had to add in square clusters before we were able to solve them.",
            "But we do."
        ],
        [
            "Solve all of them.",
            "So finally we asked.",
            "Whether aggressively adding clusters would be a good heuristic.",
            "So rather than adding the one best cluster at each step and then running 20 iterations, we might want to try adding, let's say, the best 20 clusters at each step before running additional message passing iterations.",
            "So we found that if we had just the one best cluster, we had many few clusters to the relaxation before optimally solving them.",
            "Versus adding the 20 best, but on the other hand.",
            "Regardless of having added very many more clusters, being more aggressive actually results in faster runtimes.",
            "And finally, randomly adding clusters was completely."
        ],
        [
            "Useless.",
            "So in future work we're interested in pursuing this question of how to efficiently find the clusters which minimize the bound.",
            "So this work we assume that someone hands us a bucket of clusters.",
            "We show how to select which of these clusters would be best to use.",
            "There's a straightforward extension of our work to structured prediction.",
            "In large microphone fields with generally intractable structure and using Tamir's algorithm which was presented in the previous talk, we could try extending our results to approximating modules and the partition function.",
            "Finally, will be releasing some optimized code soon, so look forward to it and ask me if you want it."
        ],
        [
            "Thank you very much.",
            "Constraints.",
            "That last example where you're adding batches.",
            "So yeah, so that's a very common approach in the primal, right?",
            "So if you're solving the primal, you add constraints and remove them when they're no longer binding in the dual.",
            "It's a bit more difficult because once you start optimizing with the constraint in your, in essence, adding the algorithm in essence adds a potential function to those constraints, and we can no longer remove them.",
            "Try looking at other.",
            "Families.",
            "Look at the loops.",
            "Yes, that's exactly right.",
            "So the squares that we added our loops, but you're right that in general you could use larger cycles.",
            "One of the big problems is finding these cycles.",
            "So once you start moving to larger cycles, you have to enumerate over all of them to valuate the bounded.",
            "Valuing.",
            "Bound is easy, but there are a lot of them regardless for well structured problems such as grids, this is easy to do.",
            "An would be.",
            "A perfectly interesting direction for future work.",
            "Or strange for a cluster with another cluster, you optimize it.",
            "Optimize.",
            "It's a block coordinate descent algorithm, but but the blocks are relatively small in our current imitation.",
            "One of the interesting directions for future work is to use larger blocks, which is a bit more complicated to derive, but would result in much faster convergence in general.",
            "Is not the best way to solve, so you're basically here the constraints of the particular form that makes coordinate ascent better, not necessarily better.",
            "I mean using some 2nd order method might give better convergence, but when these problems are very, very large, it's been found various times over the last few years that.",
            "Using these graph structured algorithms, of which coordinate descent makes easy, actually, is able to scale to much larger problems.",
            "Relationship.",
            "It's a good question.",
            "I don't know because we haven't tried.",
            "My guess would be that the protein design problems have nondeterministic but almost deterministic relations, because there really are hard constraints here that two side chains cannot be in the same location at the same time, so we could try.",
            "So we were able to solve this problem.",
            "That gives some intuition of what to expect."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So many interesting rowald problems could be posed as.",
                    "label": 0
                },
                {
                    "sent": "An inference task of finding the most likely assignment in some undirected graphical model.",
                    "label": 0
                },
                {
                    "sent": "The distributions of these models are given by some graph structure where the vertices correspond to the variables and will be considering discrete variables.",
                    "label": 0
                },
                {
                    "sent": "And for every edge we have a potential function of real value potential function defined on XI and XJ for edge.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hi Jay the joint distribution is given as is proportional to the exponential of the sum of these edge potentials and the problem of finding most likely assignment is simply that of maximizing the sum of edge potentials for all possible assignments.",
                    "label": 1
                },
                {
                    "sent": "Although map is tractable for some graphical models, in general, the math problem is NP hard.",
                    "label": 0
                },
                {
                    "sent": "Nonetheless, we shouldn't give up many real problems are easier than their theoretical worst case, and one of the goals of our work is to design algorithms that can take advantage of the hidden structure in these real world problems.",
                    "label": 0
                },
                {
                    "sent": "In this talk, we give an algorithm which allows us to improve our approximation at the cost of additional computation.",
                    "label": 0
                },
                {
                    "sent": "One of the key features of our algorithm is that it allows us to.",
                    "label": 1
                },
                {
                    "sent": "Improved approximation in a problem specific way, using additional computation for the hard parts of each problem.",
                    "label": 0
                },
                {
                    "sent": "We will show that this problem is that this algorithm allows us to solve 2 interesting and hard real world problems given by the protein design and stereo.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No problems.",
                    "label": 0
                },
                {
                    "sent": "Our approach will be to.",
                    "label": 0
                },
                {
                    "sent": "Formulate the map problem as a linear program.",
                    "label": 0
                },
                {
                    "sent": "So we first rewrite the the map problem by including.",
                    "label": 0
                },
                {
                    "sent": "Delta distribution for every edge.",
                    "label": 0
                },
                {
                    "sent": "Summing this distribution over the assignments to that edge.",
                    "label": 0
                },
                {
                    "sent": "So right there that is a equivalent transformation and then we take the expectation of the right hand side with respect to some distribution over assignments, and we get the following linear program.",
                    "label": 0
                },
                {
                    "sent": "The variables in this linear program are for every edge and assignment some marginal distribution and the joint.",
                    "label": 0
                },
                {
                    "sent": "And each of each feasible vector.",
                    "label": 0
                },
                {
                    "sent": "So each each of the points which were optimizing over must be constrained such that the edge marginal can be realized as marginal of some joint distribution.",
                    "label": 0
                },
                {
                    "sent": "So an equivalent way of viewing this linear program is as optimizing over the convex Hull of these Delta distributions corresponding to each assignment to the graphical model.",
                    "label": 0
                },
                {
                    "sent": "So just because we formulated this linear program doesn't make this problem any easier.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, this polytope this feasible set has very many constraints and is in general very hard to describe.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So instead of optimizing for the marginal polytope, we could try optimizing over relaxation of the marginal polytope given by a small number of.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lights.",
                    "label": 0
                },
                {
                    "sent": "So since we're now optimize over a larger set, the optimal linear program is going to upper bound the value of the map assignment.",
                    "label": 0
                },
                {
                    "sent": "One simple outer bound simply enforces that each edge marginal thumbs to one.",
                    "label": 0
                },
                {
                    "sent": "And this gives us our first upper bound on the map assignment.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "Sorry, I in this talk our approach will be to iteratively refine our approximation are of the of the marginal polytope starting with a very loose approximation and adding additional enforcing consistency of larger and larger clusters, until eventually we recover the marginal polytope.",
                    "label": 0
                },
                {
                    "sent": "The hopefully we don't go so far.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we begin with for example some very simple constraints which we know must hold true for any marginal vector.",
                    "label": 0
                },
                {
                    "sent": "So the pairwise consistent constraints simply enforce that for any 2 edge marginals that share some node that they give the same single node marginal distribution on that node.",
                    "label": 0
                },
                {
                    "sent": "We keep adding these consistency constraints until we're happy with the solution given by.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Linear program.",
                    "label": 0
                },
                {
                    "sent": "So for example, here I show a different pairwise consistency.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Taxation and in general we we could add these.",
                    "label": 0
                },
                {
                    "sent": "One by one until all pairwise sensitive like constraints are included in the model.",
                    "label": 0
                },
                {
                    "sent": "We can then consider enforcing consistency in larger clusters.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For example, we can enforce that for every three nodes where there's a triangle in the graph on those three nodes that the edge marginals for these nodes must be consistent with some joint distribution on those three nodes.",
                    "label": 0
                },
                {
                    "sent": "We can implement this in the linear program by adding variables to explicitly represent the joint distribution of these three very.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Both shown here.",
                    "label": 0
                },
                {
                    "sent": "We can do the same thing for clusters of four variables and so on.",
                    "label": 0
                },
                {
                    "sent": "Note that will need to represent, so every time we add these consistent extremes, the additional cost in the linear program is going to be exponential in the state space of the cluster of the cluster size.",
                    "label": 0
                },
                {
                    "sent": "For the cluster that we add, so we could continue adding larger and larger clusters until eventually we use a cluster and all of the nodes and recover the marginal polytope.",
                    "label": 0
                },
                {
                    "sent": "But of course this is precisely what we.",
                    "label": 0
                },
                {
                    "sent": "Hoped not to do because the marginal polytope is very, very.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hard to describe.",
                    "label": 0
                },
                {
                    "sent": "Luckily for many interesting real world problems, we find that just enforcing a few consistency constraints suffices to find the map assignment, and we can show that adding any additional constraints would not result in a better solution.",
                    "label": 0
                },
                {
                    "sent": "Now, this approach sounds good, but it raises several difficult questions.",
                    "label": 0
                },
                {
                    "sent": "First, how could we efficiently solve this linear program even though we're working with the relaxation in general for large undirected model, these LP's will be very large and working within the Primal LP might be infeasible.",
                    "label": 0
                },
                {
                    "sent": "Second, how do we choose what clusters to add?",
                    "label": 0
                },
                {
                    "sent": "If we're going to be iteratively adding clusters to the relaxation and we hope to be able to stop early, then we had better be choosing the right clusters early on.",
                    "label": 0
                },
                {
                    "sent": "And third, how can we avoid re solving the linear program after we add each additional cluster consistency constraint?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In our in our paper, we propose to efficiently solve the linear program by working with the dual with the dual of the primal linear program.",
                    "label": 0
                },
                {
                    "sent": "So the dual LP.",
                    "label": 0
                },
                {
                    "sent": "Upper bounds the optimum of the Primal LP, and thus, as I mentioned to you before the primal upper bounds the map.",
                    "label": 0
                },
                {
                    "sent": "So the dual also upper bounds the map.",
                    "label": 0
                },
                {
                    "sent": "We use a particular dual formulation given by globerson yakala from last year, which has the advantage that they give an efficient message passing algorithm to optimize this dual LP.",
                    "label": 0
                },
                {
                    "sent": "This message passing algorithm corresponds to doing coordinate descent in the dual, which has the nice property that every step of the algorithm monotonically decreases in upper bound on the map.",
                    "label": 0
                },
                {
                    "sent": "So what cluster do we add next?",
                    "label": 0
                },
                {
                    "sent": "Or we propose using a greedy bound minimization algorithm where we add clusters which minimize the upper bound on the map as much as possible.",
                    "label": 0
                },
                {
                    "sent": "In doing so, we're guaranteed that every cluster we add is going to improve our solution.",
                    "label": 0
                },
                {
                    "sent": "We we avoid re solving the LP by warm starting the new messages.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With the old messages, so are now illustrate the dual algorithm.",
                    "label": 1
                },
                {
                    "sent": "Which is the main results of this talk.",
                    "label": 0
                },
                {
                    "sent": "So we first run message passing using some some initial relaxation, such as that given by pairwise consistency constraints.",
                    "label": 0
                },
                {
                    "sent": "Where as I said before, each each message that's being sent out correspondence to optimizing the dual P. Once we converge with the with the current relaxation, we try to decode an assignment from the messages and check to see if there is a large integrality gap.",
                    "label": 0
                },
                {
                    "sent": "If the gap is smaller, then we're done, but in this case it isn't, so we try adding in some cluster to the relaxation.",
                    "label": 1
                },
                {
                    "sent": "I'll tell you in a little bit how to choose what cluster to add.",
                    "label": 0
                },
                {
                    "sent": "So we then warm start by initializing the message from the cluster to zero.",
                    "label": 0
                },
                {
                    "sent": "It turns out that this is a new dual feasible solution.",
                    "label": 0
                },
                {
                    "sent": "It turns out for this special do LB and this new dual feasible solution has the same dual objective value as.",
                    "label": 0
                },
                {
                    "sent": "As we had in the previous relaxation, so we've now saved the effort of optimizing up to the current spot.",
                    "label": 0
                },
                {
                    "sent": "So then we continue running message passing until again we converge.",
                    "label": 0
                },
                {
                    "sent": "We decode the next assignment, see there still gap, choose a new cluster, worm starts and continue.",
                    "label": 0
                },
                {
                    "sent": "So we iterate this process until eventually.",
                    "label": 0
                },
                {
                    "sent": "We reached the map assignment and we're done.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I go straight.",
                    "label": 0
                },
                {
                    "sent": "So this is the same diagram, but for one of our real world problems.",
                    "label": 0
                },
                {
                    "sent": "The blue line is the dual objective, the red line is the integer solution as we go along.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So what cluster do we add to the relaxation well, so we could try to fully optimize LP after adding an additional cluster and try this for every cluster and choose the cluster which optimizes which decreases the upper bound as much as possible.",
                    "label": 0
                },
                {
                    "sent": "But this will be quite intense, computationally intensive.",
                    "label": 0
                },
                {
                    "sent": "An alternative would be a bit more greedy.",
                    "label": 0
                },
                {
                    "sent": "We just tried one message passing iteration with each new cluster and choose the cluster which decreases the bound as much as possible.",
                    "label": 0
                },
                {
                    "sent": "What we use is actually even more greedy.",
                    "label": 0
                },
                {
                    "sent": "We simply choose we look at for every possible cluster to add.",
                    "label": 0
                },
                {
                    "sent": "How much does just sending one outgoing message from that cluster decrease the dual bound?",
                    "label": 0
                },
                {
                    "sent": "And we choose the cluster which maximizes the decrease.",
                    "label": 0
                },
                {
                    "sent": "So using this coordinate descent algorithm, we get a very nice formula for what the dual decrease will be for any particular cluster.",
                    "label": 0
                },
                {
                    "sent": "And this is given.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The bottom so it has an intuitive meaning in terms of the difference between an independent decoding of the edge beliefs.",
                    "label": 0
                },
                {
                    "sent": "So these believes B12.",
                    "label": 0
                },
                {
                    "sent": "For example, we construct from the messages of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So it's the difference between an independent decoding and a global decoding of the edge beliefs for that cluster.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as an example, suppose that we have edge beliefs where for edge IJ the belief is 99.",
                    "label": 0
                },
                {
                    "sent": "If X is not equal, DXJ and minus 10 effects I = X J.",
                    "label": 0
                },
                {
                    "sent": "Well, you can easily, it's easy to show that the left some is going to be 3 * 99 and the right sum.",
                    "label": 0
                },
                {
                    "sent": "No matter how you choose, the assignment is going to be 2 * 99 -- 10.",
                    "label": 0
                },
                {
                    "sent": "So we see here that the if we were to add in the cluster on nodes 1, two and three, the dual, that is to say that the upper bound on the map will decrease by at least this much.",
                    "label": 0
                },
                {
                    "sent": "For those of you familiar with with spin graph models in statistical physics.",
                    "label": 0
                },
                {
                    "sent": "The notion of frustration.",
                    "label": 0
                },
                {
                    "sent": "You could show that if the dual in decreases.",
                    "label": 0
                },
                {
                    "sent": "So if this sum is, this difference is larger than zero, then there was frustration in the model prior to.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this cluster.",
                    "label": 0
                },
                {
                    "sent": "So now move on to some related work before showing you the experimental results.",
                    "label": 0
                },
                {
                    "sent": "So the region pursuit algorithm given by Max Welling, an UI of 2004 is qualitatively very similar to our algorithm, and his approach was to try to.",
                    "label": 0
                },
                {
                    "sent": "The goal was to approximate marginals well.",
                    "label": 0
                },
                {
                    "sent": "Here we're trying to do map, but he adds again just like us.",
                    "label": 0
                },
                {
                    "sent": "He adds regions iteratively such that the region he chooses, the regions which most changed, the region free energy approximation.",
                    "label": 0
                },
                {
                    "sent": "So because they were using this region free energy approximation and not some bound which we're minimizing like like in our work they found that unfortunately sometimes adding regions gives worse results.",
                    "label": 0
                },
                {
                    "sent": "Well in this work, that problem doesn't arise.",
                    "label": 0
                },
                {
                    "sent": "Another related work which was by myself and Tommy Apla last year, worked with the Primal LP.",
                    "label": 0
                },
                {
                    "sent": "Oh so iteratively tightening the LP relaxation.",
                    "label": 0
                },
                {
                    "sent": "In that work we were we used a selection criteria of the consistency constraint, which is most violated.",
                    "label": 0
                },
                {
                    "sent": "While in this work we use a bound minimization criteria.",
                    "label": 0
                },
                {
                    "sent": "These approaches are possibly complementary, and it would be interesting to consider both primal and dual approaches for optimizing this.",
                    "label": 0
                },
                {
                    "sent": "Well, in that earlier work we were able to efficiently find violated constraints.",
                    "label": 0
                },
                {
                    "sent": "We found that resolving the primal was very difficult and using this dual formulation gets around that problem.",
                    "label": 0
                },
                {
                    "sent": "I'd like to note that there are many other dual formulations for the for the LP that I proposed, and we use the dual formulation given by Globerson Yacula last year, which has the very nice property that the coordinate descent step, which is what happens when you pass the messages.",
                    "label": 0
                },
                {
                    "sent": "It gives a very nice and easy to understand.",
                    "label": 0
                },
                {
                    "sent": "Guaranteed bound decrease.",
                    "label": 0
                },
                {
                    "sent": "That was the formula that I showed you earlier.",
                    "label": 0
                },
                {
                    "sent": "I also I'd like to note that concurrently to our work, a similar approach has been proposed by Thomas Warner and CPR this year.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So moving on to the experiments, the protein design problem is we're given a 3D.",
                    "label": 0
                },
                {
                    "sent": "We're given a protein is 3D shape, and our goal is to choose amino acids along the backbone of this protein such that we get the most stable structure.",
                    "label": 0
                },
                {
                    "sent": "Each state corresponds to the choice of amino acid and sidechain angle.",
                    "label": 0
                },
                {
                    "sent": "Because we have to choose not only the amino acids, but we have to figure out what angle does amino acids are in where the angle the angles relate to each other and give some energy function.",
                    "label": 0
                },
                {
                    "sent": "The microphone and fields have between 41180 variables, each variable having a very large number of states between 95 and 158.",
                    "label": 0
                },
                {
                    "sent": "So these problems are very large.",
                    "label": 0
                },
                {
                    "sent": "Have large tree with, have very many small cycles.",
                    "label": 0
                },
                {
                    "sent": "In particular 20,000 triangles, about 20,000 and are very frustrated.",
                    "label": 0
                },
                {
                    "sent": "For those of you who know, this means, so they're not.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To solve in particular, if you were to try to use the primal LP formulation, you find that for typical protein it has.",
                    "label": 0
                },
                {
                    "sent": "the LP formulation has over 10,000,000 variables and this is using the pairwise consistency relaxation, which is sort of the bare minimum you need for these problems and the number of constraints is in order of 300,000.",
                    "label": 0
                },
                {
                    "sent": "Using commercial linear programming solver on the Primal LP would only be able to run on three of these proteins before running out of memory.",
                    "label": 0
                },
                {
                    "sent": "So this motivates the need to have to solve these problems in other ways.",
                    "label": 0
                },
                {
                    "sent": "In particular, we moved to the dual.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Even if we were able to solve them in the primal.",
                    "label": 0
                },
                {
                    "sent": "It was shown in earlier work that the pairwise consistency relaxation only solves two of the 97 proteins in this data set.",
                    "label": 0
                },
                {
                    "sent": "So we really need to be able to tighten the relaxation efficiently.",
                    "label": 0
                },
                {
                    "sent": "With triplets so we start with the pairwise consistency and add triplets iteratively in the method I showed you earlier, we were able to solve 96 of the 97 proteins in this data set.",
                    "label": 0
                },
                {
                    "sent": "Now we found these results to be very surprising.",
                    "label": 0
                },
                {
                    "sent": "We hadn't known of anyone else previously.",
                    "label": 0
                },
                {
                    "sent": "They were able to solve these problems so.",
                    "label": 0
                },
                {
                    "sent": "We're pretty excited about this.",
                    "label": 0
                },
                {
                    "sent": "We added between 5 and 735 triplets out of the possible 20,000 that you could have added, and notice that because of the large state spaces of these models even just one triplet message requires over a million computations.",
                    "label": 0
                },
                {
                    "sent": "So it would be clearly impossible to use all triplets messages.",
                    "label": 0
                },
                {
                    "sent": "We really need to be able to accurately select which clusters to use in our relaxation, which is what we do.",
                    "label": 0
                },
                {
                    "sent": "On average, these took about 10 hours to solve with the largest proteins taking 11 days.",
                    "label": 0
                },
                {
                    "sent": "We used MATLAB and C implementation which was very unoptimized and we have lots of ideas of how to optimize both the code, but also the algorithmic details to get much.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Federer results.",
                    "label": 0
                },
                {
                    "sent": "So one question which we looked into was, would it be?",
                    "label": 0
                },
                {
                    "sent": "Do we really need to run to convergence prior to choosing a new cluster to add to the relaxation?",
                    "label": 0
                },
                {
                    "sent": "So we looked at this and on the left on the red line we show.",
                    "label": 0
                },
                {
                    "sent": "The algorithm where we only run 20 message passing iterations before choosing a new cluster to add in and the blue line is if we were to add if we were to run until convergence before adding in the clusters.",
                    "label": 0
                },
                {
                    "sent": "So we found that it was much better to stop early.",
                    "label": 0
                },
                {
                    "sent": "That is not wait until convergence before adding clusters and we were able to solve these problems much much faster.",
                    "label": 0
                },
                {
                    "sent": "Note that the reason that we can do this is because the because precisely because of our use of this greedy bound, this criterion that we have is valid regardless of whether we ran to convergence or not.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Next we looked at a stereo vision problem where roughly in stereo vision were given 2 images corresponding to the view from the left in the right eye, and our goal is to find the depth of the objects in the image.",
                    "label": 0
                },
                {
                    "sent": "This again can be posed as an undirected graphical model, where we have one variable for every pixel, and we looked at problems which have about 100, six, 16 by 154 pixels, so 13,000 variables in the model an each variable takes 16 states.",
                    "label": 0
                },
                {
                    "sent": "These problems are also somewhat hard to solve.",
                    "label": 0
                },
                {
                    "sent": "Have lots of small.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Eagles so we we didn't do that extensive experiment.",
                    "label": 0
                },
                {
                    "sent": "We just took 10 images from from the top and it all energy function and the and 10 different settings of parameters that they suggested.",
                    "label": 0
                },
                {
                    "sent": "And we tried solving the corresponding map problems.",
                    "label": 0
                },
                {
                    "sent": "The pairwise relaxation was able to solve 6 of the 10 problems, but for the remaining ones we had to add in square clusters before we were able to solve them.",
                    "label": 0
                },
                {
                    "sent": "But we do.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Solve all of them.",
                    "label": 0
                },
                {
                    "sent": "So finally we asked.",
                    "label": 0
                },
                {
                    "sent": "Whether aggressively adding clusters would be a good heuristic.",
                    "label": 0
                },
                {
                    "sent": "So rather than adding the one best cluster at each step and then running 20 iterations, we might want to try adding, let's say, the best 20 clusters at each step before running additional message passing iterations.",
                    "label": 0
                },
                {
                    "sent": "So we found that if we had just the one best cluster, we had many few clusters to the relaxation before optimally solving them.",
                    "label": 0
                },
                {
                    "sent": "Versus adding the 20 best, but on the other hand.",
                    "label": 0
                },
                {
                    "sent": "Regardless of having added very many more clusters, being more aggressive actually results in faster runtimes.",
                    "label": 0
                },
                {
                    "sent": "And finally, randomly adding clusters was completely.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Useless.",
                    "label": 0
                },
                {
                    "sent": "So in future work we're interested in pursuing this question of how to efficiently find the clusters which minimize the bound.",
                    "label": 0
                },
                {
                    "sent": "So this work we assume that someone hands us a bucket of clusters.",
                    "label": 0
                },
                {
                    "sent": "We show how to select which of these clusters would be best to use.",
                    "label": 0
                },
                {
                    "sent": "There's a straightforward extension of our work to structured prediction.",
                    "label": 0
                },
                {
                    "sent": "In large microphone fields with generally intractable structure and using Tamir's algorithm which was presented in the previous talk, we could try extending our results to approximating modules and the partition function.",
                    "label": 0
                },
                {
                    "sent": "Finally, will be releasing some optimized code soon, so look forward to it and ask me if you want it.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Constraints.",
                    "label": 0
                },
                {
                    "sent": "That last example where you're adding batches.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so that's a very common approach in the primal, right?",
                    "label": 0
                },
                {
                    "sent": "So if you're solving the primal, you add constraints and remove them when they're no longer binding in the dual.",
                    "label": 1
                },
                {
                    "sent": "It's a bit more difficult because once you start optimizing with the constraint in your, in essence, adding the algorithm in essence adds a potential function to those constraints, and we can no longer remove them.",
                    "label": 0
                },
                {
                    "sent": "Try looking at other.",
                    "label": 0
                },
                {
                    "sent": "Families.",
                    "label": 0
                },
                {
                    "sent": "Look at the loops.",
                    "label": 0
                },
                {
                    "sent": "Yes, that's exactly right.",
                    "label": 0
                },
                {
                    "sent": "So the squares that we added our loops, but you're right that in general you could use larger cycles.",
                    "label": 0
                },
                {
                    "sent": "One of the big problems is finding these cycles.",
                    "label": 0
                },
                {
                    "sent": "So once you start moving to larger cycles, you have to enumerate over all of them to valuate the bounded.",
                    "label": 0
                },
                {
                    "sent": "Valuing.",
                    "label": 0
                },
                {
                    "sent": "Bound is easy, but there are a lot of them regardless for well structured problems such as grids, this is easy to do.",
                    "label": 0
                },
                {
                    "sent": "An would be.",
                    "label": 1
                },
                {
                    "sent": "A perfectly interesting direction for future work.",
                    "label": 0
                },
                {
                    "sent": "Or strange for a cluster with another cluster, you optimize it.",
                    "label": 0
                },
                {
                    "sent": "Optimize.",
                    "label": 0
                },
                {
                    "sent": "It's a block coordinate descent algorithm, but but the blocks are relatively small in our current imitation.",
                    "label": 0
                },
                {
                    "sent": "One of the interesting directions for future work is to use larger blocks, which is a bit more complicated to derive, but would result in much faster convergence in general.",
                    "label": 0
                },
                {
                    "sent": "Is not the best way to solve, so you're basically here the constraints of the particular form that makes coordinate ascent better, not necessarily better.",
                    "label": 0
                },
                {
                    "sent": "I mean using some 2nd order method might give better convergence, but when these problems are very, very large, it's been found various times over the last few years that.",
                    "label": 0
                },
                {
                    "sent": "Using these graph structured algorithms, of which coordinate descent makes easy, actually, is able to scale to much larger problems.",
                    "label": 0
                },
                {
                    "sent": "Relationship.",
                    "label": 0
                },
                {
                    "sent": "It's a good question.",
                    "label": 0
                },
                {
                    "sent": "I don't know because we haven't tried.",
                    "label": 0
                },
                {
                    "sent": "My guess would be that the protein design problems have nondeterministic but almost deterministic relations, because there really are hard constraints here that two side chains cannot be in the same location at the same time, so we could try.",
                    "label": 0
                },
                {
                    "sent": "So we were able to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "That gives some intuition of what to expect.",
                    "label": 0
                }
            ]
        }
    }
}