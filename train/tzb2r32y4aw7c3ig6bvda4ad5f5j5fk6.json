{
    "id": "tzb2r32y4aw7c3ig6bvda4ad5f5j5fk6",
    "title": "Confidence Measures in Speech Recognition",
    "info": {
        "author": [
            "Stephen Cox, University of East Anglia"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "June 2004",
        "category": [
            "Top->Computer Science->Speech Analysis"
        ]
    },
    "url": "http://videolectures.net/mlmi04ch_cox_cmsr/",
    "segmentation": [
        [
            "So the the title of my talk is confidence measures in speech recognition and here is what I'm going to say.",
            "In outline.",
            "I'll start by motivating the talk.",
            "Why do we?",
            "Why do I think we need confidence measures in speech systems for speech systems?",
            "I want to give some motivation here for recognizer independent measures because there's been quite a lot of work done on measures which are highly recognized as specific.",
            "This talk is going to concentrate on recognizer independent measures.",
            "The talk then itself is really in two parts in the."
        ],
        [
            "First part, I'm going to look at.",
            "I'm going to describe two methods that we've developed for estimating confidence measures based on phone and word models, which are essentially phone correlation, something I call meta models, and then in the second part I'm going to talk about using semantic information to estimate confidence measures.",
            "And usually end with a discussion.",
            "So first question, why confidence measures?",
            "What is a con?"
        ],
        [
            "Evidence measure well.",
            "There are plenty of definitions, but one which is as good as any is that it's a number between zero and one that indicates what you might say is a degree of belief that some unit output by the recognizer could be.",
            "A phrase could be at work, could be a phone, is correct, and the most important application of something like confidence measures is probably in speech dialogue systems.",
            "You know things like ticket ticket booking, call routing or routing if you prefer.",
            "Which is highly relevant to this sort of things we're talking about at this conference whi well, of course, uncorrected errors can be completely disastrous in a dialogue system.",
            "But if you've got to confirm every single content word, that's very tedious.",
            "So if you can have a confidence measure, you can try and decide which words are correct and which need to be confirmed, or to be corrected.",
            "Some other applications, one which has been looked at recently, is using these things.",
            "Confidence measures for unsupervised speaker adaptations, so that you're fairly sure that models you adapt to the right models, and you can also use them, for instance to re Score N best hypothesis.",
            "So, um, a lot of the."
        ],
        [
            "Who work that has been done on confidence measures has been as used, but you might say ad hoc features that have been derived from Earth side output from the recognizer.",
            "So things like number of competing hypothesis word is decoded, which is 1.",
            "I'll talk about in a second likelihood ratios, stabilities of the words output in a lattice, and so on.",
            "Even looking at the training data and saying well, there were fewer training instances of this word or.",
            "Or phoneme, so it's less likely to be decoded correctly.",
            "Difficulty with quite a few of these is there highly recognize a specific I want to give you an example of this."
        ],
        [
            "This was some work I did using the HTK recognizer, I think, and here the X axis is time.",
            "It's the frame number of something from the Wall Street Journal.",
            "What go back?",
            "Y axis is the number of competing word hypothesis at any point in the dashed lines show where a word has been decoded and where there's a no, it's been decoded correctly.",
            "It's a correct word.",
            "Aware there's an X, it's been decoded incorrectly, and you can see there is some correlation here between the OHS and low competing hypothesis and the X's and high competing hypothesis not perfect, sorry.",
            "But there was something there.",
            "However, when I tried this again using a different recognizer, it worked on a different principle.",
            "This did not work at all, so I think it shows the difficulty of using of using highly specific recognizes specific confidence measures.",
            "So, um."
        ],
        [
            "Part One general approach, well, I'll skip through this slide quite quickly because we all know how Bayes theorem is used in recognition.",
            "We've got the probability of word sequence given some acoustics and the acoustic models gives the probability of the acoustics given the word.",
            "The language model gives us the probability of the word sequence and we can normalize by the probability of the acoustics, but of course we don't need to do that for recognition anyway.",
            "But the point is that the."
        ],
        [
            "The errors the decoding errors can be said to occur when the language model is inaccurate or when the acoustic models are inaccurate in some way in the sense they don't represent the situation.",
            "And knowing when the recognizer does the decoding, these two probabilities of course integrated and one of the ways we can attempt to disentangle these effects to some extent, is by using a parallel phone recognizer in tandem with the word recognizer.",
            "What I'm going to talk about is these two approaches.",
            "One where we use the correlation between the two independent recognizers to form a confidence measure and then.",
            "In a second approach, we use the phone record phone recognizer to hypothesize word strings would correlate them with the word recognizer output to for my confidence measure."
        ],
        [
            "So let's take the first one, doing some correlation on phone sequences from phony and word recognizers.",
            "So we put the speech into two recognizers of phoneme recognizer in a word recognizer, which may or may not have different phoneme models.",
            "This that that is actually an issue which I don't have time to talk about here.",
            "We get a sequence of phonemes from the phone recognizer from the word recognizer.",
            "We get a sequence of words, but we use the dictionary too.",
            "Rewrite that as a sequence of phonemes, and of course, if we've got multiple pronunciations, we have to say which of those pronunciations was was actually used, and then we can combine those two strings in some way.",
            "Doing some analysis to form a confidence measure.",
            "There are two ways we could use the phoneme strings.",
            "We could either simply tag the phonemes with the number of the word that they represent.",
            "Since we've got the transcription, we can do that, or we can use dynamic programming."
        ],
        [
            "To align the string from the phone recognizer with the string derived from the word recognizer.",
            "And a very simple distance measure that we can use is really based on a confusion made."
        ],
        [
            "Tricks not between the correct and the incorrect decodings, but between the phonemes produced by the phone recognizer an the phonemes produced by the word recognizer so so it looks something like that.",
            "And if we make the very bad assumption that they are the separate phonemes are independent, then we can multiply those those probabilities so we can add the logs and that gives us a distance measure.",
            "The."
        ],
        [
            "Better distance measure can be found by using a likelihood ratio.",
            "In this kind of way.",
            "In this scenario, we actually Form 2 Co occurrence matrices.",
            "One is a Co occurrence matrix matrix.",
            "When the words are correctly.",
            "When a word is correctly decoded by the recognizer and one is a Co occurrence matrix.",
            "When a word is incorrectly decoded by the by the recognizer and then we can.",
            "Combine those two in a likelihood ratio, the probability essentially sorry of correct over incorrect in that kind of way and I'll show you the results for these little bit later on.",
            "So yet another technique?"
        ],
        [
            "Is where we actually hypothesize words from phone strings.",
            "So the probability of the word given the acoustics we can.",
            "Represent.",
            "In in this way we can.",
            "We can sum over all the possible phonemes, the probability of word of the word given the phonemes times approach probability of the phonemes given the acoustics and.",
            "We can approximate that in the standard way by using the most likely phoning sequence P star.",
            "So the probability of the most likely phoneme sequence given the acoustics is of course by definition estimated from the parallel phoneme recognizer.",
            "The probability of a word string given the most likely phoneme string is what I want to talk about.",
            "Two methods that we developed for estimating that.",
            "Let's list as we call it and metal models.",
            "So the idea behind the next list."
        ],
        [
            "Was this in fact I think it is the way that people used to do sorry used to do recognition.",
            "Speech goes into both the phoneme recognizer and the word recognizer.",
            "The phoneme recognizer hypothesizes a sequence of words.",
            "And we use that sequence.",
            "We slide a window over that sequence and from that we can build up lists of words whose profile fits that.",
            "Sorry which which contain that Mr phonemes within them.",
            "And we can refine that technique.",
            "Of course, we know that this string is full of, is full of errors, is full of substitutions, deletions, insertions.",
            "So we can refine the.",
            "We can refine the data by using the confusion matrix of the phoneme recognizer, which we've built up a training time to look at different.",
            "Sequence is there and to build a different lists and you can see the sort of thing that happens.",
            "You rapidly get very long lists of words building up.",
            "So.",
            "The idea is then that these words are hypothesized by the same language model that goes into the word recognizer, and then we can.",
            "We can correlate the output of the word recognizer with words in this list, and in this case this is actually the sample or one of the transcriptions of the word.",
            "Unusually, we see that unusually appears several times in this list, and when we correlate it with the output from the word recognizer.",
            "That's also produced the word unusually, and so we can use that kind of correlation.",
            "You can think of several ways in which you could do it to form a confidence measure."
        ],
        [
            "I think I'll leave that one, actually.",
            "Um?"
        ],
        [
            "The difficulty with that technique is there are all sorts of ad hoc decisions that you've got to make about the size of the sliding window you're going to run on the phoneme string about what to do with short words and things like like that, and I think you can see that when you when you add into that the confusion matrix, so every every phoneme has got a probability of being confused with many other phonemes or being inserted, or.",
            "Delete it, etc.",
            "There is a combinatorial explosion in the candidate words and the thing gets unmanageable so that technique was abandoned.",
            "Really, when we saw that that was going to be unworkable and we came up with something which we call meta models.",
            "And the idea of the meta model is it uses knowledge of folding confusions within an HMM framework to produce these candidate word list that we can then use for confidence measures.",
            "So here I'll try to explain how the meta models are built."
        ],
        [
            "So this is a training time.",
            "We take the speech, we put it into the phoneme, recognize that we get a set of N best transcriptions.",
            "We compare these with the reference transcription and then for each phoneme in the inventory we build a little hidden Markov model like this and this is.",
            "This is not a model that represents acoustic observations.",
            "This is actually a model of the confusion probability of this phony.",
            "So each state here has got a a discrete set of 45.",
            "If there are 45 phonemes, it's got a discrete set of.",
            "45 probabilities associated with it, and the idea is, let's say that this Foley are was always perfectly decoded by our recognizer.",
            "In that case, all three distributions here would have one for Anna, Zero everywhere else.",
            "This transition probability that skips the first date would be 1.",
            "The transition probability that skips the final state will also be one, and so if you think of this thing as a as a generator.",
            "It would always just generate the phoneme app with probability one.",
            "These two states.",
            "This represents an insertion before this is a insertion after and this arc of course is a dilution of the.",
            "So the idea was to build up models of the confusion of the different phonemes and they trained in just the standard way you train acoustic Hmm's.",
            "Now when we want to get a confidence."
        ],
        [
            "Made here from a set of those what we do is again we put the speech into the parallel recognizers and the crucial thing is we use the same language model as the word recognizer to decode the output from the meta models, and so that gives us a set of word strings coming out here which actually represent the posterior probability of the words given these noisy strings.",
            "Sorry this noisy phoneme string.",
            "It's coming in here, so if we then correlate the word string from the word recognizer with this word string produced by the.",
            "Meta models we can get a confidence measure."
        ],
        [
            "OK, so here here quickly is some.",
            "Now move on to the experiments.",
            "Here's some details about the models and things.",
            "It's not really very revolution.",
            "We revolutionary.",
            "We use WSJ camel database.",
            "There are 92 speakers, 10,000 sentences, etc around go all through this, but it's you know fairly standard large large vocabulary.",
            "Recognition stuff.",
            "For the confidence measures, we used independent training and testing sets, of course.",
            "There's a niche."
        ],
        [
            "Q About the How you quote the performance of confidence measures.",
            "What we do is of course we we we set a threshold on the confidence measure.",
            "We can alter the threshold to get a receiver operating curve, which I'll show you a little later.",
            "But we use the threshold to tag each decoded word as being either we think correct or incorrect.",
            "If you think about this for a second, you will realize it.",
            "If your recognizer is, say, 80% accurate.",
            "If you just tag every word is correct, your confidence measure is apparently working at 80%.",
            "But of course it's not, it's just, it's just guessing that's the most.",
            "That's the most sensible thing you can do.",
            "So you've got to get the gain over guessing, and there are there are some much more sophisticated ways of measuring that than what I've got here.",
            "One of them is.",
            "Normalized cross entropy, but we just use the improvement over guessing, so the guessing rate is the number of errors made by the recognizer of the total number of words from the confidence measure.",
            "It's the number of correct of incorrectly tagged words over the total words and the gain therefore is the guessing measure minus the baseline divided by the guessing measure.",
            "Sorry, wrong way round Mr. Forgetting measurements.",
            "The confidence measure error.",
            "Over the guessing error.",
            "Now, the baseline that we used was the so called N best confidence that had been a popular one up to the time that we we we."
        ],
        [
            "Doing this work and it does perform quite well and.",
            "The way that this works is you get the best transcriptions out of the recognizer and then you align them in some way.",
            "There are various ways of doing this.",
            "What I'm showing here is we take the top transcription as being quotes.",
            "The truth we align all the other transcriptions to thetop transcription using dynamic programming, and then we just count the number of times that a certain word appears in the same place.",
            "The news that proportion, as our measure of confidence that the word is correctly decoded so you can see here, for instance, can appears as the word in this position four times.",
            "There are nine decodings, so it's got a confidence of four ninths, etc etc."
        ],
        [
            "So here are the results.",
            "This is the percentage improvement over just guessing and you see that the baseline actually does rather well.",
            "And actually the phone correlation.",
            "Whichever way you do it using either lead distance measure or the likelihood ratio just just does not perform really.",
            "There's just not enough information in those in those strings to be able to to get something out, but the metamodels achieves a considerable performance improvement on the baseline, and we've used that subsequently and is very successful.",
            "So that was part one of the talk.",
            "I'm going to talk about another approach to."
        ],
        [
            "Confidence measures which is using semantic information.",
            "And those of us who played with speech recognition of all seen how was like this, and they quite often appear in magazines or one of the articles in magazines where this, this kind of power is quoted.",
            "So this is an actual decoding of a Wall Street Journal sentence from our recognizer.",
            "It's managed to put the word violin in there.",
            "And that should have been oil and but it got violin.",
            "Presumably the bigram violin gas was not in the training set.",
            "I don't think it would have been.",
            "Presumably this was an unsuccessful back off to a unigram.",
            "But the point about something like this is that it's it's very easy to spot that the word violin must be wrong.",
            "So could one use this as a confidence measure?",
            "Well probably, but only a small proportion of the incorrect words could could possibly be identified in this way, so that's the downside.",
            "But the upside is that that that kind of mistake is is likely to be independent.",
            "Of the sort of measures that are based on the things the inside the decoder.",
            "So we might be able to combine that rather weak information with some stronger information to get a better confidence measure.",
            "And that is indeed the case.",
            "And the nice thing about it is it requires no recognizer side information whatsoever.",
            "If you could, if you could make this work, just just the one best output would be enough.",
            "So to see whether this was viable, I actually myself went through about 600 sentences.",
            "During a new meeting, I think so.",
            "Not all the EU meetings are completely useless.",
            "Unmarked any word that I considered to be in."
        ],
        [
            "Correct on grounds of semantics.",
            "So things like a violin power and then I checked the results against the transcriptions and what I find what I found was there were 67680 plus 49 correct words in the transcription 2720 plus 421 incorrect of the words that were actually correct.",
            "I'd marked 49 as being incorrect of the words that were actually incorrect.",
            "I'd marked.",
            "421 sorry, 421 is being incorrect.",
            "So of the 470 words that that were marked as incorrect.",
            "Sorry of the total of 3000 words that were incorrect.",
            "I'd only managed to spot using this technique 470 so we could call that the recall in information retrieval terms, and that's pretty low.",
            "However, the precision is quite high because of those 470 words are marked.",
            "Actually, 90% of them were incorrect, so.",
            "It appeared that there was something there, but only at low recall."
        ],
        [
            "So what we what we need then, if we're going to use something like this, is a way of identifying words that are semantically distant from the other decoded words in the sentence.",
            "Just as the violin was in that example sentence.",
            "There's been lots of work on clustering words, of course, but there is a problem of data sparsity, and there's also quite interesting problem.",
            "Many semantically close word pairs actually rarely Co occur together.",
            "For instance, take words movie and film, which.",
            "All synonyms, but which would really be in the same text side by side.",
            "You would use one or the other or say striker and batsman Witcher related semantically, but they're in games played in different parts of the world.",
            "So clustering is not really a very strong candidate.",
            "Latent semantic analysis is a technique that's been successfully used to associate semantically similar words.",
            "Just give just briefly rundown how it works."
        ],
        [
            "What windows is to former Co occurrence matrix where the rows of the matrix are.",
            "The other words that occur in the set of documents that you've gotten, we we assume that the documents are semantically coherent in some way that they represent the same topic.",
            "And that, of course, is a huge and very sparse matrix, and the way that we deal with that matrix is to use singular value D."
        ],
        [
            "Position on it.",
            "So we start with this huge matrix W and we decompose it as an orthogonal matrix.",
            "U turns a matrix of eigenvalues S times an orthogonal matrix V transpose.",
            "And in fact that decomposition will be exact when this internal dimension are actually matches the external dimension end there.",
            "So the idea of this is to represent the words in the subspace where they are semantically sorry from a semantical point of view, they they are closer and so we moved down from a matrix that about 20,000 words by 20,000 documents to represent the words in 100 dimensional space.",
            "So we."
        ],
        [
            "The Wall Street Journal corpus, again for the.",
            "For these experiments we use the paragraphs of the from the corpus as the documents.",
            "The paragraphs are fairly much semantically coherent when you look at them.",
            "About 20,000 documents and there also about 20,000 different words, and after some experimentation with this internal space size, the size of about 100 seems to be seems to give about the best performance, which maybe says says that there are about 100 quotes.",
            "Different concepts in that in the Wall Street Journal Corpus we used.",
            "And we compute we.",
            "So we project the words into this subspace and then we compute the semantic similarity, just as the dot product of the vectors representing the words just like that.",
            "Panda is quite interesting to look at the distribution."
        ],
        [
            "Use of the scores for for the 20,000 words in the in the vocabulary.",
            "If you rank the words by their mean by their mean score, then the word that comes out top is the word out and you can see probably can't see from the back, but that is a score of zero there.",
            "That's a score of 1 is this is a cosine distance.",
            "The range is actually minus one 2.",
            "To one you see the word out has a huge range of scores, so that indicating that it occurs with a lot of different words.",
            "And then as you go down the down the ranking the the school starts to move in towards 0 so you get fewer and fewer high scores because you're getting words which only occur with well which occur infrequently anyway.",
            "But when they do occur.",
            "Only occur with certain other words, and so their average score is close to 0 and that's the bottom word denomination, and you see it's the scores are very tightly clustered around zero there.",
            "So how do we use all that to make a config?"
        ],
        [
            "This measure.",
            "But there are several things you could sensibly do with it.",
            "Just look at the mean score of the each word to the other decoded words.",
            "Try making a bit more robust, perhaps by looking at the rank.",
            "The most successful thing by just a little bit was actually to model those distributions.",
            "I showed you as a mixture of Gaussians and compute the computer probability of the similarity of a word to the to the other decoded words.",
            "Um?"
        ],
        [
            "It's quite important to use a stop list here so function words like to and often out and so on.",
            "These occur with most words, so they just.",
            "They just contribute noise and so we just get rid of those.",
            "So, so we set some threshold and we say, well, a word that's gotta semantic score more than some threshold is is limited.",
            "There is a slight problem with that.",
            "As soon as you do that then the baseline performance of the recognizer increases, so you have to take that account in the results.",
            "His, um, the distribution of."
        ],
        [
            "One of the measures we use this is the one where we modeled the distributions of the semantic scores as a mixture of Gaussians.",
            "And on top here you can see the distribution of the scores from the correctly decoded words and on the on the bottom you can see the scores from the incorrectly.",
            "Under this was a bit of a surprise when I saw that when I first service, because what I've been expecting to happen or what was that lead for the incorrectly decoded words, there'd be a big peak here of words that had no sorry, that had a very low semantic distance to the other words because they had been incorrectly decoded, but in fact, when you look at the details of the two distributions.",
            "There's very little in it.",
            "Um, but there's a large, large difference at the top of the of the graph there so, so that's the.",
            "Passwords which are very close have a high summer, have a high score."
        ],
        [
            "So this is just what I'm saying.",
            "This technique derives seems to derive what discrimination is passed by identifying not the incorrectly decoded words, but the correctly decoded words.",
            "But when we looked at what it was doing, it was actually it's actually finding words that were words like numbers in financial terms that were highly cognate with each other, and so they had a nice high score to each other, and so it was able to discriminate and quite nicely.",
            "When we looked."
        ],
        [
            "Other words at the bottom actually a lot of them were common words that had been correctly decoded, and yet they had a very low score, and so we just came to the conclusion that when you've got these fairly common words, there's just not enough training material to capture all the Co occurrences with the other words, and so you will get these effects.",
            "There were actually, of course, out of vocabulary words as well in there.",
            "So, um, just look at the performance on a."
        ],
        [
            "Receiver operating curve.",
            "So there are actually 3 here so.",
            "The semantic confidence measure is the squares here, so this is now just the straight correct recall against the straight precision.",
            "The baseline.",
            "The end best recognizer.",
            "Sorry, the N best confidence measure is here.",
            "Now this is clearly sorry.",
            "This is recall and precision, so we would like to be as far up to the top right corner as we could be.",
            "That would be the ideal system.",
            "So this is clearly well superior to the semantic confidence measure.",
            "But one one point you should notice is this outlier points on the left here, and this is a well known phenomenon with N best.",
            "This occurs because you when when you use the N best confidence measure, there are always several words which are incorrectly decoded but which appear in every single decoding.",
            "So they give you a confidence of 1 even though they're incorrectly decoded and that leads to this.",
            "Drop off in performance here.",
            "Now when we combined the N best with the semantic confidence measure, we get the asterisks so it's nowhere any worse.",
            "And for this important region where you've got rather low recall but you're interested in, so you've got a small number of words, but you want to say with high confidence whether they're correct or not.",
            "This this does rather better.",
            "So on its own, it's quite a weak indicator of confidence."
        ],
        [
            "But it's useful when used with something else.",
            "OK, so this is the final slide, So what I've said here is I've been talking about techniques for identifying incorrect words in the output of the speech recognizer that are not recognized as specific that don't don't depend on side information and by far the most successful.",
            "These is the metamodels technique.",
            "The idea here is to form models of the.",
            "Of the confusion of phonemes and to use these to generate words which are correlated with the word recognizer.",
            "Using some semantic information is a nice a nice idea, but it's the amount of the number of words that can be recognized as being correct or incorrect using this ways rather is rather limited, but it can be combined with the N best to to give some increase in performance.",
            "And finally I haven't said anything, of course about the performance of these.",
            "Of these measures in real systems, but of course leave the real assessment of them comes when you use them in a real system to see how well how much they improve your dialogue system.",
            "And that's it.",
            "So we have time for some questions.",
            "Anybody has one?",
            "You replaced.",
            "Guys over the phone recognizer.",
            "Did he look if the confidence called computation now?",
            "Oh, how much depended on the specifics of the phone recognizer.",
            "Well, I think the point about a phone recognizer is, on the whole it's you know they are, they are.",
            "They are pretty vanilla things.",
            "You generally have a fun a tactic model in there.",
            "Course you can have different number of mixture components and things like that but but they are less variable.",
            "I would claim the word recognizes.",
            "Any other questions?",
            "So my question which is?",
            "Using the site information and the semantic information you have, you're in effect building a different model for your confidence measure.",
            "So do you think it's possible to close the loop and actually use that information to get better speech recognizer?",
            "Yeah, I mean, I suppose I suppose some people would claim that you shouldn't have to use confidence measures at all.",
            "You should.",
            "You should just be working on getting a better speech recognizer.",
            "But yes, there have been attempts, haven't they?",
            "To integrate semantic information into into recognizes, but I think it is.",
            "It's quite tricky to put it into a language model.",
            "Approach.",
            "So if you set up a cheap scription, sorry, I couldn't get along.",
            "Did you try to use of alteration?",
            "Motor 2 no?",
            "Um, we didn't use that, but then of course all the sentence is were from century the same domain.",
            "What we were, what we were trying to pick out because they were from Wall Street Journal.",
            "What we were trying to pick out was asleep until the finals in that, but I suppose it would have been possible.",
            "Ouch."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the the title of my talk is confidence measures in speech recognition and here is what I'm going to say.",
                    "label": 1
                },
                {
                    "sent": "In outline.",
                    "label": 0
                },
                {
                    "sent": "I'll start by motivating the talk.",
                    "label": 0
                },
                {
                    "sent": "Why do we?",
                    "label": 0
                },
                {
                    "sent": "Why do I think we need confidence measures in speech systems for speech systems?",
                    "label": 0
                },
                {
                    "sent": "I want to give some motivation here for recognizer independent measures because there's been quite a lot of work done on measures which are highly recognized as specific.",
                    "label": 0
                },
                {
                    "sent": "This talk is going to concentrate on recognizer independent measures.",
                    "label": 0
                },
                {
                    "sent": "The talk then itself is really in two parts in the.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First part, I'm going to look at.",
                    "label": 0
                },
                {
                    "sent": "I'm going to describe two methods that we've developed for estimating confidence measures based on phone and word models, which are essentially phone correlation, something I call meta models, and then in the second part I'm going to talk about using semantic information to estimate confidence measures.",
                    "label": 1
                },
                {
                    "sent": "And usually end with a discussion.",
                    "label": 1
                },
                {
                    "sent": "So first question, why confidence measures?",
                    "label": 0
                },
                {
                    "sent": "What is a con?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Evidence measure well.",
                    "label": 0
                },
                {
                    "sent": "There are plenty of definitions, but one which is as good as any is that it's a number between zero and one that indicates what you might say is a degree of belief that some unit output by the recognizer could be.",
                    "label": 1
                },
                {
                    "sent": "A phrase could be at work, could be a phone, is correct, and the most important application of something like confidence measures is probably in speech dialogue systems.",
                    "label": 1
                },
                {
                    "sent": "You know things like ticket ticket booking, call routing or routing if you prefer.",
                    "label": 1
                },
                {
                    "sent": "Which is highly relevant to this sort of things we're talking about at this conference whi well, of course, uncorrected errors can be completely disastrous in a dialogue system.",
                    "label": 0
                },
                {
                    "sent": "But if you've got to confirm every single content word, that's very tedious.",
                    "label": 0
                },
                {
                    "sent": "So if you can have a confidence measure, you can try and decide which words are correct and which need to be confirmed, or to be corrected.",
                    "label": 1
                },
                {
                    "sent": "Some other applications, one which has been looked at recently, is using these things.",
                    "label": 0
                },
                {
                    "sent": "Confidence measures for unsupervised speaker adaptations, so that you're fairly sure that models you adapt to the right models, and you can also use them, for instance to re Score N best hypothesis.",
                    "label": 0
                },
                {
                    "sent": "So, um, a lot of the.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Who work that has been done on confidence measures has been as used, but you might say ad hoc features that have been derived from Earth side output from the recognizer.",
                    "label": 1
                },
                {
                    "sent": "So things like number of competing hypothesis word is decoded, which is 1.",
                    "label": 1
                },
                {
                    "sent": "I'll talk about in a second likelihood ratios, stabilities of the words output in a lattice, and so on.",
                    "label": 1
                },
                {
                    "sent": "Even looking at the training data and saying well, there were fewer training instances of this word or.",
                    "label": 0
                },
                {
                    "sent": "Or phoneme, so it's less likely to be decoded correctly.",
                    "label": 0
                },
                {
                    "sent": "Difficulty with quite a few of these is there highly recognize a specific I want to give you an example of this.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This was some work I did using the HTK recognizer, I think, and here the X axis is time.",
                    "label": 0
                },
                {
                    "sent": "It's the frame number of something from the Wall Street Journal.",
                    "label": 1
                },
                {
                    "sent": "What go back?",
                    "label": 0
                },
                {
                    "sent": "Y axis is the number of competing word hypothesis at any point in the dashed lines show where a word has been decoded and where there's a no, it's been decoded correctly.",
                    "label": 0
                },
                {
                    "sent": "It's a correct word.",
                    "label": 0
                },
                {
                    "sent": "Aware there's an X, it's been decoded incorrectly, and you can see there is some correlation here between the OHS and low competing hypothesis and the X's and high competing hypothesis not perfect, sorry.",
                    "label": 0
                },
                {
                    "sent": "But there was something there.",
                    "label": 0
                },
                {
                    "sent": "However, when I tried this again using a different recognizer, it worked on a different principle.",
                    "label": 0
                },
                {
                    "sent": "This did not work at all, so I think it shows the difficulty of using of using highly specific recognizes specific confidence measures.",
                    "label": 0
                },
                {
                    "sent": "So, um.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Part One general approach, well, I'll skip through this slide quite quickly because we all know how Bayes theorem is used in recognition.",
                    "label": 1
                },
                {
                    "sent": "We've got the probability of word sequence given some acoustics and the acoustic models gives the probability of the acoustics given the word.",
                    "label": 1
                },
                {
                    "sent": "The language model gives us the probability of the word sequence and we can normalize by the probability of the acoustics, but of course we don't need to do that for recognition anyway.",
                    "label": 0
                },
                {
                    "sent": "But the point is that the.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The errors the decoding errors can be said to occur when the language model is inaccurate or when the acoustic models are inaccurate in some way in the sense they don't represent the situation.",
                    "label": 1
                },
                {
                    "sent": "And knowing when the recognizer does the decoding, these two probabilities of course integrated and one of the ways we can attempt to disentangle these effects to some extent, is by using a parallel phone recognizer in tandem with the word recognizer.",
                    "label": 1
                },
                {
                    "sent": "What I'm going to talk about is these two approaches.",
                    "label": 0
                },
                {
                    "sent": "One where we use the correlation between the two independent recognizers to form a confidence measure and then.",
                    "label": 0
                },
                {
                    "sent": "In a second approach, we use the phone record phone recognizer to hypothesize word strings would correlate them with the word recognizer output to for my confidence measure.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's take the first one, doing some correlation on phone sequences from phony and word recognizers.",
                    "label": 0
                },
                {
                    "sent": "So we put the speech into two recognizers of phoneme recognizer in a word recognizer, which may or may not have different phoneme models.",
                    "label": 1
                },
                {
                    "sent": "This that that is actually an issue which I don't have time to talk about here.",
                    "label": 0
                },
                {
                    "sent": "We get a sequence of phonemes from the phone recognizer from the word recognizer.",
                    "label": 1
                },
                {
                    "sent": "We get a sequence of words, but we use the dictionary too.",
                    "label": 0
                },
                {
                    "sent": "Rewrite that as a sequence of phonemes, and of course, if we've got multiple pronunciations, we have to say which of those pronunciations was was actually used, and then we can combine those two strings in some way.",
                    "label": 0
                },
                {
                    "sent": "Doing some analysis to form a confidence measure.",
                    "label": 1
                },
                {
                    "sent": "There are two ways we could use the phoneme strings.",
                    "label": 0
                },
                {
                    "sent": "We could either simply tag the phonemes with the number of the word that they represent.",
                    "label": 0
                },
                {
                    "sent": "Since we've got the transcription, we can do that, or we can use dynamic programming.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To align the string from the phone recognizer with the string derived from the word recognizer.",
                    "label": 0
                },
                {
                    "sent": "And a very simple distance measure that we can use is really based on a confusion made.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tricks not between the correct and the incorrect decodings, but between the phonemes produced by the phone recognizer an the phonemes produced by the word recognizer so so it looks something like that.",
                    "label": 0
                },
                {
                    "sent": "And if we make the very bad assumption that they are the separate phonemes are independent, then we can multiply those those probabilities so we can add the logs and that gives us a distance measure.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Better distance measure can be found by using a likelihood ratio.",
                    "label": 0
                },
                {
                    "sent": "In this kind of way.",
                    "label": 0
                },
                {
                    "sent": "In this scenario, we actually Form 2 Co occurrence matrices.",
                    "label": 0
                },
                {
                    "sent": "One is a Co occurrence matrix matrix.",
                    "label": 0
                },
                {
                    "sent": "When the words are correctly.",
                    "label": 0
                },
                {
                    "sent": "When a word is correctly decoded by the recognizer and one is a Co occurrence matrix.",
                    "label": 0
                },
                {
                    "sent": "When a word is incorrectly decoded by the by the recognizer and then we can.",
                    "label": 0
                },
                {
                    "sent": "Combine those two in a likelihood ratio, the probability essentially sorry of correct over incorrect in that kind of way and I'll show you the results for these little bit later on.",
                    "label": 0
                },
                {
                    "sent": "So yet another technique?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is where we actually hypothesize words from phone strings.",
                    "label": 1
                },
                {
                    "sent": "So the probability of the word given the acoustics we can.",
                    "label": 0
                },
                {
                    "sent": "Represent.",
                    "label": 0
                },
                {
                    "sent": "In in this way we can.",
                    "label": 0
                },
                {
                    "sent": "We can sum over all the possible phonemes, the probability of word of the word given the phonemes times approach probability of the phonemes given the acoustics and.",
                    "label": 0
                },
                {
                    "sent": "We can approximate that in the standard way by using the most likely phoning sequence P star.",
                    "label": 0
                },
                {
                    "sent": "So the probability of the most likely phoneme sequence given the acoustics is of course by definition estimated from the parallel phoneme recognizer.",
                    "label": 1
                },
                {
                    "sent": "The probability of a word string given the most likely phoneme string is what I want to talk about.",
                    "label": 0
                },
                {
                    "sent": "Two methods that we developed for estimating that.",
                    "label": 0
                },
                {
                    "sent": "Let's list as we call it and metal models.",
                    "label": 0
                },
                {
                    "sent": "So the idea behind the next list.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Was this in fact I think it is the way that people used to do sorry used to do recognition.",
                    "label": 0
                },
                {
                    "sent": "Speech goes into both the phoneme recognizer and the word recognizer.",
                    "label": 0
                },
                {
                    "sent": "The phoneme recognizer hypothesizes a sequence of words.",
                    "label": 0
                },
                {
                    "sent": "And we use that sequence.",
                    "label": 0
                },
                {
                    "sent": "We slide a window over that sequence and from that we can build up lists of words whose profile fits that.",
                    "label": 0
                },
                {
                    "sent": "Sorry which which contain that Mr phonemes within them.",
                    "label": 0
                },
                {
                    "sent": "And we can refine that technique.",
                    "label": 0
                },
                {
                    "sent": "Of course, we know that this string is full of, is full of errors, is full of substitutions, deletions, insertions.",
                    "label": 0
                },
                {
                    "sent": "So we can refine the.",
                    "label": 0
                },
                {
                    "sent": "We can refine the data by using the confusion matrix of the phoneme recognizer, which we've built up a training time to look at different.",
                    "label": 0
                },
                {
                    "sent": "Sequence is there and to build a different lists and you can see the sort of thing that happens.",
                    "label": 0
                },
                {
                    "sent": "You rapidly get very long lists of words building up.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The idea is then that these words are hypothesized by the same language model that goes into the word recognizer, and then we can.",
                    "label": 0
                },
                {
                    "sent": "We can correlate the output of the word recognizer with words in this list, and in this case this is actually the sample or one of the transcriptions of the word.",
                    "label": 0
                },
                {
                    "sent": "Unusually, we see that unusually appears several times in this list, and when we correlate it with the output from the word recognizer.",
                    "label": 0
                },
                {
                    "sent": "That's also produced the word unusually, and so we can use that kind of correlation.",
                    "label": 0
                },
                {
                    "sent": "You can think of several ways in which you could do it to form a confidence measure.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think I'll leave that one, actually.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The difficulty with that technique is there are all sorts of ad hoc decisions that you've got to make about the size of the sliding window you're going to run on the phoneme string about what to do with short words and things like like that, and I think you can see that when you when you add into that the confusion matrix, so every every phoneme has got a probability of being confused with many other phonemes or being inserted, or.",
                    "label": 0
                },
                {
                    "sent": "Delete it, etc.",
                    "label": 0
                },
                {
                    "sent": "There is a combinatorial explosion in the candidate words and the thing gets unmanageable so that technique was abandoned.",
                    "label": 0
                },
                {
                    "sent": "Really, when we saw that that was going to be unworkable and we came up with something which we call meta models.",
                    "label": 0
                },
                {
                    "sent": "And the idea of the meta model is it uses knowledge of folding confusions within an HMM framework to produce these candidate word list that we can then use for confidence measures.",
                    "label": 1
                },
                {
                    "sent": "So here I'll try to explain how the meta models are built.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is a training time.",
                    "label": 0
                },
                {
                    "sent": "We take the speech, we put it into the phoneme, recognize that we get a set of N best transcriptions.",
                    "label": 1
                },
                {
                    "sent": "We compare these with the reference transcription and then for each phoneme in the inventory we build a little hidden Markov model like this and this is.",
                    "label": 0
                },
                {
                    "sent": "This is not a model that represents acoustic observations.",
                    "label": 0
                },
                {
                    "sent": "This is actually a model of the confusion probability of this phony.",
                    "label": 0
                },
                {
                    "sent": "So each state here has got a a discrete set of 45.",
                    "label": 1
                },
                {
                    "sent": "If there are 45 phonemes, it's got a discrete set of.",
                    "label": 0
                },
                {
                    "sent": "45 probabilities associated with it, and the idea is, let's say that this Foley are was always perfectly decoded by our recognizer.",
                    "label": 0
                },
                {
                    "sent": "In that case, all three distributions here would have one for Anna, Zero everywhere else.",
                    "label": 0
                },
                {
                    "sent": "This transition probability that skips the first date would be 1.",
                    "label": 0
                },
                {
                    "sent": "The transition probability that skips the final state will also be one, and so if you think of this thing as a as a generator.",
                    "label": 0
                },
                {
                    "sent": "It would always just generate the phoneme app with probability one.",
                    "label": 1
                },
                {
                    "sent": "These two states.",
                    "label": 0
                },
                {
                    "sent": "This represents an insertion before this is a insertion after and this arc of course is a dilution of the.",
                    "label": 0
                },
                {
                    "sent": "So the idea was to build up models of the confusion of the different phonemes and they trained in just the standard way you train acoustic Hmm's.",
                    "label": 0
                },
                {
                    "sent": "Now when we want to get a confidence.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Made here from a set of those what we do is again we put the speech into the parallel recognizers and the crucial thing is we use the same language model as the word recognizer to decode the output from the meta models, and so that gives us a set of word strings coming out here which actually represent the posterior probability of the words given these noisy strings.",
                    "label": 1
                },
                {
                    "sent": "Sorry this noisy phoneme string.",
                    "label": 0
                },
                {
                    "sent": "It's coming in here, so if we then correlate the word string from the word recognizer with this word string produced by the.",
                    "label": 0
                },
                {
                    "sent": "Meta models we can get a confidence measure.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so here here quickly is some.",
                    "label": 0
                },
                {
                    "sent": "Now move on to the experiments.",
                    "label": 0
                },
                {
                    "sent": "Here's some details about the models and things.",
                    "label": 0
                },
                {
                    "sent": "It's not really very revolution.",
                    "label": 0
                },
                {
                    "sent": "We revolutionary.",
                    "label": 0
                },
                {
                    "sent": "We use WSJ camel database.",
                    "label": 0
                },
                {
                    "sent": "There are 92 speakers, 10,000 sentences, etc around go all through this, but it's you know fairly standard large large vocabulary.",
                    "label": 1
                },
                {
                    "sent": "Recognition stuff.",
                    "label": 0
                },
                {
                    "sent": "For the confidence measures, we used independent training and testing sets, of course.",
                    "label": 1
                },
                {
                    "sent": "There's a niche.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Q About the How you quote the performance of confidence measures.",
                    "label": 0
                },
                {
                    "sent": "What we do is of course we we we set a threshold on the confidence measure.",
                    "label": 0
                },
                {
                    "sent": "We can alter the threshold to get a receiver operating curve, which I'll show you a little later.",
                    "label": 0
                },
                {
                    "sent": "But we use the threshold to tag each decoded word as being either we think correct or incorrect.",
                    "label": 1
                },
                {
                    "sent": "If you think about this for a second, you will realize it.",
                    "label": 0
                },
                {
                    "sent": "If your recognizer is, say, 80% accurate.",
                    "label": 0
                },
                {
                    "sent": "If you just tag every word is correct, your confidence measure is apparently working at 80%.",
                    "label": 0
                },
                {
                    "sent": "But of course it's not, it's just, it's just guessing that's the most.",
                    "label": 0
                },
                {
                    "sent": "That's the most sensible thing you can do.",
                    "label": 0
                },
                {
                    "sent": "So you've got to get the gain over guessing, and there are there are some much more sophisticated ways of measuring that than what I've got here.",
                    "label": 0
                },
                {
                    "sent": "One of them is.",
                    "label": 0
                },
                {
                    "sent": "Normalized cross entropy, but we just use the improvement over guessing, so the guessing rate is the number of errors made by the recognizer of the total number of words from the confidence measure.",
                    "label": 1
                },
                {
                    "sent": "It's the number of correct of incorrectly tagged words over the total words and the gain therefore is the guessing measure minus the baseline divided by the guessing measure.",
                    "label": 0
                },
                {
                    "sent": "Sorry, wrong way round Mr. Forgetting measurements.",
                    "label": 0
                },
                {
                    "sent": "The confidence measure error.",
                    "label": 0
                },
                {
                    "sent": "Over the guessing error.",
                    "label": 0
                },
                {
                    "sent": "Now, the baseline that we used was the so called N best confidence that had been a popular one up to the time that we we we.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Doing this work and it does perform quite well and.",
                    "label": 0
                },
                {
                    "sent": "The way that this works is you get the best transcriptions out of the recognizer and then you align them in some way.",
                    "label": 0
                },
                {
                    "sent": "There are various ways of doing this.",
                    "label": 0
                },
                {
                    "sent": "What I'm showing here is we take the top transcription as being quotes.",
                    "label": 0
                },
                {
                    "sent": "The truth we align all the other transcriptions to thetop transcription using dynamic programming, and then we just count the number of times that a certain word appears in the same place.",
                    "label": 0
                },
                {
                    "sent": "The news that proportion, as our measure of confidence that the word is correctly decoded so you can see here, for instance, can appears as the word in this position four times.",
                    "label": 0
                },
                {
                    "sent": "There are nine decodings, so it's got a confidence of four ninths, etc etc.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here are the results.",
                    "label": 0
                },
                {
                    "sent": "This is the percentage improvement over just guessing and you see that the baseline actually does rather well.",
                    "label": 1
                },
                {
                    "sent": "And actually the phone correlation.",
                    "label": 1
                },
                {
                    "sent": "Whichever way you do it using either lead distance measure or the likelihood ratio just just does not perform really.",
                    "label": 0
                },
                {
                    "sent": "There's just not enough information in those in those strings to be able to to get something out, but the metamodels achieves a considerable performance improvement on the baseline, and we've used that subsequently and is very successful.",
                    "label": 0
                },
                {
                    "sent": "So that was part one of the talk.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about another approach to.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Confidence measures which is using semantic information.",
                    "label": 1
                },
                {
                    "sent": "And those of us who played with speech recognition of all seen how was like this, and they quite often appear in magazines or one of the articles in magazines where this, this kind of power is quoted.",
                    "label": 0
                },
                {
                    "sent": "So this is an actual decoding of a Wall Street Journal sentence from our recognizer.",
                    "label": 0
                },
                {
                    "sent": "It's managed to put the word violin in there.",
                    "label": 0
                },
                {
                    "sent": "And that should have been oil and but it got violin.",
                    "label": 0
                },
                {
                    "sent": "Presumably the bigram violin gas was not in the training set.",
                    "label": 0
                },
                {
                    "sent": "I don't think it would have been.",
                    "label": 0
                },
                {
                    "sent": "Presumably this was an unsuccessful back off to a unigram.",
                    "label": 0
                },
                {
                    "sent": "But the point about something like this is that it's it's very easy to spot that the word violin must be wrong.",
                    "label": 0
                },
                {
                    "sent": "So could one use this as a confidence measure?",
                    "label": 0
                },
                {
                    "sent": "Well probably, but only a small proportion of the incorrect words could could possibly be identified in this way, so that's the downside.",
                    "label": 1
                },
                {
                    "sent": "But the upside is that that that kind of mistake is is likely to be independent.",
                    "label": 0
                },
                {
                    "sent": "Of the sort of measures that are based on the things the inside the decoder.",
                    "label": 0
                },
                {
                    "sent": "So we might be able to combine that rather weak information with some stronger information to get a better confidence measure.",
                    "label": 0
                },
                {
                    "sent": "And that is indeed the case.",
                    "label": 1
                },
                {
                    "sent": "And the nice thing about it is it requires no recognizer side information whatsoever.",
                    "label": 0
                },
                {
                    "sent": "If you could, if you could make this work, just just the one best output would be enough.",
                    "label": 0
                },
                {
                    "sent": "So to see whether this was viable, I actually myself went through about 600 sentences.",
                    "label": 0
                },
                {
                    "sent": "During a new meeting, I think so.",
                    "label": 0
                },
                {
                    "sent": "Not all the EU meetings are completely useless.",
                    "label": 0
                },
                {
                    "sent": "Unmarked any word that I considered to be in.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Correct on grounds of semantics.",
                    "label": 1
                },
                {
                    "sent": "So things like a violin power and then I checked the results against the transcriptions and what I find what I found was there were 67680 plus 49 correct words in the transcription 2720 plus 421 incorrect of the words that were actually correct.",
                    "label": 0
                },
                {
                    "sent": "I'd marked 49 as being incorrect of the words that were actually incorrect.",
                    "label": 0
                },
                {
                    "sent": "I'd marked.",
                    "label": 0
                },
                {
                    "sent": "421 sorry, 421 is being incorrect.",
                    "label": 1
                },
                {
                    "sent": "So of the 470 words that that were marked as incorrect.",
                    "label": 0
                },
                {
                    "sent": "Sorry of the total of 3000 words that were incorrect.",
                    "label": 0
                },
                {
                    "sent": "I'd only managed to spot using this technique 470 so we could call that the recall in information retrieval terms, and that's pretty low.",
                    "label": 1
                },
                {
                    "sent": "However, the precision is quite high because of those 470 words are marked.",
                    "label": 1
                },
                {
                    "sent": "Actually, 90% of them were incorrect, so.",
                    "label": 0
                },
                {
                    "sent": "It appeared that there was something there, but only at low recall.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we what we need then, if we're going to use something like this, is a way of identifying words that are semantically distant from the other decoded words in the sentence.",
                    "label": 1
                },
                {
                    "sent": "Just as the violin was in that example sentence.",
                    "label": 0
                },
                {
                    "sent": "There's been lots of work on clustering words, of course, but there is a problem of data sparsity, and there's also quite interesting problem.",
                    "label": 0
                },
                {
                    "sent": "Many semantically close word pairs actually rarely Co occur together.",
                    "label": 0
                },
                {
                    "sent": "For instance, take words movie and film, which.",
                    "label": 0
                },
                {
                    "sent": "All synonyms, but which would really be in the same text side by side.",
                    "label": 0
                },
                {
                    "sent": "You would use one or the other or say striker and batsman Witcher related semantically, but they're in games played in different parts of the world.",
                    "label": 0
                },
                {
                    "sent": "So clustering is not really a very strong candidate.",
                    "label": 1
                },
                {
                    "sent": "Latent semantic analysis is a technique that's been successfully used to associate semantically similar words.",
                    "label": 0
                },
                {
                    "sent": "Just give just briefly rundown how it works.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What windows is to former Co occurrence matrix where the rows of the matrix are.",
                    "label": 0
                },
                {
                    "sent": "The other words that occur in the set of documents that you've gotten, we we assume that the documents are semantically coherent in some way that they represent the same topic.",
                    "label": 0
                },
                {
                    "sent": "And that, of course, is a huge and very sparse matrix, and the way that we deal with that matrix is to use singular value D.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Position on it.",
                    "label": 0
                },
                {
                    "sent": "So we start with this huge matrix W and we decompose it as an orthogonal matrix.",
                    "label": 0
                },
                {
                    "sent": "U turns a matrix of eigenvalues S times an orthogonal matrix V transpose.",
                    "label": 0
                },
                {
                    "sent": "And in fact that decomposition will be exact when this internal dimension are actually matches the external dimension end there.",
                    "label": 0
                },
                {
                    "sent": "So the idea of this is to represent the words in the subspace where they are semantically sorry from a semantical point of view, they they are closer and so we moved down from a matrix that about 20,000 words by 20,000 documents to represent the words in 100 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The Wall Street Journal corpus, again for the.",
                    "label": 0
                },
                {
                    "sent": "For these experiments we use the paragraphs of the from the corpus as the documents.",
                    "label": 0
                },
                {
                    "sent": "The paragraphs are fairly much semantically coherent when you look at them.",
                    "label": 0
                },
                {
                    "sent": "About 20,000 documents and there also about 20,000 different words, and after some experimentation with this internal space size, the size of about 100 seems to be seems to give about the best performance, which maybe says says that there are about 100 quotes.",
                    "label": 0
                },
                {
                    "sent": "Different concepts in that in the Wall Street Journal Corpus we used.",
                    "label": 1
                },
                {
                    "sent": "And we compute we.",
                    "label": 0
                },
                {
                    "sent": "So we project the words into this subspace and then we compute the semantic similarity, just as the dot product of the vectors representing the words just like that.",
                    "label": 1
                },
                {
                    "sent": "Panda is quite interesting to look at the distribution.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Use of the scores for for the 20,000 words in the in the vocabulary.",
                    "label": 0
                },
                {
                    "sent": "If you rank the words by their mean by their mean score, then the word that comes out top is the word out and you can see probably can't see from the back, but that is a score of zero there.",
                    "label": 0
                },
                {
                    "sent": "That's a score of 1 is this is a cosine distance.",
                    "label": 0
                },
                {
                    "sent": "The range is actually minus one 2.",
                    "label": 0
                },
                {
                    "sent": "To one you see the word out has a huge range of scores, so that indicating that it occurs with a lot of different words.",
                    "label": 0
                },
                {
                    "sent": "And then as you go down the down the ranking the the school starts to move in towards 0 so you get fewer and fewer high scores because you're getting words which only occur with well which occur infrequently anyway.",
                    "label": 0
                },
                {
                    "sent": "But when they do occur.",
                    "label": 0
                },
                {
                    "sent": "Only occur with certain other words, and so their average score is close to 0 and that's the bottom word denomination, and you see it's the scores are very tightly clustered around zero there.",
                    "label": 0
                },
                {
                    "sent": "So how do we use all that to make a config?",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This measure.",
                    "label": 0
                },
                {
                    "sent": "But there are several things you could sensibly do with it.",
                    "label": 0
                },
                {
                    "sent": "Just look at the mean score of the each word to the other decoded words.",
                    "label": 1
                },
                {
                    "sent": "Try making a bit more robust, perhaps by looking at the rank.",
                    "label": 0
                },
                {
                    "sent": "The most successful thing by just a little bit was actually to model those distributions.",
                    "label": 0
                },
                {
                    "sent": "I showed you as a mixture of Gaussians and compute the computer probability of the similarity of a word to the to the other decoded words.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's quite important to use a stop list here so function words like to and often out and so on.",
                    "label": 1
                },
                {
                    "sent": "These occur with most words, so they just.",
                    "label": 0
                },
                {
                    "sent": "They just contribute noise and so we just get rid of those.",
                    "label": 0
                },
                {
                    "sent": "So, so we set some threshold and we say, well, a word that's gotta semantic score more than some threshold is is limited.",
                    "label": 0
                },
                {
                    "sent": "There is a slight problem with that.",
                    "label": 0
                },
                {
                    "sent": "As soon as you do that then the baseline performance of the recognizer increases, so you have to take that account in the results.",
                    "label": 0
                },
                {
                    "sent": "His, um, the distribution of.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One of the measures we use this is the one where we modeled the distributions of the semantic scores as a mixture of Gaussians.",
                    "label": 0
                },
                {
                    "sent": "And on top here you can see the distribution of the scores from the correctly decoded words and on the on the bottom you can see the scores from the incorrectly.",
                    "label": 1
                },
                {
                    "sent": "Under this was a bit of a surprise when I saw that when I first service, because what I've been expecting to happen or what was that lead for the incorrectly decoded words, there'd be a big peak here of words that had no sorry, that had a very low semantic distance to the other words because they had been incorrectly decoded, but in fact, when you look at the details of the two distributions.",
                    "label": 0
                },
                {
                    "sent": "There's very little in it.",
                    "label": 0
                },
                {
                    "sent": "Um, but there's a large, large difference at the top of the of the graph there so, so that's the.",
                    "label": 1
                },
                {
                    "sent": "Passwords which are very close have a high summer, have a high score.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is just what I'm saying.",
                    "label": 0
                },
                {
                    "sent": "This technique derives seems to derive what discrimination is passed by identifying not the incorrectly decoded words, but the correctly decoded words.",
                    "label": 1
                },
                {
                    "sent": "But when we looked at what it was doing, it was actually it's actually finding words that were words like numbers in financial terms that were highly cognate with each other, and so they had a nice high score to each other, and so it was able to discriminate and quite nicely.",
                    "label": 0
                },
                {
                    "sent": "When we looked.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Other words at the bottom actually a lot of them were common words that had been correctly decoded, and yet they had a very low score, and so we just came to the conclusion that when you've got these fairly common words, there's just not enough training material to capture all the Co occurrences with the other words, and so you will get these effects.",
                    "label": 1
                },
                {
                    "sent": "There were actually, of course, out of vocabulary words as well in there.",
                    "label": 0
                },
                {
                    "sent": "So, um, just look at the performance on a.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Receiver operating curve.",
                    "label": 0
                },
                {
                    "sent": "So there are actually 3 here so.",
                    "label": 0
                },
                {
                    "sent": "The semantic confidence measure is the squares here, so this is now just the straight correct recall against the straight precision.",
                    "label": 0
                },
                {
                    "sent": "The baseline.",
                    "label": 0
                },
                {
                    "sent": "The end best recognizer.",
                    "label": 0
                },
                {
                    "sent": "Sorry, the N best confidence measure is here.",
                    "label": 0
                },
                {
                    "sent": "Now this is clearly sorry.",
                    "label": 0
                },
                {
                    "sent": "This is recall and precision, so we would like to be as far up to the top right corner as we could be.",
                    "label": 0
                },
                {
                    "sent": "That would be the ideal system.",
                    "label": 0
                },
                {
                    "sent": "So this is clearly well superior to the semantic confidence measure.",
                    "label": 0
                },
                {
                    "sent": "But one one point you should notice is this outlier points on the left here, and this is a well known phenomenon with N best.",
                    "label": 0
                },
                {
                    "sent": "This occurs because you when when you use the N best confidence measure, there are always several words which are incorrectly decoded but which appear in every single decoding.",
                    "label": 0
                },
                {
                    "sent": "So they give you a confidence of 1 even though they're incorrectly decoded and that leads to this.",
                    "label": 0
                },
                {
                    "sent": "Drop off in performance here.",
                    "label": 0
                },
                {
                    "sent": "Now when we combined the N best with the semantic confidence measure, we get the asterisks so it's nowhere any worse.",
                    "label": 0
                },
                {
                    "sent": "And for this important region where you've got rather low recall but you're interested in, so you've got a small number of words, but you want to say with high confidence whether they're correct or not.",
                    "label": 0
                },
                {
                    "sent": "This this does rather better.",
                    "label": 0
                },
                {
                    "sent": "So on its own, it's quite a weak indicator of confidence.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But it's useful when used with something else.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the final slide, So what I've said here is I've been talking about techniques for identifying incorrect words in the output of the speech recognizer that are not recognized as specific that don't don't depend on side information and by far the most successful.",
                    "label": 1
                },
                {
                    "sent": "These is the metamodels technique.",
                    "label": 1
                },
                {
                    "sent": "The idea here is to form models of the.",
                    "label": 0
                },
                {
                    "sent": "Of the confusion of phonemes and to use these to generate words which are correlated with the word recognizer.",
                    "label": 0
                },
                {
                    "sent": "Using some semantic information is a nice a nice idea, but it's the amount of the number of words that can be recognized as being correct or incorrect using this ways rather is rather limited, but it can be combined with the N best to to give some increase in performance.",
                    "label": 0
                },
                {
                    "sent": "And finally I haven't said anything, of course about the performance of these.",
                    "label": 1
                },
                {
                    "sent": "Of these measures in real systems, but of course leave the real assessment of them comes when you use them in a real system to see how well how much they improve your dialogue system.",
                    "label": 0
                },
                {
                    "sent": "And that's it.",
                    "label": 0
                },
                {
                    "sent": "So we have time for some questions.",
                    "label": 0
                },
                {
                    "sent": "Anybody has one?",
                    "label": 0
                },
                {
                    "sent": "You replaced.",
                    "label": 0
                },
                {
                    "sent": "Guys over the phone recognizer.",
                    "label": 0
                },
                {
                    "sent": "Did he look if the confidence called computation now?",
                    "label": 0
                },
                {
                    "sent": "Oh, how much depended on the specifics of the phone recognizer.",
                    "label": 0
                },
                {
                    "sent": "Well, I think the point about a phone recognizer is, on the whole it's you know they are, they are.",
                    "label": 0
                },
                {
                    "sent": "They are pretty vanilla things.",
                    "label": 0
                },
                {
                    "sent": "You generally have a fun a tactic model in there.",
                    "label": 0
                },
                {
                    "sent": "Course you can have different number of mixture components and things like that but but they are less variable.",
                    "label": 0
                },
                {
                    "sent": "I would claim the word recognizes.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "So my question which is?",
                    "label": 0
                },
                {
                    "sent": "Using the site information and the semantic information you have, you're in effect building a different model for your confidence measure.",
                    "label": 0
                },
                {
                    "sent": "So do you think it's possible to close the loop and actually use that information to get better speech recognizer?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean, I suppose I suppose some people would claim that you shouldn't have to use confidence measures at all.",
                    "label": 0
                },
                {
                    "sent": "You should.",
                    "label": 0
                },
                {
                    "sent": "You should just be working on getting a better speech recognizer.",
                    "label": 0
                },
                {
                    "sent": "But yes, there have been attempts, haven't they?",
                    "label": 0
                },
                {
                    "sent": "To integrate semantic information into into recognizes, but I think it is.",
                    "label": 0
                },
                {
                    "sent": "It's quite tricky to put it into a language model.",
                    "label": 0
                },
                {
                    "sent": "Approach.",
                    "label": 0
                },
                {
                    "sent": "So if you set up a cheap scription, sorry, I couldn't get along.",
                    "label": 0
                },
                {
                    "sent": "Did you try to use of alteration?",
                    "label": 0
                },
                {
                    "sent": "Motor 2 no?",
                    "label": 0
                },
                {
                    "sent": "Um, we didn't use that, but then of course all the sentence is were from century the same domain.",
                    "label": 0
                },
                {
                    "sent": "What we were, what we were trying to pick out because they were from Wall Street Journal.",
                    "label": 0
                },
                {
                    "sent": "What we were trying to pick out was asleep until the finals in that, but I suppose it would have been possible.",
                    "label": 0
                },
                {
                    "sent": "Ouch.",
                    "label": 0
                }
            ]
        }
    }
}