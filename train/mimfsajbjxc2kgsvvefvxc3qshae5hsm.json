{
    "id": "mimfsajbjxc2kgsvvefvxc3qshae5hsm",
    "title": "Kernel-based predictive modelling of drug-protein binding affinities",
    "info": {
        "author": [
            "Anna Cichonska, Aalto University"
        ],
        "published": "June 28, 2019",
        "recorded": "May 2019",
        "category": [
            "Top->Computer Science->Big Data",
            "Top->Computer Science",
            "Top->Data Science"
        ]
    },
    "url": "http://videolectures.net/icgeb_cichonska_protein_binding_affinities/",
    "segmentation": [
        [
            "Hello everyone and today I would like to tell you something about the using kernels for the prediction of binding affinity's between drugs and proteins.",
            "But first of all I would like to ask you questions.",
            "So how many of you have worked with kernels before?",
            "Is there anyone who has some experience?",
            "OK, but not too many, so I'm glad that I have like quite some introduction slides so that we can actually like going through that.",
            "And hopefully you will then like know why, why kernels can be nice."
        ],
        [
            "So first of all, we all know that drug achieves therapeutic effect through modulating molecules in the human body, such as proteins, metabolites or DNA and RNA.",
            "But the most common targets of drugs are proteins, so that's why we will focus on proteins here.",
            "And it is actually not surprising because proteins, as we all know, have many important functions and they are also involved in many disease processes."
        ],
        [
            "So one example of drug protein interaction in the cancer treatment involves a drug imatinib which has revolutionized the treatment of leukemia patients, so imatinib interacts with and specifically blocks the BCR ABL, which is a Fusion protein that causes chronic Milo."
        ],
        [
            "Leukemia.",
            "But drug action is of course way more complicated process, so we usually a drug interacts with more than this single protein.",
            "That is its intended target.",
            "Like BCR ABL in case of pneumatic.",
            "So other proteins that the drug interacts with are called off targets and this off target interactions can be either neutral.",
            "But they could also be harmful or beneficial.",
            "So for example, imatinib can cause cardiotoxic side effects because unintentional it also inhibit C ABL protein.",
            "But in some other cases of targets of targets, can or the knowledge of those targets can help us to identify the news of a drug?",
            "And in this case, using again Dean Martin example, it turned out that the unexpected interaction of imatinib with the kit protein is beneficial for the treatment of gastrointestinal cancers.",
            "So."
        ],
        [
            "This example shows us that in order to design effective and safe therapies, we ideally should know the interactions of a drug or a drug candidate with all the proteins in the human body.",
            "But the problem is that the determining those drug protein interactions in the lab is still quite time consuming and expensive.",
            "Even if we use the modern high throughput technologies."
        ],
        [
            "And Furthermore, the universe of all possible chemical molecules that either already exist or could be designed is really enormous.",
            "And of course, not all the chemicals could be potential drugs because they need to have like so called so called good pharmacological properties.",
            "For instance those that are released in the lipinski's rule of five.",
            "But it turns out that even if we would narrow this enormous space of all chemical compounds only to those compounds that have good pharmacological properties, it is estimated that this universe of potential drug candidates.",
            "It's still really huge."
        ],
        [
            "So this way machine learning comes in handy and probably even with machine learning we cannot right now explore this like 10 to the power of 24 drugs.",
            "But of course we can.",
            "We can like at least use it to get some help in this process.",
            "So basically the idea here is not to replace the experiments in the lab with machine learning methods, But instead this methods could prioritize the most voted target interactions and drug candidates for further evaluation in the lab."
        ],
        [
            "And different machine learning methods could be broadly categorized as drug based, protein based and systems based.",
            "That's just like one possible classification, so the drug based methods they are actually the most traditional ones in machine learning, and they typically aim to relate this structural properties of the drugs to thereby activity profiles, and they are known traditionally under disk user name.",
            "So quantitative structure, activity relationship models, and probably all of you have heard about dogs.",
            "Then we could also think of protein based methods where we use the available by activity data and the protein information instead of the drug information, But these methods are not as popular and then systems based methods that are also known as proteo chemometric models or pairwise models depends on like what kind of background do you come from.",
            "So those systems based methods they unify the other two frameworks because they take advantage of the properties of both drugs and proteins.",
            "And the general assumption is that similar drugs are likely to interact with similar proteins.",
            "And this will be our focus here, focusing on the systems based or like pairwise models.",
            "So generally the goal is to find the relationship between chemical and genomic profiles and drug by activities."
        ],
        [
            "And here is the high level overview of different components of this systems based framework.",
            "So we have a space of drugs, space of protein targets and some known interactions between them that we can use for training the models.",
            "So for instance the link between imatinib and BCR ABL would be there and then we also have similarities.",
            "We also have some information, additional information about both drugs and proteins that can help us to establish this.",
            "Disconnections between the same types of molecules that are marked with the dotted lines.",
            "In the scheme and similarities between molecules can be defined by lots of different means and we will of course go through that.",
            "Now in the lecture in more detail.",
            "So given all that, we can train a classifier or regressor and then predict some novel drug protein binding affinity's and potentially find some novel interesting interactions.",
            "So the classification setup is, I would say it's still the most common in this problem it seems, but regression is more more realistic and also like more exciting.",
            "I would say because we actually want to know the full activity spectrum over drug rather than the binary profile of like what it could potentially interact with because it's quite difficult to just like set a single binding affinity threshold to decide what is an interaction and what is not, what is not."
        ],
        [
            "So the high level assumption I mentioned earlier is that similar drugs are likely to interact with similar proteins, and the similarities between molecules can be actually encoded using kernel functions.",
            "So here we're kernels come in."
        ],
        [
            "So kernels why they are nice?",
            "They offer several useful properties.",
            "So first of all, kernel function embeds the input data into a new feature space with ever high number of dimensions.",
            "Of course, this is only free because I couldn't we paint more dimensions here, but but technically it's just like a very large.",
            "It could be very large, normal dimensions, even infinite dimensional space 8 and now a linear model that we can learn in this space will correspond to a nonlinear model in the original space.",
            "So this basically means that thanks to kernels we can look for nonlinear patterns in the data, but we can use very well established linear learning algorithms to do so.",
            "And what is important is that this whole process can be actually performed in a very efficient manner, because we don't need to explicitly compute this mapping and evaluate the coordinates in this like high dimensional feature space.",
            "So the kernel can be calculated using the original features, such as.",
            "Here it would be X.",
            "So for instance, then chemical properties of a drug and we do it by replacing the inner product in this definition with a proper properly chosen kernel function and the Gaussian kernel would be one example.",
            "So it will imply that they are there, that there is this underlying high dimensional feature representation X, but we actually sorry 8 high dimensional representation age, but we don't need to compute it.",
            "We don't actually need to explicitly move to this space.",
            "So for instance, in case of the Goshen kernel, this underlying space age is of infinite dimension because it contains all possible monomials of input features.",
            "But then again, the advantage we don't need to actually like deal with this with this Space 8."
        ],
        [
            "So here is a simple example.",
            "So we have that 2 dimensional input space and then a feature map that takes the data from from 2D to 3D space and it is done in such a way that the linear relations in this new feature space will correspond to the quadratic relations in the in the input space.",
            "So here we have the kernel definition, the inner product that we saw in the previous slide and we just evaluate.",
            "We just plug in our feature map and we evaluate this inner product and basically we end up with the kernel function that looks like that, which is just the inner product between the original data that we had originally X and set.",
            "And then we just square it.",
            "So this implies that there is this underlying 3D space, but we don't need to actually evaluate the coordinates in this space."
        ],
        [
            "And here is the same just just as an example with the toy data.",
            "So for instance, imagine that you would like to separate now the red points and this blue stars using linear function and you cannot do it here in the 2D space using a linear function.",
            "But then we are using the feature map that we just we've just seen and this actually gives you this kind of space where you can see you can quite easily separate the points using a linear separating hyperplane.",
            "But the advantage is that we don't need to actually.",
            "Evaluate the coordinates in the fridge space and of course, remember that this is very simple and we don't mind going into this free space because it's very very simple in this example.",
            "But then in reality would have like way higher dimensional spaces, and that's why that's an advantage that we don't have to explicitly move to 28.",
            "But we can get the advantage of having linear nonlinear relations in the in the original space."
        ],
        [
            "And kernels have also many other useful properties, so one of them is that often in machine learning we have this problem that we have way more instances where sorry way more features than the that the instances or samples that they describe.",
            "So for instance we could have some number of drugs.",
            "And, well, maybe we don't have that many chemical properties seriously, but if you think of gene expression profile or some other data in bioinformatics so often you have like millions of features even but, but not as many samples.",
            "And with kernels it's not a problem because the data actually appears only through the entries in the kernel matrix, which relates or pairs of samples.",
            "So for instance, here drugs in this in this example.",
            "And kernels are especially well suited for representing structured objects such as images or molecules, because they don't always have, just like natural, natural natural, like vectorized representation.",
            "So there are like some methods for comparing images directly, or the mass Spectra or like graphs and so on.",
            "So that could be very convenient when you have when you have this kind of structured objects.",
            "Enter, one important thing is to remember here.",
            "Kernel can be considered as a similarity measure between your inputs.",
            "So here drugs so also not not every similarity measure is a valid kernel function.",
            "It doesn't work the other way around because kernel needs to be a positive semidefinite function and so on.",
            "But once you have a kernel, once you have a valid kernel, you can always interpret it as a similarity measure."
        ],
        [
            "So we will now have a look at each one of these components and first of all we were start with the labels.",
            "So just briefly like where could we find information about the binding affinity's between different drugs and proteins to train the models?"
        ],
        [
            "So there are of course lots of different databases.",
            "Campbell is like one of the maybe best available data resources, so their data are manually extracted from the literature and the current version contains almost 2 million information, about 2 million compounds, and this data is extracted from over 70,000 different publications, then another quite well known database would be popped in and it is way larger because their data are not manually extracted, but data generators can deposit their data.",
            "And it contains right now, around 98 million compounds.",
            "But there you have to be more careful, because since it's not curated so kind of maybe you should like do some kind of curation yourself.",
            "And then an interesting data resource, drug target Commons DTC that was established quite recently in Finland actually.",
            "So it is a community effort to curate and notate, and also harmonized by activity data that are existing in other databases.",
            "So as I mentioned, for instance with popcorn that like, you should be like careful with some data you might get.",
            "If you are like not absolutely sure that they were curated, and then there are lots of different databases with lots of different informations about drug target interactions, but often like you will not know, can you actually put them together like how different the different assets were that generated the data.",
            "So like the high level aim with the drug, target Commons is to actually accomplish this, kind of like standardization, harmonization of the data.",
            "So it would be easier to get like the data you actually want.",
            "And be sure about their quality.",
            "Like when you train your models.",
            "So right now DTC contains roughly 2 million compounds and most of them are not all but like white men are coming from Campbell, but then with some like additional assay annotations."
        ],
        [
            "So now we will have a look at kernels and we will start with the drag kernels."
        ],
        [
            "So I think you already have heard this yesterday, at least that we can encode a structure of a drug into a vector using the concept of a molecular fingerprint.",
            "So fingerprint is a vector where each bit represents the presence or absence of a specific sub structure in the molecule.",
            "As you can see in this very simple example and the binary fingerprints are most common, so exactly the ones like here just way longer.",
            "We have like more substructures, but there are also versions of the fingerprints that are quantitative.",
            "Where instead of just like marking that is then substructure present or not, you actually count how many times it is present and you input the count instead of this binary indicator.",
            "If it's present more than once."
        ],
        [
            "So there are lots of different types of fingerprints and the most well known ones are the dictionary based fingerprints where we actually have a predefined fragments and then each one of them Maps to a single bit in the fingerprint vector.",
            "But then we have also hashed fingerprints where we actually don't use any kind of dictionary and the fragments are generated algorithmically.",
            "So basically we could for instance consider like a certain fingerprint could consider all kinds of paths up to a predefined number of nine.",
            "900 non hydrogen atoms, for instance, starting from the source Atom.",
            "So we would consider all kinds of linear fragments.",
            "And then another class, also of the Hearst fingerprints would be done instead of looking at the linear patterns like the paths you could actually consider the neighborhood like the environment of an Atom, so those would be circular circular fingerprints that are working quite well and are very popular.",
            "So there we measure the radius in bonds and we can set this radius to or or tune it idea, little certain radius that we think is the best for for achieving the optimal predictive performance.",
            "So this fingerprints were based on the 2 dimensional substructures of the molecules, but we."
        ],
        [
            "And also design fingerprints based on the 3D substructures.",
            "So for instance we could consider the geometric features such as triplets of atoms at a certain distance or torsion angles, for instance between atoms.",
            "And then once we have the fingerprints or like a certain fingerprint that we would have chosen to war."
        ],
        [
            "Quiz.",
            "We can compare the fingerprints using Tanimoto kernel and this allows us to determine the similarity between the two molecules.",
            "So it is based on the size of common substructures of the molecules represented by their fingerprints and you have we have seen it for instance yesterday, but maybe not all of you have known that this is actually a valid kernel function.",
            "So most of you probably know that this is a similarity measure, but there is a proof in the literature that this is a valid kernel function.",
            "And another way of computing the similarity kind of similar, but would be that instead of looking at this size of common substructures, we could compare the volumes and kind of check what is the common volume that is not that popular.",
            "But there are like couple of softwares where you can find this implementation as well."
        ],
        [
            "So this was based on the fingerprint, but there are also other ways of how we can compare our chemical compounds, so graph kernels would be quite natural representation because it's like when you look at this example here you can see that actually a chemical structure.",
            "It naturally looks like a graph and it is a very natural representation for it.",
            "So basically each Atom represents a note, and an edge indicates a bond between the two atoms, and you can either have a labeled graph or unlabeled one.",
            "So if you have a labeled one, you could.",
            "Include the Atom type, so is it oxygen or nitrogen?",
            "As the note labor, you could have some additional properties as well, and there are actually lots of different drug kernels.",
            "We will just look at a single example here.",
            "We just like probably like not the best one, but just to explain and show you an idea of like what are we comparing.",
            "So I would like to show you a random walk kernel.",
            "As an example here."
        ],
        [
            "But first, some like.",
            "Let's recall some basic concepts about graphs.",
            "We don't need to manage.",
            "So basically a graph G is a set of nodes or vertices V and edges E, and then we have the adjacency matrix which is indexed by the notes.",
            "And if two nodes in a graph are connected by an edge, then the corresponding entry in this adjacency matrix is set to 1."
        ],
        [
            "And the walk is defined as a sequence of notes that allows repetitions of notes.",
            "And the work can end on the same note on which it began, or on a different note.",
            "So now if we want to know what is the number of walks of a certain length length K that exist between all pairs of nodes, it can be computed from the adjacency matrix of the graph by taking it to the power of K, where K was like the length of the walk that we are interested in."
        ],
        [
            "So now the random walk red graph kernel.",
            "What does it do?",
            "It computes the number of all pairs of matching walks in a pair of graphs.",
            "So this could be quite difficult to do as such, but there are some tricks of like that help us to calculate this efficiently.",
            "So basically what we can do, we can use the concept of the product graphs of the product graph DX and to explain it I will use this example here.",
            "So we have two graphs, G1 and G2 and their product graph GX.",
            "And now you can see that they each node in GX has a label that consists of two components and the first one, the first component.",
            "Always comes from the Note label from the D1 from the first graph and then the second part of the label.",
            "Index comes from the second graph from G2.",
            "So for instance, here you can see that notes one and one prime and free and two prime are adjacent in this product graph GX and this is because there exists an edge between nodes one and three in the first graph and one prime and two prime in the second graph.",
            "So that's how the product graph is constructed by going through all this, all these connections.",
            "So now the key is that performing a random walk on the product graph GX, it corresponds to performing a simultaneous random walk on the G1 and G2, so that's why it allows us to compare how similar molecules are in terms of this random walks that we can do because you can do it like kind of simultaneously just by using the product graph.",
            "And then the number of matching walks of an electric length can be calculated from the adjacency matrix of the product graph.",
            "So AX.",
            "And there are many other graph kernels.",
            "There were sign clue.",
            "Did the reference there.",
            "If you are.",
            "If you are interested, but this is this is just one example to give you an idea.",
            "You could compare the graphs also by looking the common subtrees.",
            "For instance common paths and so on."
        ],
        [
            "So in practice, in many cases using structural information about compounds, it works quite well in predicting drug target interactions, but often it might not be enough, like in cases where we have some some structures that look very similar but at just a minor change in the structure can cause a dramatic change in the activity.",
            "So this is 1 example.",
            "For instance morphine and heroin.",
            "They are very structurally similar, but drugbank lists for targets for both and three are common, but there's one target that is actually different.",
            "Even though the structures look so similar and this is the case with many different drugs, not just this."
        ],
        [
            "So that's why it would be worth also to think of some additional data sources and that we could that we could use to compute the similarities between our compounds.",
            "So for instance, using the common side effects to compare to molecules, we could also take advantage of this ATC classification system, which basically divides the drugs into groups at 5 different levels, and this depends on the organ or system on which the drug acts, as well as therapeutic and chemical characteristics, so it's kind of like maybe like gene ontologies for.",
            "Protein is not exactly but some kind of system like this for drugs.",
            "Another interesting data resource would be gene expression responses to drugs so we could calculate the correlation of these different gene expression responses or apply a Goshen kernel on top of that.",
            "And for instance, connectivity MAP project is 1 example of the resource where this kind of data could be extracted from.",
            "And also what I find very powerful are like actually using the binding affinity profiles of different drugs and the only disadvantage with that of course like in many cases we will not have this kind of data available.",
            "We might be interested in some drugs that are like very new molecules and we cannot take advantage of this resource.",
            "But I mean once it's available, if you are like working with some molecules acquire well known and maybe kind of like fill in some missing entries.",
            "So that's quite powerful for calculating similarities because then even if you have this like.",
            "Minor structural differences that cause very big changes to the activity.",
            "This will kind of handle this problem because it will directly look at the binding affinity profiles, so directly add activities and here you could either like compared to binary profiles, or like actually like looking at the binding affinity's.",
            "So."
        ],
        [
            "Now we will move on to some examples of protein kernels."
        ],
        [
            "So protein is built of amino acids, so the most natural way to compare proteins is of course just to compare the strings of amino acids and the standard approach in case of drug protein interaction prediction is to do the Smith Waterman local alignment and this is not a valid kernel actually.",
            "Actually even though in some publications you might kind of say this kernels, but just for you to know that it's like as such it's not a valid kernel function, but of course we could like apply a kernel function on top of this, calculated Smith, Waterman.",
            "Horse"
        ],
        [
            "But this was like very like standard approach.",
            "My like one of my or my favorite, probably protein Colonel is this generic string kernel.",
            "I haven't developed it.",
            "I have nothing to do with it, so it's not the reason why it's my favorite, but So what?",
            "Is kernel is the wink.",
            "It compares the protein sequences, but accounting for amino acid properties and it also has this nice property that it would allow you to March 2 amino acid sub subsequences even if their positions in the full protein sequence differ quite a lot.",
            "So it is kind of like an advanced alignment.",
            "So DS Kernel will compare each substring of protein X of size L with each substring of ex prime having the same length.",
            "And we need to decide what is the maximum substring length that we are comparing.",
            "Which is this capital L. And this is just the hyperparameter that we need to tune.",
            "So each comparison will result in a score that will depend on the shifting contribution term, which is basically just the difference in the position of two substrings into amino acid sequences to proteins, annex annex prime.",
            "So basically this inj are the indices like kind of indicating the position you are just comparing, like how different they are they are.",
            "And basically then we have.",
            "Then we have another hyperparameter here that we need to tune the Sigma P and the Sigma Phi.",
            "What it is doing it basically if you if you set it to a large value then basically you allow longer shifts.",
            "So we are kind of like saying that like maybe the two subsequences are like very far apart but you really generally want to match short subsequences like regardless of their position.",
            "So generally would like to set it to a very high value.",
            "But of course in general we don't really know.",
            "So we like we need to tune this hyperparameters just the intuition behind it like how changing this parameter.",
            "Allows you to control like world will be considered similar or dissimilar advent.",
            "And then the second term is then basically just the difference between properties of amino acids that are included in the two substrings that you are currently comparing.",
            "And this is controlled by the Sigma C parameter and basically the intuition behind it is very similar that we are just like changing this value of this parameter to control what we will consider to be similar and what we consider to be this similar at the end kind of and we of course need to tune it again.",
            "So the kernel then at the end it the equation is kind of like long maybe and complicated, but basically a dent.",
            "The kernel just outputs the sum of this course from all the possible substrings comparison, so it will compare all possible substrings and it will check the difference in their positions in the full sequence.",
            "It will check how different their amino acid properties are, depending you can choose what kind of properties you want to use and then at the end.",
            "Once like all kind of substrings were compared, then you just like have a sum of all of them.",
            "So the shifting contribution term, like we already kind of went through, is just the difference in the positions.",
            "But maybe we can have a look at what actually we have in the second term.",
            "So the idea is just that each type of amino acid, for instance alanine, it has a corresponding feature vector which defines its properties.",
            "So we could consider, let's say hydrophobicity, volume, polarity.",
            "Like anything else you can find, for instance this AA index database.",
            "It contains like hundreds of or thousands.",
            "Even like different properties of amino acids."
        ],
        [
            "And then once you have a string, this fee deci El that is in the equation above in the GS kernel, it is encoding function that concatenates L vectors that are."
        ],
        [
            "Scribing each amino acid this string is composed of.",
            "So you're basically like once you have the properties this function, it concatenates it into a vector and then that's what's comparative then."
        ],
        [
            "And then again, like I mean, acid sequences are of course I'm nice resource, but there are lots of other information that we should consider.",
            "So especially like protein binding site and protein binding sites and protein surfaces are quite important in the context of this interactions.",
            "So for instance, you could think about like for instance, kinase inhibitors, so most of them binds to ATP binding pockets of the proteins, so it might not be really important that overall two amino acid sequences or structures are like very similar.",
            "If, for instance, there ATP binding pocket subsequences like or dislike, if you compare structures like in Freddy.",
            "If if they are different than actually most likely the outcome the binding of a certain drug will be quite different as well, so that's why like focusing on the for instance the binding sites or just subsequences of the binding sites and even using some simple kernel like the Smith Waterman or like like using Smith Waterman to calculate the kernel.",
            "It could also be a good idea.",
            "And we could, for instance, calculate also similarities using Gene Ontology annotations of the of the proteins, and as such I don't expect it to be like very predictive alone, but in combination with other data sources it can add some orthogonal information."
        ],
        [
            "So now we will have a look at the algorithms.",
            "So we have a couple of very basic slides, so if you have machine learning background then just please bear with me but just sounded like we're on."
        ],
        [
            "Same same page, so drug, protein binding affinity prediction is a supervised regression problem and the notation we will be using is that X is a space of inputs, so our drug protein purse.",
            "Why is there is a space of output so binding of an interest in this case, then we have a set of models G mapping input to output some training data set which we markers as here and the loss function that measures.",
            "What is the difference between the outputs that are predicted by the model?",
            "And the original output, so the binding affinity is here.",
            "So the goal here is to find such a model gene that minimizes the expected loss on future drug protein pairs."
        ],
        [
            "But if we would rely on minimizing the loss only, it could lead to overfitting to the particular training data."
        ],
        [
            "So for instance, you can see here do you want fits almost perfectly to this toy training data, but it describes the noise instead of like the actual underlying relationship.",
            "And on that."
        ],
        [
            "Hand due to it would have a higher associated loss, but it is simpler, and it generalizes better too."
        ],
        [
            "The future observations that we will get and this is generally the ultimate goal of machine learning.",
            "So what do we do to avoid the over?"
        ],
        [
            "I think so.",
            "We at regularization term to our objective function.",
            "And here in this equation, Omega G is the function whose value will increase with the increasing complexity of the model, and we also have Lambda, which is the regularization parameter.",
            "Again, the parameter that we need to tune, we don't know apriori or still like what would be the optimal value for it.",
            "But the role of this parameter is to control the balance between the training error and the model complexity."
        ],
        [
            "So we will be focusing here on the linear functions of this following form, where W is the vector of model parameters that is found by minimizing the structuralized empirical risk that we just saw.",
            "And then the choice of this loss function and the regularization.",
            "It basically determines the learning algorithm that you get.",
            "So for instance, if you have them hinge loss and the squared Euclidean norm of W, which is known as well to regular regularizer, you get the support vector machine algorithm.",
            "Then if you keep the same regularizer, but you actually change the loss function into the squared loss, then you get rich regression algorithm and we will be looking.",
            "We will be continuing with Ridge regression and then rich regression with kernels.",
            "And then if you would keep the same loss function but change the regularization to L1 norm, you get the last algorithm."
        ],
        [
            "So this is just very quickly.",
            "For those of you who don't have like machine learning background, but just to see like how how this is done.",
            "So if we have this squared loss, we decided on our squared loss and on the regularizer.",
            "So here the quadratic regularizer we just write the optimization problem as follows and we can also write it like that's the same equation just in the matrix vector notation.",
            "And then what we actually want to find to train our model.",
            "Here it is, we would like to know what is our W. What are the optimal, what is the?",
            "Optimal W. So how we do it?",
            "We take the derivative risk with respect to W and we set it equal to the zero vector.",
            "Then we basically just need to solve it and we got our closed form solution for W. But even though we are focusing on linear models that I showed you, so we had this in the previous slide.",
            "This was there in red in the model.",
            "So it's a simple linear model, but we will use kernels in order to look for nonlinear patterns in the data."
        ],
        [
            "So what we do now is that so first of all, the optimal W. It can be written as a linear combination of the training examples and this equation follows from the representative forum, but for now it's just important that you that you see this equation and you see that by introducing this so called do our variables Alpha.",
            "We can basically write optimal W as a combination of the training examples that you have, such as drug, protein, purse.",
            "And then we just basically plug it into the previous equation that we saw and we see that we can represent the model predictions in terms of inner product of training examples.",
            "And this means that we can use kernels because we saw the definition of kernel earlier involving the inner product.",
            "And basically the interpretation behind this, like important parameters here, so W and Alpha.",
            "So W it was actually informing about like the importance of each of the features that we have as our inputs.",
            "So like if you would actually look at that what you have in W would get the importance of the features for a given prediction task.",
            "Whereas in in case of kernels we are not anymore looking at the features but Alphas would give you the importance of each of the training examples.",
            "So we had N training examples.",
            "Alpha will be a vector of length an an it will like show you how important each drug protein per was.",
            "Like for making this prediction.",
            "So just different way how we are interpreting W and Alpha.",
            "So Alpha is in the dual space."
        ],
        [
            "So now I'm moving on from this from basically what we had.",
            "So from the retrogression moving to this dual space and this will form the kernel Ridge regression algorithm.",
            "So just to get this solution you could use one of these equations that we have seen previously."
        ],
        [
            "And then we just rewrite it in terms of W and then again we just need to solve it so we get the solution closed form solution for Alpha.",
            "So it's very simple and then again we see the inner product of training examples.",
            "Here there's X transpose times X.",
            "So that basically means that we can use kernels so rich regression with kernels like this is called Kernel Ridge regression algorithm."
        ],
        [
            "So now the annotation in this our drug protein binding affinity prediction problem would be that we have any different drugs and P different proteins which form together a set of end training examples.",
            "So drug, protein, purse.",
            "And we have also real values indicating what is the strength of their interaction.",
            "So the binding affinity Y and then we have also a pairwise kernel matrix K. So now the question is, what is actually the pairwise kernel K because we worked so far with the drug kernels and the protein kernels.",
            "But we haven't seen yet the pairwise kernel K. So it is."
        ],
        [
            "So we expect wise Colonel.",
            "It measures the similarity between drug protein pairs and it is typically calculated as a Chronicle product between drug kernel and the protein kernel.",
            "And as a reminder of."
        ],
        [
            "For the Chronicle.",
            "Product is it's when we take a Chronicle product of two matrices.",
            "We basically did the resulting matrix.",
            "It contains all possible products of entries that we had in the matrices that we are multiplying.",
            "But now The thing is so OK, Now we know what is the what is the third wise kernel.",
            "And this is the kernel that we need.",
            "But now what is?"
        ],
        [
            "The problem is that the size of a pairwise kernel it increases very quickly with the number of drugs and proteins that we have.",
            "So for instance, if we have just hundred drugs and 100 different proteins that we want to work with, the corresponding pairwise kernel matrix, it would have 100 million entries and takes approximately 1 gigabyte of memory.",
            "Then if we just like double this, I'm in terms of drugs and targets.",
            "So if we have 200 drugs and 200 proteins, then your pairwise kernel would have 1.6 billion entries and it will take roughly 12 gigabytes of memory.",
            "And that's just a single kernel.",
            "So basically this makes the model training infeasible in practice.",
            "In typical applications, in terms of both memory and computational power time that you would need to do it, so it's so it's it's.",
            "It's very inconvenient to work with pairwise kernel directly and just plug it to the standard.",
            "Colonel Regression algorithm in this case."
        ],
        [
            "But Luckily it is possible to use the algebraic properties of the Chronicle product to speed up the training of the model.",
            "So here you can see we had the we have the close form solution to Colonel introgression, that we sell, and we are just like rewriting it.",
            "Of course, here we are missing, like multiple lines of derivation, but just to show you like what's the what's the final equation that doesn't involve anymore any pairwise matrix K?",
            "We only have the components of the smaller matrices of drug kernels and protein kernels, and this started with doing the egg and the composition of the kernel matrices.",
            "So here then.",
            "Why now it's not even anymore vector?",
            "It's a matrix that stores the binding affinity between your drugs as rose and between proteins indexed.",
            "So we have them as columns in the matrix Y.",
            "Then the QD is the matrix containing the eigenvectors of drug, kernel and QP.",
            "Similarly, same matrix containing the eigenvectors of the protein kernel and that lamp does contain the eigenvalues.",
            "So we can see like so in the final equation we can see that there is still like one Chronicle product left, which is this one.",
            "But actually we can write it a bit differently to avoid like taking any kinds of Chronicle products to avoid like having this like working in the large spaces.",
            "And basically it comes from the fact that the Kronecker product of 2 diagonal matrices is also diagonal matrix.",
            "So you can see that we completely avoid the computations of any pairwise kernels, and we work in much smaller spaces of drugs and proteins.",
            "But there is still one caveat here.",
            "So basically why the matrix Y with your drug protein binding affinity's it cannot have missing values.",
            "For this to work, the matrix that you use in the training.",
            "And of course, in practice you will often see missing values.",
            "So there are two different approaches here now.",
            "So if you have depends on what kind of problem you work with.",
            "If the number of missing values is not due to huge, so maybe like 10% of missing values in the matrix Y, then you could just use some matrix imputation technique to fill in this matrix and then you can like work with it.",
            "But if you have quite a lot of missing values, if this is very sparse then there is still a way to speed up the model training.",
            "This is described in this second reference at the bottom, so the difference is like that.",
            "In that case we will not have anymore a closed form solution as you have here, but you could still use efficient solver of the system of linear equation of linear equations and this is combined with the generalized form of electric, that is, that is introduced in this in this paper below.",
            "So this will be still this will be not as fast and efficient at this algorithm that has a closed form solution.",
            "But if if you have too many missing values in your in your matrix and you cannot impute them, that is still a good idea to use it because it will be way faster than moving to the pairwise space directly because often in the pairwise space actually like it will be just like tool computationally heavy to get any kind of solution.",
            "So this is still a very good algorithm to work with."
        ],
        [
            "So I have shown you that there are actually like multiple different ways to measure similarities between molecules between drugs and proteins.",
            "So we actually easily get not just one kernel for drugs and not just one kernel for proteins, but we have multiple drug kernels and multiple protein kernels, which then in turn leads to even more pairwise kernels.",
            "Because these are like all combinations between dragon protein kernels.",
            "So the classical algorithms there rely only on a single pairwise kernel.",
            "So typically what you would do is that you would run a single experiment for each combination of a drug kernel and pro in kernel that you have, and then you would like repeat it for all different kernels that you have.",
            "And basically you would have.",
            "You would then based on the predicted performance like for instance from the cross validation you could select what is the best kernels?",
            "What is the best model that you want to use for making then some predictions that you are actually interested in.",
            "But this is of course kind of, not maybe the optimal because you have to run so many different experiments, whereas with multiple kernel learning you have all of this in a single experiment and you can learn what is the importance of each of the input kernels.",
            "And of course, another thing also is that if you are using, so yeah there will be this cases when maybe there is like very good predictive drag kernel and very good predictive protein kernel.",
            "And just like they give you awesome results and you actually don't need to think of anything else there, the kovit is just need to run all the experiments to like find what is this kernel.",
            "But then in many cases it could also be that actually you want to actually want to integrate all kinds of information sources that you have.",
            "So just taking advantage of all different data types that you have soda side effects for instance and.",
            "Gene expression responses and also the different fingerprints and so on.",
            "So basically like you just want to integrate them to achieve a better predictive performance which was shown in the literature especially recently.",
            "Then this data integration makes a lot of sense and can improve the predictive performance.",
            "So multiple kernel learning methods.",
            "They search for an optimal combination of several kernels and they allow us to use information different information sources simultaneously, and they also learn the importance of this information for the prediction task.",
            "So basically here you can see in the equation.",
            "So now our goal is to learn this optimal kernel weights mu to combine our kernels.",
            "And broadly, this different MCL methods.",
            "They could be divided into one stage and two stage techniques.",
            "So in the one stage technique it would be designed in such a way that we are learning the optimal kernel combination and model parameters such that this Alpha.",
            "For instance, in kernel shagrat and that we learn them kind of simultaneously, but in the two statements we first find the optimal kernel weights and only then, given the optimal kernel, we train a classifier or regressor.",
            "And here we will go through.",
            "I would like to focus on this two stage multiple kernel learning."
        ],
        [
            "Because I think this modularity is very nice.",
            "So basically, once you have constructed your optimal optimal kernel, it can be used with different machine learning models, so maybe you would like to try support vector regression or kernel Ridge regression so you have the kernel you can.",
            "You can like use it for different applications and for instance in the second stage you could have this kernel Ridge regression.",
            "I mean not directly kernel regression but for instance the algorithm that we just like looked at before the other one in the that I mentioned the reference of something efficient.",
            "It can still work with pairwise kernels.",
            "But now let's have a look at the first stage that we haven't discussed yet.",
            "So how to actually find this optimal kernel weights and what's the intuition behind this?"
        ],
        [
            "Saving the most popular two state MCL, Methodist align F and the idea of a line F. What is what it does is to maximize the so called centered alignment between the optimal combined kernel an so called ideal response kernel Ky.",
            "So now we will have a look what does it actually mean?",
            "So we have our labels our binding affinity's between drug protein pairs and now first of all what we do we calculate this so-called response kernel based on them.",
            "And now with this kernel tells you is basically how similar your drug protein pairs are in terms of their binding affinity's and why this kernel is important here.",
            "And why is it kind of called ideal is because like this is your target.",
            "This is actually what you want to predict, so you want to have flag such similarities like in your inputs you want to try to find like such a way of measuring the similarities that it would give you basically the same information about your drug protein pairs.",
            "Where does this come from?",
            "Which part from there from the labels?",
            "So you calculate it from the equation.",
            "Actually will be also later, but this is calculated from the labels that you have in the training data.",
            "So this is this is a vector, for instance, like here it could be in a matrix form, but let's think that now each entry in this label in this label vector Y.",
            "It is each entry corresponds to the binding affinity of a certain drug protein per.",
            "So for instance, the first entry would be.",
            "The first entry would be imatinib and BCR ABL, and then we have some some binding affinity, for instance KD value or like dissociation constant KD or something else measured in the lab.",
            "So this is this area.",
            "Training data or labels that you work with.",
            "And from that we calculate this kernel Ky.",
            "Yes, this answer.",
            "The question right there.",
            "So yeah, maybe like.",
            "Yes, the recently so we know why aren't labels that you like typically have in your machine learning.",
            "So like for instance if you want to predict divining companies, we can drive some proteins and what you need is to have some training data first of all, and you can expect This is why is the vector of target players?",
            "Yes, yes, yes yes.",
            "I just don't like to put target experience propane partners.",
            "It could be the cycle response, but yes, it's like the target value.",
            "Multi multi target impact or multi response.",
            "Yes, so basically this would be amazing, because if you imagine we have like 100 drugs and hundred properties, you actually have a matrix of like all possible landing amenities between them.",
            "This is now just like the regular footwear breakfast sandwiches vectorizes to have like more.",
            "Understandable representation, kind of, but basically this will contain binding communities of all drug protein pairs that you have in your training.",
            "Event is much dependence.",
            "You can find him some publicly available data resources, so based on that.",
            "So this is a vector.",
            "We can calculate the Colonel so we have.",
            "So here the roles were indexed on the elements of the vector were indexed by the drop protein powders and also here there will be indexed based on both rows and columns will be indexed by dropping pairs because we want to learn now how similar the drug propane spur R, but in terms of the.",
            "Mining companies.",
            "So this would be kind of the ideal Colonel Cousins who want to predict.",
            "So like this idea because he wants to printing the binding up on this.",
            "So if you are like I mentioned earlier that the high level assumption is that similar drugs are interacting with similar proteins or the similar drug propane purse will have similar binding armies.",
            "So then the whole big question is like how you actually represent the similarities.",
            "What is the optimal drug kernel?",
            "What is optimal propane kernel?",
            "But at the end, like they really optimal one for this prediction.",
            "Is what it would actually calculate from your labels.",
            "So this is what you want to predict.",
            "So you can measure the similarity in terms of what you want to predict.",
            "It's kind of like optimal, but now we'd like.",
            "Some of you might think if you don't have machine learning background, so we have.",
            "This cable is so awesome, why don't we actually like use it for making the prediction directly?",
            "And the answer to that question is that this one is only available for the training data, so we can learn from this kernel.",
            "But actually we cannot use it.",
            "This Patch for like maybe the connection.",
            "So that's why I like this idea.",
            "But like we can only learn from it, we cannot like use it like with the tiny motor kernel and the GS kernel.",
            "You can calculate it for the drugs and proteins for which we don't have anybody coming in today to because he was before did drugs encodings.",
            "You're interested, but you will not have much finding out new data, but you can still use their chemical structures there.",
            "I mean ask sequences there side effects if there are and so on.",
            "So we finished all of that, but unfortunately this funding will usually don't have something, so that's why we only use as part of their.",
            "But not as much for making the predictions.",
            "Does anyone have any more questions about this?",
            "Because it's important to understand, like I mean, it's nice if you understand the concept for the rest of the talk is then kind of like based on this type of methods.",
            "OK.",
            "But they're so like the first step that we is that we utilized our labels.",
            "Our binding affinity is to calculate this response kernel.",
            "It can also be called target.",
            "Actually, in the literature is called target kernel Ky and then basically what we want to achieve is to compare this kernel and either the input kernel or here in this algorithm directly the kernel that will be our final combined kernels of the kernel that he will combine from all other maybe hundreds or thousands.",
            "Of drug and protein kernels that you had.",
            "So basically what we want to do is to maximize the similarity.",
            "Choose the way it's mu in such a way that the similarity between this kernel came.",
            "You the final combined kernel came you and the response Kernel Ky would be maximized, so that's why we have Arg Max in the equation an otherwise this A indicates the alignment and this alignment.",
            "It basically just measures the similarity between the two matrices.",
            "It can be interpreted as a cosine of the angle that is defined between two matrices.",
            "So just a normalized tribunas product.",
            "But yes, the whole the whole concept is like about about maximizing the similarity between these two matrices that you see here.",
            "And based on that, which was our optimal weights and now another thing.",
            "So like in the definition earlier I mentioned centered alignment so you can see that the kernel that we have in the equation on when we maximize the the kernel weights mule, we have actually Casey and Casey means it's a centered kernel.",
            "And here is the definition.",
            "Here it is we you can see how do we actually center Colonel.",
            "So what it means is in practice is that what we are doing is actually we are centering.",
            "The feature mapping that is associated with the kernel and in terms of the equation you just take this matrix K and you your kernel K and you multiply it from left and right hand side with the centering operator C. And in the indication for the centering operator, we just have the identity matrix of the same size as your kernel and then one is a vector of all components all equal to 1.",
            "But here's the thing to remember is that basically it's about centering the associated feature mapping the future mapping associated with the kernel."
        ],
        [
            "So now we have this optimization problem.",
            "We just we saw it in the previous slide and it can actually be solved quite efficiently by using the simple quadratic programming that you see here.",
            "And to solve it we need to calculate vector A and symmetric matrix M and Now what?",
            "What this what this R?",
            "So the vector a it contains this.",
            "Normally it contains the Frobenius products between each input kernel that you have and this response kernel Ky.",
            "So the similarities between the each input kernel.",
            "And the ideal response kernel and the matrix M contains all purse of similarities between your input kernels and discard the pairwise pairwise kernels.",
            "So this is how we solve it.",
            "So once you have calculated your vector A and your matrix M, so you know all kinds of similarities between your matrices, then you can just use them.",
            "You just like use the solver for this.",
            "So this is generally like very very simple procedure."
        ],
        [
            "But now we have the same problem as before that we are right now we are working with the pairwise kernels and the same problem in terms of the like memory usage and the training time.",
            "We cannot really like train it as such and you can hear see, hear that really.",
            "We've already very small number of drugs and proteins.",
            "This becomes infeasible in terms of processing and memory requirements.",
            "So now what are there?",
            "There's still a way to solve it."
        ],
        [
            "So basically we have introduced this pairwise MCL algorithm, which is just a lineup for permits kernels I would say.",
            "So the whole the goal here was to work with the drag kernels and the protein kernels.",
            "So work in much smaller spaces of drugs and proteins instead of working with this huge pairwise space.",
            "So this is a dent like the overview of how it looks like that you actually don't have to work in the pairwise space at all."
        ],
        [
            "And this is now like showing that you can actually then calculate it very efficiently, like it kind of takes as much time as if you would be working with smaller drug kernels only or protein kernels only in more like standard, not pairwise learning scenarios.",
            "And now."
        ],
        [
            "Like couple of technical details for those of you that are interested.",
            "So kind of the main bottleneck in using this align F was that it requires this entering of the kernel because basically I mean for those of you who kind of like maybe know a bit more about Chronicle products and so on.",
            "Maybe you could easily notice from the previous equations that if there wouldn't be a centering then you could actually again use some properties of the Chronicle product, some algebraic properties to like rewrite the equations in such a way that.",
            "You end up with having just dropped kernels and protein kernels, and not pairwise kernels, but then the centering is this bottleneck that doesn't allow you to do this because centering a drug kernel and then centering approaching curl and taking a Chronicle product doesn't give you a center pairwise kernel, so center pairwise kernel cannot be simply calculated from center drag kernel and centered protein kernel.",
            "So what we have done to make it possible is that we introduced a highly efficient Chronicle, the composition of the centering operator for the pairwise kernel.",
            "So here QD and QP are the factors of C of the centering operator and just what is important because we're not going into like all the details, but I want to just mention that this is very efficient because it always comes down to solving as his singular value problem for a matrix of size 2 * 2 only.",
            "So it doesn't matter how many drug protein pairs common examples you have, you could have.",
            "Like 1000 lichen could be 1000 or a million.",
            "But because we exploit this particular structure of the matrix C at the end, we only need to solve the singular value problem for a matrix of size 2 * 2.",
            "And and that's why this is extremely efficient.",
            "To calculate this, this factors QD and QC, and basically once we have this factors once we can like write matrix, see in this form we can just rewrite all the equations and I mean these are of course very long.",
            "But like just the main point here, I want you to notice that we don't have anymore any pairwise kernels.",
            "Everything is based on some components of the pairwise.",
            "I mean just the drug kernels, protein kernels and then agen vectors and so on so we don't actually know Dragon vector.",
            "Sorry, just the factors of them.",
            "Centering operator.",
            "So basically we are not using any pairwise matrices in any point, so that's why that's why it's faster, that's why that's why it's feasible, because we avoid going to the pairwise space."
        ],
        [
            "And now another thing is that in multiple kernel learning, so for classification it would be usual to calculate the response Colonel in this following form just the linear kernel and this works really well because it separates the positive and negative classes perfectly.",
            "So the idea is that if you have so in classification you will offer label like for instance interaction will be labeled as plus one and non interaction is minus one and then if you see in this equation if you have.",
            "If you're comparing 2 drug protein pairs that were not interacting, you have minus 1 * -- 1.",
            "You get +12 protein purse that we're interacting.",
            "You're gonna get plus one, but if the labels are different, you get minus one.",
            "So we just like separating like what is the same gets value of 1 and the different purse get a value of minus one.",
            "So that's why this kernel works very in this response.",
            "Kernel K. Why?",
            "This is a very good choice for classification and in all the multiple currently multiple kernel learning literature like this INF paper and so on.",
            "It was always introduced like that.",
            "But we thought that of course, since we are focusing on regression task, it doesn't make sense to use this kernel because what it does, as you can see in the example, it just gives you a small kernel values for the low numbers, low binding affinity's and large kernel values for the large binding affinity's.",
            "So this is all the usual linear response kernel would do, which doesn't make sense for the regression task.",
            "We want to compare our binding companies want to compare numbers, so of course the natural choice.",
            "Would be too.",
            "Is the Gaussian kernel.",
            "It's like a gold standard for comparing real numbers.",
            "But why we cannot use it directly?",
            "Why we cannot just plug this in?",
            "Because we have as I showed in the previous slide, we have this equations for calculating our kernels and we cannot just like plug it in there because it wouldn't like we will have our speedup anymore basically."
        ],
        [
            "So what we have done is that we have like.",
            "Approximated form the Gaussian kernel?",
            "So first of all, we fit a mixture of Gaussians onto the frequency histogram of our labels, and then for each value Y.",
            "So for each label we define a window of S bins around it.",
            "And then basically we define, so each label will have a feature vector that we derive from there.",
            "So the whole point of looking at it like this is to derive a feature vector for a label.",
            "So this feature vector is read off from this bin densities and normalized and we just like plug it in the corresponding element of this of this matrix side.",
            "And now the kernel can be simply compute calculated as a sum of products of as being densities.",
            "So basically we have now a very nice representation because like if we just have we don't have a Gaussian kernel equation, But in this kind of form we can plug it into the equations that we saw before."
        ],
        [
            "And this gives us the kind of representation of the response kernel as we wanted to have, because for instance, from here in the linear kernel it would just multiply 7 * 9 and minus 5 * 7 and so on.",
            "But here you can see that a dent, we see that seven and nine are similar to each other and minus five and seven, and for instance this similar, so we're just measuring the similarities between the real numbers, but using like another equation, not the Goshen equation for it.",
            "So once we have the.",
            "Response kernel in this form."
        ],
        [
            "We can just like plug it in so the response kernel was used in the equation for calculating vector A and once we plug it in, that's what we get.",
            "So again we are not working the pairwise space.",
            "So all of that is done in the spaces of drugs and proteins."
        ],
        [
            "So now I have one application example just to show you kind of what we can do.",
            "So here we use then drug target interaction data comprising of roughly 160,000 PC 50 values between roughly 3000 kinase inhibitor drugs and 226 kinases, and this is publicly available data.",
            "And we calculated 10 different drug kernels and 312 kinase protein kinase kernels.",
            "So this gave us we just need to multiply the two because we consider all different pairs always when working in the pairwise space pairwise kernel space.",
            "So this gives us 3120 pairwise kernels."
        ],
        [
            "So 10 different drug kernels that I mentioned here.",
            "They were all calculated as Tanimoto kernels using different types of fingerprints."
        ],
        [
            "And then for protein kernels we have considered as the input data sources the amino acid sequences and the gene ontology annotations.",
            "And we considered specifically full amino acid sequences of the proteins of the kinases in our set, but also the subsequences of the kinase domains and subsequences of ATP binding pockets.",
            "So these are the shortest, the final ones.",
            "And then we calculate this standard Smith, Waterman kernel and the generic string kernel that that I showed you earlier.",
            "And then for the gene Ontology annotations we derived like Gene Ontology profile for each protein.",
            "And this profile is basically we look at the proportion of the proteins that are annotated with a particular gene ontology term and then we take a negative logarithm of debt.",
            "But basically what it means is that we will get large values for more for the terms that are more specific to a certain protein.",
            "So kind of.",
            "Just like maybe highlight this a bit like or like make it more, tell it the algorithm it that's more important, because if it's somehow more specific if.",
            "Of course, some proteins are especially kinases.",
            "There will be lots of gene ontology terms that all of them are on notated with, so that's maybe not the ones that we want to like.",
            "Give too much importance.",
            "And since we have this real value profiles, we applied Gaussian kernels on top.",
            "And now why we have 312 different kernels is because then Gaussian kernel has one hyperparameter GS.",
            "Kernel has free hyperparameters and Smith Waterman.",
            "It doesn't have any hyperparameters.",
            "So basically instead of tuning the hyperparameters like for instance in the cross validation in the nested cross validation, when you know, especially with the nested cross validation and the grid search, it is.",
            "It can be quite time consuming.",
            "So you have already here for different kernels.",
            "And there's also regularization parameter of the algorithm.",
            "So it takes quite a lot of time to tune.",
            "All these parameters this way, so we wanted to take advantage of the multiple kernel learning also for the parameter tuning, so that's why we just calculated kernels with different different values of the hyperparameters from the reasonable range, and we got 312 protein kernels.",
            "So now this was our our training data and our kernels."
        ],
        [
            "And then here is the summary of the results.",
            "So pairwise some kill it selected out of this over 3000 kernels, it selected eight different kernels and we can see then the selected kernels selected pairwise kernels and their weights here.",
            "So one thing to emphasize is that actually we observed that ATP binding pockets turned out to be quite predictive, as kind of could be expected, as I mentioned, because with the emergent earlier this for kinase inhibitors, most of them not all, but most of them binds to the ATP binding pockets of the proteins.",
            "So that's why, like just focusing on comparing ATP, binding pockets makes more sense than comparing full amino acid sequences.",
            "And another thing that you can see or like one of the main things is that the GS Colonel turned out to be.",
            "Quite productive and more productive than Standard Smith Waterman kernel.",
            "That of course kind of makes sense because Smith Waterman is like the simplest and most like standard approach and also one thing that I could mention here is that so that it's actually like with kernels like one of the disadvantages is maybe that interpretation can be hard.",
            "So for instance if you are if you are for instance working on the prediction of drug responses in cancer cell lines, it is quite well known that gene expression.",
            "Is very predictive data source.",
            "Of course they have also mutation complete number variation, metalation profiles and so on, but it's generally well known and it has been shown that also in dream challenge that gene expression seemed to be the best and the most productive.",
            "So now if you would calculate the kernels, typically would calculate the kernel for the gene expression values.",
            "A kernel for the mutations and so on you would get a couple of kernels and you would get some, let's say nice predictive performance and probably what the algorithm would tell you that most of the weight was assigned to the gene expression.",
            "So OK, it's nice, but you kind of knew it like it was expected that the gene expression is more productive.",
            "So I don't have it results for that because it's still like we still haven't like done it, but like my idea.",
            "What can be done with multiple kernel learning?",
            "Why it is nice?",
            "Because you can also like kind of get more into interpretation of your results because instead of for instance calculating kernel for the gene expression values, you could for instance like look at pathways instead of looking at all gene expressions together.",
            "You could look at different pathways and for instance calculate multiple kernels for each different pathway.",
            "Let's say same for the drag kernels.",
            "Instead of calculating Tanimoto kernel.",
            "Based on your whole fingerprint, you could like think of how to divide them into like more like manageable substructures that could inform about the activity somehow.",
            "So we could have like from a single fingerprint you could have like multiple different kernels and then at the end because this algorithm can handle lots of different kernels as input and for sure there will be other algorithms as well.",
            "So basically then your interpretation would be would see that maybe there was like a certain pathway in a certain like substructures that were very predictive.",
            "So that can of course it's not a perfect interpretation.",
            "But at least it can like increase the interpretation of the kernel methods in a way or that would be like 1 one way with multiple kernel learning."
        ],
        [
            "And then something that is very important in practice is that in case of drug protein interaction prediction, we need to consider four different prediction scenarios depending on whether or not training and tests share common drugs and proteins or both, and this is extremely important, but it's very often forgotten and this doesn't only apply to drug protein interactions, but like any kind of machine learning problem where you have pert inputs.",
            "So when you're like inputs consists of 2 two different parts.",
            "So we can see here that in this like first one, most left hand side, this is like the most standard cross validation.",
            "So this is like how you would do it like you have your you have your data, you have your drug protein binding affinity would just like randomly remove some of them, do the training like predict like what were the motives accuracy for those.",
            "Then you would remove like randomly other like just do the cross validation in a standard way.",
            "But then for instance.",
            "Wait, why does this like not optimal?",
            "So maybe you have an algorithm like some either like existing or new algorithm and you would like to knit and like assess using the cross validation.",
            "The predictive performance.",
            "And let's say you were very happy with the predictive performance is very high and then you have a collaborator who for instance designed a new chemical structure and the collaborate and knows that there is one on target that the structure was like a design for.",
            "But like he comes to you and asks you like I would like to know the binding profile.",
            "Of this new compound, new chemical structure that I designed with all the kinases, all humankind and for instance.",
            "And then you might think like, OK, I have this algorithm.",
            "I tuned it, I get a good predictive performance, so I will now just apply it and do the predictions.",
            "And then if you have done it in this way, you have forgotten about like this different prediction scenarios you would get over optimistic predictive performance because what you should do in this case is that your cross validation needs to mimic the problem that you are so.",
            "Design across validation in such a way that you remove always single driver multiple drugs at a time from your training data and you do it both for assessing the performance of the method but also for tuning the parameters.",
            "So this is extremely important just to remember and especially dislike both on paper.",
            "This is a very nice, quite short paper that kind of explains the same ideas generally for part input predictions."
        ],
        [
            "And then just a dent couple of slides about the dream challenge.",
            "Maybe some of you have heard or even participated.",
            "I don't know, but basically we were just we were organizing the dream challenge on drug kinase binding affinity prediction.",
            "And the basic idea was to evaluate how well different machine learning models work in practice for this problem.",
            "So the three overall questions that we're hoping to answer based on the challenge or what are the best machine learning approaches for predicting drug protein binding affinity's?",
            "At what are the most productive compound and protein features?",
            "And also what are the Best Buy activity data for the model training?",
            "Because you know, it often comes down not just that there is like one algorithm that solves everything.",
            "But of course also the training data should be like of high quality and so on.",
            "And also like the appropriate features and so on.",
            "So it's always a combination of multiple factors.",
            "So we want to learn as much as possible."
        ],
        [
            "And we have put an emphasis here on quantitative modeling of the compound protein interactions.",
            "So the regression setting as we had in this lecture, and we focus on protein kinases and kinase inhibitors because of their clinical importance.",
            "Of course, in cancer they are very important in treating cancer, but also other diseases and there are some clinical trials testing kindness inhibitors like for other than cancer, other indications and cancer."
        ],
        [
            "And here is the overview.",
            "Sorry if the font is too small in some places, but basically we had two rounds in the challenge.",
            "In the first round we ask the participants to predict 430 compound binding affinities of 430 compound kindness purse and in the second round we have 394 compound kinase purse and the data the actual data that we use them because it needs to be unpublished data and then we need to participants don't have an access to that and then we are.",
            "Evaluating how well their model performed.",
            "So this was provided by the illuminating trackable Genome Kindness Consortium.",
            "So in the first round we had 77 teams.",
            "Some teams consisted just of 1 participant, but we will call it teams here.",
            "7077 teams and 454 teams.",
            "In the second round.",
            "And we also had a baseline model which was a pairwise kernel regression.",
            "But it was supposed to be baseline, so not not the multiple kernel learning and not using like more sophisticated kernels, but a single kernel and just pairwise kernel regression basically.",
            "So the first one, the first round, had lasted 1 1/2 months and declaw it closed in the mid November and then the second round closed just a month ago.",
            "And basically in the first round we didn't have any winner, so there the idea was just to let the participant see how well they perform across different evaluation metrics.",
            "At that point we didn't reveal what is the winning metric, just not to like have maybe like encouraging for optimizing the predictions for a particular evaluation metric.",
            "So just to see how well they perform.",
            "But no winners such, whereas in the second round we have two different sub challenges and the winner of the first Step Challenge was determined based on the Spearman correlation.",
            "And the winner of the second sub challenge based on the Harmacy and then right now even though I mentioned that we closed at one month ago but we don't have like official winners as such because also very important part of the dream challenges is that everything that we learn and everything that we can get out of there is made publicly available so that the whole community can benefit.",
            "So right now we are still collecting Docker containers of the models and write ups of the methods and all of that from the participants because if the like for instance best participants is not.",
            "Able to provide this because it's like maybe developed in a company for instance and so on.",
            "Then unfortunately we cannot like announce like this participant a winner, so that's why I'm not like able to show you like very detailed results it because we are now in the still in the process of like making this this final ranking and so on.",
            "But one thing like this is."
        ],
        [
            "Just in general overview of like focusing on our machine, Spearman.",
            "What is the distribution of these different different values, different prediction, predictive performance that we got and you can see that there were quite good models and models that were not working that well.",
            "The red lines and the blue line they mark the baseline results.",
            "And so I should also mention, because like maybe the like, you can see the top performers.",
            "They had the correlation.",
            "Spearman correlation of like a bit less than .6, and it might seem like it's not super high.",
            "But actually this this problem this problem in the Dream Challenge data that we're considering?",
            "They corresponded to quite challenging scenario.",
            "So actually like mostly this was this new drug setting.",
            "So if you would look at the if we would come back to this like when we had different cross validation scenarios, it would be the second case where in most of the cases of drug doesn't really have.",
            "And then sorry, and then prior binding affinity data that you could use for the training.",
            "And actually, I saw some of the scatter plots, like from for the best participants and they looked quite reasonable.",
            "So actually I think we have that we will have some interesting insights.",
            "So now we will analyze all the results and models to learn what are the things work and what don't, what things like don't work.",
            "So basically what are the best methods?",
            "What are the training data that worked well?",
            "What kinds of features an I can just briefly tell you since we don't have yet and we know it cannot say specifically, but we have actually looked quite different methods among like the very top performing teams.",
            "So there is deep learning.",
            "There is random forests and there is also some kernel learning, so it's not really like the respect.",
            "One algorithm that works best for everything.",
            "But soon there will be a challenge paper available.",
            "So for all of you that are interested, like how does it actually look like and what we were able to learn from this from this challenge like you are welcome to to read and.",
            "And I hope that there was some interesting insights."
        ],
        [
            "That's all, thank you for your attention and I'm happy to take any questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hello everyone and today I would like to tell you something about the using kernels for the prediction of binding affinity's between drugs and proteins.",
                    "label": 0
                },
                {
                    "sent": "But first of all I would like to ask you questions.",
                    "label": 0
                },
                {
                    "sent": "So how many of you have worked with kernels before?",
                    "label": 0
                },
                {
                    "sent": "Is there anyone who has some experience?",
                    "label": 0
                },
                {
                    "sent": "OK, but not too many, so I'm glad that I have like quite some introduction slides so that we can actually like going through that.",
                    "label": 0
                },
                {
                    "sent": "And hopefully you will then like know why, why kernels can be nice.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first of all, we all know that drug achieves therapeutic effect through modulating molecules in the human body, such as proteins, metabolites or DNA and RNA.",
                    "label": 1
                },
                {
                    "sent": "But the most common targets of drugs are proteins, so that's why we will focus on proteins here.",
                    "label": 0
                },
                {
                    "sent": "And it is actually not surprising because proteins, as we all know, have many important functions and they are also involved in many disease processes.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one example of drug protein interaction in the cancer treatment involves a drug imatinib which has revolutionized the treatment of leukemia patients, so imatinib interacts with and specifically blocks the BCR ABL, which is a Fusion protein that causes chronic Milo.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Leukemia.",
                    "label": 0
                },
                {
                    "sent": "But drug action is of course way more complicated process, so we usually a drug interacts with more than this single protein.",
                    "label": 0
                },
                {
                    "sent": "That is its intended target.",
                    "label": 0
                },
                {
                    "sent": "Like BCR ABL in case of pneumatic.",
                    "label": 0
                },
                {
                    "sent": "So other proteins that the drug interacts with are called off targets and this off target interactions can be either neutral.",
                    "label": 0
                },
                {
                    "sent": "But they could also be harmful or beneficial.",
                    "label": 0
                },
                {
                    "sent": "So for example, imatinib can cause cardiotoxic side effects because unintentional it also inhibit C ABL protein.",
                    "label": 1
                },
                {
                    "sent": "But in some other cases of targets of targets, can or the knowledge of those targets can help us to identify the news of a drug?",
                    "label": 1
                },
                {
                    "sent": "And in this case, using again Dean Martin example, it turned out that the unexpected interaction of imatinib with the kit protein is beneficial for the treatment of gastrointestinal cancers.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This example shows us that in order to design effective and safe therapies, we ideally should know the interactions of a drug or a drug candidate with all the proteins in the human body.",
                    "label": 0
                },
                {
                    "sent": "But the problem is that the determining those drug protein interactions in the lab is still quite time consuming and expensive.",
                    "label": 1
                },
                {
                    "sent": "Even if we use the modern high throughput technologies.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And Furthermore, the universe of all possible chemical molecules that either already exist or could be designed is really enormous.",
                    "label": 0
                },
                {
                    "sent": "And of course, not all the chemicals could be potential drugs because they need to have like so called so called good pharmacological properties.",
                    "label": 1
                },
                {
                    "sent": "For instance those that are released in the lipinski's rule of five.",
                    "label": 1
                },
                {
                    "sent": "But it turns out that even if we would narrow this enormous space of all chemical compounds only to those compounds that have good pharmacological properties, it is estimated that this universe of potential drug candidates.",
                    "label": 0
                },
                {
                    "sent": "It's still really huge.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this way machine learning comes in handy and probably even with machine learning we cannot right now explore this like 10 to the power of 24 drugs.",
                    "label": 0
                },
                {
                    "sent": "But of course we can.",
                    "label": 0
                },
                {
                    "sent": "We can like at least use it to get some help in this process.",
                    "label": 0
                },
                {
                    "sent": "So basically the idea here is not to replace the experiments in the lab with machine learning methods, But instead this methods could prioritize the most voted target interactions and drug candidates for further evaluation in the lab.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And different machine learning methods could be broadly categorized as drug based, protein based and systems based.",
                    "label": 0
                },
                {
                    "sent": "That's just like one possible classification, so the drug based methods they are actually the most traditional ones in machine learning, and they typically aim to relate this structural properties of the drugs to thereby activity profiles, and they are known traditionally under disk user name.",
                    "label": 0
                },
                {
                    "sent": "So quantitative structure, activity relationship models, and probably all of you have heard about dogs.",
                    "label": 0
                },
                {
                    "sent": "Then we could also think of protein based methods where we use the available by activity data and the protein information instead of the drug information, But these methods are not as popular and then systems based methods that are also known as proteo chemometric models or pairwise models depends on like what kind of background do you come from.",
                    "label": 0
                },
                {
                    "sent": "So those systems based methods they unify the other two frameworks because they take advantage of the properties of both drugs and proteins.",
                    "label": 0
                },
                {
                    "sent": "And the general assumption is that similar drugs are likely to interact with similar proteins.",
                    "label": 1
                },
                {
                    "sent": "And this will be our focus here, focusing on the systems based or like pairwise models.",
                    "label": 0
                },
                {
                    "sent": "So generally the goal is to find the relationship between chemical and genomic profiles and drug by activities.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here is the high level overview of different components of this systems based framework.",
                    "label": 0
                },
                {
                    "sent": "So we have a space of drugs, space of protein targets and some known interactions between them that we can use for training the models.",
                    "label": 0
                },
                {
                    "sent": "So for instance the link between imatinib and BCR ABL would be there and then we also have similarities.",
                    "label": 0
                },
                {
                    "sent": "We also have some information, additional information about both drugs and proteins that can help us to establish this.",
                    "label": 0
                },
                {
                    "sent": "Disconnections between the same types of molecules that are marked with the dotted lines.",
                    "label": 0
                },
                {
                    "sent": "In the scheme and similarities between molecules can be defined by lots of different means and we will of course go through that.",
                    "label": 0
                },
                {
                    "sent": "Now in the lecture in more detail.",
                    "label": 0
                },
                {
                    "sent": "So given all that, we can train a classifier or regressor and then predict some novel drug protein binding affinity's and potentially find some novel interesting interactions.",
                    "label": 0
                },
                {
                    "sent": "So the classification setup is, I would say it's still the most common in this problem it seems, but regression is more more realistic and also like more exciting.",
                    "label": 0
                },
                {
                    "sent": "I would say because we actually want to know the full activity spectrum over drug rather than the binary profile of like what it could potentially interact with because it's quite difficult to just like set a single binding affinity threshold to decide what is an interaction and what is not, what is not.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the high level assumption I mentioned earlier is that similar drugs are likely to interact with similar proteins, and the similarities between molecules can be actually encoded using kernel functions.",
                    "label": 0
                },
                {
                    "sent": "So here we're kernels come in.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So kernels why they are nice?",
                    "label": 0
                },
                {
                    "sent": "They offer several useful properties.",
                    "label": 0
                },
                {
                    "sent": "So first of all, kernel function embeds the input data into a new feature space with ever high number of dimensions.",
                    "label": 0
                },
                {
                    "sent": "Of course, this is only free because I couldn't we paint more dimensions here, but but technically it's just like a very large.",
                    "label": 0
                },
                {
                    "sent": "It could be very large, normal dimensions, even infinite dimensional space 8 and now a linear model that we can learn in this space will correspond to a nonlinear model in the original space.",
                    "label": 0
                },
                {
                    "sent": "So this basically means that thanks to kernels we can look for nonlinear patterns in the data, but we can use very well established linear learning algorithms to do so.",
                    "label": 1
                },
                {
                    "sent": "And what is important is that this whole process can be actually performed in a very efficient manner, because we don't need to explicitly compute this mapping and evaluate the coordinates in this like high dimensional feature space.",
                    "label": 0
                },
                {
                    "sent": "So the kernel can be calculated using the original features, such as.",
                    "label": 0
                },
                {
                    "sent": "Here it would be X.",
                    "label": 0
                },
                {
                    "sent": "So for instance, then chemical properties of a drug and we do it by replacing the inner product in this definition with a proper properly chosen kernel function and the Gaussian kernel would be one example.",
                    "label": 0
                },
                {
                    "sent": "So it will imply that they are there, that there is this underlying high dimensional feature representation X, but we actually sorry 8 high dimensional representation age, but we don't need to compute it.",
                    "label": 0
                },
                {
                    "sent": "We don't actually need to explicitly move to this space.",
                    "label": 0
                },
                {
                    "sent": "So for instance, in case of the Goshen kernel, this underlying space age is of infinite dimension because it contains all possible monomials of input features.",
                    "label": 0
                },
                {
                    "sent": "But then again, the advantage we don't need to actually like deal with this with this Space 8.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is a simple example.",
                    "label": 0
                },
                {
                    "sent": "So we have that 2 dimensional input space and then a feature map that takes the data from from 2D to 3D space and it is done in such a way that the linear relations in this new feature space will correspond to the quadratic relations in the in the input space.",
                    "label": 0
                },
                {
                    "sent": "So here we have the kernel definition, the inner product that we saw in the previous slide and we just evaluate.",
                    "label": 0
                },
                {
                    "sent": "We just plug in our feature map and we evaluate this inner product and basically we end up with the kernel function that looks like that, which is just the inner product between the original data that we had originally X and set.",
                    "label": 1
                },
                {
                    "sent": "And then we just square it.",
                    "label": 0
                },
                {
                    "sent": "So this implies that there is this underlying 3D space, but we don't need to actually evaluate the coordinates in this space.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here is the same just just as an example with the toy data.",
                    "label": 0
                },
                {
                    "sent": "So for instance, imagine that you would like to separate now the red points and this blue stars using linear function and you cannot do it here in the 2D space using a linear function.",
                    "label": 0
                },
                {
                    "sent": "But then we are using the feature map that we just we've just seen and this actually gives you this kind of space where you can see you can quite easily separate the points using a linear separating hyperplane.",
                    "label": 0
                },
                {
                    "sent": "But the advantage is that we don't need to actually.",
                    "label": 0
                },
                {
                    "sent": "Evaluate the coordinates in the fridge space and of course, remember that this is very simple and we don't mind going into this free space because it's very very simple in this example.",
                    "label": 0
                },
                {
                    "sent": "But then in reality would have like way higher dimensional spaces, and that's why that's an advantage that we don't have to explicitly move to 28.",
                    "label": 0
                },
                {
                    "sent": "But we can get the advantage of having linear nonlinear relations in the in the original space.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And kernels have also many other useful properties, so one of them is that often in machine learning we have this problem that we have way more instances where sorry way more features than the that the instances or samples that they describe.",
                    "label": 0
                },
                {
                    "sent": "So for instance we could have some number of drugs.",
                    "label": 0
                },
                {
                    "sent": "And, well, maybe we don't have that many chemical properties seriously, but if you think of gene expression profile or some other data in bioinformatics so often you have like millions of features even but, but not as many samples.",
                    "label": 0
                },
                {
                    "sent": "And with kernels it's not a problem because the data actually appears only through the entries in the kernel matrix, which relates or pairs of samples.",
                    "label": 1
                },
                {
                    "sent": "So for instance, here drugs in this in this example.",
                    "label": 1
                },
                {
                    "sent": "And kernels are especially well suited for representing structured objects such as images or molecules, because they don't always have, just like natural, natural natural, like vectorized representation.",
                    "label": 0
                },
                {
                    "sent": "So there are like some methods for comparing images directly, or the mass Spectra or like graphs and so on.",
                    "label": 0
                },
                {
                    "sent": "So that could be very convenient when you have when you have this kind of structured objects.",
                    "label": 0
                },
                {
                    "sent": "Enter, one important thing is to remember here.",
                    "label": 0
                },
                {
                    "sent": "Kernel can be considered as a similarity measure between your inputs.",
                    "label": 1
                },
                {
                    "sent": "So here drugs so also not not every similarity measure is a valid kernel function.",
                    "label": 0
                },
                {
                    "sent": "It doesn't work the other way around because kernel needs to be a positive semidefinite function and so on.",
                    "label": 0
                },
                {
                    "sent": "But once you have a kernel, once you have a valid kernel, you can always interpret it as a similarity measure.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we will now have a look at each one of these components and first of all we were start with the labels.",
                    "label": 0
                },
                {
                    "sent": "So just briefly like where could we find information about the binding affinity's between different drugs and proteins to train the models?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there are of course lots of different databases.",
                    "label": 0
                },
                {
                    "sent": "Campbell is like one of the maybe best available data resources, so their data are manually extracted from the literature and the current version contains almost 2 million information, about 2 million compounds, and this data is extracted from over 70,000 different publications, then another quite well known database would be popped in and it is way larger because their data are not manually extracted, but data generators can deposit their data.",
                    "label": 1
                },
                {
                    "sent": "And it contains right now, around 98 million compounds.",
                    "label": 0
                },
                {
                    "sent": "But there you have to be more careful, because since it's not curated so kind of maybe you should like do some kind of curation yourself.",
                    "label": 0
                },
                {
                    "sent": "And then an interesting data resource, drug target Commons DTC that was established quite recently in Finland actually.",
                    "label": 1
                },
                {
                    "sent": "So it is a community effort to curate and notate, and also harmonized by activity data that are existing in other databases.",
                    "label": 0
                },
                {
                    "sent": "So as I mentioned, for instance with popcorn that like, you should be like careful with some data you might get.",
                    "label": 0
                },
                {
                    "sent": "If you are like not absolutely sure that they were curated, and then there are lots of different databases with lots of different informations about drug target interactions, but often like you will not know, can you actually put them together like how different the different assets were that generated the data.",
                    "label": 0
                },
                {
                    "sent": "So like the high level aim with the drug, target Commons is to actually accomplish this, kind of like standardization, harmonization of the data.",
                    "label": 0
                },
                {
                    "sent": "So it would be easier to get like the data you actually want.",
                    "label": 0
                },
                {
                    "sent": "And be sure about their quality.",
                    "label": 0
                },
                {
                    "sent": "Like when you train your models.",
                    "label": 0
                },
                {
                    "sent": "So right now DTC contains roughly 2 million compounds and most of them are not all but like white men are coming from Campbell, but then with some like additional assay annotations.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we will have a look at kernels and we will start with the drag kernels.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I think you already have heard this yesterday, at least that we can encode a structure of a drug into a vector using the concept of a molecular fingerprint.",
                    "label": 0
                },
                {
                    "sent": "So fingerprint is a vector where each bit represents the presence or absence of a specific sub structure in the molecule.",
                    "label": 1
                },
                {
                    "sent": "As you can see in this very simple example and the binary fingerprints are most common, so exactly the ones like here just way longer.",
                    "label": 0
                },
                {
                    "sent": "We have like more substructures, but there are also versions of the fingerprints that are quantitative.",
                    "label": 0
                },
                {
                    "sent": "Where instead of just like marking that is then substructure present or not, you actually count how many times it is present and you input the count instead of this binary indicator.",
                    "label": 0
                },
                {
                    "sent": "If it's present more than once.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there are lots of different types of fingerprints and the most well known ones are the dictionary based fingerprints where we actually have a predefined fragments and then each one of them Maps to a single bit in the fingerprint vector.",
                    "label": 1
                },
                {
                    "sent": "But then we have also hashed fingerprints where we actually don't use any kind of dictionary and the fragments are generated algorithmically.",
                    "label": 1
                },
                {
                    "sent": "So basically we could for instance consider like a certain fingerprint could consider all kinds of paths up to a predefined number of nine.",
                    "label": 0
                },
                {
                    "sent": "900 non hydrogen atoms, for instance, starting from the source Atom.",
                    "label": 0
                },
                {
                    "sent": "So we would consider all kinds of linear fragments.",
                    "label": 0
                },
                {
                    "sent": "And then another class, also of the Hearst fingerprints would be done instead of looking at the linear patterns like the paths you could actually consider the neighborhood like the environment of an Atom, so those would be circular circular fingerprints that are working quite well and are very popular.",
                    "label": 0
                },
                {
                    "sent": "So there we measure the radius in bonds and we can set this radius to or or tune it idea, little certain radius that we think is the best for for achieving the optimal predictive performance.",
                    "label": 0
                },
                {
                    "sent": "So this fingerprints were based on the 2 dimensional substructures of the molecules, but we.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And also design fingerprints based on the 3D substructures.",
                    "label": 0
                },
                {
                    "sent": "So for instance we could consider the geometric features such as triplets of atoms at a certain distance or torsion angles, for instance between atoms.",
                    "label": 1
                },
                {
                    "sent": "And then once we have the fingerprints or like a certain fingerprint that we would have chosen to war.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Quiz.",
                    "label": 0
                },
                {
                    "sent": "We can compare the fingerprints using Tanimoto kernel and this allows us to determine the similarity between the two molecules.",
                    "label": 1
                },
                {
                    "sent": "So it is based on the size of common substructures of the molecules represented by their fingerprints and you have we have seen it for instance yesterday, but maybe not all of you have known that this is actually a valid kernel function.",
                    "label": 1
                },
                {
                    "sent": "So most of you probably know that this is a similarity measure, but there is a proof in the literature that this is a valid kernel function.",
                    "label": 0
                },
                {
                    "sent": "And another way of computing the similarity kind of similar, but would be that instead of looking at this size of common substructures, we could compare the volumes and kind of check what is the common volume that is not that popular.",
                    "label": 0
                },
                {
                    "sent": "But there are like couple of softwares where you can find this implementation as well.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this was based on the fingerprint, but there are also other ways of how we can compare our chemical compounds, so graph kernels would be quite natural representation because it's like when you look at this example here you can see that actually a chemical structure.",
                    "label": 0
                },
                {
                    "sent": "It naturally looks like a graph and it is a very natural representation for it.",
                    "label": 0
                },
                {
                    "sent": "So basically each Atom represents a note, and an edge indicates a bond between the two atoms, and you can either have a labeled graph or unlabeled one.",
                    "label": 1
                },
                {
                    "sent": "So if you have a labeled one, you could.",
                    "label": 0
                },
                {
                    "sent": "Include the Atom type, so is it oxygen or nitrogen?",
                    "label": 0
                },
                {
                    "sent": "As the note labor, you could have some additional properties as well, and there are actually lots of different drug kernels.",
                    "label": 0
                },
                {
                    "sent": "We will just look at a single example here.",
                    "label": 0
                },
                {
                    "sent": "We just like probably like not the best one, but just to explain and show you an idea of like what are we comparing.",
                    "label": 1
                },
                {
                    "sent": "So I would like to show you a random walk kernel.",
                    "label": 0
                },
                {
                    "sent": "As an example here.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But first, some like.",
                    "label": 0
                },
                {
                    "sent": "Let's recall some basic concepts about graphs.",
                    "label": 0
                },
                {
                    "sent": "We don't need to manage.",
                    "label": 0
                },
                {
                    "sent": "So basically a graph G is a set of nodes or vertices V and edges E, and then we have the adjacency matrix which is indexed by the notes.",
                    "label": 1
                },
                {
                    "sent": "And if two nodes in a graph are connected by an edge, then the corresponding entry in this adjacency matrix is set to 1.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the walk is defined as a sequence of notes that allows repetitions of notes.",
                    "label": 1
                },
                {
                    "sent": "And the work can end on the same note on which it began, or on a different note.",
                    "label": 0
                },
                {
                    "sent": "So now if we want to know what is the number of walks of a certain length length K that exist between all pairs of nodes, it can be computed from the adjacency matrix of the graph by taking it to the power of K, where K was like the length of the walk that we are interested in.",
                    "label": 1
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now the random walk red graph kernel.",
                    "label": 0
                },
                {
                    "sent": "What does it do?",
                    "label": 0
                },
                {
                    "sent": "It computes the number of all pairs of matching walks in a pair of graphs.",
                    "label": 1
                },
                {
                    "sent": "So this could be quite difficult to do as such, but there are some tricks of like that help us to calculate this efficiently.",
                    "label": 0
                },
                {
                    "sent": "So basically what we can do, we can use the concept of the product graphs of the product graph DX and to explain it I will use this example here.",
                    "label": 1
                },
                {
                    "sent": "So we have two graphs, G1 and G2 and their product graph GX.",
                    "label": 0
                },
                {
                    "sent": "And now you can see that they each node in GX has a label that consists of two components and the first one, the first component.",
                    "label": 0
                },
                {
                    "sent": "Always comes from the Note label from the D1 from the first graph and then the second part of the label.",
                    "label": 0
                },
                {
                    "sent": "Index comes from the second graph from G2.",
                    "label": 0
                },
                {
                    "sent": "So for instance, here you can see that notes one and one prime and free and two prime are adjacent in this product graph GX and this is because there exists an edge between nodes one and three in the first graph and one prime and two prime in the second graph.",
                    "label": 0
                },
                {
                    "sent": "So that's how the product graph is constructed by going through all this, all these connections.",
                    "label": 0
                },
                {
                    "sent": "So now the key is that performing a random walk on the product graph GX, it corresponds to performing a simultaneous random walk on the G1 and G2, so that's why it allows us to compare how similar molecules are in terms of this random walks that we can do because you can do it like kind of simultaneously just by using the product graph.",
                    "label": 0
                },
                {
                    "sent": "And then the number of matching walks of an electric length can be calculated from the adjacency matrix of the product graph.",
                    "label": 1
                },
                {
                    "sent": "So AX.",
                    "label": 0
                },
                {
                    "sent": "And there are many other graph kernels.",
                    "label": 0
                },
                {
                    "sent": "There were sign clue.",
                    "label": 0
                },
                {
                    "sent": "Did the reference there.",
                    "label": 0
                },
                {
                    "sent": "If you are.",
                    "label": 0
                },
                {
                    "sent": "If you are interested, but this is this is just one example to give you an idea.",
                    "label": 0
                },
                {
                    "sent": "You could compare the graphs also by looking the common subtrees.",
                    "label": 0
                },
                {
                    "sent": "For instance common paths and so on.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in practice, in many cases using structural information about compounds, it works quite well in predicting drug target interactions, but often it might not be enough, like in cases where we have some some structures that look very similar but at just a minor change in the structure can cause a dramatic change in the activity.",
                    "label": 0
                },
                {
                    "sent": "So this is 1 example.",
                    "label": 0
                },
                {
                    "sent": "For instance morphine and heroin.",
                    "label": 0
                },
                {
                    "sent": "They are very structurally similar, but drugbank lists for targets for both and three are common, but there's one target that is actually different.",
                    "label": 0
                },
                {
                    "sent": "Even though the structures look so similar and this is the case with many different drugs, not just this.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's why it would be worth also to think of some additional data sources and that we could that we could use to compute the similarities between our compounds.",
                    "label": 0
                },
                {
                    "sent": "So for instance, using the common side effects to compare to molecules, we could also take advantage of this ATC classification system, which basically divides the drugs into groups at 5 different levels, and this depends on the organ or system on which the drug acts, as well as therapeutic and chemical characteristics, so it's kind of like maybe like gene ontologies for.",
                    "label": 0
                },
                {
                    "sent": "Protein is not exactly but some kind of system like this for drugs.",
                    "label": 0
                },
                {
                    "sent": "Another interesting data resource would be gene expression responses to drugs so we could calculate the correlation of these different gene expression responses or apply a Goshen kernel on top of that.",
                    "label": 0
                },
                {
                    "sent": "And for instance, connectivity MAP project is 1 example of the resource where this kind of data could be extracted from.",
                    "label": 0
                },
                {
                    "sent": "And also what I find very powerful are like actually using the binding affinity profiles of different drugs and the only disadvantage with that of course like in many cases we will not have this kind of data available.",
                    "label": 0
                },
                {
                    "sent": "We might be interested in some drugs that are like very new molecules and we cannot take advantage of this resource.",
                    "label": 0
                },
                {
                    "sent": "But I mean once it's available, if you are like working with some molecules acquire well known and maybe kind of like fill in some missing entries.",
                    "label": 0
                },
                {
                    "sent": "So that's quite powerful for calculating similarities because then even if you have this like.",
                    "label": 0
                },
                {
                    "sent": "Minor structural differences that cause very big changes to the activity.",
                    "label": 0
                },
                {
                    "sent": "This will kind of handle this problem because it will directly look at the binding affinity profiles, so directly add activities and here you could either like compared to binary profiles, or like actually like looking at the binding affinity's.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we will move on to some examples of protein kernels.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So protein is built of amino acids, so the most natural way to compare proteins is of course just to compare the strings of amino acids and the standard approach in case of drug protein interaction prediction is to do the Smith Waterman local alignment and this is not a valid kernel actually.",
                    "label": 0
                },
                {
                    "sent": "Actually even though in some publications you might kind of say this kernels, but just for you to know that it's like as such it's not a valid kernel function, but of course we could like apply a kernel function on top of this, calculated Smith, Waterman.",
                    "label": 0
                },
                {
                    "sent": "Horse",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But this was like very like standard approach.",
                    "label": 0
                },
                {
                    "sent": "My like one of my or my favorite, probably protein Colonel is this generic string kernel.",
                    "label": 0
                },
                {
                    "sent": "I haven't developed it.",
                    "label": 0
                },
                {
                    "sent": "I have nothing to do with it, so it's not the reason why it's my favorite, but So what?",
                    "label": 0
                },
                {
                    "sent": "Is kernel is the wink.",
                    "label": 0
                },
                {
                    "sent": "It compares the protein sequences, but accounting for amino acid properties and it also has this nice property that it would allow you to March 2 amino acid sub subsequences even if their positions in the full protein sequence differ quite a lot.",
                    "label": 1
                },
                {
                    "sent": "So it is kind of like an advanced alignment.",
                    "label": 0
                },
                {
                    "sent": "So DS Kernel will compare each substring of protein X of size L with each substring of ex prime having the same length.",
                    "label": 0
                },
                {
                    "sent": "And we need to decide what is the maximum substring length that we are comparing.",
                    "label": 0
                },
                {
                    "sent": "Which is this capital L. And this is just the hyperparameter that we need to tune.",
                    "label": 0
                },
                {
                    "sent": "So each comparison will result in a score that will depend on the shifting contribution term, which is basically just the difference in the position of two substrings into amino acid sequences to proteins, annex annex prime.",
                    "label": 1
                },
                {
                    "sent": "So basically this inj are the indices like kind of indicating the position you are just comparing, like how different they are they are.",
                    "label": 0
                },
                {
                    "sent": "And basically then we have.",
                    "label": 0
                },
                {
                    "sent": "Then we have another hyperparameter here that we need to tune the Sigma P and the Sigma Phi.",
                    "label": 0
                },
                {
                    "sent": "What it is doing it basically if you if you set it to a large value then basically you allow longer shifts.",
                    "label": 0
                },
                {
                    "sent": "So we are kind of like saying that like maybe the two subsequences are like very far apart but you really generally want to match short subsequences like regardless of their position.",
                    "label": 0
                },
                {
                    "sent": "So generally would like to set it to a very high value.",
                    "label": 0
                },
                {
                    "sent": "But of course in general we don't really know.",
                    "label": 0
                },
                {
                    "sent": "So we like we need to tune this hyperparameters just the intuition behind it like how changing this parameter.",
                    "label": 0
                },
                {
                    "sent": "Allows you to control like world will be considered similar or dissimilar advent.",
                    "label": 0
                },
                {
                    "sent": "And then the second term is then basically just the difference between properties of amino acids that are included in the two substrings that you are currently comparing.",
                    "label": 0
                },
                {
                    "sent": "And this is controlled by the Sigma C parameter and basically the intuition behind it is very similar that we are just like changing this value of this parameter to control what we will consider to be similar and what we consider to be this similar at the end kind of and we of course need to tune it again.",
                    "label": 0
                },
                {
                    "sent": "So the kernel then at the end it the equation is kind of like long maybe and complicated, but basically a dent.",
                    "label": 0
                },
                {
                    "sent": "The kernel just outputs the sum of this course from all the possible substrings comparison, so it will compare all possible substrings and it will check the difference in their positions in the full sequence.",
                    "label": 1
                },
                {
                    "sent": "It will check how different their amino acid properties are, depending you can choose what kind of properties you want to use and then at the end.",
                    "label": 0
                },
                {
                    "sent": "Once like all kind of substrings were compared, then you just like have a sum of all of them.",
                    "label": 0
                },
                {
                    "sent": "So the shifting contribution term, like we already kind of went through, is just the difference in the positions.",
                    "label": 0
                },
                {
                    "sent": "But maybe we can have a look at what actually we have in the second term.",
                    "label": 0
                },
                {
                    "sent": "So the idea is just that each type of amino acid, for instance alanine, it has a corresponding feature vector which defines its properties.",
                    "label": 0
                },
                {
                    "sent": "So we could consider, let's say hydrophobicity, volume, polarity.",
                    "label": 0
                },
                {
                    "sent": "Like anything else you can find, for instance this AA index database.",
                    "label": 0
                },
                {
                    "sent": "It contains like hundreds of or thousands.",
                    "label": 0
                },
                {
                    "sent": "Even like different properties of amino acids.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then once you have a string, this fee deci El that is in the equation above in the GS kernel, it is encoding function that concatenates L vectors that are.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Scribing each amino acid this string is composed of.",
                    "label": 0
                },
                {
                    "sent": "So you're basically like once you have the properties this function, it concatenates it into a vector and then that's what's comparative then.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then again, like I mean, acid sequences are of course I'm nice resource, but there are lots of other information that we should consider.",
                    "label": 0
                },
                {
                    "sent": "So especially like protein binding site and protein binding sites and protein surfaces are quite important in the context of this interactions.",
                    "label": 0
                },
                {
                    "sent": "So for instance, you could think about like for instance, kinase inhibitors, so most of them binds to ATP binding pockets of the proteins, so it might not be really important that overall two amino acid sequences or structures are like very similar.",
                    "label": 0
                },
                {
                    "sent": "If, for instance, there ATP binding pocket subsequences like or dislike, if you compare structures like in Freddy.",
                    "label": 0
                },
                {
                    "sent": "If if they are different than actually most likely the outcome the binding of a certain drug will be quite different as well, so that's why like focusing on the for instance the binding sites or just subsequences of the binding sites and even using some simple kernel like the Smith Waterman or like like using Smith Waterman to calculate the kernel.",
                    "label": 0
                },
                {
                    "sent": "It could also be a good idea.",
                    "label": 0
                },
                {
                    "sent": "And we could, for instance, calculate also similarities using Gene Ontology annotations of the of the proteins, and as such I don't expect it to be like very predictive alone, but in combination with other data sources it can add some orthogonal information.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we will have a look at the algorithms.",
                    "label": 0
                },
                {
                    "sent": "So we have a couple of very basic slides, so if you have machine learning background then just please bear with me but just sounded like we're on.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Same same page, so drug, protein binding affinity prediction is a supervised regression problem and the notation we will be using is that X is a space of inputs, so our drug protein purse.",
                    "label": 0
                },
                {
                    "sent": "Why is there is a space of output so binding of an interest in this case, then we have a set of models G mapping input to output some training data set which we markers as here and the loss function that measures.",
                    "label": 0
                },
                {
                    "sent": "What is the difference between the outputs that are predicted by the model?",
                    "label": 0
                },
                {
                    "sent": "And the original output, so the binding affinity is here.",
                    "label": 0
                },
                {
                    "sent": "So the goal here is to find such a model gene that minimizes the expected loss on future drug protein pairs.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But if we would rely on minimizing the loss only, it could lead to overfitting to the particular training data.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for instance, you can see here do you want fits almost perfectly to this toy training data, but it describes the noise instead of like the actual underlying relationship.",
                    "label": 0
                },
                {
                    "sent": "And on that.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hand due to it would have a higher associated loss, but it is simpler, and it generalizes better too.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The future observations that we will get and this is generally the ultimate goal of machine learning.",
                    "label": 0
                },
                {
                    "sent": "So what do we do to avoid the over?",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think so.",
                    "label": 0
                },
                {
                    "sent": "We at regularization term to our objective function.",
                    "label": 0
                },
                {
                    "sent": "And here in this equation, Omega G is the function whose value will increase with the increasing complexity of the model, and we also have Lambda, which is the regularization parameter.",
                    "label": 0
                },
                {
                    "sent": "Again, the parameter that we need to tune, we don't know apriori or still like what would be the optimal value for it.",
                    "label": 0
                },
                {
                    "sent": "But the role of this parameter is to control the balance between the training error and the model complexity.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we will be focusing here on the linear functions of this following form, where W is the vector of model parameters that is found by minimizing the structuralized empirical risk that we just saw.",
                    "label": 0
                },
                {
                    "sent": "And then the choice of this loss function and the regularization.",
                    "label": 0
                },
                {
                    "sent": "It basically determines the learning algorithm that you get.",
                    "label": 0
                },
                {
                    "sent": "So for instance, if you have them hinge loss and the squared Euclidean norm of W, which is known as well to regular regularizer, you get the support vector machine algorithm.",
                    "label": 0
                },
                {
                    "sent": "Then if you keep the same regularizer, but you actually change the loss function into the squared loss, then you get rich regression algorithm and we will be looking.",
                    "label": 0
                },
                {
                    "sent": "We will be continuing with Ridge regression and then rich regression with kernels.",
                    "label": 0
                },
                {
                    "sent": "And then if you would keep the same loss function but change the regularization to L1 norm, you get the last algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is just very quickly.",
                    "label": 0
                },
                {
                    "sent": "For those of you who don't have like machine learning background, but just to see like how how this is done.",
                    "label": 0
                },
                {
                    "sent": "So if we have this squared loss, we decided on our squared loss and on the regularizer.",
                    "label": 0
                },
                {
                    "sent": "So here the quadratic regularizer we just write the optimization problem as follows and we can also write it like that's the same equation just in the matrix vector notation.",
                    "label": 0
                },
                {
                    "sent": "And then what we actually want to find to train our model.",
                    "label": 0
                },
                {
                    "sent": "Here it is, we would like to know what is our W. What are the optimal, what is the?",
                    "label": 1
                },
                {
                    "sent": "Optimal W. So how we do it?",
                    "label": 0
                },
                {
                    "sent": "We take the derivative risk with respect to W and we set it equal to the zero vector.",
                    "label": 0
                },
                {
                    "sent": "Then we basically just need to solve it and we got our closed form solution for W. But even though we are focusing on linear models that I showed you, so we had this in the previous slide.",
                    "label": 0
                },
                {
                    "sent": "This was there in red in the model.",
                    "label": 0
                },
                {
                    "sent": "So it's a simple linear model, but we will use kernels in order to look for nonlinear patterns in the data.",
                    "label": 1
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we do now is that so first of all, the optimal W. It can be written as a linear combination of the training examples and this equation follows from the representative forum, but for now it's just important that you that you see this equation and you see that by introducing this so called do our variables Alpha.",
                    "label": 1
                },
                {
                    "sent": "We can basically write optimal W as a combination of the training examples that you have, such as drug, protein, purse.",
                    "label": 0
                },
                {
                    "sent": "And then we just basically plug it into the previous equation that we saw and we see that we can represent the model predictions in terms of inner product of training examples.",
                    "label": 0
                },
                {
                    "sent": "And this means that we can use kernels because we saw the definition of kernel earlier involving the inner product.",
                    "label": 0
                },
                {
                    "sent": "And basically the interpretation behind this, like important parameters here, so W and Alpha.",
                    "label": 0
                },
                {
                    "sent": "So W it was actually informing about like the importance of each of the features that we have as our inputs.",
                    "label": 0
                },
                {
                    "sent": "So like if you would actually look at that what you have in W would get the importance of the features for a given prediction task.",
                    "label": 0
                },
                {
                    "sent": "Whereas in in case of kernels we are not anymore looking at the features but Alphas would give you the importance of each of the training examples.",
                    "label": 0
                },
                {
                    "sent": "So we had N training examples.",
                    "label": 0
                },
                {
                    "sent": "Alpha will be a vector of length an an it will like show you how important each drug protein per was.",
                    "label": 0
                },
                {
                    "sent": "Like for making this prediction.",
                    "label": 0
                },
                {
                    "sent": "So just different way how we are interpreting W and Alpha.",
                    "label": 0
                },
                {
                    "sent": "So Alpha is in the dual space.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now I'm moving on from this from basically what we had.",
                    "label": 0
                },
                {
                    "sent": "So from the retrogression moving to this dual space and this will form the kernel Ridge regression algorithm.",
                    "label": 0
                },
                {
                    "sent": "So just to get this solution you could use one of these equations that we have seen previously.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we just rewrite it in terms of W and then again we just need to solve it so we get the solution closed form solution for Alpha.",
                    "label": 0
                },
                {
                    "sent": "So it's very simple and then again we see the inner product of training examples.",
                    "label": 0
                },
                {
                    "sent": "Here there's X transpose times X.",
                    "label": 0
                },
                {
                    "sent": "So that basically means that we can use kernels so rich regression with kernels like this is called Kernel Ridge regression algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now the annotation in this our drug protein binding affinity prediction problem would be that we have any different drugs and P different proteins which form together a set of end training examples.",
                    "label": 0
                },
                {
                    "sent": "So drug, protein, purse.",
                    "label": 0
                },
                {
                    "sent": "And we have also real values indicating what is the strength of their interaction.",
                    "label": 0
                },
                {
                    "sent": "So the binding affinity Y and then we have also a pairwise kernel matrix K. So now the question is, what is actually the pairwise kernel K because we worked so far with the drug kernels and the protein kernels.",
                    "label": 0
                },
                {
                    "sent": "But we haven't seen yet the pairwise kernel K. So it is.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we expect wise Colonel.",
                    "label": 0
                },
                {
                    "sent": "It measures the similarity between drug protein pairs and it is typically calculated as a Chronicle product between drug kernel and the protein kernel.",
                    "label": 0
                },
                {
                    "sent": "And as a reminder of.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the Chronicle.",
                    "label": 0
                },
                {
                    "sent": "Product is it's when we take a Chronicle product of two matrices.",
                    "label": 0
                },
                {
                    "sent": "We basically did the resulting matrix.",
                    "label": 0
                },
                {
                    "sent": "It contains all possible products of entries that we had in the matrices that we are multiplying.",
                    "label": 0
                },
                {
                    "sent": "But now The thing is so OK, Now we know what is the what is the third wise kernel.",
                    "label": 0
                },
                {
                    "sent": "And this is the kernel that we need.",
                    "label": 0
                },
                {
                    "sent": "But now what is?",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The problem is that the size of a pairwise kernel it increases very quickly with the number of drugs and proteins that we have.",
                    "label": 0
                },
                {
                    "sent": "So for instance, if we have just hundred drugs and 100 different proteins that we want to work with, the corresponding pairwise kernel matrix, it would have 100 million entries and takes approximately 1 gigabyte of memory.",
                    "label": 0
                },
                {
                    "sent": "Then if we just like double this, I'm in terms of drugs and targets.",
                    "label": 0
                },
                {
                    "sent": "So if we have 200 drugs and 200 proteins, then your pairwise kernel would have 1.6 billion entries and it will take roughly 12 gigabytes of memory.",
                    "label": 0
                },
                {
                    "sent": "And that's just a single kernel.",
                    "label": 0
                },
                {
                    "sent": "So basically this makes the model training infeasible in practice.",
                    "label": 0
                },
                {
                    "sent": "In typical applications, in terms of both memory and computational power time that you would need to do it, so it's so it's it's.",
                    "label": 0
                },
                {
                    "sent": "It's very inconvenient to work with pairwise kernel directly and just plug it to the standard.",
                    "label": 0
                },
                {
                    "sent": "Colonel Regression algorithm in this case.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But Luckily it is possible to use the algebraic properties of the Chronicle product to speed up the training of the model.",
                    "label": 0
                },
                {
                    "sent": "So here you can see we had the we have the close form solution to Colonel introgression, that we sell, and we are just like rewriting it.",
                    "label": 0
                },
                {
                    "sent": "Of course, here we are missing, like multiple lines of derivation, but just to show you like what's the what's the final equation that doesn't involve anymore any pairwise matrix K?",
                    "label": 0
                },
                {
                    "sent": "We only have the components of the smaller matrices of drug kernels and protein kernels, and this started with doing the egg and the composition of the kernel matrices.",
                    "label": 0
                },
                {
                    "sent": "So here then.",
                    "label": 0
                },
                {
                    "sent": "Why now it's not even anymore vector?",
                    "label": 0
                },
                {
                    "sent": "It's a matrix that stores the binding affinity between your drugs as rose and between proteins indexed.",
                    "label": 0
                },
                {
                    "sent": "So we have them as columns in the matrix Y.",
                    "label": 0
                },
                {
                    "sent": "Then the QD is the matrix containing the eigenvectors of drug, kernel and QP.",
                    "label": 0
                },
                {
                    "sent": "Similarly, same matrix containing the eigenvectors of the protein kernel and that lamp does contain the eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "So we can see like so in the final equation we can see that there is still like one Chronicle product left, which is this one.",
                    "label": 0
                },
                {
                    "sent": "But actually we can write it a bit differently to avoid like taking any kinds of Chronicle products to avoid like having this like working in the large spaces.",
                    "label": 0
                },
                {
                    "sent": "And basically it comes from the fact that the Kronecker product of 2 diagonal matrices is also diagonal matrix.",
                    "label": 0
                },
                {
                    "sent": "So you can see that we completely avoid the computations of any pairwise kernels, and we work in much smaller spaces of drugs and proteins.",
                    "label": 0
                },
                {
                    "sent": "But there is still one caveat here.",
                    "label": 0
                },
                {
                    "sent": "So basically why the matrix Y with your drug protein binding affinity's it cannot have missing values.",
                    "label": 0
                },
                {
                    "sent": "For this to work, the matrix that you use in the training.",
                    "label": 0
                },
                {
                    "sent": "And of course, in practice you will often see missing values.",
                    "label": 0
                },
                {
                    "sent": "So there are two different approaches here now.",
                    "label": 0
                },
                {
                    "sent": "So if you have depends on what kind of problem you work with.",
                    "label": 0
                },
                {
                    "sent": "If the number of missing values is not due to huge, so maybe like 10% of missing values in the matrix Y, then you could just use some matrix imputation technique to fill in this matrix and then you can like work with it.",
                    "label": 0
                },
                {
                    "sent": "But if you have quite a lot of missing values, if this is very sparse then there is still a way to speed up the model training.",
                    "label": 1
                },
                {
                    "sent": "This is described in this second reference at the bottom, so the difference is like that.",
                    "label": 0
                },
                {
                    "sent": "In that case we will not have anymore a closed form solution as you have here, but you could still use efficient solver of the system of linear equation of linear equations and this is combined with the generalized form of electric, that is, that is introduced in this in this paper below.",
                    "label": 0
                },
                {
                    "sent": "So this will be still this will be not as fast and efficient at this algorithm that has a closed form solution.",
                    "label": 0
                },
                {
                    "sent": "But if if you have too many missing values in your in your matrix and you cannot impute them, that is still a good idea to use it because it will be way faster than moving to the pairwise space directly because often in the pairwise space actually like it will be just like tool computationally heavy to get any kind of solution.",
                    "label": 0
                },
                {
                    "sent": "So this is still a very good algorithm to work with.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I have shown you that there are actually like multiple different ways to measure similarities between molecules between drugs and proteins.",
                    "label": 0
                },
                {
                    "sent": "So we actually easily get not just one kernel for drugs and not just one kernel for proteins, but we have multiple drug kernels and multiple protein kernels, which then in turn leads to even more pairwise kernels.",
                    "label": 0
                },
                {
                    "sent": "Because these are like all combinations between dragon protein kernels.",
                    "label": 0
                },
                {
                    "sent": "So the classical algorithms there rely only on a single pairwise kernel.",
                    "label": 0
                },
                {
                    "sent": "So typically what you would do is that you would run a single experiment for each combination of a drug kernel and pro in kernel that you have, and then you would like repeat it for all different kernels that you have.",
                    "label": 0
                },
                {
                    "sent": "And basically you would have.",
                    "label": 0
                },
                {
                    "sent": "You would then based on the predicted performance like for instance from the cross validation you could select what is the best kernels?",
                    "label": 0
                },
                {
                    "sent": "What is the best model that you want to use for making then some predictions that you are actually interested in.",
                    "label": 0
                },
                {
                    "sent": "But this is of course kind of, not maybe the optimal because you have to run so many different experiments, whereas with multiple kernel learning you have all of this in a single experiment and you can learn what is the importance of each of the input kernels.",
                    "label": 0
                },
                {
                    "sent": "And of course, another thing also is that if you are using, so yeah there will be this cases when maybe there is like very good predictive drag kernel and very good predictive protein kernel.",
                    "label": 0
                },
                {
                    "sent": "And just like they give you awesome results and you actually don't need to think of anything else there, the kovit is just need to run all the experiments to like find what is this kernel.",
                    "label": 0
                },
                {
                    "sent": "But then in many cases it could also be that actually you want to actually want to integrate all kinds of information sources that you have.",
                    "label": 0
                },
                {
                    "sent": "So just taking advantage of all different data types that you have soda side effects for instance and.",
                    "label": 0
                },
                {
                    "sent": "Gene expression responses and also the different fingerprints and so on.",
                    "label": 0
                },
                {
                    "sent": "So basically like you just want to integrate them to achieve a better predictive performance which was shown in the literature especially recently.",
                    "label": 0
                },
                {
                    "sent": "Then this data integration makes a lot of sense and can improve the predictive performance.",
                    "label": 0
                },
                {
                    "sent": "So multiple kernel learning methods.",
                    "label": 0
                },
                {
                    "sent": "They search for an optimal combination of several kernels and they allow us to use information different information sources simultaneously, and they also learn the importance of this information for the prediction task.",
                    "label": 0
                },
                {
                    "sent": "So basically here you can see in the equation.",
                    "label": 0
                },
                {
                    "sent": "So now our goal is to learn this optimal kernel weights mu to combine our kernels.",
                    "label": 0
                },
                {
                    "sent": "And broadly, this different MCL methods.",
                    "label": 0
                },
                {
                    "sent": "They could be divided into one stage and two stage techniques.",
                    "label": 0
                },
                {
                    "sent": "So in the one stage technique it would be designed in such a way that we are learning the optimal kernel combination and model parameters such that this Alpha.",
                    "label": 0
                },
                {
                    "sent": "For instance, in kernel shagrat and that we learn them kind of simultaneously, but in the two statements we first find the optimal kernel weights and only then, given the optimal kernel, we train a classifier or regressor.",
                    "label": 0
                },
                {
                    "sent": "And here we will go through.",
                    "label": 0
                },
                {
                    "sent": "I would like to focus on this two stage multiple kernel learning.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Because I think this modularity is very nice.",
                    "label": 0
                },
                {
                    "sent": "So basically, once you have constructed your optimal optimal kernel, it can be used with different machine learning models, so maybe you would like to try support vector regression or kernel Ridge regression so you have the kernel you can.",
                    "label": 1
                },
                {
                    "sent": "You can like use it for different applications and for instance in the second stage you could have this kernel Ridge regression.",
                    "label": 1
                },
                {
                    "sent": "I mean not directly kernel regression but for instance the algorithm that we just like looked at before the other one in the that I mentioned the reference of something efficient.",
                    "label": 0
                },
                {
                    "sent": "It can still work with pairwise kernels.",
                    "label": 1
                },
                {
                    "sent": "But now let's have a look at the first stage that we haven't discussed yet.",
                    "label": 0
                },
                {
                    "sent": "So how to actually find this optimal kernel weights and what's the intuition behind this?",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Saving the most popular two state MCL, Methodist align F and the idea of a line F. What is what it does is to maximize the so called centered alignment between the optimal combined kernel an so called ideal response kernel Ky.",
                    "label": 0
                },
                {
                    "sent": "So now we will have a look what does it actually mean?",
                    "label": 0
                },
                {
                    "sent": "So we have our labels our binding affinity's between drug protein pairs and now first of all what we do we calculate this so-called response kernel based on them.",
                    "label": 0
                },
                {
                    "sent": "And now with this kernel tells you is basically how similar your drug protein pairs are in terms of their binding affinity's and why this kernel is important here.",
                    "label": 0
                },
                {
                    "sent": "And why is it kind of called ideal is because like this is your target.",
                    "label": 0
                },
                {
                    "sent": "This is actually what you want to predict, so you want to have flag such similarities like in your inputs you want to try to find like such a way of measuring the similarities that it would give you basically the same information about your drug protein pairs.",
                    "label": 0
                },
                {
                    "sent": "Where does this come from?",
                    "label": 0
                },
                {
                    "sent": "Which part from there from the labels?",
                    "label": 0
                },
                {
                    "sent": "So you calculate it from the equation.",
                    "label": 0
                },
                {
                    "sent": "Actually will be also later, but this is calculated from the labels that you have in the training data.",
                    "label": 0
                },
                {
                    "sent": "So this is this is a vector, for instance, like here it could be in a matrix form, but let's think that now each entry in this label in this label vector Y.",
                    "label": 0
                },
                {
                    "sent": "It is each entry corresponds to the binding affinity of a certain drug protein per.",
                    "label": 0
                },
                {
                    "sent": "So for instance, the first entry would be.",
                    "label": 0
                },
                {
                    "sent": "The first entry would be imatinib and BCR ABL, and then we have some some binding affinity, for instance KD value or like dissociation constant KD or something else measured in the lab.",
                    "label": 0
                },
                {
                    "sent": "So this is this area.",
                    "label": 0
                },
                {
                    "sent": "Training data or labels that you work with.",
                    "label": 0
                },
                {
                    "sent": "And from that we calculate this kernel Ky.",
                    "label": 0
                },
                {
                    "sent": "Yes, this answer.",
                    "label": 0
                },
                {
                    "sent": "The question right there.",
                    "label": 0
                },
                {
                    "sent": "So yeah, maybe like.",
                    "label": 0
                },
                {
                    "sent": "Yes, the recently so we know why aren't labels that you like typically have in your machine learning.",
                    "label": 0
                },
                {
                    "sent": "So like for instance if you want to predict divining companies, we can drive some proteins and what you need is to have some training data first of all, and you can expect This is why is the vector of target players?",
                    "label": 0
                },
                {
                    "sent": "Yes, yes, yes yes.",
                    "label": 0
                },
                {
                    "sent": "I just don't like to put target experience propane partners.",
                    "label": 0
                },
                {
                    "sent": "It could be the cycle response, but yes, it's like the target value.",
                    "label": 0
                },
                {
                    "sent": "Multi multi target impact or multi response.",
                    "label": 0
                },
                {
                    "sent": "Yes, so basically this would be amazing, because if you imagine we have like 100 drugs and hundred properties, you actually have a matrix of like all possible landing amenities between them.",
                    "label": 0
                },
                {
                    "sent": "This is now just like the regular footwear breakfast sandwiches vectorizes to have like more.",
                    "label": 0
                },
                {
                    "sent": "Understandable representation, kind of, but basically this will contain binding communities of all drug protein pairs that you have in your training.",
                    "label": 0
                },
                {
                    "sent": "Event is much dependence.",
                    "label": 0
                },
                {
                    "sent": "You can find him some publicly available data resources, so based on that.",
                    "label": 0
                },
                {
                    "sent": "So this is a vector.",
                    "label": 0
                },
                {
                    "sent": "We can calculate the Colonel so we have.",
                    "label": 0
                },
                {
                    "sent": "So here the roles were indexed on the elements of the vector were indexed by the drop protein powders and also here there will be indexed based on both rows and columns will be indexed by dropping pairs because we want to learn now how similar the drug propane spur R, but in terms of the.",
                    "label": 0
                },
                {
                    "sent": "Mining companies.",
                    "label": 0
                },
                {
                    "sent": "So this would be kind of the ideal Colonel Cousins who want to predict.",
                    "label": 0
                },
                {
                    "sent": "So like this idea because he wants to printing the binding up on this.",
                    "label": 0
                },
                {
                    "sent": "So if you are like I mentioned earlier that the high level assumption is that similar drugs are interacting with similar proteins or the similar drug propane purse will have similar binding armies.",
                    "label": 0
                },
                {
                    "sent": "So then the whole big question is like how you actually represent the similarities.",
                    "label": 0
                },
                {
                    "sent": "What is the optimal drug kernel?",
                    "label": 0
                },
                {
                    "sent": "What is optimal propane kernel?",
                    "label": 0
                },
                {
                    "sent": "But at the end, like they really optimal one for this prediction.",
                    "label": 0
                },
                {
                    "sent": "Is what it would actually calculate from your labels.",
                    "label": 0
                },
                {
                    "sent": "So this is what you want to predict.",
                    "label": 0
                },
                {
                    "sent": "So you can measure the similarity in terms of what you want to predict.",
                    "label": 0
                },
                {
                    "sent": "It's kind of like optimal, but now we'd like.",
                    "label": 0
                },
                {
                    "sent": "Some of you might think if you don't have machine learning background, so we have.",
                    "label": 0
                },
                {
                    "sent": "This cable is so awesome, why don't we actually like use it for making the prediction directly?",
                    "label": 0
                },
                {
                    "sent": "And the answer to that question is that this one is only available for the training data, so we can learn from this kernel.",
                    "label": 0
                },
                {
                    "sent": "But actually we cannot use it.",
                    "label": 0
                },
                {
                    "sent": "This Patch for like maybe the connection.",
                    "label": 0
                },
                {
                    "sent": "So that's why I like this idea.",
                    "label": 0
                },
                {
                    "sent": "But like we can only learn from it, we cannot like use it like with the tiny motor kernel and the GS kernel.",
                    "label": 0
                },
                {
                    "sent": "You can calculate it for the drugs and proteins for which we don't have anybody coming in today to because he was before did drugs encodings.",
                    "label": 0
                },
                {
                    "sent": "You're interested, but you will not have much finding out new data, but you can still use their chemical structures there.",
                    "label": 0
                },
                {
                    "sent": "I mean ask sequences there side effects if there are and so on.",
                    "label": 0
                },
                {
                    "sent": "So we finished all of that, but unfortunately this funding will usually don't have something, so that's why we only use as part of their.",
                    "label": 0
                },
                {
                    "sent": "But not as much for making the predictions.",
                    "label": 0
                },
                {
                    "sent": "Does anyone have any more questions about this?",
                    "label": 0
                },
                {
                    "sent": "Because it's important to understand, like I mean, it's nice if you understand the concept for the rest of the talk is then kind of like based on this type of methods.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "But they're so like the first step that we is that we utilized our labels.",
                    "label": 0
                },
                {
                    "sent": "Our binding affinity is to calculate this response kernel.",
                    "label": 0
                },
                {
                    "sent": "It can also be called target.",
                    "label": 0
                },
                {
                    "sent": "Actually, in the literature is called target kernel Ky and then basically what we want to achieve is to compare this kernel and either the input kernel or here in this algorithm directly the kernel that will be our final combined kernels of the kernel that he will combine from all other maybe hundreds or thousands.",
                    "label": 0
                },
                {
                    "sent": "Of drug and protein kernels that you had.",
                    "label": 0
                },
                {
                    "sent": "So basically what we want to do is to maximize the similarity.",
                    "label": 0
                },
                {
                    "sent": "Choose the way it's mu in such a way that the similarity between this kernel came.",
                    "label": 0
                },
                {
                    "sent": "You the final combined kernel came you and the response Kernel Ky would be maximized, so that's why we have Arg Max in the equation an otherwise this A indicates the alignment and this alignment.",
                    "label": 0
                },
                {
                    "sent": "It basically just measures the similarity between the two matrices.",
                    "label": 0
                },
                {
                    "sent": "It can be interpreted as a cosine of the angle that is defined between two matrices.",
                    "label": 0
                },
                {
                    "sent": "So just a normalized tribunas product.",
                    "label": 0
                },
                {
                    "sent": "But yes, the whole the whole concept is like about about maximizing the similarity between these two matrices that you see here.",
                    "label": 0
                },
                {
                    "sent": "And based on that, which was our optimal weights and now another thing.",
                    "label": 0
                },
                {
                    "sent": "So like in the definition earlier I mentioned centered alignment so you can see that the kernel that we have in the equation on when we maximize the the kernel weights mule, we have actually Casey and Casey means it's a centered kernel.",
                    "label": 0
                },
                {
                    "sent": "And here is the definition.",
                    "label": 0
                },
                {
                    "sent": "Here it is we you can see how do we actually center Colonel.",
                    "label": 0
                },
                {
                    "sent": "So what it means is in practice is that what we are doing is actually we are centering.",
                    "label": 0
                },
                {
                    "sent": "The feature mapping that is associated with the kernel and in terms of the equation you just take this matrix K and you your kernel K and you multiply it from left and right hand side with the centering operator C. And in the indication for the centering operator, we just have the identity matrix of the same size as your kernel and then one is a vector of all components all equal to 1.",
                    "label": 0
                },
                {
                    "sent": "But here's the thing to remember is that basically it's about centering the associated feature mapping the future mapping associated with the kernel.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now we have this optimization problem.",
                    "label": 0
                },
                {
                    "sent": "We just we saw it in the previous slide and it can actually be solved quite efficiently by using the simple quadratic programming that you see here.",
                    "label": 0
                },
                {
                    "sent": "And to solve it we need to calculate vector A and symmetric matrix M and Now what?",
                    "label": 0
                },
                {
                    "sent": "What this what this R?",
                    "label": 0
                },
                {
                    "sent": "So the vector a it contains this.",
                    "label": 0
                },
                {
                    "sent": "Normally it contains the Frobenius products between each input kernel that you have and this response kernel Ky.",
                    "label": 0
                },
                {
                    "sent": "So the similarities between the each input kernel.",
                    "label": 1
                },
                {
                    "sent": "And the ideal response kernel and the matrix M contains all purse of similarities between your input kernels and discard the pairwise pairwise kernels.",
                    "label": 1
                },
                {
                    "sent": "So this is how we solve it.",
                    "label": 0
                },
                {
                    "sent": "So once you have calculated your vector A and your matrix M, so you know all kinds of similarities between your matrices, then you can just use them.",
                    "label": 0
                },
                {
                    "sent": "You just like use the solver for this.",
                    "label": 0
                },
                {
                    "sent": "So this is generally like very very simple procedure.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But now we have the same problem as before that we are right now we are working with the pairwise kernels and the same problem in terms of the like memory usage and the training time.",
                    "label": 0
                },
                {
                    "sent": "We cannot really like train it as such and you can hear see, hear that really.",
                    "label": 0
                },
                {
                    "sent": "We've already very small number of drugs and proteins.",
                    "label": 0
                },
                {
                    "sent": "This becomes infeasible in terms of processing and memory requirements.",
                    "label": 0
                },
                {
                    "sent": "So now what are there?",
                    "label": 0
                },
                {
                    "sent": "There's still a way to solve it.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So basically we have introduced this pairwise MCL algorithm, which is just a lineup for permits kernels I would say.",
                    "label": 0
                },
                {
                    "sent": "So the whole the goal here was to work with the drag kernels and the protein kernels.",
                    "label": 0
                },
                {
                    "sent": "So work in much smaller spaces of drugs and proteins instead of working with this huge pairwise space.",
                    "label": 0
                },
                {
                    "sent": "So this is a dent like the overview of how it looks like that you actually don't have to work in the pairwise space at all.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is now like showing that you can actually then calculate it very efficiently, like it kind of takes as much time as if you would be working with smaller drug kernels only or protein kernels only in more like standard, not pairwise learning scenarios.",
                    "label": 0
                },
                {
                    "sent": "And now.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Like couple of technical details for those of you that are interested.",
                    "label": 0
                },
                {
                    "sent": "So kind of the main bottleneck in using this align F was that it requires this entering of the kernel because basically I mean for those of you who kind of like maybe know a bit more about Chronicle products and so on.",
                    "label": 0
                },
                {
                    "sent": "Maybe you could easily notice from the previous equations that if there wouldn't be a centering then you could actually again use some properties of the Chronicle product, some algebraic properties to like rewrite the equations in such a way that.",
                    "label": 0
                },
                {
                    "sent": "You end up with having just dropped kernels and protein kernels, and not pairwise kernels, but then the centering is this bottleneck that doesn't allow you to do this because centering a drug kernel and then centering approaching curl and taking a Chronicle product doesn't give you a center pairwise kernel, so center pairwise kernel cannot be simply calculated from center drag kernel and centered protein kernel.",
                    "label": 0
                },
                {
                    "sent": "So what we have done to make it possible is that we introduced a highly efficient Chronicle, the composition of the centering operator for the pairwise kernel.",
                    "label": 0
                },
                {
                    "sent": "So here QD and QP are the factors of C of the centering operator and just what is important because we're not going into like all the details, but I want to just mention that this is very efficient because it always comes down to solving as his singular value problem for a matrix of size 2 * 2 only.",
                    "label": 0
                },
                {
                    "sent": "So it doesn't matter how many drug protein pairs common examples you have, you could have.",
                    "label": 0
                },
                {
                    "sent": "Like 1000 lichen could be 1000 or a million.",
                    "label": 0
                },
                {
                    "sent": "But because we exploit this particular structure of the matrix C at the end, we only need to solve the singular value problem for a matrix of size 2 * 2.",
                    "label": 0
                },
                {
                    "sent": "And and that's why this is extremely efficient.",
                    "label": 0
                },
                {
                    "sent": "To calculate this, this factors QD and QC, and basically once we have this factors once we can like write matrix, see in this form we can just rewrite all the equations and I mean these are of course very long.",
                    "label": 0
                },
                {
                    "sent": "But like just the main point here, I want you to notice that we don't have anymore any pairwise kernels.",
                    "label": 1
                },
                {
                    "sent": "Everything is based on some components of the pairwise.",
                    "label": 0
                },
                {
                    "sent": "I mean just the drug kernels, protein kernels and then agen vectors and so on so we don't actually know Dragon vector.",
                    "label": 0
                },
                {
                    "sent": "Sorry, just the factors of them.",
                    "label": 0
                },
                {
                    "sent": "Centering operator.",
                    "label": 0
                },
                {
                    "sent": "So basically we are not using any pairwise matrices in any point, so that's why that's why it's faster, that's why that's why it's feasible, because we avoid going to the pairwise space.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now another thing is that in multiple kernel learning, so for classification it would be usual to calculate the response Colonel in this following form just the linear kernel and this works really well because it separates the positive and negative classes perfectly.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that if you have so in classification you will offer label like for instance interaction will be labeled as plus one and non interaction is minus one and then if you see in this equation if you have.",
                    "label": 0
                },
                {
                    "sent": "If you're comparing 2 drug protein pairs that were not interacting, you have minus 1 * -- 1.",
                    "label": 0
                },
                {
                    "sent": "You get +12 protein purse that we're interacting.",
                    "label": 0
                },
                {
                    "sent": "You're gonna get plus one, but if the labels are different, you get minus one.",
                    "label": 0
                },
                {
                    "sent": "So we just like separating like what is the same gets value of 1 and the different purse get a value of minus one.",
                    "label": 0
                },
                {
                    "sent": "So that's why this kernel works very in this response.",
                    "label": 0
                },
                {
                    "sent": "Kernel K. Why?",
                    "label": 0
                },
                {
                    "sent": "This is a very good choice for classification and in all the multiple currently multiple kernel learning literature like this INF paper and so on.",
                    "label": 0
                },
                {
                    "sent": "It was always introduced like that.",
                    "label": 0
                },
                {
                    "sent": "But we thought that of course, since we are focusing on regression task, it doesn't make sense to use this kernel because what it does, as you can see in the example, it just gives you a small kernel values for the low numbers, low binding affinity's and large kernel values for the large binding affinity's.",
                    "label": 0
                },
                {
                    "sent": "So this is all the usual linear response kernel would do, which doesn't make sense for the regression task.",
                    "label": 0
                },
                {
                    "sent": "We want to compare our binding companies want to compare numbers, so of course the natural choice.",
                    "label": 0
                },
                {
                    "sent": "Would be too.",
                    "label": 0
                },
                {
                    "sent": "Is the Gaussian kernel.",
                    "label": 0
                },
                {
                    "sent": "It's like a gold standard for comparing real numbers.",
                    "label": 0
                },
                {
                    "sent": "But why we cannot use it directly?",
                    "label": 0
                },
                {
                    "sent": "Why we cannot just plug this in?",
                    "label": 0
                },
                {
                    "sent": "Because we have as I showed in the previous slide, we have this equations for calculating our kernels and we cannot just like plug it in there because it wouldn't like we will have our speedup anymore basically.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we have done is that we have like.",
                    "label": 0
                },
                {
                    "sent": "Approximated form the Gaussian kernel?",
                    "label": 0
                },
                {
                    "sent": "So first of all, we fit a mixture of Gaussians onto the frequency histogram of our labels, and then for each value Y.",
                    "label": 0
                },
                {
                    "sent": "So for each label we define a window of S bins around it.",
                    "label": 0
                },
                {
                    "sent": "And then basically we define, so each label will have a feature vector that we derive from there.",
                    "label": 0
                },
                {
                    "sent": "So the whole point of looking at it like this is to derive a feature vector for a label.",
                    "label": 0
                },
                {
                    "sent": "So this feature vector is read off from this bin densities and normalized and we just like plug it in the corresponding element of this of this matrix side.",
                    "label": 0
                },
                {
                    "sent": "And now the kernel can be simply compute calculated as a sum of products of as being densities.",
                    "label": 0
                },
                {
                    "sent": "So basically we have now a very nice representation because like if we just have we don't have a Gaussian kernel equation, But in this kind of form we can plug it into the equations that we saw before.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this gives us the kind of representation of the response kernel as we wanted to have, because for instance, from here in the linear kernel it would just multiply 7 * 9 and minus 5 * 7 and so on.",
                    "label": 1
                },
                {
                    "sent": "But here you can see that a dent, we see that seven and nine are similar to each other and minus five and seven, and for instance this similar, so we're just measuring the similarities between the real numbers, but using like another equation, not the Goshen equation for it.",
                    "label": 0
                },
                {
                    "sent": "So once we have the.",
                    "label": 0
                },
                {
                    "sent": "Response kernel in this form.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can just like plug it in so the response kernel was used in the equation for calculating vector A and once we plug it in, that's what we get.",
                    "label": 0
                },
                {
                    "sent": "So again we are not working the pairwise space.",
                    "label": 0
                },
                {
                    "sent": "So all of that is done in the spaces of drugs and proteins.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now I have one application example just to show you kind of what we can do.",
                    "label": 0
                },
                {
                    "sent": "So here we use then drug target interaction data comprising of roughly 160,000 PC 50 values between roughly 3000 kinase inhibitor drugs and 226 kinases, and this is publicly available data.",
                    "label": 0
                },
                {
                    "sent": "And we calculated 10 different drug kernels and 312 kinase protein kinase kernels.",
                    "label": 0
                },
                {
                    "sent": "So this gave us we just need to multiply the two because we consider all different pairs always when working in the pairwise space pairwise kernel space.",
                    "label": 0
                },
                {
                    "sent": "So this gives us 3120 pairwise kernels.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So 10 different drug kernels that I mentioned here.",
                    "label": 0
                },
                {
                    "sent": "They were all calculated as Tanimoto kernels using different types of fingerprints.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then for protein kernels we have considered as the input data sources the amino acid sequences and the gene ontology annotations.",
                    "label": 0
                },
                {
                    "sent": "And we considered specifically full amino acid sequences of the proteins of the kinases in our set, but also the subsequences of the kinase domains and subsequences of ATP binding pockets.",
                    "label": 0
                },
                {
                    "sent": "So these are the shortest, the final ones.",
                    "label": 0
                },
                {
                    "sent": "And then we calculate this standard Smith, Waterman kernel and the generic string kernel that that I showed you earlier.",
                    "label": 0
                },
                {
                    "sent": "And then for the gene Ontology annotations we derived like Gene Ontology profile for each protein.",
                    "label": 0
                },
                {
                    "sent": "And this profile is basically we look at the proportion of the proteins that are annotated with a particular gene ontology term and then we take a negative logarithm of debt.",
                    "label": 0
                },
                {
                    "sent": "But basically what it means is that we will get large values for more for the terms that are more specific to a certain protein.",
                    "label": 0
                },
                {
                    "sent": "So kind of.",
                    "label": 0
                },
                {
                    "sent": "Just like maybe highlight this a bit like or like make it more, tell it the algorithm it that's more important, because if it's somehow more specific if.",
                    "label": 0
                },
                {
                    "sent": "Of course, some proteins are especially kinases.",
                    "label": 0
                },
                {
                    "sent": "There will be lots of gene ontology terms that all of them are on notated with, so that's maybe not the ones that we want to like.",
                    "label": 0
                },
                {
                    "sent": "Give too much importance.",
                    "label": 0
                },
                {
                    "sent": "And since we have this real value profiles, we applied Gaussian kernels on top.",
                    "label": 0
                },
                {
                    "sent": "And now why we have 312 different kernels is because then Gaussian kernel has one hyperparameter GS.",
                    "label": 0
                },
                {
                    "sent": "Kernel has free hyperparameters and Smith Waterman.",
                    "label": 0
                },
                {
                    "sent": "It doesn't have any hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "So basically instead of tuning the hyperparameters like for instance in the cross validation in the nested cross validation, when you know, especially with the nested cross validation and the grid search, it is.",
                    "label": 0
                },
                {
                    "sent": "It can be quite time consuming.",
                    "label": 0
                },
                {
                    "sent": "So you have already here for different kernels.",
                    "label": 0
                },
                {
                    "sent": "And there's also regularization parameter of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So it takes quite a lot of time to tune.",
                    "label": 0
                },
                {
                    "sent": "All these parameters this way, so we wanted to take advantage of the multiple kernel learning also for the parameter tuning, so that's why we just calculated kernels with different different values of the hyperparameters from the reasonable range, and we got 312 protein kernels.",
                    "label": 0
                },
                {
                    "sent": "So now this was our our training data and our kernels.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then here is the summary of the results.",
                    "label": 0
                },
                {
                    "sent": "So pairwise some kill it selected out of this over 3000 kernels, it selected eight different kernels and we can see then the selected kernels selected pairwise kernels and their weights here.",
                    "label": 0
                },
                {
                    "sent": "So one thing to emphasize is that actually we observed that ATP binding pockets turned out to be quite predictive, as kind of could be expected, as I mentioned, because with the emergent earlier this for kinase inhibitors, most of them not all, but most of them binds to the ATP binding pockets of the proteins.",
                    "label": 0
                },
                {
                    "sent": "So that's why, like just focusing on comparing ATP, binding pockets makes more sense than comparing full amino acid sequences.",
                    "label": 1
                },
                {
                    "sent": "And another thing that you can see or like one of the main things is that the GS Colonel turned out to be.",
                    "label": 0
                },
                {
                    "sent": "Quite productive and more productive than Standard Smith Waterman kernel.",
                    "label": 0
                },
                {
                    "sent": "That of course kind of makes sense because Smith Waterman is like the simplest and most like standard approach and also one thing that I could mention here is that so that it's actually like with kernels like one of the disadvantages is maybe that interpretation can be hard.",
                    "label": 0
                },
                {
                    "sent": "So for instance if you are if you are for instance working on the prediction of drug responses in cancer cell lines, it is quite well known that gene expression.",
                    "label": 0
                },
                {
                    "sent": "Is very predictive data source.",
                    "label": 0
                },
                {
                    "sent": "Of course they have also mutation complete number variation, metalation profiles and so on, but it's generally well known and it has been shown that also in dream challenge that gene expression seemed to be the best and the most productive.",
                    "label": 0
                },
                {
                    "sent": "So now if you would calculate the kernels, typically would calculate the kernel for the gene expression values.",
                    "label": 0
                },
                {
                    "sent": "A kernel for the mutations and so on you would get a couple of kernels and you would get some, let's say nice predictive performance and probably what the algorithm would tell you that most of the weight was assigned to the gene expression.",
                    "label": 0
                },
                {
                    "sent": "So OK, it's nice, but you kind of knew it like it was expected that the gene expression is more productive.",
                    "label": 0
                },
                {
                    "sent": "So I don't have it results for that because it's still like we still haven't like done it, but like my idea.",
                    "label": 0
                },
                {
                    "sent": "What can be done with multiple kernel learning?",
                    "label": 0
                },
                {
                    "sent": "Why it is nice?",
                    "label": 0
                },
                {
                    "sent": "Because you can also like kind of get more into interpretation of your results because instead of for instance calculating kernel for the gene expression values, you could for instance like look at pathways instead of looking at all gene expressions together.",
                    "label": 0
                },
                {
                    "sent": "You could look at different pathways and for instance calculate multiple kernels for each different pathway.",
                    "label": 0
                },
                {
                    "sent": "Let's say same for the drag kernels.",
                    "label": 0
                },
                {
                    "sent": "Instead of calculating Tanimoto kernel.",
                    "label": 0
                },
                {
                    "sent": "Based on your whole fingerprint, you could like think of how to divide them into like more like manageable substructures that could inform about the activity somehow.",
                    "label": 0
                },
                {
                    "sent": "So we could have like from a single fingerprint you could have like multiple different kernels and then at the end because this algorithm can handle lots of different kernels as input and for sure there will be other algorithms as well.",
                    "label": 0
                },
                {
                    "sent": "So basically then your interpretation would be would see that maybe there was like a certain pathway in a certain like substructures that were very predictive.",
                    "label": 0
                },
                {
                    "sent": "So that can of course it's not a perfect interpretation.",
                    "label": 0
                },
                {
                    "sent": "But at least it can like increase the interpretation of the kernel methods in a way or that would be like 1 one way with multiple kernel learning.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then something that is very important in practice is that in case of drug protein interaction prediction, we need to consider four different prediction scenarios depending on whether or not training and tests share common drugs and proteins or both, and this is extremely important, but it's very often forgotten and this doesn't only apply to drug protein interactions, but like any kind of machine learning problem where you have pert inputs.",
                    "label": 0
                },
                {
                    "sent": "So when you're like inputs consists of 2 two different parts.",
                    "label": 0
                },
                {
                    "sent": "So we can see here that in this like first one, most left hand side, this is like the most standard cross validation.",
                    "label": 0
                },
                {
                    "sent": "So this is like how you would do it like you have your you have your data, you have your drug protein binding affinity would just like randomly remove some of them, do the training like predict like what were the motives accuracy for those.",
                    "label": 0
                },
                {
                    "sent": "Then you would remove like randomly other like just do the cross validation in a standard way.",
                    "label": 0
                },
                {
                    "sent": "But then for instance.",
                    "label": 0
                },
                {
                    "sent": "Wait, why does this like not optimal?",
                    "label": 0
                },
                {
                    "sent": "So maybe you have an algorithm like some either like existing or new algorithm and you would like to knit and like assess using the cross validation.",
                    "label": 0
                },
                {
                    "sent": "The predictive performance.",
                    "label": 0
                },
                {
                    "sent": "And let's say you were very happy with the predictive performance is very high and then you have a collaborator who for instance designed a new chemical structure and the collaborate and knows that there is one on target that the structure was like a design for.",
                    "label": 0
                },
                {
                    "sent": "But like he comes to you and asks you like I would like to know the binding profile.",
                    "label": 0
                },
                {
                    "sent": "Of this new compound, new chemical structure that I designed with all the kinases, all humankind and for instance.",
                    "label": 0
                },
                {
                    "sent": "And then you might think like, OK, I have this algorithm.",
                    "label": 0
                },
                {
                    "sent": "I tuned it, I get a good predictive performance, so I will now just apply it and do the predictions.",
                    "label": 0
                },
                {
                    "sent": "And then if you have done it in this way, you have forgotten about like this different prediction scenarios you would get over optimistic predictive performance because what you should do in this case is that your cross validation needs to mimic the problem that you are so.",
                    "label": 0
                },
                {
                    "sent": "Design across validation in such a way that you remove always single driver multiple drugs at a time from your training data and you do it both for assessing the performance of the method but also for tuning the parameters.",
                    "label": 0
                },
                {
                    "sent": "So this is extremely important just to remember and especially dislike both on paper.",
                    "label": 0
                },
                {
                    "sent": "This is a very nice, quite short paper that kind of explains the same ideas generally for part input predictions.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then just a dent couple of slides about the dream challenge.",
                    "label": 0
                },
                {
                    "sent": "Maybe some of you have heard or even participated.",
                    "label": 0
                },
                {
                    "sent": "I don't know, but basically we were just we were organizing the dream challenge on drug kinase binding affinity prediction.",
                    "label": 0
                },
                {
                    "sent": "And the basic idea was to evaluate how well different machine learning models work in practice for this problem.",
                    "label": 0
                },
                {
                    "sent": "So the three overall questions that we're hoping to answer based on the challenge or what are the best machine learning approaches for predicting drug protein binding affinity's?",
                    "label": 0
                },
                {
                    "sent": "At what are the most productive compound and protein features?",
                    "label": 0
                },
                {
                    "sent": "And also what are the Best Buy activity data for the model training?",
                    "label": 0
                },
                {
                    "sent": "Because you know, it often comes down not just that there is like one algorithm that solves everything.",
                    "label": 0
                },
                {
                    "sent": "But of course also the training data should be like of high quality and so on.",
                    "label": 0
                },
                {
                    "sent": "And also like the appropriate features and so on.",
                    "label": 0
                },
                {
                    "sent": "So it's always a combination of multiple factors.",
                    "label": 0
                },
                {
                    "sent": "So we want to learn as much as possible.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we have put an emphasis here on quantitative modeling of the compound protein interactions.",
                    "label": 0
                },
                {
                    "sent": "So the regression setting as we had in this lecture, and we focus on protein kinases and kinase inhibitors because of their clinical importance.",
                    "label": 0
                },
                {
                    "sent": "Of course, in cancer they are very important in treating cancer, but also other diseases and there are some clinical trials testing kindness inhibitors like for other than cancer, other indications and cancer.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here is the overview.",
                    "label": 0
                },
                {
                    "sent": "Sorry if the font is too small in some places, but basically we had two rounds in the challenge.",
                    "label": 0
                },
                {
                    "sent": "In the first round we ask the participants to predict 430 compound binding affinities of 430 compound kindness purse and in the second round we have 394 compound kinase purse and the data the actual data that we use them because it needs to be unpublished data and then we need to participants don't have an access to that and then we are.",
                    "label": 0
                },
                {
                    "sent": "Evaluating how well their model performed.",
                    "label": 0
                },
                {
                    "sent": "So this was provided by the illuminating trackable Genome Kindness Consortium.",
                    "label": 0
                },
                {
                    "sent": "So in the first round we had 77 teams.",
                    "label": 0
                },
                {
                    "sent": "Some teams consisted just of 1 participant, but we will call it teams here.",
                    "label": 0
                },
                {
                    "sent": "7077 teams and 454 teams.",
                    "label": 0
                },
                {
                    "sent": "In the second round.",
                    "label": 0
                },
                {
                    "sent": "And we also had a baseline model which was a pairwise kernel regression.",
                    "label": 0
                },
                {
                    "sent": "But it was supposed to be baseline, so not not the multiple kernel learning and not using like more sophisticated kernels, but a single kernel and just pairwise kernel regression basically.",
                    "label": 0
                },
                {
                    "sent": "So the first one, the first round, had lasted 1 1/2 months and declaw it closed in the mid November and then the second round closed just a month ago.",
                    "label": 0
                },
                {
                    "sent": "And basically in the first round we didn't have any winner, so there the idea was just to let the participant see how well they perform across different evaluation metrics.",
                    "label": 0
                },
                {
                    "sent": "At that point we didn't reveal what is the winning metric, just not to like have maybe like encouraging for optimizing the predictions for a particular evaluation metric.",
                    "label": 0
                },
                {
                    "sent": "So just to see how well they perform.",
                    "label": 0
                },
                {
                    "sent": "But no winners such, whereas in the second round we have two different sub challenges and the winner of the first Step Challenge was determined based on the Spearman correlation.",
                    "label": 0
                },
                {
                    "sent": "And the winner of the second sub challenge based on the Harmacy and then right now even though I mentioned that we closed at one month ago but we don't have like official winners as such because also very important part of the dream challenges is that everything that we learn and everything that we can get out of there is made publicly available so that the whole community can benefit.",
                    "label": 0
                },
                {
                    "sent": "So right now we are still collecting Docker containers of the models and write ups of the methods and all of that from the participants because if the like for instance best participants is not.",
                    "label": 0
                },
                {
                    "sent": "Able to provide this because it's like maybe developed in a company for instance and so on.",
                    "label": 0
                },
                {
                    "sent": "Then unfortunately we cannot like announce like this participant a winner, so that's why I'm not like able to show you like very detailed results it because we are now in the still in the process of like making this this final ranking and so on.",
                    "label": 0
                },
                {
                    "sent": "But one thing like this is.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just in general overview of like focusing on our machine, Spearman.",
                    "label": 1
                },
                {
                    "sent": "What is the distribution of these different different values, different prediction, predictive performance that we got and you can see that there were quite good models and models that were not working that well.",
                    "label": 0
                },
                {
                    "sent": "The red lines and the blue line they mark the baseline results.",
                    "label": 0
                },
                {
                    "sent": "And so I should also mention, because like maybe the like, you can see the top performers.",
                    "label": 0
                },
                {
                    "sent": "They had the correlation.",
                    "label": 0
                },
                {
                    "sent": "Spearman correlation of like a bit less than .6, and it might seem like it's not super high.",
                    "label": 1
                },
                {
                    "sent": "But actually this this problem this problem in the Dream Challenge data that we're considering?",
                    "label": 0
                },
                {
                    "sent": "They corresponded to quite challenging scenario.",
                    "label": 0
                },
                {
                    "sent": "So actually like mostly this was this new drug setting.",
                    "label": 0
                },
                {
                    "sent": "So if you would look at the if we would come back to this like when we had different cross validation scenarios, it would be the second case where in most of the cases of drug doesn't really have.",
                    "label": 1
                },
                {
                    "sent": "And then sorry, and then prior binding affinity data that you could use for the training.",
                    "label": 0
                },
                {
                    "sent": "And actually, I saw some of the scatter plots, like from for the best participants and they looked quite reasonable.",
                    "label": 0
                },
                {
                    "sent": "So actually I think we have that we will have some interesting insights.",
                    "label": 0
                },
                {
                    "sent": "So now we will analyze all the results and models to learn what are the things work and what don't, what things like don't work.",
                    "label": 0
                },
                {
                    "sent": "So basically what are the best methods?",
                    "label": 0
                },
                {
                    "sent": "What are the training data that worked well?",
                    "label": 0
                },
                {
                    "sent": "What kinds of features an I can just briefly tell you since we don't have yet and we know it cannot say specifically, but we have actually looked quite different methods among like the very top performing teams.",
                    "label": 0
                },
                {
                    "sent": "So there is deep learning.",
                    "label": 0
                },
                {
                    "sent": "There is random forests and there is also some kernel learning, so it's not really like the respect.",
                    "label": 0
                },
                {
                    "sent": "One algorithm that works best for everything.",
                    "label": 0
                },
                {
                    "sent": "But soon there will be a challenge paper available.",
                    "label": 0
                },
                {
                    "sent": "So for all of you that are interested, like how does it actually look like and what we were able to learn from this from this challenge like you are welcome to to read and.",
                    "label": 0
                },
                {
                    "sent": "And I hope that there was some interesting insights.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's all, thank you for your attention and I'm happy to take any questions.",
                    "label": 0
                }
            ]
        }
    }
}