{
    "id": "b45hakmvl3kdxt3iadfcaeozhsvyxpcl",
    "title": "Knowledge Graph Identification",
    "info": {
        "author": [
            "Jay Pujara, Department of Computer Science, University of Maryland"
        ],
        "published": "Nov. 28, 2013",
        "recorded": "October 2013",
        "category": [
            "Top->Computer Science->Information Extraction",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2013_pujara_graph_identification/",
    "segmentation": [
        [
            "Thank you, this is joint work with Umaly.",
            "Sakura William Cohen."
        ],
        [
            "So give you an overview of what this talk is going to be about.",
            "This is one minute.",
            "It gives you the entire talk.",
            "So the problem that we were going to solve is to build a knowledge graph from noisy data from the web and our approach, we argue, is our approach is not ification.",
            "We argue that you need to jointly reason over all of these million facts and apply ontological constraints to truly build a knowledge graph.",
            "The method we used to build this knowledge Graph is a framework called probabilistic soft logic, and the great thing about probabilistic soft logic is that it's easy to specify models and it sufficient to optimize them.",
            "So the results we have are state of the art.",
            "These are you never been seen before.",
            "Millions of extractions, joint reasoning, building a knowledge graph and I'm very excited about these results.",
            "So let me motivate this knowledge graph problem for you so."
        ],
        [
            "So."
        ],
        [
            "We are in this new age where the Internet has this massive amount of information and we have these cutting edge information extraction techniques which do parsing part of speech, tagging, semantic role labeling, labeling, named entity recognition and our hope is that we can build this knowledge graph.",
            "This structured representation of entities, their labels and the relationships between them.",
            "But as we all know the word."
        ],
        [
            "It's not that simple.",
            "There's some real challenges that we face.",
            "the Internet is very noisy there.",
            "There's incorrect information.",
            "Hypothetical information.",
            "The extraction task is still extremely difficult, especially in the real world, and as a result, the knowledge graph that's produced through this pipeline is inconsistent, and it contains many errors.",
            "So rather than just say there is, let's, let's look at a few of them and we're going to use a key."
        ],
        [
            "This study, the never Ending language learner ornel.",
            "This is a project at Carnegie Mellon that's been running for the last three years, and what now aims to do is lifelong learning.",
            "So it's going to read the web one day, learn some things from the web and the next day, use what learn to better read the web.",
            "And so it tweets everyday.",
            "CMU nails the Twitter handle and it will tell you some of the things that's learned that day in addition to just learning from the web, it has a very rich ontology and you can see that on the right there there's a.",
            "One small snippet of it of the different sub labels of person and the knowledge base now contains millions of facts and because it contains billions of facts and."
        ],
        [
            "Many errors, so let's look at some of these errors that we see."
        ],
        [
            "In the nail knowledge base.",
            "So the first type of error we see is an entity coreference error so the country Kurdistan has many, many different variants that are five that are shown there and you can see different web pages where people have misspelled or use different variants of the name of this country.",
            "So we know that these are all underlying the same entity."
        ],
        [
            "The second pair is missing or spurious labels, so Nell believes Kyrgyzstan is both a bird and a country, and the reason I think it's a bird is because there's a web page where everybody is posted, links to birds and one person is posted links to countries where he is seen birds and now we believe this to be a bird, and then the third."
        ],
        [
            "Error that we're concerned with are missing in serious relations, so Tristan might be in Kazakhstan, Russia, or the US according to Nell, and there are just different web pages where this extraction pattern seemed to believe seem to match something that was incorrect, and so when we look at these types of error."
        ],
        [
            "As we see them as violations of ontological knowledge.",
            "In the first case, we can look at coreference information about entities and be able to decide that they are equivalent.",
            "In the second case, where we had incorrect labels, we can look at disjoint label classes and realize that these two labels cannot coexist for the same entity and in the third case we can look at a selectional preference such as a domain or range, and say that the relation country location has a range that is continent an not country, and when you think of these types of ontological.",
            "Relationships to enforce them, we have to reason jointly over all of the facts because they're all interconnected and so that process."
        ],
        [
            "Our proposal for that process is called knowledge graph identification, and So what we've done is we."
        ],
        [
            "Refined the problem that we originally saw, so we're not going to take the Internet and apply information extraction and come out with the Knowledge Graph.",
            "We're going to come out with an extraction graph, and this graph will be noisy by nature, and now we're going to do joint reasoning to come up with the Knowledge Graph and this knowledge graph will resolve coreference entities and have the true labels in relations that we need.",
            "And."
        ],
        [
            "So what we do in Knowledge Graph identification, we perform graph identification which is entity resolution, a collective classification of labels and link prediction of relations.",
            "While we're doing these, three tasks were also enforcing ontological constraints, and we're also using uncertain information from multiple sources.",
            "So now there are multiple extractors that each produced some set of extractions and they have different liabilities.",
            "So to illustrate what this process looks like."
        ],
        [
            "Let's look at a small set of extractions motivated by the examples I gave previously so you can see in purple.",
            "There are different extractions with confidence values produced by Nell and we can."
        ],
        [
            "Sent this as an extraction graph and here you can see there are a number of relationships, but they're very noisy.",
            "No real dependencies between them, they're just all independent.",
            "There's no way to reason about them, so we can."
        ],
        [
            "Annotating this extraction graph with the ontology with the mutual exclusion.",
            "The domain and entity coreference, and now we have a more complex graph and our."
        ],
        [
            "Hope is to reason and produce a consistent knowledge graph which has the entities that are resolved.",
            "Corporate entities resolved to a single entity with a capital Bishkek and a label country.",
            "So this is the goal, and this sounds pretty complicated, so let me tell you a little bit about how we model this very complicated process so."
        ],
        [
            "What?"
        ],
        [
            "We're using as a probabilistic graphical model, and so the variables in this model are these sorts of facts that we've seen before labels in relationships, so here every possible label in this small universe for Kurdistan and every possible relationship is enumerated, and as you can see, these are."
        ],
        [
            "And so we can add the extractions that support these different facts from the Internet, and again you can see these variables are all independent.",
            "This is sort of the state of most approaches to this problem right now.",
            "They treat these variables independently and use local features, but we can add dependencies so we can add the mutual exclusion dependencies.",
            "We can add the domain range dependencies and we can add the entity coreference dependencies and now you can see this is a very rich model with a lot of code with a lot of dependencies which we have to solve.",
            "So the way we solve this problem is we use."
        ],
        [
            "Are called probabilistic soft logic.",
            "So what this is is a templating language for hinge loss, Markov random fields, and it's extremely scalable and the other really strong selling point is that it's the model can be specified as a collection of logical formula.",
            "So you can see one logical formula here which says that if E1 and E2 are the same entity and each one has label L Denise, you should also have label L. But the connective is not a simple and it's this.",
            "You see the little~ which suggested soft logic, so the atoms in these rules will take values between zero and one, and we're going to evaluate them with the truth value.",
            "Between zero and one, so it's a soft logical interpretation that's consistent with the."
        ],
        [
            "With Boolean logic.",
            "So now that we have this model specified biological formulas, we can ground the rules and then we can form a probability distribution over possible knowledge graphs conditioned on the extraction.",
            "So that's the equation here and you can see the rules that the ground rules are going to some of those ground rules, and we're going to look at the truth value and derive a distance to satisfaction for each rule.",
            "So there's a weighted distance to satisfaction, which is that potential function Phi, and we're going to use that to define the probability of a.",
            "Particular assignment of truth values to facts, and So what we have now is this probability distribution over all possible knowledge graphs, and now that we have this distribution, what we?"
        ],
        [
            "To do is inference, so we're going to do most probable explanation inference which maximizes that probability distribution by finding the best knowledge graph an in PSL.",
            "This inference is a convex optimization, so it's very efficient, and it's actually it scales with linearly with the number of ground rules, so that turns out to be very fast, so let's look."
        ],
        [
            "Under the hood and see some of the rules that are in our model.",
            "So the first rule."
        ],
        [
            "Will read right to left, you see?"
        ],
        [
            "On the right, the the actual variables that are in that graphical model and what's?"
        ],
        [
            "Supporting them are the extractions from the information extraction system and you can see that subscript with the T that T is the the technique or the extractor that suggests that this this.",
            "This fact is true, and each of them is going to have a different reliab."
        ],
        [
            "So we're going to assign them different weights."
        ],
        [
            "Next set of rules are for entity resolution, so we have a predicate statement which captures Co reference information and what we want these rules do is enforce an equivalence class between entities that are Co referent, such that those entities all have the same labels and relationships.",
            "And then."
        ],
        [
            "Finally, we have a set of rules that specify the ontology, and this is a very standard sort of a standard set of rules for inverse selectional preferences such as domain and range, subsumption, and mutual exclusion.",
            "So you're probably pretty familiar with these types of logical rules.",
            "So let's look at the evaluation."
        ],
        [
            "So we look."
        ],
        [
            "At two different datasets, the first one is link brain, so it's derived from Musicbrainz in the music ontology, and it's a community driven set of data.",
            "And since that data is actually fairly clean, when we get it, we add some realistic noise and it's fairly small ontology.",
            "But our sample includes nearly a million extractions, and then the second data set is this null data set, which I'm motivated the problem with, so it's generated by reading the web by extracting information from the web, and it's very noisy.",
            "The data set has over a million extractions and you can see the ontology is actually very large.",
            "There are 70,000.",
            "Ontological constraints that we're going to see in this ontology.",
            "So let's look a little."
        ],
        [
            "More closely at Link Brains, here's the schema you can see that what we have are artists, their creative works and labels, and the mapping from the ontology that I showed you previously to sort of a standard set of ontological relationships in music ontology is given, so it's fairly straightforward.",
            "Nothing too surprising there."
        ],
        [
            "And so let's talk about the noise model here.",
            "So the types of noise we add correspond to the different types of noise we've seen so coreference we add misspellings for each artist for missing in space labels we generate, we swap the artist in the album in some cases, so that these labels are swapped and their incorrect.",
            "An for relationships we either add albums that don't exist, or we exclude albums that do exist, and for all of these facts we generate truth values with Gaussian noise.",
            "So the idea is that this is the sort of noise you would seen a real web application."
        ],
        [
            "So what we want to do in our experiments is to look at a baseline which treats all of these independently, so it just uses the truth value, which is a noisy truth value to generate the knowledge base versus using entity resolution or ontological reasoning or the full Knowledge Graph identification model so you can see that both entity resolution and ontological reasoning improve the results, but doing them together in our Knowledge Graph model produces the best results for this data set.",
            "So it's substantially better than the baseline.",
            "The second date."
        ],
        [
            "Insight is null and so we wanted to compare against some previous work in the area, which is a very different evaluation setting.",
            "The more familiar with it, a lot simpler in fact, so the assumptions in previous work have been a closed world model where there is an explicit target set or query set based on the evaluation data and the variables you're allowed to reason about.",
            "Or a two hop neighborhood around that query set, and in fact it's a little bit simpler because trivially satisfied variables are excluded.",
            "So earlier we talked about Tristan being both the burden of country.",
            "If Kristen being a country is in the target set, there's no reason to reason about Kirkus Tan being a bird, even if it's in the extractions in this in this methodology.",
            "So we believe this is a similar methodology, but we wanted to compare against other work that's been done in this area, and then the full Knowledge Graph approach.",
            "The one sort of process used for link brains involves a open world model where we look at all possible variable and assign truth values to all of them."
        ],
        [
            "So let's look at the simpler evaluation setting with the target set, so the baseline again is just using the truth values that come from now and then.",
            "Nelly has a set of heuristics that uses to decide what gets added to the knowledge base, so that's the second comparison we have and the third comparison is work from jeonghan.",
            "ICM 2012, where they use a simpler model that uses a discrete probability probabilistic graphical model.",
            "Does approximate inference so it's an MLN or Markov logic network, and so they also do some joint reasoning and we compare against their work with our full Knowledge Graph identification model.",
            "And you can see our F1 is much higher and are running time is only 10 seconds for the 25,000 facts that are in this target set.",
            "The MLN which does joint reasoning takes a few minutes to an hour to run.",
            "So this is a significant speedup, and it's such a speed up that we can handle the full knowledge graph from using the 1.3 million extractions from melting."
        ],
        [
            "For sort of a complete knowledge graph that contains over 4 million facts in just about 2 hours, 130 minutes.",
            "So comparing against the nail baseline, which is some heuristics heuristics for promotion, we did significantly better both in AUC and F1, so to conclude."
        ],
        [
            "I'm really excited about this work because this is a very sophisticated technique to build knowledge graphs that does joint inference.",
            "It uses both the uncertain information from the web as well as ontological information, and we're using PSL, probabilistic soft logic were able to do this efficiently and were able to handle problems that before were not amenable to joint inference, so please feel free to check out our code which is available on GitHub and I'm happy to take questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you, this is joint work with Umaly.",
                    "label": 0
                },
                {
                    "sent": "Sakura William Cohen.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So give you an overview of what this talk is going to be about.",
                    "label": 0
                },
                {
                    "sent": "This is one minute.",
                    "label": 0
                },
                {
                    "sent": "It gives you the entire talk.",
                    "label": 0
                },
                {
                    "sent": "So the problem that we were going to solve is to build a knowledge graph from noisy data from the web and our approach, we argue, is our approach is not ification.",
                    "label": 0
                },
                {
                    "sent": "We argue that you need to jointly reason over all of these million facts and apply ontological constraints to truly build a knowledge graph.",
                    "label": 1
                },
                {
                    "sent": "The method we used to build this knowledge Graph is a framework called probabilistic soft logic, and the great thing about probabilistic soft logic is that it's easy to specify models and it sufficient to optimize them.",
                    "label": 1
                },
                {
                    "sent": "So the results we have are state of the art.",
                    "label": 1
                },
                {
                    "sent": "These are you never been seen before.",
                    "label": 0
                },
                {
                    "sent": "Millions of extractions, joint reasoning, building a knowledge graph and I'm very excited about these results.",
                    "label": 0
                },
                {
                    "sent": "So let me motivate this knowledge graph problem for you so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We are in this new age where the Internet has this massive amount of information and we have these cutting edge information extraction techniques which do parsing part of speech, tagging, semantic role labeling, labeling, named entity recognition and our hope is that we can build this knowledge graph.",
                    "label": 0
                },
                {
                    "sent": "This structured representation of entities, their labels and the relationships between them.",
                    "label": 1
                },
                {
                    "sent": "But as we all know the word.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's not that simple.",
                    "label": 0
                },
                {
                    "sent": "There's some real challenges that we face.",
                    "label": 1
                },
                {
                    "sent": "the Internet is very noisy there.",
                    "label": 0
                },
                {
                    "sent": "There's incorrect information.",
                    "label": 0
                },
                {
                    "sent": "Hypothetical information.",
                    "label": 0
                },
                {
                    "sent": "The extraction task is still extremely difficult, especially in the real world, and as a result, the knowledge graph that's produced through this pipeline is inconsistent, and it contains many errors.",
                    "label": 1
                },
                {
                    "sent": "So rather than just say there is, let's, let's look at a few of them and we're going to use a key.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This study, the never Ending language learner ornel.",
                    "label": 1
                },
                {
                    "sent": "This is a project at Carnegie Mellon that's been running for the last three years, and what now aims to do is lifelong learning.",
                    "label": 0
                },
                {
                    "sent": "So it's going to read the web one day, learn some things from the web and the next day, use what learn to better read the web.",
                    "label": 1
                },
                {
                    "sent": "And so it tweets everyday.",
                    "label": 0
                },
                {
                    "sent": "CMU nails the Twitter handle and it will tell you some of the things that's learned that day in addition to just learning from the web, it has a very rich ontology and you can see that on the right there there's a.",
                    "label": 0
                },
                {
                    "sent": "One small snippet of it of the different sub labels of person and the knowledge base now contains millions of facts and because it contains billions of facts and.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Many errors, so let's look at some of these errors that we see.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the nail knowledge base.",
                    "label": 0
                },
                {
                    "sent": "So the first type of error we see is an entity coreference error so the country Kurdistan has many, many different variants that are five that are shown there and you can see different web pages where people have misspelled or use different variants of the name of this country.",
                    "label": 1
                },
                {
                    "sent": "So we know that these are all underlying the same entity.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second pair is missing or spurious labels, so Nell believes Kyrgyzstan is both a bird and a country, and the reason I think it's a bird is because there's a web page where everybody is posted, links to birds and one person is posted links to countries where he is seen birds and now we believe this to be a bird, and then the third.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Error that we're concerned with are missing in serious relations, so Tristan might be in Kazakhstan, Russia, or the US according to Nell, and there are just different web pages where this extraction pattern seemed to believe seem to match something that was incorrect, and so when we look at these types of error.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As we see them as violations of ontological knowledge.",
                    "label": 1
                },
                {
                    "sent": "In the first case, we can look at coreference information about entities and be able to decide that they are equivalent.",
                    "label": 0
                },
                {
                    "sent": "In the second case, where we had incorrect labels, we can look at disjoint label classes and realize that these two labels cannot coexist for the same entity and in the third case we can look at a selectional preference such as a domain or range, and say that the relation country location has a range that is continent an not country, and when you think of these types of ontological.",
                    "label": 0
                },
                {
                    "sent": "Relationships to enforce them, we have to reason jointly over all of the facts because they're all interconnected and so that process.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our proposal for that process is called knowledge graph identification, and So what we've done is we.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Refined the problem that we originally saw, so we're not going to take the Internet and apply information extraction and come out with the Knowledge Graph.",
                    "label": 0
                },
                {
                    "sent": "We're going to come out with an extraction graph, and this graph will be noisy by nature, and now we're going to do joint reasoning to come up with the Knowledge Graph and this knowledge graph will resolve coreference entities and have the true labels in relations that we need.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we do in Knowledge Graph identification, we perform graph identification which is entity resolution, a collective classification of labels and link prediction of relations.",
                    "label": 1
                },
                {
                    "sent": "While we're doing these, three tasks were also enforcing ontological constraints, and we're also using uncertain information from multiple sources.",
                    "label": 0
                },
                {
                    "sent": "So now there are multiple extractors that each produced some set of extractions and they have different liabilities.",
                    "label": 0
                },
                {
                    "sent": "So to illustrate what this process looks like.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's look at a small set of extractions motivated by the examples I gave previously so you can see in purple.",
                    "label": 0
                },
                {
                    "sent": "There are different extractions with confidence values produced by Nell and we can.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sent this as an extraction graph and here you can see there are a number of relationships, but they're very noisy.",
                    "label": 0
                },
                {
                    "sent": "No real dependencies between them, they're just all independent.",
                    "label": 0
                },
                {
                    "sent": "There's no way to reason about them, so we can.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Annotating this extraction graph with the ontology with the mutual exclusion.",
                    "label": 0
                },
                {
                    "sent": "The domain and entity coreference, and now we have a more complex graph and our.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hope is to reason and produce a consistent knowledge graph which has the entities that are resolved.",
                    "label": 0
                },
                {
                    "sent": "Corporate entities resolved to a single entity with a capital Bishkek and a label country.",
                    "label": 0
                },
                {
                    "sent": "So this is the goal, and this sounds pretty complicated, so let me tell you a little bit about how we model this very complicated process so.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're using as a probabilistic graphical model, and so the variables in this model are these sorts of facts that we've seen before labels in relationships, so here every possible label in this small universe for Kurdistan and every possible relationship is enumerated, and as you can see, these are.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so we can add the extractions that support these different facts from the Internet, and again you can see these variables are all independent.",
                    "label": 0
                },
                {
                    "sent": "This is sort of the state of most approaches to this problem right now.",
                    "label": 0
                },
                {
                    "sent": "They treat these variables independently and use local features, but we can add dependencies so we can add the mutual exclusion dependencies.",
                    "label": 0
                },
                {
                    "sent": "We can add the domain range dependencies and we can add the entity coreference dependencies and now you can see this is a very rich model with a lot of code with a lot of dependencies which we have to solve.",
                    "label": 0
                },
                {
                    "sent": "So the way we solve this problem is we use.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Are called probabilistic soft logic.",
                    "label": 1
                },
                {
                    "sent": "So what this is is a templating language for hinge loss, Markov random fields, and it's extremely scalable and the other really strong selling point is that it's the model can be specified as a collection of logical formula.",
                    "label": 1
                },
                {
                    "sent": "So you can see one logical formula here which says that if E1 and E2 are the same entity and each one has label L Denise, you should also have label L. But the connective is not a simple and it's this.",
                    "label": 0
                },
                {
                    "sent": "You see the little~ which suggested soft logic, so the atoms in these rules will take values between zero and one, and we're going to evaluate them with the truth value.",
                    "label": 0
                },
                {
                    "sent": "Between zero and one, so it's a soft logical interpretation that's consistent with the.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With Boolean logic.",
                    "label": 0
                },
                {
                    "sent": "So now that we have this model specified biological formulas, we can ground the rules and then we can form a probability distribution over possible knowledge graphs conditioned on the extraction.",
                    "label": 1
                },
                {
                    "sent": "So that's the equation here and you can see the rules that the ground rules are going to some of those ground rules, and we're going to look at the truth value and derive a distance to satisfaction for each rule.",
                    "label": 0
                },
                {
                    "sent": "So there's a weighted distance to satisfaction, which is that potential function Phi, and we're going to use that to define the probability of a.",
                    "label": 1
                },
                {
                    "sent": "Particular assignment of truth values to facts, and So what we have now is this probability distribution over all possible knowledge graphs, and now that we have this distribution, what we?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To do is inference, so we're going to do most probable explanation inference which maximizes that probability distribution by finding the best knowledge graph an in PSL.",
                    "label": 0
                },
                {
                    "sent": "This inference is a convex optimization, so it's very efficient, and it's actually it scales with linearly with the number of ground rules, so that turns out to be very fast, so let's look.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Under the hood and see some of the rules that are in our model.",
                    "label": 0
                },
                {
                    "sent": "So the first rule.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Will read right to left, you see?",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the right, the the actual variables that are in that graphical model and what's?",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Supporting them are the extractions from the information extraction system and you can see that subscript with the T that T is the the technique or the extractor that suggests that this this.",
                    "label": 0
                },
                {
                    "sent": "This fact is true, and each of them is going to have a different reliab.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we're going to assign them different weights.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Next set of rules are for entity resolution, so we have a predicate statement which captures Co reference information and what we want these rules do is enforce an equivalence class between entities that are Co referent, such that those entities all have the same labels and relationships.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finally, we have a set of rules that specify the ontology, and this is a very standard sort of a standard set of rules for inverse selectional preferences such as domain and range, subsumption, and mutual exclusion.",
                    "label": 0
                },
                {
                    "sent": "So you're probably pretty familiar with these types of logical rules.",
                    "label": 0
                },
                {
                    "sent": "So let's look at the evaluation.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we look.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At two different datasets, the first one is link brain, so it's derived from Musicbrainz in the music ontology, and it's a community driven set of data.",
                    "label": 0
                },
                {
                    "sent": "And since that data is actually fairly clean, when we get it, we add some realistic noise and it's fairly small ontology.",
                    "label": 0
                },
                {
                    "sent": "But our sample includes nearly a million extractions, and then the second data set is this null data set, which I'm motivated the problem with, so it's generated by reading the web by extracting information from the web, and it's very noisy.",
                    "label": 0
                },
                {
                    "sent": "The data set has over a million extractions and you can see the ontology is actually very large.",
                    "label": 0
                },
                {
                    "sent": "There are 70,000.",
                    "label": 0
                },
                {
                    "sent": "Ontological constraints that we're going to see in this ontology.",
                    "label": 0
                },
                {
                    "sent": "So let's look a little.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More closely at Link Brains, here's the schema you can see that what we have are artists, their creative works and labels, and the mapping from the ontology that I showed you previously to sort of a standard set of ontological relationships in music ontology is given, so it's fairly straightforward.",
                    "label": 0
                },
                {
                    "sent": "Nothing too surprising there.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so let's talk about the noise model here.",
                    "label": 0
                },
                {
                    "sent": "So the types of noise we add correspond to the different types of noise we've seen so coreference we add misspellings for each artist for missing in space labels we generate, we swap the artist in the album in some cases, so that these labels are swapped and their incorrect.",
                    "label": 0
                },
                {
                    "sent": "An for relationships we either add albums that don't exist, or we exclude albums that do exist, and for all of these facts we generate truth values with Gaussian noise.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that this is the sort of noise you would seen a real web application.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we want to do in our experiments is to look at a baseline which treats all of these independently, so it just uses the truth value, which is a noisy truth value to generate the knowledge base versus using entity resolution or ontological reasoning or the full Knowledge Graph identification model so you can see that both entity resolution and ontological reasoning improve the results, but doing them together in our Knowledge Graph model produces the best results for this data set.",
                    "label": 1
                },
                {
                    "sent": "So it's substantially better than the baseline.",
                    "label": 0
                },
                {
                    "sent": "The second date.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Insight is null and so we wanted to compare against some previous work in the area, which is a very different evaluation setting.",
                    "label": 0
                },
                {
                    "sent": "The more familiar with it, a lot simpler in fact, so the assumptions in previous work have been a closed world model where there is an explicit target set or query set based on the evaluation data and the variables you're allowed to reason about.",
                    "label": 0
                },
                {
                    "sent": "Or a two hop neighborhood around that query set, and in fact it's a little bit simpler because trivially satisfied variables are excluded.",
                    "label": 1
                },
                {
                    "sent": "So earlier we talked about Tristan being both the burden of country.",
                    "label": 0
                },
                {
                    "sent": "If Kristen being a country is in the target set, there's no reason to reason about Kirkus Tan being a bird, even if it's in the extractions in this in this methodology.",
                    "label": 1
                },
                {
                    "sent": "So we believe this is a similar methodology, but we wanted to compare against other work that's been done in this area, and then the full Knowledge Graph approach.",
                    "label": 0
                },
                {
                    "sent": "The one sort of process used for link brains involves a open world model where we look at all possible variable and assign truth values to all of them.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's look at the simpler evaluation setting with the target set, so the baseline again is just using the truth values that come from now and then.",
                    "label": 0
                },
                {
                    "sent": "Nelly has a set of heuristics that uses to decide what gets added to the knowledge base, so that's the second comparison we have and the third comparison is work from jeonghan.",
                    "label": 0
                },
                {
                    "sent": "ICM 2012, where they use a simpler model that uses a discrete probability probabilistic graphical model.",
                    "label": 0
                },
                {
                    "sent": "Does approximate inference so it's an MLN or Markov logic network, and so they also do some joint reasoning and we compare against their work with our full Knowledge Graph identification model.",
                    "label": 1
                },
                {
                    "sent": "And you can see our F1 is much higher and are running time is only 10 seconds for the 25,000 facts that are in this target set.",
                    "label": 0
                },
                {
                    "sent": "The MLN which does joint reasoning takes a few minutes to an hour to run.",
                    "label": 0
                },
                {
                    "sent": "So this is a significant speedup, and it's such a speed up that we can handle the full knowledge graph from using the 1.3 million extractions from melting.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For sort of a complete knowledge graph that contains over 4 million facts in just about 2 hours, 130 minutes.",
                    "label": 0
                },
                {
                    "sent": "So comparing against the nail baseline, which is some heuristics heuristics for promotion, we did significantly better both in AUC and F1, so to conclude.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm really excited about this work because this is a very sophisticated technique to build knowledge graphs that does joint inference.",
                    "label": 0
                },
                {
                    "sent": "It uses both the uncertain information from the web as well as ontological information, and we're using PSL, probabilistic soft logic were able to do this efficiently and were able to handle problems that before were not amenable to joint inference, so please feel free to check out our code which is available on GitHub and I'm happy to take questions.",
                    "label": 0
                }
            ]
        }
    }
}