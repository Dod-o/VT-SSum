{
    "id": "d4so2us24t6x6ms3npjsyy3iwezhtcn5",
    "title": "From collaborative filtering to multitask learning",
    "info": {
        "author": [
            "Alex Smola, Amazon"
        ],
        "published": "July 20, 2010",
        "recorded": "June 2010",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml2010_smola_fcfm/",
    "segmentation": [
        [
            "OK, so I think this is a bit of a change of gear Fridays from well hypergraph matching.",
            "It's actually a very very simple observation that's behind this idea.",
            "So if you see the once you see this connection, you can essentially fall asleep for the rest of the talk, but I'll probably want to.",
            "It'll be good if you stayed awake until then.",
            "OK, so and besides that I mean this is really work with these three guys D. Paycheck, Liberty way too.",
            "In Marcus Vimol three at Yahoo.",
            "And yeah, Marcus did.",
            "And why did most part of the experiment so they actually deserve most of the credit for it?"
        ],
        [
            "Yeah, So what I'm going to talk about.",
            "Essentially it's very obvious connection, at least in hindsight.",
            "It's actually obvious how multi task learning and collaborative filtering are connected.",
            "Well, vice is nice because it actually.",
            "Allows you to import and export theorems about conversions and algorithms from one to the other.",
            "On top of that, it also gives you a connection to kernel learning because in multitask learning quite often you implicitly end up learning the kernel.",
            "Well, this is the part where I think it would be good if you stayed awake after that.",
            "Well."
        ],
        [
            "The factorization approach and so on is kind of fairly immediate once you see once you get this connection, you can just pick and choose between techniques from you know those three problems.",
            "And then I'll show you a bit Al.",
            "You connect those what you can actually do."
        ],
        [
            "With it, and of course the obligatory experiments that show that this is not a bad idea.",
            "OK, so."
        ],
        [
            "Here comes the fun."
        ],
        [
            "Edit learn."
        ],
        [
            "In the kernel.",
            "So how would you usually do it?",
            "Well, so you have some empirical risk term like you classify you regress.",
            "Maybe you want to rank, maybe have a graphical model so you have some loss here and.",
            "If you did it right, it's convex, and if you didn't, it's not convex, but it doesn't really actually matter so much in."
        ],
        [
            "Actors.",
            "And then you typically go and pick some reproducing kernel, Hilbert space, regularizer, at least if you're in the kernel framework and or you use sparsity.",
            "But basically we're just going to stick for this purpose within our cage, so we have some functional Omega of F, which is 1/2 normal if squared.",
            "In practice is something like function values, F, transposed, inverse kernel matrix times F."
        ],
        [
            "OK, and then you realize.",
            "Well, actually, maybe my credit wasn't so great, so you optimize the kernel too.",
            "So you pack on top of these two expressions.",
            "Also penalty for the kernel itself.",
            "One, for instance, might be to pick the trace of the kernel, or you force the trace of the kernel to be bounded by one.",
            "I mean it's kind of equivalent.",
            "What you do, you could pick a wishard regularizer.",
            "There's a lot of stuff that you can do, but I'm going to pick.",
            "Here is 1 where we just bounded trace of the kernel, so it's actually reasonably natural thing to do.",
            "For instance in a lot of Rothermel capacity bounds, it's actually the trace that matter.",
            "So OK, there's a bit of theoretical motivation why it's not very stupid."
        ],
        [
            "OK, so we minimize this piece.",
            "Here we minimize over a phone K. And by tweaking K implicitly, we also play with this regularizer here and a lot of."
        ],
        [
            "People before us have done this.",
            "And the thing that maybe not so many people have really put together to this to that extent is to realize that this actually gives us a backspace norm.",
            "So if you optimize away K, you end up with interesting backspaced norms on your functions.",
            "So a lot of the multiple kernel learning frameworks where you have hierarchies, composite absolute penalties, and so on.",
            "These are essentially just, well, very interesting.",
            "Backspace norms on the function space.",
            "Which are non trivial.",
            "So in other words they don't just work on the set of coefficients, but also which are tractable.",
            "Namely, I can workout the nonparametric parts by just appealing to reducing controllable space.",
            "Now, if you opt for fixed, if you optimize over K, you get another expression Omega prime of F. Which is kind of interesting, and then you can go."
        ],
        [
            "And minimize empirical risk.",
            "+2 newly minted regularise on.",
            "If that's now not an arcade, chase anymore.",
            "There's plenty of papers which do this.",
            "For instance, last year I smell there was a paper by and ring and one of his students whose name I unfortunately forgot where they basically argue.",
            "Well, you know, how should we set the regularization constant?",
            "And then they basically pick up his heart, and lo and behold, once you integrate that out, you get rather than the square of the archest norm.",
            "You get something that goes logarithmically and lo and behold, it works better.",
            "Well, what they did is basically they just had.",
            "You know one functional here they optimize things away and they just switch switch from quadratic Arcadis norm to something that's potentially more interesting.",
            "OK, so this is.",
            "I probably a bit of a caricature of what multiple kernel learning is about, but I think a lot of the algorithms can be actually cleanly subsumed in such a framework now."
        ],
        [
            "It is multi task learning coming.",
            "Well actually.",
            "Multitask learning is also kind of more.",
            "This actually also kind of kernel learning, right?",
            "So what would you usually do?",
            "You have your empirical risk now one per task, you have your regularizer.",
            "Now one per task and you have been some penalty which captures somehow how closely aligned the various function spaces are.",
            "OK. And now if you actually go and perform the same optimization as well, I showed you on the slide before and I'll go through it.",
            "What exactly you get out of it?",
            "It basically means you have some regularizer Omega Prime which depends on F1 through FT.",
            "So these are if I have T tasks.",
            "These are just all the functions for the various tasks stacked."
        ],
        [
            "This is, for instance what Andreas Argyriou did, so his paper I think 2008, 2009.",
            "He wrote several of them, but the gist of them is pretty much to use.",
            "This type of regularizer for, you know.",
            "This connection here, so this is the standard darkest RKS regularizer just written with the inverse kernel matrix as you would do it in a Gaussian process.",
            "So you have per task if FJ, transposed, K, inverse FJ.",
            "Under the constraint that will K obviously positive semidefinite and that its trace is bounded by 1.",
            "OK, so now if you don't get this optimization problem, this is actually one that you can solve, and that's exactly."
        ],
        [
            "But we did.",
            "And it's not very difficult to see.",
            "Basically, you just push this around.",
            "You pulque inverse out and you write a trace in front of it that this can be written as the solution of this optimization is the Kyphon norm of basically the entire set of functions stacked together.",
            "So the Kyphon norm is just the sum over the singular values of this beast.",
            "Now you've probably seen this before, right?"
        ],
        [
            "This is the nuclear norm in collaborative filtering that we all love and use.",
            "So in other words, this is exactly the norm that you would use if you didn't know that these were individual tasks, But basically just treat them as users and, well, just where, rather than having the movies, you might just have the features of the movies.",
            "So in collaborative filtering you basically have users and movies and usually just have their IDs.",
            "Now imagine that rather than actually knowing their idea of the movie, I just knew well this is an action movie and it has these actors and it was done at the given time and it that's its length and so on and so on.",
            "If you just had a feature based approach then multi task learning problem would be one of trying to do, you know.",
            "Basically treat each user as a separate task to do well for all the users.",
            "Now it's just essentially a small change of perspective, right?",
            "We just go from an index to features and we're from collaborative filtering land in multi task learning land, so it's not very surprising that you should get the same norm, and it's also not very surprising.",
            "The things that this thing will actually works really well because, well, it's actually doing the same thing as what you would have in collaborative filtering.",
            "OK."
        ],
        [
            "So."
        ],
        [
            "This is what you do in collaborative filtering.",
            "You have this expression.",
            "We have some loss while YJR for instance, the ratings that observe if is what I estimate and then you take this caifan norm.",
            "That's what nutty flavor and I wrote in a whole bunch of papers with very nice uniform convergence theory and all that to it.",
            "And then you can make your life a Royal pain to actually optimize with this convex problem.",
            "Why is the Royal pain?",
            "Because you have.",
            "Well, it's a convex from which is nice, but it has a fairly monstrous matrix in there and solving it directly by brute force is expensive.",
            "So what essentially everybody does is they say this is this nice connection that you figured out that.",
            "Well, yeah, you can actually write it as a two factor model.",
            "We now have to Frobenius norm squared for each of the factors.",
            "OK, that's what everybody uses."
        ],
        [
            "Now we can do the very same thing for multitask learning, right?",
            "So rather than taking this caifan norm.",
            "The equivalent formulation is to have again the two factor model, so we just have a latent space of functions that describe what our problem is like.",
            "We have a couple of combination factors which tell us you know how.",
            "For individual tasks those latent functions should be combined, and then we just have the standard for business norm on the matrix.",
            "Another business norm on the set of well lighting functions, and that is basically nothing else.",
            "And just taking the sum over all the Arcgis norms of the individual functions.",
            "So to recap, what has happened is we've now gone from a multitask learning problem.",
            "That is fairly popular in the kernels and frequentist world to something that Bayesians would do right away anyway, right?",
            "What they would do is they would just say well look, let's use a two factor model, one for the users, one for the movies and we have to interact.",
            "And if there's something parametric in it, so be it so.",
            "And that just shows those two things are connected.",
            "Now you can go and take your favorite papers from collaborative filtering or multitask learning.",
            "Look at the bounds and translate.",
            "OK. We didn't do that, but we actually want we had a problem to solve in Yahoo, so let me give you a bit of a background why you would care about stuff like that."
        ],
        [
            "So fat."
        ],
        [
            "Translation this is just again.",
            "While driving home, the point that I had before this is what you would do in a very simple collaborative filtering model, and this is just the plate notation, so each of those things is a for loop and where they intersect.",
            "It just means you have essentially for loops of subsets OK. Now if IJUI just post MJ users movies, latent features, observed writing."
        ],
        [
            "OK, here's a feature based system and this is what you will typically do in machine learning ranking.",
            "You have.",
            "You know features about the user, features about the movie machine learning ranking.",
            "It's basically page and query and you get the ratings.",
            "And So what you would do is or what you could, for instance, do is.",
            "You could just have a bilinear model, so you have feature of the user, transpose some interaction matrix features of the movie, so there's for instance, a paper that Olivia Chappelle and Jason Weston published.",
            "But I cannot pull this EM apart, I just factorize it and so I have a two factor model.",
            "This is equivalent if I just pick a large enough rank.",
            "So I have few times.",
            "Well now whatever is mapped in terms of features of the user in a product with whatever happens to the features of the movie.",
            "OK. Now you've probably been wondering why there's a bit of space here, right?"
        ],
        [
            "So you can combine this."
        ],
        [
            "And that's actually what gives you a bit of a better model.",
            "So what you do is you basically have the feature based component."
        ],
        [
            "This is what you will get from multitask learning."
        ],
        [
            "Then the collaborative filtering part."
        ],
        [
            "Then the biased part that actually makes collaborative filtering work better and you have the same thing for users in movies.",
            "So now what you've done is you've immediately combined the feature based model.",
            "And the like factor model.",
            "Why would you care about this?",
            "Well, if you get the new movie, you don't know anything about that movie yet besides its features.",
            "You haven't actually shown it to many users yet.",
            "Well, at that point you can just use your default.",
            "OK, good.",
            "So we use this as I show the user to more the movie to more users.",
            "Well, of course I can personalize here data for the user."
        ],
        [
            "And yes, you can use it for collaborative filtering, ranking, multi task data, set integration.",
            "So here the problem is basically 1/2 on the same domain.",
            "A bunch of ontologies I don't quite know how they match up together and actually want to translate between them.",
            "Or of course time series prediction.",
            "Stock values are correlated and you assume they live in a lower dimensional space."
        ],
        [
            "And of course you can go further.",
            "You can pack more terms onto it and this is."
        ],
        [
            "Actually well known it's called attacker factorization."
        ],
        [
            "I.",
            "So rather than just users in movies I have had, for instance some other context like the time of day and so I factor this."
        ],
        [
            "OK, how you optimize it you could do something really complicated like alternating convex optimization.",
            "And this is expensive, and it means that you have to do a full pass or data set.",
            "Then you make progress on the users.",
            "Then you make progress in the movies and so on.",
            "We did this and you need a couple of dozen passes.",
            "This is expensive if you have a large data set.",
            "So what can you do instead?",
            "You just use the caster antecedent.",
            "Well, actually stochastic.",
            "Granted St isn't that dumb if you think about it.",
            "Think about what happens if you do successive quadratic optimization.",
            "So the problem is convex in the users.",
            "The problem problem is convex in the movies, but it's not jointly convex.",
            "So you basically take a convex upper bound on the problem.",
            "You minimize that you take another convex upper bound for the movies.",
            "You minimize that and you keep on alternating.",
            "Now if you do stochastic gradient descent, your pointwise locally getting the best convex upper bound on both things, you take a step and then move on.",
            "So basically your upper bounds are a lot more recent because they're only, they're always just.",
            "You only take basically one parameter step, and then you update them again.",
            "So in that sense, stochastic granted sent isn't such a dumb procedure there.",
            "OK, does it work?"
        ],
        [
            "In practice, yes it does.",
            "Otherwise, I would be embarrassed to talk about it here."
        ],
        [
            "So we took a couple of small data sets where we were actually allowed to publish things about right?",
            "So we have like 3 million users and well, 150,000 documents.",
            "It's not too much.",
            "This basically just click prediction in terms of you know what documents you want to see.",
            "And well, so we had about 43 million user document pairs where users might have clicked or not clicked on it.",
            "OK, the other problem was a page classification problems is actually something that you might want to do it, for instance to find out whether a page has adult content or whether it's a blog or a home page.",
            "And.",
            "Well, so this is actually a close approximation to something that's running in production, so there are 82 different classification problems.",
            "And why this problem was solved before?",
            "Is that basically about five or six engineers trained altogether?",
            "Let's say more than 82 different classifiers on it, right?",
            "Anne.",
            "The so the problems, I mean you can see is for instance spam or not US market.",
            "That's the type of classification problems that you want to solve.",
            "And you want to get this really right?",
            "Especially for adult content, because otherwise you will get somebody saying, Oh my child got IDC's because it's all naked skin.",
            "So to say.",
            "And I mean, people in the United States get really upset about this.",
            "So you'd rather want to get this right?",
            "So anyway?",
            "OK, so this is a sum."
        ],
        [
            "And features.",
            "And so let me just show you what happens in the click model.",
            "So what we did is we.",
            "Just this, once using pages without any user features.",
            "Well, OK, you get something that works.",
            "Kind of OK.",
            "If you know.",
            "So this was no feature whatsoever.",
            "If we add the feature about the user.",
            "Then of course performance goes off, so this is basically the personalized version.",
            "If we then also add the information about the page that that contains the data, they now performance goes up further.",
            "OK, so why can we do anything useful here at all?",
            "Because I mean even if you have no idea what page you're displaying, if you just show it often enough, you'll find out whether users click on it.",
            "So this is the fully nampar personalized model that you would just, you know, rank documents overall.",
            "This would be a personalized one where you still don't look at the content.",
            "This is basically you look at the content and you look at the user."
        ],
        [
            "Lynn Waffle page classification.",
            "This was quite a fun example.",
            "So OK, so this doesn't work terribly well.",
            "It's just a simple baseline.",
            "Those two lines are interesting.",
            "So remember the task ID was something like US market spam or not spam and we just took them verbatim.",
            "So we just really took the ASCII sequence and we use that as a feature.",
            "So we just broke it down into this camel case.",
            "So we just took the individual words.",
            "We use them as features so that was enough information to get us quite a bit of a gain from the baseline.",
            "So that told us, you know how the various tasks are related, because I mean we have another problem that is spam or not spam UK market spam or not spam.",
            "Let's say India and so on its own and will adult or I don't know homepage or blog and whatever the other into."
        ],
        [
            "Thing is the following.",
            "And OK, this is probably due to the fact that we probably didn't regularize so well.",
            "So for the clicks like very small number of dimensions actually worked fairly well.",
            "Now this is something that some of our AI engineers probably should worry about, right?",
            "Is it basically means out of those 8283 different tasks?",
            "There's really only like 3, maybe 4.",
            "Different problems in the entire data set.",
            "And everything else is just fine tuning and weighting the tasks against each other.",
            "Rather than building a T2 classifier separately.",
            "So I guess what I?"
        ],
        [
            "At least what I've tried."
        ],
        [
            "To show you is."
        ],
        [
            "Fat you know?",
            "Those three problems are actually very innately connected.",
            "Multitask learning.",
            "Collaborative filtering and learning the kernel.",
            "It doesn't work for all possible choices of penalty on the kernel, but you could probably workout very similar relations to what we did for different ways of how to set the kernel and how you how you learn it and how you set the multitask problem.",
            "And.",
            "Well, once you have that, of course you can import the algorithms and the theory and everything between the different problems.",
            "And OK, well, we haven't really pulled the theoretical guarantees from 1 from 1 area yet and imported into the other, but that would be a very easy thing to do now that you have to have to handle.",
            "And then yes, you can use it and it actually gives you reasonably integrated models between you know.",
            "Multitask learning Hotstar problems were actually rather cold start problems.",
            "We don't know anything else about the problem and then OK, the optimization is just idiot proof simple.",
            "I mean, we just do stochastic gradient ascent and in the end I hope that at least based on those two toy examples with Yahoo data, I managed to convince you that you know this stuff actually does something useful.",
            "And that concludes my talk."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I think this is a bit of a change of gear Fridays from well hypergraph matching.",
                    "label": 0
                },
                {
                    "sent": "It's actually a very very simple observation that's behind this idea.",
                    "label": 0
                },
                {
                    "sent": "So if you see the once you see this connection, you can essentially fall asleep for the rest of the talk, but I'll probably want to.",
                    "label": 0
                },
                {
                    "sent": "It'll be good if you stayed awake until then.",
                    "label": 0
                },
                {
                    "sent": "OK, so and besides that I mean this is really work with these three guys D. Paycheck, Liberty way too.",
                    "label": 0
                },
                {
                    "sent": "In Marcus Vimol three at Yahoo.",
                    "label": 0
                },
                {
                    "sent": "And yeah, Marcus did.",
                    "label": 0
                },
                {
                    "sent": "And why did most part of the experiment so they actually deserve most of the credit for it?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, So what I'm going to talk about.",
                    "label": 0
                },
                {
                    "sent": "Essentially it's very obvious connection, at least in hindsight.",
                    "label": 0
                },
                {
                    "sent": "It's actually obvious how multi task learning and collaborative filtering are connected.",
                    "label": 1
                },
                {
                    "sent": "Well, vice is nice because it actually.",
                    "label": 0
                },
                {
                    "sent": "Allows you to import and export theorems about conversions and algorithms from one to the other.",
                    "label": 0
                },
                {
                    "sent": "On top of that, it also gives you a connection to kernel learning because in multitask learning quite often you implicitly end up learning the kernel.",
                    "label": 1
                },
                {
                    "sent": "Well, this is the part where I think it would be good if you stayed awake after that.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The factorization approach and so on is kind of fairly immediate once you see once you get this connection, you can just pick and choose between techniques from you know those three problems.",
                    "label": 1
                },
                {
                    "sent": "And then I'll show you a bit Al.",
                    "label": 0
                },
                {
                    "sent": "You connect those what you can actually do.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With it, and of course the obligatory experiments that show that this is not a bad idea.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here comes the fun.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Edit learn.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the kernel.",
                    "label": 0
                },
                {
                    "sent": "So how would you usually do it?",
                    "label": 0
                },
                {
                    "sent": "Well, so you have some empirical risk term like you classify you regress.",
                    "label": 1
                },
                {
                    "sent": "Maybe you want to rank, maybe have a graphical model so you have some loss here and.",
                    "label": 0
                },
                {
                    "sent": "If you did it right, it's convex, and if you didn't, it's not convex, but it doesn't really actually matter so much in.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Actors.",
                    "label": 0
                },
                {
                    "sent": "And then you typically go and pick some reproducing kernel, Hilbert space, regularizer, at least if you're in the kernel framework and or you use sparsity.",
                    "label": 1
                },
                {
                    "sent": "But basically we're just going to stick for this purpose within our cage, so we have some functional Omega of F, which is 1/2 normal if squared.",
                    "label": 0
                },
                {
                    "sent": "In practice is something like function values, F, transposed, inverse kernel matrix times F.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and then you realize.",
                    "label": 0
                },
                {
                    "sent": "Well, actually, maybe my credit wasn't so great, so you optimize the kernel too.",
                    "label": 0
                },
                {
                    "sent": "So you pack on top of these two expressions.",
                    "label": 0
                },
                {
                    "sent": "Also penalty for the kernel itself.",
                    "label": 1
                },
                {
                    "sent": "One, for instance, might be to pick the trace of the kernel, or you force the trace of the kernel to be bounded by one.",
                    "label": 0
                },
                {
                    "sent": "I mean it's kind of equivalent.",
                    "label": 1
                },
                {
                    "sent": "What you do, you could pick a wishard regularizer.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of stuff that you can do, but I'm going to pick.",
                    "label": 0
                },
                {
                    "sent": "Here is 1 where we just bounded trace of the kernel, so it's actually reasonably natural thing to do.",
                    "label": 0
                },
                {
                    "sent": "For instance in a lot of Rothermel capacity bounds, it's actually the trace that matter.",
                    "label": 0
                },
                {
                    "sent": "So OK, there's a bit of theoretical motivation why it's not very stupid.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we minimize this piece.",
                    "label": 0
                },
                {
                    "sent": "Here we minimize over a phone K. And by tweaking K implicitly, we also play with this regularizer here and a lot of.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "People before us have done this.",
                    "label": 0
                },
                {
                    "sent": "And the thing that maybe not so many people have really put together to this to that extent is to realize that this actually gives us a backspace norm.",
                    "label": 0
                },
                {
                    "sent": "So if you optimize away K, you end up with interesting backspaced norms on your functions.",
                    "label": 0
                },
                {
                    "sent": "So a lot of the multiple kernel learning frameworks where you have hierarchies, composite absolute penalties, and so on.",
                    "label": 0
                },
                {
                    "sent": "These are essentially just, well, very interesting.",
                    "label": 0
                },
                {
                    "sent": "Backspace norms on the function space.",
                    "label": 0
                },
                {
                    "sent": "Which are non trivial.",
                    "label": 0
                },
                {
                    "sent": "So in other words they don't just work on the set of coefficients, but also which are tractable.",
                    "label": 0
                },
                {
                    "sent": "Namely, I can workout the nonparametric parts by just appealing to reducing controllable space.",
                    "label": 0
                },
                {
                    "sent": "Now, if you opt for fixed, if you optimize over K, you get another expression Omega prime of F. Which is kind of interesting, and then you can go.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And minimize empirical risk.",
                    "label": 0
                },
                {
                    "sent": "+2 newly minted regularise on.",
                    "label": 0
                },
                {
                    "sent": "If that's now not an arcade, chase anymore.",
                    "label": 0
                },
                {
                    "sent": "There's plenty of papers which do this.",
                    "label": 0
                },
                {
                    "sent": "For instance, last year I smell there was a paper by and ring and one of his students whose name I unfortunately forgot where they basically argue.",
                    "label": 0
                },
                {
                    "sent": "Well, you know, how should we set the regularization constant?",
                    "label": 0
                },
                {
                    "sent": "And then they basically pick up his heart, and lo and behold, once you integrate that out, you get rather than the square of the archest norm.",
                    "label": 0
                },
                {
                    "sent": "You get something that goes logarithmically and lo and behold, it works better.",
                    "label": 0
                },
                {
                    "sent": "Well, what they did is basically they just had.",
                    "label": 0
                },
                {
                    "sent": "You know one functional here they optimize things away and they just switch switch from quadratic Arcadis norm to something that's potentially more interesting.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is.",
                    "label": 0
                },
                {
                    "sent": "I probably a bit of a caricature of what multiple kernel learning is about, but I think a lot of the algorithms can be actually cleanly subsumed in such a framework now.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It is multi task learning coming.",
                    "label": 0
                },
                {
                    "sent": "Well actually.",
                    "label": 0
                },
                {
                    "sent": "Multitask learning is also kind of more.",
                    "label": 1
                },
                {
                    "sent": "This actually also kind of kernel learning, right?",
                    "label": 0
                },
                {
                    "sent": "So what would you usually do?",
                    "label": 0
                },
                {
                    "sent": "You have your empirical risk now one per task, you have your regularizer.",
                    "label": 0
                },
                {
                    "sent": "Now one per task and you have been some penalty which captures somehow how closely aligned the various function spaces are.",
                    "label": 0
                },
                {
                    "sent": "OK. And now if you actually go and perform the same optimization as well, I showed you on the slide before and I'll go through it.",
                    "label": 0
                },
                {
                    "sent": "What exactly you get out of it?",
                    "label": 0
                },
                {
                    "sent": "It basically means you have some regularizer Omega Prime which depends on F1 through FT.",
                    "label": 0
                },
                {
                    "sent": "So these are if I have T tasks.",
                    "label": 0
                },
                {
                    "sent": "These are just all the functions for the various tasks stacked.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is, for instance what Andreas Argyriou did, so his paper I think 2008, 2009.",
                    "label": 0
                },
                {
                    "sent": "He wrote several of them, but the gist of them is pretty much to use.",
                    "label": 0
                },
                {
                    "sent": "This type of regularizer for, you know.",
                    "label": 0
                },
                {
                    "sent": "This connection here, so this is the standard darkest RKS regularizer just written with the inverse kernel matrix as you would do it in a Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "So you have per task if FJ, transposed, K, inverse FJ.",
                    "label": 0
                },
                {
                    "sent": "Under the constraint that will K obviously positive semidefinite and that its trace is bounded by 1.",
                    "label": 0
                },
                {
                    "sent": "OK, so now if you don't get this optimization problem, this is actually one that you can solve, and that's exactly.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But we did.",
                    "label": 0
                },
                {
                    "sent": "And it's not very difficult to see.",
                    "label": 0
                },
                {
                    "sent": "Basically, you just push this around.",
                    "label": 0
                },
                {
                    "sent": "You pulque inverse out and you write a trace in front of it that this can be written as the solution of this optimization is the Kyphon norm of basically the entire set of functions stacked together.",
                    "label": 0
                },
                {
                    "sent": "So the Kyphon norm is just the sum over the singular values of this beast.",
                    "label": 0
                },
                {
                    "sent": "Now you've probably seen this before, right?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is the nuclear norm in collaborative filtering that we all love and use.",
                    "label": 1
                },
                {
                    "sent": "So in other words, this is exactly the norm that you would use if you didn't know that these were individual tasks, But basically just treat them as users and, well, just where, rather than having the movies, you might just have the features of the movies.",
                    "label": 0
                },
                {
                    "sent": "So in collaborative filtering you basically have users and movies and usually just have their IDs.",
                    "label": 0
                },
                {
                    "sent": "Now imagine that rather than actually knowing their idea of the movie, I just knew well this is an action movie and it has these actors and it was done at the given time and it that's its length and so on and so on.",
                    "label": 0
                },
                {
                    "sent": "If you just had a feature based approach then multi task learning problem would be one of trying to do, you know.",
                    "label": 0
                },
                {
                    "sent": "Basically treat each user as a separate task to do well for all the users.",
                    "label": 0
                },
                {
                    "sent": "Now it's just essentially a small change of perspective, right?",
                    "label": 0
                },
                {
                    "sent": "We just go from an index to features and we're from collaborative filtering land in multi task learning land, so it's not very surprising that you should get the same norm, and it's also not very surprising.",
                    "label": 0
                },
                {
                    "sent": "The things that this thing will actually works really well because, well, it's actually doing the same thing as what you would have in collaborative filtering.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is what you do in collaborative filtering.",
                    "label": 1
                },
                {
                    "sent": "You have this expression.",
                    "label": 0
                },
                {
                    "sent": "We have some loss while YJR for instance, the ratings that observe if is what I estimate and then you take this caifan norm.",
                    "label": 0
                },
                {
                    "sent": "That's what nutty flavor and I wrote in a whole bunch of papers with very nice uniform convergence theory and all that to it.",
                    "label": 0
                },
                {
                    "sent": "And then you can make your life a Royal pain to actually optimize with this convex problem.",
                    "label": 0
                },
                {
                    "sent": "Why is the Royal pain?",
                    "label": 0
                },
                {
                    "sent": "Because you have.",
                    "label": 0
                },
                {
                    "sent": "Well, it's a convex from which is nice, but it has a fairly monstrous matrix in there and solving it directly by brute force is expensive.",
                    "label": 0
                },
                {
                    "sent": "So what essentially everybody does is they say this is this nice connection that you figured out that.",
                    "label": 0
                },
                {
                    "sent": "Well, yeah, you can actually write it as a two factor model.",
                    "label": 0
                },
                {
                    "sent": "We now have to Frobenius norm squared for each of the factors.",
                    "label": 0
                },
                {
                    "sent": "OK, that's what everybody uses.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we can do the very same thing for multitask learning, right?",
                    "label": 0
                },
                {
                    "sent": "So rather than taking this caifan norm.",
                    "label": 0
                },
                {
                    "sent": "The equivalent formulation is to have again the two factor model, so we just have a latent space of functions that describe what our problem is like.",
                    "label": 1
                },
                {
                    "sent": "We have a couple of combination factors which tell us you know how.",
                    "label": 0
                },
                {
                    "sent": "For individual tasks those latent functions should be combined, and then we just have the standard for business norm on the matrix.",
                    "label": 0
                },
                {
                    "sent": "Another business norm on the set of well lighting functions, and that is basically nothing else.",
                    "label": 0
                },
                {
                    "sent": "And just taking the sum over all the Arcgis norms of the individual functions.",
                    "label": 0
                },
                {
                    "sent": "So to recap, what has happened is we've now gone from a multitask learning problem.",
                    "label": 0
                },
                {
                    "sent": "That is fairly popular in the kernels and frequentist world to something that Bayesians would do right away anyway, right?",
                    "label": 0
                },
                {
                    "sent": "What they would do is they would just say well look, let's use a two factor model, one for the users, one for the movies and we have to interact.",
                    "label": 0
                },
                {
                    "sent": "And if there's something parametric in it, so be it so.",
                    "label": 0
                },
                {
                    "sent": "And that just shows those two things are connected.",
                    "label": 0
                },
                {
                    "sent": "Now you can go and take your favorite papers from collaborative filtering or multitask learning.",
                    "label": 1
                },
                {
                    "sent": "Look at the bounds and translate.",
                    "label": 0
                },
                {
                    "sent": "OK. We didn't do that, but we actually want we had a problem to solve in Yahoo, so let me give you a bit of a background why you would care about stuff like that.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So fat.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Translation this is just again.",
                    "label": 0
                },
                {
                    "sent": "While driving home, the point that I had before this is what you would do in a very simple collaborative filtering model, and this is just the plate notation, so each of those things is a for loop and where they intersect.",
                    "label": 0
                },
                {
                    "sent": "It just means you have essentially for loops of subsets OK. Now if IJUI just post MJ users movies, latent features, observed writing.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, here's a feature based system and this is what you will typically do in machine learning ranking.",
                    "label": 0
                },
                {
                    "sent": "You have.",
                    "label": 0
                },
                {
                    "sent": "You know features about the user, features about the movie machine learning ranking.",
                    "label": 0
                },
                {
                    "sent": "It's basically page and query and you get the ratings.",
                    "label": 0
                },
                {
                    "sent": "And So what you would do is or what you could, for instance, do is.",
                    "label": 0
                },
                {
                    "sent": "You could just have a bilinear model, so you have feature of the user, transpose some interaction matrix features of the movie, so there's for instance, a paper that Olivia Chappelle and Jason Weston published.",
                    "label": 0
                },
                {
                    "sent": "But I cannot pull this EM apart, I just factorize it and so I have a two factor model.",
                    "label": 0
                },
                {
                    "sent": "This is equivalent if I just pick a large enough rank.",
                    "label": 0
                },
                {
                    "sent": "So I have few times.",
                    "label": 0
                },
                {
                    "sent": "Well now whatever is mapped in terms of features of the user in a product with whatever happens to the features of the movie.",
                    "label": 0
                },
                {
                    "sent": "OK. Now you've probably been wondering why there's a bit of space here, right?",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you can combine this.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's actually what gives you a bit of a better model.",
                    "label": 0
                },
                {
                    "sent": "So what you do is you basically have the feature based component.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is what you will get from multitask learning.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then the collaborative filtering part.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then the biased part that actually makes collaborative filtering work better and you have the same thing for users in movies.",
                    "label": 0
                },
                {
                    "sent": "So now what you've done is you've immediately combined the feature based model.",
                    "label": 0
                },
                {
                    "sent": "And the like factor model.",
                    "label": 0
                },
                {
                    "sent": "Why would you care about this?",
                    "label": 0
                },
                {
                    "sent": "Well, if you get the new movie, you don't know anything about that movie yet besides its features.",
                    "label": 0
                },
                {
                    "sent": "You haven't actually shown it to many users yet.",
                    "label": 0
                },
                {
                    "sent": "Well, at that point you can just use your default.",
                    "label": 0
                },
                {
                    "sent": "OK, good.",
                    "label": 0
                },
                {
                    "sent": "So we use this as I show the user to more the movie to more users.",
                    "label": 0
                },
                {
                    "sent": "Well, of course I can personalize here data for the user.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And yes, you can use it for collaborative filtering, ranking, multi task data, set integration.",
                    "label": 0
                },
                {
                    "sent": "So here the problem is basically 1/2 on the same domain.",
                    "label": 0
                },
                {
                    "sent": "A bunch of ontologies I don't quite know how they match up together and actually want to translate between them.",
                    "label": 0
                },
                {
                    "sent": "Or of course time series prediction.",
                    "label": 0
                },
                {
                    "sent": "Stock values are correlated and you assume they live in a lower dimensional space.",
                    "label": 1
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And of course you can go further.",
                    "label": 0
                },
                {
                    "sent": "You can pack more terms onto it and this is.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually well known it's called attacker factorization.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "So rather than just users in movies I have had, for instance some other context like the time of day and so I factor this.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, how you optimize it you could do something really complicated like alternating convex optimization.",
                    "label": 0
                },
                {
                    "sent": "And this is expensive, and it means that you have to do a full pass or data set.",
                    "label": 0
                },
                {
                    "sent": "Then you make progress on the users.",
                    "label": 0
                },
                {
                    "sent": "Then you make progress in the movies and so on.",
                    "label": 0
                },
                {
                    "sent": "We did this and you need a couple of dozen passes.",
                    "label": 0
                },
                {
                    "sent": "This is expensive if you have a large data set.",
                    "label": 0
                },
                {
                    "sent": "So what can you do instead?",
                    "label": 0
                },
                {
                    "sent": "You just use the caster antecedent.",
                    "label": 0
                },
                {
                    "sent": "Well, actually stochastic.",
                    "label": 0
                },
                {
                    "sent": "Granted St isn't that dumb if you think about it.",
                    "label": 0
                },
                {
                    "sent": "Think about what happens if you do successive quadratic optimization.",
                    "label": 0
                },
                {
                    "sent": "So the problem is convex in the users.",
                    "label": 0
                },
                {
                    "sent": "The problem problem is convex in the movies, but it's not jointly convex.",
                    "label": 0
                },
                {
                    "sent": "So you basically take a convex upper bound on the problem.",
                    "label": 0
                },
                {
                    "sent": "You minimize that you take another convex upper bound for the movies.",
                    "label": 0
                },
                {
                    "sent": "You minimize that and you keep on alternating.",
                    "label": 0
                },
                {
                    "sent": "Now if you do stochastic gradient descent, your pointwise locally getting the best convex upper bound on both things, you take a step and then move on.",
                    "label": 1
                },
                {
                    "sent": "So basically your upper bounds are a lot more recent because they're only, they're always just.",
                    "label": 0
                },
                {
                    "sent": "You only take basically one parameter step, and then you update them again.",
                    "label": 0
                },
                {
                    "sent": "So in that sense, stochastic granted sent isn't such a dumb procedure there.",
                    "label": 0
                },
                {
                    "sent": "OK, does it work?",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In practice, yes it does.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, I would be embarrassed to talk about it here.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we took a couple of small data sets where we were actually allowed to publish things about right?",
                    "label": 0
                },
                {
                    "sent": "So we have like 3 million users and well, 150,000 documents.",
                    "label": 0
                },
                {
                    "sent": "It's not too much.",
                    "label": 0
                },
                {
                    "sent": "This basically just click prediction in terms of you know what documents you want to see.",
                    "label": 1
                },
                {
                    "sent": "And well, so we had about 43 million user document pairs where users might have clicked or not clicked on it.",
                    "label": 1
                },
                {
                    "sent": "OK, the other problem was a page classification problems is actually something that you might want to do it, for instance to find out whether a page has adult content or whether it's a blog or a home page.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Well, so this is actually a close approximation to something that's running in production, so there are 82 different classification problems.",
                    "label": 0
                },
                {
                    "sent": "And why this problem was solved before?",
                    "label": 0
                },
                {
                    "sent": "Is that basically about five or six engineers trained altogether?",
                    "label": 0
                },
                {
                    "sent": "Let's say more than 82 different classifiers on it, right?",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "The so the problems, I mean you can see is for instance spam or not US market.",
                    "label": 1
                },
                {
                    "sent": "That's the type of classification problems that you want to solve.",
                    "label": 0
                },
                {
                    "sent": "And you want to get this really right?",
                    "label": 0
                },
                {
                    "sent": "Especially for adult content, because otherwise you will get somebody saying, Oh my child got IDC's because it's all naked skin.",
                    "label": 0
                },
                {
                    "sent": "So to say.",
                    "label": 0
                },
                {
                    "sent": "And I mean, people in the United States get really upset about this.",
                    "label": 0
                },
                {
                    "sent": "So you'd rather want to get this right?",
                    "label": 0
                },
                {
                    "sent": "So anyway?",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a sum.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And features.",
                    "label": 0
                },
                {
                    "sent": "And so let me just show you what happens in the click model.",
                    "label": 1
                },
                {
                    "sent": "So what we did is we.",
                    "label": 0
                },
                {
                    "sent": "Just this, once using pages without any user features.",
                    "label": 0
                },
                {
                    "sent": "Well, OK, you get something that works.",
                    "label": 0
                },
                {
                    "sent": "Kind of OK.",
                    "label": 0
                },
                {
                    "sent": "If you know.",
                    "label": 0
                },
                {
                    "sent": "So this was no feature whatsoever.",
                    "label": 1
                },
                {
                    "sent": "If we add the feature about the user.",
                    "label": 0
                },
                {
                    "sent": "Then of course performance goes off, so this is basically the personalized version.",
                    "label": 0
                },
                {
                    "sent": "If we then also add the information about the page that that contains the data, they now performance goes up further.",
                    "label": 0
                },
                {
                    "sent": "OK, so why can we do anything useful here at all?",
                    "label": 0
                },
                {
                    "sent": "Because I mean even if you have no idea what page you're displaying, if you just show it often enough, you'll find out whether users click on it.",
                    "label": 0
                },
                {
                    "sent": "So this is the fully nampar personalized model that you would just, you know, rank documents overall.",
                    "label": 0
                },
                {
                    "sent": "This would be a personalized one where you still don't look at the content.",
                    "label": 0
                },
                {
                    "sent": "This is basically you look at the content and you look at the user.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lynn Waffle page classification.",
                    "label": 0
                },
                {
                    "sent": "This was quite a fun example.",
                    "label": 0
                },
                {
                    "sent": "So OK, so this doesn't work terribly well.",
                    "label": 0
                },
                {
                    "sent": "It's just a simple baseline.",
                    "label": 0
                },
                {
                    "sent": "Those two lines are interesting.",
                    "label": 0
                },
                {
                    "sent": "So remember the task ID was something like US market spam or not spam and we just took them verbatim.",
                    "label": 0
                },
                {
                    "sent": "So we just really took the ASCII sequence and we use that as a feature.",
                    "label": 0
                },
                {
                    "sent": "So we just broke it down into this camel case.",
                    "label": 0
                },
                {
                    "sent": "So we just took the individual words.",
                    "label": 0
                },
                {
                    "sent": "We use them as features so that was enough information to get us quite a bit of a gain from the baseline.",
                    "label": 0
                },
                {
                    "sent": "So that told us, you know how the various tasks are related, because I mean we have another problem that is spam or not spam UK market spam or not spam.",
                    "label": 0
                },
                {
                    "sent": "Let's say India and so on its own and will adult or I don't know homepage or blog and whatever the other into.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thing is the following.",
                    "label": 0
                },
                {
                    "sent": "And OK, this is probably due to the fact that we probably didn't regularize so well.",
                    "label": 0
                },
                {
                    "sent": "So for the clicks like very small number of dimensions actually worked fairly well.",
                    "label": 0
                },
                {
                    "sent": "Now this is something that some of our AI engineers probably should worry about, right?",
                    "label": 0
                },
                {
                    "sent": "Is it basically means out of those 8283 different tasks?",
                    "label": 0
                },
                {
                    "sent": "There's really only like 3, maybe 4.",
                    "label": 0
                },
                {
                    "sent": "Different problems in the entire data set.",
                    "label": 0
                },
                {
                    "sent": "And everything else is just fine tuning and weighting the tasks against each other.",
                    "label": 0
                },
                {
                    "sent": "Rather than building a T2 classifier separately.",
                    "label": 0
                },
                {
                    "sent": "So I guess what I?",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At least what I've tried.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To show you is.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Fat you know?",
                    "label": 0
                },
                {
                    "sent": "Those three problems are actually very innately connected.",
                    "label": 0
                },
                {
                    "sent": "Multitask learning.",
                    "label": 0
                },
                {
                    "sent": "Collaborative filtering and learning the kernel.",
                    "label": 1
                },
                {
                    "sent": "It doesn't work for all possible choices of penalty on the kernel, but you could probably workout very similar relations to what we did for different ways of how to set the kernel and how you how you learn it and how you set the multitask problem.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Well, once you have that, of course you can import the algorithms and the theory and everything between the different problems.",
                    "label": 0
                },
                {
                    "sent": "And OK, well, we haven't really pulled the theoretical guarantees from 1 from 1 area yet and imported into the other, but that would be a very easy thing to do now that you have to have to handle.",
                    "label": 0
                },
                {
                    "sent": "And then yes, you can use it and it actually gives you reasonably integrated models between you know.",
                    "label": 0
                },
                {
                    "sent": "Multitask learning Hotstar problems were actually rather cold start problems.",
                    "label": 0
                },
                {
                    "sent": "We don't know anything else about the problem and then OK, the optimization is just idiot proof simple.",
                    "label": 0
                },
                {
                    "sent": "I mean, we just do stochastic gradient ascent and in the end I hope that at least based on those two toy examples with Yahoo data, I managed to convince you that you know this stuff actually does something useful.",
                    "label": 0
                },
                {
                    "sent": "And that concludes my talk.",
                    "label": 0
                }
            ]
        }
    }
}