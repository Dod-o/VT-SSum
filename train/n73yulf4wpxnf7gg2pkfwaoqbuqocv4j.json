{
    "id": "n73yulf4wpxnf7gg2pkfwaoqbuqocv4j",
    "title": "Semi-Supervised Learning in Gigantic Image Collections",
    "info": {
        "author": [
            "Rob Fergus, New York University (NYU)"
        ],
        "published": "Jan. 19, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/nips09_fergus_sslg/",
    "segmentation": [
        [
            "So our goal is to develop search image search scale to the billions of images on the web.",
            "So one property of intent images is that they contain a wide range of label information, so a tiny fraction have been manually annotated, so have labels that are reliable."
        ],
        [
            "But a far larger fraction have some kind of noisy label, perhaps derived from the filename or the surrounding HTML."
        ],
        [
            "And these cues provides some indication as to the content of the picture, but aren't entirely reliable.",
            "And finally we of course have a large amount of images which we have no label whatsoever.",
            "And what we'd like is a framework which can use all these different types of label."
        ],
        [
            "Now, one such method is semi supervised learning and we introduced it by means of a little toy 2 dimensional example.",
            "So you can see some data here with just two labeled examples shown in red and green and in standard supervised setting we would just take those two labels and the."
        ],
        [
            "Vision surface would be some line that bisects the two points.",
            "And the resulting classification will be something like this that splits the two clusters in semi supervised learning.",
            "We want to use the density structure of the data and we want."
        ],
        [
            "Classification function to be smooth with respect to the data density, like you can see here.",
            "Now the particular variant of semi supervised learning we're going to use is based on the graph of passion."
        ],
        [
            "And to build this, we first construct an end by Infinity Matrix and his number of points we have and essentially looks at this sort of the distances between pairs of points, and so if you can look at an example here, the intuition is the nearby points will have a high affinity and points far away will have a small affinity.",
            "And once we've constructed this matrix, we can then compute the Laplacian as follows, and so this matrix L is also going to be N by N in size."
        ],
        [
            "OK, so in summary, supervised learning.",
            "We're looking to find a label function F that minimizes the sum of two terms.",
            "So the first term uses the graph of passion to measure the smoothness of the label function and the second term looks at how the label function agrees with the labels that we've been provided, and this matrix diagonal matrix Lambda encodes as the reliability of the labels.",
            "So if the things unlabeled, then it has zero on the diagonal.",
            "If it's might be manually labeled, it will have some large value.",
            "Now we can find the F that minimizes this expression.",
            "In fact, it's just simply the solution to this little linear system, which is going to be an end by end system if we have N data points."
        ],
        [
            "But we don't necessarily need to work with the entire graph Laplacian, we can just in fact consider the eigenvectors of will have the smallest eigenvalues of the graph Laplacian.",
            "So you here are the eigenvectors and Alpha set of coefficients on them, and we can use that to model to represent our label function.",
            "And the intuition is that these these eigenvectors will be smooth and respect to the data density.",
            "So here's the toy data.",
            "Again, has the smallest eigenvector you can see.",
            "It's in fact just a constant over the data points, and then the second smallest splits the data horizontally and the third smallest split it vertically.",
            "So we can use the eigenvectors as."
        ],
        [
            "Basis and if we just pick, say, the smallest case, OK, some user parameter here, so typically 100 or so we can rewrite the linear system we saw previously in the following fashion.",
            "So this now instead of being a big end by end system, is a much smaller case by case system that we can solve quite easily.",
            "But the problem is that if we."
        ],
        [
            "A large amount of data we hit a computational bottleneck with both approaches.",
            "So if we want to solve the original linear system then we need to invert essentially an 80 million 80,000,000 matrix.",
            "If we've got a big data set to 80,000,000 images, if we use the eigenvector approach, then just to compute the eigenvectors themselves is going to require diagonalizing and 80,000,000 by 80,000,000 matrix.",
            "And both these options are just not practical.",
            "Now there are a range of."
        ],
        [
            "Methods for doing large scale, semi supervised learning.",
            "Many of them are quite similar to the Nystrom method which picks a small set of landmark points like so and then it computes and exactly exact eigenvectors on these and then interpolates the remaining points in giving approximate eigenvectors.",
            "There's also a range of other approaches which adaptively cluster the data and then compute."
        ],
        [
            "Exact solution on these clusters?"
        ],
        [
            "OK, so approach is going to be something slightly different, so we're still going to compute approximate eigenvectors for the graph Laplacian, but instead of subsampling like techniques like Nystrom, do which reduced the number of data points we're going to consider what happens in the limit case is the number of data points goes to Infinity and you have a continuous density.",
            "Now our approach from a complexity perspective is linear in the number of data points, which makes it easy to scale to very large collections of data.",
            "By contrast, the Nystrom is polynomial in the number of landmarks, which, in practice, prevents limits the number of landmarks that can be used."
        ],
        [
            "OK, so that's the number of points goes to Infinity.",
            "We can think about our data coming from some sort of continuous distribution.",
            "So for the example here, this is going to be a 2 dimensional distribution P of X.",
            "And we define some smooth operator LP of F which measures the smoothness of functions F on this density.",
            "So the intuition is that the smoothest operator is going to penalize functions that vary in areas of high density OK, and the what we're going to do is to look at the eigen functions of this smoothness operator on function F. So just to get an intuition."
        ],
        [
            "What these things look like?",
            "So here's the discrete case we saw previously, and now in the continuous case you can see we have the continuous density and the first eigen function that is the one with the smallest eigenvalue is going to effect completely smooth function over the space and then the second one is going to split the data horizontally and the third one is going to split vertically and you'll notice that the eigenvalues associated with these eigenfunctions are actually very similar to the eigenvalues of the discrete eigenvectors.",
            "OK so no one cares."
        ],
        [
            "Function we make in our work is that we the input distribution is separable, so that is in our little toy example here.",
            "We've got 2 dimensions, so we've got a joint distribution P of X1 and X2, and we assume that we can model that as a product of the two marginal distributions P of X1 and P of X2.",
            "OK, now it turns out that if we can compute the eigenfunctions of these marginal distributions, they will in fact be eigenfunctions of the overall joint density with the same eigenvalue.",
            "OK, so let's just consider what we."
        ],
        [
            "With one of those marginal distributions, so has the has the density.",
            "If we look at one of the marginals, we can see the look something like this.",
            "Of course, we're actually giving a discrete set of data, so we can then form an approximation to this true marginal by building a histogram each of X1 here.",
            "And then we can take this histogram and we can use it to solve for the values of the eigen fun."
        ],
        [
            "And numerically at a set of discrete locations that essentially the centers of the histogram bins OK. And in addition to doing that, we can also solve for the associated eigenvalue and to do this in fact just requires solving BBB system where B is the number of bins in histogram and typically this is pretty small, something like 50 or so.",
            "So here's the marginal, just the histogram we saw just a moment ago and this is an example of one of the eigen functions.",
            "So you can see that we've started a series of discrete points.",
            "So each circle here represents one of the values we solve for, and that is fairly constant in areas of high density.",
            "But then it changes rapidly in the middle here with the density is low.",
            "Now of course, for each dimension we're going to solve for, not just."
        ],
        [
            "One eigenfunction, but a whole set, and so I'm showing you here and the three eigen functions which have the smallest eigenvalue and you can see that they become increasingly sort of this ones got one King and this got 2.",
            "And then we that this is for the first dimension of the input, and of course we then have to consider the second marginal distribution which is looks."
        ],
        [
            "More Gaussian like and you can see in this case the first eigenfunction is something close to linear, second one as it looks like a bit like a quadratic and then cubic.",
            "OK, so having computed these numerical approximations to the eigenfunctions, we then we're in."
        ],
        [
            "Testing going back to the eigenvectors for the actual data we have, and we do this by taking each data point and for each coordinate in the data point, we interpolate it into the eigenfunction like so.",
            "OK, and this is a very fast operation.",
            "It's essentially a sort of 1 dimensional linear interpolation.",
            "So that's pretty much the algorithm.",
            "So one important prepro."
        ],
        [
            "Testing step is that we did assume the data was separable and of course by default that's not likely to be true.",
            "So what we do is to rotate the data using PCA to try and make it more separable like so.",
            "So, just to summarize, the algorithm, we first of all rotate the data just to maximize it."
        ],
        [
            "Separability and we currently use PCA, but you could imagine using something else.",
            "And then for each of the D input dimensions, we're going to construct a 1 dimensional histogram, and for that histogram going to solve numerically for the eigen functions and their associated eigenvalues.",
            "And then we're going to take the eigenfunctions from all the different dimensions in our input and sort them by increasing eigenvalue.",
            "That is, to take the smallest ones 1st, and we're then going to just take the smallest K. So K again is the user parameter that we select maybe 100 or so.",
            "And then we're going to take our data points and interpolate them back into those K eigen functions.",
            "And that's going to give us an approximate set of eigenvectors of our graph Laplacian.",
            "And then we can take those approximate eigenvectors and plug them into that Kabi Kaylee square system.",
            "We looked at earlier just to solve for the label function over the data.",
            "OK, so let's."
        ],
        [
            "Got some experiments on toy data, so here we've got another two dimension distribute."
        ],
        [
            "And we have two class clumps of data separated by a narrow gap and we have one label in each.",
            "Now, in Nystrom it's selecting a small set of landmarks and often if you can see that it can result in sort of unstable eigenvectors which give you a incorrect solution that you see here.",
            "By contrast, our approach uses all the data, so in this case actually correctly separates the two clusters.",
            "However, our approach breaks down when the data has significant dependencies between the input dimensions, so you can see."
        ],
        [
            "Here 2 concentric circles point on the inner and outer and Nystrom.",
            "Even though it's using very small subset, gets the right solution whereas in contrast our eigenfunction approach breaks down."
        ],
        [
            "OK, so let."
        ],
        [
            "Now look at some real data.",
            "So in the first set of experiments were going to use the data set of 63,000 images we've downloaded from the Internet, and we took 126 different nouns, and for each one of those nouns we plug them into an image search engine and downloaded several 1000 images.",
            "Also that we get from things like Google Image Search.",
            "I'm just showing 2 examples here.",
            "And these images were then manually annotated by by users as part of an effort run by Alex Kryszewski, Vinod Nair, and Jeff Hinton, just sponsored by see far to provide ground truth on this data set.",
            "So for each image, we're going to use a single global disk."
        ],
        [
            "Chapter is the input representation, and in this case we're going to suggest descriptor by Ollivander Alba and just briefly.",
            "This essentially measures the energy of the boards are different scales and orientations in the image and.",
            "One natural question, of course, is how independent of different dimensions in this."
        ],
        [
            "Scripter so in fact, if you take pairs of dimensions from the descriptor, and you just put out the joint histogram using a large number of descriptors, you can see in fact that they're not really that quite dependent, so we can measure here the mutual information scores between different pairs of dimensions.",
            "And of course, if they're independent, we expect this number to be 0, but you can see that in fact the values are quite high.",
            "Now for this particular data we do PCA on it, though it seems to do a fairly good job of making the data dimensions in the input space separable.",
            "So in this case we're doing projecting down with PCA, just 64 dimensions and you can see the mutual information scores now between different pairs is actually much lower.",
            "It's not quite 0, but it's certainly very small, so the separability assumption that we're using, at least for this type of data isn't so unreasonable.",
            "OK, so one this."
        ],
        [
            "It shows two things.",
            "So first of all it's showing you what the eigen functions look like of this PCA.",
            "Just descriptive data, which is essentially each one of these little squiggles is an eigenfunction in one dimension.",
            "And you can see some of them quite smooth and others are sort of high frequency.",
            "And the other thing is showing is using the color it's showing which input dimension the eigenfunction came from.",
            "OK, so you're using PCA data, so the variance will be higher in the first few dimensions, so you would expect is not.",
            "Unsurprisingly, you're seeing the predominantly red ones at the top here, which have the smallest eigenvalue.",
            "As you move down, you start to see eigenfunctions chosen from different dimensions of the input.",
            "OK, so the task."
        ],
        [
            "We're going to perform is to rerank the images of each class.",
            "So for example, if you take the Airbus images here, you can see the initial ordering given by the search engine's is fairly noisy.",
            "There are many outliers that are at the ranked high, and we're going to do is to train our semi supervised scheme to rerank the images to hopefully bring improve the quality and bring up the good examples up to the top.",
            "So we're essentially going to performing here a series of two class operations, sort of Airbus non Airbus, and then we're going to sort of average performance over many of the different nouns.",
            "To get a stable estimate.",
            "OK, now in our semi supervised learning approach we're going to build the eigen functions using all 63,000 images.",
            "OK, because it's linear, we can do this first very easily.",
            "And then we're going to vary the number of labeled examples and see how it affects performance and our performance measure is going to be the precision at 15% recall.",
            "Which corresponds to roughly a couple of web pages worth of images, typically.",
            "OK, so."
        ],
        [
            "On the X axis, here we're varying the number of labeled training examples on the far left.",
            "We're looking at the unsupervised case.",
            "This is 1 training example 248 and so on.",
            "As we vary the number of labeled examples, it does actually change number of total number of images, and you can see that changing on the top.",
            "And on the Y axis, we're looking at the performance, so higher is better, and the chance level performance is shown here on as the black dashed line, and we're averaging over 16 different classes.",
            "Now we're showing two curves on this plot, so the first curve in dark Blue is a standard supervised approach.",
            "This is a radial basis function SVM.",
            "You can see it horribly overfits the data initially, but then once you have enough training examples labeled training examples, you can see performance is actually pretty good.",
            "In Cyan, we're showing the performance of the the semi supervised approach I introduced at the very beginning.",
            "That's the end by end linear system you have to solve and it does pretty well when you have a few examples.",
            "But unfortunately when you have more data.",
            "Becomes too expensive to compute the inverse of the matrix, and therefore we can't go any further.",
            "Now, if you use the nice."
        ],
        [
            "Approach it does certainly better than the supervised case, but there's a distinct drop in performance compared to the exact least squares solution.",
            "Now, Nystrom does do pretty well when you have the landmarks or a large fraction of the input data, but in this case there are fairly small fraction and impairs performance.",
            "So in Red was showing."
        ],
        [
            "I confront function approach so you can see it does about as well as the exactly squares one, even though it's approximate, but of course because it's linear number of training examples, we can continue to extend to a large sets of data so it can go all the way up and at the very top here.",
            "Perhaps the SVM is beginning to beat it.",
            "But it's certainly above the Nystrom method.",
            "And the."
        ],
        [
            "Finally, we show two other methods here which this is nearest neighbors.",
            "Again, it's another supervised method, and in then we shown in pink an eigenvector approach using the exact eigenvectors."
        ],
        [
            "So finally I just showed some experiments on 80,000,000 images.",
            "So here we took the gist descriptor from each image."
        ],
        [
            "We project down to those two dimensions, computed 48 eigenfunctions, and then using these eigenfunctions we can propagate the labels through all 80,000,000 images, and in fact we can precompute things to make things faster.",
            "We can precompute the eigenvectors, it's a big 80,000,000 by 48 matrix, and using this week and just need to solve a very small little 48 by 48 system to propagate labels so we can do it very quickly in just a fraction of a second."
        ],
        [
            "So taking it one query term, say Japanese spaniel, this is the raw ranking produced by the search engine.",
            "We take 3 positive negative labels from the faucet and using a supervised approach in this case nearest neighbors, we end up with this result.",
            "But if we use the eigenfunctions to regularize our solution, which computer on all 80 million we get a much nicer result.",
            "As you can see here."
        ],
        [
            "And then these are three more query terms.",
            "Airbus, Ostrich and Auto in the center column you can see the nearest neighbor performance and then on the far right you can see the Eigenfunction re ranking and so you can see great consistency in these results compared to either the standard supervised approach or the initial ranking from the search engine."
        ],
        [
            "OK, so just to summarize, we've introduced a semi supervised scheme here which can scale to really large problems, which essentially because it's linear in the number of data points, right?",
            "So rather than subsampling the data like other approaches, do we take the limit of infinite unlabeled data?",
            "And we do make a big assumption.",
            "We do assume that input distribution is separable, But if we do."
        ],
        [
            "That we can propagate labels on really big graphs with 18 million nodes in just a fraction of a second.",
            "And finally there are some of the theoretical issues in this paper explored by another paper here at NIPS by Natalie Srebro and Zoo, which is a spotlight on Wednesday.",
            "OK. A couple of quick questions.",
            "Just come up to the mikes.",
            "Does that assume we're not assuming here that's finished mentioned?",
            "Yeah, OK, he's asking why we didn't compare themselves algorithm, which he says is used to compute computer computerize functions.",
            "So I'm not does that.",
            "Assume that we're not assuming here that the affinity matrix is sparse.",
            "Some of these numerical schemes do work well if you're assuming sparsity in the affinity matrix, but if you do that, you graph you sort of reduce the bandwidth in the graph, and you can end up with disconnected islands or points which the labels can't propagate too.",
            "So I guess I'd have to look in more detail exactly what the Manchester algorithm is doing.",
            "Hello.",
            "I have a question about it.",
            "OK so you you select the first K eigenvalues.",
            "So do you learn the key value or do you appreciate it?",
            "We know it's fixed.",
            "We did actually try a couple of experiments where we.",
            "So what do you?",
            "What are you for?",
            "Kata too small or OK so it's too small.",
            "It doesn't do very well if you have too many stars over fit, so we invite you 156 which is this column in R plot."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our goal is to develop search image search scale to the billions of images on the web.",
                    "label": 0
                },
                {
                    "sent": "So one property of intent images is that they contain a wide range of label information, so a tiny fraction have been manually annotated, so have labels that are reliable.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But a far larger fraction have some kind of noisy label, perhaps derived from the filename or the surrounding HTML.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And these cues provides some indication as to the content of the picture, but aren't entirely reliable.",
                    "label": 0
                },
                {
                    "sent": "And finally we of course have a large amount of images which we have no label whatsoever.",
                    "label": 0
                },
                {
                    "sent": "And what we'd like is a framework which can use all these different types of label.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, one such method is semi supervised learning and we introduced it by means of a little toy 2 dimensional example.",
                    "label": 0
                },
                {
                    "sent": "So you can see some data here with just two labeled examples shown in red and green and in standard supervised setting we would just take those two labels and the.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Vision surface would be some line that bisects the two points.",
                    "label": 0
                },
                {
                    "sent": "And the resulting classification will be something like this that splits the two clusters in semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "We want to use the density structure of the data and we want.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Classification function to be smooth with respect to the data density, like you can see here.",
                    "label": 0
                },
                {
                    "sent": "Now the particular variant of semi supervised learning we're going to use is based on the graph of passion.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And to build this, we first construct an end by Infinity Matrix and his number of points we have and essentially looks at this sort of the distances between pairs of points, and so if you can look at an example here, the intuition is the nearby points will have a high affinity and points far away will have a small affinity.",
                    "label": 0
                },
                {
                    "sent": "And once we've constructed this matrix, we can then compute the Laplacian as follows, and so this matrix L is also going to be N by N in size.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so in summary, supervised learning.",
                    "label": 0
                },
                {
                    "sent": "We're looking to find a label function F that minimizes the sum of two terms.",
                    "label": 1
                },
                {
                    "sent": "So the first term uses the graph of passion to measure the smoothness of the label function and the second term looks at how the label function agrees with the labels that we've been provided, and this matrix diagonal matrix Lambda encodes as the reliability of the labels.",
                    "label": 0
                },
                {
                    "sent": "So if the things unlabeled, then it has zero on the diagonal.",
                    "label": 0
                },
                {
                    "sent": "If it's might be manually labeled, it will have some large value.",
                    "label": 0
                },
                {
                    "sent": "Now we can find the F that minimizes this expression.",
                    "label": 0
                },
                {
                    "sent": "In fact, it's just simply the solution to this little linear system, which is going to be an end by end system if we have N data points.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But we don't necessarily need to work with the entire graph Laplacian, we can just in fact consider the eigenvectors of will have the smallest eigenvalues of the graph Laplacian.",
                    "label": 1
                },
                {
                    "sent": "So you here are the eigenvectors and Alpha set of coefficients on them, and we can use that to model to represent our label function.",
                    "label": 0
                },
                {
                    "sent": "And the intuition is that these these eigenvectors will be smooth and respect to the data density.",
                    "label": 1
                },
                {
                    "sent": "So here's the toy data.",
                    "label": 0
                },
                {
                    "sent": "Again, has the smallest eigenvector you can see.",
                    "label": 0
                },
                {
                    "sent": "It's in fact just a constant over the data points, and then the second smallest splits the data horizontally and the third smallest split it vertically.",
                    "label": 0
                },
                {
                    "sent": "So we can use the eigenvectors as.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Basis and if we just pick, say, the smallest case, OK, some user parameter here, so typically 100 or so we can rewrite the linear system we saw previously in the following fashion.",
                    "label": 1
                },
                {
                    "sent": "So this now instead of being a big end by end system, is a much smaller case by case system that we can solve quite easily.",
                    "label": 0
                },
                {
                    "sent": "But the problem is that if we.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A large amount of data we hit a computational bottleneck with both approaches.",
                    "label": 1
                },
                {
                    "sent": "So if we want to solve the original linear system then we need to invert essentially an 80 million 80,000,000 matrix.",
                    "label": 0
                },
                {
                    "sent": "If we've got a big data set to 80,000,000 images, if we use the eigenvector approach, then just to compute the eigenvectors themselves is going to require diagonalizing and 80,000,000 by 80,000,000 matrix.",
                    "label": 0
                },
                {
                    "sent": "And both these options are just not practical.",
                    "label": 0
                },
                {
                    "sent": "Now there are a range of.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Methods for doing large scale, semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "Many of them are quite similar to the Nystrom method which picks a small set of landmark points like so and then it computes and exactly exact eigenvectors on these and then interpolates the remaining points in giving approximate eigenvectors.",
                    "label": 1
                },
                {
                    "sent": "There's also a range of other approaches which adaptively cluster the data and then compute.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Exact solution on these clusters?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so approach is going to be something slightly different, so we're still going to compute approximate eigenvectors for the graph Laplacian, but instead of subsampling like techniques like Nystrom, do which reduced the number of data points we're going to consider what happens in the limit case is the number of data points goes to Infinity and you have a continuous density.",
                    "label": 0
                },
                {
                    "sent": "Now our approach from a complexity perspective is linear in the number of data points, which makes it easy to scale to very large collections of data.",
                    "label": 0
                },
                {
                    "sent": "By contrast, the Nystrom is polynomial in the number of landmarks, which, in practice, prevents limits the number of landmarks that can be used.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so that's the number of points goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "We can think about our data coming from some sort of continuous distribution.",
                    "label": 0
                },
                {
                    "sent": "So for the example here, this is going to be a 2 dimensional distribution P of X.",
                    "label": 1
                },
                {
                    "sent": "And we define some smooth operator LP of F which measures the smoothness of functions F on this density.",
                    "label": 0
                },
                {
                    "sent": "So the intuition is that the smoothest operator is going to penalize functions that vary in areas of high density OK, and the what we're going to do is to look at the eigen functions of this smoothness operator on function F. So just to get an intuition.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What these things look like?",
                    "label": 0
                },
                {
                    "sent": "So here's the discrete case we saw previously, and now in the continuous case you can see we have the continuous density and the first eigen function that is the one with the smallest eigenvalue is going to effect completely smooth function over the space and then the second one is going to split the data horizontally and the third one is going to split vertically and you'll notice that the eigenvalues associated with these eigenfunctions are actually very similar to the eigenvalues of the discrete eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "OK so no one cares.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Function we make in our work is that we the input distribution is separable, so that is in our little toy example here.",
                    "label": 0
                },
                {
                    "sent": "We've got 2 dimensions, so we've got a joint distribution P of X1 and X2, and we assume that we can model that as a product of the two marginal distributions P of X1 and P of X2.",
                    "label": 0
                },
                {
                    "sent": "OK, now it turns out that if we can compute the eigenfunctions of these marginal distributions, they will in fact be eigenfunctions of the overall joint density with the same eigenvalue.",
                    "label": 1
                },
                {
                    "sent": "OK, so let's just consider what we.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With one of those marginal distributions, so has the has the density.",
                    "label": 0
                },
                {
                    "sent": "If we look at one of the marginals, we can see the look something like this.",
                    "label": 0
                },
                {
                    "sent": "Of course, we're actually giving a discrete set of data, so we can then form an approximation to this true marginal by building a histogram each of X1 here.",
                    "label": 0
                },
                {
                    "sent": "And then we can take this histogram and we can use it to solve for the values of the eigen fun.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And numerically at a set of discrete locations that essentially the centers of the histogram bins OK. And in addition to doing that, we can also solve for the associated eigenvalue and to do this in fact just requires solving BBB system where B is the number of bins in histogram and typically this is pretty small, something like 50 or so.",
                    "label": 1
                },
                {
                    "sent": "So here's the marginal, just the histogram we saw just a moment ago and this is an example of one of the eigen functions.",
                    "label": 0
                },
                {
                    "sent": "So you can see that we've started a series of discrete points.",
                    "label": 0
                },
                {
                    "sent": "So each circle here represents one of the values we solve for, and that is fairly constant in areas of high density.",
                    "label": 0
                },
                {
                    "sent": "But then it changes rapidly in the middle here with the density is low.",
                    "label": 0
                },
                {
                    "sent": "Now of course, for each dimension we're going to solve for, not just.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One eigenfunction, but a whole set, and so I'm showing you here and the three eigen functions which have the smallest eigenvalue and you can see that they become increasingly sort of this ones got one King and this got 2.",
                    "label": 0
                },
                {
                    "sent": "And then we that this is for the first dimension of the input, and of course we then have to consider the second marginal distribution which is looks.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More Gaussian like and you can see in this case the first eigenfunction is something close to linear, second one as it looks like a bit like a quadratic and then cubic.",
                    "label": 0
                },
                {
                    "sent": "OK, so having computed these numerical approximations to the eigenfunctions, we then we're in.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Testing going back to the eigenvectors for the actual data we have, and we do this by taking each data point and for each coordinate in the data point, we interpolate it into the eigenfunction like so.",
                    "label": 0
                },
                {
                    "sent": "OK, and this is a very fast operation.",
                    "label": 1
                },
                {
                    "sent": "It's essentially a sort of 1 dimensional linear interpolation.",
                    "label": 0
                },
                {
                    "sent": "So that's pretty much the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So one important prepro.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Testing step is that we did assume the data was separable and of course by default that's not likely to be true.",
                    "label": 0
                },
                {
                    "sent": "So what we do is to rotate the data using PCA to try and make it more separable like so.",
                    "label": 1
                },
                {
                    "sent": "So, just to summarize, the algorithm, we first of all rotate the data just to maximize it.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Separability and we currently use PCA, but you could imagine using something else.",
                    "label": 0
                },
                {
                    "sent": "And then for each of the D input dimensions, we're going to construct a 1 dimensional histogram, and for that histogram going to solve numerically for the eigen functions and their associated eigenvalues.",
                    "label": 1
                },
                {
                    "sent": "And then we're going to take the eigenfunctions from all the different dimensions in our input and sort them by increasing eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "That is, to take the smallest ones 1st, and we're then going to just take the smallest K. So K again is the user parameter that we select maybe 100 or so.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to take our data points and interpolate them back into those K eigen functions.",
                    "label": 0
                },
                {
                    "sent": "And that's going to give us an approximate set of eigenvectors of our graph Laplacian.",
                    "label": 0
                },
                {
                    "sent": "And then we can take those approximate eigenvectors and plug them into that Kabi Kaylee square system.",
                    "label": 0
                },
                {
                    "sent": "We looked at earlier just to solve for the label function over the data.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Got some experiments on toy data, so here we've got another two dimension distribute.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we have two class clumps of data separated by a narrow gap and we have one label in each.",
                    "label": 0
                },
                {
                    "sent": "Now, in Nystrom it's selecting a small set of landmarks and often if you can see that it can result in sort of unstable eigenvectors which give you a incorrect solution that you see here.",
                    "label": 1
                },
                {
                    "sent": "By contrast, our approach uses all the data, so in this case actually correctly separates the two clusters.",
                    "label": 0
                },
                {
                    "sent": "However, our approach breaks down when the data has significant dependencies between the input dimensions, so you can see.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here 2 concentric circles point on the inner and outer and Nystrom.",
                    "label": 0
                },
                {
                    "sent": "Even though it's using very small subset, gets the right solution whereas in contrast our eigenfunction approach breaks down.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now look at some real data.",
                    "label": 0
                },
                {
                    "sent": "So in the first set of experiments were going to use the data set of 63,000 images we've downloaded from the Internet, and we took 126 different nouns, and for each one of those nouns we plug them into an image search engine and downloaded several 1000 images.",
                    "label": 0
                },
                {
                    "sent": "Also that we get from things like Google Image Search.",
                    "label": 0
                },
                {
                    "sent": "I'm just showing 2 examples here.",
                    "label": 0
                },
                {
                    "sent": "And these images were then manually annotated by by users as part of an effort run by Alex Kryszewski, Vinod Nair, and Jeff Hinton, just sponsored by see far to provide ground truth on this data set.",
                    "label": 1
                },
                {
                    "sent": "So for each image, we're going to use a single global disk.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Chapter is the input representation, and in this case we're going to suggest descriptor by Ollivander Alba and just briefly.",
                    "label": 0
                },
                {
                    "sent": "This essentially measures the energy of the boards are different scales and orientations in the image and.",
                    "label": 0
                },
                {
                    "sent": "One natural question, of course, is how independent of different dimensions in this.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Scripter so in fact, if you take pairs of dimensions from the descriptor, and you just put out the joint histogram using a large number of descriptors, you can see in fact that they're not really that quite dependent, so we can measure here the mutual information scores between different pairs of dimensions.",
                    "label": 1
                },
                {
                    "sent": "And of course, if they're independent, we expect this number to be 0, but you can see that in fact the values are quite high.",
                    "label": 0
                },
                {
                    "sent": "Now for this particular data we do PCA on it, though it seems to do a fairly good job of making the data dimensions in the input space separable.",
                    "label": 0
                },
                {
                    "sent": "So in this case we're doing projecting down with PCA, just 64 dimensions and you can see the mutual information scores now between different pairs is actually much lower.",
                    "label": 0
                },
                {
                    "sent": "It's not quite 0, but it's certainly very small, so the separability assumption that we're using, at least for this type of data isn't so unreasonable.",
                    "label": 0
                },
                {
                    "sent": "OK, so one this.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It shows two things.",
                    "label": 0
                },
                {
                    "sent": "So first of all it's showing you what the eigen functions look like of this PCA.",
                    "label": 0
                },
                {
                    "sent": "Just descriptive data, which is essentially each one of these little squiggles is an eigenfunction in one dimension.",
                    "label": 0
                },
                {
                    "sent": "And you can see some of them quite smooth and others are sort of high frequency.",
                    "label": 0
                },
                {
                    "sent": "And the other thing is showing is using the color it's showing which input dimension the eigenfunction came from.",
                    "label": 1
                },
                {
                    "sent": "OK, so you're using PCA data, so the variance will be higher in the first few dimensions, so you would expect is not.",
                    "label": 0
                },
                {
                    "sent": "Unsurprisingly, you're seeing the predominantly red ones at the top here, which have the smallest eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "As you move down, you start to see eigenfunctions chosen from different dimensions of the input.",
                    "label": 0
                },
                {
                    "sent": "OK, so the task.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We're going to perform is to rerank the images of each class.",
                    "label": 1
                },
                {
                    "sent": "So for example, if you take the Airbus images here, you can see the initial ordering given by the search engine's is fairly noisy.",
                    "label": 0
                },
                {
                    "sent": "There are many outliers that are at the ranked high, and we're going to do is to train our semi supervised scheme to rerank the images to hopefully bring improve the quality and bring up the good examples up to the top.",
                    "label": 0
                },
                {
                    "sent": "So we're essentially going to performing here a series of two class operations, sort of Airbus non Airbus, and then we're going to sort of average performance over many of the different nouns.",
                    "label": 0
                },
                {
                    "sent": "To get a stable estimate.",
                    "label": 0
                },
                {
                    "sent": "OK, now in our semi supervised learning approach we're going to build the eigen functions using all 63,000 images.",
                    "label": 0
                },
                {
                    "sent": "OK, because it's linear, we can do this first very easily.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to vary the number of labeled examples and see how it affects performance and our performance measure is going to be the precision at 15% recall.",
                    "label": 0
                },
                {
                    "sent": "Which corresponds to roughly a couple of web pages worth of images, typically.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On the X axis, here we're varying the number of labeled training examples on the far left.",
                    "label": 0
                },
                {
                    "sent": "We're looking at the unsupervised case.",
                    "label": 0
                },
                {
                    "sent": "This is 1 training example 248 and so on.",
                    "label": 0
                },
                {
                    "sent": "As we vary the number of labeled examples, it does actually change number of total number of images, and you can see that changing on the top.",
                    "label": 1
                },
                {
                    "sent": "And on the Y axis, we're looking at the performance, so higher is better, and the chance level performance is shown here on as the black dashed line, and we're averaging over 16 different classes.",
                    "label": 0
                },
                {
                    "sent": "Now we're showing two curves on this plot, so the first curve in dark Blue is a standard supervised approach.",
                    "label": 0
                },
                {
                    "sent": "This is a radial basis function SVM.",
                    "label": 0
                },
                {
                    "sent": "You can see it horribly overfits the data initially, but then once you have enough training examples labeled training examples, you can see performance is actually pretty good.",
                    "label": 0
                },
                {
                    "sent": "In Cyan, we're showing the performance of the the semi supervised approach I introduced at the very beginning.",
                    "label": 0
                },
                {
                    "sent": "That's the end by end linear system you have to solve and it does pretty well when you have a few examples.",
                    "label": 0
                },
                {
                    "sent": "But unfortunately when you have more data.",
                    "label": 0
                },
                {
                    "sent": "Becomes too expensive to compute the inverse of the matrix, and therefore we can't go any further.",
                    "label": 0
                },
                {
                    "sent": "Now, if you use the nice.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Approach it does certainly better than the supervised case, but there's a distinct drop in performance compared to the exact least squares solution.",
                    "label": 0
                },
                {
                    "sent": "Now, Nystrom does do pretty well when you have the landmarks or a large fraction of the input data, but in this case there are fairly small fraction and impairs performance.",
                    "label": 0
                },
                {
                    "sent": "So in Red was showing.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I confront function approach so you can see it does about as well as the exactly squares one, even though it's approximate, but of course because it's linear number of training examples, we can continue to extend to a large sets of data so it can go all the way up and at the very top here.",
                    "label": 0
                },
                {
                    "sent": "Perhaps the SVM is beginning to beat it.",
                    "label": 0
                },
                {
                    "sent": "But it's certainly above the Nystrom method.",
                    "label": 0
                },
                {
                    "sent": "And the.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finally, we show two other methods here which this is nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "Again, it's another supervised method, and in then we shown in pink an eigenvector approach using the exact eigenvectors.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So finally I just showed some experiments on 80,000,000 images.",
                    "label": 0
                },
                {
                    "sent": "So here we took the gist descriptor from each image.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We project down to those two dimensions, computed 48 eigenfunctions, and then using these eigenfunctions we can propagate the labels through all 80,000,000 images, and in fact we can precompute things to make things faster.",
                    "label": 0
                },
                {
                    "sent": "We can precompute the eigenvectors, it's a big 80,000,000 by 48 matrix, and using this week and just need to solve a very small little 48 by 48 system to propagate labels so we can do it very quickly in just a fraction of a second.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So taking it one query term, say Japanese spaniel, this is the raw ranking produced by the search engine.",
                    "label": 0
                },
                {
                    "sent": "We take 3 positive negative labels from the faucet and using a supervised approach in this case nearest neighbors, we end up with this result.",
                    "label": 1
                },
                {
                    "sent": "But if we use the eigenfunctions to regularize our solution, which computer on all 80 million we get a much nicer result.",
                    "label": 0
                },
                {
                    "sent": "As you can see here.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then these are three more query terms.",
                    "label": 0
                },
                {
                    "sent": "Airbus, Ostrich and Auto in the center column you can see the nearest neighbor performance and then on the far right you can see the Eigenfunction re ranking and so you can see great consistency in these results compared to either the standard supervised approach or the initial ranking from the search engine.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so just to summarize, we've introduced a semi supervised scheme here which can scale to really large problems, which essentially because it's linear in the number of data points, right?",
                    "label": 0
                },
                {
                    "sent": "So rather than subsampling the data like other approaches, do we take the limit of infinite unlabeled data?",
                    "label": 1
                },
                {
                    "sent": "And we do make a big assumption.",
                    "label": 1
                },
                {
                    "sent": "We do assume that input distribution is separable, But if we do.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That we can propagate labels on really big graphs with 18 million nodes in just a fraction of a second.",
                    "label": 0
                },
                {
                    "sent": "And finally there are some of the theoretical issues in this paper explored by another paper here at NIPS by Natalie Srebro and Zoo, which is a spotlight on Wednesday.",
                    "label": 0
                },
                {
                    "sent": "OK. A couple of quick questions.",
                    "label": 0
                },
                {
                    "sent": "Just come up to the mikes.",
                    "label": 0
                },
                {
                    "sent": "Does that assume we're not assuming here that's finished mentioned?",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, he's asking why we didn't compare themselves algorithm, which he says is used to compute computer computerize functions.",
                    "label": 0
                },
                {
                    "sent": "So I'm not does that.",
                    "label": 0
                },
                {
                    "sent": "Assume that we're not assuming here that the affinity matrix is sparse.",
                    "label": 0
                },
                {
                    "sent": "Some of these numerical schemes do work well if you're assuming sparsity in the affinity matrix, but if you do that, you graph you sort of reduce the bandwidth in the graph, and you can end up with disconnected islands or points which the labels can't propagate too.",
                    "label": 0
                },
                {
                    "sent": "So I guess I'd have to look in more detail exactly what the Manchester algorithm is doing.",
                    "label": 0
                },
                {
                    "sent": "Hello.",
                    "label": 0
                },
                {
                    "sent": "I have a question about it.",
                    "label": 0
                },
                {
                    "sent": "OK so you you select the first K eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "So do you learn the key value or do you appreciate it?",
                    "label": 0
                },
                {
                    "sent": "We know it's fixed.",
                    "label": 0
                },
                {
                    "sent": "We did actually try a couple of experiments where we.",
                    "label": 0
                },
                {
                    "sent": "So what do you?",
                    "label": 0
                },
                {
                    "sent": "What are you for?",
                    "label": 0
                },
                {
                    "sent": "Kata too small or OK so it's too small.",
                    "label": 0
                },
                {
                    "sent": "It doesn't do very well if you have too many stars over fit, so we invite you 156 which is this column in R plot.",
                    "label": 0
                }
            ]
        }
    }
}