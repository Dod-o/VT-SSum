{
    "id": "yyb4u2okbyeiuupa6iqlyriouyptdsqx",
    "title": "Fourier Kernel Learning",
    "info": {
        "author": [
            "Eduard Gabriel B\u0103z\u0103van, Institute of Mathematics of the Romanian Academy"
        ],
        "chairman": [
            "Michal Irani, Weizmann Institute of Science",
            "Andrea Vedaldi, Department of Engineering Science, University of Oxford"
        ],
        "published": "Nov. 12, 2012",
        "recorded": "October 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/eccv2012_bazavan_learning/",
    "segmentation": [
        [
            "Hello.",
            "So good afternoon, my name is Eddie and my talk will be about making kernel learning practical for large problems.",
            "Hence we aim to tell you a truly in our story if you do not like it, you can blame my coauthors functionally, and Christians mikeska."
        ],
        [
            "Computer vision is nowadays exploring the opportunities an faces.",
            "The challenge is open by the availability of large volumes of data.",
            "This data comes with annotations.",
            "The labels in the form of, for example, image classes or object bounding boxes.",
            "Successful system leverage such large quantities of training data in order to classify new images or detect objects under constraints of both good accuracy and reasonable training and testing speeds.",
            "We may wish not only to identify which objects are present in."
        ],
        [
            "An image or place bounding boxes around them, but also estimate the accurate spatial layout of these objects at the level of pixels.",
            "However, the space of possible image partitions, regions, or segments that can be obtained from an image is very large.",
            "Consequently, recent method methods exploit fast classifiers to analyze large sets of segments hypothesis extracted at multiple locations and scales in an image and efficiently rank their relevance."
        ],
        [
            "Kernel methods are among the top performance in image classification detection or segmentation tasks.",
            "They operate by effectively lifting the original input space into a high dimensional, possibly infinite, feature space where linear methods can be used to separate the data.",
            "The listing is given by the choice of the kernel function and provides a new representation for the data.",
            "Linear classifiers in nonlinear feature spaces can be expressed in terms of DOT products between features vectors.",
            "And can be made very accurate.",
            "To avoid working in feature space directly, which may be impossible known in our methods, exploit the kernel trick which allows to express dot products in feature space space is by means of kernel revelations in input space.",
            "This makes training and testing tractable for any feature map, but it will be highly dependent on the training set size."
        ],
        [
            "This table summarizes the dilemma between working in feature space and working input space and use the kernel.",
            "The take home message that training the training time for kernel method scales all this quadratically in the training set size, whereas working with linear model models.",
            "Would be quadratic in the input dimension, which is potentially much smaller than the training set size for large datasets.",
            "On the other hand, you can see that working with kernels gives a significant boost in performance.",
            "Compared to linear methods hence.",
            "The non linearity implicit in kernel methods is worth preserving, but we have to speed up their operation to achieve large scale applicability.",
            "Our approach would be to give up the kernel trick and work with that."
        ],
        [
            "There are model induced by the kernel, but since the lifting fight may be intractable to compute, we're aimed to explicitly obtain it by means of a lower dimensional approximation site.",
            "Our goal, however, is not to provide another approximation of the kernel lifting in this work.",
            "Here we want to provide a scalable approach to learn not only the model weights W, but also the kernel parameters.",
            "Implicit insight in the next slides I will give you background for one particular approximation of a kernel lifting obtained for a choice of kernel parameters, and then explain how we can apply a kernel learning method in conjunction with this app."
        ],
        [
            "Exclamation.",
            "One approach to approximate the lifting of a kernel exploits Wagner theorem.",
            "Which makes a connection between a positive definite kernel function and its Fourier expansion.",
            "If the kernel is properly scaled, then Wagner's theorem guarantees that is Fourier transform.",
            "You is a proper probability distribution.",
            "You can see that the kernel can be written as a Fourier transform.",
            "In fact, you can view the integral as an expectation of the complex exponential with respect to the distribution mu, which is the Fourier transform of the kernel.",
            "We can use Euler's relation to write the complex exponential as a cosine plus an imaginary sign component.",
            "Now, since both can, you are real.",
            "We can replace the complex exponential with only its corresponding cosine."
        ],
        [
            "The idea of a Fourier approximation is to estimate the integral by a finite sum amount iCarly estimate by sampling from the positive measure mu through straightforward trigonometric calculations.",
            "We can show that the kernel can be approximated by means of a dot product among the vectors side, which have cosine components."
        ],
        [
            "And now we give you the algorithm to approximate the lifting of a kernel as follows.",
            "The first step is to.",
            "Obtain is Fourier transform.",
            "You then sample a number of directions gamehi from you and this have the same dimensionality as the input X project.",
            "Each input X onto the set of gamma eyes and take the cosine.",
            "Now the approximate lifting of the vector X under the kernel K will be the vector PSI of X.",
            "From a geometric point of view, the process can be understood as a projection of your internal input data.",
            "On the set of random directions defined by the samples gamma obtained from from you and then mapped to the unit circle in the real plane.",
            "This is a convergent kernel approximation.",
            "Given enough projection direction.",
            "Now."
        ],
        [
            "We know how to approximate the lifting of a kernel.",
            "Let us write a general cost function for kernel learning.",
            "This function will have an empirical loss component L, which measures how good our predictor is over the training set.",
            "Which is here given by X&Y.",
            "Here the inputs X were placed in a matrix and the outputs why in a vector?",
            "Also sigh of X gives the corresponding matrix of inputs replaced by the their approximate kernel feature Maps.",
            "The learning model has regularizer for the weights of the predictors and the kernel parameters, and during optimization we need to compute.",
            "The gradient of the loss function.",
            "We regard the weights and the kernel parameters.",
            "This general framework covers both single and multiple kernel learning formulations.",
            "We refer you to the paper for more detail.",
            "For kernel learning we have to be able to compute the gradient of the loss with respect to both the linear model weights and the kernel parameters by applying the chain rule you can see that a key component is the gradient of the feature map size of Sigma with respect to the kernel parameters Sigma.",
            "So how can we do that?",
            "The reason?"
        ],
        [
            "This is difficult.",
            "Is that every time we change the kernel parameters, the sampling distribution mu changes.",
            "Therefore we should draw the new sample directions and create new features and it is not clear, however, that the old and the new dimensions of the map corresponding to different random projection directions can be compared."
        ],
        [
            "To handle this key issue, we consider probability distribution functions mu with close form quantiles, which are linear in the kernel parameters.",
            "In this case we can draw samples Omega only once from the uniform distribution, then pass them to the nonlinear part of the quantile age that can be obtained from you and multiply the result with the kernel parameter Sigma.",
            "This will give us the correct samples gamma we seek.",
            "Using this procedure we can update the kernel parameters and it becomes easy to compute the feature map size and its gradient.",
            "This slide confirms that our."
        ],
        [
            "Resumption of closed form quantizes not unreasonable for the Goshen.",
            "The skewed intersection and the skewed chisquare kernels.",
            "We have probability distribution.",
            "With close form quantized, which are linear in the kernel parameter Sigma."
        ],
        [
            "We have performed several experiments to compare our Fourier models with their exact nonlinear counterparts showed in green and red, respectively.",
            "We start by showing results for single kernel models.",
            "On the X axis we showed the average time necessary to learn a classifier, and on the Y axis we plot the accuracy of ranking or pull of image segments in the VOC data set.",
            "We had to limit the nonlinear models no more than 10,000 samples to be able to run them in reasonable time.",
            "When comparing accuracies for the same number of training examples, the nonlinear model is better.",
            "Interestingly, however, we see that when we use more training data, the linear model recovers the performance loss.",
            "We also comp."
        ],
        [
            "Shared learning approaches based on multiple kernels, both in the Fourier domain an for exact nonlinear models.",
            "The models have seven kernels computed over Sifton Hog features on the X axis we varied the number of training samples and on the Y axis we showed the accuracy of ranking the same pool of image segments as in the previous experiment.",
            "On the left blood you see that as we provide more training data, the Fourier model gives better performance on the right plot plot we compare the running times for the exact nonlinear model and its Fourier approximation.",
            "You see the significant differences in running time.",
            "Notice that the running time blows up.",
            "For nonlinear models, train with more than 20,000 examples.",
            "We have also."
        ],
        [
            "The set of experiments on the image net data set where we classify images into 1000 categories.",
            "We compare several for your models, both single and multiple kernel based with different regularizers.",
            "Both the multiple kernel models use three kernels on two suites.",
            "Sift features and one hog feature.",
            "We measure the accuracy and top force prediction.",
            "Multiple kernel models predict better but are slower than the single kernel model.",
            "This is expected since the usage of several kernels makes the model more powerful.",
            "The example illustrates some of the key issues and the clear advantage of using the Fourier approximations.",
            "The exact nonlinear models cannot be run on such a large data set, whereas the performance of the linear predictor on raw inputs is under 8%.",
            "Concluding"
        ],
        [
            "This work we address the question of how to do kernel learning in a scalable way.",
            "We observe that while many approximations are available for learning a predictive model based on a pre specified kernel, there are very few approaches that would allow us to also learn the kernel parameters.",
            "We therefore have developed a methodology to cause these large scale kernel learning approach in the Fourier domain.",
            "We also showed that our approximation compare favorably.",
            "With exact nonlinear models.",
            "Moreover, when known in our models, can no longer be applied in large problems.",
            "Fourier Kernel Learning offers an alternative that is both accurate and scalable.",
            "Thank you for your attention.",
            "I would be happy now to answer your questions."
        ],
        [
            "So I use as well.",
            "It's running for the features of locations and I notice that very often in order to get very good accuracy, you need to sample many many of these high dimensional random projections.",
            "What is your experience, your experience with that?",
            "Well, this has to do with a couple of 1000 dimensions to recover the Proto Rico get to get a good performance, but our goal was to propose a model where we also learned the kernel parameters.",
            "So we worked on.",
            "We focused on that to learn the kernel parameters on this for stage and we are looking forward to improve the performance and all the you know the sampling.",
            "I hope this isn't a naive question, but I think that some.",
            "Kernels who used the matrix because the explicit kernel mapping would map into a space of infinite dimension, and I was wondering if your approximation handles that kind of thing.",
            "Well.",
            "As far as I understood the question was what happens when the lifting function gets your kernel to an infinite dimensional space?",
            "Yeah, that was the question well.",
            "The thing is that we approximate the kernel in the original in from the original input space, so we don't have to care about the lifting space.",
            "As I said, we don't care about the file, we just try to work in the side in the Fourier domain.",
            "Any other questions?",
            "So I think not so, let's thank the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hello.",
                    "label": 0
                },
                {
                    "sent": "So good afternoon, my name is Eddie and my talk will be about making kernel learning practical for large problems.",
                    "label": 0
                },
                {
                    "sent": "Hence we aim to tell you a truly in our story if you do not like it, you can blame my coauthors functionally, and Christians mikeska.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Computer vision is nowadays exploring the opportunities an faces.",
                    "label": 1
                },
                {
                    "sent": "The challenge is open by the availability of large volumes of data.",
                    "label": 0
                },
                {
                    "sent": "This data comes with annotations.",
                    "label": 0
                },
                {
                    "sent": "The labels in the form of, for example, image classes or object bounding boxes.",
                    "label": 0
                },
                {
                    "sent": "Successful system leverage such large quantities of training data in order to classify new images or detect objects under constraints of both good accuracy and reasonable training and testing speeds.",
                    "label": 0
                },
                {
                    "sent": "We may wish not only to identify which objects are present in.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An image or place bounding boxes around them, but also estimate the accurate spatial layout of these objects at the level of pixels.",
                    "label": 0
                },
                {
                    "sent": "However, the space of possible image partitions, regions, or segments that can be obtained from an image is very large.",
                    "label": 0
                },
                {
                    "sent": "Consequently, recent method methods exploit fast classifiers to analyze large sets of segments hypothesis extracted at multiple locations and scales in an image and efficiently rank their relevance.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kernel methods are among the top performance in image classification detection or segmentation tasks.",
                    "label": 0
                },
                {
                    "sent": "They operate by effectively lifting the original input space into a high dimensional, possibly infinite, feature space where linear methods can be used to separate the data.",
                    "label": 0
                },
                {
                    "sent": "The listing is given by the choice of the kernel function and provides a new representation for the data.",
                    "label": 0
                },
                {
                    "sent": "Linear classifiers in nonlinear feature spaces can be expressed in terms of DOT products between features vectors.",
                    "label": 0
                },
                {
                    "sent": "And can be made very accurate.",
                    "label": 0
                },
                {
                    "sent": "To avoid working in feature space directly, which may be impossible known in our methods, exploit the kernel trick which allows to express dot products in feature space space is by means of kernel revelations in input space.",
                    "label": 0
                },
                {
                    "sent": "This makes training and testing tractable for any feature map, but it will be highly dependent on the training set size.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This table summarizes the dilemma between working in feature space and working input space and use the kernel.",
                    "label": 0
                },
                {
                    "sent": "The take home message that training the training time for kernel method scales all this quadratically in the training set size, whereas working with linear model models.",
                    "label": 0
                },
                {
                    "sent": "Would be quadratic in the input dimension, which is potentially much smaller than the training set size for large datasets.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, you can see that working with kernels gives a significant boost in performance.",
                    "label": 0
                },
                {
                    "sent": "Compared to linear methods hence.",
                    "label": 0
                },
                {
                    "sent": "The non linearity implicit in kernel methods is worth preserving, but we have to speed up their operation to achieve large scale applicability.",
                    "label": 0
                },
                {
                    "sent": "Our approach would be to give up the kernel trick and work with that.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There are model induced by the kernel, but since the lifting fight may be intractable to compute, we're aimed to explicitly obtain it by means of a lower dimensional approximation site.",
                    "label": 0
                },
                {
                    "sent": "Our goal, however, is not to provide another approximation of the kernel lifting in this work.",
                    "label": 1
                },
                {
                    "sent": "Here we want to provide a scalable approach to learn not only the model weights W, but also the kernel parameters.",
                    "label": 0
                },
                {
                    "sent": "Implicit insight in the next slides I will give you background for one particular approximation of a kernel lifting obtained for a choice of kernel parameters, and then explain how we can apply a kernel learning method in conjunction with this app.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Exclamation.",
                    "label": 0
                },
                {
                    "sent": "One approach to approximate the lifting of a kernel exploits Wagner theorem.",
                    "label": 0
                },
                {
                    "sent": "Which makes a connection between a positive definite kernel function and its Fourier expansion.",
                    "label": 0
                },
                {
                    "sent": "If the kernel is properly scaled, then Wagner's theorem guarantees that is Fourier transform.",
                    "label": 0
                },
                {
                    "sent": "You is a proper probability distribution.",
                    "label": 0
                },
                {
                    "sent": "You can see that the kernel can be written as a Fourier transform.",
                    "label": 0
                },
                {
                    "sent": "In fact, you can view the integral as an expectation of the complex exponential with respect to the distribution mu, which is the Fourier transform of the kernel.",
                    "label": 0
                },
                {
                    "sent": "We can use Euler's relation to write the complex exponential as a cosine plus an imaginary sign component.",
                    "label": 0
                },
                {
                    "sent": "Now, since both can, you are real.",
                    "label": 0
                },
                {
                    "sent": "We can replace the complex exponential with only its corresponding cosine.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The idea of a Fourier approximation is to estimate the integral by a finite sum amount iCarly estimate by sampling from the positive measure mu through straightforward trigonometric calculations.",
                    "label": 0
                },
                {
                    "sent": "We can show that the kernel can be approximated by means of a dot product among the vectors side, which have cosine components.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now we give you the algorithm to approximate the lifting of a kernel as follows.",
                    "label": 0
                },
                {
                    "sent": "The first step is to.",
                    "label": 0
                },
                {
                    "sent": "Obtain is Fourier transform.",
                    "label": 0
                },
                {
                    "sent": "You then sample a number of directions gamehi from you and this have the same dimensionality as the input X project.",
                    "label": 0
                },
                {
                    "sent": "Each input X onto the set of gamma eyes and take the cosine.",
                    "label": 0
                },
                {
                    "sent": "Now the approximate lifting of the vector X under the kernel K will be the vector PSI of X.",
                    "label": 0
                },
                {
                    "sent": "From a geometric point of view, the process can be understood as a projection of your internal input data.",
                    "label": 0
                },
                {
                    "sent": "On the set of random directions defined by the samples gamma obtained from from you and then mapped to the unit circle in the real plane.",
                    "label": 0
                },
                {
                    "sent": "This is a convergent kernel approximation.",
                    "label": 0
                },
                {
                    "sent": "Given enough projection direction.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We know how to approximate the lifting of a kernel.",
                    "label": 0
                },
                {
                    "sent": "Let us write a general cost function for kernel learning.",
                    "label": 0
                },
                {
                    "sent": "This function will have an empirical loss component L, which measures how good our predictor is over the training set.",
                    "label": 0
                },
                {
                    "sent": "Which is here given by X&Y.",
                    "label": 0
                },
                {
                    "sent": "Here the inputs X were placed in a matrix and the outputs why in a vector?",
                    "label": 0
                },
                {
                    "sent": "Also sigh of X gives the corresponding matrix of inputs replaced by the their approximate kernel feature Maps.",
                    "label": 0
                },
                {
                    "sent": "The learning model has regularizer for the weights of the predictors and the kernel parameters, and during optimization we need to compute.",
                    "label": 0
                },
                {
                    "sent": "The gradient of the loss function.",
                    "label": 0
                },
                {
                    "sent": "We regard the weights and the kernel parameters.",
                    "label": 0
                },
                {
                    "sent": "This general framework covers both single and multiple kernel learning formulations.",
                    "label": 0
                },
                {
                    "sent": "We refer you to the paper for more detail.",
                    "label": 0
                },
                {
                    "sent": "For kernel learning we have to be able to compute the gradient of the loss with respect to both the linear model weights and the kernel parameters by applying the chain rule you can see that a key component is the gradient of the feature map size of Sigma with respect to the kernel parameters Sigma.",
                    "label": 0
                },
                {
                    "sent": "So how can we do that?",
                    "label": 0
                },
                {
                    "sent": "The reason?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is difficult.",
                    "label": 0
                },
                {
                    "sent": "Is that every time we change the kernel parameters, the sampling distribution mu changes.",
                    "label": 0
                },
                {
                    "sent": "Therefore we should draw the new sample directions and create new features and it is not clear, however, that the old and the new dimensions of the map corresponding to different random projection directions can be compared.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To handle this key issue, we consider probability distribution functions mu with close form quantiles, which are linear in the kernel parameters.",
                    "label": 0
                },
                {
                    "sent": "In this case we can draw samples Omega only once from the uniform distribution, then pass them to the nonlinear part of the quantile age that can be obtained from you and multiply the result with the kernel parameter Sigma.",
                    "label": 0
                },
                {
                    "sent": "This will give us the correct samples gamma we seek.",
                    "label": 0
                },
                {
                    "sent": "Using this procedure we can update the kernel parameters and it becomes easy to compute the feature map size and its gradient.",
                    "label": 0
                },
                {
                    "sent": "This slide confirms that our.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Resumption of closed form quantizes not unreasonable for the Goshen.",
                    "label": 0
                },
                {
                    "sent": "The skewed intersection and the skewed chisquare kernels.",
                    "label": 1
                },
                {
                    "sent": "We have probability distribution.",
                    "label": 0
                },
                {
                    "sent": "With close form quantized, which are linear in the kernel parameter Sigma.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have performed several experiments to compare our Fourier models with their exact nonlinear counterparts showed in green and red, respectively.",
                    "label": 0
                },
                {
                    "sent": "We start by showing results for single kernel models.",
                    "label": 1
                },
                {
                    "sent": "On the X axis we showed the average time necessary to learn a classifier, and on the Y axis we plot the accuracy of ranking or pull of image segments in the VOC data set.",
                    "label": 0
                },
                {
                    "sent": "We had to limit the nonlinear models no more than 10,000 samples to be able to run them in reasonable time.",
                    "label": 1
                },
                {
                    "sent": "When comparing accuracies for the same number of training examples, the nonlinear model is better.",
                    "label": 0
                },
                {
                    "sent": "Interestingly, however, we see that when we use more training data, the linear model recovers the performance loss.",
                    "label": 0
                },
                {
                    "sent": "We also comp.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Shared learning approaches based on multiple kernels, both in the Fourier domain an for exact nonlinear models.",
                    "label": 0
                },
                {
                    "sent": "The models have seven kernels computed over Sifton Hog features on the X axis we varied the number of training samples and on the Y axis we showed the accuracy of ranking the same pool of image segments as in the previous experiment.",
                    "label": 0
                },
                {
                    "sent": "On the left blood you see that as we provide more training data, the Fourier model gives better performance on the right plot plot we compare the running times for the exact nonlinear model and its Fourier approximation.",
                    "label": 1
                },
                {
                    "sent": "You see the significant differences in running time.",
                    "label": 0
                },
                {
                    "sent": "Notice that the running time blows up.",
                    "label": 0
                },
                {
                    "sent": "For nonlinear models, train with more than 20,000 examples.",
                    "label": 0
                },
                {
                    "sent": "We have also.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The set of experiments on the image net data set where we classify images into 1000 categories.",
                    "label": 0
                },
                {
                    "sent": "We compare several for your models, both single and multiple kernel based with different regularizers.",
                    "label": 0
                },
                {
                    "sent": "Both the multiple kernel models use three kernels on two suites.",
                    "label": 0
                },
                {
                    "sent": "Sift features and one hog feature.",
                    "label": 0
                },
                {
                    "sent": "We measure the accuracy and top force prediction.",
                    "label": 0
                },
                {
                    "sent": "Multiple kernel models predict better but are slower than the single kernel model.",
                    "label": 1
                },
                {
                    "sent": "This is expected since the usage of several kernels makes the model more powerful.",
                    "label": 0
                },
                {
                    "sent": "The example illustrates some of the key issues and the clear advantage of using the Fourier approximations.",
                    "label": 0
                },
                {
                    "sent": "The exact nonlinear models cannot be run on such a large data set, whereas the performance of the linear predictor on raw inputs is under 8%.",
                    "label": 0
                },
                {
                    "sent": "Concluding",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This work we address the question of how to do kernel learning in a scalable way.",
                    "label": 0
                },
                {
                    "sent": "We observe that while many approximations are available for learning a predictive model based on a pre specified kernel, there are very few approaches that would allow us to also learn the kernel parameters.",
                    "label": 0
                },
                {
                    "sent": "We therefore have developed a methodology to cause these large scale kernel learning approach in the Fourier domain.",
                    "label": 1
                },
                {
                    "sent": "We also showed that our approximation compare favorably.",
                    "label": 0
                },
                {
                    "sent": "With exact nonlinear models.",
                    "label": 0
                },
                {
                    "sent": "Moreover, when known in our models, can no longer be applied in large problems.",
                    "label": 0
                },
                {
                    "sent": "Fourier Kernel Learning offers an alternative that is both accurate and scalable.",
                    "label": 0
                },
                {
                    "sent": "Thank you for your attention.",
                    "label": 0
                },
                {
                    "sent": "I would be happy now to answer your questions.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I use as well.",
                    "label": 0
                },
                {
                    "sent": "It's running for the features of locations and I notice that very often in order to get very good accuracy, you need to sample many many of these high dimensional random projections.",
                    "label": 0
                },
                {
                    "sent": "What is your experience, your experience with that?",
                    "label": 0
                },
                {
                    "sent": "Well, this has to do with a couple of 1000 dimensions to recover the Proto Rico get to get a good performance, but our goal was to propose a model where we also learned the kernel parameters.",
                    "label": 0
                },
                {
                    "sent": "So we worked on.",
                    "label": 0
                },
                {
                    "sent": "We focused on that to learn the kernel parameters on this for stage and we are looking forward to improve the performance and all the you know the sampling.",
                    "label": 0
                },
                {
                    "sent": "I hope this isn't a naive question, but I think that some.",
                    "label": 0
                },
                {
                    "sent": "Kernels who used the matrix because the explicit kernel mapping would map into a space of infinite dimension, and I was wondering if your approximation handles that kind of thing.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "As far as I understood the question was what happens when the lifting function gets your kernel to an infinite dimensional space?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that was the question well.",
                    "label": 0
                },
                {
                    "sent": "The thing is that we approximate the kernel in the original in from the original input space, so we don't have to care about the lifting space.",
                    "label": 0
                },
                {
                    "sent": "As I said, we don't care about the file, we just try to work in the side in the Fourier domain.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "So I think not so, let's thank the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}