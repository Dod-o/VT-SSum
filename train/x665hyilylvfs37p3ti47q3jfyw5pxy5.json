{
    "id": "x665hyilylvfs37p3ti47q3jfyw5pxy5",
    "title": "Entropy Properties of a Decision Rule Class in Connection with machine learning abilities",
    "info": {
        "author": [
            "Alexey Chervonenkis, Computer Learning Research Centre, Royal Holloway, University of London"
        ],
        "published": "Oct. 8, 2007",
        "recorded": "September 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Computational Learning Theory"
        ]
    },
    "url": "http://videolectures.net/ida07_chervonenkis_ep/",
    "segmentation": [
        [
            "OK, welcome to the.",
            "Second day of the conference and today we are very lucky to begin with the.",
            "An invited speaker who.",
            "Many of you, I think will have heard of Alexi Chevening Kiss is his name, but probably not so many will have seen before.",
            "I think his main renown is for the joint work with Bladimir Vapnik on the derivation of the fundamental theorem of learning, as I think it's often referred to which characterizes the learnability of a classification task in terms of the so called VC dimension, which of course stands for that Nick Chervon kiss.",
            "This they worked on in the 70s in in Moscow, and he grew up in Moscow.",
            "In fact, remembers seeing styling at the age of seven in 1946, which tells you something about his age.",
            "In fact, he is 69 today.",
            "It is his birthday today so.",
            "And thank you for agreeing to talk on your birthday like say so he's been working for many years at the Research Institute of Control Problems at the Russian Academy of Sciences.",
            "Together with happening, they went on to develop the so-called generalized portrait method, which is essentially foreshadows the support vector machine approach in terms of maximizing the margin relative to.",
            "A set of training data.",
            "So these ideas were already being developed in Moscow in the 70s.",
            "He now has a part time position at Royal Holloway, University of London.",
            "But he does not limit himself to just theoretical work and has worked on a number of application areas, including geological work, reconstructing the grade field in three dimensions for mine deposits under the ground and so on.",
            "So he has a wide range of experience in both applications, but of course his main renown is for theoretical work, and today he's going to speak about entropy properties of a decision rule class.",
            "In connection with machine learning abilities, and so I'd like to hand over to Alexey.",
            "OK.",
            "I think I am.",
            "I got loud enough noise for the recording.",
            "You just put it up so it doesn't actually, yeah."
        ],
        [
            "So what do we need double?",
            "What do we want from machine learning?",
            "For instance, let's first consider learning to pattern recognition.",
            "The objects are presented here by the points at this picture in 2 dimensional space, but generally it can be presented in multidimensional space or even not as the points in some space.",
            "But by some description.",
            "And we are looking for a decision rule which.",
            "In total, minimize the number of errors.",
            "Possibly it can be not just the number of errors, but some average value of penalty function.",
            "For instance, in the cases when errors are.",
            "Created differently for different kinds of errors and so on.",
            "So we are looking for a decision rule which makes as small number of errors or minimize a penalty function in average."
        ],
        [
            "A similar picture we have when we are learning to estimate some continuous functions or numerical functions, not as in pattern recognition we have.",
            "We have only a district number of possible values bylong an object belong to this or that class, but also the case when we have.",
            "A function, in this case our goal over goal in learning is to find such a function which deviates from the true function as.",
            "As small as possible, it could be mean square error or any other kind of deviation.",
            "And in average we want to find such a function that deviates from the true function.",
            "Which is as close to the truth function as possible.",
            "Of course this picture it is shown only the case when we have one argument, but it is possible also that we have multiple number of arguments or even in the cases when the objects are presented not numerical numerically but for instance by graphs or images or any other kind of presentation."
        ],
        [
            "I shall go back first.",
            "Not the usual way.",
            "Is the following we have the training set.",
            "We have some set of possible decision rules or functions and we are looking for such a function that minimizes the error on the training set.",
            "But"
        ],
        [
            "You see, for instance, in the case of pattern recognition you can.",
            "Be fine that all objects within the red circles belong to the first course and these circles are around the points of the training set of this class of the first class and all other belong to the second class.",
            "For instance, you understand that in general this decision rule will be will work wrong.",
            "Because of course, not only the points within this red circles belong to the first class.",
            "Still it makes no error on the training set cause all points of the first class on the training set are within the red circles.",
            "Why does it happen and?",
            "I shall speak about it in more details."
        ],
        [
            "The same happens with the function reconstruction.",
            "If, for instance, you want to find the function which approximates those.",
            "No.",
            "Battery seems to come OK. Those empirical points and you try to approximate the by a polynomial function then.",
            "Putting large and large degree of a polynomial polynomial large enough degree of a polynomial you of course can approximate the empirical points without any error.",
            "But when you have and you data, have you have new data?",
            "You see large number of errors.",
            "I presented here the case of 1 dimensional dependence dependence on one argument and polynomial approximation, but it is well known that if you have for instance.",
            "Multiple number of arguments and the training set is not.",
            "Is not long enough.",
            "Again, you can approximate the function, for instance by linear function on.",
            "Without any error and you will have a lot of errors on the new data.",
            "The same refers of course, to the cases when the data are presented, not by not in numerical form, but in many other cases in general.",
            "Intuitively, it can be seen that if you have.",
            "Two large number of.",
            "Degrees of freedom in your decision rule.",
            "Then you have.",
            "You can approximate any data by your decision rule, but it will work wrong on the new data."
        ],
        [
            "This is formal, formal definition of the of that I have told you before.",
            "You have some penalty function function between Q between the real value and predicted value.",
            "You want to minimize the true risk with his average over all possible values of X&Y's given some probability distribution on axes and wise.",
            "And empirical risk is just what you have on your training data.",
            "You just look at the penalty effect function calculated over the training set ever aged and try to minimize the empirical risk.",
            "Was the reason intuitively.",
            "Due to the fact that you took the large number law.",
            "Women value converts to average value.",
            "And that's why there were even such papers which.",
            "Follow up with the foundation of this idea that as far as empirical risk converge by probability to the true risk, then we can minimize empirical risk.",
            "Instead of minimizing average risk.",
            "But as we have seen at the previous examples, it doesn't always.",
            "Make right this isn't true.",
            "Why does it happen?"
        ],
        [
            "Here is the.",
            "That thank you.",
            "This line characterized true risk, and here is its minimum value.",
            "This line characterize empirical risk.",
            "And if at some points empirical risk deviates large for a large extent from the true risk, it is possible that you will find not real minimum but some other point where the true risk is large.",
            "But it would be not so if the dependencies of true risk and empirical risk on the parameters of a decision rule are uniformly close.",
            "That means that if you construct some epsilon tube around the curve characterizing true risk, and if your empirical risk lies within this tube, then.",
            "And this video is small enough then minimum of empirical risk will be close to that of the true risk."
        ],
        [
            "And then arises the following question you see, for instance you have a send Justin probability theory but not in learning theory, only in learning theory.",
            "If you have a system of random events, PA is probability of an event, a ICS One XL is a random sample sequence gained under this probability distribution.",
            "In dependently an you A is frequency calculated over this sequence of each event A.",
            "Their newly Tellem says that if we fix the event a then frequency converts to probability in different senses.",
            "But in general it converges with probability one, and there are estimates how close they are for any fixed L length of the training set.",
            "But if you want to have uniform convergence, it is written so this is super am value of deviation between frequency and probability over all.",
            "Events of the system S. Is it true?",
            "If it is true with?",
            "What are the estimates?",
            "It is not true in general, as we have seen on those examples here.",
            "Can make as long training sequence as you want.",
            "Then again make circles over the points of one class and declare that all other points belongs to the other clause and you will have.",
            "You will see that probabilities doesn't work converge to.",
            "Frequencies over this decision room.",
            "So what are the conditions of when probability frequencies converge to probabilities?"
        ],
        [
            "The same refers to the function class.",
            "For instance function classes parameterized by some parameter Alpha abstract parameter.",
            "Again, we have expectation, which depends on this parameter and we can calculate average value of this function of each of these functions.",
            "That gives us average value.",
            "Are on.",
            "Alpha but large number law says that ever is value converts to expectation with probability one.",
            "Really, if this functions are uniformly limited then you can find.",
            "Estimate how close they should be for each training set length.",
            "And again, there appears the question whether uniform convergence is true.",
            "And as we have seen, it is not true in general, Becauses, for instance, if you have the set of polynomials of arbitrary degree, then you can approximate any function without errors.",
            "But really, if there is noise.",
            "True risk will be always have some non 0 value."
        ],
        [
            "I shall go no.",
            "Yep.",
            "So we tried to find the the conditions of uniform convergence of frequencies to probabilities.",
            "For instance, there are there is a.",
            "Training sequence or just a sequence, and it is in this picture.",
            "They are represented by the points.",
            "You have a set of events or set of decision rules, and you can calculate all possible ways to.",
            "Two divide this points to two clauses using this decision rules or a set of events.",
            "Recall the recall it at this picture.",
            "It is shown an example when this.",
            "This isn't rules are half planes, but in general it can be any other.",
            "For instance in uneral neural networks you can find all possible.",
            "Number of all possible dividing over training set using some kind of.",
            "Neural network."
        ],
        [
            "And.",
            "If you find the weed.",
            "Defined this number of possible divisions of the.",
            "Sequence by this set of by the given set of decision rules.",
            "In dice of this set over a sequence X1XL, and if we find maximum value over all possible sequences of length L, we run such function South of El which depends only on the.",
            "System of events.",
            "The length of the training set, and that's all, but not on the particular.",
            "Sequence.",
            "It appears that this function behaves in following way.",
            "It is either just equal to two in degree L. Or it is?",
            "Limited but a polynomial.",
            "And the degree of the polynomial is just equal to the first number when this function is not equal to two in degree L."
        ],
        [
            "And it appears that polynomial growth is sufficient condition for uniform convergence of frequencies to probabilities over the event Class S. In this case, not only conditions of uniform convergence can be found, but also estimates of the uniform convergence of probabilities to frequencies to probabilities can be found.",
            "But this condition appears to be sufficient, but not necessary, so there are there exist examples when the growth function is equal to two in degree L, but still the uniform convergence holds in particularly, so if the SpaceX is countable and the system is consists of all subsets of the state SpaceX.",
            "But it appears that any estimate of uniform convergence.",
            "Not depending on probability distribution, not trivial estimates can be found only in the case when this.",
            "This sufficient condition holds only in the case when growth function is equal to two in degree L. But if instead of maximum of Indus we take expectation, then we get necessary and sufficient conditions for uniform convergence or frequencies to probabilities and they are such.",
            "Average value of.",
            "Entropy we call this value entropy of the set of events over sequence X1.",
            "Excel.",
            "It depends only on the length of the sequence and of course on probability distribution.",
            "So if entropy per symbol goes to 0, this is a necessary and sufficient condition for the uniform convergence of frequencies to probabilities.",
            "But as far as this value depends on.",
            "Distribution probability distribution know estimates in general can be found.",
            "Until you know the probability distribution.",
            "So this condition can be interpreted as follows.",
            "Central paper symbol must go to zero while examples sequence goes to Infinity."
        ],
        [
            "Simeral again we can try to find.",
            "The conditions of uniform convergence of the means to expectations.",
            "We consider only the case of uniformly limited classes of functions becausw.",
            "In the case of non limited functions.",
            "The conditions may be quite other then it is possible to move to find such a function that at one of the points goes to Infinity and in this way maximized average value.",
            "But it's not interesting from our point of view that's why we consider only the case of uniform limited classes or functions.",
            "So again, given random function F depending on parameter Alpha, we find M on Alpha's expectation of this function error on Alpha's mean value, and we ask whether there is uniform convergence of means to probability, which is possible to reviews the problem to the previous one.",
            "It is enough to construct a set South.",
            "Of events defined as as follows.",
            "For all possible Alphas and C values and deployed conditions and estimates of the uniform convergence of this set of events.",
            "For this sense of events."
        ],
        [
            "But then we get only sufficient conditions to deduce necessary and sufficient conditions.",
            "We propose the following construction also."
        ],
        [
            "First show the picture.",
            "In L dimensional space, for each Alpha we make, we construct a point.",
            "Which coordinates equal to the to the value of the function over each X.",
            "At this X we have value of the function for the first point of the of the training sequence.",
            "This for the 2nd at that, for the third, and so on, and dimension of the space is just equal to the length of the sequence.",
            "Then we.",
            "Allow Alpha to take all possible values of the set where Alpha is defined and then we get such a set.",
            "This set can be of course have a dimension much less than.",
            "And to calculate volumes we just.",
            "Make an extension of this set, forming a cube with the side epsilon.",
            "Overall, points of that set.",
            "Now I return to.",
            "So.",
            "To the case."
        ],
        [
            "Here is written what I have told you on the picture.",
            "Having a sequence sample sequence, we can construct some set of all points with coordinates equal to the values of this function for a fixed Alpha and then allow Alpha go through all possible values within Alpha and then we define epsilon extensions of this set as unification of all cubes with edge.",
            "Length epsilon and centers in the points of the set T. And."
        ],
        [
            "Then we can calculate its volume.",
            "The volume of this extended set.",
            "And we define epsilon entropy of this of the class of function as expectation of log of this value volume volume of epsilon extension of the set T and then the necessary and sufficient condition for the uniform convergence or means to expectation is just following.",
            "Again.",
            "Entropy per symbol should go to log epsilon and that means that this set in some set sense behaves as a single cube.",
            "So extension by the set D does not influence on the volume.",
            "And it appears that if this condition holds for some epsilon greater than zero, then it holds for all reference greater than 0."
        ],
        [
            "And it was interesting to find what happens really with the loss of function if.",
            "This entropy per symbol doesn't go to 02 log epsilon.",
            "It goes.",
            "In any case, it goes to some limit, but it can go to log excellent or to some value greater than log epsilon.",
            "For instance, it goes to the value log epsilon plus some value.",
            "Duh.",
            "Then there exists two functions, the upper function.",
            "Shown in red and the lower function showed in shown in blue.",
            "And these functions are definitely different.",
            "In general, it's.",
            "This function is greater or equal to that, but."
        ],
        [
            "The integral of difference.",
            "So the average difference between these two functions is greater than zero and is estimated if you have epsilon and the editor.",
            "So the average distance between this function is greater than zero and can be estimated in dependently on the sample size.",
            "And then it appears that it."
        ],
        [
            "It is possible for almost for any sequence sequence of axis and for all possible.",
            "Assignments at this points to find such a function Alpha that in the points where it is assigned zero it goes.",
            "It is as close as possible to the upper functions and the points when is equal to 1, it is as close as possible to the lower function.",
            "And then you see it is possible just to make overfitting, as in in the learning theory you can just fit the function to the lower or to the to the upper function or to the lower function arbitrary."
        ],
        [
            "And the similar result holds for the case when we consider not uniform convergence of mean of mean values to expectations, but for.",
            "The uniform convergence of frequencies to probabilities.",
            "In this case, it is not necessary to make excellent extension becausw the function has discrete values and they are far from each other.",
            "The points and in this case if entropy per symbol goes not to 0 but to some value C greater than zero, then there exists some set.",
            "Within the space of.",
            "Elementary events that almost for any sample sequence X1XL and any binary sequence you can find such value Alpha that F. Has value Omega.",
            "I chose zero over an arbitrarily.",
            "All this set.",
            "So it is quite seen why the uniform convergence does not hold.",
            "For instance, you have a sequence.",
            "You then find such Alpha that the first half of this sequence has values over this set.",
            "S has value one and the second half of the sequence has the value zero.",
            "When you calculate average value.",
            "Have different values it cannot converge to the same value.",
            "And it is quite similar to the case of overfitting in the case of.",
            "Learning to pattern recognition or.",
            "Numerical function request reconstruction.",
            "So we see that this abstract results in probability theory has tight connections with the problem of pattern recognition of learning to pattern recognition and just machine learning procedures.",
            "My finger, that's all.",
            "Any questions?",
            "Almost.",
            "Dimensionality.",
            "GIF sickness and dimensionality.",
            "Thanks."
        ],
        [
            "Look up device from the other day.",
            "If you take a point and you have a sequence X1, Excel then forgiven Alpha for a given Alpha, you have L values of the function over the point X1XL.",
            "You just have the function F of X Alpha.",
            "Fix Alpha and put instead of XX1XL one by one.",
            "No, just L. Becausw for each.",
            "Sample you have one coordinate.",
            "So the dimensionality is quite equal to the length of the training set.",
            "I was gonna ask at the end the.",
            "Set ass on the last slide.",
            "The one that has essentially on this one.",
            "Other."
        ],
        [
            "Set.",
            "Yes, yes, so you're saying it's sort of like a.",
            "Problem set in some sense, everything goes wrong in that sense.",
            "Can you estimate its size?",
            "Yes it has.",
            "Is it not written here?",
            "Oh no, it might be.",
            "It's not the last.",
            "Is its probability is quite equal.",
            "To see precisely able to see, though it is not written here.",
            "So in a sense you could say that if you're willing to forget probability C. Samples or events?",
            "Yeah, if you agree with the number of errors, average percentage of acral.",
            "Great less than C, then everything will be a key.",
            "So in a sense you are able to learn to an error and error C, but not not more.",
            "Yes, there is some limit of a learning possibility, but it happens for instance, in the case in such a simple case, for instance, you have.",
            "No important recognitions.",
            "The number of simple cases and they can be divided by some simple decision rule and some cases that are very difficult to.",
            "To detect.",
            "Then you can reduce the number of errors up to the probability of these difficult cases, but not less.",
            "And they it really happens often in pattern recognition problems there is some limit, for instance in medicine.",
            "There are some exceptions, possibly they can be.",
            "Also diagonals and diagnosised using some additional additional features or having much longer training set.",
            "But when you have a fixed brannick set, you see that it is possible to reduce the number of errors up to some level, but not less.",
            "But this is saying that even within infinite training, yes.",
            "In this case even.",
            "And it is.",
            "It is really very simple to construct such examples.",
            "For instance you have two subsets, indifference.",
            "At this subset you have linear decision rules and that all possible this is in truth and in that case it is important.",
            "So this is certainly an interesting theoretical result at home, so if you're wondering, are you having thought about this more than we've had a chance to have suggestions for how we can apply this to practical learning algorithms?",
            "Are there things that learning algorithms should do, for example 2?",
            "Twista mate, what the what, what the probability of error that should be acceptable in the airport, which samples they should produce?",
            "As I told you?",
            "Good estimates can be found on the in the case of finite visit, invested dimension, but this general approach.",
            "Can be.",
            "Give a trace.",
            "Two particular problems, for instance as.",
            "John told we applied our method to reconstructions of the grade of some minerals underground.",
            "There are particular methods of to do it, but this general ideas guide us in this case also and in person recognition is the same.",
            "Yeah, maybe picking up that question.",
            "Just say I mean in a concrete application, do you have any way of estimating seed?",
            "No.",
            "Never or not yet not yet.",
            "No.",
            "In machine learning and pattern recognition, let's say decision tree learning.",
            "There are ways of using probabilities, estimating probabilities, and people usually either take relative frequencies or loss estimate or M estimate, which are of course non ideal which assumes kind of uniform distribution.",
            "So can you suggest any practical different approach or you are in line with this approach for practical algorithms I can say only the following.",
            "For instance, if you estimate probabilities in dependently then the uniform convergence is not so important becausw.",
            "The only case when it is just the case of.",
            "Leaving can tell this case when it is the theorem about uniform convergence of empirical distribution.",
            "One dimension and empirical distribution to probability, but it converges route rather quickly, but if you try to find.",
            "Distribution over multidimensional space.",
            "Then these problems arise.",
            "Thank the speaker again.",
            "And I'd like to offer this.",
            "Coffee cups, thank you.",
            "Recognition token of our esteem may take."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, welcome to the.",
                    "label": 0
                },
                {
                    "sent": "Second day of the conference and today we are very lucky to begin with the.",
                    "label": 0
                },
                {
                    "sent": "An invited speaker who.",
                    "label": 0
                },
                {
                    "sent": "Many of you, I think will have heard of Alexi Chevening Kiss is his name, but probably not so many will have seen before.",
                    "label": 0
                },
                {
                    "sent": "I think his main renown is for the joint work with Bladimir Vapnik on the derivation of the fundamental theorem of learning, as I think it's often referred to which characterizes the learnability of a classification task in terms of the so called VC dimension, which of course stands for that Nick Chervon kiss.",
                    "label": 0
                },
                {
                    "sent": "This they worked on in the 70s in in Moscow, and he grew up in Moscow.",
                    "label": 0
                },
                {
                    "sent": "In fact, remembers seeing styling at the age of seven in 1946, which tells you something about his age.",
                    "label": 0
                },
                {
                    "sent": "In fact, he is 69 today.",
                    "label": 0
                },
                {
                    "sent": "It is his birthday today so.",
                    "label": 0
                },
                {
                    "sent": "And thank you for agreeing to talk on your birthday like say so he's been working for many years at the Research Institute of Control Problems at the Russian Academy of Sciences.",
                    "label": 0
                },
                {
                    "sent": "Together with happening, they went on to develop the so-called generalized portrait method, which is essentially foreshadows the support vector machine approach in terms of maximizing the margin relative to.",
                    "label": 0
                },
                {
                    "sent": "A set of training data.",
                    "label": 0
                },
                {
                    "sent": "So these ideas were already being developed in Moscow in the 70s.",
                    "label": 0
                },
                {
                    "sent": "He now has a part time position at Royal Holloway, University of London.",
                    "label": 1
                },
                {
                    "sent": "But he does not limit himself to just theoretical work and has worked on a number of application areas, including geological work, reconstructing the grade field in three dimensions for mine deposits under the ground and so on.",
                    "label": 0
                },
                {
                    "sent": "So he has a wide range of experience in both applications, but of course his main renown is for theoretical work, and today he's going to speak about entropy properties of a decision rule class.",
                    "label": 0
                },
                {
                    "sent": "In connection with machine learning abilities, and so I'd like to hand over to Alexey.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I think I am.",
                    "label": 0
                },
                {
                    "sent": "I got loud enough noise for the recording.",
                    "label": 0
                },
                {
                    "sent": "You just put it up so it doesn't actually, yeah.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what do we need double?",
                    "label": 0
                },
                {
                    "sent": "What do we want from machine learning?",
                    "label": 0
                },
                {
                    "sent": "For instance, let's first consider learning to pattern recognition.",
                    "label": 1
                },
                {
                    "sent": "The objects are presented here by the points at this picture in 2 dimensional space, but generally it can be presented in multidimensional space or even not as the points in some space.",
                    "label": 0
                },
                {
                    "sent": "But by some description.",
                    "label": 1
                },
                {
                    "sent": "And we are looking for a decision rule which.",
                    "label": 0
                },
                {
                    "sent": "In total, minimize the number of errors.",
                    "label": 1
                },
                {
                    "sent": "Possibly it can be not just the number of errors, but some average value of penalty function.",
                    "label": 0
                },
                {
                    "sent": "For instance, in the cases when errors are.",
                    "label": 0
                },
                {
                    "sent": "Created differently for different kinds of errors and so on.",
                    "label": 0
                },
                {
                    "sent": "So we are looking for a decision rule which makes as small number of errors or minimize a penalty function in average.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A similar picture we have when we are learning to estimate some continuous functions or numerical functions, not as in pattern recognition we have.",
                    "label": 0
                },
                {
                    "sent": "We have only a district number of possible values bylong an object belong to this or that class, but also the case when we have.",
                    "label": 0
                },
                {
                    "sent": "A function, in this case our goal over goal in learning is to find such a function which deviates from the true function as.",
                    "label": 0
                },
                {
                    "sent": "As small as possible, it could be mean square error or any other kind of deviation.",
                    "label": 1
                },
                {
                    "sent": "And in average we want to find such a function that deviates from the true function.",
                    "label": 0
                },
                {
                    "sent": "Which is as close to the truth function as possible.",
                    "label": 0
                },
                {
                    "sent": "Of course this picture it is shown only the case when we have one argument, but it is possible also that we have multiple number of arguments or even in the cases when the objects are presented not numerical numerically but for instance by graphs or images or any other kind of presentation.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I shall go back first.",
                    "label": 0
                },
                {
                    "sent": "Not the usual way.",
                    "label": 0
                },
                {
                    "sent": "Is the following we have the training set.",
                    "label": 0
                },
                {
                    "sent": "We have some set of possible decision rules or functions and we are looking for such a function that minimizes the error on the training set.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You see, for instance, in the case of pattern recognition you can.",
                    "label": 0
                },
                {
                    "sent": "Be fine that all objects within the red circles belong to the first course and these circles are around the points of the training set of this class of the first class and all other belong to the second class.",
                    "label": 1
                },
                {
                    "sent": "For instance, you understand that in general this decision rule will be will work wrong.",
                    "label": 0
                },
                {
                    "sent": "Because of course, not only the points within this red circles belong to the first class.",
                    "label": 1
                },
                {
                    "sent": "Still it makes no error on the training set cause all points of the first class on the training set are within the red circles.",
                    "label": 0
                },
                {
                    "sent": "Why does it happen and?",
                    "label": 0
                },
                {
                    "sent": "I shall speak about it in more details.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The same happens with the function reconstruction.",
                    "label": 0
                },
                {
                    "sent": "If, for instance, you want to find the function which approximates those.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Battery seems to come OK. Those empirical points and you try to approximate the by a polynomial function then.",
                    "label": 0
                },
                {
                    "sent": "Putting large and large degree of a polynomial polynomial large enough degree of a polynomial you of course can approximate the empirical points without any error.",
                    "label": 1
                },
                {
                    "sent": "But when you have and you data, have you have new data?",
                    "label": 0
                },
                {
                    "sent": "You see large number of errors.",
                    "label": 0
                },
                {
                    "sent": "I presented here the case of 1 dimensional dependence dependence on one argument and polynomial approximation, but it is well known that if you have for instance.",
                    "label": 1
                },
                {
                    "sent": "Multiple number of arguments and the training set is not.",
                    "label": 0
                },
                {
                    "sent": "Is not long enough.",
                    "label": 0
                },
                {
                    "sent": "Again, you can approximate the function, for instance by linear function on.",
                    "label": 1
                },
                {
                    "sent": "Without any error and you will have a lot of errors on the new data.",
                    "label": 0
                },
                {
                    "sent": "The same refers of course, to the cases when the data are presented, not by not in numerical form, but in many other cases in general.",
                    "label": 0
                },
                {
                    "sent": "Intuitively, it can be seen that if you have.",
                    "label": 0
                },
                {
                    "sent": "Two large number of.",
                    "label": 0
                },
                {
                    "sent": "Degrees of freedom in your decision rule.",
                    "label": 0
                },
                {
                    "sent": "Then you have.",
                    "label": 0
                },
                {
                    "sent": "You can approximate any data by your decision rule, but it will work wrong on the new data.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is formal, formal definition of the of that I have told you before.",
                    "label": 0
                },
                {
                    "sent": "You have some penalty function function between Q between the real value and predicted value.",
                    "label": 1
                },
                {
                    "sent": "You want to minimize the true risk with his average over all possible values of X&Y's given some probability distribution on axes and wise.",
                    "label": 0
                },
                {
                    "sent": "And empirical risk is just what you have on your training data.",
                    "label": 0
                },
                {
                    "sent": "You just look at the penalty effect function calculated over the training set ever aged and try to minimize the empirical risk.",
                    "label": 0
                },
                {
                    "sent": "Was the reason intuitively.",
                    "label": 0
                },
                {
                    "sent": "Due to the fact that you took the large number law.",
                    "label": 1
                },
                {
                    "sent": "Women value converts to average value.",
                    "label": 1
                },
                {
                    "sent": "And that's why there were even such papers which.",
                    "label": 0
                },
                {
                    "sent": "Follow up with the foundation of this idea that as far as empirical risk converge by probability to the true risk, then we can minimize empirical risk.",
                    "label": 1
                },
                {
                    "sent": "Instead of minimizing average risk.",
                    "label": 0
                },
                {
                    "sent": "But as we have seen at the previous examples, it doesn't always.",
                    "label": 0
                },
                {
                    "sent": "Make right this isn't true.",
                    "label": 0
                },
                {
                    "sent": "Why does it happen?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is the.",
                    "label": 0
                },
                {
                    "sent": "That thank you.",
                    "label": 0
                },
                {
                    "sent": "This line characterized true risk, and here is its minimum value.",
                    "label": 0
                },
                {
                    "sent": "This line characterize empirical risk.",
                    "label": 0
                },
                {
                    "sent": "And if at some points empirical risk deviates large for a large extent from the true risk, it is possible that you will find not real minimum but some other point where the true risk is large.",
                    "label": 0
                },
                {
                    "sent": "But it would be not so if the dependencies of true risk and empirical risk on the parameters of a decision rule are uniformly close.",
                    "label": 1
                },
                {
                    "sent": "That means that if you construct some epsilon tube around the curve characterizing true risk, and if your empirical risk lies within this tube, then.",
                    "label": 1
                },
                {
                    "sent": "And this video is small enough then minimum of empirical risk will be close to that of the true risk.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then arises the following question you see, for instance you have a send Justin probability theory but not in learning theory, only in learning theory.",
                    "label": 0
                },
                {
                    "sent": "If you have a system of random events, PA is probability of an event, a ICS One XL is a random sample sequence gained under this probability distribution.",
                    "label": 1
                },
                {
                    "sent": "In dependently an you A is frequency calculated over this sequence of each event A.",
                    "label": 0
                },
                {
                    "sent": "Their newly Tellem says that if we fix the event a then frequency converts to probability in different senses.",
                    "label": 0
                },
                {
                    "sent": "But in general it converges with probability one, and there are estimates how close they are for any fixed L length of the training set.",
                    "label": 0
                },
                {
                    "sent": "But if you want to have uniform convergence, it is written so this is super am value of deviation between frequency and probability over all.",
                    "label": 0
                },
                {
                    "sent": "Events of the system S. Is it true?",
                    "label": 0
                },
                {
                    "sent": "If it is true with?",
                    "label": 0
                },
                {
                    "sent": "What are the estimates?",
                    "label": 0
                },
                {
                    "sent": "It is not true in general, as we have seen on those examples here.",
                    "label": 0
                },
                {
                    "sent": "Can make as long training sequence as you want.",
                    "label": 0
                },
                {
                    "sent": "Then again make circles over the points of one class and declare that all other points belongs to the other clause and you will have.",
                    "label": 0
                },
                {
                    "sent": "You will see that probabilities doesn't work converge to.",
                    "label": 0
                },
                {
                    "sent": "Frequencies over this decision room.",
                    "label": 0
                },
                {
                    "sent": "So what are the conditions of when probability frequencies converge to probabilities?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The same refers to the function class.",
                    "label": 0
                },
                {
                    "sent": "For instance function classes parameterized by some parameter Alpha abstract parameter.",
                    "label": 0
                },
                {
                    "sent": "Again, we have expectation, which depends on this parameter and we can calculate average value of this function of each of these functions.",
                    "label": 0
                },
                {
                    "sent": "That gives us average value.",
                    "label": 0
                },
                {
                    "sent": "Are on.",
                    "label": 0
                },
                {
                    "sent": "Alpha but large number law says that ever is value converts to expectation with probability one.",
                    "label": 0
                },
                {
                    "sent": "Really, if this functions are uniformly limited then you can find.",
                    "label": 0
                },
                {
                    "sent": "Estimate how close they should be for each training set length.",
                    "label": 0
                },
                {
                    "sent": "And again, there appears the question whether uniform convergence is true.",
                    "label": 0
                },
                {
                    "sent": "And as we have seen, it is not true in general, Becauses, for instance, if you have the set of polynomials of arbitrary degree, then you can approximate any function without errors.",
                    "label": 0
                },
                {
                    "sent": "But really, if there is noise.",
                    "label": 0
                },
                {
                    "sent": "True risk will be always have some non 0 value.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I shall go no.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "So we tried to find the the conditions of uniform convergence of frequencies to probabilities.",
                    "label": 0
                },
                {
                    "sent": "For instance, there are there is a.",
                    "label": 0
                },
                {
                    "sent": "Training sequence or just a sequence, and it is in this picture.",
                    "label": 1
                },
                {
                    "sent": "They are represented by the points.",
                    "label": 1
                },
                {
                    "sent": "You have a set of events or set of decision rules, and you can calculate all possible ways to.",
                    "label": 0
                },
                {
                    "sent": "Two divide this points to two clauses using this decision rules or a set of events.",
                    "label": 0
                },
                {
                    "sent": "Recall the recall it at this picture.",
                    "label": 1
                },
                {
                    "sent": "It is shown an example when this.",
                    "label": 0
                },
                {
                    "sent": "This isn't rules are half planes, but in general it can be any other.",
                    "label": 1
                },
                {
                    "sent": "For instance in uneral neural networks you can find all possible.",
                    "label": 0
                },
                {
                    "sent": "Number of all possible dividing over training set using some kind of.",
                    "label": 0
                },
                {
                    "sent": "Neural network.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "If you find the weed.",
                    "label": 0
                },
                {
                    "sent": "Defined this number of possible divisions of the.",
                    "label": 0
                },
                {
                    "sent": "Sequence by this set of by the given set of decision rules.",
                    "label": 0
                },
                {
                    "sent": "In dice of this set over a sequence X1XL, and if we find maximum value over all possible sequences of length L, we run such function South of El which depends only on the.",
                    "label": 0
                },
                {
                    "sent": "System of events.",
                    "label": 0
                },
                {
                    "sent": "The length of the training set, and that's all, but not on the particular.",
                    "label": 0
                },
                {
                    "sent": "Sequence.",
                    "label": 0
                },
                {
                    "sent": "It appears that this function behaves in following way.",
                    "label": 0
                },
                {
                    "sent": "It is either just equal to two in degree L. Or it is?",
                    "label": 1
                },
                {
                    "sent": "Limited but a polynomial.",
                    "label": 0
                },
                {
                    "sent": "And the degree of the polynomial is just equal to the first number when this function is not equal to two in degree L.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And it appears that polynomial growth is sufficient condition for uniform convergence of frequencies to probabilities over the event Class S. In this case, not only conditions of uniform convergence can be found, but also estimates of the uniform convergence of probabilities to frequencies to probabilities can be found.",
                    "label": 0
                },
                {
                    "sent": "But this condition appears to be sufficient, but not necessary, so there are there exist examples when the growth function is equal to two in degree L, but still the uniform convergence holds in particularly, so if the SpaceX is countable and the system is consists of all subsets of the state SpaceX.",
                    "label": 1
                },
                {
                    "sent": "But it appears that any estimate of uniform convergence.",
                    "label": 0
                },
                {
                    "sent": "Not depending on probability distribution, not trivial estimates can be found only in the case when this.",
                    "label": 1
                },
                {
                    "sent": "This sufficient condition holds only in the case when growth function is equal to two in degree L. But if instead of maximum of Indus we take expectation, then we get necessary and sufficient conditions for uniform convergence or frequencies to probabilities and they are such.",
                    "label": 0
                },
                {
                    "sent": "Average value of.",
                    "label": 0
                },
                {
                    "sent": "Entropy we call this value entropy of the set of events over sequence X1.",
                    "label": 0
                },
                {
                    "sent": "Excel.",
                    "label": 0
                },
                {
                    "sent": "It depends only on the length of the sequence and of course on probability distribution.",
                    "label": 1
                },
                {
                    "sent": "So if entropy per symbol goes to 0, this is a necessary and sufficient condition for the uniform convergence of frequencies to probabilities.",
                    "label": 0
                },
                {
                    "sent": "But as far as this value depends on.",
                    "label": 0
                },
                {
                    "sent": "Distribution probability distribution know estimates in general can be found.",
                    "label": 1
                },
                {
                    "sent": "Until you know the probability distribution.",
                    "label": 1
                },
                {
                    "sent": "So this condition can be interpreted as follows.",
                    "label": 0
                },
                {
                    "sent": "Central paper symbol must go to zero while examples sequence goes to Infinity.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Simeral again we can try to find.",
                    "label": 0
                },
                {
                    "sent": "The conditions of uniform convergence of the means to expectations.",
                    "label": 1
                },
                {
                    "sent": "We consider only the case of uniformly limited classes of functions becausw.",
                    "label": 1
                },
                {
                    "sent": "In the case of non limited functions.",
                    "label": 0
                },
                {
                    "sent": "The conditions may be quite other then it is possible to move to find such a function that at one of the points goes to Infinity and in this way maximized average value.",
                    "label": 1
                },
                {
                    "sent": "But it's not interesting from our point of view that's why we consider only the case of uniform limited classes or functions.",
                    "label": 0
                },
                {
                    "sent": "So again, given random function F depending on parameter Alpha, we find M on Alpha's expectation of this function error on Alpha's mean value, and we ask whether there is uniform convergence of means to probability, which is possible to reviews the problem to the previous one.",
                    "label": 1
                },
                {
                    "sent": "It is enough to construct a set South.",
                    "label": 1
                },
                {
                    "sent": "Of events defined as as follows.",
                    "label": 0
                },
                {
                    "sent": "For all possible Alphas and C values and deployed conditions and estimates of the uniform convergence of this set of events.",
                    "label": 1
                },
                {
                    "sent": "For this sense of events.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But then we get only sufficient conditions to deduce necessary and sufficient conditions.",
                    "label": 0
                },
                {
                    "sent": "We propose the following construction also.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First show the picture.",
                    "label": 0
                },
                {
                    "sent": "In L dimensional space, for each Alpha we make, we construct a point.",
                    "label": 0
                },
                {
                    "sent": "Which coordinates equal to the to the value of the function over each X.",
                    "label": 0
                },
                {
                    "sent": "At this X we have value of the function for the first point of the of the training sequence.",
                    "label": 0
                },
                {
                    "sent": "This for the 2nd at that, for the third, and so on, and dimension of the space is just equal to the length of the sequence.",
                    "label": 0
                },
                {
                    "sent": "Then we.",
                    "label": 0
                },
                {
                    "sent": "Allow Alpha to take all possible values of the set where Alpha is defined and then we get such a set.",
                    "label": 0
                },
                {
                    "sent": "This set can be of course have a dimension much less than.",
                    "label": 0
                },
                {
                    "sent": "And to calculate volumes we just.",
                    "label": 0
                },
                {
                    "sent": "Make an extension of this set, forming a cube with the side epsilon.",
                    "label": 0
                },
                {
                    "sent": "Overall, points of that set.",
                    "label": 0
                },
                {
                    "sent": "Now I return to.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "To the case.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is written what I have told you on the picture.",
                    "label": 0
                },
                {
                    "sent": "Having a sequence sample sequence, we can construct some set of all points with coordinates equal to the values of this function for a fixed Alpha and then allow Alpha go through all possible values within Alpha and then we define epsilon extensions of this set as unification of all cubes with edge.",
                    "label": 1
                },
                {
                    "sent": "Length epsilon and centers in the points of the set T. And.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then we can calculate its volume.",
                    "label": 0
                },
                {
                    "sent": "The volume of this extended set.",
                    "label": 0
                },
                {
                    "sent": "And we define epsilon entropy of this of the class of function as expectation of log of this value volume volume of epsilon extension of the set T and then the necessary and sufficient condition for the uniform convergence or means to expectation is just following.",
                    "label": 1
                },
                {
                    "sent": "Again.",
                    "label": 1
                },
                {
                    "sent": "Entropy per symbol should go to log epsilon and that means that this set in some set sense behaves as a single cube.",
                    "label": 0
                },
                {
                    "sent": "So extension by the set D does not influence on the volume.",
                    "label": 0
                },
                {
                    "sent": "And it appears that if this condition holds for some epsilon greater than zero, then it holds for all reference greater than 0.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And it was interesting to find what happens really with the loss of function if.",
                    "label": 0
                },
                {
                    "sent": "This entropy per symbol doesn't go to 02 log epsilon.",
                    "label": 0
                },
                {
                    "sent": "It goes.",
                    "label": 0
                },
                {
                    "sent": "In any case, it goes to some limit, but it can go to log excellent or to some value greater than log epsilon.",
                    "label": 0
                },
                {
                    "sent": "For instance, it goes to the value log epsilon plus some value.",
                    "label": 0
                },
                {
                    "sent": "Duh.",
                    "label": 0
                },
                {
                    "sent": "Then there exists two functions, the upper function.",
                    "label": 1
                },
                {
                    "sent": "Shown in red and the lower function showed in shown in blue.",
                    "label": 0
                },
                {
                    "sent": "And these functions are definitely different.",
                    "label": 0
                },
                {
                    "sent": "In general, it's.",
                    "label": 0
                },
                {
                    "sent": "This function is greater or equal to that, but.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The integral of difference.",
                    "label": 0
                },
                {
                    "sent": "So the average difference between these two functions is greater than zero and is estimated if you have epsilon and the editor.",
                    "label": 1
                },
                {
                    "sent": "So the average distance between this function is greater than zero and can be estimated in dependently on the sample size.",
                    "label": 1
                },
                {
                    "sent": "And then it appears that it.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It is possible for almost for any sequence sequence of axis and for all possible.",
                    "label": 0
                },
                {
                    "sent": "Assignments at this points to find such a function Alpha that in the points where it is assigned zero it goes.",
                    "label": 0
                },
                {
                    "sent": "It is as close as possible to the upper functions and the points when is equal to 1, it is as close as possible to the lower function.",
                    "label": 1
                },
                {
                    "sent": "And then you see it is possible just to make overfitting, as in in the learning theory you can just fit the function to the lower or to the to the upper function or to the lower function arbitrary.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the similar result holds for the case when we consider not uniform convergence of mean of mean values to expectations, but for.",
                    "label": 1
                },
                {
                    "sent": "The uniform convergence of frequencies to probabilities.",
                    "label": 1
                },
                {
                    "sent": "In this case, it is not necessary to make excellent extension becausw the function has discrete values and they are far from each other.",
                    "label": 0
                },
                {
                    "sent": "The points and in this case if entropy per symbol goes not to 0 but to some value C greater than zero, then there exists some set.",
                    "label": 0
                },
                {
                    "sent": "Within the space of.",
                    "label": 0
                },
                {
                    "sent": "Elementary events that almost for any sample sequence X1XL and any binary sequence you can find such value Alpha that F. Has value Omega.",
                    "label": 1
                },
                {
                    "sent": "I chose zero over an arbitrarily.",
                    "label": 0
                },
                {
                    "sent": "All this set.",
                    "label": 0
                },
                {
                    "sent": "So it is quite seen why the uniform convergence does not hold.",
                    "label": 0
                },
                {
                    "sent": "For instance, you have a sequence.",
                    "label": 0
                },
                {
                    "sent": "You then find such Alpha that the first half of this sequence has values over this set.",
                    "label": 0
                },
                {
                    "sent": "S has value one and the second half of the sequence has the value zero.",
                    "label": 0
                },
                {
                    "sent": "When you calculate average value.",
                    "label": 0
                },
                {
                    "sent": "Have different values it cannot converge to the same value.",
                    "label": 0
                },
                {
                    "sent": "And it is quite similar to the case of overfitting in the case of.",
                    "label": 0
                },
                {
                    "sent": "Learning to pattern recognition or.",
                    "label": 0
                },
                {
                    "sent": "Numerical function request reconstruction.",
                    "label": 0
                },
                {
                    "sent": "So we see that this abstract results in probability theory has tight connections with the problem of pattern recognition of learning to pattern recognition and just machine learning procedures.",
                    "label": 0
                },
                {
                    "sent": "My finger, that's all.",
                    "label": 0
                },
                {
                    "sent": "Any questions?",
                    "label": 0
                },
                {
                    "sent": "Almost.",
                    "label": 0
                },
                {
                    "sent": "Dimensionality.",
                    "label": 0
                },
                {
                    "sent": "GIF sickness and dimensionality.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look up device from the other day.",
                    "label": 0
                },
                {
                    "sent": "If you take a point and you have a sequence X1, Excel then forgiven Alpha for a given Alpha, you have L values of the function over the point X1XL.",
                    "label": 0
                },
                {
                    "sent": "You just have the function F of X Alpha.",
                    "label": 0
                },
                {
                    "sent": "Fix Alpha and put instead of XX1XL one by one.",
                    "label": 0
                },
                {
                    "sent": "No, just L. Becausw for each.",
                    "label": 0
                },
                {
                    "sent": "Sample you have one coordinate.",
                    "label": 0
                },
                {
                    "sent": "So the dimensionality is quite equal to the length of the training set.",
                    "label": 0
                },
                {
                    "sent": "I was gonna ask at the end the.",
                    "label": 0
                },
                {
                    "sent": "Set ass on the last slide.",
                    "label": 0
                },
                {
                    "sent": "The one that has essentially on this one.",
                    "label": 0
                },
                {
                    "sent": "Other.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Set.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes, so you're saying it's sort of like a.",
                    "label": 0
                },
                {
                    "sent": "Problem set in some sense, everything goes wrong in that sense.",
                    "label": 0
                },
                {
                    "sent": "Can you estimate its size?",
                    "label": 0
                },
                {
                    "sent": "Yes it has.",
                    "label": 0
                },
                {
                    "sent": "Is it not written here?",
                    "label": 0
                },
                {
                    "sent": "Oh no, it might be.",
                    "label": 0
                },
                {
                    "sent": "It's not the last.",
                    "label": 0
                },
                {
                    "sent": "Is its probability is quite equal.",
                    "label": 0
                },
                {
                    "sent": "To see precisely able to see, though it is not written here.",
                    "label": 0
                },
                {
                    "sent": "So in a sense you could say that if you're willing to forget probability C. Samples or events?",
                    "label": 0
                },
                {
                    "sent": "Yeah, if you agree with the number of errors, average percentage of acral.",
                    "label": 0
                },
                {
                    "sent": "Great less than C, then everything will be a key.",
                    "label": 0
                },
                {
                    "sent": "So in a sense you are able to learn to an error and error C, but not not more.",
                    "label": 0
                },
                {
                    "sent": "Yes, there is some limit of a learning possibility, but it happens for instance, in the case in such a simple case, for instance, you have.",
                    "label": 1
                },
                {
                    "sent": "No important recognitions.",
                    "label": 0
                },
                {
                    "sent": "The number of simple cases and they can be divided by some simple decision rule and some cases that are very difficult to.",
                    "label": 0
                },
                {
                    "sent": "To detect.",
                    "label": 0
                },
                {
                    "sent": "Then you can reduce the number of errors up to the probability of these difficult cases, but not less.",
                    "label": 1
                },
                {
                    "sent": "And they it really happens often in pattern recognition problems there is some limit, for instance in medicine.",
                    "label": 0
                },
                {
                    "sent": "There are some exceptions, possibly they can be.",
                    "label": 0
                },
                {
                    "sent": "Also diagonals and diagnosised using some additional additional features or having much longer training set.",
                    "label": 0
                },
                {
                    "sent": "But when you have a fixed brannick set, you see that it is possible to reduce the number of errors up to some level, but not less.",
                    "label": 0
                },
                {
                    "sent": "But this is saying that even within infinite training, yes.",
                    "label": 0
                },
                {
                    "sent": "In this case even.",
                    "label": 0
                },
                {
                    "sent": "And it is.",
                    "label": 0
                },
                {
                    "sent": "It is really very simple to construct such examples.",
                    "label": 0
                },
                {
                    "sent": "For instance you have two subsets, indifference.",
                    "label": 0
                },
                {
                    "sent": "At this subset you have linear decision rules and that all possible this is in truth and in that case it is important.",
                    "label": 0
                },
                {
                    "sent": "So this is certainly an interesting theoretical result at home, so if you're wondering, are you having thought about this more than we've had a chance to have suggestions for how we can apply this to practical learning algorithms?",
                    "label": 0
                },
                {
                    "sent": "Are there things that learning algorithms should do, for example 2?",
                    "label": 0
                },
                {
                    "sent": "Twista mate, what the what, what the probability of error that should be acceptable in the airport, which samples they should produce?",
                    "label": 0
                },
                {
                    "sent": "As I told you?",
                    "label": 0
                },
                {
                    "sent": "Good estimates can be found on the in the case of finite visit, invested dimension, but this general approach.",
                    "label": 0
                },
                {
                    "sent": "Can be.",
                    "label": 0
                },
                {
                    "sent": "Give a trace.",
                    "label": 0
                },
                {
                    "sent": "Two particular problems, for instance as.",
                    "label": 0
                },
                {
                    "sent": "John told we applied our method to reconstructions of the grade of some minerals underground.",
                    "label": 1
                },
                {
                    "sent": "There are particular methods of to do it, but this general ideas guide us in this case also and in person recognition is the same.",
                    "label": 0
                },
                {
                    "sent": "Yeah, maybe picking up that question.",
                    "label": 0
                },
                {
                    "sent": "Just say I mean in a concrete application, do you have any way of estimating seed?",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Never or not yet not yet.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "In machine learning and pattern recognition, let's say decision tree learning.",
                    "label": 0
                },
                {
                    "sent": "There are ways of using probabilities, estimating probabilities, and people usually either take relative frequencies or loss estimate or M estimate, which are of course non ideal which assumes kind of uniform distribution.",
                    "label": 0
                },
                {
                    "sent": "So can you suggest any practical different approach or you are in line with this approach for practical algorithms I can say only the following.",
                    "label": 1
                },
                {
                    "sent": "For instance, if you estimate probabilities in dependently then the uniform convergence is not so important becausw.",
                    "label": 0
                },
                {
                    "sent": "The only case when it is just the case of.",
                    "label": 0
                },
                {
                    "sent": "Leaving can tell this case when it is the theorem about uniform convergence of empirical distribution.",
                    "label": 1
                },
                {
                    "sent": "One dimension and empirical distribution to probability, but it converges route rather quickly, but if you try to find.",
                    "label": 0
                },
                {
                    "sent": "Distribution over multidimensional space.",
                    "label": 0
                },
                {
                    "sent": "Then these problems arise.",
                    "label": 0
                },
                {
                    "sent": "Thank the speaker again.",
                    "label": 0
                },
                {
                    "sent": "And I'd like to offer this.",
                    "label": 0
                },
                {
                    "sent": "Coffee cups, thank you.",
                    "label": 0
                },
                {
                    "sent": "Recognition token of our esteem may take.",
                    "label": 0
                }
            ]
        }
    }
}