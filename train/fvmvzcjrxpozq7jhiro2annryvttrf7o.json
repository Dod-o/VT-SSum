{
    "id": "fvmvzcjrxpozq7jhiro2annryvttrf7o",
    "title": "High-dimensional Spectral Feature Selection for 3D Object Recognition based on Reeb Graphs",
    "info": {
        "author": [
            "Francisco Escolano, Department of Science of the Computation and Artificial Intelligence, University of Alicante"
        ],
        "published": "Sept. 13, 2010",
        "recorded": "August 2010",
        "category": [
            "Top->Computer Science->Pattern Recognition"
        ]
    },
    "url": "http://videolectures.net/ssspr2010_escolano_hds/",
    "segmentation": [
        [
            "So the idea is that."
        ],
        [
            "Feature selection is mainly motivated by reducing the dimensionality of for improving the performance of classifiers.",
            "And.",
            "The pursuit of some subsets for better understanding the roles of these features in the in the patterns.",
            "In this case, the patterns are going to be graphs.",
            "So.",
            "There are."
        ],
        [
            "Attached are two types of approaches.",
            "Rappers and filters.",
            "Rappers depend on specific classifiers, that is you select back of features.",
            "Keep the features based on the performance of the classifier.",
            "Filters are independent of the classifier.",
            "That is, you select the back of filters which is more informative for four."
        ],
        [
            "About the class of the big supervise problem, so we're talking about filters and they're good."
        ],
        [
            "And the starting point is that we need the criterion for selecting the features and the typical guitar is mutual information, while mutual information between the features and the class.",
            "Because they are the most informative and becausw the devices error is bounded by mutual information.",
            "The big problem here now is how to estimate mutual information cause.",
            "It depends on the entropy of the class and the entropy of the of this conditional distribution."
        ],
        [
            "So some of the features are.",
            "Can be removed and someone returned for instance."
        ],
        [
            "X1 has the same role and that X2 and X3 so."
        ],
        [
            "This is the formal definition of mutual information.",
            "This is the average Cooper divergents.",
            "And the."
        ],
        [
            "The idea is that in many, when you have a lot of features for in this case coming from graphs and you want to find the most informative features, imagine that you have 500 features."
        ],
        [
            "The typical approach is to assume some kind of independence between the features and select introducing the back one feature.",
            "That's a computer which is informative in conjunction with the previous one and is not redundant with respect to this."
        ],
        [
            "So basically it can be proved that this cute alien called mean redundancy Max relevance from Pankonin Pammy in 2005.",
            "Estimate estimate is the what is the called the maximum dependence criterion which is the joint the mutual information between not isolated features.",
            "Better bet between all the interactions within the features.",
            "Because if you take only isolated features, you may have probably.",
            "Local minimum or something like that."
        ],
        [
            "So the idea is how to estimate that in high dimensions and."
        ],
        [
            "For doing that in high dimensions, we use the I."
        ],
        [
            "This one is about learning costimator learning costimator reason estimator.",
            "I got directly of the of the."
        ],
        [
            "Entropy?",
            "We depends on the neighborhood of the points in any dimensions.",
            "The."
        ],
        [
            "So.",
            "Things sometimes with few number of features overestimated the true entropy of the true conditional entropy in this case.",
            "But typically, in a maximization minimization problem, maximization in this case of information works pretty well so."
        ],
        [
            "I want to apply this idea too."
        ],
        [
            "The structure of the Commission and for that what we use our re graphs, big graphs.",
            "The graphs are defined by real function is defined over the graph of if you're going."
        ],
        [
            "Two there.",
            "Level set approach and you retain some critical points.",
            "All the level sets you have different versions if you change the function you change the type of graph.",
            "One question is what type of graph is more interesting for classifying the objects we don't know.",
            "In addition, what kind of?"
        ],
        [
            "Pictures coming from the graphs are more interesting.",
            "Have has an important role for classification.",
            "Don't know that for that and we design it.",
            "Not exhaustive, but complete catalog of features, for instance of graph not know centrality.",
            "And my sister is going to talk about that tomorrow.",
            "The typical performance eigenvector of the.",
            "The largest segment of the adjacency matrix that agency Spectra."
        ],
        [
            "Spectrum laplacians the feeling vector which has very important information about the connectivity of the graph commuting times.",
            "The heat flow complexity trace, which is a characterization of complexity of the graph."
        ],
        [
            "And OK from all these elements we translate, we make, we compute histogram from them and put all the filters together in a big histogram.",
            "So thing that we may have, we have three types of nodes and 99 types of features an depending on the number of bins we may have close to 600 features, and for these features.",
            "Around the mutual information the selection."
        ],
        [
            "With these new range of features.",
            "Using this trick, databases are benchmark for 3D object recognition.",
            "And.",
            "What we find is that.",
            "As the mutual information with respect to the class grows with the number of features, the error you have some minimal error.",
            "This is the test error and this is the training error.",
            "We have a minimum of error and this marks what is the optimal number of."
        ],
        [
            "So in this case, the document number of features is close to 200 of them, But in this."
        ],
        [
            "OK, this areas represent what is the participation, what is their role, the importance of each kind of features is interesting to know that for this experiment with 300 shapes 2D shapes.",
            "Which means 900 graphs.",
            "All the representations play a similar role, so."
        ],
        [
            "But in the case of.",
            "The significance of the features selected.",
            "Many of the features come from a from the field of vector which is."
        ],
        [
            "Can be can be understood as classical result.",
            "Some of them come from from complexity flow.",
            "Some of them come from the degrees, and some of them, many of them come from computer times.",
            "Either in the diversion of normalized oppression, or unnormalized classic.",
            "So now we know what features are interesting.",
            "For this in spectral domain for classifying graphs and the good thing is that we get only 23% error.",
            "Which is higher?",
            "It is lower than the error reported for this database using attributes we are not using attributes.",
            "In this case we are only using the topology of the graphs.",
            "So."
        ],
        [
            "We plan to do that so we have done some experiments with several categories to check the consistency of frindle vector and computer times.",
            "That is not the full database but in partial database.",
            "Also we find the same pattern of participation of their features.",
            "This again."
        ],
        [
            "It happens the same here Friedler and committed times have domain 18 and.",
            "Regarding the other methods which can do feature selection forward, that is getting a feature."
        ],
        [
            "I know that I know that we are able to do that backwards.",
            "That is, all the features and suppressing there is informative, each time doing that.",
            "And doing some bootstrapping we can."
        ],
        [
            "See here that the the variance of the error is very low.",
            "In comparison with the, we doing doing the same forward, which means that we are less prone to local minimum going backwards in feature selection.",
            "So we're playing in five dimensional field.",
            "We are selecting things bottled water efficiently.",
            "And we are finding the role of spectral features of several expected, filtered out of all of them in graph classification."
        ],
        [
            "Think that's all?",
            "So the challenges are."
        ],
        [
            "This is the 4th step forward to deal with attribute graphs.",
            "If you change in the Laplacian the adjacency by the avoid metrics.",
            "You can do that, and expanding meant what will happens with that that we had not done yet.",
            "So that is modifying the Laplacian enough or would do we need another kind of?",
            "Approach what is the limit of the bypass entropy estimator?",
            "We regarding the number of features and the number of samples we have.",
            "What happens if we need to measure meet mutual information?",
            "Not between several features.",
            "Anna class, which is a several features double dimensional.",
            "Other classes, well labeled.",
            "But imagine that we want to measure mutual information between several features and another body able multidimensional.",
            "Also there's some is the extension of the estimators.",
            "Bypass estimators for estimating mutual information, but we have not no confident on them for several reasons about their construction.",
            "So this could be very nice for progression problems, not only for classification problems.",
            "And finally it does.",
            "The method provides an insight on how to incorporate other graph descriptors, or not necessarily a spectral or any other kind of that is unified way of incorporating features and selecting them.",
            "And."
        ],
        [
            "Axel."
        ],
        [
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the idea is that.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Feature selection is mainly motivated by reducing the dimensionality of for improving the performance of classifiers.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The pursuit of some subsets for better understanding the roles of these features in the in the patterns.",
                    "label": 0
                },
                {
                    "sent": "In this case, the patterns are going to be graphs.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "There are.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Attached are two types of approaches.",
                    "label": 0
                },
                {
                    "sent": "Rappers and filters.",
                    "label": 0
                },
                {
                    "sent": "Rappers depend on specific classifiers, that is you select back of features.",
                    "label": 0
                },
                {
                    "sent": "Keep the features based on the performance of the classifier.",
                    "label": 1
                },
                {
                    "sent": "Filters are independent of the classifier.",
                    "label": 0
                },
                {
                    "sent": "That is, you select the back of filters which is more informative for four.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About the class of the big supervise problem, so we're talking about filters and they're good.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the starting point is that we need the criterion for selecting the features and the typical guitar is mutual information, while mutual information between the features and the class.",
                    "label": 1
                },
                {
                    "sent": "Because they are the most informative and becausw the devices error is bounded by mutual information.",
                    "label": 0
                },
                {
                    "sent": "The big problem here now is how to estimate mutual information cause.",
                    "label": 0
                },
                {
                    "sent": "It depends on the entropy of the class and the entropy of the of this conditional distribution.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So some of the features are.",
                    "label": 0
                },
                {
                    "sent": "Can be removed and someone returned for instance.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "X1 has the same role and that X2 and X3 so.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is the formal definition of mutual information.",
                    "label": 1
                },
                {
                    "sent": "This is the average Cooper divergents.",
                    "label": 0
                },
                {
                    "sent": "And the.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The idea is that in many, when you have a lot of features for in this case coming from graphs and you want to find the most informative features, imagine that you have 500 features.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The typical approach is to assume some kind of independence between the features and select introducing the back one feature.",
                    "label": 0
                },
                {
                    "sent": "That's a computer which is informative in conjunction with the previous one and is not redundant with respect to this.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So basically it can be proved that this cute alien called mean redundancy Max relevance from Pankonin Pammy in 2005.",
                    "label": 0
                },
                {
                    "sent": "Estimate estimate is the what is the called the maximum dependence criterion which is the joint the mutual information between not isolated features.",
                    "label": 0
                },
                {
                    "sent": "Better bet between all the interactions within the features.",
                    "label": 0
                },
                {
                    "sent": "Because if you take only isolated features, you may have probably.",
                    "label": 0
                },
                {
                    "sent": "Local minimum or something like that.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the idea is how to estimate that in high dimensions and.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For doing that in high dimensions, we use the I.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This one is about learning costimator learning costimator reason estimator.",
                    "label": 0
                },
                {
                    "sent": "I got directly of the of the.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Entropy?",
                    "label": 0
                },
                {
                    "sent": "We depends on the neighborhood of the points in any dimensions.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Things sometimes with few number of features overestimated the true entropy of the true conditional entropy in this case.",
                    "label": 0
                },
                {
                    "sent": "But typically, in a maximization minimization problem, maximization in this case of information works pretty well so.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I want to apply this idea too.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The structure of the Commission and for that what we use our re graphs, big graphs.",
                    "label": 0
                },
                {
                    "sent": "The graphs are defined by real function is defined over the graph of if you're going.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two there.",
                    "label": 0
                },
                {
                    "sent": "Level set approach and you retain some critical points.",
                    "label": 0
                },
                {
                    "sent": "All the level sets you have different versions if you change the function you change the type of graph.",
                    "label": 0
                },
                {
                    "sent": "One question is what type of graph is more interesting for classifying the objects we don't know.",
                    "label": 0
                },
                {
                    "sent": "In addition, what kind of?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Pictures coming from the graphs are more interesting.",
                    "label": 0
                },
                {
                    "sent": "Have has an important role for classification.",
                    "label": 0
                },
                {
                    "sent": "Don't know that for that and we design it.",
                    "label": 0
                },
                {
                    "sent": "Not exhaustive, but complete catalog of features, for instance of graph not know centrality.",
                    "label": 1
                },
                {
                    "sent": "And my sister is going to talk about that tomorrow.",
                    "label": 0
                },
                {
                    "sent": "The typical performance eigenvector of the.",
                    "label": 0
                },
                {
                    "sent": "The largest segment of the adjacency matrix that agency Spectra.",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Spectrum laplacians the feeling vector which has very important information about the connectivity of the graph commuting times.",
                    "label": 0
                },
                {
                    "sent": "The heat flow complexity trace, which is a characterization of complexity of the graph.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And OK from all these elements we translate, we make, we compute histogram from them and put all the filters together in a big histogram.",
                    "label": 0
                },
                {
                    "sent": "So thing that we may have, we have three types of nodes and 99 types of features an depending on the number of bins we may have close to 600 features, and for these features.",
                    "label": 1
                },
                {
                    "sent": "Around the mutual information the selection.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With these new range of features.",
                    "label": 0
                },
                {
                    "sent": "Using this trick, databases are benchmark for 3D object recognition.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "What we find is that.",
                    "label": 0
                },
                {
                    "sent": "As the mutual information with respect to the class grows with the number of features, the error you have some minimal error.",
                    "label": 1
                },
                {
                    "sent": "This is the test error and this is the training error.",
                    "label": 0
                },
                {
                    "sent": "We have a minimum of error and this marks what is the optimal number of.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this case, the document number of features is close to 200 of them, But in this.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, this areas represent what is the participation, what is their role, the importance of each kind of features is interesting to know that for this experiment with 300 shapes 2D shapes.",
                    "label": 0
                },
                {
                    "sent": "Which means 900 graphs.",
                    "label": 0
                },
                {
                    "sent": "All the representations play a similar role, so.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But in the case of.",
                    "label": 0
                },
                {
                    "sent": "The significance of the features selected.",
                    "label": 0
                },
                {
                    "sent": "Many of the features come from a from the field of vector which is.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can be can be understood as classical result.",
                    "label": 0
                },
                {
                    "sent": "Some of them come from from complexity flow.",
                    "label": 0
                },
                {
                    "sent": "Some of them come from the degrees, and some of them, many of them come from computer times.",
                    "label": 0
                },
                {
                    "sent": "Either in the diversion of normalized oppression, or unnormalized classic.",
                    "label": 0
                },
                {
                    "sent": "So now we know what features are interesting.",
                    "label": 0
                },
                {
                    "sent": "For this in spectral domain for classifying graphs and the good thing is that we get only 23% error.",
                    "label": 0
                },
                {
                    "sent": "Which is higher?",
                    "label": 0
                },
                {
                    "sent": "It is lower than the error reported for this database using attributes we are not using attributes.",
                    "label": 0
                },
                {
                    "sent": "In this case we are only using the topology of the graphs.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We plan to do that so we have done some experiments with several categories to check the consistency of frindle vector and computer times.",
                    "label": 0
                },
                {
                    "sent": "That is not the full database but in partial database.",
                    "label": 0
                },
                {
                    "sent": "Also we find the same pattern of participation of their features.",
                    "label": 0
                },
                {
                    "sent": "This again.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It happens the same here Friedler and committed times have domain 18 and.",
                    "label": 0
                },
                {
                    "sent": "Regarding the other methods which can do feature selection forward, that is getting a feature.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I know that I know that we are able to do that backwards.",
                    "label": 0
                },
                {
                    "sent": "That is, all the features and suppressing there is informative, each time doing that.",
                    "label": 0
                },
                {
                    "sent": "And doing some bootstrapping we can.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See here that the the variance of the error is very low.",
                    "label": 0
                },
                {
                    "sent": "In comparison with the, we doing doing the same forward, which means that we are less prone to local minimum going backwards in feature selection.",
                    "label": 0
                },
                {
                    "sent": "So we're playing in five dimensional field.",
                    "label": 0
                },
                {
                    "sent": "We are selecting things bottled water efficiently.",
                    "label": 0
                },
                {
                    "sent": "And we are finding the role of spectral features of several expected, filtered out of all of them in graph classification.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Think that's all?",
                    "label": 0
                },
                {
                    "sent": "So the challenges are.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is the 4th step forward to deal with attribute graphs.",
                    "label": 1
                },
                {
                    "sent": "If you change in the Laplacian the adjacency by the avoid metrics.",
                    "label": 0
                },
                {
                    "sent": "You can do that, and expanding meant what will happens with that that we had not done yet.",
                    "label": 0
                },
                {
                    "sent": "So that is modifying the Laplacian enough or would do we need another kind of?",
                    "label": 0
                },
                {
                    "sent": "Approach what is the limit of the bypass entropy estimator?",
                    "label": 1
                },
                {
                    "sent": "We regarding the number of features and the number of samples we have.",
                    "label": 1
                },
                {
                    "sent": "What happens if we need to measure meet mutual information?",
                    "label": 0
                },
                {
                    "sent": "Not between several features.",
                    "label": 0
                },
                {
                    "sent": "Anna class, which is a several features double dimensional.",
                    "label": 0
                },
                {
                    "sent": "Other classes, well labeled.",
                    "label": 0
                },
                {
                    "sent": "But imagine that we want to measure mutual information between several features and another body able multidimensional.",
                    "label": 0
                },
                {
                    "sent": "Also there's some is the extension of the estimators.",
                    "label": 0
                },
                {
                    "sent": "Bypass estimators for estimating mutual information, but we have not no confident on them for several reasons about their construction.",
                    "label": 0
                },
                {
                    "sent": "So this could be very nice for progression problems, not only for classification problems.",
                    "label": 0
                },
                {
                    "sent": "And finally it does.",
                    "label": 1
                },
                {
                    "sent": "The method provides an insight on how to incorporate other graph descriptors, or not necessarily a spectral or any other kind of that is unified way of incorporating features and selecting them.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Axel.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}