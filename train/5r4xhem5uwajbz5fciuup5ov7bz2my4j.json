{
    "id": "5r4xhem5uwajbz5fciuup5ov7bz2my4j",
    "title": "On the Expressive Efficiency of Overlapping Architectures of Deep Learning",
    "info": {
        "author": [
            "Or Sharir, School of Computer Science and Engineering, The Hebrew University of Jerusalem"
        ],
        "published": "July 27, 2017",
        "recorded": "June 2017",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2017_sharir_deep_learning/",
    "segmentation": [
        [
            "That's just sad.",
            "The efficiency of the overlapping architectural deep learning and it's a joint work with Alan Shaw."
        ],
        [
            "So I'm going to stop by, so the goal is work is to compare between overlapping and non overlapping architectures.",
            "Where we define an overlapping architecture is one of the where there exists at least one layer in which the receptive field is larger than astride.",
            "In this case, if you look at nearby neurons, then the receptive fields have some amount of overlap.",
            "And we define none of it up in architecture as well, where all of the layers are receptive fields equal to the stride, in which case there are several fields are always done overlapping."
        ],
        [
            "So why do we care about this distinction?",
            "Is that if you look for theoretical perspective?",
            "And none of open architecture actually makes some sense.",
            "So for instance, we can prove that prove that now overlapping architecture universal, which means that for any function given sufficient number of channels and other open architecture can approximate this function.",
            "An in terms of optimization, there was a paper recently which showed that for certain types of neural networks then of up in case has better convergence.",
            "Convergence guarantees when overlapping one.",
            "However, however, when we look at in practice, we see that the only very few cases where people use an actual non overlapping architecture.",
            "One such rare cases in article on Wavenet where because the user dilation related convolutions then to predict each sample of the network the network is affectively nonoverlapping.",
            "But beyond this and a few others, multiple don't use completely overlapping architectures, and if we look at modern practice more broadly.",
            "And we see that.",
            "Over the years people use every small receptive fields including many non overlapping layers, most famously one by one convolution or two by two pooling but never all the layers.",
            "So we asked two questions.",
            "One is why are non overlapping architecture so uncommon and white seems that having just a little bit of overlapping is somewhat sufficient for most tasks.",
            "The."
        ],
        [
            "The way we're going to approach it.",
            "We want to address this question by looking at the expressive efficiency.",
            "So."
        ],
        [
            "First define this term broadly, so next slide in the next slide going to find formally so expressive efficiency is simply a way to compare 2 network architecture.",
            "Which are limited to other small size, so do not buy HA&HB, just spaces of functions that can be represented by a smaller network architecture of type A or B.",
            "Let me say that a sufficient with respect to be if H District super set of HB, which means that there exists at least one function which Akron present efficiently.",
            "But we cannot and we also defined at a is completely efficient with respect to V. If HB has zero volume inside HAA, which essentially means that for almost all function the acre proficiently be cannot."
        ],
        [
            "Formally, we say that A is exponentially efficient with respect to be if for all functions realizable by be of size RB, that can be realized by a of size are of size RA, which is its most polynomial in RB, and Additionally that there exists one.",
            "There exists a function that can be realized by a of size RA which requires B to 12 size RB, which is super polynomial in RA.",
            "And finally we find that.",
            "Is completely efficient with respect to V if the second condition holds for almost all functions beside the measure 0 set."
        ],
        [
            "So to give you some example this information which was also briefly mentioned in the area talk at this morning is the efficiency of depth.",
            "So we know from particle experience that deep networks perform much better than shallow ones.",
            "An example we see in this graph.",
            "The error rates on emergent access ification as well as well as important as well the number of layers of each model, and we see a strong correlation between number of layers used and the performance achieved by each model.",
            "So one way to explain this phenomena?",
            "Is Vice actually proving that deep networks are exponentially efficient with this vector shallow ones?",
            "An many people are on this problem all the way back from the late 80s on Boolean circuits, as well as more recent papers as the one mentioned earlier today.",
            "And for the rest of this talk, I want to we want to show a similar relation between overlapping and non overlapping architectures.",
            "So before I move on."
        ],
        [
            "So briefly mentioned that in our group we work on many ways to study evolution networks including deficiency problem, which I mentioned earlier as well as we have also worked on connectivity patterns and number of channels.",
            "So if this topic interests you as it should check this out."
        ],
        [
            "So movie on."
        ],
        [
            "To analyze the expensive efficiency, we're going to look at consider a special case of covenants which we call convolutional arithmetic circuits, or convey C for short, and the reason not to consider consider this special case is because we can actually show that they are equivalent to Article 10 of the compositions which open door to using various mathematical tools to analyze, to analyze them.",
            "And we have also shown that in some cases we even extend this use of these tools to more traditional components, including one with early activations.",
            "I would also have to mention that these are not just a theoretical framework to analyze the coordinates, but we've also used this networks in practice various into other papers."
        ],
        [
            "So the baseline conveys the architecture.",
            "Looks like this.",
            "We start with some convolutional layer followed by a pairs of Kanpur layers and with some dense layer and where it differ from regular covenants is that we limit the convolution layers beside the first one to just one by one convolution followed by linear activations instead of the common rally activation and all pooling layers are limited to product pooling instead of average or Max pooling and adjust non overlapping windows.",
            "This is the network which we analyzing previous work and showed various.",
            "Georgica results, but its main limitation is that it doesn't support the overlapping architecture, only non overlapping ones in this way."
        ],
        [
            "Visualize this framework to what we call jealous conversion architectures.",
            "So in general is converses.",
            "By designing new generous conversion operation, we generalize the one by one convolution with linear activation and the product pooling operation.",
            "Where we define a pulling, pulling their layer simply by astride larger than one similar, similar to how it was done in the ocean net type architecture.",
            "And what's special about this type of architecture is when we limited to novel of non overlapping case it reduces back to the same conversation network which we've analyzed before.",
            "So we can trust for all other tools back to this architecture.",
            "So with this we are set to analyze the overlapping case so."
        ],
        [
            "First of those is shown that an overlapping architecture can replicate any function realizable by another open architecture of similar size in same sequence of strides, which essentially showed proof that overlapping architecture are just as expressive as known overlapping architectures.",
            "And what we would like to show is that maybe over an architecture are in fact more expressive so."
        ],
        [
            "To show that we have to define more.",
            "I wanted to find some measure of the degree of overlapping in the architecture.",
            "So the way we do that is we look at a layer in the middle of the network layer L and projects receptive field all the way back to the input layer and we did not the size as the total receptive field and we also define the total stride as the distance you move in the input layer.",
            "We take a single step in the feature map.",
            "These two properties essentially capturing some sense the degree of overlap in the architecture."
        ],
        [
            "Now we're ready to represent our main theorem, which shows that almost all functions realizable by an overlapping architecture cannot be replicated by non overlapping architecture and as its size is exponential in the overlapping degree.",
            "So what this mean is that for some architectures we can actually show an exponential separation between the overlapping cases, another another one, essentially proving that overlapping architecture are exponentially efficient with respective novel open ones.",
            "But what's more interesting that if we look at the more common case of alternating BBB convolution four by two by two pulling type networks, then we can show that even for very limited degree of overlapping we already get an exponential separation between the overlapping cases.",
            "An envelope in case.",
            "So."
        ],
        [
            "This point you might be wondering what?",
            "But we're wondering what what will be the case for the standard coordinates?",
            "So we're still working on analyzing the theoretical service, but we do have some experiments with suggest that our result do apply to the standard case as well, so."
        ],
        [
            "We've taken the same type of.",
            "Picture of the network that was used in the last claim of alternating be by big convolution followed by two by two pooling, but using certain covenants with really activations and two by two Max pooling and we train them on cifar.",
            "10 ball vary in the number of channels and the self destructive field which is denoted here baby where we measure the training accuracy because we're interested in the expressive capacity and not about the generalization.",
            "And we've got the.",
            "For each day, for each subject reported that determine accuracy versus the number of channels and on the left figure on the right figure we did, we did the same versus the number of parameters and what we see we see a very striking separation between the non overlapping case which is the black plot here in both sides against all the other networks which are overlapping.",
            "But there's another interesting phenomenon that we've noticed here that once you have some little little degree of overlapping, the doesn't seem to help to increase the amount of overlap in more, we see that almost all, especially on the right figure, which puts all the network versus number of parameters.",
            "We see that all the overlapping networks follow most of the same plot.",
            "And which we think might suggest that it increases often agree on certain points, brings little to no gains, inexpensive efficiency.",
            "Now you do have to notice that it doesn't contradict our results because we've only compared the overlapping case.",
            "This is the non overlapping ones and we have the tools to compare two different type of overlapping architectures.",
            "But we do believe this might be the case.",
            "I also want to point out that last claim we've shown that.",
            "But having only a little bit of little amount of overlapping, you already get the lot separation between overlapping.",
            "Now in case which which, which also manifested in this graph shows that even we have equal two, we only get the large separation.",
            "And so too."
        ],
        [
            "Summarize we've shown that we compare different type of network architectures through the expressive efficiency.",
            "Ann, we've shown that overlapping architecture are efficient with with spectrum nonoverlapping.",
            "Once we've proven for the case of converses and show that it's cold even for the case of small overlapping degree and experiments suggest it also hosts for starting comments as well.",
            "Ann to come back to the question from the beginning of the talk.",
            "We can't, we assume that nobody architecture so uncommon, probably because they lack the efficiency of the overlapping case and conjecture that smaller degree might be all we need in practice."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's just sad.",
                    "label": 0
                },
                {
                    "sent": "The efficiency of the overlapping architectural deep learning and it's a joint work with Alan Shaw.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm going to stop by, so the goal is work is to compare between overlapping and non overlapping architectures.",
                    "label": 1
                },
                {
                    "sent": "Where we define an overlapping architecture is one of the where there exists at least one layer in which the receptive field is larger than astride.",
                    "label": 0
                },
                {
                    "sent": "In this case, if you look at nearby neurons, then the receptive fields have some amount of overlap.",
                    "label": 0
                },
                {
                    "sent": "And we define none of it up in architecture as well, where all of the layers are receptive fields equal to the stride, in which case there are several fields are always done overlapping.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So why do we care about this distinction?",
                    "label": 0
                },
                {
                    "sent": "Is that if you look for theoretical perspective?",
                    "label": 0
                },
                {
                    "sent": "And none of open architecture actually makes some sense.",
                    "label": 0
                },
                {
                    "sent": "So for instance, we can prove that prove that now overlapping architecture universal, which means that for any function given sufficient number of channels and other open architecture can approximate this function.",
                    "label": 0
                },
                {
                    "sent": "An in terms of optimization, there was a paper recently which showed that for certain types of neural networks then of up in case has better convergence.",
                    "label": 0
                },
                {
                    "sent": "Convergence guarantees when overlapping one.",
                    "label": 1
                },
                {
                    "sent": "However, however, when we look at in practice, we see that the only very few cases where people use an actual non overlapping architecture.",
                    "label": 0
                },
                {
                    "sent": "One such rare cases in article on Wavenet where because the user dilation related convolutions then to predict each sample of the network the network is affectively nonoverlapping.",
                    "label": 0
                },
                {
                    "sent": "But beyond this and a few others, multiple don't use completely overlapping architectures, and if we look at modern practice more broadly.",
                    "label": 0
                },
                {
                    "sent": "And we see that.",
                    "label": 0
                },
                {
                    "sent": "Over the years people use every small receptive fields including many non overlapping layers, most famously one by one convolution or two by two pooling but never all the layers.",
                    "label": 1
                },
                {
                    "sent": "So we asked two questions.",
                    "label": 0
                },
                {
                    "sent": "One is why are non overlapping architecture so uncommon and white seems that having just a little bit of overlapping is somewhat sufficient for most tasks.",
                    "label": 1
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The way we're going to approach it.",
                    "label": 0
                },
                {
                    "sent": "We want to address this question by looking at the expressive efficiency.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First define this term broadly, so next slide in the next slide going to find formally so expressive efficiency is simply a way to compare 2 network architecture.",
                    "label": 0
                },
                {
                    "sent": "Which are limited to other small size, so do not buy HA&HB, just spaces of functions that can be represented by a smaller network architecture of type A or B.",
                    "label": 0
                },
                {
                    "sent": "Let me say that a sufficient with respect to be if H District super set of HB, which means that there exists at least one function which Akron present efficiently.",
                    "label": 0
                },
                {
                    "sent": "But we cannot and we also defined at a is completely efficient with respect to V. If HB has zero volume inside HAA, which essentially means that for almost all function the acre proficiently be cannot.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Formally, we say that A is exponentially efficient with respect to be if for all functions realizable by be of size RB, that can be realized by a of size are of size RA, which is its most polynomial in RB, and Additionally that there exists one.",
                    "label": 1
                },
                {
                    "sent": "There exists a function that can be realized by a of size RA which requires B to 12 size RB, which is super polynomial in RA.",
                    "label": 0
                },
                {
                    "sent": "And finally we find that.",
                    "label": 1
                },
                {
                    "sent": "Is completely efficient with respect to V if the second condition holds for almost all functions beside the measure 0 set.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to give you some example this information which was also briefly mentioned in the area talk at this morning is the efficiency of depth.",
                    "label": 0
                },
                {
                    "sent": "So we know from particle experience that deep networks perform much better than shallow ones.",
                    "label": 0
                },
                {
                    "sent": "An example we see in this graph.",
                    "label": 0
                },
                {
                    "sent": "The error rates on emergent access ification as well as well as important as well the number of layers of each model, and we see a strong correlation between number of layers used and the performance achieved by each model.",
                    "label": 0
                },
                {
                    "sent": "So one way to explain this phenomena?",
                    "label": 0
                },
                {
                    "sent": "Is Vice actually proving that deep networks are exponentially efficient with this vector shallow ones?",
                    "label": 1
                },
                {
                    "sent": "An many people are on this problem all the way back from the late 80s on Boolean circuits, as well as more recent papers as the one mentioned earlier today.",
                    "label": 0
                },
                {
                    "sent": "And for the rest of this talk, I want to we want to show a similar relation between overlapping and non overlapping architectures.",
                    "label": 0
                },
                {
                    "sent": "So before I move on.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So briefly mentioned that in our group we work on many ways to study evolution networks including deficiency problem, which I mentioned earlier as well as we have also worked on connectivity patterns and number of channels.",
                    "label": 0
                },
                {
                    "sent": "So if this topic interests you as it should check this out.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So movie on.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To analyze the expensive efficiency, we're going to look at consider a special case of covenants which we call convolutional arithmetic circuits, or convey C for short, and the reason not to consider consider this special case is because we can actually show that they are equivalent to Article 10 of the compositions which open door to using various mathematical tools to analyze, to analyze them.",
                    "label": 1
                },
                {
                    "sent": "And we have also shown that in some cases we even extend this use of these tools to more traditional components, including one with early activations.",
                    "label": 0
                },
                {
                    "sent": "I would also have to mention that these are not just a theoretical framework to analyze the coordinates, but we've also used this networks in practice various into other papers.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the baseline conveys the architecture.",
                    "label": 0
                },
                {
                    "sent": "Looks like this.",
                    "label": 0
                },
                {
                    "sent": "We start with some convolutional layer followed by a pairs of Kanpur layers and with some dense layer and where it differ from regular covenants is that we limit the convolution layers beside the first one to just one by one convolution followed by linear activations instead of the common rally activation and all pooling layers are limited to product pooling instead of average or Max pooling and adjust non overlapping windows.",
                    "label": 0
                },
                {
                    "sent": "This is the network which we analyzing previous work and showed various.",
                    "label": 0
                },
                {
                    "sent": "Georgica results, but its main limitation is that it doesn't support the overlapping architecture, only non overlapping ones in this way.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Visualize this framework to what we call jealous conversion architectures.",
                    "label": 0
                },
                {
                    "sent": "So in general is converses.",
                    "label": 0
                },
                {
                    "sent": "By designing new generous conversion operation, we generalize the one by one convolution with linear activation and the product pooling operation.",
                    "label": 0
                },
                {
                    "sent": "Where we define a pulling, pulling their layer simply by astride larger than one similar, similar to how it was done in the ocean net type architecture.",
                    "label": 0
                },
                {
                    "sent": "And what's special about this type of architecture is when we limited to novel of non overlapping case it reduces back to the same conversation network which we've analyzed before.",
                    "label": 0
                },
                {
                    "sent": "So we can trust for all other tools back to this architecture.",
                    "label": 0
                },
                {
                    "sent": "So with this we are set to analyze the overlapping case so.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First of those is shown that an overlapping architecture can replicate any function realizable by another open architecture of similar size in same sequence of strides, which essentially showed proof that overlapping architecture are just as expressive as known overlapping architectures.",
                    "label": 0
                },
                {
                    "sent": "And what we would like to show is that maybe over an architecture are in fact more expressive so.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To show that we have to define more.",
                    "label": 0
                },
                {
                    "sent": "I wanted to find some measure of the degree of overlapping in the architecture.",
                    "label": 1
                },
                {
                    "sent": "So the way we do that is we look at a layer in the middle of the network layer L and projects receptive field all the way back to the input layer and we did not the size as the total receptive field and we also define the total stride as the distance you move in the input layer.",
                    "label": 1
                },
                {
                    "sent": "We take a single step in the feature map.",
                    "label": 0
                },
                {
                    "sent": "These two properties essentially capturing some sense the degree of overlap in the architecture.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we're ready to represent our main theorem, which shows that almost all functions realizable by an overlapping architecture cannot be replicated by non overlapping architecture and as its size is exponential in the overlapping degree.",
                    "label": 1
                },
                {
                    "sent": "So what this mean is that for some architectures we can actually show an exponential separation between the overlapping cases, another another one, essentially proving that overlapping architecture are exponentially efficient with respective novel open ones.",
                    "label": 0
                },
                {
                    "sent": "But what's more interesting that if we look at the more common case of alternating BBB convolution four by two by two pulling type networks, then we can show that even for very limited degree of overlapping we already get an exponential separation between the overlapping cases.",
                    "label": 0
                },
                {
                    "sent": "An envelope in case.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This point you might be wondering what?",
                    "label": 0
                },
                {
                    "sent": "But we're wondering what what will be the case for the standard coordinates?",
                    "label": 0
                },
                {
                    "sent": "So we're still working on analyzing the theoretical service, but we do have some experiments with suggest that our result do apply to the standard case as well, so.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We've taken the same type of.",
                    "label": 0
                },
                {
                    "sent": "Picture of the network that was used in the last claim of alternating be by big convolution followed by two by two pooling, but using certain covenants with really activations and two by two Max pooling and we train them on cifar.",
                    "label": 0
                },
                {
                    "sent": "10 ball vary in the number of channels and the self destructive field which is denoted here baby where we measure the training accuracy because we're interested in the expressive capacity and not about the generalization.",
                    "label": 0
                },
                {
                    "sent": "And we've got the.",
                    "label": 0
                },
                {
                    "sent": "For each day, for each subject reported that determine accuracy versus the number of channels and on the left figure on the right figure we did, we did the same versus the number of parameters and what we see we see a very striking separation between the non overlapping case which is the black plot here in both sides against all the other networks which are overlapping.",
                    "label": 1
                },
                {
                    "sent": "But there's another interesting phenomenon that we've noticed here that once you have some little little degree of overlapping, the doesn't seem to help to increase the amount of overlap in more, we see that almost all, especially on the right figure, which puts all the network versus number of parameters.",
                    "label": 1
                },
                {
                    "sent": "We see that all the overlapping networks follow most of the same plot.",
                    "label": 1
                },
                {
                    "sent": "And which we think might suggest that it increases often agree on certain points, brings little to no gains, inexpensive efficiency.",
                    "label": 1
                },
                {
                    "sent": "Now you do have to notice that it doesn't contradict our results because we've only compared the overlapping case.",
                    "label": 1
                },
                {
                    "sent": "This is the non overlapping ones and we have the tools to compare two different type of overlapping architectures.",
                    "label": 0
                },
                {
                    "sent": "But we do believe this might be the case.",
                    "label": 0
                },
                {
                    "sent": "I also want to point out that last claim we've shown that.",
                    "label": 0
                },
                {
                    "sent": "But having only a little bit of little amount of overlapping, you already get the lot separation between overlapping.",
                    "label": 0
                },
                {
                    "sent": "Now in case which which, which also manifested in this graph shows that even we have equal two, we only get the large separation.",
                    "label": 0
                },
                {
                    "sent": "And so too.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Summarize we've shown that we compare different type of network architectures through the expressive efficiency.",
                    "label": 0
                },
                {
                    "sent": "Ann, we've shown that overlapping architecture are efficient with with spectrum nonoverlapping.",
                    "label": 0
                },
                {
                    "sent": "Once we've proven for the case of converses and show that it's cold even for the case of small overlapping degree and experiments suggest it also hosts for starting comments as well.",
                    "label": 1
                },
                {
                    "sent": "Ann to come back to the question from the beginning of the talk.",
                    "label": 1
                },
                {
                    "sent": "We can't, we assume that nobody architecture so uncommon, probably because they lack the efficiency of the overlapping case and conjecture that smaller degree might be all we need in practice.",
                    "label": 0
                }
            ]
        }
    }
}