{
    "id": "o3uiirw23iejzvsvmkku7ect6ykzamjx",
    "title": "Reinforcement Learning with Limited Reinforcement: Using Bayes Risk for Active Learning in POMDPs",
    "info": {
        "author": [
            "Finale Doshi, Department of Engineering, University of Cambridge"
        ],
        "published": "Aug. 6, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Bayesian Learning",
            "Top->Computer Science->Machine Learning->Markov Processes"
        ]
    },
    "url": "http://videolectures.net/icml08_doshi_rlw/",
    "segmentation": [
        [
            "Sorry about that.",
            "OK, thank you very much and this is joint work with Joelle, Pineau, Annick Roy."
        ],
        [
            "Great, so here's a particular problem that I'm interested in.",
            "We've got a robotic wheelchair in our lab and we'd like to design an agent for it that can adapt to new environments.",
            "That's going to be robust when it reaches a new situation, and that's also going to be easy and natural for someone to train, and in order to do this well, what are we going to do?",
            "Well, we're going to use some risk sensitive action selection, some particle filtering to keep track of our knowledge of the world, as well.",
            "As you know, if we get completely confused or just going to throw our hands and we're going to ask for help."
        ],
        [
            "But before I go into the details, let me fill in just some key points about the reinforcement learning paradigm that we're going to be that we're going to be using.",
            "So we have an agent an it's going to perform some sort of action that affects the state of the world, and we assume that the agent does not have access to the state of the world, but the world sends back some sort of observation and also sends back some reinforcement signal in our goal is our agent's goal is to maximize.",
            "The reward that it gets, and in order to maximize this reward, it has to trade off between.",
            "Exploration is finding out about how the world behaves, an exploitation which is actually gathering these reinforcement signals.",
            "Now there's some common issues that we're going to run in."
        ],
        [
            "Two in this scenario that are under undesirable when working with something like a robotic wheelchair and the first which is that the agent has to make mistakes in or."
        ],
        [
            "To learn, note that the reinforcement signal is coming in, and that's telling it how good or how bad a certain."
        ],
        [
            "Option was.",
            "And again, if we have a robotic wheelchair, we don't want to drive granny down the stairs to discover that was a bad idea."
        ],
        [
            "What else the other thing is that, aside from policy and model convergence type arguments during the learning process, in many algorithms, the agent is not aware if it's uncertainty in the model, so it may have some silly thoughts and maybe acting on some silly thoughts and not really knowing that it's knowledge is fuzzy."
        ],
        [
            "And finally, in this human robot interaction type scenario, specifying numerical reinforcement may not always be the most natural way of."
        ],
        [
            "Approaching problem.",
            "So what do we do?",
            "We're going to apply first of all, upon DP framework.",
            "That's a partially observable Markov decision process, and this is a Bayesian framework that's going to allow the agent to be aware of its own uncertainty about the knowledge of its world.",
            "The second key element that we introduced is asking meta queries or questions about what questions the agent should be asking, and in this way the agent is going to be able to gather policy information that helps it act robustly.",
            "An actively make decisions to gather more information about the world.",
            "Today's resulting approach is going to combine Bayesian, an inverse reinforcement learning, and we're going to learn the entire model.",
            "That's the transitions, the observations and the rewards simultaneously and robustly."
        ],
        [
            "But before I go into the details of how we actually do this, let me start out with just the planning process for standard Palm DP framework.",
            "So in this scenario I've just blown up the picture of the agent and the action that we started out with, and here the world sends in an observation.",
            "We know how the world behaves and so between the observation and how the world behaves, we can put together a probability distribution over what states we may be in.",
            "We don't know the actual state, but we can put a distribution and we call this a belief.",
            "Now, given the belief, the dynamics of the world and what we're trying to achieve, we can decide what action to take next, and that's the planning step."
        ],
        [
            "One small problem we generally don't know the world dynamics and we need to discover those.",
            "So what can we do?",
            "Well, one option is to just estimate the world dynamics.",
            "Just use a point estimate and this is pretty fast and can be done, but it's not necessarily robust."
        ],
        [
            "So perhaps a more principled approach would be to treat the world dynamics itself is additional hidden state, just as we don't know the state of the world, and we keep a belief, now we don't know the true world dynamics.",
            "We may also not be clear on the objective, so we keep a belief over these elements as well, and treat this large thing now as one massive palm D."
        ],
        [
            "He so now we've just described A2 ends of the spectrum.",
            "We can ignore the uncertainty and it's going to be fast, but not necessarily robust, and we can plan with the parameters as additional hidden state, and this is in some ways optimal, but it's slow at best and intractable at worst."
        ],
        [
            "What we propose now is an approximate planning scheme using Bayes risk.",
            "In these policy queries, that's going to allow us to be fast, and while not optimal, is still going to be robust."
        ],
        [
            "And so we're going to call this thing the model uncertainty, Pompey.",
            "And now I'm going to describe how we're going to actually behave in this environment.",
            "So as I said, there's two key steps.",
            "Planning and belief update.",
            "So I'll begin with plan."
        ],
        [
            "So planning involves selecting an action and the way we're going to select an action is we're going to choose the one that's the safest."
        ],
        [
            "Or the least risky.",
            "In order to do this, the Q function here is the value of taking action A in belief B under model M. So this right here represents the loss that we experience if we take action A instead of the optimal action according to the model.",
            "So this is some negative number.",
            "This is the loss, and now we're taking the expectation of that loss over all possible models, where model again as a reward that transitions, and the observation.",
            "So we take this expectation and then we take the action that has the least expected loss.",
            "That's the least risky action that we can take.",
            "Unfortunately, as I just said, this model space is huge.",
            "We can't actually perform this integral.",
            "So what we do is we take we approximate this integral with the sum instead.",
            "So here we have a sample of Palm DP's each given a certain weight.",
            "For now, I'll just say that we can approximate.",
            "We can bound the approximation error trying to keep this pretty high level, but the details are in the paper.",
            "We can talk about them."
        ],
        [
            "Afterwards.",
            "The larger question though, are the is well, what if the expected loss is still too big?",
            "I mean if there's an action, even if the safest action is kind of risky, we may not want to take that risk."
        ],
        [
            "So let's add another step, so use Bayes risk to find the safest action, and if the safest action is still too risky, which is where we define what is too risky based on our threshold, then we're going to ask for help."
        ],
        [
            "And we're going to ask for help by asking for a policy for policy information, and we like this because first of all, we don't need to take that large risk to discover the consequences of a poor decision.",
            "And it's also nice because the user needs to provide reinforcement signal only when the agent is confused.",
            "So it's active learning, and in certain scenarios, policy information may be more natural to provide than actual numerical feedback.",
            "Another light, nice little side benefit is that.",
            "This allows us to provide performance bounds throughout the learning process, not just after some training phase."
        ],
        [
            "In terms of implementation, well, if we have some sort of simulator, our simulator can tell us you know what the policy should be at any particular point in time when it comes to working with people.",
            "This is a sort of stuff we've been experimenting with, and I'll say that this bit is still a work in progress, but we've been asking questions of the form I think you might want to go to the printer.",
            "Should I go to the printer or I'm certain you want to go to the printer?",
            "Or instead, should I ask, you should have asked to confirm your location.",
            "Again.",
            "This is a robotic wheelchair that's driving people around, and we're assuming that by asking these series of questions we can actually determine what the optimal action is, or at least for the most part we were able to extract the optimal action from people."
        ],
        [
            "Great, so I've just summarized how we do the planning step and now let's move on to the belief update.",
            "So when it comes to incorporating updating our belief over possible models, I'm not going to talk about updating the state because that's pretty standard.",
            "But over models we have two sources of information.",
            "We have this history of actions and observations that we've just observed, and we also have a set Q.",
            "This is a set of questions that we've asked, and what our policy information has told us."
        ],
        [
            "We need to incorporate both of these sorts of information query and history in order to get a posterior over models now.",
            "So here let's say this is just a cartoon, but let's say we start out with some posterior over models."
        ],
        [
            "Well, it turns out that using an M and actually incorporate the history information in closed form, so that's great.",
            "We've come up with a."
        ],
        [
            "Interracially distribution.",
            "But the trouble is, once we start incorporating this query information either in model is consistent with the response to the query or the model is not.",
            "For example, consider the robotic wheelchair scenario.",
            "Suppose I hear as the robot take me to the coffee machine and I said and then, but I'm still kind of confused.",
            "So I ask the user should I go to the coffee machine and the user says no, no, you were supposed to go to the copy machine.",
            "Then I have learned that something was wrong in my observation model.",
            "I basically learned that I tend to mistake coffee and copy together.",
            "So this model was inconsistent and I should truncate it out.",
            "Now.",
            "The issue here now is that we no longer have a closed form for the posterior, so we're going."
        ],
        [
            "So this makes it tough and we're going to approximate it using."
        ],
        [
            "Particle filtering.",
            "So how does our particle filter work?",
            "Again?",
            "I'm going to describe this at a high level, but all the transition kernels in the math are all in the paper.",
            "During a trial, we don't want to spend a lot of time re sampling palm DPS because while we've developed some very efficient methods for solving these, it still takes a lot of time.",
            "So we have some set of samples and then we're going to re weight them based on the query information that we received and I just said in the previous slide that really query information is 01.",
            "Either models consistent or it's not consistent.",
            "But to account for approximations in the solver and also the fact that someone might give us an incorrect answer, we actually treat each query as a Bernoulli trial and assume that there's some probability of error in terms of the response that's given."
        ],
        [
            "So things get more interesting between trials, where we assume that we have a little bit more time to update our beliefs.",
            "So the first step is to re sample the palm.",
            "DP's basically filtering and then what we do is we perturb these models based on the probability of the model given the history and remember that we can compute this bit this underlying distribution in closed form, so we're trying to just nudge the models in directions where they are more likely to be according to the history information and then we re weight them.",
            "Based on the query information, and that's how we update our filter."
        ],
        [
            "Now if we use this procedure, we can provide some performance guarantees in expectation.",
            "So if V Prime is the value of our policy, we can say that that's going to be bigger than some fraction times the optimal policy, and here we're subtracting out.",
            "Let's see the cost of asking a question.",
            "So the optimal policy minus some cost of asking questions something relatively small.",
            "And the possibility that we totally goof and we end up in this abysmal bad state forever.",
            "That gives us minimal reward and this trade off parameter between these two elements depends on the discount factor of the palm DP and more importantly, how well we can approximate the Bayes risk.",
            "Because remember, we had replaced that integral with some a couple of slides before.",
            "We can also say that we will eventually converge to a transition."
        ],
        [
            "Enter award model.",
            "Let's"
        ],
        [
            "I wanted some results.",
            "So we tried this on first of all on several standard palm DP problems with three three types of learners.",
            "We have our control which started out with a sample of beliefs.",
            "Use the Bayes risk for action selection but never updated its posterior.",
            "We have a passive learner who incorporated history information but nothing else.",
            "And then we had our active learner which incorporated history information and also ask questions and I will say this test is not completely fair because I mean without asking questions you really can't get reward information out of this out of this system.",
            "But you'll see a more fair test in the next slide, but what I do want to point out here is it well as kind of expected.",
            "The passive learner and non learner can't really get anywhere and the active learner their performance starts out well and it stays constant.",
            "And what I want to point out is that the performance stays constant, but what's going on inside is initially it's asking a lot of questions and later on it's asking fewer questions because it's learn."
        ],
        [
            "About the model.",
            "So we also tried this out on a simulated dialogue task and here we tried it with two different priors.",
            "So here's a prior that that was fairly informative.",
            "Remember, I mean this is a Bayesian method, so we could start our priors off pretty much anywhere.",
            "And all of the all of the agents do relatively well, even though the active learners is up on top.",
            "There's a fair amount of noise.",
            "Now notice that the scale changes a lot on these axis it when we go to the noninformative prior.",
            "So this guy still stays relatively the same up here and we did give the passive learner or the prior some information about.",
            "OK, you're trying to achieve.",
            "You're trying to take the user to the right location so it did have some direction and what it was trying to do.",
            "But the non learners still can't really do very well and the passive learner can only do so much.",
            "Based on history information alone, to tune its parameters."
        ],
        [
            "Finally, here's some results from a sample dialogue, and we have again a longer dialogue.",
            "If you're interested in seeing it.",
            "But this is just for a quick illustration where the user says something like give me the forecast so it with this robotic wheelchair could also ask.",
            "You could also ask it for more general questions.",
            "We had a series of things that it could do.",
            "The robot says I'm confused.",
            "What should I do now when the user indicated that the robot should provide the weather forecast in this situation?",
            "So the robot provides the weather forecast?",
            "In a later conversation, the user asks what's the forecast and the robot asks, would you like the weather so it's not completely sure yet that the word forecast is associated with weather because it's asking a confirmation question first, but it started to link these two words together and when the user confirms then the robot sees the weather for the day."
        ],
        [
            "So in conclusion, although Palm DP's are incredibly high dimensional and have many parameters in them, using the Bayes risk for action selection turns out to be a way to make planning tractable and robust at the same time.",
            "And also the learning process can be further improved if we use these policy queries, which is a different way of looking at the problem than other forms of articles in terms of extensions.",
            "Well, by far the most expensive part of this algorithm is having to sample these palm DP's and so we're currently quite keen on developing methods to produce a more reasonable set of samples instead of having to tune these transition kernels.",
            "And very specifically.",
            "And finally, since we are interested in human robot interaction, one of the key challenges is how do you extract information from people who tend to be extremely variable."
        ],
        [
            "So thank you very much and do you have any questions?",
            "Question.",
            "The impact of the quality of response in Cuba.",
            "You should be very high right?",
            "So we set our error threshold to .2 in that Bernoulli trial and it seemed to roughly work.",
            "But we means a bit and we explained that this is what the system is trying to do, and if we didn't coach them, it was pretty horrendous.",
            "But once we gave them information, they actually did fairly well.",
            "Love, you might expect the first nomenclature that confused himself.",
            "Yes, yes, so there's definitely tuning that error parameter to make things commercial.",
            "So at the airport.",
            "3."
        ],
        [
            "Is that this covers than you might.",
            "Avoid stated that you would really need to go to and maybe ask for help, which is called collect information there.",
            "And so how does this work?",
            "Is there about that?",
            "How will you not?",
            "Well, the thing?",
            "Risky to use it.",
            "To achieve good path, so you won't actually go there, but what you will do is if you.",
            "If you think you might want to go there should like then you then you ask for policy information and your policy information says you should have gone there.",
            "That already tells you how good that State was an that allows you to cut off a bit of the model space.",
            "And if you have the thing says you shouldn't have gone there well then you're good because you didn't actually go there.",
            "So the policy information allows you to cutoff bids of the reward space.",
            "As well as observation and transition space.",
            "Have basically happened, yeah.",
            "How do you associate the word order forecast with world weather?",
            "In this scenario, So what happens is that there are certain models, but like if we have a sample over over all possible models, we don't have all possible models, but over many models there are some models that say, well, the word forecast should be associated with weather, and there's some models that say, well the word forecast.",
            "I don't know is associated with coffee.",
            "So when the user told you give me the forecast and then they later said provide the weather then the models that said that weather was associated with forecast was associate weather get end up having high weight.",
            "And the ones that say it was associated with coffee or something end up having low weight, and that's how you do your learning.",
            "Actually ask 2 questions.",
            "Do you want the weather or do you want a coffee well so so by getting the the policy information that allows us to prune away all the models that are inconsistent with a particular case.",
            "So I wanted to ask about scaling that because these examples that use here are still pretty small public keys.",
            "Yes.",
            "More models which will be more costly, so have you looked at that right?",
            "So So what we've been doing for for now is it we type parameters to make the effective size of the palm DP smaller, and in other ways exploit symmetry.",
            "For example, in the dialogue task, even if you don't tie all the parameters, there's away some ways you can quickly solve Palm DP's even though they have lots of parameters, but the dimensionality is definitely the biggest challenge, and that's what we're currently working on."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sorry about that.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you very much and this is joint work with Joelle, Pineau, Annick Roy.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Great, so here's a particular problem that I'm interested in.",
                    "label": 0
                },
                {
                    "sent": "We've got a robotic wheelchair in our lab and we'd like to design an agent for it that can adapt to new environments.",
                    "label": 1
                },
                {
                    "sent": "That's going to be robust when it reaches a new situation, and that's also going to be easy and natural for someone to train, and in order to do this well, what are we going to do?",
                    "label": 1
                },
                {
                    "sent": "Well, we're going to use some risk sensitive action selection, some particle filtering to keep track of our knowledge of the world, as well.",
                    "label": 0
                },
                {
                    "sent": "As you know, if we get completely confused or just going to throw our hands and we're going to ask for help.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But before I go into the details, let me fill in just some key points about the reinforcement learning paradigm that we're going to be that we're going to be using.",
                    "label": 1
                },
                {
                    "sent": "So we have an agent an it's going to perform some sort of action that affects the state of the world, and we assume that the agent does not have access to the state of the world, but the world sends back some sort of observation and also sends back some reinforcement signal in our goal is our agent's goal is to maximize.",
                    "label": 0
                },
                {
                    "sent": "The reward that it gets, and in order to maximize this reward, it has to trade off between.",
                    "label": 1
                },
                {
                    "sent": "Exploration is finding out about how the world behaves, an exploitation which is actually gathering these reinforcement signals.",
                    "label": 0
                },
                {
                    "sent": "Now there's some common issues that we're going to run in.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two in this scenario that are under undesirable when working with something like a robotic wheelchair and the first which is that the agent has to make mistakes in or.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To learn, note that the reinforcement signal is coming in, and that's telling it how good or how bad a certain.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Option was.",
                    "label": 0
                },
                {
                    "sent": "And again, if we have a robotic wheelchair, we don't want to drive granny down the stairs to discover that was a bad idea.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What else the other thing is that, aside from policy and model convergence type arguments during the learning process, in many algorithms, the agent is not aware if it's uncertainty in the model, so it may have some silly thoughts and maybe acting on some silly thoughts and not really knowing that it's knowledge is fuzzy.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finally, in this human robot interaction type scenario, specifying numerical reinforcement may not always be the most natural way of.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Approaching problem.",
                    "label": 0
                },
                {
                    "sent": "So what do we do?",
                    "label": 0
                },
                {
                    "sent": "We're going to apply first of all, upon DP framework.",
                    "label": 0
                },
                {
                    "sent": "That's a partially observable Markov decision process, and this is a Bayesian framework that's going to allow the agent to be aware of its own uncertainty about the knowledge of its world.",
                    "label": 1
                },
                {
                    "sent": "The second key element that we introduced is asking meta queries or questions about what questions the agent should be asking, and in this way the agent is going to be able to gather policy information that helps it act robustly.",
                    "label": 0
                },
                {
                    "sent": "An actively make decisions to gather more information about the world.",
                    "label": 1
                },
                {
                    "sent": "Today's resulting approach is going to combine Bayesian, an inverse reinforcement learning, and we're going to learn the entire model.",
                    "label": 0
                },
                {
                    "sent": "That's the transitions, the observations and the rewards simultaneously and robustly.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But before I go into the details of how we actually do this, let me start out with just the planning process for standard Palm DP framework.",
                    "label": 1
                },
                {
                    "sent": "So in this scenario I've just blown up the picture of the agent and the action that we started out with, and here the world sends in an observation.",
                    "label": 0
                },
                {
                    "sent": "We know how the world behaves and so between the observation and how the world behaves, we can put together a probability distribution over what states we may be in.",
                    "label": 0
                },
                {
                    "sent": "We don't know the actual state, but we can put a distribution and we call this a belief.",
                    "label": 0
                },
                {
                    "sent": "Now, given the belief, the dynamics of the world and what we're trying to achieve, we can decide what action to take next, and that's the planning step.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One small problem we generally don't know the world dynamics and we need to discover those.",
                    "label": 1
                },
                {
                    "sent": "So what can we do?",
                    "label": 0
                },
                {
                    "sent": "Well, one option is to just estimate the world dynamics.",
                    "label": 1
                },
                {
                    "sent": "Just use a point estimate and this is pretty fast and can be done, but it's not necessarily robust.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So perhaps a more principled approach would be to treat the world dynamics itself is additional hidden state, just as we don't know the state of the world, and we keep a belief, now we don't know the true world dynamics.",
                    "label": 0
                },
                {
                    "sent": "We may also not be clear on the objective, so we keep a belief over these elements as well, and treat this large thing now as one massive palm D.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "He so now we've just described A2 ends of the spectrum.",
                    "label": 0
                },
                {
                    "sent": "We can ignore the uncertainty and it's going to be fast, but not necessarily robust, and we can plan with the parameters as additional hidden state, and this is in some ways optimal, but it's slow at best and intractable at worst.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we propose now is an approximate planning scheme using Bayes risk.",
                    "label": 0
                },
                {
                    "sent": "In these policy queries, that's going to allow us to be fast, and while not optimal, is still going to be robust.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so we're going to call this thing the model uncertainty, Pompey.",
                    "label": 0
                },
                {
                    "sent": "And now I'm going to describe how we're going to actually behave in this environment.",
                    "label": 0
                },
                {
                    "sent": "So as I said, there's two key steps.",
                    "label": 0
                },
                {
                    "sent": "Planning and belief update.",
                    "label": 0
                },
                {
                    "sent": "So I'll begin with plan.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So planning involves selecting an action and the way we're going to select an action is we're going to choose the one that's the safest.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or the least risky.",
                    "label": 0
                },
                {
                    "sent": "In order to do this, the Q function here is the value of taking action A in belief B under model M. So this right here represents the loss that we experience if we take action A instead of the optimal action according to the model.",
                    "label": 0
                },
                {
                    "sent": "So this is some negative number.",
                    "label": 0
                },
                {
                    "sent": "This is the loss, and now we're taking the expectation of that loss over all possible models, where model again as a reward that transitions, and the observation.",
                    "label": 0
                },
                {
                    "sent": "So we take this expectation and then we take the action that has the least expected loss.",
                    "label": 0
                },
                {
                    "sent": "That's the least risky action that we can take.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, as I just said, this model space is huge.",
                    "label": 0
                },
                {
                    "sent": "We can't actually perform this integral.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we take we approximate this integral with the sum instead.",
                    "label": 0
                },
                {
                    "sent": "So here we have a sample of Palm DP's each given a certain weight.",
                    "label": 0
                },
                {
                    "sent": "For now, I'll just say that we can approximate.",
                    "label": 0
                },
                {
                    "sent": "We can bound the approximation error trying to keep this pretty high level, but the details are in the paper.",
                    "label": 1
                },
                {
                    "sent": "We can talk about them.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Afterwards.",
                    "label": 0
                },
                {
                    "sent": "The larger question though, are the is well, what if the expected loss is still too big?",
                    "label": 1
                },
                {
                    "sent": "I mean if there's an action, even if the safest action is kind of risky, we may not want to take that risk.",
                    "label": 1
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's add another step, so use Bayes risk to find the safest action, and if the safest action is still too risky, which is where we define what is too risky based on our threshold, then we're going to ask for help.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we're going to ask for help by asking for a policy for policy information, and we like this because first of all, we don't need to take that large risk to discover the consequences of a poor decision.",
                    "label": 1
                },
                {
                    "sent": "And it's also nice because the user needs to provide reinforcement signal only when the agent is confused.",
                    "label": 1
                },
                {
                    "sent": "So it's active learning, and in certain scenarios, policy information may be more natural to provide than actual numerical feedback.",
                    "label": 0
                },
                {
                    "sent": "Another light, nice little side benefit is that.",
                    "label": 0
                },
                {
                    "sent": "This allows us to provide performance bounds throughout the learning process, not just after some training phase.",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In terms of implementation, well, if we have some sort of simulator, our simulator can tell us you know what the policy should be at any particular point in time when it comes to working with people.",
                    "label": 0
                },
                {
                    "sent": "This is a sort of stuff we've been experimenting with, and I'll say that this bit is still a work in progress, but we've been asking questions of the form I think you might want to go to the printer.",
                    "label": 1
                },
                {
                    "sent": "Should I go to the printer or I'm certain you want to go to the printer?",
                    "label": 1
                },
                {
                    "sent": "Or instead, should I ask, you should have asked to confirm your location.",
                    "label": 0
                },
                {
                    "sent": "Again.",
                    "label": 0
                },
                {
                    "sent": "This is a robotic wheelchair that's driving people around, and we're assuming that by asking these series of questions we can actually determine what the optimal action is, or at least for the most part we were able to extract the optimal action from people.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Great, so I've just summarized how we do the planning step and now let's move on to the belief update.",
                    "label": 0
                },
                {
                    "sent": "So when it comes to incorporating updating our belief over possible models, I'm not going to talk about updating the state because that's pretty standard.",
                    "label": 0
                },
                {
                    "sent": "But over models we have two sources of information.",
                    "label": 1
                },
                {
                    "sent": "We have this history of actions and observations that we've just observed, and we also have a set Q.",
                    "label": 1
                },
                {
                    "sent": "This is a set of questions that we've asked, and what our policy information has told us.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We need to incorporate both of these sorts of information query and history in order to get a posterior over models now.",
                    "label": 0
                },
                {
                    "sent": "So here let's say this is just a cartoon, but let's say we start out with some posterior over models.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, it turns out that using an M and actually incorporate the history information in closed form, so that's great.",
                    "label": 0
                },
                {
                    "sent": "We've come up with a.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Interracially distribution.",
                    "label": 0
                },
                {
                    "sent": "But the trouble is, once we start incorporating this query information either in model is consistent with the response to the query or the model is not.",
                    "label": 1
                },
                {
                    "sent": "For example, consider the robotic wheelchair scenario.",
                    "label": 0
                },
                {
                    "sent": "Suppose I hear as the robot take me to the coffee machine and I said and then, but I'm still kind of confused.",
                    "label": 0
                },
                {
                    "sent": "So I ask the user should I go to the coffee machine and the user says no, no, you were supposed to go to the copy machine.",
                    "label": 0
                },
                {
                    "sent": "Then I have learned that something was wrong in my observation model.",
                    "label": 0
                },
                {
                    "sent": "I basically learned that I tend to mistake coffee and copy together.",
                    "label": 0
                },
                {
                    "sent": "So this model was inconsistent and I should truncate it out.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "The issue here now is that we no longer have a closed form for the posterior, so we're going.",
                    "label": 1
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this makes it tough and we're going to approximate it using.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Particle filtering.",
                    "label": 0
                },
                {
                    "sent": "So how does our particle filter work?",
                    "label": 0
                },
                {
                    "sent": "Again?",
                    "label": 0
                },
                {
                    "sent": "I'm going to describe this at a high level, but all the transition kernels in the math are all in the paper.",
                    "label": 0
                },
                {
                    "sent": "During a trial, we don't want to spend a lot of time re sampling palm DPS because while we've developed some very efficient methods for solving these, it still takes a lot of time.",
                    "label": 1
                },
                {
                    "sent": "So we have some set of samples and then we're going to re weight them based on the query information that we received and I just said in the previous slide that really query information is 01.",
                    "label": 0
                },
                {
                    "sent": "Either models consistent or it's not consistent.",
                    "label": 0
                },
                {
                    "sent": "But to account for approximations in the solver and also the fact that someone might give us an incorrect answer, we actually treat each query as a Bernoulli trial and assume that there's some probability of error in terms of the response that's given.",
                    "label": 1
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So things get more interesting between trials, where we assume that we have a little bit more time to update our beliefs.",
                    "label": 0
                },
                {
                    "sent": "So the first step is to re sample the palm.",
                    "label": 0
                },
                {
                    "sent": "DP's basically filtering and then what we do is we perturb these models based on the probability of the model given the history and remember that we can compute this bit this underlying distribution in closed form, so we're trying to just nudge the models in directions where they are more likely to be according to the history information and then we re weight them.",
                    "label": 0
                },
                {
                    "sent": "Based on the query information, and that's how we update our filter.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now if we use this procedure, we can provide some performance guarantees in expectation.",
                    "label": 1
                },
                {
                    "sent": "So if V Prime is the value of our policy, we can say that that's going to be bigger than some fraction times the optimal policy, and here we're subtracting out.",
                    "label": 0
                },
                {
                    "sent": "Let's see the cost of asking a question.",
                    "label": 1
                },
                {
                    "sent": "So the optimal policy minus some cost of asking questions something relatively small.",
                    "label": 0
                },
                {
                    "sent": "And the possibility that we totally goof and we end up in this abysmal bad state forever.",
                    "label": 0
                },
                {
                    "sent": "That gives us minimal reward and this trade off parameter between these two elements depends on the discount factor of the palm DP and more importantly, how well we can approximate the Bayes risk.",
                    "label": 0
                },
                {
                    "sent": "Because remember, we had replaced that integral with some a couple of slides before.",
                    "label": 0
                },
                {
                    "sent": "We can also say that we will eventually converge to a transition.",
                    "label": 1
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Enter award model.",
                    "label": 0
                },
                {
                    "sent": "Let's",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I wanted some results.",
                    "label": 0
                },
                {
                    "sent": "So we tried this on first of all on several standard palm DP problems with three three types of learners.",
                    "label": 0
                },
                {
                    "sent": "We have our control which started out with a sample of beliefs.",
                    "label": 0
                },
                {
                    "sent": "Use the Bayes risk for action selection but never updated its posterior.",
                    "label": 0
                },
                {
                    "sent": "We have a passive learner who incorporated history information but nothing else.",
                    "label": 0
                },
                {
                    "sent": "And then we had our active learner which incorporated history information and also ask questions and I will say this test is not completely fair because I mean without asking questions you really can't get reward information out of this out of this system.",
                    "label": 0
                },
                {
                    "sent": "But you'll see a more fair test in the next slide, but what I do want to point out here is it well as kind of expected.",
                    "label": 0
                },
                {
                    "sent": "The passive learner and non learner can't really get anywhere and the active learner their performance starts out well and it stays constant.",
                    "label": 1
                },
                {
                    "sent": "And what I want to point out is that the performance stays constant, but what's going on inside is initially it's asking a lot of questions and later on it's asking fewer questions because it's learn.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "About the model.",
                    "label": 0
                },
                {
                    "sent": "So we also tried this out on a simulated dialogue task and here we tried it with two different priors.",
                    "label": 0
                },
                {
                    "sent": "So here's a prior that that was fairly informative.",
                    "label": 0
                },
                {
                    "sent": "Remember, I mean this is a Bayesian method, so we could start our priors off pretty much anywhere.",
                    "label": 0
                },
                {
                    "sent": "And all of the all of the agents do relatively well, even though the active learners is up on top.",
                    "label": 1
                },
                {
                    "sent": "There's a fair amount of noise.",
                    "label": 0
                },
                {
                    "sent": "Now notice that the scale changes a lot on these axis it when we go to the noninformative prior.",
                    "label": 1
                },
                {
                    "sent": "So this guy still stays relatively the same up here and we did give the passive learner or the prior some information about.",
                    "label": 0
                },
                {
                    "sent": "OK, you're trying to achieve.",
                    "label": 0
                },
                {
                    "sent": "You're trying to take the user to the right location so it did have some direction and what it was trying to do.",
                    "label": 0
                },
                {
                    "sent": "But the non learners still can't really do very well and the passive learner can only do so much.",
                    "label": 0
                },
                {
                    "sent": "Based on history information alone, to tune its parameters.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Finally, here's some results from a sample dialogue, and we have again a longer dialogue.",
                    "label": 0
                },
                {
                    "sent": "If you're interested in seeing it.",
                    "label": 0
                },
                {
                    "sent": "But this is just for a quick illustration where the user says something like give me the forecast so it with this robotic wheelchair could also ask.",
                    "label": 0
                },
                {
                    "sent": "You could also ask it for more general questions.",
                    "label": 0
                },
                {
                    "sent": "We had a series of things that it could do.",
                    "label": 0
                },
                {
                    "sent": "The robot says I'm confused.",
                    "label": 0
                },
                {
                    "sent": "What should I do now when the user indicated that the robot should provide the weather forecast in this situation?",
                    "label": 1
                },
                {
                    "sent": "So the robot provides the weather forecast?",
                    "label": 0
                },
                {
                    "sent": "In a later conversation, the user asks what's the forecast and the robot asks, would you like the weather so it's not completely sure yet that the word forecast is associated with weather because it's asking a confirmation question first, but it started to link these two words together and when the user confirms then the robot sees the weather for the day.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in conclusion, although Palm DP's are incredibly high dimensional and have many parameters in them, using the Bayes risk for action selection turns out to be a way to make planning tractable and robust at the same time.",
                    "label": 1
                },
                {
                    "sent": "And also the learning process can be further improved if we use these policy queries, which is a different way of looking at the problem than other forms of articles in terms of extensions.",
                    "label": 1
                },
                {
                    "sent": "Well, by far the most expensive part of this algorithm is having to sample these palm DP's and so we're currently quite keen on developing methods to produce a more reasonable set of samples instead of having to tune these transition kernels.",
                    "label": 0
                },
                {
                    "sent": "And very specifically.",
                    "label": 0
                },
                {
                    "sent": "And finally, since we are interested in human robot interaction, one of the key challenges is how do you extract information from people who tend to be extremely variable.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So thank you very much and do you have any questions?",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "The impact of the quality of response in Cuba.",
                    "label": 0
                },
                {
                    "sent": "You should be very high right?",
                    "label": 0
                },
                {
                    "sent": "So we set our error threshold to .2 in that Bernoulli trial and it seemed to roughly work.",
                    "label": 0
                },
                {
                    "sent": "But we means a bit and we explained that this is what the system is trying to do, and if we didn't coach them, it was pretty horrendous.",
                    "label": 0
                },
                {
                    "sent": "But once we gave them information, they actually did fairly well.",
                    "label": 0
                },
                {
                    "sent": "Love, you might expect the first nomenclature that confused himself.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes, so there's definitely tuning that error parameter to make things commercial.",
                    "label": 0
                },
                {
                    "sent": "So at the airport.",
                    "label": 0
                },
                {
                    "sent": "3.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that this covers than you might.",
                    "label": 0
                },
                {
                    "sent": "Avoid stated that you would really need to go to and maybe ask for help, which is called collect information there.",
                    "label": 0
                },
                {
                    "sent": "And so how does this work?",
                    "label": 0
                },
                {
                    "sent": "Is there about that?",
                    "label": 0
                },
                {
                    "sent": "How will you not?",
                    "label": 0
                },
                {
                    "sent": "Well, the thing?",
                    "label": 0
                },
                {
                    "sent": "Risky to use it.",
                    "label": 0
                },
                {
                    "sent": "To achieve good path, so you won't actually go there, but what you will do is if you.",
                    "label": 0
                },
                {
                    "sent": "If you think you might want to go there should like then you then you ask for policy information and your policy information says you should have gone there.",
                    "label": 0
                },
                {
                    "sent": "That already tells you how good that State was an that allows you to cut off a bit of the model space.",
                    "label": 0
                },
                {
                    "sent": "And if you have the thing says you shouldn't have gone there well then you're good because you didn't actually go there.",
                    "label": 0
                },
                {
                    "sent": "So the policy information allows you to cutoff bids of the reward space.",
                    "label": 0
                },
                {
                    "sent": "As well as observation and transition space.",
                    "label": 0
                },
                {
                    "sent": "Have basically happened, yeah.",
                    "label": 0
                },
                {
                    "sent": "How do you associate the word order forecast with world weather?",
                    "label": 0
                },
                {
                    "sent": "In this scenario, So what happens is that there are certain models, but like if we have a sample over over all possible models, we don't have all possible models, but over many models there are some models that say, well, the word forecast should be associated with weather, and there's some models that say, well the word forecast.",
                    "label": 0
                },
                {
                    "sent": "I don't know is associated with coffee.",
                    "label": 0
                },
                {
                    "sent": "So when the user told you give me the forecast and then they later said provide the weather then the models that said that weather was associated with forecast was associate weather get end up having high weight.",
                    "label": 0
                },
                {
                    "sent": "And the ones that say it was associated with coffee or something end up having low weight, and that's how you do your learning.",
                    "label": 0
                },
                {
                    "sent": "Actually ask 2 questions.",
                    "label": 0
                },
                {
                    "sent": "Do you want the weather or do you want a coffee well so so by getting the the policy information that allows us to prune away all the models that are inconsistent with a particular case.",
                    "label": 0
                },
                {
                    "sent": "So I wanted to ask about scaling that because these examples that use here are still pretty small public keys.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "More models which will be more costly, so have you looked at that right?",
                    "label": 0
                },
                {
                    "sent": "So So what we've been doing for for now is it we type parameters to make the effective size of the palm DP smaller, and in other ways exploit symmetry.",
                    "label": 0
                },
                {
                    "sent": "For example, in the dialogue task, even if you don't tie all the parameters, there's away some ways you can quickly solve Palm DP's even though they have lots of parameters, but the dimensionality is definitely the biggest challenge, and that's what we're currently working on.",
                    "label": 0
                }
            ]
        }
    }
}