{
    "id": "kstidmtvlfoz2tjkssnn7rw3f2lwmxla",
    "title": "Sample Complexity Bounds for Differentially Private Learning",
    "info": {
        "author": [
            "Daniel Hsu, Microsoft Research New England, Microsoft Research"
        ],
        "published": "Aug. 2, 2011",
        "recorded": "July 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2011_hsu_learning/",
    "segmentation": [
        [
            "OK, so here's the first part."
        ],
        [
            "Let me give it some motivation for the problem we were looking at.",
            "So in a lot of machine learning application, the data that the training data that we have actually is composed of individuals, sensitive information and So what I mean here.",
            "So in some examples like an E Commerce, the data that you have maybe customers browsing or purchase histories in clinical studies you might be looking at medical records of patients and their various test results.",
            "And in genomic studies you have basically individuals genetic sequences like their DNA or protein sequences.",
            "And so, in all of these applications, what you'd like to do is somehow learn something about the population from these data about individuals."
        ],
        [
            "So in this work will look at the particular problem of learning a binary classifier from labeled examples, and the thing that keep keep in mind here is that the training each training example is basically an individual sensitive information, and this is something that they like to keep private."
        ],
        [
            "OK, so the first question you might want to ask is like if there even a problem that we have to address here so we have some sensitive training data.",
            "It gets fed into a learning algorithm.",
            "This algorithm produces a classifier and this classifier and then is released to the public so it becomes public knowledge.",
            "So is there some sort of problem that we have to address and the question is, you know, if we release this classifier to the public, if it gets deployed, does this somehow violate the privacy of the individuals who are in the training training data?"
        ],
        [
            "And here."
        ],
        [
            "Answer is yes and remains true even after simple anonymization procedures or even when you're just releasing aggregate statistics.",
            "And the reason here is because an adversary who's trying to violate or compromise someone's privacy, he might have access to some side information about the individual."
        ],
        [
            "OK, so here's an example where it seems like this has happened.",
            "This was the recent work by winning at all from Indiana.",
            "And what they were able to do was look at some genome wide Association studies which published some correlation statistics about individuals and their genetic markers, and they were able to combine this with some site information to determine whether you know individual in the study was in a particular group.",
            "So the group of people who had a particular disease, or if they didn't have the disease, that they were in the healthy group, and that bit of information with something that these individuals expected to keep private.",
            "But you know the adversary who had some side information, was able to compromise this privacy."
        ],
        [
            "OK, so it seems like there's you know we have to do a little bit more than just standard machine learning.",
            "Just combining statistics in order to achieve both the goal of learning an accurate classifier while also preserving the privacy of the data.",
            "OK, so in this work will look at the specific question of sample complexity, that is, how many labeled examples do we need to achieve both of these goals simultaneously?"
        ],
        [
            "OK, So what do I mean by privacy?",
            "So you could think of possibly think of a lot of different notions of privacy and you know some might be very strong notion, so might be very weak.",
            "But you know there was one notion of privacy that's been popular in recent years that seemed to strike a nice balance between being strong but still being feasable and something that you could achieve.",
            "And this is called differential privacy is due to Cynthia Dwork in your coworkers in 2006, and basically what it says is that an individual's inclusion inside.",
            "The training data shouldn't change much when an adversary can possibly learn about that individual sensitive information.",
            "So if I'm an adversary and you're trying to decide whether you should participate in.",
            "In this training or just, you know, just database of information or not, you know.",
            "Differential privacy guarantee would say that whatever I could learn about you, it doesn't matter whether you joined the database or not.",
            "So this is a.",
            "This is the guarantee that differential privacy would ensure."
        ],
        [
            "OK, so formally will.",
            "Placing the context of learning which was done by Cassie fonts on at all in 2008 will look at basically a learning algorithm that preserve differential privacy, so we'll think of learning algorithms as functions on training sets that produce a hypothesis around the Class H, and these will think of these actually randomized algorithms, because it turns out they have to be randomized, so will say such a learning algorithm is Alpha, differentially, private.",
            "If essentially the induced distribution that has over hypothesis when.",
            "Upon input training set doesn't change too much.",
            "After you perturb or change one of the examples in the training set.",
            "So formally, this is for every pair of training sets, S&S Prime differing in at most one example.",
            "And for every possible output set G, the probability of the algorithm producing hypothesis in G upon input S divided by the probability that the algorithm produces the hypothesis in G appan input S prime should be bounded by Y to the Alpha.",
            "Here you should think of the office just about one plus Alpha, so it's a small multiplicative factor.",
            "OK, so here the probability is over the internal randomness of the algorithm has nothing to do with where the database or the training data comes from is just the property of the algorithm.",
            "So roughly what is saying is that if now given similar training sets should behave similarly in the sense that it's distribution over hypothesis should not change too much.",
            "And the other thing to notice is that here the parameter Alpha controls the strength of the privacy.",
            "The house trying to guarantee is K so smaller.",
            "Value of Alpha between zero and one correspond to stronger guarantee."
        ],
        [
            "So the other goal, of course, is a good learning or learning an accurate classifier, and we'll just adopt sort of the standard statistical model of learning.",
            "So if you have an ID sample of labeled examples from a distribution, than the algorithm should produce a hypothesis such that with high probability is almost as good as almost as accurate as the best one in the class.",
            "And roughly, you know the only thing to keep in mind here is that you know with probability 1 minus Delta that's taken over the choice of the random.",
            "Random Draw the training examples as well as the internal randomness in the algorithm."
        ],
        [
            "OK, so those very few goals.",
            "So what was known about this in before?",
            "So in previous work by Cassie Vishwanathan at all an extended by Blum, it all in being on on what they found, with that what they study was basically sample complexity for learning, finding hypothesis classes or BC classes over discrete data domains.",
            "So by a discrete data domain I mean something like the Boolean hypercube in dimensions.",
            "OK, so they look this study.",
            "The sample complexity question and what they found was that the sample complexity really just blows up by some small factor.",
            "So you have this term that's one over Alpha epsilon.",
            "Remember Alpha is the parameter controls the strength of the privacy guarantee.",
            "And one over Alpha plus one over epsilon squared.",
            "This is pretty standard and the sample complexity bound.",
            "And because everything is finite, you can just depend on sort of log cardinality.",
            "So log cardinality of the hypothesis class or sort of the VC dimension of hypothesis class times the log cardinality of the data domain.",
            "So this is somewhat the bound is somewhat worse than what you would have in a non private setting, but it's within polynomial factors or something.",
            "OK, so this is also related to basically other problems of releasing like a synthetic data set that.",
            "Supports queries from a hypothesis classes for those of you interested, this is some interesting work in in sort of learning type guarantees for privacy preserving learning.",
            "So this turns out to be an upper bound as well, and they have some more closely matching lower bounds, but this is roughly what characterizes the sample complexity.",
            "OK, so the problem here is that you know this is nice, but it doesn't capture some.",
            "You know, very common things that we do in machine learning, like learning an SVM or logistic regression, because in those cases we were thinking about an infinite hypothesis class in sort of a continuous data domain like R to R to the end."
        ],
        [
            "So that's the question that we want to address.",
            "What happened when we look at infinite classes and continuous data domains?"
        ],
        [
            "So that brings us to our results and their results are somewhere sort of a mixed bag in terms of good."
        ],
        [
            "Recent bad news.",
            "So bad news is the following that there's no distribution independent learning that's possible.",
            "And when you want to also preserve when you also want to guarantee differential privacy so there's not, you can't do different distribution.",
            "Free learning and distribution free, differentially private learning.",
            "OK, so this seems like bad news, but there's is not so bad because you can get around this lower bound if you're willing to change your goals a little bit.",
            "So we consider two ways of changing into goals.",
            "So one is to allow the learner some some extra power in this sense.",
            "Here we look at what happens when you give the learner some prior knowledge about the data distribution and another way to change the goals is to sort of relax the privacy requirements.",
            "So relax the notion of privacy, so I'll.",
            "Go into details about."
        ],
        [
            "The bad news first.",
            "So basically, the lower bound that we show is.",
            "For the following problem, you want to learn hypoth linear or sort of threshold function on the interval.",
            "So just one dimensional function at VC Dimension one.",
            "It's just basically the threshold function at zero and then goes up to one when it's beyond particular value in zero to 1.",
            "And so here's actually the theorem you take.",
            "Any differentially private algorithms, Alpha, differentially private algorithm and consider any sort of sample size M capital N. What the theorem says that there is a distribution P such that the following holds.",
            "There is some threshold function that will perfectly separate the data so it has 0 error while at the same time sort of any this differentially private algorithm.",
            "If it has fewer than M examples, then with large probability with probability 1/2, the algorithm will output a hypothesis with error at least 20%.",
            "So you can't get any sort of epsilon Delta type learning an if you also want to preserve differential privacy.",
            "Yes, so this is for any M answer.",
            "Any samples size bound at the algorithm I have.",
            "There's some distribution for which the algorithm will fail.",
            "OK.",
            "So let me sort of discuss the just quickly just say what the implication."
        ],
        [
            "Of this are.",
            "There's really no in the context of differentially private learning.",
            "There's no sort of direct analogue of a VC theorem that says you can do distribution free learning.",
            "There's no sort of characterization of when learning as possible in a different distribution free sense.",
            "And this is also interesting because it's a qualitative difference between learning finding hypothesis classes and discrete data domains, and versus infinite classes on continuous data domains, which is, you know, they're running.",
            "Really.",
            "There wasn't this kind of difference in the case of non private learning, but when you want to preserve differential privacy this becomes an important issue."
        ],
        [
            "OK, so the basic idea of the proof is pretty simple.",
            "But we do if we find 2 distributions P&P prime over, you know the unit interval across with the labels such that any so?",
            "Remember that any differential privacy algorithm a differentially private algorithms will induce some distribution over the hypothesis class.",
            "So it turns out what we do is we find these distribution PNP prime over to data such that a successful distribution over thresholds would differ very differ alot from the successful distribution for P prime.",
            "So you think of PNP primes just two distribution then.",
            "Sort of the picture.",
            "The distributions over thresholds you'd want, or the one that you know roughly around where the true threshold is.",
            "OK, so these distributions assessable distributions are very different, But if you're a differentially private learner and just using a small number of examples, you have to basically behave very similarly in both of these cases.",
            "Innocent because of this sort of stability property that differential privacy requires.",
            "And so therefore, if you have to behave differently in very differently or start very similarly in both of these cases, you're going to fail with respect to one of these probability distributions."
        ],
        [
            "OK, so that was the bad news, but you know, there's still some ways to get around this lower bound if you're willing to change your goals.",
            "So we consider two ways.",
            "In this work.",
            "We first look at what happens when you allow the learner some prior knowledge about the unlabeled data distribution.",
            "And the 2nd way is we look at what happens when you relax the privacy requirement.",
            "So if you say that only the labels in the data are the things that are sensitive information, but you don't guarantee anything about the unlabeled parts of the data.",
            "So maybe those those parts of the data are sort of public knowledge.",
            "OK, so I'll just talk about the first of these and."
        ],
        [
            "Second ones you can ask me about later."
        ],
        [
            "OK, So what we do here is we allowed to learner access to so-called reference distribution.",
            "You over to unlabeled data.",
            "And this is something that you should choose independently of the training data.",
            "So this is something that's sort of just built into the algorithm.",
            "And what will happen is that the sample complexity of the algorithm will depend on how close this distribution you is to the true data distribution.",
            "The true distribution over the unlabeled data D. So by close I mean in a sort of sort of a sense of how smooth with one is 2 with respect to the other one.",
            "So on the left you see that there's a distribute.",
            "The red distribution is roughly never assigns too much more mass to any part that you distribution.",
            "The blue one assigns, so those two are sort of close, but on the right the red distributions assigned some high mass where the blue one assigns very little mass.",
            "Of those distributions are far in distance."
        ],
        [
            "OK, so then we have the following upper bound on the sample complexity.",
            "OK, so this is, you know, roughly pretty standard looking sample complexity bound.",
            "Basically if you look let P be any distribution over to data with the sort of marginal distribution of over the unlabeled, unlabeled data D. Then any there is an Alpha, differentially private algorithm such that you have a sample size large enough, then with probability 1 minus Delta it will return a hypothesis that's almost as good as the best one in the class.",
            "And the sample size is.",
            "Just has some GNU terms that aren't don't appear in sort of a non private setting."
        ],
        [
            "So the first one is one over Alpha Epsilon.",
            "This is the same that turned up here in the case of finding hypothesis classes."
        ],
        [
            "OK, um thier disebut is the doubling dimension of the discrete metric space with respect to you?",
            "OK, So what I mean by doubling dimension is actually just something you can think of.",
            "The hypothesis class in the data distribution you as inducing some kind of metric space.",
            "You look at the doubling dimension of this quantity, and this is the subview, so the doubling dimension is something that has been used in the past to characterize sort of local local complexity of a hypothesis class.",
            "Coupled with the data distribution and just work by Behsudi Leanne Long in 2009, they showed that doubling dimension could could be used to upper bound the sample complexity in a non private setting.",
            "Here we use it to upper bound to sample complexity in this sort of modified private setting.",
            "But here's the thing.",
            "To notice is that it's with respect to the reference distribution you.",
            "So the so we'd like to choose the reference distribution so that this quantity is small."
        ],
        [
            "At the same time, there's this other quantity which is Kappa U, D, which is a measure of divergent between the distribution U&D.",
            "So if you have prior knowledge about how about the unlabeled data distribution, you'd like to choose reference distribution to be not too far from.",
            "The true unlabeled data distribution and this is the thing that you would control.",
            "Got any questions about the job?",
            "So D is just yeah, so the data true data distribution is P&D certain marginal of it over over to unlabeled data.",
            "So you yes, this reference distribution, that sort of is chosen independently of the data of the of the private."
        ],
        [
            "So the one over Epsilon, Alpha Epsilon is seems to be sort of the natural contribution or the natural way that differential privacy enters into.",
            "The sample complexity.",
            "OK."
        ],
        [
            "Now just now Captain.",
            "OK, so we showed basically 2 main results here.",
            "The first was sort of a sort of some bad news about differential privacy.",
            "That is that you kind of rule out when you want to require differential privacy rules out any sort of different distribution independent proper learning guarantee.",
            "And so the ways that we considered the forgetting around this was to consider data dependent bounds or.",
            "That depend on prior knowledge about the unlabeled data distribution.",
            "Answered another way that we could do it.",
            "Do it with the relaxed notion of privacy is something called label privacy where the only thing you're going to guarantee is that the labels aren't compromised.",
            "The privacy of the labels aren't compromised, so there's some future direction that you might want to consider so all of the work I've been talking bout has been for proper learning in the sense that we required learning algorithm to output the hypothesis in that particular Class H. So there's been some work on improper learning in a differentially private setting.",
            "In the when you consider sort of discrete settings like.",
            "Finding hypothesis classes or discrete data domains by being well at all in 2010.",
            "It's also worth considering different notions of different ways to weaken the notion of privacy.",
            "And finally, it would be interesting to characterize sort of the sample complexity for others to estimation tests like regression or ranking or other unsupervised learning tasks."
        ],
        [
            "Thanks.",
            "So is your negative result about arbitrary distributions?",
            "What you mean?",
            "The aperture distribution enters the definition of the privacy, like the when you say in the financial privacy talks about the probability that you'll get some result.",
            "This probability is over the data generating distribution or.",
            "Personal privacy really only only talks about the internal randomness of the learning alright, but these random algorithms if the algorithm is based on random examples, then I wonder if this randomness also depends on on the generating distribution.",
            "Yeah, so this randomness that I talked about in the definition of privacy should not should be independent of any sort of data generating process.",
            "OK.",
            "So.",
            "Had a question regards to this definition of Kappa and potential.",
            "Potentially access cap is just kind of like some sort of total variation in log space or something like that, But the point is its measure between the distributions.",
            "But if you think about an algorithm which only spits out in some hypothesis space, it seems like you should be looking at some notion of the of the distances in hypothesis space, because if your algorithm spit out only a constant, then it doesn't matter if you know the distribution or not.",
            "It's just that I would probably be a bad learning algorithm that would be bad learning algorithm, But the point is.",
            "It's not reflected in these bonds because you're looking at the distance between your knowledge of the distribution and.",
            "And the true distribution kind of in that space versus some sort of induced space in the hypothesis.",
            "Based on the hypothesis.",
            "'cause somehow it's like a VC argument that what you really care about is.",
            "Somehow, how close these hypothesis I see what you mean?",
            "Yeah, I don't know how to formalize it, but it seems like something potentially computer.",
            "Yeah, yeah.",
            "So it's possible that we might be able to sort of re characterize mania, tighter characterization, or different characterization of the sample complexity when considers or the projection that hypothesis?",
            "Yeah, I'd like it to be if you always spits out a constant, that thing should vanish.",
            "Yes, I agree.",
            "Thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here's the first part.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me give it some motivation for the problem we were looking at.",
                    "label": 0
                },
                {
                    "sent": "So in a lot of machine learning application, the data that the training data that we have actually is composed of individuals, sensitive information and So what I mean here.",
                    "label": 0
                },
                {
                    "sent": "So in some examples like an E Commerce, the data that you have maybe customers browsing or purchase histories in clinical studies you might be looking at medical records of patients and their various test results.",
                    "label": 0
                },
                {
                    "sent": "And in genomic studies you have basically individuals genetic sequences like their DNA or protein sequences.",
                    "label": 0
                },
                {
                    "sent": "And so, in all of these applications, what you'd like to do is somehow learn something about the population from these data about individuals.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this work will look at the particular problem of learning a binary classifier from labeled examples, and the thing that keep keep in mind here is that the training each training example is basically an individual sensitive information, and this is something that they like to keep private.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the first question you might want to ask is like if there even a problem that we have to address here so we have some sensitive training data.",
                    "label": 0
                },
                {
                    "sent": "It gets fed into a learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "This algorithm produces a classifier and this classifier and then is released to the public so it becomes public knowledge.",
                    "label": 0
                },
                {
                    "sent": "So is there some sort of problem that we have to address and the question is, you know, if we release this classifier to the public, if it gets deployed, does this somehow violate the privacy of the individuals who are in the training training data?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Answer is yes and remains true even after simple anonymization procedures or even when you're just releasing aggregate statistics.",
                    "label": 0
                },
                {
                    "sent": "And the reason here is because an adversary who's trying to violate or compromise someone's privacy, he might have access to some side information about the individual.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here's an example where it seems like this has happened.",
                    "label": 0
                },
                {
                    "sent": "This was the recent work by winning at all from Indiana.",
                    "label": 0
                },
                {
                    "sent": "And what they were able to do was look at some genome wide Association studies which published some correlation statistics about individuals and their genetic markers, and they were able to combine this with some site information to determine whether you know individual in the study was in a particular group.",
                    "label": 0
                },
                {
                    "sent": "So the group of people who had a particular disease, or if they didn't have the disease, that they were in the healthy group, and that bit of information with something that these individuals expected to keep private.",
                    "label": 0
                },
                {
                    "sent": "But you know the adversary who had some side information, was able to compromise this privacy.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so it seems like there's you know we have to do a little bit more than just standard machine learning.",
                    "label": 0
                },
                {
                    "sent": "Just combining statistics in order to achieve both the goal of learning an accurate classifier while also preserving the privacy of the data.",
                    "label": 0
                },
                {
                    "sent": "OK, so in this work will look at the specific question of sample complexity, that is, how many labeled examples do we need to achieve both of these goals simultaneously?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what do I mean by privacy?",
                    "label": 0
                },
                {
                    "sent": "So you could think of possibly think of a lot of different notions of privacy and you know some might be very strong notion, so might be very weak.",
                    "label": 0
                },
                {
                    "sent": "But you know there was one notion of privacy that's been popular in recent years that seemed to strike a nice balance between being strong but still being feasable and something that you could achieve.",
                    "label": 0
                },
                {
                    "sent": "And this is called differential privacy is due to Cynthia Dwork in your coworkers in 2006, and basically what it says is that an individual's inclusion inside.",
                    "label": 0
                },
                {
                    "sent": "The training data shouldn't change much when an adversary can possibly learn about that individual sensitive information.",
                    "label": 0
                },
                {
                    "sent": "So if I'm an adversary and you're trying to decide whether you should participate in.",
                    "label": 0
                },
                {
                    "sent": "In this training or just, you know, just database of information or not, you know.",
                    "label": 0
                },
                {
                    "sent": "Differential privacy guarantee would say that whatever I could learn about you, it doesn't matter whether you joined the database or not.",
                    "label": 0
                },
                {
                    "sent": "So this is a.",
                    "label": 0
                },
                {
                    "sent": "This is the guarantee that differential privacy would ensure.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so formally will.",
                    "label": 0
                },
                {
                    "sent": "Placing the context of learning which was done by Cassie fonts on at all in 2008 will look at basically a learning algorithm that preserve differential privacy, so we'll think of learning algorithms as functions on training sets that produce a hypothesis around the Class H, and these will think of these actually randomized algorithms, because it turns out they have to be randomized, so will say such a learning algorithm is Alpha, differentially, private.",
                    "label": 0
                },
                {
                    "sent": "If essentially the induced distribution that has over hypothesis when.",
                    "label": 0
                },
                {
                    "sent": "Upon input training set doesn't change too much.",
                    "label": 0
                },
                {
                    "sent": "After you perturb or change one of the examples in the training set.",
                    "label": 1
                },
                {
                    "sent": "So formally, this is for every pair of training sets, S&S Prime differing in at most one example.",
                    "label": 0
                },
                {
                    "sent": "And for every possible output set G, the probability of the algorithm producing hypothesis in G upon input S divided by the probability that the algorithm produces the hypothesis in G appan input S prime should be bounded by Y to the Alpha.",
                    "label": 0
                },
                {
                    "sent": "Here you should think of the office just about one plus Alpha, so it's a small multiplicative factor.",
                    "label": 0
                },
                {
                    "sent": "OK, so here the probability is over the internal randomness of the algorithm has nothing to do with where the database or the training data comes from is just the property of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So roughly what is saying is that if now given similar training sets should behave similarly in the sense that it's distribution over hypothesis should not change too much.",
                    "label": 0
                },
                {
                    "sent": "And the other thing to notice is that here the parameter Alpha controls the strength of the privacy.",
                    "label": 0
                },
                {
                    "sent": "The house trying to guarantee is K so smaller.",
                    "label": 0
                },
                {
                    "sent": "Value of Alpha between zero and one correspond to stronger guarantee.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the other goal, of course, is a good learning or learning an accurate classifier, and we'll just adopt sort of the standard statistical model of learning.",
                    "label": 0
                },
                {
                    "sent": "So if you have an ID sample of labeled examples from a distribution, than the algorithm should produce a hypothesis such that with high probability is almost as good as almost as accurate as the best one in the class.",
                    "label": 0
                },
                {
                    "sent": "And roughly, you know the only thing to keep in mind here is that you know with probability 1 minus Delta that's taken over the choice of the random.",
                    "label": 0
                },
                {
                    "sent": "Random Draw the training examples as well as the internal randomness in the algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so those very few goals.",
                    "label": 0
                },
                {
                    "sent": "So what was known about this in before?",
                    "label": 0
                },
                {
                    "sent": "So in previous work by Cassie Vishwanathan at all an extended by Blum, it all in being on on what they found, with that what they study was basically sample complexity for learning, finding hypothesis classes or BC classes over discrete data domains.",
                    "label": 0
                },
                {
                    "sent": "So by a discrete data domain I mean something like the Boolean hypercube in dimensions.",
                    "label": 0
                },
                {
                    "sent": "OK, so they look this study.",
                    "label": 0
                },
                {
                    "sent": "The sample complexity question and what they found was that the sample complexity really just blows up by some small factor.",
                    "label": 0
                },
                {
                    "sent": "So you have this term that's one over Alpha epsilon.",
                    "label": 0
                },
                {
                    "sent": "Remember Alpha is the parameter controls the strength of the privacy guarantee.",
                    "label": 0
                },
                {
                    "sent": "And one over Alpha plus one over epsilon squared.",
                    "label": 0
                },
                {
                    "sent": "This is pretty standard and the sample complexity bound.",
                    "label": 0
                },
                {
                    "sent": "And because everything is finite, you can just depend on sort of log cardinality.",
                    "label": 0
                },
                {
                    "sent": "So log cardinality of the hypothesis class or sort of the VC dimension of hypothesis class times the log cardinality of the data domain.",
                    "label": 0
                },
                {
                    "sent": "So this is somewhat the bound is somewhat worse than what you would have in a non private setting, but it's within polynomial factors or something.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is also related to basically other problems of releasing like a synthetic data set that.",
                    "label": 0
                },
                {
                    "sent": "Supports queries from a hypothesis classes for those of you interested, this is some interesting work in in sort of learning type guarantees for privacy preserving learning.",
                    "label": 1
                },
                {
                    "sent": "So this turns out to be an upper bound as well, and they have some more closely matching lower bounds, but this is roughly what characterizes the sample complexity.",
                    "label": 0
                },
                {
                    "sent": "OK, so the problem here is that you know this is nice, but it doesn't capture some.",
                    "label": 0
                },
                {
                    "sent": "You know, very common things that we do in machine learning, like learning an SVM or logistic regression, because in those cases we were thinking about an infinite hypothesis class in sort of a continuous data domain like R to R to the end.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's the question that we want to address.",
                    "label": 0
                },
                {
                    "sent": "What happened when we look at infinite classes and continuous data domains?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that brings us to our results and their results are somewhere sort of a mixed bag in terms of good.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Recent bad news.",
                    "label": 0
                },
                {
                    "sent": "So bad news is the following that there's no distribution independent learning that's possible.",
                    "label": 0
                },
                {
                    "sent": "And when you want to also preserve when you also want to guarantee differential privacy so there's not, you can't do different distribution.",
                    "label": 0
                },
                {
                    "sent": "Free learning and distribution free, differentially private learning.",
                    "label": 0
                },
                {
                    "sent": "OK, so this seems like bad news, but there's is not so bad because you can get around this lower bound if you're willing to change your goals a little bit.",
                    "label": 0
                },
                {
                    "sent": "So we consider two ways of changing into goals.",
                    "label": 0
                },
                {
                    "sent": "So one is to allow the learner some some extra power in this sense.",
                    "label": 0
                },
                {
                    "sent": "Here we look at what happens when you give the learner some prior knowledge about the data distribution and another way to change the goals is to sort of relax the privacy requirements.",
                    "label": 0
                },
                {
                    "sent": "So relax the notion of privacy, so I'll.",
                    "label": 0
                },
                {
                    "sent": "Go into details about.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The bad news first.",
                    "label": 0
                },
                {
                    "sent": "So basically, the lower bound that we show is.",
                    "label": 0
                },
                {
                    "sent": "For the following problem, you want to learn hypoth linear or sort of threshold function on the interval.",
                    "label": 0
                },
                {
                    "sent": "So just one dimensional function at VC Dimension one.",
                    "label": 0
                },
                {
                    "sent": "It's just basically the threshold function at zero and then goes up to one when it's beyond particular value in zero to 1.",
                    "label": 0
                },
                {
                    "sent": "And so here's actually the theorem you take.",
                    "label": 0
                },
                {
                    "sent": "Any differentially private algorithms, Alpha, differentially private algorithm and consider any sort of sample size M capital N. What the theorem says that there is a distribution P such that the following holds.",
                    "label": 0
                },
                {
                    "sent": "There is some threshold function that will perfectly separate the data so it has 0 error while at the same time sort of any this differentially private algorithm.",
                    "label": 0
                },
                {
                    "sent": "If it has fewer than M examples, then with large probability with probability 1/2, the algorithm will output a hypothesis with error at least 20%.",
                    "label": 0
                },
                {
                    "sent": "So you can't get any sort of epsilon Delta type learning an if you also want to preserve differential privacy.",
                    "label": 0
                },
                {
                    "sent": "Yes, so this is for any M answer.",
                    "label": 0
                },
                {
                    "sent": "Any samples size bound at the algorithm I have.",
                    "label": 0
                },
                {
                    "sent": "There's some distribution for which the algorithm will fail.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So let me sort of discuss the just quickly just say what the implication.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of this are.",
                    "label": 0
                },
                {
                    "sent": "There's really no in the context of differentially private learning.",
                    "label": 0
                },
                {
                    "sent": "There's no sort of direct analogue of a VC theorem that says you can do distribution free learning.",
                    "label": 0
                },
                {
                    "sent": "There's no sort of characterization of when learning as possible in a different distribution free sense.",
                    "label": 0
                },
                {
                    "sent": "And this is also interesting because it's a qualitative difference between learning finding hypothesis classes and discrete data domains, and versus infinite classes on continuous data domains, which is, you know, they're running.",
                    "label": 0
                },
                {
                    "sent": "Really.",
                    "label": 0
                },
                {
                    "sent": "There wasn't this kind of difference in the case of non private learning, but when you want to preserve differential privacy this becomes an important issue.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the basic idea of the proof is pretty simple.",
                    "label": 0
                },
                {
                    "sent": "But we do if we find 2 distributions P&P prime over, you know the unit interval across with the labels such that any so?",
                    "label": 0
                },
                {
                    "sent": "Remember that any differential privacy algorithm a differentially private algorithms will induce some distribution over the hypothesis class.",
                    "label": 0
                },
                {
                    "sent": "So it turns out what we do is we find these distribution PNP prime over to data such that a successful distribution over thresholds would differ very differ alot from the successful distribution for P prime.",
                    "label": 0
                },
                {
                    "sent": "So you think of PNP primes just two distribution then.",
                    "label": 0
                },
                {
                    "sent": "Sort of the picture.",
                    "label": 0
                },
                {
                    "sent": "The distributions over thresholds you'd want, or the one that you know roughly around where the true threshold is.",
                    "label": 0
                },
                {
                    "sent": "OK, so these distributions assessable distributions are very different, But if you're a differentially private learner and just using a small number of examples, you have to basically behave very similarly in both of these cases.",
                    "label": 0
                },
                {
                    "sent": "Innocent because of this sort of stability property that differential privacy requires.",
                    "label": 0
                },
                {
                    "sent": "And so therefore, if you have to behave differently in very differently or start very similarly in both of these cases, you're going to fail with respect to one of these probability distributions.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that was the bad news, but you know, there's still some ways to get around this lower bound if you're willing to change your goals.",
                    "label": 0
                },
                {
                    "sent": "So we consider two ways.",
                    "label": 0
                },
                {
                    "sent": "In this work.",
                    "label": 0
                },
                {
                    "sent": "We first look at what happens when you allow the learner some prior knowledge about the unlabeled data distribution.",
                    "label": 0
                },
                {
                    "sent": "And the 2nd way is we look at what happens when you relax the privacy requirement.",
                    "label": 0
                },
                {
                    "sent": "So if you say that only the labels in the data are the things that are sensitive information, but you don't guarantee anything about the unlabeled parts of the data.",
                    "label": 0
                },
                {
                    "sent": "So maybe those those parts of the data are sort of public knowledge.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'll just talk about the first of these and.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Second ones you can ask me about later.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what we do here is we allowed to learner access to so-called reference distribution.",
                    "label": 1
                },
                {
                    "sent": "You over to unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "And this is something that you should choose independently of the training data.",
                    "label": 1
                },
                {
                    "sent": "So this is something that's sort of just built into the algorithm.",
                    "label": 0
                },
                {
                    "sent": "And what will happen is that the sample complexity of the algorithm will depend on how close this distribution you is to the true data distribution.",
                    "label": 0
                },
                {
                    "sent": "The true distribution over the unlabeled data D. So by close I mean in a sort of sort of a sense of how smooth with one is 2 with respect to the other one.",
                    "label": 0
                },
                {
                    "sent": "So on the left you see that there's a distribute.",
                    "label": 0
                },
                {
                    "sent": "The red distribution is roughly never assigns too much more mass to any part that you distribution.",
                    "label": 0
                },
                {
                    "sent": "The blue one assigns, so those two are sort of close, but on the right the red distributions assigned some high mass where the blue one assigns very little mass.",
                    "label": 0
                },
                {
                    "sent": "Of those distributions are far in distance.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so then we have the following upper bound on the sample complexity.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is, you know, roughly pretty standard looking sample complexity bound.",
                    "label": 0
                },
                {
                    "sent": "Basically if you look let P be any distribution over to data with the sort of marginal distribution of over the unlabeled, unlabeled data D. Then any there is an Alpha, differentially private algorithm such that you have a sample size large enough, then with probability 1 minus Delta it will return a hypothesis that's almost as good as the best one in the class.",
                    "label": 0
                },
                {
                    "sent": "And the sample size is.",
                    "label": 0
                },
                {
                    "sent": "Just has some GNU terms that aren't don't appear in sort of a non private setting.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the first one is one over Alpha Epsilon.",
                    "label": 0
                },
                {
                    "sent": "This is the same that turned up here in the case of finding hypothesis classes.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, um thier disebut is the doubling dimension of the discrete metric space with respect to you?",
                    "label": 0
                },
                {
                    "sent": "OK, So what I mean by doubling dimension is actually just something you can think of.",
                    "label": 0
                },
                {
                    "sent": "The hypothesis class in the data distribution you as inducing some kind of metric space.",
                    "label": 0
                },
                {
                    "sent": "You look at the doubling dimension of this quantity, and this is the subview, so the doubling dimension is something that has been used in the past to characterize sort of local local complexity of a hypothesis class.",
                    "label": 0
                },
                {
                    "sent": "Coupled with the data distribution and just work by Behsudi Leanne Long in 2009, they showed that doubling dimension could could be used to upper bound the sample complexity in a non private setting.",
                    "label": 0
                },
                {
                    "sent": "Here we use it to upper bound to sample complexity in this sort of modified private setting.",
                    "label": 0
                },
                {
                    "sent": "But here's the thing.",
                    "label": 0
                },
                {
                    "sent": "To notice is that it's with respect to the reference distribution you.",
                    "label": 0
                },
                {
                    "sent": "So the so we'd like to choose the reference distribution so that this quantity is small.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At the same time, there's this other quantity which is Kappa U, D, which is a measure of divergent between the distribution U&D.",
                    "label": 1
                },
                {
                    "sent": "So if you have prior knowledge about how about the unlabeled data distribution, you'd like to choose reference distribution to be not too far from.",
                    "label": 1
                },
                {
                    "sent": "The true unlabeled data distribution and this is the thing that you would control.",
                    "label": 0
                },
                {
                    "sent": "Got any questions about the job?",
                    "label": 0
                },
                {
                    "sent": "So D is just yeah, so the data true data distribution is P&D certain marginal of it over over to unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "So you yes, this reference distribution, that sort of is chosen independently of the data of the of the private.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the one over Epsilon, Alpha Epsilon is seems to be sort of the natural contribution or the natural way that differential privacy enters into.",
                    "label": 0
                },
                {
                    "sent": "The sample complexity.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now just now Captain.",
                    "label": 0
                },
                {
                    "sent": "OK, so we showed basically 2 main results here.",
                    "label": 0
                },
                {
                    "sent": "The first was sort of a sort of some bad news about differential privacy.",
                    "label": 0
                },
                {
                    "sent": "That is that you kind of rule out when you want to require differential privacy rules out any sort of different distribution independent proper learning guarantee.",
                    "label": 0
                },
                {
                    "sent": "And so the ways that we considered the forgetting around this was to consider data dependent bounds or.",
                    "label": 0
                },
                {
                    "sent": "That depend on prior knowledge about the unlabeled data distribution.",
                    "label": 1
                },
                {
                    "sent": "Answered another way that we could do it.",
                    "label": 0
                },
                {
                    "sent": "Do it with the relaxed notion of privacy is something called label privacy where the only thing you're going to guarantee is that the labels aren't compromised.",
                    "label": 0
                },
                {
                    "sent": "The privacy of the labels aren't compromised, so there's some future direction that you might want to consider so all of the work I've been talking bout has been for proper learning in the sense that we required learning algorithm to output the hypothesis in that particular Class H. So there's been some work on improper learning in a differentially private setting.",
                    "label": 0
                },
                {
                    "sent": "In the when you consider sort of discrete settings like.",
                    "label": 0
                },
                {
                    "sent": "Finding hypothesis classes or discrete data domains by being well at all in 2010.",
                    "label": 0
                },
                {
                    "sent": "It's also worth considering different notions of different ways to weaken the notion of privacy.",
                    "label": 0
                },
                {
                    "sent": "And finally, it would be interesting to characterize sort of the sample complexity for others to estimation tests like regression or ranking or other unsupervised learning tasks.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "So is your negative result about arbitrary distributions?",
                    "label": 0
                },
                {
                    "sent": "What you mean?",
                    "label": 0
                },
                {
                    "sent": "The aperture distribution enters the definition of the privacy, like the when you say in the financial privacy talks about the probability that you'll get some result.",
                    "label": 0
                },
                {
                    "sent": "This probability is over the data generating distribution or.",
                    "label": 0
                },
                {
                    "sent": "Personal privacy really only only talks about the internal randomness of the learning alright, but these random algorithms if the algorithm is based on random examples, then I wonder if this randomness also depends on on the generating distribution.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so this randomness that I talked about in the definition of privacy should not should be independent of any sort of data generating process.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Had a question regards to this definition of Kappa and potential.",
                    "label": 0
                },
                {
                    "sent": "Potentially access cap is just kind of like some sort of total variation in log space or something like that, But the point is its measure between the distributions.",
                    "label": 0
                },
                {
                    "sent": "But if you think about an algorithm which only spits out in some hypothesis space, it seems like you should be looking at some notion of the of the distances in hypothesis space, because if your algorithm spit out only a constant, then it doesn't matter if you know the distribution or not.",
                    "label": 0
                },
                {
                    "sent": "It's just that I would probably be a bad learning algorithm that would be bad learning algorithm, But the point is.",
                    "label": 0
                },
                {
                    "sent": "It's not reflected in these bonds because you're looking at the distance between your knowledge of the distribution and.",
                    "label": 0
                },
                {
                    "sent": "And the true distribution kind of in that space versus some sort of induced space in the hypothesis.",
                    "label": 0
                },
                {
                    "sent": "Based on the hypothesis.",
                    "label": 0
                },
                {
                    "sent": "'cause somehow it's like a VC argument that what you really care about is.",
                    "label": 0
                },
                {
                    "sent": "Somehow, how close these hypothesis I see what you mean?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I don't know how to formalize it, but it seems like something potentially computer.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "So it's possible that we might be able to sort of re characterize mania, tighter characterization, or different characterization of the sample complexity when considers or the projection that hypothesis?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'd like it to be if you always spits out a constant, that thing should vanish.",
                    "label": 0
                },
                {
                    "sent": "Yes, I agree.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                }
            ]
        }
    }
}