{
    "id": "4iv3t5canqp4tmqe45hfut32cyi2xc5d",
    "title": "Towards an automatic creation of localized versions of DBpedia",
    "info": {
        "author": [
            "Alessio Palmero Aprosio, Bruno Kessler Foundation"
        ],
        "published": "Nov. 28, 2013",
        "recorded": "October 2013",
        "category": [
            "Top->Computer Science->Software and Tools",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2013_palmero_aprosio_dbpedia/",
    "segmentation": [
        [
            "My paper is a towards an automatic creation of localized versions of DB pedia, and it's a joint work with cloud Rihanna and Albert Lavalley from from the center broadcaster in Trento, in Italy."
        ],
        [
            "Let's start our goals is to extend the coverage of the paideia mappings on existing editions of existing chapters in existing languages and create new mappings for new languages."
        ],
        [
            "And we want to do it automatically.",
            "Bich"
        ],
        [
            "Now the pedia is build by mapping Wikipedia influx 2.",
            "They they DP.",
            "Ontology VPN Ontology is created manually by the by the community and the mappings are created manually too by the PDF community."
        ],
        [
            "The mapping task is a illustrated here, so then infobox Sportiva that in Italian means sportsman is manually mapped to the class that pedia class sportsman an the attributes in the Wikipedia infobox.",
            "Sportiva are then manually mapped to the DB Pedia properties to the corresponding properties.",
            "There maybe attributes that don't have a property in Libya and vice versa an.",
            "From now on, when we speak about the attributes, we refers to Wikipedia an when we speaks about properties, we refer to the pedia just to don't misunderstand the items.",
            "A the part concerning classes has already been done in other research papers, so today we will concentrate only on properties.",
            "So our work is only related to the mapping between Wikipedia attributes and the beta properties."
        ],
        [
            "After this mapping task from the deep community, there is a set of scripts called Extraction Framework also developed by DB Pedia, that extract this information from the Wikipedia dumps and you with them to populate the paleontology.",
            "So the mappings are applied by the scripts to populate the ontology."
        ],
        [
            "The this is a.",
            "This parting has some issues because mappings are made manually, so if we want to make a new new chapter of the Pedia in a new language, we need to start from scratch and do them do the mapping again.",
            "And also there is a problem of homogeneity over the info box in different languages.",
            "Wikipedia.",
            "So as most frequent infoboxes are Muppets first 2 increase the coverage, some info box are missing are Mr Becauses they appears too few times.",
            "He exists."
        ],
        [
            "Approaches to this automatic mapping between Wikipedia are done using infobox alignment."
        ],
        [
            "Starting from an infobox in Wikipedia and.",
            "A any folks in other people language, for example English, usually is English.",
            "We map the first map, the division administrativa."
        ],
        [
            "In focusing this example to the settlement info box in English, because a settlement in English means division administrative in Italian.",
            "So if there exists a mapping between the English infobox settlement and the classic PDP class settlement, we can infer that.",
            "Also the reason I'm interested in Italian can be mapped to settlement."
        ],
        [
            "In an indie pedia."
        ],
        [
            "While this works for settlement, for example, maybe in some languages there is no homogeneity in the in the infobox.",
            "So for example in the page speaking about Madrid in English has the infobox settlement and the Italian while in French has the infobox cone despond that means Spain is finished down.",
            "That is not that cannot be aligned with any corresponding English info box."
        ],
        [
            "Another problem is that sometimes the one page can have more than one infobox.",
            "So for example, the Italian page of Barack Obama has two different infobox acharacle Publica Nbo one in English they said it has officeholder and person data also, in this case officeholder can be mapped to character publicain person data camera to be.",
            "But if we look at the page mapping soft person data, an officeholder we see that."
        ],
        [
            "Office folder is mapped to the office Holder class in the pedia person that is not mapped out at all, so we lose all its properties.",
            "But this is OK in English because for example, play date of birth and place of birth are repeated in the office Holder info box by the policies of the English Wikipedia.",
            "In Italian this does not happen, so if we only consider."
        ],
        [
            "The the Karika Publica and officeholder Infoboxes we lose the birth data information.",
            "OK, so."
        ],
        [
            "Our approach provides that too."
        ],
        [
            "Then ignore the infobox at all and we only consider.",
            "Properties."
        ],
        [
            "So in the example we before we completely forgot about attributes about infobox.",
            "Me only consider attributes in the Wikipedia.",
            "Wikipedia for books."
        ],
        [
            "We don't even need people language as we only consider DB pedia as to compare the values.",
            "So in our."
        ],
        [
            "In our approach, we only consider Wikipedia attributes and the DP that affect already existing."
        ],
        [
            "A I think in the end we want to provide the mappings between Wikipedia attributes and IBD properties, and we compare who we might compare all attributes in Wikipedia to all properties in the paid and.",
            "But this is a very time consumption because we need to do every possible compare."
        ],
        [
            "Reason.",
            "But Fortunately in a we do this comparison instance by instance only.",
            "Only some some attributes in the Wikipedia page are compiled an only some properties in the pedia are filled, so the comparison are less than expect."
        ],
        [
            "A."
        ],
        [
            "For example, in the Barrack Obama page, we compared Logan Asher with birthplace birthdate and this place and so on.",
            "And Madrid we compare super feature that means Aria total with RF total population, total and elevation.",
            "But we do not compare super feature with birthplace because Madrid has not had birthplace.",
            "A the compr."
        ],
        [
            "Prison is done."
        ],
        [
            "On values, so starting from the values of attributes in, for example, in Barack Obama page, for example, only in this case we want to compare that value with all the values in the chorus."
        ],
        [
            "Funding the pedia page.",
            "So we compare on along with birth date with Alma Mater and with birthplace etc.",
            "To do this comparison of in for value."
        ],
        [
            "We created a similarity function that takes as an input the attribute in Wikipedia and the property in DB pedia.",
            "The attribute in Wikipedia is famous structured data.",
            "It's free Tax day is not typed, while the property in the paper is structured data and we will use both information for our comparison."
        ],
        [
            "In fact, we create 5 similarity function that depends on the structure of the attribute in Wikipedia and the type of the property in DB pedia.",
            "The first"
        ],
        [
            "Of that is date comparison.",
            "So when the DP their property is that data we check on the Wikipedia property.",
            "If there are some parts of the date, for example they in math or only the ear on the month year and we give a value from zero to 1 to this similarity function based on the entities found in the Wikipedia text."
        ],
        [
            "The second function is the template expansion because sometimes in the attributes value there is a template.",
            "So we used Wikipedia, the Mediawiki parser to parse this template and dropped."
        ],
        [
            "Been there."
        ],
        [
            "A final value in this case for out 1971, and then we can compare it with the values in the DB pedia in the Peter properties values the third."
        ],
        [
            "Chinese link translation.",
            "So for example Italian.",
            "the Italian Wikipedia has.",
            "And played for the countries, in this case ESP means Spain.",
            "We part the template as before."
        ],
        [
            "Are we obtaining Spagna Spagna in the Italian name of?"
        ],
        [
            "Spain, but now we have to compare Spanner with pain and it's difficult to compare.",
            "So we use a cross language links in Wikipedia to track."
        ],
        [
            "Foreign Spagna into Spain.",
            "So now we can."
        ],
        [
            "Compare Spain in the DB pedia properties value."
        ],
        [
            "First function is string similarity.",
            "Sometimes the type of the property is text for example."
        ],
        [
            "Mona Lisa, the technique used to paint the Mona Lisa is in Italian P. $2 tavola, while."
        ],
        [
            "English is oil and popular, and these strings are not compatible in that in that way so."
        ],
        [
            "We again extract the links to Wikipedia pages from the text and."
        ],
        [
            "Convert them, we translate."
        ],
        [
            "Them into the language on using in DB pedia.",
            "In this case in English.",
            "The then."
        ],
        [
            "We replaced this translation in the original text and now we."
        ],
        [
            "Then compare the two text and we have two common words.",
            "To compute the similarity in general we use string kernels that takes into account two strings and looks for contiguous and non contiguous subsequences of tokens, and they have in common the two string have in common.",
            "The 5th."
        ],
        [
            "The function is number approximation.",
            "For example, in this case it's population of."
        ],
        [
            "Read the hint.",
            "the Italian Wikipedia has a particular value while."
        ],
        [
            "In the English, the Pedia has a different value.",
            "This may happen because maybe the the Wikipedia values is updated with respect to the deep value.",
            "So we."
        ],
        [
            "Set a threshold.",
            "In this example .9."
        ],
        [
            "We calculate the ratio between the two values and if the."
        ],
        [
            "Show is a greater than the threshold.",
            "We can we take into account the value.",
            "We consider the similarity as one, otherwise we consider it at 0 after that.",
            "So after we collect this similarity values for each page."
        ],
        [
            "We calculate an average.",
            "So for every every entity, if we found an end to control, calculate a global similarity of the attribute and property attribute in Wikipedia and the property in the page."
        ],
        [
            "So in this case, for example, in Barack Obama, Logan."
        ],
        [
            "That is compared to birth place and give one etc.",
            "So we then a sum and divide it all the pairs, Wikipedia, attributes and DP."
        ],
        [
            "Party.",
            "And we obtain the average."
        ],
        [
            "Sometimes hours."
        ],
        [
            "Ethan can give the wrong answer.",
            "So for example.",
            "Anyway, Now's was born and died in London, so the comparison between Rogan Ashton displays is 1.",
            "OK, he's one but a."
        ],
        [
            "Considering a lot of entities, the average is less because the Clint Eastwood and Barack Obama didn't died in the same place where they was born."
        ],
        [
            "After that, after come computing this global similarity, we set another threshold and if the value is lower than the threshold, we discard the possible mapping."
        ],
        [
            "Otherwise we select the.",
            "The infobox that maximize this function we took the argmax."
        ],
        [
            "In the previous example, for example, if we set Lambda to open."
        ],
        [
            "Three, we have two different."
        ],
        [
            "Possible mappings for Logan Ashton for birthdays in this place, but we only consider them."
        ],
        [
            "The highest one, so our guess."
        ],
        [
            "Is that Logan Ashton Italian can be mapped to birthplace in DB Pedia in the Italian Wikipedia up to birth place in DB Pedia."
        ],
        [
            "In the experimental settings, we considered the paid 3.8.",
            "So we did our experiment on the Italian Wikipedia.",
            "We annotated Bonelli 100 attributes in Wikipedia, with three annotators having 91% of agreement using Face Kappa measure, and then we try different values of Lambda.",
            "Different values of the threshold."
        ],
        [
            "And we can see this is precision or equals charge.",
            "So the epsilon is precision and the exes recall and we can see that on high Lambda some performance we have the same performance of the model mapping.",
            "The blue dot is a manual mapping considering the existing mappings in Italian, but an if we lower the Lambda threshold we lose something in precision, but we can increase the recall.",
            "280%.",
            "So also using maybe some further human validation, the process of the mapping can be speed up a lot."
        ],
        [
            "The resource even relieved on their pedia website.",
            "There be the name of the project.",
            "And we consider 31 languages A for which only 15 exist in DB pedia and our resource container 45,000 mappings.",
            "For properties and we release with with a threshold Lambda on oh point 3."
        ],
        [
            "Hey conclusion, a with our work we created new DPI mappings, an extending the existing VPN mappings and creating new one for new languages will release their source, has an open source package.",
            "In the future we want to extend our approach to every possible language for which we should exist, and we also want to try to infer the classes mapping from the properties to answer the work we already done on classes.",
            "So I finished thank you very much."
        ],
        [
            "Nation.",
            "OK, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My paper is a towards an automatic creation of localized versions of DB pedia, and it's a joint work with cloud Rihanna and Albert Lavalley from from the center broadcaster in Trento, in Italy.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's start our goals is to extend the coverage of the paideia mappings on existing editions of existing chapters in existing languages and create new mappings for new languages.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we want to do it automatically.",
                    "label": 0
                },
                {
                    "sent": "Bich",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the pedia is build by mapping Wikipedia influx 2.",
                    "label": 0
                },
                {
                    "sent": "They they DP.",
                    "label": 0
                },
                {
                    "sent": "Ontology VPN Ontology is created manually by the by the community and the mappings are created manually too by the PDF community.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The mapping task is a illustrated here, so then infobox Sportiva that in Italian means sportsman is manually mapped to the class that pedia class sportsman an the attributes in the Wikipedia infobox.",
                    "label": 1
                },
                {
                    "sent": "Sportiva are then manually mapped to the DB Pedia properties to the corresponding properties.",
                    "label": 0
                },
                {
                    "sent": "There maybe attributes that don't have a property in Libya and vice versa an.",
                    "label": 0
                },
                {
                    "sent": "From now on, when we speak about the attributes, we refers to Wikipedia an when we speaks about properties, we refer to the pedia just to don't misunderstand the items.",
                    "label": 0
                },
                {
                    "sent": "A the part concerning classes has already been done in other research papers, so today we will concentrate only on properties.",
                    "label": 0
                },
                {
                    "sent": "So our work is only related to the mapping between Wikipedia attributes and the beta properties.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "After this mapping task from the deep community, there is a set of scripts called Extraction Framework also developed by DB Pedia, that extract this information from the Wikipedia dumps and you with them to populate the paleontology.",
                    "label": 0
                },
                {
                    "sent": "So the mappings are applied by the scripts to populate the ontology.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The this is a.",
                    "label": 0
                },
                {
                    "sent": "This parting has some issues because mappings are made manually, so if we want to make a new new chapter of the Pedia in a new language, we need to start from scratch and do them do the mapping again.",
                    "label": 0
                },
                {
                    "sent": "And also there is a problem of homogeneity over the info box in different languages.",
                    "label": 1
                },
                {
                    "sent": "Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "So as most frequent infoboxes are Muppets first 2 increase the coverage, some info box are missing are Mr Becauses they appears too few times.",
                    "label": 1
                },
                {
                    "sent": "He exists.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Approaches to this automatic mapping between Wikipedia are done using infobox alignment.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Starting from an infobox in Wikipedia and.",
                    "label": 0
                },
                {
                    "sent": "A any folks in other people language, for example English, usually is English.",
                    "label": 0
                },
                {
                    "sent": "We map the first map, the division administrativa.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In focusing this example to the settlement info box in English, because a settlement in English means division administrative in Italian.",
                    "label": 0
                },
                {
                    "sent": "So if there exists a mapping between the English infobox settlement and the classic PDP class settlement, we can infer that.",
                    "label": 0
                },
                {
                    "sent": "Also the reason I'm interested in Italian can be mapped to settlement.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In an indie pedia.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "While this works for settlement, for example, maybe in some languages there is no homogeneity in the in the infobox.",
                    "label": 0
                },
                {
                    "sent": "So for example in the page speaking about Madrid in English has the infobox settlement and the Italian while in French has the infobox cone despond that means Spain is finished down.",
                    "label": 0
                },
                {
                    "sent": "That is not that cannot be aligned with any corresponding English info box.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another problem is that sometimes the one page can have more than one infobox.",
                    "label": 0
                },
                {
                    "sent": "So for example, the Italian page of Barack Obama has two different infobox acharacle Publica Nbo one in English they said it has officeholder and person data also, in this case officeholder can be mapped to character publicain person data camera to be.",
                    "label": 0
                },
                {
                    "sent": "But if we look at the page mapping soft person data, an officeholder we see that.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Office folder is mapped to the office Holder class in the pedia person that is not mapped out at all, so we lose all its properties.",
                    "label": 0
                },
                {
                    "sent": "But this is OK in English because for example, play date of birth and place of birth are repeated in the office Holder info box by the policies of the English Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "In Italian this does not happen, so if we only consider.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The the Karika Publica and officeholder Infoboxes we lose the birth data information.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our approach provides that too.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then ignore the infobox at all and we only consider.",
                    "label": 0
                },
                {
                    "sent": "Properties.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in the example we before we completely forgot about attributes about infobox.",
                    "label": 0
                },
                {
                    "sent": "Me only consider attributes in the Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "Wikipedia for books.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We don't even need people language as we only consider DB pedia as to compare the values.",
                    "label": 0
                },
                {
                    "sent": "So in our.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In our approach, we only consider Wikipedia attributes and the DP that affect already existing.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A I think in the end we want to provide the mappings between Wikipedia attributes and IBD properties, and we compare who we might compare all attributes in Wikipedia to all properties in the paid and.",
                    "label": 0
                },
                {
                    "sent": "But this is a very time consumption because we need to do every possible compare.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Reason.",
                    "label": 0
                },
                {
                    "sent": "But Fortunately in a we do this comparison instance by instance only.",
                    "label": 0
                },
                {
                    "sent": "Only some some attributes in the Wikipedia page are compiled an only some properties in the pedia are filled, so the comparison are less than expect.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For example, in the Barrack Obama page, we compared Logan Asher with birthplace birthdate and this place and so on.",
                    "label": 0
                },
                {
                    "sent": "And Madrid we compare super feature that means Aria total with RF total population, total and elevation.",
                    "label": 0
                },
                {
                    "sent": "But we do not compare super feature with birthplace because Madrid has not had birthplace.",
                    "label": 0
                },
                {
                    "sent": "A the compr.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Prison is done.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On values, so starting from the values of attributes in, for example, in Barack Obama page, for example, only in this case we want to compare that value with all the values in the chorus.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Funding the pedia page.",
                    "label": 0
                },
                {
                    "sent": "So we compare on along with birth date with Alma Mater and with birthplace etc.",
                    "label": 0
                },
                {
                    "sent": "To do this comparison of in for value.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We created a similarity function that takes as an input the attribute in Wikipedia and the property in DB pedia.",
                    "label": 1
                },
                {
                    "sent": "The attribute in Wikipedia is famous structured data.",
                    "label": 0
                },
                {
                    "sent": "It's free Tax day is not typed, while the property in the paper is structured data and we will use both information for our comparison.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In fact, we create 5 similarity function that depends on the structure of the attribute in Wikipedia and the type of the property in DB pedia.",
                    "label": 0
                },
                {
                    "sent": "The first",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of that is date comparison.",
                    "label": 0
                },
                {
                    "sent": "So when the DP their property is that data we check on the Wikipedia property.",
                    "label": 0
                },
                {
                    "sent": "If there are some parts of the date, for example they in math or only the ear on the month year and we give a value from zero to 1 to this similarity function based on the entities found in the Wikipedia text.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second function is the template expansion because sometimes in the attributes value there is a template.",
                    "label": 0
                },
                {
                    "sent": "So we used Wikipedia, the Mediawiki parser to parse this template and dropped.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Been there.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A final value in this case for out 1971, and then we can compare it with the values in the DB pedia in the Peter properties values the third.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Chinese link translation.",
                    "label": 0
                },
                {
                    "sent": "So for example Italian.",
                    "label": 0
                },
                {
                    "sent": "the Italian Wikipedia has.",
                    "label": 0
                },
                {
                    "sent": "And played for the countries, in this case ESP means Spain.",
                    "label": 0
                },
                {
                    "sent": "We part the template as before.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are we obtaining Spagna Spagna in the Italian name of?",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Spain, but now we have to compare Spanner with pain and it's difficult to compare.",
                    "label": 0
                },
                {
                    "sent": "So we use a cross language links in Wikipedia to track.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Foreign Spagna into Spain.",
                    "label": 0
                },
                {
                    "sent": "So now we can.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Compare Spain in the DB pedia properties value.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First function is string similarity.",
                    "label": 0
                },
                {
                    "sent": "Sometimes the type of the property is text for example.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mona Lisa, the technique used to paint the Mona Lisa is in Italian P. $2 tavola, while.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "English is oil and popular, and these strings are not compatible in that in that way so.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We again extract the links to Wikipedia pages from the text and.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Convert them, we translate.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Them into the language on using in DB pedia.",
                    "label": 0
                },
                {
                    "sent": "In this case in English.",
                    "label": 0
                },
                {
                    "sent": "The then.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We replaced this translation in the original text and now we.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then compare the two text and we have two common words.",
                    "label": 0
                },
                {
                    "sent": "To compute the similarity in general we use string kernels that takes into account two strings and looks for contiguous and non contiguous subsequences of tokens, and they have in common the two string have in common.",
                    "label": 0
                },
                {
                    "sent": "The 5th.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The function is number approximation.",
                    "label": 0
                },
                {
                    "sent": "For example, in this case it's population of.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Read the hint.",
                    "label": 0
                },
                {
                    "sent": "the Italian Wikipedia has a particular value while.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the English, the Pedia has a different value.",
                    "label": 0
                },
                {
                    "sent": "This may happen because maybe the the Wikipedia values is updated with respect to the deep value.",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Set a threshold.",
                    "label": 0
                },
                {
                    "sent": "In this example .9.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We calculate the ratio between the two values and if the.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Show is a greater than the threshold.",
                    "label": 0
                },
                {
                    "sent": "We can we take into account the value.",
                    "label": 0
                },
                {
                    "sent": "We consider the similarity as one, otherwise we consider it at 0 after that.",
                    "label": 0
                },
                {
                    "sent": "So after we collect this similarity values for each page.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We calculate an average.",
                    "label": 0
                },
                {
                    "sent": "So for every every entity, if we found an end to control, calculate a global similarity of the attribute and property attribute in Wikipedia and the property in the page.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this case, for example, in Barack Obama, Logan.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That is compared to birth place and give one etc.",
                    "label": 0
                },
                {
                    "sent": "So we then a sum and divide it all the pairs, Wikipedia, attributes and DP.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Party.",
                    "label": 0
                },
                {
                    "sent": "And we obtain the average.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sometimes hours.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ethan can give the wrong answer.",
                    "label": 0
                },
                {
                    "sent": "So for example.",
                    "label": 0
                },
                {
                    "sent": "Anyway, Now's was born and died in London, so the comparison between Rogan Ashton displays is 1.",
                    "label": 0
                },
                {
                    "sent": "OK, he's one but a.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Considering a lot of entities, the average is less because the Clint Eastwood and Barack Obama didn't died in the same place where they was born.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "After that, after come computing this global similarity, we set another threshold and if the value is lower than the threshold, we discard the possible mapping.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Otherwise we select the.",
                    "label": 0
                },
                {
                    "sent": "The infobox that maximize this function we took the argmax.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the previous example, for example, if we set Lambda to open.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Three, we have two different.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Possible mappings for Logan Ashton for birthdays in this place, but we only consider them.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The highest one, so our guess.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that Logan Ashton Italian can be mapped to birthplace in DB Pedia in the Italian Wikipedia up to birth place in DB Pedia.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the experimental settings, we considered the paid 3.8.",
                    "label": 0
                },
                {
                    "sent": "So we did our experiment on the Italian Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "We annotated Bonelli 100 attributes in Wikipedia, with three annotators having 91% of agreement using Face Kappa measure, and then we try different values of Lambda.",
                    "label": 0
                },
                {
                    "sent": "Different values of the threshold.",
                    "label": 1
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we can see this is precision or equals charge.",
                    "label": 0
                },
                {
                    "sent": "So the epsilon is precision and the exes recall and we can see that on high Lambda some performance we have the same performance of the model mapping.",
                    "label": 0
                },
                {
                    "sent": "The blue dot is a manual mapping considering the existing mappings in Italian, but an if we lower the Lambda threshold we lose something in precision, but we can increase the recall.",
                    "label": 0
                },
                {
                    "sent": "280%.",
                    "label": 0
                },
                {
                    "sent": "So also using maybe some further human validation, the process of the mapping can be speed up a lot.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The resource even relieved on their pedia website.",
                    "label": 0
                },
                {
                    "sent": "There be the name of the project.",
                    "label": 0
                },
                {
                    "sent": "And we consider 31 languages A for which only 15 exist in DB pedia and our resource container 45,000 mappings.",
                    "label": 1
                },
                {
                    "sent": "For properties and we release with with a threshold Lambda on oh point 3.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hey conclusion, a with our work we created new DPI mappings, an extending the existing VPN mappings and creating new one for new languages will release their source, has an open source package.",
                    "label": 1
                },
                {
                    "sent": "In the future we want to extend our approach to every possible language for which we should exist, and we also want to try to infer the classes mapping from the properties to answer the work we already done on classes.",
                    "label": 0
                },
                {
                    "sent": "So I finished thank you very much.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nation.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                }
            ]
        }
    }
}