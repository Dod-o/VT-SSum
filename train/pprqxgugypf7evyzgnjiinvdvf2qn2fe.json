{
    "id": "pprqxgugypf7evyzgnjiinvdvf2qn2fe",
    "title": "Approximation and Inference using Latent Variable Sparse Linear Models",
    "info": {
        "author": [
            "David P Wipf, Biomagnetic Imaging Lab, University of California, San Francisco"
        ],
        "published": "Feb. 1, 2008",
        "recorded": "December 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Linear Models"
        ]
    },
    "url": "http://videolectures.net/abi07_wipf_aiu/",
    "segmentation": [
        [
            "Alright thanks everyone for coming.",
            "This is joint work with seven.",
            "Several of my collaborators, mostly Jason Palmer, at who's a postdoc at UCSD."
        ],
        [
            "So, so this is an overview of the talk I'm going to give today.",
            "Basically it's about sparse linear models, so I'll go over exactly what I mean by that, and these are generally difficult to deal with in approximate Bayesian setting.",
            "A lot of the required distributions are intractable, so but there's a latent variable representation of sparse priors that leads to four possibilities for approximation that I'm going to talk about.",
            "There's other ones I guess people have been talking a lot about EPA.",
            "I'm not going to talk about that, but these are the four I'm going to be considering.",
            "There's standard map estimation of the unknown model weights.",
            "There is map estimation of model hyperparameters, often called type 2.",
            "There's local variational approximations using convex lower bounding, and there's global variational approximation.",
            "What I called variational Bayes and a lot of people do as well.",
            "So these are four different methods, but I'm going to show basically they can all be unified and there are special cases of hyperparameter map using different implicit hyper priors.",
            "So basically, once you kind of unify and see the similarities between these methods, then you can decide how to choose this different hyper priors, which is basically what distinguishes these different methods.",
            "And basically how you choose this hyper prior will depend on the application and the two.",
            "I'm going to look at is finding maximally sparse representations, sparse coding and our VM's are kind of special cases and this is something I did a lot of my PhD work on and then active learning and experimental design is something new that has very different requirements on how to pick this implicit hyper prior.",
            "So these are kind of two contrasting applications that affect how you do this approximation, and then if there's time I'll talk about some extensions.",
            "To nonnegativity constraints, can variance component estimation and."
        ],
        [
            "Classification so.",
            "So here's the basic sparse linear model.",
            "You have some observed data T. You have known matrix of feature features Fi and.",
            "Then you have some unknown model weights W an.",
            "Of course some noise.",
            "I'm going to assume that this makes these features are known, but all the methods I talk about today, you can add a dictionary learning step and learn this matrix as well blindly if you want.",
            "So the likelihood model we assume is just Gaussian here and we have a Gaussian data likelihood and then we have a sparse prior here which is factorial, meaning that it's independent across each of the different coefficients and the key property that we assume to kind of rigorously define sparsity is that it can be expressed in this form or this function G is nondecreasing an it's concave, so this kind of is a generalized notion of sparsity that encompass is all the.",
            "Common sparse priors people use like student, T, Jeffreys, prior, Laplacian.",
            "All these sorts of things."
        ],
        [
            "So here's just a quick picture to show a sparse priors look like.",
            "Here's a Gaussian.",
            "It kind of distributes all the probability mass uniformly about the the origin.",
            "And then here's a sparse prior, which has a big peak at zero, but then it has very heavy tails, so it encourages weights that are either 0, or maybe have some big non 0 value."
        ],
        [
            "So practical issues with this model is that basically the joint distribution given the likelihood in the prior is is often very highly multimodal.",
            "There are certain cases where it's not, but in a lot of the applications we look at it, it is very multimodal, even if it's not, it may be difficult to evaluate where the predominant mass is located.",
            "An normalization is intractable, so you can't compute the model evidence and you can't normalize.",
            "So you need some approximation to the posterior and.",
            "Maybe the model evidence as well?",
            "I'm going to look mostly at approximations to to this."
        ],
        [
            "So latent variable models of sparse priors.",
            "It turns out that sparse priors have a nice structure which is useful both from an analysis standpoint and also from an implementation standpoint, and basically all those.",
            "All the sparse priors that are of the form I mentioned earlier, with that G that's square concave can be written in either of these two forms.",
            "The first is called a Gaussian scale mixture, and it's basically it's called a scale mixture because the mean here is zero of this Gaussian Ann, you're just integrating over different scales modulated by some hyper prior.",
            "So this is one way and then slightly more.",
            "General Way is convexity based representation, where you representing each prior as a supremum over Gaussians and then this is some basic arbitrary function that controls the different scales and basically all sparse priors can be represented in both forms.",
            "This one is a little more general general there's some special cases where this one doesn't apply, but basically for all the priors you've probably come in contact with, you can represent either way.",
            "An for basically any non negative function for this prior an for this fee term there the resulting prior kernel.",
            "I guess G will be non decreasing and concave so it'll be sparse.",
            "So these two representations are kind of intrinsically produce sparse heavy tailed distributions."
        ],
        [
            "So just to see the former ones, probably more common, but just to see kind of in."
        ],
        [
            "But if Lee, what this one looks like, here's the outer envelope of a sparse prior, and you can see that this scaling factor just scales a bunch of different Gaussians that go inside of this.",
            "And if you take the upper outer, the upper envelope of all of these different Gaussians, you basically get the."
        ],
        [
            "True prior, so that's how this."
        ],
        [
            "Maximization can kind of give you a sparse representation."
        ],
        [
            "So there's four possibilities for approximation using this latent variable structure that I'm going to talk about today, and the first I mentioned was standard map estimation of these unknown ways.",
            "An if you can develop very nice EM algorithms using either representation where you consider these."
        ],
        [
            "Latent variables these unknown variances.",
            "Here you consider them as."
        ],
        [
            "Is hidden data and you can do nice EM algorithms for basically any sparse prior using either type of representation.",
            "The second thing you can do is map estimation of the unknown hyperparameters, and that you typically assume the Gaussian scale mixture and you can.",
            "Come up with an ICM.",
            "Algorithms as well.",
            "Then there's local variational approximation convex bounding and that requires the convex representation.",
            "Not surprisingly, and then global variational approximation or variational Bayes use the scale mixture representation of the prior."
        ],
        [
            "So I'll go through each of these in a little bit more detail and I guess this is a workshop, so someone has a question in the any.",
            "Something's not clear at any point.",
            "Feel free to interrupt.",
            "So basic method one is just map estimation of the weights.",
            "Nothing surprising here.",
            "You have likelihood prior and you have this G which is concave in nondecreasing.",
            "And so you basically just optimize this objective function an if you want to call it a Gaussian approximation you can say, well, we're approximating the true posterior with a Gaussian degenerate Gaussian with no variance, and it's just a Delta function at the map estimate you get here, and I'm not going to go into the EM algorithms for doing this, but.",
            "Again, you can do it for basically any concave nondecreasing G here."
        ],
        [
            "The second one is hyperparameter map, and so this is a little bit different.",
            "Here we integrate out the unknown weights and we're optimizing the posterior over the hyperparameters gamma.",
            "So if you do this integral, basically you get.",
            "Here's the prior, the hyper prior, so that gives you this term over here, which is just the negative log of the hyper prior, just some function, and then these two terms come from.",
            "Basically, when you do this integral, you get another Gaussian which is 0 mean.",
            "And it has covariance given by this term here.",
            "So this is basically a Gaussian covariance, but it's a highly nonlinear function of these hyperparameters, so there's a vector, one for each weight, and this capital gamma is just a diagonal of all these hyper priors.",
            "This first term shouldn't be 0.",
            "This should be gamma, one.",
            "So this is just a vector of all the hyper prior variances, one for each weight.",
            "And this is the cost function you get by gamma map.",
            "The only thing.",
            "Specified is what you pick for this function F. And once you optimize this, you get some map estimate for this hyperparameter vector.",
            "Then you get an approximation which is also Gaussian.",
            "Was given with a mean mu Anna covariance Sigma an.",
            "These are computed in closed form.",
            "Once you're given these unknown hyperparameters that it's available in closed form, the mean is given by this kind of standard expression, and the covariance is given by this so.",
            "The what you did once you.",
            "Once this is the hard part here.",
            "Once once you get these hyperparameters you, you're basically done.",
            "You have a nice posterior approach."
        ],
        [
            "Estimation.",
            "So the third method is convex bounding and the convex type representation remember represents each unknown coefficient as the supremum over Gaussians.",
            "So if you remove the maximization, you get an approximate prior, which is like a lower bound on the true prior and it's given by this expression.",
            "It's just a scaled Gaussian which obviously doesn't integrate to one, but it's a useful lower bound, and this gives in a family of approximate distributions for the joint distribution.",
            "Given by this expression, which again is a lower bounding approximation.",
            "So basically you want to optimize this lower bound.",
            "One thing you could do is minimize the sum of the misaligned mass.",
            "So you just integrate over the absolute value of the difference between the two distributions.",
            "And since this one is a lower bound on this one, you can remove the absolute value and just maximize this which is called the model evidence in a lot of cases.",
            "So basically once you.",
            "Do this and you solve for the optimal hyper."
        ],
        [
            "The moments you get are exactly the same."
        ],
        [
            "What you had in the previous case so.",
            "It's it's pretty similar idea I guess."
        ],
        [
            "The last method is a variational Bayes.",
            "Basically, here you look at the posterior of the complete data of the both the distribution of the unknown weights and these unknown hyperparameters given the hyper prior an.",
            "Assuming the data is observed an this is typically multimodal, an intractable as well.",
            "So variational Bayes says let's form a tractable approximate distribution.",
            "So basically you want to approximate this full joint distribution by a factorization over an approximate distribution, just in terms of the model weights, and then just in terms of the hyperparameters.",
            "So besides this factorization that the forms of these two distributions is unspecified and that falls out of the modeling and basically you optimize these two distributions by minimizing the KL divergent between a factorized functional form and then the true joint distribution, and you minimize over these two via coordinate descent, and you get 2 approximate distributions.",
            "So and then once you get these two then you just assume that the true distribution.",
            "Is approximated by this one an it's again, it turns out to be a Gaussian even though you didn't specify when you were doing this, that it had to be Gaussian.",
            "It turns out that the one that optimizes this expression will be Gaussian, and again the mu and the Sigma from before are basically up."
        ],
        [
            "The same form as we had back here, only you replaced the map Esta."
        ],
        [
            "Here with now, the optimal.",
            "Basically, the expected value of these hyperparameters given this approximate model, and you can do all of this in closed form."
        ],
        [
            "So we have now four different approximate methods for dealing with this kind of simple hierarchical Bayesian model, and but how different are they and what do you really get by these different methods?",
            "An I guess this is kind of important 'cause if you're trying to choose which one to use, it's kind of nice to know how they differ or if any of them are the same or this sort of thing.",
            "So the first thing if we allow for improper hyper priors here, then by inspection, basically the gamma map.",
            "Which I called method two in the convex bounding given equivalent approximate distribution when we just make this equality here, the associated true posteriors will generally be different, but the approximate method you get out of these two methods is identical, so you don't really get a new class of approximate models and.",
            "So the second one, which is not as obvious, is that VB and convex bounding give equivalent approximations to a fixed posterior distribution, so the underlying hyper prior and then this scaling functional will not be the same, but the."
        ],
        [
            "Approximate distribution you get here."
        ],
        [
            "Will be identical and this is not something I guess that's widely assumed, but these two methods actually give you an identical result.",
            "So the conclusion is that gamma map just forming some finding some map estimate of these hyperparameters encompases all the approximations you can get out of VB or convex bounding.",
            "So you're really not getting anything new by doing these two, it's only a different interpretation.",
            "Maybe of what's being optimized, but you don't actually get something new in terms of cost functions or."
        ],
        [
            "Approximations.",
            "A final thing, what about just doing map estimation over the weights themselves?",
            "This is actually also a special case.",
            "If the map is some solution to any map W map problem, then there exists a limiting gamma map problem that produces a posterior approximation given by this, which is just a Delta function at W map.",
            "So again, this is actually a limiting case of hyperparameter map, even though one is optimizing in weight space and one is optimizing and hyperparameter space.",
            "This is actually just a special case of that.",
            "Again, the associated true posterior and the associated true underlying hyper prior will not necessarily be the same, but the approximation that you get will be the same."
        ],
        [
            "Name?",
            "So the summary, I guess at the end of this is that you have four seemingly very different approximation strategies, but they can all be viewed as special cases of just doing standard hyperparameter map estimation or evidence maximization.",
            "So the only real distinction is this choice of the F which is embedded in the hyper prior, and what you choose for that.",
            "That's the only thing that separates any of the cost functions you get out of these different methods, or the approximate models you get at the end of the day.",
            "It's the only real distinction is this F. So I would argue rather than decide whether you should do convex bounding or variational Bayes or something else, the realist you should just decide is what should you pick for this function.",
            "F and the kind of general principles I would think of when considering it is that it's better to pick a good value for this function consistent with your application and I'll talk about this more later.",
            "It's better to pick a good value for this function then to pick a plausable prior to start with and then work backwards through VB or convex bounding or something like that.",
            "And if the ultimate goal is a good posterior approximation, maybe an absurd full model leads to a very useful approximation, and that's fine.",
            "So maybe the optimal F for your application is corresponds with kind of ridiculous full model, and I'll give some examples of that, but it doesn't matter if you get a good approximation out of it for the real world data you care about.",
            "So the two issues I would look at for deciding how to pick this F or basically optimization considerations and then application independent requirements.",
            "These are basically the two things that I would say kind of decide how to go about choosing this F 'cause so far it's been basically."
        ],
        [
            "Arbitrary.",
            "So first of all, from an optimization standpoint, if F is concave, you can implement this hyperparameter map using efficient reweighted L1 minimization procedure.",
            "So for any concave differentiable F, so that's a consideration if speed and efficiency of implementation are important.",
            "If this F that you want to use is not even available in closed form, you can still, but you can compute from the original prior the gradient of this G function with respect to the argument there.",
            "Then you can still do em even if you don't know the exact form of this.",
            "This F if it's not available in closed form and you get a very simple M, you compute this mean in this Sigma with this standard equations I showed before and then the M step.",
            "All you do is optimize the hyperparameters by sending them.",
            "Equal to this gradient evaluated at the previous values you computed here.",
            "So basically for this this method ends up being much slower than this, But if you can do this, you can.",
            "Basically it's very general.",
            "You can for any value of F, so it's kind of nice.",
            "And then finally there's greedy methods that you can do.",
            "Tipping and fall have like a incremental greedy method for implementing this type of thing in certain special cases, so this could be a concern.",
            "Depending on the size of your problem and what you're really after, these these kind of ideas."
        ],
        [
            "So here's two example applications.",
            "One is finding maximally sparse representations.",
            "This I do brain imaging, and there's a lot of applications of this to localising brain sources and stuff like that, and sparse coding and relevance vector machines kind of are in this same genre of application, and then I won't talk much about this 'cause I don't know too much about it, but experimental design.",
            "For example finding non random projections for compressed sensing.",
            "This is an area where you have.",
            "Fair."
        ],
        [
            "Different requirements for."
        ],
        [
            "For how to choose this F, then you do here, so I'll talk about choose."
        ],
        [
            "F in these two situations, so maximal sparse sparse is basically kind of formal problem studied by a lot of people in information theory and elsewhere in the noiseless case.",
            "The idea is to solve this minimization problem, so we're assuming this file here is over complete, meaning there's more more columns than rows, so there's an infinite number of solutions to this linear problem, and you want to find the solution with minimal L zero norm, which means you want.",
            "Most of that you want to find the solution to this equation such that most elements of this W are equal to 0.",
            "And the noisy case you relax the requirement of equality here and you just solve this this problem here where Lambda is a trade off parameter balances sparsity and data fit.",
            "So the problem with encountered by everyone who wants to solve this, and again there's a lot of applications to this, is that there's a common material number of local minima, and then the objective function is discontinuous.",
            "Because this is just a count of non zero elements and the forward model.",
            "Yeah, it's very simple to.",
            "Go forward given a sparse vector, compute what data would be observed with some noise.",
            "But again, the inverse problem is."
        ],
        [
            "Is intractable.",
            "This is just a toy example of what sparse solution is.",
            "What you would be looking for.",
            "Here's data vector toy matrix, and you want to find a W that solves this equation.",
            "And here's a non sparse solution.",
            "An if you actually sit down, it does equal this I think, and this is a very sparse solution with mostly zero valued entries.",
            "So this is the kind of solution you're looking for as opposed to this."
        ],
        [
            "So how do you find this double?"
        ],
        [
            "Not this this."
        ],
        [
            "Maximally sparse solution here.",
            "Using this framework, I've been talking about the key idea is to choose, you need to choose an appropriate hyper prior, and particularly the energy here this F, and once you choose some F, which I'll talk about.",
            "Maybe some ideas how you might go about choosing it.",
            "Then you need to compute the map estimate of the hyperparameters, and then once you have those you have this Gaussian approximation.",
            "You assume the mean is.",
            "That's what you use."
        ],
        [
            "Is your approximation?",
            "So how would you choose this F for this problem?",
            "And here's two rather than say, well, this particular prior is sparse, so I'm just going to use this one and do variational Bayes or something more heuristic.",
            "We just go directly to this function and say whatever.",
            "What would be simple criteria for choosing this F to solve this problem?",
            "An one simple criteria is just to say if this maximally sparse solution only has one non zero element, then the associated cost function has a similar.",
            "A single minimum and the mean equals this kind of simple solution.",
            "So in words this is saying finding one non zero element should be very simple task, so that's that's kind of a pretty we."
        ],
        [
            "Requirement.",
            "And then the second criteria refers to local minima.",
            "We would prefer that all local minima produce approximations with this posterior mean such that the L0 norm of the mean, which is account of the non zero elements is less than or equal to the signal dimension.",
            "And this is just saying you should never require more than N nonzero coefficients to represent a N dimensional signal T, so these are not such."
        ],
        [
            "Strong criteria, but basically if you just have these two criteria then over all possible choices for this F that only this selection satisfies the two criteria.",
            "So kind of reduces the class of functions you might want to use for finding these sparse rapid representations vary considerably, and then sparse Bayesian learning, which is a machine learning technique by tipping and the lasso are special cases when Alpha goes to 0 or Alpha goes to Infinity.",
            "So and these are.",
            "Both work very well finding sparse representations, so maybe it's not surprising that there are special cases.",
            "But so I've been saying all you really need to do is pick this F function and do hyperparameter map.",
            "Yeah, go ahead.",
            "So.",
            "Two very different things too, right?",
            "Different.",
            "How different this solution?",
            "How how do the sparser presentation to get promise being invested compare, given that right?",
            "I mean the real tradeoff here between the Alpha is that when Alpha goes to 0, you get a a more local minima, but then the global minima is more likely to be the maximally sparse solution.",
            "So in the limit, when this Alpha goes to zero, you're guaranteeing that the global minima is good, but the price you're paying is you're adding lots of local minima and then here when the Alpha goes to Infinity.",
            "You're driving the cost function to a more convex form, but the price you're paying is that the global minimum of this new cost function that's getting more and more almost convex is that the global global minimum of it can be biased away from the maximum sparse solution, so that's the tradeoff that's happening when you move this Alpha back and forth.",
            "Continuous pop though as you move Alpha.",
            "A continuous.",
            "I mean that it will it doesn't.",
            "It won't necessarily be continuous.",
            "Now it will not.",
            "The cost function kind of varies continuously, but the minimum can jump, yeah?",
            "Actually, that's the way I implement SBL now is.",
            "I start with Lasso and then I do a re weighted lasso an convert.",
            "So that's kind of a separate issue.",
            "But the lasso can be a good starting point.",
            "I haven't tried though running varying this Alpha at different times.",
            "You could, but I haven't tried that."
        ],
        [
            "So one thing what I was saying before is that I've been assuming all that's different.",
            "Here is the F. All these different methods, convex bounding variational, Bayes, hyperparameter map?",
            "All these methods are really just different choices of this F, so it doesn't really change much, and it's true it doesn't affect the hyperparameters you get or the approximate distribution you get, but it does change the implicit full model that you started with, if that matters to you.",
            "So if you want to, if you choose to use this app for this problem and you're just doing standard hyperparameter map, then the implicit full model is a PMW that is Laplacian and a posterior that is log concave in.",
            "You know model, but if you assume that you're doing convex lower bounding or variational Bayes, the PFW, the implicit full model, whatever that means is highly sparse.",
            "You can compute this exactly like what what prior you would need here.",
            "To make convex lower bounding give you the exact same cost function as this, but it's it's a very different prior and posterior.",
            "It's highly multimodal and very, very sparse, so the implicit full model is definitely different, even though the cost function you're optimizing is identical.",
            "So."
        ],
        [
            "So I think it's kind of an important distinction.",
            "And then a note about SBL and relevance vector machines kind of ties in with.",
            "All of this is that in the original derivation of the relevance vector machine, the full model assumes a hyper prior like this, which is just a Jefferies and that gives you a weight prior like this.",
            "There should be absolute values around here I think, but basically.",
            "So this is, this is what the full model of of SPL starts with an exact inference with this model would be very undesirable because this has basically infinite mass right around 0, so the best probably approximate model to this full model would just be to put a Delta function at zero, which is kind of degenerate and useless.",
            "And if you applied gamma map directly to the full model, which implies that this F equals log of gamma when you do the negative log up here.",
            "It gives you a trivial solution.",
            "It gives you the gamma, the hyperparameter map estimate is 0, the mean is zero and the variance is 0, so it's kind of a good approximation to the full model, but the full model is kind of useless anyway, so SPL doesn't even do gamma map in log space.",
            "It actually it does.",
            "I'm sorry it doesn't do directly gamma map, it does it in log space, which kind of zeroes out this F function and that's why."
        ],
        [
            "Get good results and that's why you get back to here, because this Alpha."
        ],
        [
            "Is to zero.",
            "Basically you have F = 0."
        ],
        [
            "So.",
            "OK, so switching gears a little.",
            "That's one way you might want to choose F, But this is for sparsity and Matthias is pointed out to me several times, and it's definitely true you're getting something useful for finding sparse solutions, but you're not really there.",
            "The true distribution that you're working with would never have a mean with exactly 0 valued elements.",
            "In general, even a sparse prior.",
            "If you take the mean, it's not going to be exactly 0, and there's other.",
            "There's other applications where this is very desirable to have.",
            "The approximate model have the mean that's not perfectly aligned with the axes with individual elements exactly equal to zero with no variant, so.",
            "So experimental design is 1 case where you have a very different requirement for choosing this F so you don't have this issue with a degenerate Gaussian collapsing to the axis.",
            "So one idea proposed by sure how Jian others related to compressed sensing, is to use the approximate posterior you get from some gamma map.",
            "No matter how you chose the F to begin with and take this approximate distribution and then learn new rows of the design matrix Phi such that uncertainty about the weights is reduced and the cost function they propose is to add additional rows to minimize differential entropy.",
            "And so they want to minimize this function by adding rows here."
        ],
        [
            "So a big problem with this is exactly what I was saying before.",
            "If this F is concave, you can show that at least N -- N hyperparameters will be set to exactly 0 regardless of the data.",
            "So even if you have almost no data at all, if this F is concave, you're going to be setting hyperparameters to zero.",
            "And what does that do?",
            "If this gamma map is sparse, the posterior mean is forced to be sparse even if the desired posterior mean is far from sparse.",
            "So this gamma just refresh your memory is the prior variance of each unknown coefficient.",
            "So if one of these elements goes to 0, then the associated coefficient is held to zero and it can't deviate, and likewise the posterior variance is how the zero as well.",
            "So the posterior mean is forced to be sparse and there's no posterior uncertainty in the associated 0 value coefficients.",
            "So even with limited data, the model is assigning zero posterior variance to these coefficients.",
            "So how do you reduce uncertainty?"
        ],
        [
            "Beyond that.",
            "So this is just very heuristic thing you could do and they implement some form of this in their paper.",
            "Just as a post processing step almost but basically one way you could do it within this model is to choose a different value for this F function.",
            "So the F that you can show falls out of the variational relevance vector machine from Bishop and Tipping is one that looks like this and basically if any of these gammas go to zero this function goes to Infinity so it's preventing.",
            "Any of the hyperparameters going to 0 or even near 0, unless you have a lot of data supporting it, so this this would prevent that problem.",
            "But I'm not going to say it's going to work great in practice, but it's kind of a slightly more principled way to prevent this from happening.",
            "It could be useful for experimental design."
        ],
        [
            "So now I'm just going to briefly how much time is left here?",
            "OK, well I'll just briefly go."
        ],
        [
            "I'll just go through this really fast non negative sparse coding is the same sparse problem I talked about before, only that you constrain the weights to be greater than zero and the problem is that it's intractable.",
            "Even compute this posterior distribution to do hyperparameter map.",
            "So you have basically an intractable cost function and then the posterior moments given some hyperparameters are also interact."
        ],
        [
            "So.",
            "One way to get around this is you can show that the posterior mean you get from regular map estimation.",
            "What I've been talking about before it satisfies this equation where this G is given by a concave conjugate.",
            "I won't go into that now since I guess we're running out of time, but basically you can show that the posterior mean you got from the regular hyperparameter map you were doing before is just given by minimizing this cost function, and this G will end up being a concave penalty, so it's giving you sparsity.",
            "So if the.",
            "Posterior mean in the regular cases given by minimizing this, it's actually really easy to add a non negative non negativity constraint here and then you can optimize this cost function directly using a non negative lah."
        ],
        [
            "So and here's a just a quick toy problem to say show that this works pretty well in pretty quickly as you can generate here you generate some data, so this is a sparse weight vector.",
            "It's generated by implicit wait prior looking like this, with the Delta function at zero and then a Gaussian that's positive and cut off on the other side, and so you run the proposed algorithm and you try and learn what this W is given the T."
        ],
        [
            "The five, so here's an example.",
            "You initialize with the non negative lasso and with just a couple related iterations you can basically do."
        ],
        [
            "Much, much better by minimizing this subject, this sub."
        ],
        [
            "The function, so maybe within this is 1 related iteration, second third, 4th.",
            "You can basically converge and go from 30 some percent success to 90% success.",
            "Recovering these sparse coefficients so."
        ],
        [
            "It works pretty well."
        ],
        [
            "I'll skip this stuff and just say some final thoughts.",
            "Maybe in the context of the sparse linear model there's a variety of approximate inference methods.",
            "But they can all be reduced to hyperparameter map estimation using some implicit hyper prior.",
            "An note you can do inference without worrying about conjugate hyper priors.",
            "You can do a large class of these F of gammas.",
            "It's so it's pretty pretty convenient, but the choice should be application dependent and focus on the underlying cost function, not the plausibility of the full model.",
            "And here's something I throw out here.",
            "I don't know much about EP, and this could be ridiculous, but I would say that you can probably Terrell or this F to get similar Gaussian approximation to EP.",
            "Maybe in some special cases, but the key is that you wouldn't want to start with the same full model because a lot of times when you start with the same full MoD.",
            "And then you do variational Bayes or convex bounding.",
            "You get an F. That's kind of ridiculous.",
            "And if you were to compare it with EP, you might not get a good comparison.",
            "But if all you're concerned about is tailoring this F to such that the final approximation is close to the EP approximation, maybe in some cases you could do it so."
        ],
        [
            "That's it.",
            "It's sort of interesting with the unification of the approximation, it's almost like.",
            "Most of the time when we're working with the model that we want to approximate, the model stays constant and the approximation change, right?",
            "It's the opposite, right?",
            "Approximation is always ending up at the, so the right is that because you say it's more problem driven, but you're just interested in sparsity and you don't care about, say, probabilistic predictions and things like that, well, I think some of it just falls out of the math of these models.",
            "Is that the approximate the full models are not unique for a given approximation, if you assume you were doing VB, you wouldn't get this full model if you assume you're doing convex bounding, you would get this format, but the approximation is the same, and for what we do.",
            "The approximation is what matters because the full model is just something you make up anyway.",
            "It's just something to start with, and so a lot of these you just tailor the full model until you get some approximation that's nice.",
            "And then that's what what you go with an in sparsity, that's what.",
            "How it tends to workout for these maximally sparse things so I don't know?",
            "Anyway.",
            "Using sparse in terms of finding qualitative because first is desirable from like computational point of view and.",
            "Mate.",
            "I don't see why.",
            "First, the nature might be actually right.",
            "Well, it also depends on what you're using sparsity for.",
            "If you're just using it to predictive purposes, maybe so, but a lot of times the sparse coefficients have physical significance, so we use it for localising brain sources so you care about fitting the data.",
            "But the real point is that sparsely profiled, the vector you learn corresponds with brain sources, so the sparsity is absolutely essential because.",
            "You're most parts of the brain are inactive for certain experiments, and the sparsity profile corresponds with the active locations, so you could predict the observed data an infinite number of ways, but neurophysiologic Lee, you are, sparsity is what you want, and this carries over to a lot of areas of signal processing, like sonar radar applications of localising sources.",
            "So there are results though.",
            "Related to theoretical performance of estimating sparsity profiles and sparsity and bounds on representing the data and all these sorts of things.",
            "But for the applications we're looking at, the sparsity assumption is pretty justified I guess, so you don't have to worry about exactly what you're talking about, so.",
            "In the end.",
            "Shouldn't.",
            "Worry about the possibility of any of the other line moves with you.",
            "Would you agree with following statement that sparse Bayesian learning is not facing?",
            "It's not fully Bayesian and it's based on a absurd full model.",
            "The one that actually works, so there's a lot of different variants I guess, But the one that works that everyone uses has an absurd full model.",
            "So yeah, it's not.",
            "It's not fully model beige, and I hope that's not offensive, but.",
            "Some.",
            "Important models things like slab spike and slab priors mean they don't seem to fit in this sort of scaled framework, so it's gotta be shared.",
            "Frameworks that means do you have any comments about that kind of so?",
            "These are kind of.",
            "Right, so I guess I'm not exactly sure of the specific prior, but there are some differentiability constraints when you do the scale mixture representation of a prior, but those basically go away with the convex type representation.",
            "So you can probably represent them that way and then do the same analysis assuming that there.",
            "When you take the the negative log, their square is concave and non decreasing.",
            "It's OK if it's non differentiable and as bizarre as you want, but as long as it has those properties you can represent represent it as a scale mixture so."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright thanks everyone for coming.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with seven.",
                    "label": 0
                },
                {
                    "sent": "Several of my collaborators, mostly Jason Palmer, at who's a postdoc at UCSD.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, so this is an overview of the talk I'm going to give today.",
                    "label": 0
                },
                {
                    "sent": "Basically it's about sparse linear models, so I'll go over exactly what I mean by that, and these are generally difficult to deal with in approximate Bayesian setting.",
                    "label": 0
                },
                {
                    "sent": "A lot of the required distributions are intractable, so but there's a latent variable representation of sparse priors that leads to four possibilities for approximation that I'm going to talk about.",
                    "label": 1
                },
                {
                    "sent": "There's other ones I guess people have been talking a lot about EPA.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to talk about that, but these are the four I'm going to be considering.",
                    "label": 0
                },
                {
                    "sent": "There's standard map estimation of the unknown model weights.",
                    "label": 1
                },
                {
                    "sent": "There is map estimation of model hyperparameters, often called type 2.",
                    "label": 0
                },
                {
                    "sent": "There's local variational approximations using convex lower bounding, and there's global variational approximation.",
                    "label": 0
                },
                {
                    "sent": "What I called variational Bayes and a lot of people do as well.",
                    "label": 0
                },
                {
                    "sent": "So these are four different methods, but I'm going to show basically they can all be unified and there are special cases of hyperparameter map using different implicit hyper priors.",
                    "label": 1
                },
                {
                    "sent": "So basically, once you kind of unify and see the similarities between these methods, then you can decide how to choose this different hyper priors, which is basically what distinguishes these different methods.",
                    "label": 1
                },
                {
                    "sent": "And basically how you choose this hyper prior will depend on the application and the two.",
                    "label": 0
                },
                {
                    "sent": "I'm going to look at is finding maximally sparse representations, sparse coding and our VM's are kind of special cases and this is something I did a lot of my PhD work on and then active learning and experimental design is something new that has very different requirements on how to pick this implicit hyper prior.",
                    "label": 0
                },
                {
                    "sent": "So these are kind of two contrasting applications that affect how you do this approximation, and then if there's time I'll talk about some extensions.",
                    "label": 0
                },
                {
                    "sent": "To nonnegativity constraints, can variance component estimation and.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Classification so.",
                    "label": 0
                },
                {
                    "sent": "So here's the basic sparse linear model.",
                    "label": 1
                },
                {
                    "sent": "You have some observed data T. You have known matrix of feature features Fi and.",
                    "label": 0
                },
                {
                    "sent": "Then you have some unknown model weights W an.",
                    "label": 0
                },
                {
                    "sent": "Of course some noise.",
                    "label": 0
                },
                {
                    "sent": "I'm going to assume that this makes these features are known, but all the methods I talk about today, you can add a dictionary learning step and learn this matrix as well blindly if you want.",
                    "label": 0
                },
                {
                    "sent": "So the likelihood model we assume is just Gaussian here and we have a Gaussian data likelihood and then we have a sparse prior here which is factorial, meaning that it's independent across each of the different coefficients and the key property that we assume to kind of rigorously define sparsity is that it can be expressed in this form or this function G is nondecreasing an it's concave, so this kind of is a generalized notion of sparsity that encompass is all the.",
                    "label": 0
                },
                {
                    "sent": "Common sparse priors people use like student, T, Jeffreys, prior, Laplacian.",
                    "label": 0
                },
                {
                    "sent": "All these sorts of things.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's just a quick picture to show a sparse priors look like.",
                    "label": 0
                },
                {
                    "sent": "Here's a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "It kind of distributes all the probability mass uniformly about the the origin.",
                    "label": 0
                },
                {
                    "sent": "And then here's a sparse prior, which has a big peak at zero, but then it has very heavy tails, so it encourages weights that are either 0, or maybe have some big non 0 value.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So practical issues with this model is that basically the joint distribution given the likelihood in the prior is is often very highly multimodal.",
                    "label": 1
                },
                {
                    "sent": "There are certain cases where it's not, but in a lot of the applications we look at it, it is very multimodal, even if it's not, it may be difficult to evaluate where the predominant mass is located.",
                    "label": 1
                },
                {
                    "sent": "An normalization is intractable, so you can't compute the model evidence and you can't normalize.",
                    "label": 0
                },
                {
                    "sent": "So you need some approximation to the posterior and.",
                    "label": 1
                },
                {
                    "sent": "Maybe the model evidence as well?",
                    "label": 0
                },
                {
                    "sent": "I'm going to look mostly at approximations to to this.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So latent variable models of sparse priors.",
                    "label": 1
                },
                {
                    "sent": "It turns out that sparse priors have a nice structure which is useful both from an analysis standpoint and also from an implementation standpoint, and basically all those.",
                    "label": 0
                },
                {
                    "sent": "All the sparse priors that are of the form I mentioned earlier, with that G that's square concave can be written in either of these two forms.",
                    "label": 0
                },
                {
                    "sent": "The first is called a Gaussian scale mixture, and it's basically it's called a scale mixture because the mean here is zero of this Gaussian Ann, you're just integrating over different scales modulated by some hyper prior.",
                    "label": 0
                },
                {
                    "sent": "So this is one way and then slightly more.",
                    "label": 0
                },
                {
                    "sent": "General Way is convexity based representation, where you representing each prior as a supremum over Gaussians and then this is some basic arbitrary function that controls the different scales and basically all sparse priors can be represented in both forms.",
                    "label": 1
                },
                {
                    "sent": "This one is a little more general general there's some special cases where this one doesn't apply, but basically for all the priors you've probably come in contact with, you can represent either way.",
                    "label": 0
                },
                {
                    "sent": "An for basically any non negative function for this prior an for this fee term there the resulting prior kernel.",
                    "label": 0
                },
                {
                    "sent": "I guess G will be non decreasing and concave so it'll be sparse.",
                    "label": 0
                },
                {
                    "sent": "So these two representations are kind of intrinsically produce sparse heavy tailed distributions.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just to see the former ones, probably more common, but just to see kind of in.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But if Lee, what this one looks like, here's the outer envelope of a sparse prior, and you can see that this scaling factor just scales a bunch of different Gaussians that go inside of this.",
                    "label": 0
                },
                {
                    "sent": "And if you take the upper outer, the upper envelope of all of these different Gaussians, you basically get the.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "True prior, so that's how this.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maximization can kind of give you a sparse representation.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there's four possibilities for approximation using this latent variable structure that I'm going to talk about today, and the first I mentioned was standard map estimation of these unknown ways.",
                    "label": 0
                },
                {
                    "sent": "An if you can develop very nice EM algorithms using either representation where you consider these.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Latent variables these unknown variances.",
                    "label": 0
                },
                {
                    "sent": "Here you consider them as.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is hidden data and you can do nice EM algorithms for basically any sparse prior using either type of representation.",
                    "label": 0
                },
                {
                    "sent": "The second thing you can do is map estimation of the unknown hyperparameters, and that you typically assume the Gaussian scale mixture and you can.",
                    "label": 0
                },
                {
                    "sent": "Come up with an ICM.",
                    "label": 0
                },
                {
                    "sent": "Algorithms as well.",
                    "label": 0
                },
                {
                    "sent": "Then there's local variational approximation convex bounding and that requires the convex representation.",
                    "label": 1
                },
                {
                    "sent": "Not surprisingly, and then global variational approximation or variational Bayes use the scale mixture representation of the prior.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'll go through each of these in a little bit more detail and I guess this is a workshop, so someone has a question in the any.",
                    "label": 0
                },
                {
                    "sent": "Something's not clear at any point.",
                    "label": 0
                },
                {
                    "sent": "Feel free to interrupt.",
                    "label": 0
                },
                {
                    "sent": "So basic method one is just map estimation of the weights.",
                    "label": 0
                },
                {
                    "sent": "Nothing surprising here.",
                    "label": 0
                },
                {
                    "sent": "You have likelihood prior and you have this G which is concave in nondecreasing.",
                    "label": 0
                },
                {
                    "sent": "And so you basically just optimize this objective function an if you want to call it a Gaussian approximation you can say, well, we're approximating the true posterior with a Gaussian degenerate Gaussian with no variance, and it's just a Delta function at the map estimate you get here, and I'm not going to go into the EM algorithms for doing this, but.",
                    "label": 0
                },
                {
                    "sent": "Again, you can do it for basically any concave nondecreasing G here.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second one is hyperparameter map, and so this is a little bit different.",
                    "label": 0
                },
                {
                    "sent": "Here we integrate out the unknown weights and we're optimizing the posterior over the hyperparameters gamma.",
                    "label": 0
                },
                {
                    "sent": "So if you do this integral, basically you get.",
                    "label": 0
                },
                {
                    "sent": "Here's the prior, the hyper prior, so that gives you this term over here, which is just the negative log of the hyper prior, just some function, and then these two terms come from.",
                    "label": 0
                },
                {
                    "sent": "Basically, when you do this integral, you get another Gaussian which is 0 mean.",
                    "label": 0
                },
                {
                    "sent": "And it has covariance given by this term here.",
                    "label": 0
                },
                {
                    "sent": "So this is basically a Gaussian covariance, but it's a highly nonlinear function of these hyperparameters, so there's a vector, one for each weight, and this capital gamma is just a diagonal of all these hyper priors.",
                    "label": 0
                },
                {
                    "sent": "This first term shouldn't be 0.",
                    "label": 0
                },
                {
                    "sent": "This should be gamma, one.",
                    "label": 0
                },
                {
                    "sent": "So this is just a vector of all the hyper prior variances, one for each weight.",
                    "label": 0
                },
                {
                    "sent": "And this is the cost function you get by gamma map.",
                    "label": 0
                },
                {
                    "sent": "The only thing.",
                    "label": 0
                },
                {
                    "sent": "Specified is what you pick for this function F. And once you optimize this, you get some map estimate for this hyperparameter vector.",
                    "label": 0
                },
                {
                    "sent": "Then you get an approximation which is also Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Was given with a mean mu Anna covariance Sigma an.",
                    "label": 0
                },
                {
                    "sent": "These are computed in closed form.",
                    "label": 0
                },
                {
                    "sent": "Once you're given these unknown hyperparameters that it's available in closed form, the mean is given by this kind of standard expression, and the covariance is given by this so.",
                    "label": 0
                },
                {
                    "sent": "The what you did once you.",
                    "label": 0
                },
                {
                    "sent": "Once this is the hard part here.",
                    "label": 0
                },
                {
                    "sent": "Once once you get these hyperparameters you, you're basically done.",
                    "label": 0
                },
                {
                    "sent": "You have a nice posterior approach.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Estimation.",
                    "label": 0
                },
                {
                    "sent": "So the third method is convex bounding and the convex type representation remember represents each unknown coefficient as the supremum over Gaussians.",
                    "label": 0
                },
                {
                    "sent": "So if you remove the maximization, you get an approximate prior, which is like a lower bound on the true prior and it's given by this expression.",
                    "label": 0
                },
                {
                    "sent": "It's just a scaled Gaussian which obviously doesn't integrate to one, but it's a useful lower bound, and this gives in a family of approximate distributions for the joint distribution.",
                    "label": 1
                },
                {
                    "sent": "Given by this expression, which again is a lower bounding approximation.",
                    "label": 0
                },
                {
                    "sent": "So basically you want to optimize this lower bound.",
                    "label": 0
                },
                {
                    "sent": "One thing you could do is minimize the sum of the misaligned mass.",
                    "label": 0
                },
                {
                    "sent": "So you just integrate over the absolute value of the difference between the two distributions.",
                    "label": 0
                },
                {
                    "sent": "And since this one is a lower bound on this one, you can remove the absolute value and just maximize this which is called the model evidence in a lot of cases.",
                    "label": 0
                },
                {
                    "sent": "So basically once you.",
                    "label": 0
                },
                {
                    "sent": "Do this and you solve for the optimal hyper.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The moments you get are exactly the same.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What you had in the previous case so.",
                    "label": 0
                },
                {
                    "sent": "It's it's pretty similar idea I guess.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The last method is a variational Bayes.",
                    "label": 1
                },
                {
                    "sent": "Basically, here you look at the posterior of the complete data of the both the distribution of the unknown weights and these unknown hyperparameters given the hyper prior an.",
                    "label": 1
                },
                {
                    "sent": "Assuming the data is observed an this is typically multimodal, an intractable as well.",
                    "label": 0
                },
                {
                    "sent": "So variational Bayes says let's form a tractable approximate distribution.",
                    "label": 0
                },
                {
                    "sent": "So basically you want to approximate this full joint distribution by a factorization over an approximate distribution, just in terms of the model weights, and then just in terms of the hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "So besides this factorization that the forms of these two distributions is unspecified and that falls out of the modeling and basically you optimize these two distributions by minimizing the KL divergent between a factorized functional form and then the true joint distribution, and you minimize over these two via coordinate descent, and you get 2 approximate distributions.",
                    "label": 0
                },
                {
                    "sent": "So and then once you get these two then you just assume that the true distribution.",
                    "label": 0
                },
                {
                    "sent": "Is approximated by this one an it's again, it turns out to be a Gaussian even though you didn't specify when you were doing this, that it had to be Gaussian.",
                    "label": 0
                },
                {
                    "sent": "It turns out that the one that optimizes this expression will be Gaussian, and again the mu and the Sigma from before are basically up.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The same form as we had back here, only you replaced the map Esta.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here with now, the optimal.",
                    "label": 0
                },
                {
                    "sent": "Basically, the expected value of these hyperparameters given this approximate model, and you can do all of this in closed form.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we have now four different approximate methods for dealing with this kind of simple hierarchical Bayesian model, and but how different are they and what do you really get by these different methods?",
                    "label": 1
                },
                {
                    "sent": "An I guess this is kind of important 'cause if you're trying to choose which one to use, it's kind of nice to know how they differ or if any of them are the same or this sort of thing.",
                    "label": 0
                },
                {
                    "sent": "So the first thing if we allow for improper hyper priors here, then by inspection, basically the gamma map.",
                    "label": 1
                },
                {
                    "sent": "Which I called method two in the convex bounding given equivalent approximate distribution when we just make this equality here, the associated true posteriors will generally be different, but the approximate method you get out of these two methods is identical, so you don't really get a new class of approximate models and.",
                    "label": 0
                },
                {
                    "sent": "So the second one, which is not as obvious, is that VB and convex bounding give equivalent approximations to a fixed posterior distribution, so the underlying hyper prior and then this scaling functional will not be the same, but the.",
                    "label": 1
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Approximate distribution you get here.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Will be identical and this is not something I guess that's widely assumed, but these two methods actually give you an identical result.",
                    "label": 0
                },
                {
                    "sent": "So the conclusion is that gamma map just forming some finding some map estimate of these hyperparameters encompases all the approximations you can get out of VB or convex bounding.",
                    "label": 0
                },
                {
                    "sent": "So you're really not getting anything new by doing these two, it's only a different interpretation.",
                    "label": 0
                },
                {
                    "sent": "Maybe of what's being optimized, but you don't actually get something new in terms of cost functions or.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Approximations.",
                    "label": 0
                },
                {
                    "sent": "A final thing, what about just doing map estimation over the weights themselves?",
                    "label": 0
                },
                {
                    "sent": "This is actually also a special case.",
                    "label": 0
                },
                {
                    "sent": "If the map is some solution to any map W map problem, then there exists a limiting gamma map problem that produces a posterior approximation given by this, which is just a Delta function at W map.",
                    "label": 1
                },
                {
                    "sent": "So again, this is actually a limiting case of hyperparameter map, even though one is optimizing in weight space and one is optimizing and hyperparameter space.",
                    "label": 0
                },
                {
                    "sent": "This is actually just a special case of that.",
                    "label": 1
                },
                {
                    "sent": "Again, the associated true posterior and the associated true underlying hyper prior will not necessarily be the same, but the approximation that you get will be the same.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Name?",
                    "label": 0
                },
                {
                    "sent": "So the summary, I guess at the end of this is that you have four seemingly very different approximation strategies, but they can all be viewed as special cases of just doing standard hyperparameter map estimation or evidence maximization.",
                    "label": 1
                },
                {
                    "sent": "So the only real distinction is this choice of the F which is embedded in the hyper prior, and what you choose for that.",
                    "label": 0
                },
                {
                    "sent": "That's the only thing that separates any of the cost functions you get out of these different methods, or the approximate models you get at the end of the day.",
                    "label": 0
                },
                {
                    "sent": "It's the only real distinction is this F. So I would argue rather than decide whether you should do convex bounding or variational Bayes or something else, the realist you should just decide is what should you pick for this function.",
                    "label": 0
                },
                {
                    "sent": "F and the kind of general principles I would think of when considering it is that it's better to pick a good value for this function consistent with your application and I'll talk about this more later.",
                    "label": 1
                },
                {
                    "sent": "It's better to pick a good value for this function then to pick a plausable prior to start with and then work backwards through VB or convex bounding or something like that.",
                    "label": 0
                },
                {
                    "sent": "And if the ultimate goal is a good posterior approximation, maybe an absurd full model leads to a very useful approximation, and that's fine.",
                    "label": 1
                },
                {
                    "sent": "So maybe the optimal F for your application is corresponds with kind of ridiculous full model, and I'll give some examples of that, but it doesn't matter if you get a good approximation out of it for the real world data you care about.",
                    "label": 0
                },
                {
                    "sent": "So the two issues I would look at for deciding how to pick this F or basically optimization considerations and then application independent requirements.",
                    "label": 0
                },
                {
                    "sent": "These are basically the two things that I would say kind of decide how to go about choosing this F 'cause so far it's been basically.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Arbitrary.",
                    "label": 0
                },
                {
                    "sent": "So first of all, from an optimization standpoint, if F is concave, you can implement this hyperparameter map using efficient reweighted L1 minimization procedure.",
                    "label": 1
                },
                {
                    "sent": "So for any concave differentiable F, so that's a consideration if speed and efficiency of implementation are important.",
                    "label": 1
                },
                {
                    "sent": "If this F that you want to use is not even available in closed form, you can still, but you can compute from the original prior the gradient of this G function with respect to the argument there.",
                    "label": 0
                },
                {
                    "sent": "Then you can still do em even if you don't know the exact form of this.",
                    "label": 0
                },
                {
                    "sent": "This F if it's not available in closed form and you get a very simple M, you compute this mean in this Sigma with this standard equations I showed before and then the M step.",
                    "label": 0
                },
                {
                    "sent": "All you do is optimize the hyperparameters by sending them.",
                    "label": 0
                },
                {
                    "sent": "Equal to this gradient evaluated at the previous values you computed here.",
                    "label": 0
                },
                {
                    "sent": "So basically for this this method ends up being much slower than this, But if you can do this, you can.",
                    "label": 0
                },
                {
                    "sent": "Basically it's very general.",
                    "label": 0
                },
                {
                    "sent": "You can for any value of F, so it's kind of nice.",
                    "label": 1
                },
                {
                    "sent": "And then finally there's greedy methods that you can do.",
                    "label": 0
                },
                {
                    "sent": "Tipping and fall have like a incremental greedy method for implementing this type of thing in certain special cases, so this could be a concern.",
                    "label": 0
                },
                {
                    "sent": "Depending on the size of your problem and what you're really after, these these kind of ideas.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's two example applications.",
                    "label": 0
                },
                {
                    "sent": "One is finding maximally sparse representations.",
                    "label": 1
                },
                {
                    "sent": "This I do brain imaging, and there's a lot of applications of this to localising brain sources and stuff like that, and sparse coding and relevance vector machines kind of are in this same genre of application, and then I won't talk much about this 'cause I don't know too much about it, but experimental design.",
                    "label": 0
                },
                {
                    "sent": "For example finding non random projections for compressed sensing.",
                    "label": 1
                },
                {
                    "sent": "This is an area where you have.",
                    "label": 0
                },
                {
                    "sent": "Fair.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Different requirements for.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For how to choose this F, then you do here, so I'll talk about choose.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "F in these two situations, so maximal sparse sparse is basically kind of formal problem studied by a lot of people in information theory and elsewhere in the noiseless case.",
                    "label": 0
                },
                {
                    "sent": "The idea is to solve this minimization problem, so we're assuming this file here is over complete, meaning there's more more columns than rows, so there's an infinite number of solutions to this linear problem, and you want to find the solution with minimal L zero norm, which means you want.",
                    "label": 0
                },
                {
                    "sent": "Most of that you want to find the solution to this equation such that most elements of this W are equal to 0.",
                    "label": 0
                },
                {
                    "sent": "And the noisy case you relax the requirement of equality here and you just solve this this problem here where Lambda is a trade off parameter balances sparsity and data fit.",
                    "label": 0
                },
                {
                    "sent": "So the problem with encountered by everyone who wants to solve this, and again there's a lot of applications to this, is that there's a common material number of local minima, and then the objective function is discontinuous.",
                    "label": 1
                },
                {
                    "sent": "Because this is just a count of non zero elements and the forward model.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's very simple to.",
                    "label": 0
                },
                {
                    "sent": "Go forward given a sparse vector, compute what data would be observed with some noise.",
                    "label": 1
                },
                {
                    "sent": "But again, the inverse problem is.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is intractable.",
                    "label": 0
                },
                {
                    "sent": "This is just a toy example of what sparse solution is.",
                    "label": 0
                },
                {
                    "sent": "What you would be looking for.",
                    "label": 0
                },
                {
                    "sent": "Here's data vector toy matrix, and you want to find a W that solves this equation.",
                    "label": 1
                },
                {
                    "sent": "And here's a non sparse solution.",
                    "label": 0
                },
                {
                    "sent": "An if you actually sit down, it does equal this I think, and this is a very sparse solution with mostly zero valued entries.",
                    "label": 0
                },
                {
                    "sent": "So this is the kind of solution you're looking for as opposed to this.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how do you find this double?",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not this this.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maximally sparse solution here.",
                    "label": 0
                },
                {
                    "sent": "Using this framework, I've been talking about the key idea is to choose, you need to choose an appropriate hyper prior, and particularly the energy here this F, and once you choose some F, which I'll talk about.",
                    "label": 0
                },
                {
                    "sent": "Maybe some ideas how you might go about choosing it.",
                    "label": 0
                },
                {
                    "sent": "Then you need to compute the map estimate of the hyperparameters, and then once you have those you have this Gaussian approximation.",
                    "label": 0
                },
                {
                    "sent": "You assume the mean is.",
                    "label": 0
                },
                {
                    "sent": "That's what you use.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is your approximation?",
                    "label": 0
                },
                {
                    "sent": "So how would you choose this F for this problem?",
                    "label": 0
                },
                {
                    "sent": "And here's two rather than say, well, this particular prior is sparse, so I'm just going to use this one and do variational Bayes or something more heuristic.",
                    "label": 0
                },
                {
                    "sent": "We just go directly to this function and say whatever.",
                    "label": 0
                },
                {
                    "sent": "What would be simple criteria for choosing this F to solve this problem?",
                    "label": 0
                },
                {
                    "sent": "An one simple criteria is just to say if this maximally sparse solution only has one non zero element, then the associated cost function has a similar.",
                    "label": 1
                },
                {
                    "sent": "A single minimum and the mean equals this kind of simple solution.",
                    "label": 0
                },
                {
                    "sent": "So in words this is saying finding one non zero element should be very simple task, so that's that's kind of a pretty we.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Requirement.",
                    "label": 0
                },
                {
                    "sent": "And then the second criteria refers to local minima.",
                    "label": 0
                },
                {
                    "sent": "We would prefer that all local minima produce approximations with this posterior mean such that the L0 norm of the mean, which is account of the non zero elements is less than or equal to the signal dimension.",
                    "label": 0
                },
                {
                    "sent": "And this is just saying you should never require more than N nonzero coefficients to represent a N dimensional signal T, so these are not such.",
                    "label": 1
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Strong criteria, but basically if you just have these two criteria then over all possible choices for this F that only this selection satisfies the two criteria.",
                    "label": 1
                },
                {
                    "sent": "So kind of reduces the class of functions you might want to use for finding these sparse rapid representations vary considerably, and then sparse Bayesian learning, which is a machine learning technique by tipping and the lasso are special cases when Alpha goes to 0 or Alpha goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "So and these are.",
                    "label": 0
                },
                {
                    "sent": "Both work very well finding sparse representations, so maybe it's not surprising that there are special cases.",
                    "label": 0
                },
                {
                    "sent": "But so I've been saying all you really need to do is pick this F function and do hyperparameter map.",
                    "label": 0
                },
                {
                    "sent": "Yeah, go ahead.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Two very different things too, right?",
                    "label": 0
                },
                {
                    "sent": "Different.",
                    "label": 0
                },
                {
                    "sent": "How different this solution?",
                    "label": 0
                },
                {
                    "sent": "How how do the sparser presentation to get promise being invested compare, given that right?",
                    "label": 0
                },
                {
                    "sent": "I mean the real tradeoff here between the Alpha is that when Alpha goes to 0, you get a a more local minima, but then the global minima is more likely to be the maximally sparse solution.",
                    "label": 0
                },
                {
                    "sent": "So in the limit, when this Alpha goes to zero, you're guaranteeing that the global minima is good, but the price you're paying is you're adding lots of local minima and then here when the Alpha goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "You're driving the cost function to a more convex form, but the price you're paying is that the global minimum of this new cost function that's getting more and more almost convex is that the global global minimum of it can be biased away from the maximum sparse solution, so that's the tradeoff that's happening when you move this Alpha back and forth.",
                    "label": 0
                },
                {
                    "sent": "Continuous pop though as you move Alpha.",
                    "label": 0
                },
                {
                    "sent": "A continuous.",
                    "label": 0
                },
                {
                    "sent": "I mean that it will it doesn't.",
                    "label": 0
                },
                {
                    "sent": "It won't necessarily be continuous.",
                    "label": 0
                },
                {
                    "sent": "Now it will not.",
                    "label": 0
                },
                {
                    "sent": "The cost function kind of varies continuously, but the minimum can jump, yeah?",
                    "label": 0
                },
                {
                    "sent": "Actually, that's the way I implement SBL now is.",
                    "label": 0
                },
                {
                    "sent": "I start with Lasso and then I do a re weighted lasso an convert.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of a separate issue.",
                    "label": 0
                },
                {
                    "sent": "But the lasso can be a good starting point.",
                    "label": 0
                },
                {
                    "sent": "I haven't tried though running varying this Alpha at different times.",
                    "label": 0
                },
                {
                    "sent": "You could, but I haven't tried that.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one thing what I was saying before is that I've been assuming all that's different.",
                    "label": 0
                },
                {
                    "sent": "Here is the F. All these different methods, convex bounding variational, Bayes, hyperparameter map?",
                    "label": 1
                },
                {
                    "sent": "All these methods are really just different choices of this F, so it doesn't really change much, and it's true it doesn't affect the hyperparameters you get or the approximate distribution you get, but it does change the implicit full model that you started with, if that matters to you.",
                    "label": 0
                },
                {
                    "sent": "So if you want to, if you choose to use this app for this problem and you're just doing standard hyperparameter map, then the implicit full model is a PMW that is Laplacian and a posterior that is log concave in.",
                    "label": 0
                },
                {
                    "sent": "You know model, but if you assume that you're doing convex lower bounding or variational Bayes, the PFW, the implicit full model, whatever that means is highly sparse.",
                    "label": 1
                },
                {
                    "sent": "You can compute this exactly like what what prior you would need here.",
                    "label": 0
                },
                {
                    "sent": "To make convex lower bounding give you the exact same cost function as this, but it's it's a very different prior and posterior.",
                    "label": 0
                },
                {
                    "sent": "It's highly multimodal and very, very sparse, so the implicit full model is definitely different, even though the cost function you're optimizing is identical.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I think it's kind of an important distinction.",
                    "label": 0
                },
                {
                    "sent": "And then a note about SBL and relevance vector machines kind of ties in with.",
                    "label": 1
                },
                {
                    "sent": "All of this is that in the original derivation of the relevance vector machine, the full model assumes a hyper prior like this, which is just a Jefferies and that gives you a weight prior like this.",
                    "label": 0
                },
                {
                    "sent": "There should be absolute values around here I think, but basically.",
                    "label": 0
                },
                {
                    "sent": "So this is, this is what the full model of of SPL starts with an exact inference with this model would be very undesirable because this has basically infinite mass right around 0, so the best probably approximate model to this full model would just be to put a Delta function at zero, which is kind of degenerate and useless.",
                    "label": 1
                },
                {
                    "sent": "And if you applied gamma map directly to the full model, which implies that this F equals log of gamma when you do the negative log up here.",
                    "label": 0
                },
                {
                    "sent": "It gives you a trivial solution.",
                    "label": 1
                },
                {
                    "sent": "It gives you the gamma, the hyperparameter map estimate is 0, the mean is zero and the variance is 0, so it's kind of a good approximation to the full model, but the full model is kind of useless anyway, so SPL doesn't even do gamma map in log space.",
                    "label": 0
                },
                {
                    "sent": "It actually it does.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry it doesn't do directly gamma map, it does it in log space, which kind of zeroes out this F function and that's why.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Get good results and that's why you get back to here, because this Alpha.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is to zero.",
                    "label": 0
                },
                {
                    "sent": "Basically you have F = 0.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK, so switching gears a little.",
                    "label": 0
                },
                {
                    "sent": "That's one way you might want to choose F, But this is for sparsity and Matthias is pointed out to me several times, and it's definitely true you're getting something useful for finding sparse solutions, but you're not really there.",
                    "label": 0
                },
                {
                    "sent": "The true distribution that you're working with would never have a mean with exactly 0 valued elements.",
                    "label": 0
                },
                {
                    "sent": "In general, even a sparse prior.",
                    "label": 0
                },
                {
                    "sent": "If you take the mean, it's not going to be exactly 0, and there's other.",
                    "label": 0
                },
                {
                    "sent": "There's other applications where this is very desirable to have.",
                    "label": 0
                },
                {
                    "sent": "The approximate model have the mean that's not perfectly aligned with the axes with individual elements exactly equal to zero with no variant, so.",
                    "label": 0
                },
                {
                    "sent": "So experimental design is 1 case where you have a very different requirement for choosing this F so you don't have this issue with a degenerate Gaussian collapsing to the axis.",
                    "label": 0
                },
                {
                    "sent": "So one idea proposed by sure how Jian others related to compressed sensing, is to use the approximate posterior you get from some gamma map.",
                    "label": 1
                },
                {
                    "sent": "No matter how you chose the F to begin with and take this approximate distribution and then learn new rows of the design matrix Phi such that uncertainty about the weights is reduced and the cost function they propose is to add additional rows to minimize differential entropy.",
                    "label": 1
                },
                {
                    "sent": "And so they want to minimize this function by adding rows here.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So a big problem with this is exactly what I was saying before.",
                    "label": 0
                },
                {
                    "sent": "If this F is concave, you can show that at least N -- N hyperparameters will be set to exactly 0 regardless of the data.",
                    "label": 1
                },
                {
                    "sent": "So even if you have almost no data at all, if this F is concave, you're going to be setting hyperparameters to zero.",
                    "label": 0
                },
                {
                    "sent": "And what does that do?",
                    "label": 0
                },
                {
                    "sent": "If this gamma map is sparse, the posterior mean is forced to be sparse even if the desired posterior mean is far from sparse.",
                    "label": 1
                },
                {
                    "sent": "So this gamma just refresh your memory is the prior variance of each unknown coefficient.",
                    "label": 0
                },
                {
                    "sent": "So if one of these elements goes to 0, then the associated coefficient is held to zero and it can't deviate, and likewise the posterior variance is how the zero as well.",
                    "label": 1
                },
                {
                    "sent": "So the posterior mean is forced to be sparse and there's no posterior uncertainty in the associated 0 value coefficients.",
                    "label": 0
                },
                {
                    "sent": "So even with limited data, the model is assigning zero posterior variance to these coefficients.",
                    "label": 0
                },
                {
                    "sent": "So how do you reduce uncertainty?",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Beyond that.",
                    "label": 0
                },
                {
                    "sent": "So this is just very heuristic thing you could do and they implement some form of this in their paper.",
                    "label": 0
                },
                {
                    "sent": "Just as a post processing step almost but basically one way you could do it within this model is to choose a different value for this F function.",
                    "label": 0
                },
                {
                    "sent": "So the F that you can show falls out of the variational relevance vector machine from Bishop and Tipping is one that looks like this and basically if any of these gammas go to zero this function goes to Infinity so it's preventing.",
                    "label": 1
                },
                {
                    "sent": "Any of the hyperparameters going to 0 or even near 0, unless you have a lot of data supporting it, so this this would prevent that problem.",
                    "label": 0
                },
                {
                    "sent": "But I'm not going to say it's going to work great in practice, but it's kind of a slightly more principled way to prevent this from happening.",
                    "label": 0
                },
                {
                    "sent": "It could be useful for experimental design.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now I'm just going to briefly how much time is left here?",
                    "label": 0
                },
                {
                    "sent": "OK, well I'll just briefly go.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'll just go through this really fast non negative sparse coding is the same sparse problem I talked about before, only that you constrain the weights to be greater than zero and the problem is that it's intractable.",
                    "label": 0
                },
                {
                    "sent": "Even compute this posterior distribution to do hyperparameter map.",
                    "label": 0
                },
                {
                    "sent": "So you have basically an intractable cost function and then the posterior moments given some hyperparameters are also interact.",
                    "label": 1
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "One way to get around this is you can show that the posterior mean you get from regular map estimation.",
                    "label": 1
                },
                {
                    "sent": "What I've been talking about before it satisfies this equation where this G is given by a concave conjugate.",
                    "label": 0
                },
                {
                    "sent": "I won't go into that now since I guess we're running out of time, but basically you can show that the posterior mean you got from the regular hyperparameter map you were doing before is just given by minimizing this cost function, and this G will end up being a concave penalty, so it's giving you sparsity.",
                    "label": 0
                },
                {
                    "sent": "So if the.",
                    "label": 0
                },
                {
                    "sent": "Posterior mean in the regular cases given by minimizing this, it's actually really easy to add a non negative non negativity constraint here and then you can optimize this cost function directly using a non negative lah.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So and here's a just a quick toy problem to say show that this works pretty well in pretty quickly as you can generate here you generate some data, so this is a sparse weight vector.",
                    "label": 0
                },
                {
                    "sent": "It's generated by implicit wait prior looking like this, with the Delta function at zero and then a Gaussian that's positive and cut off on the other side, and so you run the proposed algorithm and you try and learn what this W is given the T.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The five, so here's an example.",
                    "label": 0
                },
                {
                    "sent": "You initialize with the non negative lasso and with just a couple related iterations you can basically do.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Much, much better by minimizing this subject, this sub.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The function, so maybe within this is 1 related iteration, second third, 4th.",
                    "label": 0
                },
                {
                    "sent": "You can basically converge and go from 30 some percent success to 90% success.",
                    "label": 0
                },
                {
                    "sent": "Recovering these sparse coefficients so.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It works pretty well.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll skip this stuff and just say some final thoughts.",
                    "label": 0
                },
                {
                    "sent": "Maybe in the context of the sparse linear model there's a variety of approximate inference methods.",
                    "label": 0
                },
                {
                    "sent": "But they can all be reduced to hyperparameter map estimation using some implicit hyper prior.",
                    "label": 0
                },
                {
                    "sent": "An note you can do inference without worrying about conjugate hyper priors.",
                    "label": 0
                },
                {
                    "sent": "You can do a large class of these F of gammas.",
                    "label": 0
                },
                {
                    "sent": "It's so it's pretty pretty convenient, but the choice should be application dependent and focus on the underlying cost function, not the plausibility of the full model.",
                    "label": 0
                },
                {
                    "sent": "And here's something I throw out here.",
                    "label": 0
                },
                {
                    "sent": "I don't know much about EP, and this could be ridiculous, but I would say that you can probably Terrell or this F to get similar Gaussian approximation to EP.",
                    "label": 0
                },
                {
                    "sent": "Maybe in some special cases, but the key is that you wouldn't want to start with the same full model because a lot of times when you start with the same full MoD.",
                    "label": 0
                },
                {
                    "sent": "And then you do variational Bayes or convex bounding.",
                    "label": 0
                },
                {
                    "sent": "You get an F. That's kind of ridiculous.",
                    "label": 0
                },
                {
                    "sent": "And if you were to compare it with EP, you might not get a good comparison.",
                    "label": 0
                },
                {
                    "sent": "But if all you're concerned about is tailoring this F to such that the final approximation is close to the EP approximation, maybe in some cases you could do it so.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's it.",
                    "label": 0
                },
                {
                    "sent": "It's sort of interesting with the unification of the approximation, it's almost like.",
                    "label": 1
                },
                {
                    "sent": "Most of the time when we're working with the model that we want to approximate, the model stays constant and the approximation change, right?",
                    "label": 0
                },
                {
                    "sent": "It's the opposite, right?",
                    "label": 0
                },
                {
                    "sent": "Approximation is always ending up at the, so the right is that because you say it's more problem driven, but you're just interested in sparsity and you don't care about, say, probabilistic predictions and things like that, well, I think some of it just falls out of the math of these models.",
                    "label": 0
                },
                {
                    "sent": "Is that the approximate the full models are not unique for a given approximation, if you assume you were doing VB, you wouldn't get this full model if you assume you're doing convex bounding, you would get this format, but the approximation is the same, and for what we do.",
                    "label": 0
                },
                {
                    "sent": "The approximation is what matters because the full model is just something you make up anyway.",
                    "label": 0
                },
                {
                    "sent": "It's just something to start with, and so a lot of these you just tailor the full model until you get some approximation that's nice.",
                    "label": 1
                },
                {
                    "sent": "And then that's what what you go with an in sparsity, that's what.",
                    "label": 0
                },
                {
                    "sent": "How it tends to workout for these maximally sparse things so I don't know?",
                    "label": 0
                },
                {
                    "sent": "Anyway.",
                    "label": 0
                },
                {
                    "sent": "Using sparse in terms of finding qualitative because first is desirable from like computational point of view and.",
                    "label": 0
                },
                {
                    "sent": "Mate.",
                    "label": 0
                },
                {
                    "sent": "I don't see why.",
                    "label": 0
                },
                {
                    "sent": "First, the nature might be actually right.",
                    "label": 0
                },
                {
                    "sent": "Well, it also depends on what you're using sparsity for.",
                    "label": 0
                },
                {
                    "sent": "If you're just using it to predictive purposes, maybe so, but a lot of times the sparse coefficients have physical significance, so we use it for localising brain sources so you care about fitting the data.",
                    "label": 0
                },
                {
                    "sent": "But the real point is that sparsely profiled, the vector you learn corresponds with brain sources, so the sparsity is absolutely essential because.",
                    "label": 0
                },
                {
                    "sent": "You're most parts of the brain are inactive for certain experiments, and the sparsity profile corresponds with the active locations, so you could predict the observed data an infinite number of ways, but neurophysiologic Lee, you are, sparsity is what you want, and this carries over to a lot of areas of signal processing, like sonar radar applications of localising sources.",
                    "label": 0
                },
                {
                    "sent": "So there are results though.",
                    "label": 0
                },
                {
                    "sent": "Related to theoretical performance of estimating sparsity profiles and sparsity and bounds on representing the data and all these sorts of things.",
                    "label": 0
                },
                {
                    "sent": "But for the applications we're looking at, the sparsity assumption is pretty justified I guess, so you don't have to worry about exactly what you're talking about, so.",
                    "label": 0
                },
                {
                    "sent": "In the end.",
                    "label": 1
                },
                {
                    "sent": "Shouldn't.",
                    "label": 0
                },
                {
                    "sent": "Worry about the possibility of any of the other line moves with you.",
                    "label": 0
                },
                {
                    "sent": "Would you agree with following statement that sparse Bayesian learning is not facing?",
                    "label": 1
                },
                {
                    "sent": "It's not fully Bayesian and it's based on a absurd full model.",
                    "label": 0
                },
                {
                    "sent": "The one that actually works, so there's a lot of different variants I guess, But the one that works that everyone uses has an absurd full model.",
                    "label": 0
                },
                {
                    "sent": "So yeah, it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not fully model beige, and I hope that's not offensive, but.",
                    "label": 0
                },
                {
                    "sent": "Some.",
                    "label": 0
                },
                {
                    "sent": "Important models things like slab spike and slab priors mean they don't seem to fit in this sort of scaled framework, so it's gotta be shared.",
                    "label": 0
                },
                {
                    "sent": "Frameworks that means do you have any comments about that kind of so?",
                    "label": 0
                },
                {
                    "sent": "These are kind of.",
                    "label": 0
                },
                {
                    "sent": "Right, so I guess I'm not exactly sure of the specific prior, but there are some differentiability constraints when you do the scale mixture representation of a prior, but those basically go away with the convex type representation.",
                    "label": 0
                },
                {
                    "sent": "So you can probably represent them that way and then do the same analysis assuming that there.",
                    "label": 0
                },
                {
                    "sent": "When you take the the negative log, their square is concave and non decreasing.",
                    "label": 0
                },
                {
                    "sent": "It's OK if it's non differentiable and as bizarre as you want, but as long as it has those properties you can represent represent it as a scale mixture so.",
                    "label": 0
                }
            ]
        }
    }
}