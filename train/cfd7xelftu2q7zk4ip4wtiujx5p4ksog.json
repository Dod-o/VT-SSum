{
    "id": "cfd7xelftu2q7zk4ip4wtiujx5p4ksog",
    "title": "PAC-Bayesian Analysis: A Link Between Inference and Statistical Physics",
    "info": {
        "author": [
            "Yevgeny Seldin, Department of Computer Science, University of Copenhagen"
        ],
        "published": "Oct. 16, 2012",
        "recorded": "September 2012",
        "category": [
            "Top->Mathematics->Control Theory",
            "Top->Physics->Statistical Physics"
        ]
    },
    "url": "http://videolectures.net/cyberstat2012_seldin_pac_bayesian/",
    "segmentation": [
        [
            "So this is the talk about progression analysis and it's a joint work with transfer.",
            "All of you let Joshua Taylor, Peter, our Nicole, sister Bianca and Ronald partner.",
            "So."
        ],
        [
            "What's general, it will be about first of all, I will show you some concentration inequality's for weighted averages of multiple, simultaneously evolving and interdependent martingales.",
            "So as you know, martingales are frequently used in modeling different stochastic processes, and possibly you have some multiple simultaneously moving martingales in your work, so maybe our results will be useful for you.",
            "And specif."
        ],
        [
            "Reckless applications that I will show is application to importance.",
            "Weighted something and applications to multi armed bandit problems which are a simple instance of reinforcement learning problems.",
            "I will describe them when I get to it.",
            "So I will start with some."
        ],
        [
            "Background about Bayesian analysis and then I will go to applications of this Park Basin analysis to Malton.",
            "Bandits with side information which will go through this three steps.",
            "First will apply importance.",
            "Weighted something in multi armed bandits.",
            "Then will I will show you the spec.",
            "Basing equalities for martingales and I will finish with how we actually derive some regret bounds for this small Karen bandit problems.",
            "Um?"
        ],
        [
            "So let's start with some introduction to pack Bayesian analysis.",
            "So first of all, what is Spock?"
        ],
        [
            "Box stays for probably approximately correct learning framework.",
            "Approximately means that we want to provide guarantees on the approximation error of empirical estimates.",
            "And probably that holds with high probability with respect to representativeness of the observed sample.",
            "So that is all this.",
            "Some probability sample is not representative, otherwise we get high probability guarantee and.",
            "Estimates"
        ],
        [
            "Some basic definitions from supervised learning in order to show you this result.",
            "So we have some sample space.",
            "We have a label space.",
            "We have a long loss function for giving current label to a sample.",
            "We have some hypothesis space.",
            "Age of exits.",
            "The prediction of a hypothesis from H on the sample X would define expected loss of thank you.",
            "Both this and the empirical loss of the parties on the sample that were there.",
            "So I assume that you observe and samples M pairs of X&Y and for each prediction rule we calculate what's the error.",
            "That prediction rule makes on those M samples.",
            "And we are going to bound the distance between this empirical estimation of the loss of the prediction rule and the expected error of the loss of this prediction rule when it sees some new samples."
        ],
        [
            "We are going to talk about randomized classifiers and randomized classifier.",
            "Its distribution over this report is Class H and what we do at each round of the game we pick a classifier according to this distribution of H. We observe the sample and we return the label that the hypothesis predicts on the sample.",
            "And."
        ],
        [
            "The loss of this prediction rule, so the expected loss.",
            "It's the expectation to respect to this probability of picking age of the loss of the correspondent corresponding report this, and we can also think about this as an infinite or I'll dimensional vector right as a dot product and amp."
        ],
        [
            "Take a loss.",
            "It's the average of the empirical losses of the parties.",
            "And now bug basehor."
        ],
        [
            "I think in the quality which goes back to the work of McAllister in 1999 ninety eight, it says that.",
            "So let's say that the loss is bounded in the 01 interval.",
            "It says that if we fix a reference distribution \u03c0 / H, then for any parameter Delta with probability greater than one minus Delta over the sample for all possible posterior distributions throw simultaneously the loss of this randomized predictor is bounded by its empirical loss plus.",
            "Square root of the KL divergent between the posterior and prior.",
            "Log of one over Delta divided by two times the sample size.",
            "Came up.",
            "Let's see if it works."
        ],
        [
            "The intuition behind the bounds.",
            "So if we take the scale, the versions, which is the average of the logarithm of the ratio between the procedure and the prior distribution.",
            "So the first step here, it's the description length.",
            "So if we assume that we used our prior in order to encode each of the hypothesis, that's the number of bits that will need in order to specify the hypothesis that we are picking according to Raw and H for it's the entropy of this theory distribution.",
            "So."
        ],
        [
            "What the bound tells us, it tells us to pick this posterior distribution Rd that minimizes the tradeoff between the empirical loss of each hypothesis.",
            "The complexity of the corresponding Potters and has maximum entropy.",
            "OK, so we're kind of recover the maximum entropy principle, but from the perspective of this generalization error, so we actually get that this maximum entropy principle provides low errors with high probability.",
            "Um?",
            "But is this found available?",
            "I mean there there is a distribution that achieves inequality in this inequality.",
            "And in that sense, yeah.",
            "OK and well, the point is that we have to pick this prior distribution before the observed the sample, so there are no miracles.",
            "If we come with good prior knowledge we can get good bounds.",
            "If you don't have good prior knowledge, the bounds still holds, but it may be not very tight bound and since this bound holds for all possible posterior distributions through our algorithm can actually look through different.",
            "Posterior distributions and pick the one that minimizes the bound.",
            "What's?"
        ],
        [
            "The relation between this Pug basin approach and based on learning so the relation is that as in Bayesian learning, we have an explicit way to incorporate prior information.",
            "Our learning where this prior distribution \u03c0.",
            "But there are several diff."
        ],
        [
            "Answers, so first of all, on the unlike in Bayesian learning where we get this posterior distribution, but we have no idea how well this posterior distribution is going to perform here.",
            "Will get explicit high probability guarantee on the expected performance of this classifier.",
            "With."
        ],
        [
            "I don't believe that our priorities correct in basin learning.",
            "Usually we assume that the data is generated like this prior.",
            "Is the correct prior over generating the parameters of the model.",
            "Here we don't assume any correctness of our prior, it's frequent is bound if it come with bad prior assumptions will get a bed bound, but it still abound.",
            "It holds with high probability."
        ],
        [
            "We have explicit dependence on the loss function in the bound.",
            "Also, unlike in basin learning, if we change the loss function, we are going to change our classifier because the loss function explicitly appears in the bound."
        ],
        [
            "It happens all the time.",
            "What's typical in decision theory?",
            "Explicit dependence on the loss function?",
            "How you make a decision loss?",
            "Yeah, but you don't have it in the basin running because that'll just compute the posterior.",
            "So that's the difference between the two.",
            "We have a bit different weighting of the prior belief versus the.",
            "Evidence that comes from this Mount.",
            "And finally, our."
        ],
        [
            "And holds for any posterior distribution role, and in particular it holds for their base posterior, so we can actually take the base posterior and calculate what's the expected error of their base posterior using color formula.",
            "So in the innovation formulation.",
            "You have the loss function enters in the likelihood term, right?",
            "So you have.",
            "The likelihood is the likelihood it's.",
            "The last that it's the log loss because you are calculating the that you're calculating the maximum posterior distribution and here this loss function can be 01.",
            "It can be quadratic loss, absolute loss.",
            "It can be also log loss, and then you will recover the.",
            "The patient saw.",
            "OK. Yeah, less than.",
            "Goal is is all about universal constant times, so I've highlighted some constants and some logarithmic factors, but I mean.",
            "Office doesn't seem to show up in your bombing.",
            "That seems a bit suspicious.",
            "The number of the purchases appears.",
            "We are the scale diversions.",
            "Child is uniform over all rows, and so you'd expect something like the covering number of the number of probability measures on their buses to show up quite beside the case.",
            "So if you have a finite number of hypothesis, you can put a uniform prior and then the scale will be just logarithm of the number of reporters is.",
            "A bit less like logarithm Holden, logarithm minus the entropy.",
            "But you can also work with infinite classes.",
            "Yeah, so and I'll show in the moment how actually get the bound, so there will be no no mysteries how it happens.",
            "For the entropy, which is why you get it all balanced for all distributions in the things.",
            "In a sense, I will see it in a couple of slides.",
            "Um?"
        ],
        [
            "So what's the relation?",
            "Difference with buckling children and children anchors, theory and Rademacher complexities?",
            "For those who know, so the relation is that we get explicit high probability guarantee on the expected performance, and we get explicit dependence on the loss function and the differences."
        ],
        [
            "So first of all, the complexities defined individually for each hypothesis.",
            "We're this prior definition, unlike for those conole visit dimension have visit dimension of the porters class, and you can't distinguish between individual hypothesis in this report's class.",
            "And we have an explicit way to incorporate prior knowledge.",
            "We are this prior which we can do in the VC type approaches.",
            "Now what's the relation to?"
        ],
        [
            "Statistical physics if we look into this inequality, we can rewrite it as a parameterized tradeoff between the empirical loss and the KL diversions.",
            "We can actually make a linear search over this better parameter, which is inverse temperature parameter.",
            "We can optimize it for every possible temple temperature, or like for a grid of temperatures we can substitute it back into the bound and we can find what is actually the correct temperature finalize our system given the sample size that we have an given, the properties of the system which are expressed in this empirical observation so."
        ],
        [
            "Bound provides the optimal temperature to study the system.",
            "Depending on the size of the sample of the number of observations from the system and the empirical properties of the system which are related to this observation that we get.",
            "OK.",
            "So now."
        ],
        [
            "Proof idea, so the proof is based on don't care but variational definition of KL diversions which was mentioned already by Sandra.",
            "So this variational definition says what's written on the slide.",
            "And if we just change sides."
        ],
        [
            "This definition we get the average of a function F with respect to some distribution.",
            "Role is bounded by this scale.",
            "Diversions plus logarithm of the average of the exponent of the function with respect to some other distribution file, and this generally holds for any pair of distributions properly defined."
        ],
        [
            "Well, a bit more background, Markov's inequality.",
            "I guess everybody knows here and."
        ],
        [
            "Holdings inequality says that if we have a number of independent random variables and then say 01 interval, then for any parameter Lambda the expectation of this variables minus the empirical average is bounded by the exponent on the right hand side.",
            "So that's all we need."
        ],
        [
            "Prove the theorem.",
            "We start with this change of measure inequality and what we do."
        ],
        [
            "Take this F of H as.",
            "Model is related to the function of interest, so we take it as Lambda.",
            "The difference between the expected and empirical loss of each hypothesis.",
            "So we are going to analyze the average of this difference with respect to posterior distribution raw.",
            "And now in order to get the bond, we have to bound.",
            "The average of the exponent of this function with respect to distribution file.",
            "So."
        ],
        [
            "How we do it.",
            "We take this quantity and we consider it as a random variable.",
            "And then by Markov's inequality, this random variable is bounded by one over Delta.",
            "The expectation of this random variable.",
            "OK.",
            "Expectation of reward expectation over the sample so.",
            "So you have some randomness in the sample, so we take the expectation with respect to the sample of this random variable."
        ],
        [
            "Now since the dot product and expectation a linear, we can take the expectation inside the DOT product and here the important point is that the prior should be independent of the sample.",
            "If the prior is independent of sample, we can take the expectation inside.",
            "And now this expectation.",
            "That's what we can bound using coding chemical."
        ],
        [
            "Pilot there."
        ],
        [
            "And now we have an average of a constant, so we just get this constant out.",
            "OK. And so this is actually the point where we can combine this not only withholding inequality.",
            "We can combine it with version equality or many other inequality's in probability theory, because most of them are based on bounding this expectation of the exponent, so that's the connection point to many other inequality's, both for independent random variables.",
            "And also we can do the same for martingales.",
            "It's going to be the same trick.",
            "So."
        ],
        [
            "So we got that this logarithm is bounded by what's written on the slide, we just."
        ],
        [
            "Apps that use that the normalized by Lambda, so that's already the quantity of."
        ],
        [
            "Stressed and we optimize over land.",
            "I'm skipping this step, but it's not very difficult and we'll get what we wanted to get.",
            "OK and then questions.",
            "Up to now.",
            "Good."
        ],
        [
            "So now we want to take this tool and to take it to multi arm bandits problem in reinforcement learning so."
        ],
        [
            "At the mall town bandits in amount.",
            "Adam bandit problem.",
            "We have a set of actions.",
            "Each action gives us some reward, distributed by some distribution.",
            "Let's say that out of it's the expectation of the reward and the policy.",
            "Here it's just a distribution over actions.",
            "And the game goes like this."
        ],
        [
            "With the connection according to this distribution, roti of a.",
            "We play this action will observe reward party and we don't see the rewards for all other actions.",
            "And my favorite example of this problem.",
            "Imagine that you have a web page where you can put advertisements.",
            "And let's say that you can put only one advertisement at a time.",
            "You put an advertisement.",
            "If the person that entered the web page clicks the advertisement, you get say 5 cents.",
            "If he doesn't like you don't get anything, and then the next person comes and you have to decide which advertisement to put and when you put the advertising to get the feedback for the advertisement that you put on the web page.",
            "But you don't know what would happen if you would put a different advertisement.",
            "So that's the problem and there are."
        ],
        [
            "An application so it's online advertising we can think about medical experiments as you have a patient, you give him some drug.",
            "Something happens to the patient.",
            "You have no idea what would have happened if you would give him a different drug and naturally want to minimize the number of patients that you kill along the process.",
            "It also appears from adaptive routing and many other problems.",
            "And."
        ],
        [
            "The main question here is this exploration exploitation tradeoff, which asks should we should we take the advertisement?",
            "For example, that was the best up to now.",
            "Or should we try some new advertisement?",
            "Or maybe we should give another try to an advertisement that was not the best up to now, but maybe it was just some statistical bias of the first few tries and that was actually great advertising.",
            "And this is the basic question and reinforcement learning and multi arm bandits.",
            "It's the simple problem where we already have this interesting tradeoff, so it's an excellent problem for studying this tradeoff.",
            "Now in Malta."
        ],
        [
            "Bandits with side information.",
            "We have some set of States and each state corresponds to a mountain bandit problem.",
            "An states drawing according to some fixed distribution.",
            "So let's go back to this example of a webpage.",
            "Assume that in addition to.",
            "Like somebody enters the web page and then you are told which country the login was made from.",
            "So this country information is some side information that you can use when you pick the advertisement.",
            "And it's like a state of the system, and then for each country you can think that you have a different multi unbanded problem that you are working with."
        ],
        [
            "So the game goes like this.",
            "You pick some policy, you observe this state.",
            "Let's say the country you play some action and you get the reward for playing that action for putting that advertisement."
        ],
        [
            "Um?",
            "Yeah, and generally the difference between this multi arm bandit problems with side information and Markov decision processes is that.",
            "The action that we make doesn't influence the next state that we will see.",
            "OK."
        ],
        [
            "So now what do we want to do in this?",
            "Multi unbanded with said information.",
            "So we want to design analyze models.",
            "That adapt to the complexity of the environment.",
            "More specifically, so let's say that.",
            "Instead of getting the country information, get a more detailed information.",
            "Let's say somebody says that you what's the city from which the login was made.",
            "If this information is relevant for decision, you want to use it.",
            "You want to get better predictions, but maybe this information is not relevant for what you are going to do.",
            "You don't want to pay for it just because your your Reporter space grows significantly because you have much more side information.",
            "So if you use classical analysis, just much bigger hypata space immediately have worse generalization bounds.",
            "But you don't want to suffer from this extra information if you are not using it.",
            "OK, so our goal is to design this system and analysis that will depend on the actual users of this site information and not just on the total amount of the site information that we get."
        ],
        [
            "And in a paper by Italian Daniel Planet, they suggested to measure the complexity of a policy as the mutual information between States and actions.",
            "That are used in order to make the action.",
            "So how much information about the state you are using?",
            "When you're playing.",
            "And."
        ],
        [
            "So generally policy, it's a conditional distribution over actions given the States and we take.",
            "Uniform distribution over the states get the joint distribution.",
            "Then we calculate the usual mutual information between States and actions.",
            "So if you have some stupid policy, you play the same action no matter what state is you have.",
            "Once again, very simple policy and the mutual information is 0.",
            "The more information you take, the more complex here policy becomes."
        ],
        [
            "Should not be appealed this."
        ],
        [
            "Where Sophie of S it's 1 / N it's it's the uniform.",
            "We assume we have an."
        ],
        [
            "Now what's the difference between supervised learning and reinforcement learning, and why we can't apply immediately?",
            "The results that I showed you previously?",
            "So this is the form of the bound for supervised learning and in supervised learning we can take any hypothesis and we can test it on the sample.",
            "We can check what's the error of each reporters on the sample that we have.",
            "In the enforcement learning, we can't take feedback from one action to say anything about another action.",
            "So we get that the sample size for different actions is different and then in the first attempt to apply this part based on analysis store enforcement, learning what people had here.",
            "They had the minimal number of times that any action was played in any state.",
            "But if we have a bad action, we don't want to play this bad action many times.",
            "But if we don't play it many times, we don't have the denominator of the bond increasing, so we don't get a good bound.",
            "So the first step."
        ],
        [
            "The solution will be how we actually solve this problem, and the solution is going to be important weighted sampling.",
            "So I define this important where it is sampling for each."
        ],
        [
            "Armed with defined this sealed reward, which is one over the probability of playing arm times reward.",
            "If you pick the arm and 0 otherwise.",
            "And as you know."
        ],
        [
            "From importance weight is something, so this random variables are unbiased estimates of the true expectation of the reward.",
            "The expectation of this random variable, given the history is the true expectation.",
            "And now we have this skilled reward for every action at every round of the game.",
            "So we kind of converted this partial information game into a full information game.",
            "Then"
        ],
        [
            "This difference is if we subtract from the reward expected reward.",
            "It's a martingale difference sequence.",
            "OK.",
            "So what we need now we need to extend this part based on analysis to martingales."
        ],
        [
            "Yeah, RT is the commute."
        ],
        [
            "This, no.",
            "It's instantaneous at time T. OK. And then like if you will look into the cumulative reward minus T times the expectation, then you will get the martingale."
        ],
        [
            "So just to remind you about a martingales for Martin Gale, different sequence sequence of random variables such that the expectation of a random variable given the past is 0.",
            "And Martin Gale, it's the sum of such martingale difference sequence and examples are random walks, capital of gambler and so on, OK?",
            "Um?",
            "Anne."
        ],
        [
            "What we're interested in, we are interested in multiple, simultaneously evolving and interdependent martingales.",
            "If you think about the small town bandit problem, for each action you have a Martin yelled at the balls overtime and there is definitely dependence between this martingales cause our strategy depends on what we saw before and even at the same time, because if we know that the reward for one action is positive and know that the reward for all other action is 0, by the way we define this variables.",
            "And accept this example of multi unbanded source.",
            "If we look for example into capital of multiple gamblers in zero sum game, that will also get multiple martingales that will simultaneously corresponding to each of the genders.",
            "And we will need in the."
        ],
        [
            "Variance of this martingale?",
            "So the variance is just defined as the sum over the expectation of the square of this martingale difference variables.",
            "More precisely, it's amount on this market variance.",
            "And."
        ],
        [
            "The point that we need the variance is that if we look into variance of the importance weighted sampling.",
            "So generally this important weighted samples.",
            "If we assume that the reward is bounded between zero and one, then the weighted samples are bounded between zero and one over this probability.",
            "But since the expectation is very close to 0, the variance instead of being the square of this interval, it's actually just the length of the interval.",
            "It's bounded by the length of the interval.",
            "And that's the reason why Bernstein type inequality's much better to use in this type of problems than holding type inequality because.",
            "We have very good balance on the balance of this process.",
            "I."
        ],
        [
            "So with the right partners version equality for martingales, which says that they have a fixed distribution over the hypothesis space than for any parameter Delta with probability greater than one minus Delta simultaneously.",
            "Once again, for all possible distributions that satisfies some technical conditions that I omitted.",
            "Here we have the average value of these martingales according to some distribution raw, it's bounded by the average variance.",
            "Times the KL divergent plus log of one over Delta.",
            "And those of you who know better than equality.",
            "The only difference with bench and equality for individual Martin Gale.",
            "It's this scale divergent Sturm.",
            "So by the price of this scale diversions term, instead of treating one single martingale, we're able to treat multiple martingales at the same time, and all possible averages software of this of the value of this martingales.",
            "OK."
        ],
        [
            "Well, how we get actually bounds from this."
        ],
        [
            "Well, for the mountain bandits with side information with different, we have to define a hypothesis space and the hippo to space that we take.",
            "It's all possible and deterministic prediction strategies.",
            "So each member of the hypothesis space just assigns one action to each state and you can notice that a policy actually defines a distribution over this Reporter space because we can go through all the action through all the stations.",
            "States and draw an action for each state according to this policy, and we get a member of this report's class.",
            "OK, and to your question, like why do we have uniform distribution over states?",
            "That's just the process that we speak about this so.",
            "For each state to just be connection.",
            "And well, I will not show you the whole direct."
        ],
        [
            "Well, they look tired.",
            "Just very briefly how we get it.",
            "So if we have our rotates, the expected reward of the policy and this is an empirical unbiased estimate that we get from the importance weighted sampling.",
            "So we can get the reward of a policy.",
            "We can write it as the expected reward minus the empirical reward plus the empirical reward and this difference.",
            "It's Martin Gale.",
            "Or more precisely, it's 1 / T times.",
            "Martin Gale.",
            "If we normalize it and then by the previous slide we get that this is bounded by the variance times the KL divergent.",
            "Divided by the number of samples and the empirical estimate of.",
            "The reward of a policy."
        ],
        [
            "And then for a certain choice of the prior distribution over the space which I want to show you, we can actually get the scale divergences bounded by the number of states times the mutual information between States and actions in the policy that we are playing.",
            "So for those who are interested, I can show offline how we get this calculation."
        ],
        [
            "And so John and I will not talk about regret, but in in the multi armed bandit problems usually what people analyze the don't analyze the reward, they analyze.",
            "What's the regret to the best thing that could be done in this problem?",
            "So this is maximum overall policies.",
            "The best reward that could be done and regretted just the difference between the best that could be done and what was done by the policy."
        ],
        [
            "And just to show you the plank strategies that we get from this.",
            "So that's the bound and the expected reward of a policy.",
            "And we have a tradeoff between the empirical reward and the mutual information."
        ],
        [
            "Which gives us this gibbsite distribution.",
            "Generally, it also can remind you the.",
            "This similar tradeoff under a distortion theory.",
            "And."
        ],
        [
            "The difference that we have is that so we take this usual solution of the red distortion theory and we add some small epsilon to the distribution in order to keep it away from zero.",
            "And the reason is this important weighted rewards 'cause we're going to divide by the probability of picking some action, and we don't want this probability to be zero or two close to 0, because otherwise we get the explosion in the variants.",
            "And finally."
        ],
        [
            "Technical things, so we don't explicitly calculate this marginal distribution over the actions, because for that we will have to go through all the States and instead we do some approximation of this marginal distribution of that just looks into the current state that displayed.",
            "So generally the complexity of all this algorithm is the number of K, which is the number of actions per round of the game.",
            "So it's very efficient algorithm.",
            "And we did several experiments."
        ],
        [
            "Finishing so in the first experiment, we have the first action, which is the best in all the states?",
            "So the entropy of the action choice of the best hypothesis zero in the second expiry."
        ],
        [
            "The first action is the best in 1/3 of the states, then the second action is the best and then the third action is the best.",
            "In"
        ],
        [
            "The third experiment will have seven actions, which are the best in different States and in the last experiments experiment all over the 20 actions are the best in some subset of the states.",
            "OK, and then we run the algorithm."
        ],
        [
            "And this is the regret.",
            "So the lower the better.",
            "And we get that for the simplest problem, the regret is lower.",
            "And then as the problem become, gets harder the regret.",
            "Growth and the baseline that you have here.",
            "It's the reward of the algorithm that just play some alter and bend it independently in each of the states so it doesn't propagate the information between the states.",
            "And you see that in all except the hardest problem, we are doing better than this based baseline.",
            "And just to show you something."
        ],
        [
            "Automation is actually propagated.",
            "We have this marginal distribution of."
        ],
        [
            "Of the actions."
        ],
        [
            "And we have all."
        ],
        [
            "So the mutual information graph where we have that the entropy the mutual information converges to the entropy of their corresponding reporters.",
            "So our algorithm actually adapts to the complexity of the environment.",
            "So just."
        ],
        [
            "To summarize, I've shown you concentration inequality is for weighted averages of multiple simultaneously holding an interdependant martingales.",
            "And most conservation qualities for individual martingales can be extended to weighted average.",
            "Is that the price of this scale diversions?",
            "I've shown you how."
        ],
        [
            "To apply it to importance with sampling and multi arm bandits with side information.",
            "And."
        ],
        [
            "Generally you can see it also as a tool to determine the optimal temperature to analyze the system based on empirically observed properties and the sample size.",
            "So thank you.",
            "Concentration."
        ],
        [
            "All of these are valid for any choice of office."
        ],
        [
            "Differentials.",
            "I."
        ],
        [
            "And then yeah, yeah.",
            "Yeah so is it."
        ],
        [
            "Impossible to derive.",
            "The policy you presented here is is, the policy is an arbitrary policy.",
            "In essence, no, yeah.",
            "And the concentration inequality somehow suggests an optimal point.",
            "So once again you have this.",
            "You have."
        ],
        [
            "Generally you can see it as a trade off between the empirical reward and mutual information, and if you optimize this tradeoff, that's the type of policy that you would get.",
            "Yeah, yeah.",
            "How difficult is it to extend this results to the MVP case and whatever difficulties that so the difficulties and the difference is that here we assume that you are taken between the States, but some process that doesn't depend on your actions.",
            "So we have a stationary distribution over States and if your actions influenced this transitions.",
            "We actually also change the stationary distribution over States and you have to control this change in order to get the bound, which is not very trivial.",
            "Need to save some time for large deviations."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the talk about progression analysis and it's a joint work with transfer.",
                    "label": 1
                },
                {
                    "sent": "All of you let Joshua Taylor, Peter, our Nicole, sister Bianca and Ronald partner.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What's general, it will be about first of all, I will show you some concentration inequality's for weighted averages of multiple, simultaneously evolving and interdependent martingales.",
                    "label": 1
                },
                {
                    "sent": "So as you know, martingales are frequently used in modeling different stochastic processes, and possibly you have some multiple simultaneously moving martingales in your work, so maybe our results will be useful for you.",
                    "label": 0
                },
                {
                    "sent": "And specif.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Reckless applications that I will show is application to importance.",
                    "label": 1
                },
                {
                    "sent": "Weighted something and applications to multi armed bandit problems which are a simple instance of reinforcement learning problems.",
                    "label": 0
                },
                {
                    "sent": "I will describe them when I get to it.",
                    "label": 0
                },
                {
                    "sent": "So I will start with some.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Background about Bayesian analysis and then I will go to applications of this Park Basin analysis to Malton.",
                    "label": 0
                },
                {
                    "sent": "Bandits with side information which will go through this three steps.",
                    "label": 1
                },
                {
                    "sent": "First will apply importance.",
                    "label": 0
                },
                {
                    "sent": "Weighted something in multi armed bandits.",
                    "label": 0
                },
                {
                    "sent": "Then will I will show you the spec.",
                    "label": 0
                },
                {
                    "sent": "Basing equalities for martingales and I will finish with how we actually derive some regret bounds for this small Karen bandit problems.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's start with some introduction to pack Bayesian analysis.",
                    "label": 0
                },
                {
                    "sent": "So first of all, what is Spock?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Box stays for probably approximately correct learning framework.",
                    "label": 1
                },
                {
                    "sent": "Approximately means that we want to provide guarantees on the approximation error of empirical estimates.",
                    "label": 1
                },
                {
                    "sent": "And probably that holds with high probability with respect to representativeness of the observed sample.",
                    "label": 1
                },
                {
                    "sent": "So that is all this.",
                    "label": 0
                },
                {
                    "sent": "Some probability sample is not representative, otherwise we get high probability guarantee and.",
                    "label": 0
                },
                {
                    "sent": "Estimates",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some basic definitions from supervised learning in order to show you this result.",
                    "label": 0
                },
                {
                    "sent": "So we have some sample space.",
                    "label": 0
                },
                {
                    "sent": "We have a label space.",
                    "label": 0
                },
                {
                    "sent": "We have a long loss function for giving current label to a sample.",
                    "label": 0
                },
                {
                    "sent": "We have some hypothesis space.",
                    "label": 0
                },
                {
                    "sent": "Age of exits.",
                    "label": 0
                },
                {
                    "sent": "The prediction of a hypothesis from H on the sample X would define expected loss of thank you.",
                    "label": 1
                },
                {
                    "sent": "Both this and the empirical loss of the parties on the sample that were there.",
                    "label": 0
                },
                {
                    "sent": "So I assume that you observe and samples M pairs of X&Y and for each prediction rule we calculate what's the error.",
                    "label": 0
                },
                {
                    "sent": "That prediction rule makes on those M samples.",
                    "label": 0
                },
                {
                    "sent": "And we are going to bound the distance between this empirical estimation of the loss of the prediction rule and the expected error of the loss of this prediction rule when it sees some new samples.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We are going to talk about randomized classifiers and randomized classifier.",
                    "label": 0
                },
                {
                    "sent": "Its distribution over this report is Class H and what we do at each round of the game we pick a classifier according to this distribution of H. We observe the sample and we return the label that the hypothesis predicts on the sample.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The loss of this prediction rule, so the expected loss.",
                    "label": 0
                },
                {
                    "sent": "It's the expectation to respect to this probability of picking age of the loss of the correspondent corresponding report this, and we can also think about this as an infinite or I'll dimensional vector right as a dot product and amp.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Take a loss.",
                    "label": 0
                },
                {
                    "sent": "It's the average of the empirical losses of the parties.",
                    "label": 0
                },
                {
                    "sent": "And now bug basehor.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I think in the quality which goes back to the work of McAllister in 1999 ninety eight, it says that.",
                    "label": 0
                },
                {
                    "sent": "So let's say that the loss is bounded in the 01 interval.",
                    "label": 0
                },
                {
                    "sent": "It says that if we fix a reference distribution \u03c0 / H, then for any parameter Delta with probability greater than one minus Delta over the sample for all possible posterior distributions throw simultaneously the loss of this randomized predictor is bounded by its empirical loss plus.",
                    "label": 1
                },
                {
                    "sent": "Square root of the KL divergent between the posterior and prior.",
                    "label": 0
                },
                {
                    "sent": "Log of one over Delta divided by two times the sample size.",
                    "label": 0
                },
                {
                    "sent": "Came up.",
                    "label": 0
                },
                {
                    "sent": "Let's see if it works.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The intuition behind the bounds.",
                    "label": 1
                },
                {
                    "sent": "So if we take the scale, the versions, which is the average of the logarithm of the ratio between the procedure and the prior distribution.",
                    "label": 1
                },
                {
                    "sent": "So the first step here, it's the description length.",
                    "label": 0
                },
                {
                    "sent": "So if we assume that we used our prior in order to encode each of the hypothesis, that's the number of bits that will need in order to specify the hypothesis that we are picking according to Raw and H for it's the entropy of this theory distribution.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What the bound tells us, it tells us to pick this posterior distribution Rd that minimizes the tradeoff between the empirical loss of each hypothesis.",
                    "label": 1
                },
                {
                    "sent": "The complexity of the corresponding Potters and has maximum entropy.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're kind of recover the maximum entropy principle, but from the perspective of this generalization error, so we actually get that this maximum entropy principle provides low errors with high probability.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "But is this found available?",
                    "label": 0
                },
                {
                    "sent": "I mean there there is a distribution that achieves inequality in this inequality.",
                    "label": 0
                },
                {
                    "sent": "And in that sense, yeah.",
                    "label": 0
                },
                {
                    "sent": "OK and well, the point is that we have to pick this prior distribution before the observed the sample, so there are no miracles.",
                    "label": 0
                },
                {
                    "sent": "If we come with good prior knowledge we can get good bounds.",
                    "label": 0
                },
                {
                    "sent": "If you don't have good prior knowledge, the bounds still holds, but it may be not very tight bound and since this bound holds for all possible posterior distributions through our algorithm can actually look through different.",
                    "label": 0
                },
                {
                    "sent": "Posterior distributions and pick the one that minimizes the bound.",
                    "label": 0
                },
                {
                    "sent": "What's?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The relation between this Pug basin approach and based on learning so the relation is that as in Bayesian learning, we have an explicit way to incorporate prior information.",
                    "label": 1
                },
                {
                    "sent": "Our learning where this prior distribution \u03c0.",
                    "label": 0
                },
                {
                    "sent": "But there are several diff.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Answers, so first of all, on the unlike in Bayesian learning where we get this posterior distribution, but we have no idea how well this posterior distribution is going to perform here.",
                    "label": 0
                },
                {
                    "sent": "Will get explicit high probability guarantee on the expected performance of this classifier.",
                    "label": 1
                },
                {
                    "sent": "With.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I don't believe that our priorities correct in basin learning.",
                    "label": 0
                },
                {
                    "sent": "Usually we assume that the data is generated like this prior.",
                    "label": 0
                },
                {
                    "sent": "Is the correct prior over generating the parameters of the model.",
                    "label": 0
                },
                {
                    "sent": "Here we don't assume any correctness of our prior, it's frequent is bound if it come with bad prior assumptions will get a bed bound, but it still abound.",
                    "label": 0
                },
                {
                    "sent": "It holds with high probability.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have explicit dependence on the loss function in the bound.",
                    "label": 0
                },
                {
                    "sent": "Also, unlike in basin learning, if we change the loss function, we are going to change our classifier because the loss function explicitly appears in the bound.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It happens all the time.",
                    "label": 0
                },
                {
                    "sent": "What's typical in decision theory?",
                    "label": 0
                },
                {
                    "sent": "Explicit dependence on the loss function?",
                    "label": 1
                },
                {
                    "sent": "How you make a decision loss?",
                    "label": 0
                },
                {
                    "sent": "Yeah, but you don't have it in the basin running because that'll just compute the posterior.",
                    "label": 0
                },
                {
                    "sent": "So that's the difference between the two.",
                    "label": 1
                },
                {
                    "sent": "We have a bit different weighting of the prior belief versus the.",
                    "label": 0
                },
                {
                    "sent": "Evidence that comes from this Mount.",
                    "label": 0
                },
                {
                    "sent": "And finally, our.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And holds for any posterior distribution role, and in particular it holds for their base posterior, so we can actually take the base posterior and calculate what's the expected error of their base posterior using color formula.",
                    "label": 1
                },
                {
                    "sent": "So in the innovation formulation.",
                    "label": 1
                },
                {
                    "sent": "You have the loss function enters in the likelihood term, right?",
                    "label": 0
                },
                {
                    "sent": "So you have.",
                    "label": 0
                },
                {
                    "sent": "The likelihood is the likelihood it's.",
                    "label": 0
                },
                {
                    "sent": "The last that it's the log loss because you are calculating the that you're calculating the maximum posterior distribution and here this loss function can be 01.",
                    "label": 0
                },
                {
                    "sent": "It can be quadratic loss, absolute loss.",
                    "label": 0
                },
                {
                    "sent": "It can be also log loss, and then you will recover the.",
                    "label": 0
                },
                {
                    "sent": "The patient saw.",
                    "label": 0
                },
                {
                    "sent": "OK. Yeah, less than.",
                    "label": 0
                },
                {
                    "sent": "Goal is is all about universal constant times, so I've highlighted some constants and some logarithmic factors, but I mean.",
                    "label": 0
                },
                {
                    "sent": "Office doesn't seem to show up in your bombing.",
                    "label": 0
                },
                {
                    "sent": "That seems a bit suspicious.",
                    "label": 0
                },
                {
                    "sent": "The number of the purchases appears.",
                    "label": 0
                },
                {
                    "sent": "We are the scale diversions.",
                    "label": 0
                },
                {
                    "sent": "Child is uniform over all rows, and so you'd expect something like the covering number of the number of probability measures on their buses to show up quite beside the case.",
                    "label": 0
                },
                {
                    "sent": "So if you have a finite number of hypothesis, you can put a uniform prior and then the scale will be just logarithm of the number of reporters is.",
                    "label": 0
                },
                {
                    "sent": "A bit less like logarithm Holden, logarithm minus the entropy.",
                    "label": 0
                },
                {
                    "sent": "But you can also work with infinite classes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so and I'll show in the moment how actually get the bound, so there will be no no mysteries how it happens.",
                    "label": 0
                },
                {
                    "sent": "For the entropy, which is why you get it all balanced for all distributions in the things.",
                    "label": 0
                },
                {
                    "sent": "In a sense, I will see it in a couple of slides.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what's the relation?",
                    "label": 0
                },
                {
                    "sent": "Difference with buckling children and children anchors, theory and Rademacher complexities?",
                    "label": 1
                },
                {
                    "sent": "For those who know, so the relation is that we get explicit high probability guarantee on the expected performance, and we get explicit dependence on the loss function and the differences.",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first of all, the complexities defined individually for each hypothesis.",
                    "label": 1
                },
                {
                    "sent": "We're this prior definition, unlike for those conole visit dimension have visit dimension of the porters class, and you can't distinguish between individual hypothesis in this report's class.",
                    "label": 0
                },
                {
                    "sent": "And we have an explicit way to incorporate prior knowledge.",
                    "label": 1
                },
                {
                    "sent": "We are this prior which we can do in the VC type approaches.",
                    "label": 0
                },
                {
                    "sent": "Now what's the relation to?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Statistical physics if we look into this inequality, we can rewrite it as a parameterized tradeoff between the empirical loss and the KL diversions.",
                    "label": 1
                },
                {
                    "sent": "We can actually make a linear search over this better parameter, which is inverse temperature parameter.",
                    "label": 0
                },
                {
                    "sent": "We can optimize it for every possible temple temperature, or like for a grid of temperatures we can substitute it back into the bound and we can find what is actually the correct temperature finalize our system given the sample size that we have an given, the properties of the system which are expressed in this empirical observation so.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Bound provides the optimal temperature to study the system.",
                    "label": 1
                },
                {
                    "sent": "Depending on the size of the sample of the number of observations from the system and the empirical properties of the system which are related to this observation that we get.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So now.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Proof idea, so the proof is based on don't care but variational definition of KL diversions which was mentioned already by Sandra.",
                    "label": 1
                },
                {
                    "sent": "So this variational definition says what's written on the slide.",
                    "label": 0
                },
                {
                    "sent": "And if we just change sides.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This definition we get the average of a function F with respect to some distribution.",
                    "label": 1
                },
                {
                    "sent": "Role is bounded by this scale.",
                    "label": 0
                },
                {
                    "sent": "Diversions plus logarithm of the average of the exponent of the function with respect to some other distribution file, and this generally holds for any pair of distributions properly defined.",
                    "label": 1
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, a bit more background, Markov's inequality.",
                    "label": 0
                },
                {
                    "sent": "I guess everybody knows here and.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Holdings inequality says that if we have a number of independent random variables and then say 01 interval, then for any parameter Lambda the expectation of this variables minus the empirical average is bounded by the exponent on the right hand side.",
                    "label": 0
                },
                {
                    "sent": "So that's all we need.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Prove the theorem.",
                    "label": 0
                },
                {
                    "sent": "We start with this change of measure inequality and what we do.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Take this F of H as.",
                    "label": 0
                },
                {
                    "sent": "Model is related to the function of interest, so we take it as Lambda.",
                    "label": 0
                },
                {
                    "sent": "The difference between the expected and empirical loss of each hypothesis.",
                    "label": 0
                },
                {
                    "sent": "So we are going to analyze the average of this difference with respect to posterior distribution raw.",
                    "label": 0
                },
                {
                    "sent": "And now in order to get the bond, we have to bound.",
                    "label": 0
                },
                {
                    "sent": "The average of the exponent of this function with respect to distribution file.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How we do it.",
                    "label": 0
                },
                {
                    "sent": "We take this quantity and we consider it as a random variable.",
                    "label": 0
                },
                {
                    "sent": "And then by Markov's inequality, this random variable is bounded by one over Delta.",
                    "label": 0
                },
                {
                    "sent": "The expectation of this random variable.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Expectation of reward expectation over the sample so.",
                    "label": 0
                },
                {
                    "sent": "So you have some randomness in the sample, so we take the expectation with respect to the sample of this random variable.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now since the dot product and expectation a linear, we can take the expectation inside the DOT product and here the important point is that the prior should be independent of the sample.",
                    "label": 0
                },
                {
                    "sent": "If the prior is independent of sample, we can take the expectation inside.",
                    "label": 0
                },
                {
                    "sent": "And now this expectation.",
                    "label": 0
                },
                {
                    "sent": "That's what we can bound using coding chemical.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pilot there.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now we have an average of a constant, so we just get this constant out.",
                    "label": 0
                },
                {
                    "sent": "OK. And so this is actually the point where we can combine this not only withholding inequality.",
                    "label": 0
                },
                {
                    "sent": "We can combine it with version equality or many other inequality's in probability theory, because most of them are based on bounding this expectation of the exponent, so that's the connection point to many other inequality's, both for independent random variables.",
                    "label": 0
                },
                {
                    "sent": "And also we can do the same for martingales.",
                    "label": 0
                },
                {
                    "sent": "It's going to be the same trick.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we got that this logarithm is bounded by what's written on the slide, we just.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Apps that use that the normalized by Lambda, so that's already the quantity of.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stressed and we optimize over land.",
                    "label": 0
                },
                {
                    "sent": "I'm skipping this step, but it's not very difficult and we'll get what we wanted to get.",
                    "label": 0
                },
                {
                    "sent": "OK and then questions.",
                    "label": 0
                },
                {
                    "sent": "Up to now.",
                    "label": 0
                },
                {
                    "sent": "Good.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we want to take this tool and to take it to multi arm bandits problem in reinforcement learning so.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At the mall town bandits in amount.",
                    "label": 0
                },
                {
                    "sent": "Adam bandit problem.",
                    "label": 0
                },
                {
                    "sent": "We have a set of actions.",
                    "label": 1
                },
                {
                    "sent": "Each action gives us some reward, distributed by some distribution.",
                    "label": 0
                },
                {
                    "sent": "Let's say that out of it's the expectation of the reward and the policy.",
                    "label": 0
                },
                {
                    "sent": "Here it's just a distribution over actions.",
                    "label": 0
                },
                {
                    "sent": "And the game goes like this.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With the connection according to this distribution, roti of a.",
                    "label": 0
                },
                {
                    "sent": "We play this action will observe reward party and we don't see the rewards for all other actions.",
                    "label": 1
                },
                {
                    "sent": "And my favorite example of this problem.",
                    "label": 0
                },
                {
                    "sent": "Imagine that you have a web page where you can put advertisements.",
                    "label": 0
                },
                {
                    "sent": "And let's say that you can put only one advertisement at a time.",
                    "label": 0
                },
                {
                    "sent": "You put an advertisement.",
                    "label": 0
                },
                {
                    "sent": "If the person that entered the web page clicks the advertisement, you get say 5 cents.",
                    "label": 0
                },
                {
                    "sent": "If he doesn't like you don't get anything, and then the next person comes and you have to decide which advertisement to put and when you put the advertising to get the feedback for the advertisement that you put on the web page.",
                    "label": 0
                },
                {
                    "sent": "But you don't know what would happen if you would put a different advertisement.",
                    "label": 0
                },
                {
                    "sent": "So that's the problem and there are.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An application so it's online advertising we can think about medical experiments as you have a patient, you give him some drug.",
                    "label": 0
                },
                {
                    "sent": "Something happens to the patient.",
                    "label": 0
                },
                {
                    "sent": "You have no idea what would have happened if you would give him a different drug and naturally want to minimize the number of patients that you kill along the process.",
                    "label": 0
                },
                {
                    "sent": "It also appears from adaptive routing and many other problems.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The main question here is this exploration exploitation tradeoff, which asks should we should we take the advertisement?",
                    "label": 0
                },
                {
                    "sent": "For example, that was the best up to now.",
                    "label": 0
                },
                {
                    "sent": "Or should we try some new advertisement?",
                    "label": 1
                },
                {
                    "sent": "Or maybe we should give another try to an advertisement that was not the best up to now, but maybe it was just some statistical bias of the first few tries and that was actually great advertising.",
                    "label": 0
                },
                {
                    "sent": "And this is the basic question and reinforcement learning and multi arm bandits.",
                    "label": 0
                },
                {
                    "sent": "It's the simple problem where we already have this interesting tradeoff, so it's an excellent problem for studying this tradeoff.",
                    "label": 0
                },
                {
                    "sent": "Now in Malta.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Bandits with side information.",
                    "label": 0
                },
                {
                    "sent": "We have some set of States and each state corresponds to a mountain bandit problem.",
                    "label": 1
                },
                {
                    "sent": "An states drawing according to some fixed distribution.",
                    "label": 0
                },
                {
                    "sent": "So let's go back to this example of a webpage.",
                    "label": 0
                },
                {
                    "sent": "Assume that in addition to.",
                    "label": 0
                },
                {
                    "sent": "Like somebody enters the web page and then you are told which country the login was made from.",
                    "label": 0
                },
                {
                    "sent": "So this country information is some side information that you can use when you pick the advertisement.",
                    "label": 0
                },
                {
                    "sent": "And it's like a state of the system, and then for each country you can think that you have a different multi unbanded problem that you are working with.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the game goes like this.",
                    "label": 0
                },
                {
                    "sent": "You pick some policy, you observe this state.",
                    "label": 0
                },
                {
                    "sent": "Let's say the country you play some action and you get the reward for playing that action for putting that advertisement.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Yeah, and generally the difference between this multi arm bandit problems with side information and Markov decision processes is that.",
                    "label": 1
                },
                {
                    "sent": "The action that we make doesn't influence the next state that we will see.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now what do we want to do in this?",
                    "label": 0
                },
                {
                    "sent": "Multi unbanded with said information.",
                    "label": 0
                },
                {
                    "sent": "So we want to design analyze models.",
                    "label": 0
                },
                {
                    "sent": "That adapt to the complexity of the environment.",
                    "label": 1
                },
                {
                    "sent": "More specifically, so let's say that.",
                    "label": 0
                },
                {
                    "sent": "Instead of getting the country information, get a more detailed information.",
                    "label": 0
                },
                {
                    "sent": "Let's say somebody says that you what's the city from which the login was made.",
                    "label": 0
                },
                {
                    "sent": "If this information is relevant for decision, you want to use it.",
                    "label": 0
                },
                {
                    "sent": "You want to get better predictions, but maybe this information is not relevant for what you are going to do.",
                    "label": 0
                },
                {
                    "sent": "You don't want to pay for it just because your your Reporter space grows significantly because you have much more side information.",
                    "label": 0
                },
                {
                    "sent": "So if you use classical analysis, just much bigger hypata space immediately have worse generalization bounds.",
                    "label": 0
                },
                {
                    "sent": "But you don't want to suffer from this extra information if you are not using it.",
                    "label": 0
                },
                {
                    "sent": "OK, so our goal is to design this system and analysis that will depend on the actual users of this site information and not just on the total amount of the site information that we get.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in a paper by Italian Daniel Planet, they suggested to measure the complexity of a policy as the mutual information between States and actions.",
                    "label": 1
                },
                {
                    "sent": "That are used in order to make the action.",
                    "label": 0
                },
                {
                    "sent": "So how much information about the state you are using?",
                    "label": 0
                },
                {
                    "sent": "When you're playing.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So generally policy, it's a conditional distribution over actions given the States and we take.",
                    "label": 0
                },
                {
                    "sent": "Uniform distribution over the states get the joint distribution.",
                    "label": 0
                },
                {
                    "sent": "Then we calculate the usual mutual information between States and actions.",
                    "label": 1
                },
                {
                    "sent": "So if you have some stupid policy, you play the same action no matter what state is you have.",
                    "label": 0
                },
                {
                    "sent": "Once again, very simple policy and the mutual information is 0.",
                    "label": 0
                },
                {
                    "sent": "The more information you take, the more complex here policy becomes.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Should not be appealed this.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where Sophie of S it's 1 / N it's it's the uniform.",
                    "label": 0
                },
                {
                    "sent": "We assume we have an.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now what's the difference between supervised learning and reinforcement learning, and why we can't apply immediately?",
                    "label": 1
                },
                {
                    "sent": "The results that I showed you previously?",
                    "label": 0
                },
                {
                    "sent": "So this is the form of the bound for supervised learning and in supervised learning we can take any hypothesis and we can test it on the sample.",
                    "label": 0
                },
                {
                    "sent": "We can check what's the error of each reporters on the sample that we have.",
                    "label": 0
                },
                {
                    "sent": "In the enforcement learning, we can't take feedback from one action to say anything about another action.",
                    "label": 0
                },
                {
                    "sent": "So we get that the sample size for different actions is different and then in the first attempt to apply this part based on analysis store enforcement, learning what people had here.",
                    "label": 0
                },
                {
                    "sent": "They had the minimal number of times that any action was played in any state.",
                    "label": 0
                },
                {
                    "sent": "But if we have a bad action, we don't want to play this bad action many times.",
                    "label": 0
                },
                {
                    "sent": "But if we don't play it many times, we don't have the denominator of the bond increasing, so we don't get a good bound.",
                    "label": 0
                },
                {
                    "sent": "So the first step.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The solution will be how we actually solve this problem, and the solution is going to be important weighted sampling.",
                    "label": 0
                },
                {
                    "sent": "So I define this important where it is sampling for each.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Armed with defined this sealed reward, which is one over the probability of playing arm times reward.",
                    "label": 0
                },
                {
                    "sent": "If you pick the arm and 0 otherwise.",
                    "label": 1
                },
                {
                    "sent": "And as you know.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From importance weight is something, so this random variables are unbiased estimates of the true expectation of the reward.",
                    "label": 0
                },
                {
                    "sent": "The expectation of this random variable, given the history is the true expectation.",
                    "label": 0
                },
                {
                    "sent": "And now we have this skilled reward for every action at every round of the game.",
                    "label": 0
                },
                {
                    "sent": "So we kind of converted this partial information game into a full information game.",
                    "label": 0
                },
                {
                    "sent": "Then",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This difference is if we subtract from the reward expected reward.",
                    "label": 0
                },
                {
                    "sent": "It's a martingale difference sequence.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So what we need now we need to extend this part based on analysis to martingales.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, RT is the commute.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This, no.",
                    "label": 0
                },
                {
                    "sent": "It's instantaneous at time T. OK. And then like if you will look into the cumulative reward minus T times the expectation, then you will get the martingale.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just to remind you about a martingales for Martin Gale, different sequence sequence of random variables such that the expectation of a random variable given the past is 0.",
                    "label": 0
                },
                {
                    "sent": "And Martin Gale, it's the sum of such martingale difference sequence and examples are random walks, capital of gambler and so on, OK?",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What we're interested in, we are interested in multiple, simultaneously evolving and interdependent martingales.",
                    "label": 1
                },
                {
                    "sent": "If you think about the small town bandit problem, for each action you have a Martin yelled at the balls overtime and there is definitely dependence between this martingales cause our strategy depends on what we saw before and even at the same time, because if we know that the reward for one action is positive and know that the reward for all other action is 0, by the way we define this variables.",
                    "label": 0
                },
                {
                    "sent": "And accept this example of multi unbanded source.",
                    "label": 1
                },
                {
                    "sent": "If we look for example into capital of multiple gamblers in zero sum game, that will also get multiple martingales that will simultaneously corresponding to each of the genders.",
                    "label": 0
                },
                {
                    "sent": "And we will need in the.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Variance of this martingale?",
                    "label": 0
                },
                {
                    "sent": "So the variance is just defined as the sum over the expectation of the square of this martingale difference variables.",
                    "label": 1
                },
                {
                    "sent": "More precisely, it's amount on this market variance.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The point that we need the variance is that if we look into variance of the importance weighted sampling.",
                    "label": 1
                },
                {
                    "sent": "So generally this important weighted samples.",
                    "label": 0
                },
                {
                    "sent": "If we assume that the reward is bounded between zero and one, then the weighted samples are bounded between zero and one over this probability.",
                    "label": 0
                },
                {
                    "sent": "But since the expectation is very close to 0, the variance instead of being the square of this interval, it's actually just the length of the interval.",
                    "label": 0
                },
                {
                    "sent": "It's bounded by the length of the interval.",
                    "label": 0
                },
                {
                    "sent": "And that's the reason why Bernstein type inequality's much better to use in this type of problems than holding type inequality because.",
                    "label": 1
                },
                {
                    "sent": "We have very good balance on the balance of this process.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So with the right partners version equality for martingales, which says that they have a fixed distribution over the hypothesis space than for any parameter Delta with probability greater than one minus Delta simultaneously.",
                    "label": 1
                },
                {
                    "sent": "Once again, for all possible distributions that satisfies some technical conditions that I omitted.",
                    "label": 0
                },
                {
                    "sent": "Here we have the average value of these martingales according to some distribution raw, it's bounded by the average variance.",
                    "label": 0
                },
                {
                    "sent": "Times the KL divergent plus log of one over Delta.",
                    "label": 0
                },
                {
                    "sent": "And those of you who know better than equality.",
                    "label": 0
                },
                {
                    "sent": "The only difference with bench and equality for individual Martin Gale.",
                    "label": 0
                },
                {
                    "sent": "It's this scale divergent Sturm.",
                    "label": 0
                },
                {
                    "sent": "So by the price of this scale diversions term, instead of treating one single martingale, we're able to treat multiple martingales at the same time, and all possible averages software of this of the value of this martingales.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, how we get actually bounds from this.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, for the mountain bandits with side information with different, we have to define a hypothesis space and the hippo to space that we take.",
                    "label": 1
                },
                {
                    "sent": "It's all possible and deterministic prediction strategies.",
                    "label": 1
                },
                {
                    "sent": "So each member of the hypothesis space just assigns one action to each state and you can notice that a policy actually defines a distribution over this Reporter space because we can go through all the action through all the stations.",
                    "label": 1
                },
                {
                    "sent": "States and draw an action for each state according to this policy, and we get a member of this report's class.",
                    "label": 0
                },
                {
                    "sent": "OK, and to your question, like why do we have uniform distribution over states?",
                    "label": 0
                },
                {
                    "sent": "That's just the process that we speak about this so.",
                    "label": 0
                },
                {
                    "sent": "For each state to just be connection.",
                    "label": 0
                },
                {
                    "sent": "And well, I will not show you the whole direct.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, they look tired.",
                    "label": 0
                },
                {
                    "sent": "Just very briefly how we get it.",
                    "label": 0
                },
                {
                    "sent": "So if we have our rotates, the expected reward of the policy and this is an empirical unbiased estimate that we get from the importance weighted sampling.",
                    "label": 1
                },
                {
                    "sent": "So we can get the reward of a policy.",
                    "label": 0
                },
                {
                    "sent": "We can write it as the expected reward minus the empirical reward plus the empirical reward and this difference.",
                    "label": 0
                },
                {
                    "sent": "It's Martin Gale.",
                    "label": 0
                },
                {
                    "sent": "Or more precisely, it's 1 / T times.",
                    "label": 0
                },
                {
                    "sent": "Martin Gale.",
                    "label": 0
                },
                {
                    "sent": "If we normalize it and then by the previous slide we get that this is bounded by the variance times the KL divergent.",
                    "label": 1
                },
                {
                    "sent": "Divided by the number of samples and the empirical estimate of.",
                    "label": 0
                },
                {
                    "sent": "The reward of a policy.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then for a certain choice of the prior distribution over the space which I want to show you, we can actually get the scale divergences bounded by the number of states times the mutual information between States and actions in the policy that we are playing.",
                    "label": 0
                },
                {
                    "sent": "So for those who are interested, I can show offline how we get this calculation.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so John and I will not talk about regret, but in in the multi armed bandit problems usually what people analyze the don't analyze the reward, they analyze.",
                    "label": 0
                },
                {
                    "sent": "What's the regret to the best thing that could be done in this problem?",
                    "label": 0
                },
                {
                    "sent": "So this is maximum overall policies.",
                    "label": 0
                },
                {
                    "sent": "The best reward that could be done and regretted just the difference between the best that could be done and what was done by the policy.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And just to show you the plank strategies that we get from this.",
                    "label": 0
                },
                {
                    "sent": "So that's the bound and the expected reward of a policy.",
                    "label": 0
                },
                {
                    "sent": "And we have a tradeoff between the empirical reward and the mutual information.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which gives us this gibbsite distribution.",
                    "label": 0
                },
                {
                    "sent": "Generally, it also can remind you the.",
                    "label": 0
                },
                {
                    "sent": "This similar tradeoff under a distortion theory.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The difference that we have is that so we take this usual solution of the red distortion theory and we add some small epsilon to the distribution in order to keep it away from zero.",
                    "label": 0
                },
                {
                    "sent": "And the reason is this important weighted rewards 'cause we're going to divide by the probability of picking some action, and we don't want this probability to be zero or two close to 0, because otherwise we get the explosion in the variants.",
                    "label": 0
                },
                {
                    "sent": "And finally.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Technical things, so we don't explicitly calculate this marginal distribution over the actions, because for that we will have to go through all the States and instead we do some approximation of this marginal distribution of that just looks into the current state that displayed.",
                    "label": 0
                },
                {
                    "sent": "So generally the complexity of all this algorithm is the number of K, which is the number of actions per round of the game.",
                    "label": 0
                },
                {
                    "sent": "So it's very efficient algorithm.",
                    "label": 0
                },
                {
                    "sent": "And we did several experiments.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finishing so in the first experiment, we have the first action, which is the best in all the states?",
                    "label": 0
                },
                {
                    "sent": "So the entropy of the action choice of the best hypothesis zero in the second expiry.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The first action is the best in 1/3 of the states, then the second action is the best and then the third action is the best.",
                    "label": 0
                },
                {
                    "sent": "In",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The third experiment will have seven actions, which are the best in different States and in the last experiments experiment all over the 20 actions are the best in some subset of the states.",
                    "label": 0
                },
                {
                    "sent": "OK, and then we run the algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is the regret.",
                    "label": 0
                },
                {
                    "sent": "So the lower the better.",
                    "label": 0
                },
                {
                    "sent": "And we get that for the simplest problem, the regret is lower.",
                    "label": 0
                },
                {
                    "sent": "And then as the problem become, gets harder the regret.",
                    "label": 0
                },
                {
                    "sent": "Growth and the baseline that you have here.",
                    "label": 0
                },
                {
                    "sent": "It's the reward of the algorithm that just play some alter and bend it independently in each of the states so it doesn't propagate the information between the states.",
                    "label": 0
                },
                {
                    "sent": "And you see that in all except the hardest problem, we are doing better than this based baseline.",
                    "label": 0
                },
                {
                    "sent": "And just to show you something.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Automation is actually propagated.",
                    "label": 0
                },
                {
                    "sent": "We have this marginal distribution of.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of the actions.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we have all.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the mutual information graph where we have that the entropy the mutual information converges to the entropy of their corresponding reporters.",
                    "label": 0
                },
                {
                    "sent": "So our algorithm actually adapts to the complexity of the environment.",
                    "label": 0
                },
                {
                    "sent": "So just.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To summarize, I've shown you concentration inequality is for weighted averages of multiple simultaneously holding an interdependant martingales.",
                    "label": 0
                },
                {
                    "sent": "And most conservation qualities for individual martingales can be extended to weighted average.",
                    "label": 0
                },
                {
                    "sent": "Is that the price of this scale diversions?",
                    "label": 0
                },
                {
                    "sent": "I've shown you how.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To apply it to importance with sampling and multi arm bandits with side information.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Generally you can see it also as a tool to determine the optimal temperature to analyze the system based on empirically observed properties and the sample size.",
                    "label": 0
                },
                {
                    "sent": "So thank you.",
                    "label": 0
                },
                {
                    "sent": "Concentration.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All of these are valid for any choice of office.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Differentials.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah so is it.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Impossible to derive.",
                    "label": 0
                },
                {
                    "sent": "The policy you presented here is is, the policy is an arbitrary policy.",
                    "label": 0
                },
                {
                    "sent": "In essence, no, yeah.",
                    "label": 0
                },
                {
                    "sent": "And the concentration inequality somehow suggests an optimal point.",
                    "label": 0
                },
                {
                    "sent": "So once again you have this.",
                    "label": 0
                },
                {
                    "sent": "You have.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Generally you can see it as a trade off between the empirical reward and mutual information, and if you optimize this tradeoff, that's the type of policy that you would get.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "How difficult is it to extend this results to the MVP case and whatever difficulties that so the difficulties and the difference is that here we assume that you are taken between the States, but some process that doesn't depend on your actions.",
                    "label": 0
                },
                {
                    "sent": "So we have a stationary distribution over States and if your actions influenced this transitions.",
                    "label": 0
                },
                {
                    "sent": "We actually also change the stationary distribution over States and you have to control this change in order to get the bound, which is not very trivial.",
                    "label": 0
                },
                {
                    "sent": "Need to save some time for large deviations.",
                    "label": 0
                }
            ]
        }
    }
}