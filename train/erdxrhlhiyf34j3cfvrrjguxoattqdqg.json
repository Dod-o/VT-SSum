{
    "id": "erdxrhlhiyf34j3cfvrrjguxoattqdqg",
    "title": "Fast Exact Matrix Completion with Finite Samples",
    "info": {
        "author": [
            "Praneeth Netrapalli, Department of Electrical and Computer Engineering, University of Texas at Austin"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_netrapalli_finite_samples/",
    "segmentation": [
        [
            "I'll briefly describe our result on fast matrix fast algorithm for matrix completion.",
            "This is joint work with Prateek, both of Microsoft Research."
        ],
        [
            "So the low rank matrix completion problem we just saw is that we are given partial entries of a matrix and we want to fill in the rest under the assumption that the matrix is low rank.",
            "Even the low rank assumption this problem is in general not well posed, so we have an additional assumption which says that the mass of the entries of the matrix we spread over most of the entries of the matrix and is not concentrated in very few elements."
        ],
        [
            "And this has applications and recommendation systems and PC with missing address and so on.",
            "OK, so.",
            "Our results for this problem is that if you're given the rank are incoherent matrix, we can complete the matrix observing.",
            "Sorry, so it's NR to the five la Cuban samples.",
            "And with the computational complexity of NR to the 7 lock Cuban and log in, or epsilon where we want up to accuracy epsilon.",
            "Just to compare this with prior work, there are two main approaches of algorithms for solving this problem.",
            "The first one is using convex relaxation.",
            "The convex relaxation approach.",
            "As the optimal sample complexity depends dependency of NR, polylog, N, but the runtime of the algorithm is cubic in North, so it does not scale very well for large scale problems.",
            "On the other hand, there is alternating minimization where the time complexity is almost linear in North, but the sample complexity depends on the condition number of the input matrix as well as the required accuracy that we want to get to.",
            "So.",
            "Our algorithm gets rid of this weakness of alternating minimization.",
            "There is no dependence on the condition, number of matrix, or the desired accuracy.",
            "But is suboptimal and its dependence on the rank of the matrix."
        ],
        [
            "So the problem can be formulated as a nonconvex problem where M is the given matrix and we want to index is the low rank matrix that we want to recover.",
            "We minimize the error on the observed entries, so summation I Jane Omega where Omega is observed entries MIT minus six Idaho Square subject to the rank constraint on X.",
            "The rank constraint on X causes this problem to be nonconvex, and so standard analysis of gradient descent or projected gradient descent does not give us any convergence guarantees here.",
            "But Even so, we can indeed show that the basic projected gradient descent algorithm where we take a gradient step and then project onto the rank space of rancor matrices does converge to the true low rank matrix under the sample complexity guarantees that I mentioned before.",
            "So the analysis goes through a non convex analysis of the projected gradient descent method."
        ],
        [
            "Let me briefly mention the techniques that we use to prove this result.",
            "The main technical result in our paper is that if you are given an incoherent rancor matrix M. Which is perturbed by an error matrix E. Then we can so M + E is not rancor because he's an error metrics, even them both Rancor and we take the projection of this perturbed matrix.",
            "So PR of empathy, we can bound the perturbation of this rank.",
            "Our matrix in the Infinity norm, whereas standard results using Davis Kahan theorem only give us perturbation results in the spectral norm of obedience.",
            "So this is the main technical part which we use to prove our result and other ingredient is to extend the discussion theorem for low rank approximation rather than singular vector perturbation.",
            "Please come here said the poster, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll briefly describe our result on fast matrix fast algorithm for matrix completion.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with Prateek, both of Microsoft Research.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the low rank matrix completion problem we just saw is that we are given partial entries of a matrix and we want to fill in the rest under the assumption that the matrix is low rank.",
                    "label": 0
                },
                {
                    "sent": "Even the low rank assumption this problem is in general not well posed, so we have an additional assumption which says that the mass of the entries of the matrix we spread over most of the entries of the matrix and is not concentrated in very few elements.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this has applications and recommendation systems and PC with missing address and so on.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Our results for this problem is that if you're given the rank are incoherent matrix, we can complete the matrix observing.",
                    "label": 0
                },
                {
                    "sent": "Sorry, so it's NR to the five la Cuban samples.",
                    "label": 0
                },
                {
                    "sent": "And with the computational complexity of NR to the 7 lock Cuban and log in, or epsilon where we want up to accuracy epsilon.",
                    "label": 0
                },
                {
                    "sent": "Just to compare this with prior work, there are two main approaches of algorithms for solving this problem.",
                    "label": 0
                },
                {
                    "sent": "The first one is using convex relaxation.",
                    "label": 1
                },
                {
                    "sent": "The convex relaxation approach.",
                    "label": 0
                },
                {
                    "sent": "As the optimal sample complexity depends dependency of NR, polylog, N, but the runtime of the algorithm is cubic in North, so it does not scale very well for large scale problems.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, there is alternating minimization where the time complexity is almost linear in North, but the sample complexity depends on the condition number of the input matrix as well as the required accuracy that we want to get to.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Our algorithm gets rid of this weakness of alternating minimization.",
                    "label": 0
                },
                {
                    "sent": "There is no dependence on the condition, number of matrix, or the desired accuracy.",
                    "label": 0
                },
                {
                    "sent": "But is suboptimal and its dependence on the rank of the matrix.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the problem can be formulated as a nonconvex problem where M is the given matrix and we want to index is the low rank matrix that we want to recover.",
                    "label": 0
                },
                {
                    "sent": "We minimize the error on the observed entries, so summation I Jane Omega where Omega is observed entries MIT minus six Idaho Square subject to the rank constraint on X.",
                    "label": 0
                },
                {
                    "sent": "The rank constraint on X causes this problem to be nonconvex, and so standard analysis of gradient descent or projected gradient descent does not give us any convergence guarantees here.",
                    "label": 0
                },
                {
                    "sent": "But Even so, we can indeed show that the basic projected gradient descent algorithm where we take a gradient step and then project onto the rank space of rancor matrices does converge to the true low rank matrix under the sample complexity guarantees that I mentioned before.",
                    "label": 0
                },
                {
                    "sent": "So the analysis goes through a non convex analysis of the projected gradient descent method.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me briefly mention the techniques that we use to prove this result.",
                    "label": 0
                },
                {
                    "sent": "The main technical result in our paper is that if you are given an incoherent rancor matrix M. Which is perturbed by an error matrix E. Then we can so M + E is not rancor because he's an error metrics, even them both Rancor and we take the projection of this perturbed matrix.",
                    "label": 0
                },
                {
                    "sent": "So PR of empathy, we can bound the perturbation of this rank.",
                    "label": 0
                },
                {
                    "sent": "Our matrix in the Infinity norm, whereas standard results using Davis Kahan theorem only give us perturbation results in the spectral norm of obedience.",
                    "label": 1
                },
                {
                    "sent": "So this is the main technical part which we use to prove our result and other ingredient is to extend the discussion theorem for low rank approximation rather than singular vector perturbation.",
                    "label": 0
                },
                {
                    "sent": "Please come here said the poster, thank you.",
                    "label": 1
                }
            ]
        }
    }
}