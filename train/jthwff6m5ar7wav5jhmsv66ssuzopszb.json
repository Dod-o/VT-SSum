{
    "id": "jthwff6m5ar7wav5jhmsv66ssuzopszb",
    "title": "Online Structure Learning for Markov Logic Networks",
    "info": {
        "produced by": [
            "Data & Web Mining Lab"
        ],
        "author": [
            "Raymond J. Mooney, Department of Computer Science, University of Texas at Austin"
        ],
        "published": "Nov. 29, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Markov Processes",
            "Top->Computer Science->Machine Learning->On-line Learning"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd2011_mooney_networks/",
    "segmentation": [
        [
            "Problem with."
        ],
        [
            "Based on this particular paper, is is natural language string segmentation.",
            "A couple of examples.",
            "Here are segmenting citations in a paper where you want to have a bibliography and you want to separate it into the author in the title and venue.",
            "Another example here is from Craigslist ad where you want to separate advertisements into a location of an apartment or the price or the features of the apartment."
        ],
        [
            "And so we're trying to use Markov logic networks for this.",
            "Problem is being used for a lot of different problems, including the natural language segmentation or information extraction.",
            "Before if you're not familiar with briefly describe it later, is introduced by Richard into Windows 2006, and it is a very powerful in general structure for doing structured prediction.",
            "But all the existing algorithms for learning the structure, which are the actual logical clauses in it rather than just the weights.",
            "Have been batch and really don't generalize well or scale well to problem with very large numbers of examples.",
            "They only test on these problems.",
            "We have a few very large rich structural examples, but we are with these problems like natural language string segmentation.",
            "You have a lot of very much smaller but structured examples that might be streaming in and there's really no online existing structured algorithm structure layout, so will present the first online as far as I know, structure learning are."
        ],
        [
            "Paramel end so quickly that was sort of on the motivation a little bit more background on Markov left will talk about our new algorithm called online structure learning and presents some experiment."
        ],
        [
            "And then a summary."
        ],
        [
            "OK, so if you're not familiar with Markov logic networks to get us introduced by Richard to make us in 2006, it's a combination of 1st order logic and undirected graphical models or Markov networks where you have a set of logical clauses which have weights.",
            "So here a couple of examples where we have for the string segmentation problem where the first one just says if some Coke and is in a particular field then the token next to it is usually in the same field.",
            "The one says well, if something is a token in its initial for this string set for this citation of field.",
            "That's usually in the author of the venues.",
            "They contain initials, but usually the other fields down.",
            "And then maybe it has a lesser weight.",
            "So larger weights indicate stronger belief that that rule holds in your domain and there's a well defined probability distribution across possible worlds.",
            "It's defined in a Markov logic where it's an exponentiated regular normalized exponential sum of the number of times that clause is satisfied in the data, so that strong related clauses make the situation more likely."
        ],
        [
            "Satisfied so they've been mostly working order Markov.",
            "Lots of learning those weights values on their rules, but there's been a little bit of work on learning the structure, which is much more closely related to inductive logic programming.",
            "You have to learn those logical clauses.",
            "There's generally been to sort of approaches.",
            "They've all been batch algorithms, either taking a sort of top down approach where you start with the empty clause and add literals to it are more bottom up logically, look at the data and immediately try to construct clauses specific particular instances.",
            "There's a number of references that a previous structure learning.",
            "We tried to find a variety of these algorithms to this string segmentation problem and and it really couldn't.",
            "None of them really scale well or worked well with the thousands of examples for those sorts of problems so."
        ],
        [
            "We were interested in developing an online structured learning, our rhythm for Markov logic.",
            "We learn those logical clauses online as new examples."
        ],
        [
            "Come in so the overall framework looks something like this.",
            "There's two components, the structure learning part in the weight learning part, both of which are online.",
            "The thing we're interested in here is the structure learning part, so you get some structured input excepte at some particular point in time.",
            "As these are streaming in and you know that you give that to your existing Markov logic that you're building up incrementally, and it outputs some structured prediction at time.",
            "Ty Sub T super P. The teacher comes in then says and gives you the correct answer.",
            "And then you compare that to the predicted answer.",
            "And 1st, you maybe add some logical clauses based on some inconsistency between the prediction and the correct value.",
            "Then once you have those 'cause you take the new clauses and go clauses in you update the weights online as well.",
            "So we have a standard.",
            "I think the first talked about L1 regularize online weight learning were not in the doing any innovation there.",
            "It's a.",
            "It's a technique I'll mention a minute that's already in the literature to update the weight.",
            "So you're updating both the structure and the weights incrementally each time, and then the new weights go in and you keep doing this as new example structured examples, infrastructure predictions."
        ],
        [
            "Comment.",
            "So the main thing is in this, we're going to talk about, of course, is the structure prediction.",
            "So each time you're getting some gold standard input of a structured input text in a structured output YT, and then it makes a prediction that might be wrong at any given point.",
            "So if it's making a prediction, is predicting some fact that's not true, will assume the weight learning can sort of manage that, 'cause it will down, wait the clauses that cause that prediction, or really interested in is correct and recall errors where it's not predicting something it should be.",
            "We need a new clause to make that prediction.",
            "So we look at these set of atoms Delta Y that is sort of the items that are true in the ground truth, but not being predicted at all.",
            "Those are things we really need.",
            "Clauses new clauses to add those predictions to the model.",
            "So we find new clauses for each Atom that's not being predicted correctly.",
            "And to do this we combine a couple ideas from fairly old ideas, actually from inductive logic programming.",
            "The idea of mode declarations and Muggleton introduced in 95 and some really all work that I worked with Brad Richardson back in 92 called Relational pathfinding.",
            "And then I will talk about more about that in a minute, and then we select new clauses that have more true groundings in the correct world than they have in the currently current prediction.",
            "And that's how many clauses we add depends on this parameter bin count dip, which is how many more grounding you need to have in the true world than the predicted world in order to make that a sufficiently good clause to actually put it in and learn ways."
        ],
        [
            "So just really briefly how we combine these two ideas of relational pathfinding anmod declarations to learn new clauses.",
            "So pathfinding again that way back in 90.",
            "Two you consider a relational example as a hyper graph, where the constants in your logical expressions are nodes, and then the each Atom defines a hyper graph that connects all the nodes that are its arguments, and you search that hyper graph for path that connects the arguments of the target literally original model.",
            "So here is a class sample I always used to illustrate relational pathfinding where you have a.",
            "Family domain where you have, you're given the parent and Mary relations and say you want to learn a logical rule for what makes someone an uncle.",
            "Here we have in this case Tom is Mary's uncle.",
            "We find a path between Tom and Mary in the relational graph and we create a clause directly from that and then we can verbalize it by just turn each of the constants into the variable and we get a decent definition for Uncle from a single example.",
            "It's missing the Mail constraint which we can add later on to make this cause a little bit better, but you're basically finding paths in relational data and then creating logical clauses."
        ],
        [
            "On those.",
            "We generalize the subject for our case.",
            "You don't necessarily need a path that connects the actual arguments of the target literally were trying to predict, like in this case.",
            "Uncle already requires that there is a connected path of relations that warm applause.",
            "So each constitute consecutive atoms in a path must share at least one input or output arguments.",
            "And this idea of generalizing relation path finding this way has been used in previous work.",
            "An instructor learning for Markov logic, particularly Stanley Cox.",
            "Recent work uses this sort of idea of pathfinding to learn structural clauses in a batch fashion for MLN.",
            "But in general this could result in a fairly large number of possible paths, and so in order to make this work in an online setting."
        ],
        [
            "What we really needed was a way of constraining and reducing the number of paths that it considers.",
            "And here's where we exploit this notion that Muggleton introduced in IOP quite a few years ago, and having mode declarations, which is sort of a language bias that constrains the search for the space of clauses that you're actually going to learn.",
            "And then we're also in this case we're going to have both decoration to specify the number of appearances over predicate in a clause, and the constraints on the types of the arguments of the."
        ],
        [
            "Predicates so particularly happy is what we call mode P constraints with their path mode constraints, where you have R, which is a term from models in the recall number, which is a non negative integer limiting the number of times of particular predicate can appear in a path and about 0.",
            "Then you can't contain that predicate at all, and then you have these so called mode constraints.",
            "In a standard logic programming framework where each argument can either be an input argument where it must be bound before you can use it, or it can be output, or it can be a free argument, or in our case we also say will never expand the path along that can't.",
            "Sense at all, which is indicated as a dot."
        ],
        [
            "These modes constraints so to give you something a little more concrete idea in this sort of.",
            "Let's take this citation segmentation example, you may be at these predicates like infield predicates.",
            "It says there's a particular position in like in some particular citation that's in a particular field.",
            "Then you have the next predicate which has some position is the, next is a contiguous position in the string, and then you know what the token is at each position in each citation.",
            "And there are some very useful.",
            "Field constraints it give you something a little bit like a conditional random field model if you're familiar with that, but it defines a very large space of features that would be intractable to just throw into a CRF model to start with is to say, well, I can have two infield predicates and I trigger off of the follow pass off the position, and then I could the next.",
            "I could either look at the previous or next token.",
            "If I treat those both as output, it can either look at the previous token or the next token, and then I can look at the actual token value at each point, which I have an input argument for the position, so I need to know what position I'm looking at, and then I can look at the tokens in that position.",
            "So these are more concise, constructive, sort of space of possible features I can construct for these sorts of segments."
        ],
        [
            "Models.",
            "Players are really quick example.",
            "Say I make a wrong prediction.",
            "I don't properly predict the position mine in some citation V2 is in the title field.",
            "Well, I start passed off of that center because that's what the mode Declaration tells me to do, and I follow up.",
            "Well.",
            "What other facts do I know about that talk about position?",
            "I and I can add a lot of other information to create a possible clause there, but I only look at past that obey my mode restrictions.",
            "So here I might look at that."
        ],
        [
            "Looking at that position, or I can look at the next token and look at its position in the mode declaration.",
            "So to tell me how I can expand that path and create a relatively more limited set of clauses based on that sort of declarative bias that the modem."
        ],
        [
            "Operations are giving me once I have a path then I verbalize it and turn it into a clause and we have set of modes and tell you what what things normally should be kept constant.",
            "Sandwich things could be turned into variables.",
            "For this case.",
            "You know we usually keep the field as a constant 'cause we want to be able to predict that, but the other ones we turn into variables.",
            "And then we get this sort of.",
            "You might do it as a feature in the final undirected model.",
            "From that we construct a couple of different clauses that go into the MLN one which is just the negation of this, which is the easiest way to put a closet, and it could have a negative weight on it.",
            "If you need to.",
            "And we also put in a horn clause version of that that directly predicts the target variable we're interested in this case, what field is that token?",
            "In both of those tend to be is full of these such looks, straight segmentation problems.",
            "So each example comes in.",
            "We use this.",
            "Relational mode guide."
        ],
        [
            "Thing to create new clauses.",
            "Then we just do normal incremental online weight learning to update the weights on both all the old clauses in a new clause we added.",
            "For that example we use L1 regularization to make a sparse solution just like the first with this thing can introduce a lot of clauses and a lot of we eventually want to zero out 'cause they won't workout in the future.",
            "So it helps to have a sparsity constraint like an L1 regularization give you.",
            "We just use a current pretty up-to-date online off the shelf, L1 regularised.",
            "Online algorithm called data graph."
        ],
        [
            "To do that?",
            "Now quickly into some experimental evaluation we looked at doing two different things with this.",
            "In the sentence segmentation problems, one is we start with a decent MLN to start with and see if we can actually improve the structure of it.",
            "The other ones kind of just learn the structure of an MLN completely from scratch.",
            "We looked at a couple of datasets in these natural language field segmentation problem.",
            "The sites here problem out of introducing the Craigslist and they both have sort of thousands of exam."
        ],
        [
            "Pick up for some baseline comparisons.",
            "One thing we compared to is a very simple sort of what you can put into animal, and so it's effectively a very simple linear chain conditional random field where you just look at the token at each point influences the field.",
            "If you're not familiar with this, plus notation in markup, logically we put every possible constant of a word and every constant possible field so it has a separate cause for every word field combination and learns a weight on that.",
            "So it's learning for each word how much it indicates each field, and then we have.",
            "The standard sort of assumption between the contiguous words in the sequence that if one word is enough for each possible field, and you look at the next token and you see what its field is, and you see how likely it is to transition from any given field into any other given field as we go from one token to the next.",
            "So we can start with that and then see if we can actually improve on it by at."
        ],
        [
            "And more interesting features to it.",
            "But we also started with a pretty good handle model for this domain- Kuehn, published paper back in 2007.",
            "He presented a markup logic for this sentence segmentation problem.",
            "He put in more than just a generic CRF.",
            "He has some other interesting clauses here.",
            "Bout well, if there's if there's punctuation between 2 fields and sometimes it's more likely they change, except if it's like a comma cousin, It's a particularly it's in an author field.",
            "There lots tends to be lots of commas in author fields, but not, say in title fields.",
            "And so it has extra clauses in there and it learns the weights on each of those constraints.",
            "So he starts out with a pretty good model on for this and he gets pretty good results.",
            "And we want to see commercial online structure learner actually improve that structure."
        ],
        [
            "Further, in addition to starting from scratch, so the first thing we compared to is just do the weight learning on an existing structure.",
            "If we have one, we don't change the structure.",
            "How well does that?",
            "Do we have two different versions of the online structure?",
            "Learning is sort of either slower or faster.",
            "I'll show you timing results in a minute, so if we set this min count diff parameter high, we get a fast version is it's very conservative by adding clauses in at each step.",
            "If we set it lower to just one, it doesn't need much evidence to throw a clause in, but then it causes the weight, learn a lot or extra work.",
            "Just eventually, maybe zero that out with the.",
            "Now one if it needs to, so that's a little bit slower, but."
        ],
        [
            "Do a little bit better.",
            "And we specify some mode declarations for the string segmentation problems.",
            "It basically give you a very rich sort of linear chain model where you allow this transition said and the output to depend on both the current previous and following stage, so it really wouldn't be tractable to just throw in all these features into our normal CRF because it's basically sort of online learning and doing feature selection is effectively what you're doing, so overall technique is general prime lens, but in the context of this you can abstract that takes.",
            "You can view, it is sort of very rich CRF model that does its own.",
            "Dynamic feature evaluation and construction.",
            "Therefore, subsets of data in this site.",
            "So your data we cross validate, cross those the standard way to evaluate these types of problems is is F1, which is the harmonic mean of recall and precision at the token level.",
            "How many tokens do you get assigned to their correct fields?"
        ],
        [
            "So there are some results on.",
            "I'll just present the results on site.",
            "So your data for time the rest of the results are in the paper for the Craigslist data.",
            "So we start with a very simple MLN model and we don't do any structural addition to that at all.",
            "If we get not too bad results in the mid 80s or low low 80s I should say if we now do some structure learning on top of that we can actually increase a simple linear chain model to get up into the lower 90s.",
            "And by having a little more slower.",
            "Version of the online structure learning we can do even a little bit better.",
            "We can actually take this pretty good model that put together an actually also show.",
            "It starts at a higher level of course, and we can show that we can improve it and add new closet if that make it structure even better than it was before.",
            "And we can start with a totally empty structure and actually build.",
            "We don't have course.",
            "It's easier to start with a human built model and improve it, and we get OK results if we just have.",
            "This thing has to learn the entire structure of the problem from scratch.",
            "So empty is, you know, we did get up to pretty good structure.",
            "If we do the slow version of the online structure learning we can get.",
            "Almost close to you know, certainly better than just what pipeline was able to put together manually.",
            "And so we can improve existing structures.",
            "And we can also learn structures from scratch reasons."
        ],
        [
            "Wait, well, there's just some runtime results, so we just do online weight learning.",
            "That's very fast.",
            "If we do our fast version of our structure.",
            "Learning it just takes a small little bit more time.",
            "The slow version.",
            "We're evaluating more clauses, which does give you better accuracy.",
            "It does dramatically increase the overall runtime, so you can sort of trade off how much faster than update.",
            "Indeed after each example, but even with a relatively fast update within."
        ],
        [
            "You'll get pretty good results.",
            "Just a couple of examples of some clauses that the system discovered for this Citeseer domain.",
            "So one thing it added to the ASM model, which was the spoon model that he built for this domain is to say, well, if one filled if the current token is a title and it's followed by a period, then it's likely that the next cocaine is a venue.",
            "And when you start with empty it sort of learns the obvious sort of sequential structure in this domain, not knowing that there's sequential structure here, it actually learns that usually when it opens in a field then the next token is in the same field.",
            "But it can learn that prescribed without having been given that improved structural information operatory."
        ],
        [
            "OK, so I'll leave plenty of time for questions hopefully and summarize, so we prevented.",
            "As far as I know, the first online structure, learner 4 MLN.",
            "You can either take an existing MLN set of clauses and add new clauses to it in an online fashion.",
            "To make it even better.",
            "And we showed that we can improve some reasonably starting state mesfer these sentence segmentation problems.",
            "Are we can just learn a whole structure a completely from scratch and it it scales pretty well to handle these problems where we have sort of small structured problems coming in, you know, and maybe have thousands of them arriving incrementally and that we can outperform some existing algorithms on a couple of these example natural language segmentation problems.",
            "Incense segmenting bibliographic citations and these advertisements on Craigslist."
        ],
        [
            "If I understand you correctly, point out that you outperform some existing techniques.",
            "Those previous techniques were not even online.",
            "If I'm correct, is that right?",
            "So given that you thought about how to make this skill, that you know thousands of little examples or not, so I'm sorry.",
            "So when I when I say, outperformed these."
        ],
        [
            "Results in red are an online weight learner performed on an existing structure, so there's no structure change at all.",
            "So now it's there.",
            "All of these results are online results.",
            "OK, you want your documents?",
            "Are you working against previously published result?",
            "Just that I think might be on the paper, but you don't need that, but it's at least competitive with the batch results.",
            "You're not really taking too much of a hit just because you're doing it online, but I guess that's sort of a second question.",
            "Do you have the motivation for this being online?",
            "Where do you see it in motivation for these datasets?",
            "And that this needs to be online learning process or these days that were more conveniently, and I guess, more the latter.",
            "So I think this is I would do see this sort of initial working online structural, and we'd like to have some problems where the where it's more naturally required to have online learning.",
            "So these were sort of convenient problems that we could use to illustrate it initially, but I don't know.",
            "So you think they're the best problems for this task?",
            "Very interesting thought.",
            "As you can start with your algorithm from from an existing model, would it be possible to start off with a fast algorithm, then run for short time the slow algorithm which gives better results or those who are now we haven't lifted mixing well, you could change that parameter overtime.",
            "You know if you want it you want to run slow in the beginning and then speed it up later or the other way so we haven't looked at automating the studying about parameters so that you could change how maybe how well it how much time it spends its doing better.",
            "'cause that's a good.",
            "Color.",
            "How about if you start with an empty model compared to where you start with a minor League or start how many clauses?",
            "How many does it learn?",
            "See, this is where you wish the student was actually here 'cause he Grammas parents and he would be able to give you at least a back of the envelope.",
            "I mean, because these have constants in them about words and field.",
            "We're talking thousands if not 10s of thousands of clauses in the end of the day, 'cause it has a separate 'cause, you know, just the generic model has a separate cost for every combination of a token in a field.",
            "And there's like 5 fields in there.",
            "In these domains, there can be thousands or more of token, so it learns pretty big at my lens with thousands if not 10s of thousands of clauses.",
            "Quick question.",
            "Have you seen if the order of the examples on the arrived?",
            "Yeah, now it's a good point.",
            "We haven't looked at how since it is online.",
            "It actually probably would be affected by the order of the presentation of the examples that we haven't looked effective algorithms.",
            "Anymore questions.",
            "Well, that's the end of this session.",
            "That's not great game."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem with.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Based on this particular paper, is is natural language string segmentation.",
                    "label": 0
                },
                {
                    "sent": "A couple of examples.",
                    "label": 0
                },
                {
                    "sent": "Here are segmenting citations in a paper where you want to have a bibliography and you want to separate it into the author in the title and venue.",
                    "label": 0
                },
                {
                    "sent": "Another example here is from Craigslist ad where you want to separate advertisements into a location of an apartment or the price or the features of the apartment.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so we're trying to use Markov logic networks for this.",
                    "label": 1
                },
                {
                    "sent": "Problem is being used for a lot of different problems, including the natural language segmentation or information extraction.",
                    "label": 0
                },
                {
                    "sent": "Before if you're not familiar with briefly describe it later, is introduced by Richard into Windows 2006, and it is a very powerful in general structure for doing structured prediction.",
                    "label": 0
                },
                {
                    "sent": "But all the existing algorithms for learning the structure, which are the actual logical clauses in it rather than just the weights.",
                    "label": 0
                },
                {
                    "sent": "Have been batch and really don't generalize well or scale well to problem with very large numbers of examples.",
                    "label": 0
                },
                {
                    "sent": "They only test on these problems.",
                    "label": 0
                },
                {
                    "sent": "We have a few very large rich structural examples, but we are with these problems like natural language string segmentation.",
                    "label": 0
                },
                {
                    "sent": "You have a lot of very much smaller but structured examples that might be streaming in and there's really no online existing structured algorithm structure layout, so will present the first online as far as I know, structure learning are.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Paramel end so quickly that was sort of on the motivation a little bit more background on Markov left will talk about our new algorithm called online structure learning and presents some experiment.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then a summary.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so if you're not familiar with Markov logic networks to get us introduced by Richard to make us in 2006, it's a combination of 1st order logic and undirected graphical models or Markov networks where you have a set of logical clauses which have weights.",
                    "label": 1
                },
                {
                    "sent": "So here a couple of examples where we have for the string segmentation problem where the first one just says if some Coke and is in a particular field then the token next to it is usually in the same field.",
                    "label": 1
                },
                {
                    "sent": "The one says well, if something is a token in its initial for this string set for this citation of field.",
                    "label": 0
                },
                {
                    "sent": "That's usually in the author of the venues.",
                    "label": 0
                },
                {
                    "sent": "They contain initials, but usually the other fields down.",
                    "label": 0
                },
                {
                    "sent": "And then maybe it has a lesser weight.",
                    "label": 1
                },
                {
                    "sent": "So larger weights indicate stronger belief that that rule holds in your domain and there's a well defined probability distribution across possible worlds.",
                    "label": 0
                },
                {
                    "sent": "It's defined in a Markov logic where it's an exponentiated regular normalized exponential sum of the number of times that clause is satisfied in the data, so that strong related clauses make the situation more likely.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Satisfied so they've been mostly working order Markov.",
                    "label": 0
                },
                {
                    "sent": "Lots of learning those weights values on their rules, but there's been a little bit of work on learning the structure, which is much more closely related to inductive logic programming.",
                    "label": 0
                },
                {
                    "sent": "You have to learn those logical clauses.",
                    "label": 0
                },
                {
                    "sent": "There's generally been to sort of approaches.",
                    "label": 0
                },
                {
                    "sent": "They've all been batch algorithms, either taking a sort of top down approach where you start with the empty clause and add literals to it are more bottom up logically, look at the data and immediately try to construct clauses specific particular instances.",
                    "label": 0
                },
                {
                    "sent": "There's a number of references that a previous structure learning.",
                    "label": 0
                },
                {
                    "sent": "We tried to find a variety of these algorithms to this string segmentation problem and and it really couldn't.",
                    "label": 0
                },
                {
                    "sent": "None of them really scale well or worked well with the thousands of examples for those sorts of problems so.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We were interested in developing an online structured learning, our rhythm for Markov logic.",
                    "label": 0
                },
                {
                    "sent": "We learn those logical clauses online as new examples.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Come in so the overall framework looks something like this.",
                    "label": 0
                },
                {
                    "sent": "There's two components, the structure learning part in the weight learning part, both of which are online.",
                    "label": 1
                },
                {
                    "sent": "The thing we're interested in here is the structure learning part, so you get some structured input excepte at some particular point in time.",
                    "label": 0
                },
                {
                    "sent": "As these are streaming in and you know that you give that to your existing Markov logic that you're building up incrementally, and it outputs some structured prediction at time.",
                    "label": 0
                },
                {
                    "sent": "Ty Sub T super P. The teacher comes in then says and gives you the correct answer.",
                    "label": 0
                },
                {
                    "sent": "And then you compare that to the predicted answer.",
                    "label": 0
                },
                {
                    "sent": "And 1st, you maybe add some logical clauses based on some inconsistency between the prediction and the correct value.",
                    "label": 0
                },
                {
                    "sent": "Then once you have those 'cause you take the new clauses and go clauses in you update the weights online as well.",
                    "label": 0
                },
                {
                    "sent": "So we have a standard.",
                    "label": 0
                },
                {
                    "sent": "I think the first talked about L1 regularize online weight learning were not in the doing any innovation there.",
                    "label": 0
                },
                {
                    "sent": "It's a.",
                    "label": 0
                },
                {
                    "sent": "It's a technique I'll mention a minute that's already in the literature to update the weight.",
                    "label": 0
                },
                {
                    "sent": "So you're updating both the structure and the weights incrementally each time, and then the new weights go in and you keep doing this as new example structured examples, infrastructure predictions.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Comment.",
                    "label": 0
                },
                {
                    "sent": "So the main thing is in this, we're going to talk about, of course, is the structure prediction.",
                    "label": 0
                },
                {
                    "sent": "So each time you're getting some gold standard input of a structured input text in a structured output YT, and then it makes a prediction that might be wrong at any given point.",
                    "label": 0
                },
                {
                    "sent": "So if it's making a prediction, is predicting some fact that's not true, will assume the weight learning can sort of manage that, 'cause it will down, wait the clauses that cause that prediction, or really interested in is correct and recall errors where it's not predicting something it should be.",
                    "label": 0
                },
                {
                    "sent": "We need a new clause to make that prediction.",
                    "label": 0
                },
                {
                    "sent": "So we look at these set of atoms Delta Y that is sort of the items that are true in the ground truth, but not being predicted at all.",
                    "label": 0
                },
                {
                    "sent": "Those are things we really need.",
                    "label": 0
                },
                {
                    "sent": "Clauses new clauses to add those predictions to the model.",
                    "label": 0
                },
                {
                    "sent": "So we find new clauses for each Atom that's not being predicted correctly.",
                    "label": 0
                },
                {
                    "sent": "And to do this we combine a couple ideas from fairly old ideas, actually from inductive logic programming.",
                    "label": 0
                },
                {
                    "sent": "The idea of mode declarations and Muggleton introduced in 95 and some really all work that I worked with Brad Richardson back in 92 called Relational pathfinding.",
                    "label": 0
                },
                {
                    "sent": "And then I will talk about more about that in a minute, and then we select new clauses that have more true groundings in the correct world than they have in the currently current prediction.",
                    "label": 0
                },
                {
                    "sent": "And that's how many clauses we add depends on this parameter bin count dip, which is how many more grounding you need to have in the true world than the predicted world in order to make that a sufficiently good clause to actually put it in and learn ways.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just really briefly how we combine these two ideas of relational pathfinding anmod declarations to learn new clauses.",
                    "label": 0
                },
                {
                    "sent": "So pathfinding again that way back in 90.",
                    "label": 0
                },
                {
                    "sent": "Two you consider a relational example as a hyper graph, where the constants in your logical expressions are nodes, and then the each Atom defines a hyper graph that connects all the nodes that are its arguments, and you search that hyper graph for path that connects the arguments of the target literally original model.",
                    "label": 1
                },
                {
                    "sent": "So here is a class sample I always used to illustrate relational pathfinding where you have a.",
                    "label": 0
                },
                {
                    "sent": "Family domain where you have, you're given the parent and Mary relations and say you want to learn a logical rule for what makes someone an uncle.",
                    "label": 0
                },
                {
                    "sent": "Here we have in this case Tom is Mary's uncle.",
                    "label": 0
                },
                {
                    "sent": "We find a path between Tom and Mary in the relational graph and we create a clause directly from that and then we can verbalize it by just turn each of the constants into the variable and we get a decent definition for Uncle from a single example.",
                    "label": 0
                },
                {
                    "sent": "It's missing the Mail constraint which we can add later on to make this cause a little bit better, but you're basically finding paths in relational data and then creating logical clauses.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On those.",
                    "label": 0
                },
                {
                    "sent": "We generalize the subject for our case.",
                    "label": 0
                },
                {
                    "sent": "You don't necessarily need a path that connects the actual arguments of the target literally were trying to predict, like in this case.",
                    "label": 1
                },
                {
                    "sent": "Uncle already requires that there is a connected path of relations that warm applause.",
                    "label": 0
                },
                {
                    "sent": "So each constitute consecutive atoms in a path must share at least one input or output arguments.",
                    "label": 1
                },
                {
                    "sent": "And this idea of generalizing relation path finding this way has been used in previous work.",
                    "label": 0
                },
                {
                    "sent": "An instructor learning for Markov logic, particularly Stanley Cox.",
                    "label": 0
                },
                {
                    "sent": "Recent work uses this sort of idea of pathfinding to learn structural clauses in a batch fashion for MLN.",
                    "label": 1
                },
                {
                    "sent": "But in general this could result in a fairly large number of possible paths, and so in order to make this work in an online setting.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What we really needed was a way of constraining and reducing the number of paths that it considers.",
                    "label": 0
                },
                {
                    "sent": "And here's where we exploit this notion that Muggleton introduced in IOP quite a few years ago, and having mode declarations, which is sort of a language bias that constrains the search for the space of clauses that you're actually going to learn.",
                    "label": 0
                },
                {
                    "sent": "And then we're also in this case we're going to have both decoration to specify the number of appearances over predicate in a clause, and the constraints on the types of the arguments of the.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Predicates so particularly happy is what we call mode P constraints with their path mode constraints, where you have R, which is a term from models in the recall number, which is a non negative integer limiting the number of times of particular predicate can appear in a path and about 0.",
                    "label": 1
                },
                {
                    "sent": "Then you can't contain that predicate at all, and then you have these so called mode constraints.",
                    "label": 1
                },
                {
                    "sent": "In a standard logic programming framework where each argument can either be an input argument where it must be bound before you can use it, or it can be output, or it can be a free argument, or in our case we also say will never expand the path along that can't.",
                    "label": 0
                },
                {
                    "sent": "Sense at all, which is indicated as a dot.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These modes constraints so to give you something a little more concrete idea in this sort of.",
                    "label": 0
                },
                {
                    "sent": "Let's take this citation segmentation example, you may be at these predicates like infield predicates.",
                    "label": 0
                },
                {
                    "sent": "It says there's a particular position in like in some particular citation that's in a particular field.",
                    "label": 0
                },
                {
                    "sent": "Then you have the next predicate which has some position is the, next is a contiguous position in the string, and then you know what the token is at each position in each citation.",
                    "label": 0
                },
                {
                    "sent": "And there are some very useful.",
                    "label": 0
                },
                {
                    "sent": "Field constraints it give you something a little bit like a conditional random field model if you're familiar with that, but it defines a very large space of features that would be intractable to just throw into a CRF model to start with is to say, well, I can have two infield predicates and I trigger off of the follow pass off the position, and then I could the next.",
                    "label": 0
                },
                {
                    "sent": "I could either look at the previous or next token.",
                    "label": 0
                },
                {
                    "sent": "If I treat those both as output, it can either look at the previous token or the next token, and then I can look at the actual token value at each point, which I have an input argument for the position, so I need to know what position I'm looking at, and then I can look at the tokens in that position.",
                    "label": 0
                },
                {
                    "sent": "So these are more concise, constructive, sort of space of possible features I can construct for these sorts of segments.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Models.",
                    "label": 0
                },
                {
                    "sent": "Players are really quick example.",
                    "label": 0
                },
                {
                    "sent": "Say I make a wrong prediction.",
                    "label": 1
                },
                {
                    "sent": "I don't properly predict the position mine in some citation V2 is in the title field.",
                    "label": 0
                },
                {
                    "sent": "Well, I start passed off of that center because that's what the mode Declaration tells me to do, and I follow up.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "What other facts do I know about that talk about position?",
                    "label": 0
                },
                {
                    "sent": "I and I can add a lot of other information to create a possible clause there, but I only look at past that obey my mode restrictions.",
                    "label": 0
                },
                {
                    "sent": "So here I might look at that.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Looking at that position, or I can look at the next token and look at its position in the mode declaration.",
                    "label": 0
                },
                {
                    "sent": "So to tell me how I can expand that path and create a relatively more limited set of clauses based on that sort of declarative bias that the modem.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Operations are giving me once I have a path then I verbalize it and turn it into a clause and we have set of modes and tell you what what things normally should be kept constant.",
                    "label": 0
                },
                {
                    "sent": "Sandwich things could be turned into variables.",
                    "label": 0
                },
                {
                    "sent": "For this case.",
                    "label": 0
                },
                {
                    "sent": "You know we usually keep the field as a constant 'cause we want to be able to predict that, but the other ones we turn into variables.",
                    "label": 0
                },
                {
                    "sent": "And then we get this sort of.",
                    "label": 0
                },
                {
                    "sent": "You might do it as a feature in the final undirected model.",
                    "label": 0
                },
                {
                    "sent": "From that we construct a couple of different clauses that go into the MLN one which is just the negation of this, which is the easiest way to put a closet, and it could have a negative weight on it.",
                    "label": 0
                },
                {
                    "sent": "If you need to.",
                    "label": 0
                },
                {
                    "sent": "And we also put in a horn clause version of that that directly predicts the target variable we're interested in this case, what field is that token?",
                    "label": 0
                },
                {
                    "sent": "In both of those tend to be is full of these such looks, straight segmentation problems.",
                    "label": 0
                },
                {
                    "sent": "So each example comes in.",
                    "label": 0
                },
                {
                    "sent": "We use this.",
                    "label": 0
                },
                {
                    "sent": "Relational mode guide.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thing to create new clauses.",
                    "label": 1
                },
                {
                    "sent": "Then we just do normal incremental online weight learning to update the weights on both all the old clauses in a new clause we added.",
                    "label": 0
                },
                {
                    "sent": "For that example we use L1 regularization to make a sparse solution just like the first with this thing can introduce a lot of clauses and a lot of we eventually want to zero out 'cause they won't workout in the future.",
                    "label": 1
                },
                {
                    "sent": "So it helps to have a sparsity constraint like an L1 regularization give you.",
                    "label": 0
                },
                {
                    "sent": "We just use a current pretty up-to-date online off the shelf, L1 regularised.",
                    "label": 0
                },
                {
                    "sent": "Online algorithm called data graph.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To do that?",
                    "label": 0
                },
                {
                    "sent": "Now quickly into some experimental evaluation we looked at doing two different things with this.",
                    "label": 0
                },
                {
                    "sent": "In the sentence segmentation problems, one is we start with a decent MLN to start with and see if we can actually improve the structure of it.",
                    "label": 0
                },
                {
                    "sent": "The other ones kind of just learn the structure of an MLN completely from scratch.",
                    "label": 0
                },
                {
                    "sent": "We looked at a couple of datasets in these natural language field segmentation problem.",
                    "label": 1
                },
                {
                    "sent": "The sites here problem out of introducing the Craigslist and they both have sort of thousands of exam.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Pick up for some baseline comparisons.",
                    "label": 0
                },
                {
                    "sent": "One thing we compared to is a very simple sort of what you can put into animal, and so it's effectively a very simple linear chain conditional random field where you just look at the token at each point influences the field.",
                    "label": 1
                },
                {
                    "sent": "If you're not familiar with this, plus notation in markup, logically we put every possible constant of a word and every constant possible field so it has a separate cause for every word field combination and learns a weight on that.",
                    "label": 0
                },
                {
                    "sent": "So it's learning for each word how much it indicates each field, and then we have.",
                    "label": 0
                },
                {
                    "sent": "The standard sort of assumption between the contiguous words in the sequence that if one word is enough for each possible field, and you look at the next token and you see what its field is, and you see how likely it is to transition from any given field into any other given field as we go from one token to the next.",
                    "label": 0
                },
                {
                    "sent": "So we can start with that and then see if we can actually improve on it by at.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And more interesting features to it.",
                    "label": 0
                },
                {
                    "sent": "But we also started with a pretty good handle model for this domain- Kuehn, published paper back in 2007.",
                    "label": 0
                },
                {
                    "sent": "He presented a markup logic for this sentence segmentation problem.",
                    "label": 0
                },
                {
                    "sent": "He put in more than just a generic CRF.",
                    "label": 0
                },
                {
                    "sent": "He has some other interesting clauses here.",
                    "label": 0
                },
                {
                    "sent": "Bout well, if there's if there's punctuation between 2 fields and sometimes it's more likely they change, except if it's like a comma cousin, It's a particularly it's in an author field.",
                    "label": 0
                },
                {
                    "sent": "There lots tends to be lots of commas in author fields, but not, say in title fields.",
                    "label": 0
                },
                {
                    "sent": "And so it has extra clauses in there and it learns the weights on each of those constraints.",
                    "label": 0
                },
                {
                    "sent": "So he starts out with a pretty good model on for this and he gets pretty good results.",
                    "label": 0
                },
                {
                    "sent": "And we want to see commercial online structure learner actually improve that structure.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Further, in addition to starting from scratch, so the first thing we compared to is just do the weight learning on an existing structure.",
                    "label": 1
                },
                {
                    "sent": "If we have one, we don't change the structure.",
                    "label": 0
                },
                {
                    "sent": "How well does that?",
                    "label": 0
                },
                {
                    "sent": "Do we have two different versions of the online structure?",
                    "label": 0
                },
                {
                    "sent": "Learning is sort of either slower or faster.",
                    "label": 0
                },
                {
                    "sent": "I'll show you timing results in a minute, so if we set this min count diff parameter high, we get a fast version is it's very conservative by adding clauses in at each step.",
                    "label": 1
                },
                {
                    "sent": "If we set it lower to just one, it doesn't need much evidence to throw a clause in, but then it causes the weight, learn a lot or extra work.",
                    "label": 0
                },
                {
                    "sent": "Just eventually, maybe zero that out with the.",
                    "label": 0
                },
                {
                    "sent": "Now one if it needs to, so that's a little bit slower, but.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do a little bit better.",
                    "label": 0
                },
                {
                    "sent": "And we specify some mode declarations for the string segmentation problems.",
                    "label": 1
                },
                {
                    "sent": "It basically give you a very rich sort of linear chain model where you allow this transition said and the output to depend on both the current previous and following stage, so it really wouldn't be tractable to just throw in all these features into our normal CRF because it's basically sort of online learning and doing feature selection is effectively what you're doing, so overall technique is general prime lens, but in the context of this you can abstract that takes.",
                    "label": 1
                },
                {
                    "sent": "You can view, it is sort of very rich CRF model that does its own.",
                    "label": 0
                },
                {
                    "sent": "Dynamic feature evaluation and construction.",
                    "label": 0
                },
                {
                    "sent": "Therefore, subsets of data in this site.",
                    "label": 0
                },
                {
                    "sent": "So your data we cross validate, cross those the standard way to evaluate these types of problems is is F1, which is the harmonic mean of recall and precision at the token level.",
                    "label": 0
                },
                {
                    "sent": "How many tokens do you get assigned to their correct fields?",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there are some results on.",
                    "label": 0
                },
                {
                    "sent": "I'll just present the results on site.",
                    "label": 0
                },
                {
                    "sent": "So your data for time the rest of the results are in the paper for the Craigslist data.",
                    "label": 0
                },
                {
                    "sent": "So we start with a very simple MLN model and we don't do any structural addition to that at all.",
                    "label": 0
                },
                {
                    "sent": "If we get not too bad results in the mid 80s or low low 80s I should say if we now do some structure learning on top of that we can actually increase a simple linear chain model to get up into the lower 90s.",
                    "label": 0
                },
                {
                    "sent": "And by having a little more slower.",
                    "label": 0
                },
                {
                    "sent": "Version of the online structure learning we can do even a little bit better.",
                    "label": 0
                },
                {
                    "sent": "We can actually take this pretty good model that put together an actually also show.",
                    "label": 0
                },
                {
                    "sent": "It starts at a higher level of course, and we can show that we can improve it and add new closet if that make it structure even better than it was before.",
                    "label": 0
                },
                {
                    "sent": "And we can start with a totally empty structure and actually build.",
                    "label": 0
                },
                {
                    "sent": "We don't have course.",
                    "label": 0
                },
                {
                    "sent": "It's easier to start with a human built model and improve it, and we get OK results if we just have.",
                    "label": 0
                },
                {
                    "sent": "This thing has to learn the entire structure of the problem from scratch.",
                    "label": 0
                },
                {
                    "sent": "So empty is, you know, we did get up to pretty good structure.",
                    "label": 0
                },
                {
                    "sent": "If we do the slow version of the online structure learning we can get.",
                    "label": 0
                },
                {
                    "sent": "Almost close to you know, certainly better than just what pipeline was able to put together manually.",
                    "label": 0
                },
                {
                    "sent": "And so we can improve existing structures.",
                    "label": 0
                },
                {
                    "sent": "And we can also learn structures from scratch reasons.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Wait, well, there's just some runtime results, so we just do online weight learning.",
                    "label": 0
                },
                {
                    "sent": "That's very fast.",
                    "label": 0
                },
                {
                    "sent": "If we do our fast version of our structure.",
                    "label": 0
                },
                {
                    "sent": "Learning it just takes a small little bit more time.",
                    "label": 0
                },
                {
                    "sent": "The slow version.",
                    "label": 0
                },
                {
                    "sent": "We're evaluating more clauses, which does give you better accuracy.",
                    "label": 0
                },
                {
                    "sent": "It does dramatically increase the overall runtime, so you can sort of trade off how much faster than update.",
                    "label": 0
                },
                {
                    "sent": "Indeed after each example, but even with a relatively fast update within.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You'll get pretty good results.",
                    "label": 0
                },
                {
                    "sent": "Just a couple of examples of some clauses that the system discovered for this Citeseer domain.",
                    "label": 0
                },
                {
                    "sent": "So one thing it added to the ASM model, which was the spoon model that he built for this domain is to say, well, if one filled if the current token is a title and it's followed by a period, then it's likely that the next cocaine is a venue.",
                    "label": 1
                },
                {
                    "sent": "And when you start with empty it sort of learns the obvious sort of sequential structure in this domain, not knowing that there's sequential structure here, it actually learns that usually when it opens in a field then the next token is in the same field.",
                    "label": 0
                },
                {
                    "sent": "But it can learn that prescribed without having been given that improved structural information operatory.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I'll leave plenty of time for questions hopefully and summarize, so we prevented.",
                    "label": 0
                },
                {
                    "sent": "As far as I know, the first online structure, learner 4 MLN.",
                    "label": 1
                },
                {
                    "sent": "You can either take an existing MLN set of clauses and add new clauses to it in an online fashion.",
                    "label": 0
                },
                {
                    "sent": "To make it even better.",
                    "label": 0
                },
                {
                    "sent": "And we showed that we can improve some reasonably starting state mesfer these sentence segmentation problems.",
                    "label": 0
                },
                {
                    "sent": "Are we can just learn a whole structure a completely from scratch and it it scales pretty well to handle these problems where we have sort of small structured problems coming in, you know, and maybe have thousands of them arriving incrementally and that we can outperform some existing algorithms on a couple of these example natural language segmentation problems.",
                    "label": 0
                },
                {
                    "sent": "Incense segmenting bibliographic citations and these advertisements on Craigslist.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If I understand you correctly, point out that you outperform some existing techniques.",
                    "label": 0
                },
                {
                    "sent": "Those previous techniques were not even online.",
                    "label": 0
                },
                {
                    "sent": "If I'm correct, is that right?",
                    "label": 0
                },
                {
                    "sent": "So given that you thought about how to make this skill, that you know thousands of little examples or not, so I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "So when I when I say, outperformed these.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Results in red are an online weight learner performed on an existing structure, so there's no structure change at all.",
                    "label": 0
                },
                {
                    "sent": "So now it's there.",
                    "label": 0
                },
                {
                    "sent": "All of these results are online results.",
                    "label": 0
                },
                {
                    "sent": "OK, you want your documents?",
                    "label": 0
                },
                {
                    "sent": "Are you working against previously published result?",
                    "label": 0
                },
                {
                    "sent": "Just that I think might be on the paper, but you don't need that, but it's at least competitive with the batch results.",
                    "label": 0
                },
                {
                    "sent": "You're not really taking too much of a hit just because you're doing it online, but I guess that's sort of a second question.",
                    "label": 0
                },
                {
                    "sent": "Do you have the motivation for this being online?",
                    "label": 0
                },
                {
                    "sent": "Where do you see it in motivation for these datasets?",
                    "label": 0
                },
                {
                    "sent": "And that this needs to be online learning process or these days that were more conveniently, and I guess, more the latter.",
                    "label": 0
                },
                {
                    "sent": "So I think this is I would do see this sort of initial working online structural, and we'd like to have some problems where the where it's more naturally required to have online learning.",
                    "label": 0
                },
                {
                    "sent": "So these were sort of convenient problems that we could use to illustrate it initially, but I don't know.",
                    "label": 0
                },
                {
                    "sent": "So you think they're the best problems for this task?",
                    "label": 0
                },
                {
                    "sent": "Very interesting thought.",
                    "label": 0
                },
                {
                    "sent": "As you can start with your algorithm from from an existing model, would it be possible to start off with a fast algorithm, then run for short time the slow algorithm which gives better results or those who are now we haven't lifted mixing well, you could change that parameter overtime.",
                    "label": 0
                },
                {
                    "sent": "You know if you want it you want to run slow in the beginning and then speed it up later or the other way so we haven't looked at automating the studying about parameters so that you could change how maybe how well it how much time it spends its doing better.",
                    "label": 0
                },
                {
                    "sent": "'cause that's a good.",
                    "label": 0
                },
                {
                    "sent": "Color.",
                    "label": 0
                },
                {
                    "sent": "How about if you start with an empty model compared to where you start with a minor League or start how many clauses?",
                    "label": 0
                },
                {
                    "sent": "How many does it learn?",
                    "label": 0
                },
                {
                    "sent": "See, this is where you wish the student was actually here 'cause he Grammas parents and he would be able to give you at least a back of the envelope.",
                    "label": 0
                },
                {
                    "sent": "I mean, because these have constants in them about words and field.",
                    "label": 0
                },
                {
                    "sent": "We're talking thousands if not 10s of thousands of clauses in the end of the day, 'cause it has a separate 'cause, you know, just the generic model has a separate cost for every combination of a token in a field.",
                    "label": 0
                },
                {
                    "sent": "And there's like 5 fields in there.",
                    "label": 0
                },
                {
                    "sent": "In these domains, there can be thousands or more of token, so it learns pretty big at my lens with thousands if not 10s of thousands of clauses.",
                    "label": 0
                },
                {
                    "sent": "Quick question.",
                    "label": 0
                },
                {
                    "sent": "Have you seen if the order of the examples on the arrived?",
                    "label": 0
                },
                {
                    "sent": "Yeah, now it's a good point.",
                    "label": 0
                },
                {
                    "sent": "We haven't looked at how since it is online.",
                    "label": 0
                },
                {
                    "sent": "It actually probably would be affected by the order of the presentation of the examples that we haven't looked effective algorithms.",
                    "label": 0
                },
                {
                    "sent": "Anymore questions.",
                    "label": 0
                },
                {
                    "sent": "Well, that's the end of this session.",
                    "label": 0
                },
                {
                    "sent": "That's not great game.",
                    "label": 0
                }
            ]
        }
    }
}