{
    "id": "pmprl2p5pbhctauezikfvvdftpbpjzbx",
    "title": "Elie: A two-level boundary classification approach to adaptive information extraction",
    "info": {
        "author": [
            "Aidan Finn, Institute of Technology Sligo"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "April 2005",
        "category": [
            "Top->Computer Science->Information Extraction"
        ]
    },
    "url": "http://videolectures.net/pcw05_finn_tlbca/",
    "segmentation": [
        [
            "OK, good afternoon everybody.",
            "Good morning so I'm going to talk about our system called.",
            "My name is Aiden fan.",
            "My supervisors name?",
            "I know from University College Dublin.",
            "If anybody is interested in getting this system, you can download it from a web page."
        ],
        [
            "So just briefly talk about what we mean by information extraction and covered it earlier on, but it's the task is to identify a set of predefined relevant items in text documents.",
            "So here's an example of the seminar announcement again, and some example fields that we want to extract would be speaker location.",
            "The start time and the end time.",
            "History.",
            "It is generally that they are independent and.",
            "We don't.",
            "We don't have sort of defined concepts and stuff for each for each document, for all that all the things that you want to extract."
        ],
        [
            "So some examples for information extraction can be useful commutes to improve information retrieval and text mining or trying to identify.",
            "Free tax.",
            "Need to extract or structured data from unstructured text so that we can create more structured search group.",
            "Database type structures.",
            "Use for automatic semantic conversation."
        ],
        [
            "So we want to talk about adaptive information extraction.",
            "So the idea is we have an unlabeled corpus and we have a human annotator such as.",
            "They annotate documents and give us a labeled set of labeled documents for training, which we passed to some kind of automatic machine learning algorithm to generate rules that we can automatically then applied to future documents."
        ],
        [
            "So just an overview following talk about it for the rest of the song.",
            "I'm going to describe A1 level SVM classification approach to information extraction.",
            "And get some results for high performance.",
            "Again then, I'm going to describe another enhancement that we give to this two level SVM approach.",
            "And give performance facilities on some benchmark information extraction tasks.",
            "Describer performance under Pascal Information extraction task.",
            "Analyze some reasons for performance and give some future work."
        ],
        [
            "So.",
            "We treat information extraction as a classification task.",
            "So we treat the identification of fragments start and end tokens as distinct token classification tasks.",
            "We extract each field independently so we don't.",
            "We don't take into account any information about other fields occurring in the document when we're trying to learn to extract one particular field.",
            "So we have a star classifier in an end classifier.",
            "For the star classifier, all the tokens that are fragments starts or positive instances and all the other tokens become negative instances for the end classifier, all the tokens that are fragmente ends or positive instances and all letter tokens into corpus become negative instances.",
            "Learning algorithm that we use is smol.",
            "And then we have A tag matching component which brings together the predictions of the star classifier in the end classifier.",
            "So it matches predicted starts and predicted it's based on a simple histogram model cord probability of a matching.",
            "In particular start would affect our end is based on.",
            "The proportion of times that fragment of that length card in the training."
        ],
        [
            "So here's a diagram just showed up.",
            "So from the training documents we generate our training instances for starts in the training instances friends and we use these classes to learning algorithms, generate models for predicting start and end classifiers.",
            "We also use the Starfarer since generate histogram model for the tag match."
        ],
        [
            "So the features that we use.",
            "The token features, which is the token itself.",
            "Part of speech information about particular.",
            "Talking gazeteer information.",
            "And orthographic information such as uppercase, lowercase, capitalized.",
            "I just mentioned that we this is the features that we use in our system originally for this particular Pascal task we used to.",
            "The gate features, but the features are much the same between the two men.",
            "So it represents a particular instance being called features for that token, as well as all tokens within fixed window before and after this open."
        ],
        [
            "So here's an example of an encoding.",
            "So first.",
            "We take the piece of text and you generate all the features from it.",
            "If you were to encode talking Alex with the windows plus or minus one in each side of it would generate the following set of features for.",
            "So from looking at the token Alex itself.",
            "We generate features, all the features that represent that particular token, and then we have additional features that present a token before and the soul can afterwards.",
            "So we do this for fixing the size.",
            "So for every instance we have features representing the token itself and the tokens before and afterwards."
        ],
        [
            "So.",
            "Originally to see how this performance this simple machine learning platforms, we had a tree, benchmark, datasets, sodas, seminar, announcements, job postings and Reuters corporate acquisitions.",
            "So at 31 fields until.",
            "And we did a 5050 split repeated 10 times.",
            "So fabulous mentioned some valuation issues earlier on, so we took the most conservative approach because whenever evaluating the system against the other ones."
        ],
        [
            "Here's a comparison.",
            "Automated system performs against the competitive systems.",
            "So.",
            "On the horizontal axis we have our system artist simple one level approach and on the vertical axis we have the competitive systems.",
            "So for anything below the diagonal line, that means are formed with their, love it to competitors are better, so for precision, fairly competitive with the other system."
        ],
        [
            "And also for Rico.",
            "The first measure."
        ],
        [
            "So this standard simple machine learning approach is competitive with the other information extraction systems.",
            "So we want to improve it further and we do this by adding a second layer of classifiers.",
            "So many errors that we have.",
            "Cases where we identify one end of a fragment with but not the other, so we get the start of the fragment, but we don't get the end or vice versa.",
            "So we had a secondary classifiers to learn to find the complementary tags for these are from tags.",
            "So these are more focused classifiers that are just set the task of given that you've identified with high probability that there's a fragment in this area of text.",
            "The secondary classifiers are harder to find the missing tag.",
            "So targets parts of the text that are more likely to contain fragments to be extracted."
        ],
        [
            "So now we have two distinct phases to learning.",
            "Level one is ascribed so far where we learn to stick the starts and ends fragments to be extracted.",
            "And it's generally has high precision, but low recall.",
            "Level 2 is designed to increase recall by adding classifiers that detect the end of a fragment given its beginning or detect the beginning of a fragment given its."
        ],
        [
            "So.",
            "For the level one approach, we have a very large number of negative instances and very small number of positive instances.",
            "And we learn to identify the starts and ends using all these instances as input.",
            "Level 2 models are built using more focused training data.",
            "So the prior probability that an instance is a boundary is much higher than it was for levolor soda level to start.",
            "Classifier uses on the instances that occur at fixed distance before and then on the level 2, and classifier uses only instances that occur fixed distance after start."
        ],
        [
            "So there's an example that shows had a level one, another two part.",
            "Level 2 approach works.",
            "For learning and extracting.",
            "So if we look at the.",
            "The learning part first.",
            "So.",
            "This example using outlook and outlook to look ahead of tree tokens.",
            "So have a fragment of text Burger will be talk by Bill Wescott at the engineering.",
            "And if the fragment we want to extract is Bill Wescott, whose speaker.",
            "Level 1 classifier switches.",
            "So to start classifier which is billed as a positive example and all the other tokens as negative examples and level one end classifier would use westgard as a positive example and all the others as negative examples on building its models.",
            "Level 2 classifier then.",
            "The Level 2 and classifier would only use Token Ville and fixed number of tokens afterwards.",
            "So we fixed it in this example tree to learn its classifier under Level 2 star classifier.",
            "Use the end of fragments and a fixed number of tokens before hand.",
            "So it's built using more focus training data.",
            "So.",
            "If we were to use these models on the entire purpose to generate huge number of false positives, so how we use them as we only applied to areas of the text where we've identified with a high probability that there should be fragmente.",
            "So we use the level of classifiers for not.",
            "So if we were in the case where it finally extract text.",
            "If level one predicted that this token professor was a star, then we don't need apply to level 2 classifiers to that token and fixed number of tokens afterwards.",
            "Same pretty for Level 2 and pacifiers."
        ],
        [
            "So these are just diagrams that show the architecture of the system.",
            "Using the two level approach, so we just have a whole second level of classifiers."
        ],
        [
            "So some experiments for level 2.",
            "As we showed earlier, for the one level approach.",
            "So here's some parameters of a window length of tree means that we.",
            "We encode.",
            "Token information for each instance of tree tokens before entry tokens afterwards.",
            "Level 2 look ahead and look back at 10.",
            "So this means that the Level 2 classifiers.",
            "So the end classifier will use the 10:10 exam 10 tokens after start Level, 2 star classifier reduced to 10 tokens before and when building this model and we only use the top 5000 features, aren't playing information gain."
        ],
        [
            "So.",
            "So this graph shows all the Turkey one fields.",
            "Performance at level one and Level 2 for each field, so it's the other precision.",
            "That it's always drops a bit when we get to Level 3 classifier level 2 causes dropping precision, and this is because.",
            "Well, if you ever wanna perfect precision and get an increase in recall at level 2 without sacrificing any precision because it doesn't, sometimes will add extra tags onto.",
            "Ones that were original mistakes at level 2.",
            "So we could see a drop in precision between level one and Level 2."
        ],
        [
            "But we see a fairly good increase in recall across all fields.",
            "So recall increases.",
            "Across every single field there, I think, except for law.",
            "Unlock more notably on the fields where we're not doing particularly well.",
            "The increase is there can be a bit bigger.",
            "So Level 2 gives us a good increase in recall at the expense of precision."
        ],
        [
            "Death measure.",
            "The increase in recall that we get.",
            "Is quite a bit better than the drop in precision, so increaseth measure across all these fields.",
            "These boots relate to the slots.",
            "Just feels from the tree tree.",
            "Example benchmark.",
            "They said.",
            "Not going to Pascal stuff in there."
        ],
        [
            "And so these are the same similar graphs that we saw a group with.",
            "Level 2 approach on the horizontal axis instead of the other one.",
            "So see."
        ],
        [
            "Better than most of the systems."
        ],
        [
            "So Pascal on the Pascal challenge they said.",
            "We did run our system.",
            "Sort of parameters we did for this experiments where we use the window link before so we can code it for tokens before and after for each instance.",
            "We used to look ahead, look back of town again, randomly deleted 50% it's negative instances because.",
            "In general, the Pascal documents were a lot longer than the benchmark datasets.",
            "Then we looked at before.",
            "Again, we used the most top 5000 frankly information game.",
            "We still need the most confident predictions.",
            "Sorry for system.",
            "Predicted safer date for a.",
            "Call for papers date or something like that.",
            "It might predict three or four different dates, so we only use the one with the most confidence and we didn't perform any parameter optimization, so we haven't tried it with any other set of parameters."
        ],
        [
            "So here's an example of historical results.",
            "On the.",
            "Pascal Challenge data set.",
            "So here's the for the Four fold cross validation experiments and this is on the test data.",
            "So.",
            "Here we have our system.",
            "And the second column is the best system of all the rest.",
            "In our system we have performance of our system on how it ranked in the list of all the others.",
            "So.",
            "As you can see, it's not.",
            "It's not doing as well on Pascal data as it was on the other benchmark datasets, so.",
            "On a lot of these on the training set, we're kind of around the middle.",
            "I think there's 21 systems and installed unconference names world.",
            "Don't look at the test.",
            "Like we're still around the middle on someone we're dropping then.",
            "So here on the conference homepage, performance is really about.",
            "Conference homepage lot of people have trouble with.",
            "I think it's an example of where the feature encoding hurts your ability to recognize it because Kate breaks up.",
            "URLs into each single token that account, so you rather quickly split into 15 or 20 tokens, and you're supposed to recognize the start and the end.",
            "Or is the URL was recognized as a single single folk and it would be a bit easier.",
            "But um.",
            "So we have to ask them why is our system not performing as well in this data as it was in the previous data?"
        ],
        [
            "So the main reason is that recall is for another one, so there are a few orphan types of Avalon, so there's little potential for the two level approach to improve it.",
            "So why do we have poor performance at level on other two reasons?",
            "We think one is the high imbalance in the data to negative instances far outnumbered positive instances.",
            "The second reason is that our system extracts fields independently, so it doesn't take account of relation information between fields, so some fields in the Pascal data.",
            "Do occur in conjunction a lot.",
            "So for example.",
            "The name of conference and conference acronym conference acronym usually occurs directly after that.",
            "The conference name.",
            "The dates.",
            "Usually occur in say, four dates in order.",
            "Like the day of the conference, followed by notification acceptance follows by.",
            "File papers do stuff like that."
        ],
        [
            "So just want to talk briefly about performance loss, would imbalance data.",
            "So it was a nice son.",
            "Paperback this email This year.",
            "So this is a problem with, I think all learning algorithms to instantiate that use all the positives and negatives for learning as opposed to just making doing induction from positive examples or something, But a particular to SPMS or SKMS are affected by this problem but not not as badly as some other learning algorithms.",
            "But the main reason for performance, loss and violence there the positive points like further from the ideal boundary.",
            "ESPN learns Avenger that is too close to an skewed towards positive instances because there's such a small number of them.",
            "Also, the weakness of soft margins.",
            "There's a tradeoff between maximizing margin and minimizing the error.",
            "So the higher the imbalance.",
            "The more likely in SPM is to learn a hyperplane that's too close to positive examples."
        ],
        [
            "So here's an example.",
            "Some imbalanced data with all the negatives here, and all the positives here.",
            "We want to learn.",
            "Hyper plan that separates the two.",
            "So the actual ideal hyperplane would be faster than negatives here because.",
            "We've seen such a small number of positives that we're going to assume that most some there's some positives in this space, whereas if there was a negatives in this area would probably have seen them already because we have such a high number of the negative examples.",
            "What actually gets learned?",
            "Is a hyperplane that's as close to the positives as possible?",
            "So this leads to the kind of over fitting that we saw going between the trainings up training set and test set.",
            "And also is the reason why we get such high precision using SVM's and I'm not so so high recall."
        ],
        [
            "So just to give you an idea of the kind of numbers we're talking about.",
            "In the writers data set.",
            "We have imbalanced ratios of the order of 100 to one, so for every positive example there's maybe 100 negative examples.",
            "Seminar announcements expect 200 to one.",
            "Jobs data sets.",
            "About 500 salon and in the past couple challenge.",
            "It's around 900 times along so it's a lot more imbalanced in previous dates.",
            "That's mainly because the documents are a lot longer and you still only have a few occurrences of each field in each document.",
            "When we look back at our performance over the different datasets, we see that our performance in relation to the other information extraction algorithms is proportional to the level imbalance in the data set, so.",
            "Our system is not robust to high levels of imbalance in the data set, so we need to take steps to."
        ],
        [
            "Address that.",
            "So.",
            "Improvements in future work.",
            "So we're working on addressing these two issues.",
            "The first one is instance filtering, so I mentioned earlier that we randomly delete 50% of the negative instances, but random deletion of instance is not really a good a good strategy because.",
            "If you delete an informative instance because there are so few informative instances in the data, if you delete an informative on, you really get hurt by it.",
            "So.",
            "So we want to filter instances at level one for highly imbalanced datasets to improve the level of performance so that level 2 can give us some more.",
            "Improvements.",
            "So we've implemented at the moment a variation of the filtering strategy destroy described by Cuisine Royale, which is actually implemented into yogurt place DC versus submission, which was one of the SVM implementations of data, did quite well in this challenge, and they implement this filtering strategy.",
            "So we have a variation of that instrument in our system there.",
            "I don't have any results here today, but some initial experiments showing that it does give us a bit of an increase on some of the fields.",
            "Um?",
            "The other thing we're looking at this moment is dependence between fields so.",
            "Need to take account of the occurrence of other fields.",
            "Trying to learn each field so.",
            "Not quite sure how to do that at the moment, but we're working on trying to adapt system safe and relational information between fields.",
            "Thanks.",
            "Expected.",
            "No, what happens is level one gives us a set of predictions for starts and then fax over the entire power level 2 looks at all those predictions for starts and ends and then supplements and with additional predictions.",
            "So for any orphan tag along Creations 11 one looks at it and sees and tries to add it.",
            "Suggested.",
            "If we have to retreat, suggestions are level.",
            "Yeah, So what do you do?",
            "We take them off and then at the end of the process.",
            "Probability zero and then the rank instead of threshold for you.",
            "Confidence measure that we use now is the.",
            "Conference for stars, times, confidence, commands, times of confidence of the language fragment, and the confidence of stars in the conference events is given by your distance from like.",
            "We only use that very end.",
            "Most expensive.",
            "Is this?",
            "Reset test instances replied start models, get predictions for level 122 or per starts and then start and then we pass these.",
            "So the predictions that we have.",
            "At this point.",
            "Car starts.",
            "And then these are things that I'm not sure which uses to thank probabilities.",
            "Baseline level, even the simple stretches it simply.",
            "Or whatever it would have went.",
            "Garden Square morning star yeah.",
            "So you have one of these chains for each class of slots you extracts, so you have to deal with possibly overlapping predictions.",
            "Well, you mean the stranger feels right, right?",
            "But we don't.",
            "We don't see that we have everything.",
            "Each field is extracted completely.",
            "Never actually happened.",
            "In this case.",
            "In that case, we just picked the most confident.",
            "Questions.",
            "Start from everything.",
            "And for the building site.",
            "Yeah.",
            "Oh yeah, that's basically just the probability for.",
            "Probability for each field fragments.",
            "Probability.",
            "Yeah.",
            "Yeah.",
            "Thank you, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, good afternoon everybody.",
                    "label": 0
                },
                {
                    "sent": "Good morning so I'm going to talk about our system called.",
                    "label": 0
                },
                {
                    "sent": "My name is Aiden fan.",
                    "label": 0
                },
                {
                    "sent": "My supervisors name?",
                    "label": 0
                },
                {
                    "sent": "I know from University College Dublin.",
                    "label": 1
                },
                {
                    "sent": "If anybody is interested in getting this system, you can download it from a web page.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just briefly talk about what we mean by information extraction and covered it earlier on, but it's the task is to identify a set of predefined relevant items in text documents.",
                    "label": 0
                },
                {
                    "sent": "So here's an example of the seminar announcement again, and some example fields that we want to extract would be speaker location.",
                    "label": 0
                },
                {
                    "sent": "The start time and the end time.",
                    "label": 0
                },
                {
                    "sent": "History.",
                    "label": 0
                },
                {
                    "sent": "It is generally that they are independent and.",
                    "label": 0
                },
                {
                    "sent": "We don't.",
                    "label": 0
                },
                {
                    "sent": "We don't have sort of defined concepts and stuff for each for each document, for all that all the things that you want to extract.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So some examples for information extraction can be useful commutes to improve information retrieval and text mining or trying to identify.",
                    "label": 1
                },
                {
                    "sent": "Free tax.",
                    "label": 1
                },
                {
                    "sent": "Need to extract or structured data from unstructured text so that we can create more structured search group.",
                    "label": 0
                },
                {
                    "sent": "Database type structures.",
                    "label": 0
                },
                {
                    "sent": "Use for automatic semantic conversation.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we want to talk about adaptive information extraction.",
                    "label": 0
                },
                {
                    "sent": "So the idea is we have an unlabeled corpus and we have a human annotator such as.",
                    "label": 0
                },
                {
                    "sent": "They annotate documents and give us a labeled set of labeled documents for training, which we passed to some kind of automatic machine learning algorithm to generate rules that we can automatically then applied to future documents.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just an overview following talk about it for the rest of the song.",
                    "label": 0
                },
                {
                    "sent": "I'm going to describe A1 level SVM classification approach to information extraction.",
                    "label": 1
                },
                {
                    "sent": "And get some results for high performance.",
                    "label": 0
                },
                {
                    "sent": "Again then, I'm going to describe another enhancement that we give to this two level SVM approach.",
                    "label": 0
                },
                {
                    "sent": "And give performance facilities on some benchmark information extraction tasks.",
                    "label": 0
                },
                {
                    "sent": "Describer performance under Pascal Information extraction task.",
                    "label": 0
                },
                {
                    "sent": "Analyze some reasons for performance and give some future work.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We treat information extraction as a classification task.",
                    "label": 0
                },
                {
                    "sent": "So we treat the identification of fragments start and end tokens as distinct token classification tasks.",
                    "label": 0
                },
                {
                    "sent": "We extract each field independently so we don't.",
                    "label": 0
                },
                {
                    "sent": "We don't take into account any information about other fields occurring in the document when we're trying to learn to extract one particular field.",
                    "label": 0
                },
                {
                    "sent": "So we have a star classifier in an end classifier.",
                    "label": 0
                },
                {
                    "sent": "For the star classifier, all the tokens that are fragments starts or positive instances and all the other tokens become negative instances for the end classifier, all the tokens that are fragmente ends or positive instances and all letter tokens into corpus become negative instances.",
                    "label": 0
                },
                {
                    "sent": "Learning algorithm that we use is smol.",
                    "label": 0
                },
                {
                    "sent": "And then we have A tag matching component which brings together the predictions of the star classifier in the end classifier.",
                    "label": 0
                },
                {
                    "sent": "So it matches predicted starts and predicted it's based on a simple histogram model cord probability of a matching.",
                    "label": 0
                },
                {
                    "sent": "In particular start would affect our end is based on.",
                    "label": 0
                },
                {
                    "sent": "The proportion of times that fragment of that length card in the training.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's a diagram just showed up.",
                    "label": 0
                },
                {
                    "sent": "So from the training documents we generate our training instances for starts in the training instances friends and we use these classes to learning algorithms, generate models for predicting start and end classifiers.",
                    "label": 0
                },
                {
                    "sent": "We also use the Starfarer since generate histogram model for the tag match.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the features that we use.",
                    "label": 0
                },
                {
                    "sent": "The token features, which is the token itself.",
                    "label": 0
                },
                {
                    "sent": "Part of speech information about particular.",
                    "label": 0
                },
                {
                    "sent": "Talking gazeteer information.",
                    "label": 0
                },
                {
                    "sent": "And orthographic information such as uppercase, lowercase, capitalized.",
                    "label": 1
                },
                {
                    "sent": "I just mentioned that we this is the features that we use in our system originally for this particular Pascal task we used to.",
                    "label": 0
                },
                {
                    "sent": "The gate features, but the features are much the same between the two men.",
                    "label": 0
                },
                {
                    "sent": "So it represents a particular instance being called features for that token, as well as all tokens within fixed window before and after this open.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's an example of an encoding.",
                    "label": 0
                },
                {
                    "sent": "So first.",
                    "label": 0
                },
                {
                    "sent": "We take the piece of text and you generate all the features from it.",
                    "label": 0
                },
                {
                    "sent": "If you were to encode talking Alex with the windows plus or minus one in each side of it would generate the following set of features for.",
                    "label": 0
                },
                {
                    "sent": "So from looking at the token Alex itself.",
                    "label": 0
                },
                {
                    "sent": "We generate features, all the features that represent that particular token, and then we have additional features that present a token before and the soul can afterwards.",
                    "label": 0
                },
                {
                    "sent": "So we do this for fixing the size.",
                    "label": 0
                },
                {
                    "sent": "So for every instance we have features representing the token itself and the tokens before and afterwards.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Originally to see how this performance this simple machine learning platforms, we had a tree, benchmark, datasets, sodas, seminar, announcements, job postings and Reuters corporate acquisitions.",
                    "label": 1
                },
                {
                    "sent": "So at 31 fields until.",
                    "label": 1
                },
                {
                    "sent": "And we did a 5050 split repeated 10 times.",
                    "label": 0
                },
                {
                    "sent": "So fabulous mentioned some valuation issues earlier on, so we took the most conservative approach because whenever evaluating the system against the other ones.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's a comparison.",
                    "label": 0
                },
                {
                    "sent": "Automated system performs against the competitive systems.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "On the horizontal axis we have our system artist simple one level approach and on the vertical axis we have the competitive systems.",
                    "label": 0
                },
                {
                    "sent": "So for anything below the diagonal line, that means are formed with their, love it to competitors are better, so for precision, fairly competitive with the other system.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And also for Rico.",
                    "label": 0
                },
                {
                    "sent": "The first measure.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this standard simple machine learning approach is competitive with the other information extraction systems.",
                    "label": 1
                },
                {
                    "sent": "So we want to improve it further and we do this by adding a second layer of classifiers.",
                    "label": 0
                },
                {
                    "sent": "So many errors that we have.",
                    "label": 1
                },
                {
                    "sent": "Cases where we identify one end of a fragment with but not the other, so we get the start of the fragment, but we don't get the end or vice versa.",
                    "label": 1
                },
                {
                    "sent": "So we had a secondary classifiers to learn to find the complementary tags for these are from tags.",
                    "label": 0
                },
                {
                    "sent": "So these are more focused classifiers that are just set the task of given that you've identified with high probability that there's a fragment in this area of text.",
                    "label": 0
                },
                {
                    "sent": "The secondary classifiers are harder to find the missing tag.",
                    "label": 0
                },
                {
                    "sent": "So targets parts of the text that are more likely to contain fragments to be extracted.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now we have two distinct phases to learning.",
                    "label": 0
                },
                {
                    "sent": "Level one is ascribed so far where we learn to stick the starts and ends fragments to be extracted.",
                    "label": 1
                },
                {
                    "sent": "And it's generally has high precision, but low recall.",
                    "label": 1
                },
                {
                    "sent": "Level 2 is designed to increase recall by adding classifiers that detect the end of a fragment given its beginning or detect the beginning of a fragment given its.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "For the level one approach, we have a very large number of negative instances and very small number of positive instances.",
                    "label": 1
                },
                {
                    "sent": "And we learn to identify the starts and ends using all these instances as input.",
                    "label": 0
                },
                {
                    "sent": "Level 2 models are built using more focused training data.",
                    "label": 1
                },
                {
                    "sent": "So the prior probability that an instance is a boundary is much higher than it was for levolor soda level to start.",
                    "label": 1
                },
                {
                    "sent": "Classifier uses on the instances that occur at fixed distance before and then on the level 2, and classifier uses only instances that occur fixed distance after start.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there's an example that shows had a level one, another two part.",
                    "label": 0
                },
                {
                    "sent": "Level 2 approach works.",
                    "label": 0
                },
                {
                    "sent": "For learning and extracting.",
                    "label": 0
                },
                {
                    "sent": "So if we look at the.",
                    "label": 0
                },
                {
                    "sent": "The learning part first.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This example using outlook and outlook to look ahead of tree tokens.",
                    "label": 0
                },
                {
                    "sent": "So have a fragment of text Burger will be talk by Bill Wescott at the engineering.",
                    "label": 0
                },
                {
                    "sent": "And if the fragment we want to extract is Bill Wescott, whose speaker.",
                    "label": 0
                },
                {
                    "sent": "Level 1 classifier switches.",
                    "label": 0
                },
                {
                    "sent": "So to start classifier which is billed as a positive example and all the other tokens as negative examples and level one end classifier would use westgard as a positive example and all the others as negative examples on building its models.",
                    "label": 0
                },
                {
                    "sent": "Level 2 classifier then.",
                    "label": 0
                },
                {
                    "sent": "The Level 2 and classifier would only use Token Ville and fixed number of tokens afterwards.",
                    "label": 0
                },
                {
                    "sent": "So we fixed it in this example tree to learn its classifier under Level 2 star classifier.",
                    "label": 0
                },
                {
                    "sent": "Use the end of fragments and a fixed number of tokens before hand.",
                    "label": 0
                },
                {
                    "sent": "So it's built using more focus training data.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If we were to use these models on the entire purpose to generate huge number of false positives, so how we use them as we only applied to areas of the text where we've identified with a high probability that there should be fragmente.",
                    "label": 0
                },
                {
                    "sent": "So we use the level of classifiers for not.",
                    "label": 0
                },
                {
                    "sent": "So if we were in the case where it finally extract text.",
                    "label": 0
                },
                {
                    "sent": "If level one predicted that this token professor was a star, then we don't need apply to level 2 classifiers to that token and fixed number of tokens afterwards.",
                    "label": 0
                },
                {
                    "sent": "Same pretty for Level 2 and pacifiers.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these are just diagrams that show the architecture of the system.",
                    "label": 0
                },
                {
                    "sent": "Using the two level approach, so we just have a whole second level of classifiers.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So some experiments for level 2.",
                    "label": 0
                },
                {
                    "sent": "As we showed earlier, for the one level approach.",
                    "label": 0
                },
                {
                    "sent": "So here's some parameters of a window length of tree means that we.",
                    "label": 1
                },
                {
                    "sent": "We encode.",
                    "label": 0
                },
                {
                    "sent": "Token information for each instance of tree tokens before entry tokens afterwards.",
                    "label": 0
                },
                {
                    "sent": "Level 2 look ahead and look back at 10.",
                    "label": 0
                },
                {
                    "sent": "So this means that the Level 2 classifiers.",
                    "label": 0
                },
                {
                    "sent": "So the end classifier will use the 10:10 exam 10 tokens after start Level, 2 star classifier reduced to 10 tokens before and when building this model and we only use the top 5000 features, aren't playing information gain.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So this graph shows all the Turkey one fields.",
                    "label": 0
                },
                {
                    "sent": "Performance at level one and Level 2 for each field, so it's the other precision.",
                    "label": 0
                },
                {
                    "sent": "That it's always drops a bit when we get to Level 3 classifier level 2 causes dropping precision, and this is because.",
                    "label": 0
                },
                {
                    "sent": "Well, if you ever wanna perfect precision and get an increase in recall at level 2 without sacrificing any precision because it doesn't, sometimes will add extra tags onto.",
                    "label": 0
                },
                {
                    "sent": "Ones that were original mistakes at level 2.",
                    "label": 0
                },
                {
                    "sent": "So we could see a drop in precision between level one and Level 2.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But we see a fairly good increase in recall across all fields.",
                    "label": 0
                },
                {
                    "sent": "So recall increases.",
                    "label": 0
                },
                {
                    "sent": "Across every single field there, I think, except for law.",
                    "label": 0
                },
                {
                    "sent": "Unlock more notably on the fields where we're not doing particularly well.",
                    "label": 0
                },
                {
                    "sent": "The increase is there can be a bit bigger.",
                    "label": 0
                },
                {
                    "sent": "So Level 2 gives us a good increase in recall at the expense of precision.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Death measure.",
                    "label": 0
                },
                {
                    "sent": "The increase in recall that we get.",
                    "label": 0
                },
                {
                    "sent": "Is quite a bit better than the drop in precision, so increaseth measure across all these fields.",
                    "label": 0
                },
                {
                    "sent": "These boots relate to the slots.",
                    "label": 0
                },
                {
                    "sent": "Just feels from the tree tree.",
                    "label": 0
                },
                {
                    "sent": "Example benchmark.",
                    "label": 0
                },
                {
                    "sent": "They said.",
                    "label": 0
                },
                {
                    "sent": "Not going to Pascal stuff in there.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so these are the same similar graphs that we saw a group with.",
                    "label": 0
                },
                {
                    "sent": "Level 2 approach on the horizontal axis instead of the other one.",
                    "label": 0
                },
                {
                    "sent": "So see.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Better than most of the systems.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So Pascal on the Pascal challenge they said.",
                    "label": 0
                },
                {
                    "sent": "We did run our system.",
                    "label": 0
                },
                {
                    "sent": "Sort of parameters we did for this experiments where we use the window link before so we can code it for tokens before and after for each instance.",
                    "label": 0
                },
                {
                    "sent": "We used to look ahead, look back of town again, randomly deleted 50% it's negative instances because.",
                    "label": 0
                },
                {
                    "sent": "In general, the Pascal documents were a lot longer than the benchmark datasets.",
                    "label": 0
                },
                {
                    "sent": "Then we looked at before.",
                    "label": 0
                },
                {
                    "sent": "Again, we used the most top 5000 frankly information game.",
                    "label": 0
                },
                {
                    "sent": "We still need the most confident predictions.",
                    "label": 1
                },
                {
                    "sent": "Sorry for system.",
                    "label": 0
                },
                {
                    "sent": "Predicted safer date for a.",
                    "label": 1
                },
                {
                    "sent": "Call for papers date or something like that.",
                    "label": 0
                },
                {
                    "sent": "It might predict three or four different dates, so we only use the one with the most confidence and we didn't perform any parameter optimization, so we haven't tried it with any other set of parameters.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's an example of historical results.",
                    "label": 0
                },
                {
                    "sent": "On the.",
                    "label": 0
                },
                {
                    "sent": "Pascal Challenge data set.",
                    "label": 0
                },
                {
                    "sent": "So here's the for the Four fold cross validation experiments and this is on the test data.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Here we have our system.",
                    "label": 0
                },
                {
                    "sent": "And the second column is the best system of all the rest.",
                    "label": 0
                },
                {
                    "sent": "In our system we have performance of our system on how it ranked in the list of all the others.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "As you can see, it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not doing as well on Pascal data as it was on the other benchmark datasets, so.",
                    "label": 0
                },
                {
                    "sent": "On a lot of these on the training set, we're kind of around the middle.",
                    "label": 0
                },
                {
                    "sent": "I think there's 21 systems and installed unconference names world.",
                    "label": 0
                },
                {
                    "sent": "Don't look at the test.",
                    "label": 0
                },
                {
                    "sent": "Like we're still around the middle on someone we're dropping then.",
                    "label": 0
                },
                {
                    "sent": "So here on the conference homepage, performance is really about.",
                    "label": 0
                },
                {
                    "sent": "Conference homepage lot of people have trouble with.",
                    "label": 0
                },
                {
                    "sent": "I think it's an example of where the feature encoding hurts your ability to recognize it because Kate breaks up.",
                    "label": 0
                },
                {
                    "sent": "URLs into each single token that account, so you rather quickly split into 15 or 20 tokens, and you're supposed to recognize the start and the end.",
                    "label": 0
                },
                {
                    "sent": "Or is the URL was recognized as a single single folk and it would be a bit easier.",
                    "label": 0
                },
                {
                    "sent": "But um.",
                    "label": 0
                },
                {
                    "sent": "So we have to ask them why is our system not performing as well in this data as it was in the previous data?",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the main reason is that recall is for another one, so there are a few orphan types of Avalon, so there's little potential for the two level approach to improve it.",
                    "label": 0
                },
                {
                    "sent": "So why do we have poor performance at level on other two reasons?",
                    "label": 0
                },
                {
                    "sent": "We think one is the high imbalance in the data to negative instances far outnumbered positive instances.",
                    "label": 1
                },
                {
                    "sent": "The second reason is that our system extracts fields independently, so it doesn't take account of relation information between fields, so some fields in the Pascal data.",
                    "label": 0
                },
                {
                    "sent": "Do occur in conjunction a lot.",
                    "label": 0
                },
                {
                    "sent": "So for example.",
                    "label": 0
                },
                {
                    "sent": "The name of conference and conference acronym conference acronym usually occurs directly after that.",
                    "label": 0
                },
                {
                    "sent": "The conference name.",
                    "label": 0
                },
                {
                    "sent": "The dates.",
                    "label": 0
                },
                {
                    "sent": "Usually occur in say, four dates in order.",
                    "label": 0
                },
                {
                    "sent": "Like the day of the conference, followed by notification acceptance follows by.",
                    "label": 0
                },
                {
                    "sent": "File papers do stuff like that.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just want to talk briefly about performance loss, would imbalance data.",
                    "label": 0
                },
                {
                    "sent": "So it was a nice son.",
                    "label": 0
                },
                {
                    "sent": "Paperback this email This year.",
                    "label": 0
                },
                {
                    "sent": "So this is a problem with, I think all learning algorithms to instantiate that use all the positives and negatives for learning as opposed to just making doing induction from positive examples or something, But a particular to SPMS or SKMS are affected by this problem but not not as badly as some other learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "But the main reason for performance, loss and violence there the positive points like further from the ideal boundary.",
                    "label": 1
                },
                {
                    "sent": "ESPN learns Avenger that is too close to an skewed towards positive instances because there's such a small number of them.",
                    "label": 0
                },
                {
                    "sent": "Also, the weakness of soft margins.",
                    "label": 1
                },
                {
                    "sent": "There's a tradeoff between maximizing margin and minimizing the error.",
                    "label": 0
                },
                {
                    "sent": "So the higher the imbalance.",
                    "label": 0
                },
                {
                    "sent": "The more likely in SPM is to learn a hyperplane that's too close to positive examples.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's an example.",
                    "label": 0
                },
                {
                    "sent": "Some imbalanced data with all the negatives here, and all the positives here.",
                    "label": 0
                },
                {
                    "sent": "We want to learn.",
                    "label": 0
                },
                {
                    "sent": "Hyper plan that separates the two.",
                    "label": 0
                },
                {
                    "sent": "So the actual ideal hyperplane would be faster than negatives here because.",
                    "label": 0
                },
                {
                    "sent": "We've seen such a small number of positives that we're going to assume that most some there's some positives in this space, whereas if there was a negatives in this area would probably have seen them already because we have such a high number of the negative examples.",
                    "label": 0
                },
                {
                    "sent": "What actually gets learned?",
                    "label": 0
                },
                {
                    "sent": "Is a hyperplane that's as close to the positives as possible?",
                    "label": 0
                },
                {
                    "sent": "So this leads to the kind of over fitting that we saw going between the trainings up training set and test set.",
                    "label": 0
                },
                {
                    "sent": "And also is the reason why we get such high precision using SVM's and I'm not so so high recall.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just to give you an idea of the kind of numbers we're talking about.",
                    "label": 0
                },
                {
                    "sent": "In the writers data set.",
                    "label": 0
                },
                {
                    "sent": "We have imbalanced ratios of the order of 100 to one, so for every positive example there's maybe 100 negative examples.",
                    "label": 0
                },
                {
                    "sent": "Seminar announcements expect 200 to one.",
                    "label": 1
                },
                {
                    "sent": "Jobs data sets.",
                    "label": 0
                },
                {
                    "sent": "About 500 salon and in the past couple challenge.",
                    "label": 0
                },
                {
                    "sent": "It's around 900 times along so it's a lot more imbalanced in previous dates.",
                    "label": 0
                },
                {
                    "sent": "That's mainly because the documents are a lot longer and you still only have a few occurrences of each field in each document.",
                    "label": 0
                },
                {
                    "sent": "When we look back at our performance over the different datasets, we see that our performance in relation to the other information extraction algorithms is proportional to the level imbalance in the data set, so.",
                    "label": 1
                },
                {
                    "sent": "Our system is not robust to high levels of imbalance in the data set, so we need to take steps to.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Address that.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Improvements in future work.",
                    "label": 0
                },
                {
                    "sent": "So we're working on addressing these two issues.",
                    "label": 0
                },
                {
                    "sent": "The first one is instance filtering, so I mentioned earlier that we randomly delete 50% of the negative instances, but random deletion of instance is not really a good a good strategy because.",
                    "label": 0
                },
                {
                    "sent": "If you delete an informative instance because there are so few informative instances in the data, if you delete an informative on, you really get hurt by it.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So we want to filter instances at level one for highly imbalanced datasets to improve the level of performance so that level 2 can give us some more.",
                    "label": 1
                },
                {
                    "sent": "Improvements.",
                    "label": 0
                },
                {
                    "sent": "So we've implemented at the moment a variation of the filtering strategy destroy described by Cuisine Royale, which is actually implemented into yogurt place DC versus submission, which was one of the SVM implementations of data, did quite well in this challenge, and they implement this filtering strategy.",
                    "label": 0
                },
                {
                    "sent": "So we have a variation of that instrument in our system there.",
                    "label": 0
                },
                {
                    "sent": "I don't have any results here today, but some initial experiments showing that it does give us a bit of an increase on some of the fields.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 1
                },
                {
                    "sent": "The other thing we're looking at this moment is dependence between fields so.",
                    "label": 0
                },
                {
                    "sent": "Need to take account of the occurrence of other fields.",
                    "label": 1
                },
                {
                    "sent": "Trying to learn each field so.",
                    "label": 0
                },
                {
                    "sent": "Not quite sure how to do that at the moment, but we're working on trying to adapt system safe and relational information between fields.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "Expected.",
                    "label": 0
                },
                {
                    "sent": "No, what happens is level one gives us a set of predictions for starts and then fax over the entire power level 2 looks at all those predictions for starts and ends and then supplements and with additional predictions.",
                    "label": 0
                },
                {
                    "sent": "So for any orphan tag along Creations 11 one looks at it and sees and tries to add it.",
                    "label": 0
                },
                {
                    "sent": "Suggested.",
                    "label": 0
                },
                {
                    "sent": "If we have to retreat, suggestions are level.",
                    "label": 0
                },
                {
                    "sent": "Yeah, So what do you do?",
                    "label": 0
                },
                {
                    "sent": "We take them off and then at the end of the process.",
                    "label": 0
                },
                {
                    "sent": "Probability zero and then the rank instead of threshold for you.",
                    "label": 0
                },
                {
                    "sent": "Confidence measure that we use now is the.",
                    "label": 0
                },
                {
                    "sent": "Conference for stars, times, confidence, commands, times of confidence of the language fragment, and the confidence of stars in the conference events is given by your distance from like.",
                    "label": 0
                },
                {
                    "sent": "We only use that very end.",
                    "label": 0
                },
                {
                    "sent": "Most expensive.",
                    "label": 0
                },
                {
                    "sent": "Is this?",
                    "label": 0
                },
                {
                    "sent": "Reset test instances replied start models, get predictions for level 122 or per starts and then start and then we pass these.",
                    "label": 0
                },
                {
                    "sent": "So the predictions that we have.",
                    "label": 0
                },
                {
                    "sent": "At this point.",
                    "label": 0
                },
                {
                    "sent": "Car starts.",
                    "label": 0
                },
                {
                    "sent": "And then these are things that I'm not sure which uses to thank probabilities.",
                    "label": 0
                },
                {
                    "sent": "Baseline level, even the simple stretches it simply.",
                    "label": 0
                },
                {
                    "sent": "Or whatever it would have went.",
                    "label": 0
                },
                {
                    "sent": "Garden Square morning star yeah.",
                    "label": 0
                },
                {
                    "sent": "So you have one of these chains for each class of slots you extracts, so you have to deal with possibly overlapping predictions.",
                    "label": 0
                },
                {
                    "sent": "Well, you mean the stranger feels right, right?",
                    "label": 0
                },
                {
                    "sent": "But we don't.",
                    "label": 0
                },
                {
                    "sent": "We don't see that we have everything.",
                    "label": 0
                },
                {
                    "sent": "Each field is extracted completely.",
                    "label": 0
                },
                {
                    "sent": "Never actually happened.",
                    "label": 0
                },
                {
                    "sent": "In this case.",
                    "label": 0
                },
                {
                    "sent": "In that case, we just picked the most confident.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "Start from everything.",
                    "label": 0
                },
                {
                    "sent": "And for the building site.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, that's basically just the probability for.",
                    "label": 0
                },
                {
                    "sent": "Probability for each field fragments.",
                    "label": 0
                },
                {
                    "sent": "Probability.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Thank you, thank you.",
                    "label": 0
                }
            ]
        }
    }
}