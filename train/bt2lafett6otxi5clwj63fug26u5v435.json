{
    "id": "bt2lafett6otxi5clwj63fug26u5v435",
    "title": "Efficient AUC Optimization for classification",
    "info": {
        "author": [
            "Toon Calders, Eindhoven University of Technology"
        ],
        "published": "Jan. 30, 2008",
        "recorded": "September 2007",
        "category": [
            "Top->Computer Science->Optimization Methods",
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/ecml07_calders_eauc/",
    "segmentation": [
        [
            "The paper is sufficient AUC optimization for classification.",
            "We would like to sign.",
            "KDUD for sponsoring.",
            "Is this allowed?",
            "Small problem Windows check or cash not yet arrive.",
            "Thinking for increasing the little bit.",
            "1 million euro.",
            "Both of us.",
            "About as you can see, they are very young and quite similar.",
            "At the same time, if I remember Shimon in 98, yes.",
            "Tom in 99.",
            "She won from the Technical University of stretching.",
            "This is northwest part of the colon.",
            "And I found from University of answered they received.",
            "Easy degree in 2003.",
            "Receive this from the University of answered and Schumann from University of Massachusetts at Boston.",
            "And recently they are both assistant professors.",
            "Sherman is the pieces of fabric communication or dies because of better communication in Warsaw and found in University of kind comment.",
            "Of course they should have also some complementary features becauses dosage very good for you can ask them what are they?",
            "Here are the diplomas.",
            "I have congratulations again and I'd like to.",
            "Home.",
            "Very glad.",
            "Is the best paper allowed for you?",
            "And now the floors is yours please.",
            "So thank you very much for the introduction.",
            "Of course we are very happy to receive the award.",
            "Thank you.",
            "Yeah, so our talk is about optimizing AUC directly to get good classify."
        ],
        [
            "Tires.",
            "So let me just give a brief overview.",
            "Will start with an introduction thanks to Peter Flack, I will be able to make it shorter.",
            "Hope I got your name right.",
            "That's thanks to my quarter.",
            "And then we will basically talk about the main idea of the paper, which is approximating AUC using polynomials.",
            "And it turns out that it can be done quite efficiently.",
            "And then we will show how to build the linear classifier that optimizes AUC directly and does it efficiently.",
            "Then we show some experiments."
        ],
        [
            "And conclude."
        ],
        [
            "So first the introduction.",
            "Is classifier accuracy is usually assessed the classifier quality is assessed through accuracy?",
            "Most algorithms are.",
            "Well, at least tested in with accuracy in mind, if not Ness."
        ],
        [
            "This early designed so accuracy has some problems.",
            "Of course when classes are skewed.",
            "Also, a classifier is a function really giving a score and you need to fix a threshold somehow.",
            "And of course, how do you do it?",
            "Accuracy only looks for at the single thresholds.",
            "You may want to get."
        ],
        [
            "The bigger picture.",
            "So arosi curves are a better solution here.",
            "They characterize the precision recall tradeoff, and Beth are better able to deal with skewed distributions and so on."
        ],
        [
            "How to of course they are.",
            "It's a bit more difficult to compare two AUC curves.",
            "One way to do it to change the curve into a number is to compute the area under the curve cause you lose some information, but.",
            "I think Peter explained very well with this area."
        ],
        [
            "The curve means.",
            "Yes, so I will briefly show again how you construct the arosi curve.",
            "You sort the example based on what based on the score your classifier gives them.",
            "Yes.",
            "So based on this.",
            "F. And the then?"
        ],
        [
            "As you move."
        ],
        [
            "Along them you move."
        ],
        [
            "For a positive example, you move."
        ],
        [
            "Right foot"
        ],
        [
            "And they got the pigs."
        ],
        [
            "This way you."
        ],
        [
            "Construct your curve and then the area under this curve is.",
            "Is what we are looking for."
        ],
        [
            "So there are some statistical properties which again were mentioned already, like the area under the curve.",
            "The statistical interpretation is that an example from Class 0 has a smaller scored and an example from Class One yes, and in a data set you can compute it using this formula.",
            "This is basically a sum over all pairs of records from both classes and.",
            "You have this really means a number of pairs which are collect correctly.",
            "Ranked yes.",
            "Now this one.",
            "Here is a step function which is given below.",
            "So if X is less than zero it is 0, otherwise it is one.",
            "I mentioned this because we will see this function."
        ],
        [
            "Later.",
            "Yes, so how do you compute AUC?",
            "You don't really use this quadratic formula usually.",
            "What can be done is you can sort.",
            "Based on the score the classifier gives and then do this algorithm, I won't get into detail.",
            "It corresponds pretty much to constructing the curve which I showed before.",
            "And the.",
            "This is nice, and unless your data set gets too large and then sorting it is quite prohibitive."
        ],
        [
            "So now I will move to the main idea of our paper, which is how to approximate the."
        ],
        [
            "To see efficiently.",
            "So suppose we take this step function which we've seen earlier, and we approximate it using some polynomial.",
            "Yes, so that's a chart that shows what you really get here and what we do is, we substitute this polynomial expansion into this quadratic formula for AUC.",
            "So let's see."
        ],
        [
            "What happens when we do this?",
            "Yes, so this is the original."
        ],
        [
            "Formula.",
            "We do the substitution.",
            "Yes, now I will just apply the binomial theorem to this.",
            "To this."
        ],
        [
            "Stern we got this.",
            "Now what happens?",
            "We can react."
        ],
        [
            "Change this summation.",
            "Let's see how it works out.",
            "Yes, it's it's possible.",
            "This is just simply moving those those two guys.",
            "In here and separating them."
        ],
        [
            "Yes, so let's look a bit more at this expression.",
            "Now we can see that this sum each of these sums can basically be computed in linear time.",
            "The same can be said about this times and then the results of this summation can.",
            "Summations can simply be combined by multiplying them by some coefficients which can be precomputed.",
            "They don't depend on the data set.",
            "Yes, So what happens?",
            "We can come the total cost of this procedure is we need to complete.",
            "We need to give these the degree of the polynomial we need to collect.",
            "D statistics, which can be computed in over the degree of the polynomial times the size of the data set and then combine them.",
            "Enough time that squared with the degree, but so it's independent of the database.",
            "So in fact we get a procedure for approximating the UC which is linear in database size.",
            "We can even say more.",
            "It's not only linear, it requires only one data scan.",
            "So we could imagine using it even for data streams.",
            "We could just keep updating those counts and at each time we can very quickly get the approximate AUC value."
        ],
        [
            "OK, the first question that comes to mind is how accurate this is and this was the first question we asked ourselves of course.",
            "When we solve this expansion and let me show a small experiment just to.",
            "Hint on what we can expect.",
            "So suppose the that the score for the for Class 0 come from a normal distribution with mean zero and for class one they come from some normal distribution with mean M. By changing MI can affect the true value of area under the Roc curve."
        ],
        [
            "So this chart shows how the absolute error of the approximation depends on the exact value of AUC.",
            "Yes, so you can see that.",
            "It's always less than the error is always less than 1%, which is so very nice.",
            "And it is.",
            "It gets worse in the case where a area under curve is.",
            "Close to one which I would say is a less interesting case, because this is the case of perfect classification and we don't see that too often and you can see this error is in logarithmic scale.",
            "So in many places you can get 4 and 5 digits, even four or 5 digits of accuracy, which.",
            "I believe he's excellent."
        ],
        [
            "So to summarize, the takeaway message from all this is that exact IUC computation requires sorting, which can be slow for huge databases and our polynomial approximation is fully sufficient.",
            "It takes 1.",
            "Database scan and.",
            "Also, it's accurate in there or is usually less than 1%, and often we can get even for 5 digits."
        ],
        [
            "I mean, going back here, you may ask, well is it?",
            "Can I guarantee that it's always like this?",
            "Well, I cannot guarantee and that it is possible to break it if, say, if you choose very skewed distributions.",
            "There are gets worse.",
            "It can be like 10%, but as we'll see in real examples this doesn't happen too often, so it doesn't really happen in, at least in our application so.",
            "If you try, you can break it, but."
        ],
        [
            "Usually it works."
        ],
        [
            "So now let me move to the second.",
            "Issue which we address here is how to apply this approximation to efficiently build a linear classifier that optimizes AUC Dyer."
        ],
        [
            "So this is a linear classifier, yes?",
            "With weights W, just multiply weights with their attributes.",
            "Simple, we don't need the W zero term.",
            "We don't need the constant term here as it does not affect."
        ],
        [
            "The AUC.",
            "So now we have an optimization problem that find weights weight vector W that.",
            "Maximizes AUC."
        ],
        [
            "Only given data set right, we will use a gradient descent method."
        ],
        [
            "Some version of it.",
            "So the way we will do this is shown in this picture.",
            "We start with some ways, zero.",
            "We find we when we approximate the gradient G at this point and then we move along this gradient.",
            "Do a linear optimization along this gradient to find the best point.",
            "Then we find the next gradient and so on and so on so we hopefully converge on that."
        ],
        [
            "The solution.",
            "So I will just briefly say how we compute the gradient.",
            "This is not.",
            "This is the form."
        ],
        [
            "For the gradient, you just need to.",
            "In fact computer few partial derivatives over a polynomial.",
            "And the."
        ],
        [
            "I would just show you the formula.",
            "It's not.",
            "It's not difficult to derive, it's just just what comes out and again you have a bunch of sums here which can be computed in one database.",
            "Come and then you need to combine them.",
            "So.",
            "The."
        ],
        [
            "This can be done easily.",
            "I won't get into much detail on that."
        ],
        [
            "Now the more important part is updating the weights along the gradient.",
            "So we need to find some number Lambda that.",
            "That gives us the highest area under curve."
        ],
        [
            "Along the gradient.",
            "So we want to find Lambda which optimizes this approximation."
        ],
        [
            "Listen, let's see how this can be worked out efficiently.",
            "So this is the."
        ],
        [
            "Formula repeated.",
            "And let's look at just one part of it, this one, some the other one can be handled in just."
        ],
        [
            "The same way.",
            "So first of all, notice that the function is linear, so we can decompose it in such a way.",
            "It's this F of this intermediate weights is the same as.",
            "The original classifier plus Lambda times the the classifier with weights."
        ],
        [
            "Equal to the gradient.",
            "So now we can plug this into the the sum and again use them."
        ],
        [
            "Binomial theorem and we."
        ],
        [
            "It's something like this, and again we use the same trick of moving summations.",
            "And what comes out is.",
            "Is this?",
            "Right, so again, notice that this whole some above can be computed.",
            "We can compute the bunch of statistiques.",
            "In one scan of the database again and combine them.",
            "I do get the.",
            "Those values, so I will summarize this in a while, but we can.",
            "Again, collect a bunch of statistics in one scan of the data set and then get the whole linear optimization for free for free.",
            "I mean not looking at the database.",
            "Is notice that those this expression here does not depend on Lambda Lambda is hidden in those."
        ],
        [
            "Christians here.",
            "So again, the takeaway messages we collect.",
            "A number of values in one scan over the data.",
            "These are those values.",
            "And then we can maximize along the gradient without accessing the data at all.",
            "So putting it all together."
        ],
        [
            "This is our gradient descent algorithm.",
            "Gray"
        ],
        [
            "Radium can be computed in one scan tool.",
            "It also depends on the number of attributes.",
            "This is omitted here."
        ],
        [
            "And then the whole linear optimization can also be learning once can."
        ],
        [
            "Yes.",
            "We get the total complexities.",
            "Again, this number of attributes."
        ],
        [
            "Is missing here.",
            "So let's now look at how."
        ],
        [
            "This works in experiments.",
            "First I will compare it with the.",
            "In SVM, which maximizes AUC directly, this is SVM Earth with the.",
            "It's able to optimize various accuracy, measure various quality measures, so also area under the curve.",
            "So as we can see, our approximation works.",
            "So just as good or better in most cases, and of course the VM depends on the C parameters, so we took a few values just to make sure we don't get some we don't pick wrong values, say Internet problems because of that.",
            "And the well, the only.",
            "The only case where it did worse was the forest data set where there was a lot of nominal attribute 01 attributes and approximation was slightly worse in this case, but for numerical attribute it works very well."
        ],
        [
            "So let alone now let us see at the time, so here.",
            "We are usually.",
            "Much faster, I mean The only exception is the small sonar data set.",
            "So I will ignore that and you can see that here we are.",
            "This is again, the time is in logarithmic scale, so this is much.",
            "Much faster.",
            "Tends to one or two degrees of magnitude faster."
        ],
        [
            "Now, of course, one may ask a question.",
            "Well, what if we just?",
            "Don't use this polynomial approximation and instead of just sample, take a small sample from the database and minimize the UC directly on this sample using this exact formula.",
            "I mean, this is perfectly good option, so we compared it and actually turned out that.",
            "Our our method seems to work better.",
            "I mean sampling eventually.",
            "Of course we approach our accuracy, but as you can see that as the sample size grows, the accuracy does not improve that much.",
            "In fact to the the the increasing sample size is quadratic in the improvement or accurate of accuracy so.",
            "Eventually sampling will fail mean I.",
            "This case sampling did actually much worse I.",
            "Not completely sure why why this happens.",
            "There is maybe a problem that when you sample, the AUC is not a smooth function, so the optimization may breakdown."
        ],
        [
            "At some point.",
            "Our polynomial approximations have the advantage that they are smooth also, and as you can see, the sampling time initially.",
            "Of course the sampling is faster for small samples and then the accuracy is not that good.",
            "And then basically becomes very slow."
        ],
        [
            "Now let me move to another experiment because there is a.",
            "Of course there is a question, does it make sense to optimize AUC at all?",
            "It was also mentioned by Peter before that many algorithms tend to produce.",
            "Good day, you see, just.",
            "Just even though they are designed for accuracy and.",
            "We did some initial experiments and on this full datasets actually linear discriminant analysis gave pretty much.",
            "Very comparably, you see to our methods, so we looked in the literature a bit and.",
            "Now we found that there are claims that for skewed class distribution this is actually not true anymore.",
            "So we did an experiment with the KDD Cup physics data set from 2004 and we.",
            "Artificially skewed the classes.",
            "I mean, we sampled some percentage of the class and as you can see for imbalanced classes.",
            "This is the.",
            "Here we have the first imbalance and here we have the AUC on the test set.",
            "So you can see that for.",
            "For imbalanced classes, there is a difference and it can be quite significant even when the imbalance is.",
            "Say quite reasonable 5%.",
            "I mean you see that quite often that one class is just."
        ],
        [
            "Percent of another."
        ],
        [
            "So let me conclude.",
            "We presented an efficient approximation for the AUC.",
            "And the which also allows us to adapt the gradient decent methods to directly optimize linearly and AUC maximizing your classifier.",
            "Empirical results.",
            "Let's say our promise."
        ],
        [
            "Signing pretty good.",
            "Some future work, of course there is a.",
            "A lot of possibilities.",
            "First, we could extend it to other measures in the paper we propose something which we call soft AUC, which smoothies this step function and actually turns out that this method works even better.",
            "Deep sorting wouldn't help here.",
            "The next big step is of course extension to nonlinear classifiers.",
            "We are thinking about it.",
            "And of course, there may be a question.",
            "I could those polynomials approximate approximation be used in some other applications?",
            "There is a paper on Scion VM on.",
            "More efficient car fitting.",
            "And maybe it could be useful in streamline."
        ],
        [
            "Thank you for your attentions.",
            "Question.",
            "Time for questions.",
            "May I have it?",
            "Please question, thank you very much for this presentation.",
            "I notice how you deal with the discontinuity of the unit function because you are approximating discontinuous function by continuous analytical affect.",
            "Well, I'm I'm not of course this discontinuity cannot be approximated perfectly but.",
            "Sing, I have a slight let me move back to this slide which shows the.",
            "It shows the."
        ],
        [
            "Approximation here it means of course you're not.",
            "You're not approximating the discontinuity, but you're getting close to it, and this turns out to be enough.",
            "In this case.",
            "This is actually a bit related to my question and I was wondering what is the sensitivity of if you're approved to the degree of the polynomial you're using you have you.",
            "Have you observed you know different quality of your approximation?",
            "It definitely improves, improves the accuracy if you increase the degree, but my conjecture is that the sort of key part here is that there is an error cancellation happening because if you see some errors are positive and some are negative and also notice that there is some symmetry so.",
            "And there's pretty much the same amount of positive and negative error, so I believe that the accuracy really comes from this cancellation.",
            "And if you increase the degree, of course this helps but does not help that match.",
            "And if you'd really increase it very high, then you may.",
            "You will get numerical problems.",
            "To where did you start?",
            "Reasonable degree to try.",
            "I used degree 30.",
            "As you can see here.",
            "Yes, so I find this very elegant and interesting, but I'm wondering about its practical importance because essentially what you're?",
            "What you're doing is having a constant factor of 30.",
            "Instead of having a factor of N log N when N is the number of training examples.",
            "So that is not the size of an is not the size of the training set, is just the number of rows in the training set.",
            "So a constant factor of 30, you don't be a win compared to like an, IT login is bigger than 30 and two to the 30 is bigger than any reasonable number of examples in a training set that.",
            "We use nowadays, even in large commercial applications, so that's one question I have.",
            "Weather is very important in practice to eliminate this log in factor when login is less than 30.",
            "And in your experiment you compared against possible vector machine with a linear kernel and there are several methods are being published a specialized in support vector machines with linear kernels that are much faster than general purpose support vector machine algorithms and.",
            "General purpose SVM algorithms have a more or less quadratic behavior in a number of examples.",
            "So general clubs SVM really don't scale well to a lot number of examples for reasons that are independent of the AUC object."
        ],
        [
            "SVM with linear kernel, especially similar to logistic regression with a slightly different objective function, and they can be trained almost as fast as logistic regression classifiers.",
            "OK, so this is not.",
            "I wouldn't say this is a standard SVM, it is.",
            "It says we imperf it.",
            "Actually if you choose the standard SVM optimization function then it really.",
            "It has quite good performance, but when and it also I believe is a pretty good Optima.",
            "It's designed especially for linear machines, so this is this season's VM optimized for linear Turner.",
            "And the only change is that it actually uses.",
            "Uses AUC as the target function.",
            "I mean it's you can find the description in the implementation is available.",
            "It's an implementation by Thurston.",
            "You are Kings and he had several papers on that so.",
            "Say yeah about about this and about comparison of those constants.",
            "I mean, this is a very tricky issue.",
            "That's where I believe this big O notation to some extent doesn't help because.",
            "OK, you can say there is a factor of log size the data set, but then you're moving data around and you're moving values around so you also have like.",
            "Cash performance issues and things like that here you just go over the data and I I did compare with sampling.",
            "Here I use sorting with the in memory quicksort.",
            "I mean that should be pretty efficient and still it's slower.",
            "You don't need to.",
            "You just need to sort the score.",
            "Yes, of course.",
            "Yes yes yes, of course that's that, of course makes it easier, yes, but still I would say in this case at least it was was better.",
            "Erica Steiner work.",
            "Off because my car has presented a eyes as hundreds of years ago, they also presented the an approximation of this figure in the number of data points, and they also accept from the user a accuracy as we can.",
            "Approximation accuracy can specify an algorithm is currently switched at least accuracy, so they use the multipole expansion.",
            "Therefore, from physics pricing, so I wonder if you compare to your approach.",
            "I sorry I didn't hear the authors names, I believe.",
            "She's going to come rescue Maryland if not sure, but I think the user nonlinear classifier, right?",
            "There, there, there there are some.",
            "I.",
            "Are you suggesting regression?",
            "Closest."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The paper is sufficient AUC optimization for classification.",
                    "label": 1
                },
                {
                    "sent": "We would like to sign.",
                    "label": 0
                },
                {
                    "sent": "KDUD for sponsoring.",
                    "label": 0
                },
                {
                    "sent": "Is this allowed?",
                    "label": 0
                },
                {
                    "sent": "Small problem Windows check or cash not yet arrive.",
                    "label": 0
                },
                {
                    "sent": "Thinking for increasing the little bit.",
                    "label": 0
                },
                {
                    "sent": "1 million euro.",
                    "label": 0
                },
                {
                    "sent": "Both of us.",
                    "label": 0
                },
                {
                    "sent": "About as you can see, they are very young and quite similar.",
                    "label": 0
                },
                {
                    "sent": "At the same time, if I remember Shimon in 98, yes.",
                    "label": 0
                },
                {
                    "sent": "Tom in 99.",
                    "label": 0
                },
                {
                    "sent": "She won from the Technical University of stretching.",
                    "label": 0
                },
                {
                    "sent": "This is northwest part of the colon.",
                    "label": 0
                },
                {
                    "sent": "And I found from University of answered they received.",
                    "label": 0
                },
                {
                    "sent": "Easy degree in 2003.",
                    "label": 0
                },
                {
                    "sent": "Receive this from the University of answered and Schumann from University of Massachusetts at Boston.",
                    "label": 0
                },
                {
                    "sent": "And recently they are both assistant professors.",
                    "label": 0
                },
                {
                    "sent": "Sherman is the pieces of fabric communication or dies because of better communication in Warsaw and found in University of kind comment.",
                    "label": 0
                },
                {
                    "sent": "Of course they should have also some complementary features becauses dosage very good for you can ask them what are they?",
                    "label": 0
                },
                {
                    "sent": "Here are the diplomas.",
                    "label": 0
                },
                {
                    "sent": "I have congratulations again and I'd like to.",
                    "label": 0
                },
                {
                    "sent": "Home.",
                    "label": 0
                },
                {
                    "sent": "Very glad.",
                    "label": 0
                },
                {
                    "sent": "Is the best paper allowed for you?",
                    "label": 0
                },
                {
                    "sent": "And now the floors is yours please.",
                    "label": 0
                },
                {
                    "sent": "So thank you very much for the introduction.",
                    "label": 0
                },
                {
                    "sent": "Of course we are very happy to receive the award.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so our talk is about optimizing AUC directly to get good classify.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tires.",
                    "label": 0
                },
                {
                    "sent": "So let me just give a brief overview.",
                    "label": 0
                },
                {
                    "sent": "Will start with an introduction thanks to Peter Flack, I will be able to make it shorter.",
                    "label": 0
                },
                {
                    "sent": "Hope I got your name right.",
                    "label": 0
                },
                {
                    "sent": "That's thanks to my quarter.",
                    "label": 0
                },
                {
                    "sent": "And then we will basically talk about the main idea of the paper, which is approximating AUC using polynomials.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that it can be done quite efficiently.",
                    "label": 0
                },
                {
                    "sent": "And then we will show how to build the linear classifier that optimizes AUC directly and does it efficiently.",
                    "label": 0
                },
                {
                    "sent": "Then we show some experiments.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And conclude.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first the introduction.",
                    "label": 0
                },
                {
                    "sent": "Is classifier accuracy is usually assessed the classifier quality is assessed through accuracy?",
                    "label": 0
                },
                {
                    "sent": "Most algorithms are.",
                    "label": 0
                },
                {
                    "sent": "Well, at least tested in with accuracy in mind, if not Ness.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This early designed so accuracy has some problems.",
                    "label": 0
                },
                {
                    "sent": "Of course when classes are skewed.",
                    "label": 1
                },
                {
                    "sent": "Also, a classifier is a function really giving a score and you need to fix a threshold somehow.",
                    "label": 1
                },
                {
                    "sent": "And of course, how do you do it?",
                    "label": 0
                },
                {
                    "sent": "Accuracy only looks for at the single thresholds.",
                    "label": 0
                },
                {
                    "sent": "You may want to get.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The bigger picture.",
                    "label": 0
                },
                {
                    "sent": "So arosi curves are a better solution here.",
                    "label": 0
                },
                {
                    "sent": "They characterize the precision recall tradeoff, and Beth are better able to deal with skewed distributions and so on.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How to of course they are.",
                    "label": 1
                },
                {
                    "sent": "It's a bit more difficult to compare two AUC curves.",
                    "label": 1
                },
                {
                    "sent": "One way to do it to change the curve into a number is to compute the area under the curve cause you lose some information, but.",
                    "label": 0
                },
                {
                    "sent": "I think Peter explained very well with this area.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The curve means.",
                    "label": 0
                },
                {
                    "sent": "Yes, so I will briefly show again how you construct the arosi curve.",
                    "label": 0
                },
                {
                    "sent": "You sort the example based on what based on the score your classifier gives them.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So based on this.",
                    "label": 0
                },
                {
                    "sent": "F. And the then?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As you move.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Along them you move.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For a positive example, you move.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right foot",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And they got the pigs.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This way you.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Construct your curve and then the area under this curve is.",
                    "label": 0
                },
                {
                    "sent": "Is what we are looking for.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there are some statistical properties which again were mentioned already, like the area under the curve.",
                    "label": 1
                },
                {
                    "sent": "The statistical interpretation is that an example from Class 0 has a smaller scored and an example from Class One yes, and in a data set you can compute it using this formula.",
                    "label": 0
                },
                {
                    "sent": "This is basically a sum over all pairs of records from both classes and.",
                    "label": 0
                },
                {
                    "sent": "You have this really means a number of pairs which are collect correctly.",
                    "label": 0
                },
                {
                    "sent": "Ranked yes.",
                    "label": 1
                },
                {
                    "sent": "Now this one.",
                    "label": 1
                },
                {
                    "sent": "Here is a step function which is given below.",
                    "label": 0
                },
                {
                    "sent": "So if X is less than zero it is 0, otherwise it is one.",
                    "label": 0
                },
                {
                    "sent": "I mentioned this because we will see this function.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Later.",
                    "label": 0
                },
                {
                    "sent": "Yes, so how do you compute AUC?",
                    "label": 0
                },
                {
                    "sent": "You don't really use this quadratic formula usually.",
                    "label": 0
                },
                {
                    "sent": "What can be done is you can sort.",
                    "label": 0
                },
                {
                    "sent": "Based on the score the classifier gives and then do this algorithm, I won't get into detail.",
                    "label": 0
                },
                {
                    "sent": "It corresponds pretty much to constructing the curve which I showed before.",
                    "label": 0
                },
                {
                    "sent": "And the.",
                    "label": 0
                },
                {
                    "sent": "This is nice, and unless your data set gets too large and then sorting it is quite prohibitive.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now I will move to the main idea of our paper, which is how to approximate the.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To see efficiently.",
                    "label": 0
                },
                {
                    "sent": "So suppose we take this step function which we've seen earlier, and we approximate it using some polynomial.",
                    "label": 0
                },
                {
                    "sent": "Yes, so that's a chart that shows what you really get here and what we do is, we substitute this polynomial expansion into this quadratic formula for AUC.",
                    "label": 0
                },
                {
                    "sent": "So let's see.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What happens when we do this?",
                    "label": 0
                },
                {
                    "sent": "Yes, so this is the original.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Formula.",
                    "label": 0
                },
                {
                    "sent": "We do the substitution.",
                    "label": 0
                },
                {
                    "sent": "Yes, now I will just apply the binomial theorem to this.",
                    "label": 0
                },
                {
                    "sent": "To this.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stern we got this.",
                    "label": 0
                },
                {
                    "sent": "Now what happens?",
                    "label": 0
                },
                {
                    "sent": "We can react.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Change this summation.",
                    "label": 0
                },
                {
                    "sent": "Let's see how it works out.",
                    "label": 0
                },
                {
                    "sent": "Yes, it's it's possible.",
                    "label": 0
                },
                {
                    "sent": "This is just simply moving those those two guys.",
                    "label": 0
                },
                {
                    "sent": "In here and separating them.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes, so let's look a bit more at this expression.",
                    "label": 0
                },
                {
                    "sent": "Now we can see that this sum each of these sums can basically be computed in linear time.",
                    "label": 0
                },
                {
                    "sent": "The same can be said about this times and then the results of this summation can.",
                    "label": 0
                },
                {
                    "sent": "Summations can simply be combined by multiplying them by some coefficients which can be precomputed.",
                    "label": 0
                },
                {
                    "sent": "They don't depend on the data set.",
                    "label": 0
                },
                {
                    "sent": "Yes, So what happens?",
                    "label": 0
                },
                {
                    "sent": "We can come the total cost of this procedure is we need to complete.",
                    "label": 0
                },
                {
                    "sent": "We need to give these the degree of the polynomial we need to collect.",
                    "label": 0
                },
                {
                    "sent": "D statistics, which can be computed in over the degree of the polynomial times the size of the data set and then combine them.",
                    "label": 0
                },
                {
                    "sent": "Enough time that squared with the degree, but so it's independent of the database.",
                    "label": 0
                },
                {
                    "sent": "So in fact we get a procedure for approximating the UC which is linear in database size.",
                    "label": 1
                },
                {
                    "sent": "We can even say more.",
                    "label": 1
                },
                {
                    "sent": "It's not only linear, it requires only one data scan.",
                    "label": 0
                },
                {
                    "sent": "So we could imagine using it even for data streams.",
                    "label": 0
                },
                {
                    "sent": "We could just keep updating those counts and at each time we can very quickly get the approximate AUC value.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, the first question that comes to mind is how accurate this is and this was the first question we asked ourselves of course.",
                    "label": 0
                },
                {
                    "sent": "When we solve this expansion and let me show a small experiment just to.",
                    "label": 0
                },
                {
                    "sent": "Hint on what we can expect.",
                    "label": 0
                },
                {
                    "sent": "So suppose the that the score for the for Class 0 come from a normal distribution with mean zero and for class one they come from some normal distribution with mean M. By changing MI can affect the true value of area under the Roc curve.",
                    "label": 1
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this chart shows how the absolute error of the approximation depends on the exact value of AUC.",
                    "label": 0
                },
                {
                    "sent": "Yes, so you can see that.",
                    "label": 0
                },
                {
                    "sent": "It's always less than the error is always less than 1%, which is so very nice.",
                    "label": 0
                },
                {
                    "sent": "And it is.",
                    "label": 0
                },
                {
                    "sent": "It gets worse in the case where a area under curve is.",
                    "label": 0
                },
                {
                    "sent": "Close to one which I would say is a less interesting case, because this is the case of perfect classification and we don't see that too often and you can see this error is in logarithmic scale.",
                    "label": 0
                },
                {
                    "sent": "So in many places you can get 4 and 5 digits, even four or 5 digits of accuracy, which.",
                    "label": 0
                },
                {
                    "sent": "I believe he's excellent.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to summarize, the takeaway message from all this is that exact IUC computation requires sorting, which can be slow for huge databases and our polynomial approximation is fully sufficient.",
                    "label": 1
                },
                {
                    "sent": "It takes 1.",
                    "label": 0
                },
                {
                    "sent": "Database scan and.",
                    "label": 1
                },
                {
                    "sent": "Also, it's accurate in there or is usually less than 1%, and often we can get even for 5 digits.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I mean, going back here, you may ask, well is it?",
                    "label": 0
                },
                {
                    "sent": "Can I guarantee that it's always like this?",
                    "label": 0
                },
                {
                    "sent": "Well, I cannot guarantee and that it is possible to break it if, say, if you choose very skewed distributions.",
                    "label": 0
                },
                {
                    "sent": "There are gets worse.",
                    "label": 0
                },
                {
                    "sent": "It can be like 10%, but as we'll see in real examples this doesn't happen too often, so it doesn't really happen in, at least in our application so.",
                    "label": 0
                },
                {
                    "sent": "If you try, you can break it, but.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Usually it works.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now let me move to the second.",
                    "label": 0
                },
                {
                    "sent": "Issue which we address here is how to apply this approximation to efficiently build a linear classifier that optimizes AUC Dyer.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is a linear classifier, yes?",
                    "label": 1
                },
                {
                    "sent": "With weights W, just multiply weights with their attributes.",
                    "label": 0
                },
                {
                    "sent": "Simple, we don't need the W zero term.",
                    "label": 0
                },
                {
                    "sent": "We don't need the constant term here as it does not affect.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The AUC.",
                    "label": 0
                },
                {
                    "sent": "So now we have an optimization problem that find weights weight vector W that.",
                    "label": 1
                },
                {
                    "sent": "Maximizes AUC.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Only given data set right, we will use a gradient descent method.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some version of it.",
                    "label": 0
                },
                {
                    "sent": "So the way we will do this is shown in this picture.",
                    "label": 0
                },
                {
                    "sent": "We start with some ways, zero.",
                    "label": 0
                },
                {
                    "sent": "We find we when we approximate the gradient G at this point and then we move along this gradient.",
                    "label": 1
                },
                {
                    "sent": "Do a linear optimization along this gradient to find the best point.",
                    "label": 1
                },
                {
                    "sent": "Then we find the next gradient and so on and so on so we hopefully converge on that.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The solution.",
                    "label": 0
                },
                {
                    "sent": "So I will just briefly say how we compute the gradient.",
                    "label": 1
                },
                {
                    "sent": "This is not.",
                    "label": 0
                },
                {
                    "sent": "This is the form.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For the gradient, you just need to.",
                    "label": 1
                },
                {
                    "sent": "In fact computer few partial derivatives over a polynomial.",
                    "label": 0
                },
                {
                    "sent": "And the.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I would just show you the formula.",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "It's not difficult to derive, it's just just what comes out and again you have a bunch of sums here which can be computed in one database.",
                    "label": 0
                },
                {
                    "sent": "Come and then you need to combine them.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This can be done easily.",
                    "label": 0
                },
                {
                    "sent": "I won't get into much detail on that.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the more important part is updating the weights along the gradient.",
                    "label": 1
                },
                {
                    "sent": "So we need to find some number Lambda that.",
                    "label": 0
                },
                {
                    "sent": "That gives us the highest area under curve.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Along the gradient.",
                    "label": 0
                },
                {
                    "sent": "So we want to find Lambda which optimizes this approximation.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Listen, let's see how this can be worked out efficiently.",
                    "label": 0
                },
                {
                    "sent": "So this is the.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Formula repeated.",
                    "label": 0
                },
                {
                    "sent": "And let's look at just one part of it, this one, some the other one can be handled in just.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The same way.",
                    "label": 0
                },
                {
                    "sent": "So first of all, notice that the function is linear, so we can decompose it in such a way.",
                    "label": 0
                },
                {
                    "sent": "It's this F of this intermediate weights is the same as.",
                    "label": 0
                },
                {
                    "sent": "The original classifier plus Lambda times the the classifier with weights.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Equal to the gradient.",
                    "label": 0
                },
                {
                    "sent": "So now we can plug this into the the sum and again use them.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Binomial theorem and we.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's something like this, and again we use the same trick of moving summations.",
                    "label": 0
                },
                {
                    "sent": "And what comes out is.",
                    "label": 0
                },
                {
                    "sent": "Is this?",
                    "label": 0
                },
                {
                    "sent": "Right, so again, notice that this whole some above can be computed.",
                    "label": 0
                },
                {
                    "sent": "We can compute the bunch of statistiques.",
                    "label": 0
                },
                {
                    "sent": "In one scan of the database again and combine them.",
                    "label": 0
                },
                {
                    "sent": "I do get the.",
                    "label": 0
                },
                {
                    "sent": "Those values, so I will summarize this in a while, but we can.",
                    "label": 0
                },
                {
                    "sent": "Again, collect a bunch of statistics in one scan of the data set and then get the whole linear optimization for free for free.",
                    "label": 0
                },
                {
                    "sent": "I mean not looking at the database.",
                    "label": 0
                },
                {
                    "sent": "Is notice that those this expression here does not depend on Lambda Lambda is hidden in those.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Christians here.",
                    "label": 0
                },
                {
                    "sent": "So again, the takeaway messages we collect.",
                    "label": 0
                },
                {
                    "sent": "A number of values in one scan over the data.",
                    "label": 1
                },
                {
                    "sent": "These are those values.",
                    "label": 1
                },
                {
                    "sent": "And then we can maximize along the gradient without accessing the data at all.",
                    "label": 0
                },
                {
                    "sent": "So putting it all together.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is our gradient descent algorithm.",
                    "label": 0
                },
                {
                    "sent": "Gray",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Radium can be computed in one scan tool.",
                    "label": 0
                },
                {
                    "sent": "It also depends on the number of attributes.",
                    "label": 0
                },
                {
                    "sent": "This is omitted here.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then the whole linear optimization can also be learning once can.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "We get the total complexities.",
                    "label": 0
                },
                {
                    "sent": "Again, this number of attributes.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is missing here.",
                    "label": 0
                },
                {
                    "sent": "So let's now look at how.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This works in experiments.",
                    "label": 0
                },
                {
                    "sent": "First I will compare it with the.",
                    "label": 0
                },
                {
                    "sent": "In SVM, which maximizes AUC directly, this is SVM Earth with the.",
                    "label": 0
                },
                {
                    "sent": "It's able to optimize various accuracy, measure various quality measures, so also area under the curve.",
                    "label": 0
                },
                {
                    "sent": "So as we can see, our approximation works.",
                    "label": 0
                },
                {
                    "sent": "So just as good or better in most cases, and of course the VM depends on the C parameters, so we took a few values just to make sure we don't get some we don't pick wrong values, say Internet problems because of that.",
                    "label": 0
                },
                {
                    "sent": "And the well, the only.",
                    "label": 0
                },
                {
                    "sent": "The only case where it did worse was the forest data set where there was a lot of nominal attribute 01 attributes and approximation was slightly worse in this case, but for numerical attribute it works very well.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let alone now let us see at the time, so here.",
                    "label": 0
                },
                {
                    "sent": "We are usually.",
                    "label": 0
                },
                {
                    "sent": "Much faster, I mean The only exception is the small sonar data set.",
                    "label": 0
                },
                {
                    "sent": "So I will ignore that and you can see that here we are.",
                    "label": 0
                },
                {
                    "sent": "This is again, the time is in logarithmic scale, so this is much.",
                    "label": 0
                },
                {
                    "sent": "Much faster.",
                    "label": 0
                },
                {
                    "sent": "Tends to one or two degrees of magnitude faster.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, of course, one may ask a question.",
                    "label": 0
                },
                {
                    "sent": "Well, what if we just?",
                    "label": 0
                },
                {
                    "sent": "Don't use this polynomial approximation and instead of just sample, take a small sample from the database and minimize the UC directly on this sample using this exact formula.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is perfectly good option, so we compared it and actually turned out that.",
                    "label": 0
                },
                {
                    "sent": "Our our method seems to work better.",
                    "label": 0
                },
                {
                    "sent": "I mean sampling eventually.",
                    "label": 0
                },
                {
                    "sent": "Of course we approach our accuracy, but as you can see that as the sample size grows, the accuracy does not improve that much.",
                    "label": 0
                },
                {
                    "sent": "In fact to the the the increasing sample size is quadratic in the improvement or accurate of accuracy so.",
                    "label": 0
                },
                {
                    "sent": "Eventually sampling will fail mean I.",
                    "label": 0
                },
                {
                    "sent": "This case sampling did actually much worse I.",
                    "label": 0
                },
                {
                    "sent": "Not completely sure why why this happens.",
                    "label": 0
                },
                {
                    "sent": "There is maybe a problem that when you sample, the AUC is not a smooth function, so the optimization may breakdown.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At some point.",
                    "label": 0
                },
                {
                    "sent": "Our polynomial approximations have the advantage that they are smooth also, and as you can see, the sampling time initially.",
                    "label": 0
                },
                {
                    "sent": "Of course the sampling is faster for small samples and then the accuracy is not that good.",
                    "label": 0
                },
                {
                    "sent": "And then basically becomes very slow.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let me move to another experiment because there is a.",
                    "label": 0
                },
                {
                    "sent": "Of course there is a question, does it make sense to optimize AUC at all?",
                    "label": 0
                },
                {
                    "sent": "It was also mentioned by Peter before that many algorithms tend to produce.",
                    "label": 0
                },
                {
                    "sent": "Good day, you see, just.",
                    "label": 0
                },
                {
                    "sent": "Just even though they are designed for accuracy and.",
                    "label": 0
                },
                {
                    "sent": "We did some initial experiments and on this full datasets actually linear discriminant analysis gave pretty much.",
                    "label": 0
                },
                {
                    "sent": "Very comparably, you see to our methods, so we looked in the literature a bit and.",
                    "label": 0
                },
                {
                    "sent": "Now we found that there are claims that for skewed class distribution this is actually not true anymore.",
                    "label": 0
                },
                {
                    "sent": "So we did an experiment with the KDD Cup physics data set from 2004 and we.",
                    "label": 1
                },
                {
                    "sent": "Artificially skewed the classes.",
                    "label": 0
                },
                {
                    "sent": "I mean, we sampled some percentage of the class and as you can see for imbalanced classes.",
                    "label": 0
                },
                {
                    "sent": "This is the.",
                    "label": 0
                },
                {
                    "sent": "Here we have the first imbalance and here we have the AUC on the test set.",
                    "label": 0
                },
                {
                    "sent": "So you can see that for.",
                    "label": 0
                },
                {
                    "sent": "For imbalanced classes, there is a difference and it can be quite significant even when the imbalance is.",
                    "label": 0
                },
                {
                    "sent": "Say quite reasonable 5%.",
                    "label": 0
                },
                {
                    "sent": "I mean you see that quite often that one class is just.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Percent of another.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me conclude.",
                    "label": 0
                },
                {
                    "sent": "We presented an efficient approximation for the AUC.",
                    "label": 1
                },
                {
                    "sent": "And the which also allows us to adapt the gradient decent methods to directly optimize linearly and AUC maximizing your classifier.",
                    "label": 0
                },
                {
                    "sent": "Empirical results.",
                    "label": 0
                },
                {
                    "sent": "Let's say our promise.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Signing pretty good.",
                    "label": 0
                },
                {
                    "sent": "Some future work, of course there is a.",
                    "label": 0
                },
                {
                    "sent": "A lot of possibilities.",
                    "label": 0
                },
                {
                    "sent": "First, we could extend it to other measures in the paper we propose something which we call soft AUC, which smoothies this step function and actually turns out that this method works even better.",
                    "label": 1
                },
                {
                    "sent": "Deep sorting wouldn't help here.",
                    "label": 1
                },
                {
                    "sent": "The next big step is of course extension to nonlinear classifiers.",
                    "label": 0
                },
                {
                    "sent": "We are thinking about it.",
                    "label": 1
                },
                {
                    "sent": "And of course, there may be a question.",
                    "label": 0
                },
                {
                    "sent": "I could those polynomials approximate approximation be used in some other applications?",
                    "label": 0
                },
                {
                    "sent": "There is a paper on Scion VM on.",
                    "label": 0
                },
                {
                    "sent": "More efficient car fitting.",
                    "label": 0
                },
                {
                    "sent": "And maybe it could be useful in streamline.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you for your attentions.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Time for questions.",
                    "label": 0
                },
                {
                    "sent": "May I have it?",
                    "label": 0
                },
                {
                    "sent": "Please question, thank you very much for this presentation.",
                    "label": 0
                },
                {
                    "sent": "I notice how you deal with the discontinuity of the unit function because you are approximating discontinuous function by continuous analytical affect.",
                    "label": 0
                },
                {
                    "sent": "Well, I'm I'm not of course this discontinuity cannot be approximated perfectly but.",
                    "label": 0
                },
                {
                    "sent": "Sing, I have a slight let me move back to this slide which shows the.",
                    "label": 0
                },
                {
                    "sent": "It shows the.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Approximation here it means of course you're not.",
                    "label": 0
                },
                {
                    "sent": "You're not approximating the discontinuity, but you're getting close to it, and this turns out to be enough.",
                    "label": 0
                },
                {
                    "sent": "In this case.",
                    "label": 0
                },
                {
                    "sent": "This is actually a bit related to my question and I was wondering what is the sensitivity of if you're approved to the degree of the polynomial you're using you have you.",
                    "label": 0
                },
                {
                    "sent": "Have you observed you know different quality of your approximation?",
                    "label": 0
                },
                {
                    "sent": "It definitely improves, improves the accuracy if you increase the degree, but my conjecture is that the sort of key part here is that there is an error cancellation happening because if you see some errors are positive and some are negative and also notice that there is some symmetry so.",
                    "label": 0
                },
                {
                    "sent": "And there's pretty much the same amount of positive and negative error, so I believe that the accuracy really comes from this cancellation.",
                    "label": 0
                },
                {
                    "sent": "And if you increase the degree, of course this helps but does not help that match.",
                    "label": 0
                },
                {
                    "sent": "And if you'd really increase it very high, then you may.",
                    "label": 0
                },
                {
                    "sent": "You will get numerical problems.",
                    "label": 0
                },
                {
                    "sent": "To where did you start?",
                    "label": 0
                },
                {
                    "sent": "Reasonable degree to try.",
                    "label": 0
                },
                {
                    "sent": "I used degree 30.",
                    "label": 0
                },
                {
                    "sent": "As you can see here.",
                    "label": 0
                },
                {
                    "sent": "Yes, so I find this very elegant and interesting, but I'm wondering about its practical importance because essentially what you're?",
                    "label": 0
                },
                {
                    "sent": "What you're doing is having a constant factor of 30.",
                    "label": 0
                },
                {
                    "sent": "Instead of having a factor of N log N when N is the number of training examples.",
                    "label": 0
                },
                {
                    "sent": "So that is not the size of an is not the size of the training set, is just the number of rows in the training set.",
                    "label": 0
                },
                {
                    "sent": "So a constant factor of 30, you don't be a win compared to like an, IT login is bigger than 30 and two to the 30 is bigger than any reasonable number of examples in a training set that.",
                    "label": 0
                },
                {
                    "sent": "We use nowadays, even in large commercial applications, so that's one question I have.",
                    "label": 0
                },
                {
                    "sent": "Weather is very important in practice to eliminate this log in factor when login is less than 30.",
                    "label": 0
                },
                {
                    "sent": "And in your experiment you compared against possible vector machine with a linear kernel and there are several methods are being published a specialized in support vector machines with linear kernels that are much faster than general purpose support vector machine algorithms and.",
                    "label": 0
                },
                {
                    "sent": "General purpose SVM algorithms have a more or less quadratic behavior in a number of examples.",
                    "label": 0
                },
                {
                    "sent": "So general clubs SVM really don't scale well to a lot number of examples for reasons that are independent of the AUC object.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "SVM with linear kernel, especially similar to logistic regression with a slightly different objective function, and they can be trained almost as fast as logistic regression classifiers.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is not.",
                    "label": 0
                },
                {
                    "sent": "I wouldn't say this is a standard SVM, it is.",
                    "label": 0
                },
                {
                    "sent": "It says we imperf it.",
                    "label": 0
                },
                {
                    "sent": "Actually if you choose the standard SVM optimization function then it really.",
                    "label": 0
                },
                {
                    "sent": "It has quite good performance, but when and it also I believe is a pretty good Optima.",
                    "label": 0
                },
                {
                    "sent": "It's designed especially for linear machines, so this is this season's VM optimized for linear Turner.",
                    "label": 0
                },
                {
                    "sent": "And the only change is that it actually uses.",
                    "label": 0
                },
                {
                    "sent": "Uses AUC as the target function.",
                    "label": 0
                },
                {
                    "sent": "I mean it's you can find the description in the implementation is available.",
                    "label": 0
                },
                {
                    "sent": "It's an implementation by Thurston.",
                    "label": 0
                },
                {
                    "sent": "You are Kings and he had several papers on that so.",
                    "label": 0
                },
                {
                    "sent": "Say yeah about about this and about comparison of those constants.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is a very tricky issue.",
                    "label": 0
                },
                {
                    "sent": "That's where I believe this big O notation to some extent doesn't help because.",
                    "label": 0
                },
                {
                    "sent": "OK, you can say there is a factor of log size the data set, but then you're moving data around and you're moving values around so you also have like.",
                    "label": 0
                },
                {
                    "sent": "Cash performance issues and things like that here you just go over the data and I I did compare with sampling.",
                    "label": 0
                },
                {
                    "sent": "Here I use sorting with the in memory quicksort.",
                    "label": 0
                },
                {
                    "sent": "I mean that should be pretty efficient and still it's slower.",
                    "label": 0
                },
                {
                    "sent": "You don't need to.",
                    "label": 0
                },
                {
                    "sent": "You just need to sort the score.",
                    "label": 0
                },
                {
                    "sent": "Yes, of course.",
                    "label": 0
                },
                {
                    "sent": "Yes yes yes, of course that's that, of course makes it easier, yes, but still I would say in this case at least it was was better.",
                    "label": 0
                },
                {
                    "sent": "Erica Steiner work.",
                    "label": 0
                },
                {
                    "sent": "Off because my car has presented a eyes as hundreds of years ago, they also presented the an approximation of this figure in the number of data points, and they also accept from the user a accuracy as we can.",
                    "label": 0
                },
                {
                    "sent": "Approximation accuracy can specify an algorithm is currently switched at least accuracy, so they use the multipole expansion.",
                    "label": 0
                },
                {
                    "sent": "Therefore, from physics pricing, so I wonder if you compare to your approach.",
                    "label": 0
                },
                {
                    "sent": "I sorry I didn't hear the authors names, I believe.",
                    "label": 0
                },
                {
                    "sent": "She's going to come rescue Maryland if not sure, but I think the user nonlinear classifier, right?",
                    "label": 0
                },
                {
                    "sent": "There, there, there there are some.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "Are you suggesting regression?",
                    "label": 0
                },
                {
                    "sent": "Closest.",
                    "label": 0
                }
            ]
        }
    }
}