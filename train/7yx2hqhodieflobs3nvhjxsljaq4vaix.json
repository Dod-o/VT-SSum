{
    "id": "7yx2hqhodieflobs3nvhjxsljaq4vaix",
    "title": "Multi-Instance Hidden Markov Model For Facial Expression Recognition",
    "info": {
        "author": [
            "Qiang Ji, Rensselaer Polytechnic Institute"
        ],
        "published": "July 2, 2015",
        "recorded": "May 2015",
        "category": [
            "Top->Computer Science->Computer Vision",
            "Top->Computer Science->Computer Vision->Face & Gesture Analysis"
        ]
    },
    "url": "http://videolectures.net/fgconference2015_ji_facial_expression/",
    "segmentation": [
        [
            "Thank you so this is a joint work with Trillian Wolfe, one and.",
            "OK, so this work is about."
        ],
        [
            "Facial expression recognition from the image sequence.",
            "And here are briefly give some motivation about risk related work.",
            "Alright, for facial expression recognition there typically there are two approach 1 instead of frame based approach where you recognize each facial expression from each image.",
            "But the frame based approaches require preprocessing.",
            "So the preprocessing you have to identify the APAC free right.",
            "You need to provide the label of the apex from during training and during testing.",
            "You also need to identify the APAC frame.",
            "So this is the.",
            "Frame based, the second approach is a sequence based where you try to recognize the expression from a sequence of images.",
            "So here you want to take advantage of the temple information in the in the in the image and like the frame based, it also require a preprocessing.",
            "So basically it required to identify the segment in the sequence that contain the expression right?",
            "You have to wonder, know the beginning and the duration of the of the expression.",
            "So so so this is also required preprocessing.",
            "So both of this work.",
            "You require the preprocessing and the preprocessing.",
            "Typically a time consuming and they are.",
            "They are typically done manually, so this is some limitation with the existing approach.",
            "So the goal of this research is proposed sequence based on facial expression recognition where we do not need pre segmentation which directly reckon recognize the expression from the whole sequence.",
            "And we only need sequence level label.",
            "No segment level label and we can identify the label for each sequence as well as identify the label.",
            "The segments that contained expression.",
            "So this is the main feature of this work.",
            "And how can we achieve.",
            "This is because."
        ],
        [
            "So we combine multi instant learning with hidden Markov model.",
            "So this is the reason that we can achieve this features.",
            "So first I want to give a quick very quick introduction about multi instance learning.",
            "So multi."
        ],
        [
            "Needs to Learn is a special kind of for supervised learning where you do not have label for each sample and you only have label for what they call the backs.",
            "So the samples the samples are divided into bags OK and and there are two for binary classification there are two type back when is called a positive back and positive guide by consist of the samples are also referred to as instance and at least one sample.",
            "In the in, the positive back must be positive, so this is a requirement for the negative back all the samples all the instance must be negative, so this is the the requirement for for the banks and the ones you have the.",
            "These are basically we divided into the negative bags and positive bags.",
            "And do the training through training all your high for you so that you have the bags you have their backs and you have the label of the packs, but they do not have the label for the samples in the packs and your goal is still constructor classifier to classify the sample in the back using only the label of the backs.",
            "So this is achieved by by by maximize the conditional log likelihood of the bike labels.",
            "The bag labels, so this is a conjoint conditional likelihood of the back labels and live with.",
            "For binary classification you have this is a joint for the positive back joint likelihood for the positive back and this is joint log likelihood for the negative back.",
            "So you put together and then you do maximize this object function.",
            "Then you will be able to do.",
            "Compute estimate the parameter of the classifier that can classify each sample.",
            "So this is the basic idea.",
            "OK, so once you have, once you have the once you finish the training during testing or you have these are back, you have a back and you also have the instance in the back.",
            "So what you do first is you use this classifier which is the instance classifier to classify each sample in the back.",
            "Once you have the label for each sample in the back then you will be able to infer the back label.",
            "And the bike label is the label that correspond to the sample with the liquid correspond to the label of the sample with the highest probability.",
            "So this is the basic idea of multi instance learning.",
            "And now we want to use this idea for for expression sequence based expression recognition.",
            "So here."
        ],
        [
            "We want to compare multi instance learning with clean market model and here are the three major steps to achieve our goal.",
            "The first step is a construct backs, so we have to because we use multi instance learning scheme right?",
            "We need to construct bags and we also need to construct the instance the samples for each bag and here we treat each sequence as a bag and we have.",
            "Then we divide.",
            "This is because the sequence into segments, so each segment become a instance of the back.",
            "And then within the you know then we did this multi instance learning framework.",
            "We train a hidden Markov model because each instance is still a subsequence right?",
            "So we we perform we propose to use hidden Markov model to do classification.",
            "So we train a market model that to perform classification of each instance.",
            "Super steps during testing.",
            "Given a query sequence, we first divide it into segments.",
            "Then we use this hidden Markov model to classify each segment and then given the label of each segment, will be able to determine the label for the back which is the label for the sequence.",
            "So this is the major three steps, so now I'm going to go over the three steps that give more details about each step first."
        ],
        [
            "Never used to construct the backs and the instance, so once we have a image during the training right, we have a sequence right?",
            "We divided into sequence into positive sequence and the negative sequence.",
            "Coded sequence corresponding called.",
            "This is the positive sequence and each positive sequence Rep Linda went back, went back right.",
            "Each policy is A1 back.",
            "And each negative sequences another bag.",
            "So so this is first, we produce the positive bags and natural bags.",
            "The second step is for each back both the positive back and neck back with divide each of these sequence into segments into segments.",
            "And here we use this in CART algorithm to automatically segment each sequence into different segments.",
            "And this is basically is a graph cut algorithm or version of graph cut algorithm to segment the sequence in two different.",
            "Segments and each of these segments segments constitute the sample with the instance of the vehicle.",
            "OK, so this is the first step, so we finished the finisher, constructed back and the instance.",
            "The second step is we trained jointly trained.",
            "The multi multi use to learning and critical model."
        ],
        [
            "Here given training sequence as I so for each training sequence right we have the.",
            "We have each label as well.",
            "We also have for each segment of the first step we have this segment.",
            "So each ISI is a bag and IJ represents a segment which is the instance right?",
            "So now the goal here is is the trend of Markov model classifier using within the multi instance learning framework.",
            "By maximizing the conditional log likelihood of the black labels.",
            "So by maximizing the conditional electrical log likelihood of the back labels will be able to estimate the parameters of the hidden Markov model.",
            "OK, and then this is capital through gradient descent method and with the method that we used to perform this gradient descent is the VFD S method.",
            "So this after this one we will have hidden Markov model that can classify each segment and doing under under.",
            "If we have multiple class multiple class then we train we do one meal mark model for each class.",
            "OK sometimes we are not just binary right?",
            "For some expression we have multiple express classes.",
            "So we need to train multiple such model for each class.",
            "And."
        ],
        [
            "Done doing testing, the idea is pretty simple.",
            "During testing we are given a sequence, right?",
            "The goal here is a given sequence, but we do not know where is the expression.",
            "So we want to what we want to determine its label Expression label.",
            "So first we have to divide the sequence into segments.",
            "So we produce this age which is a segment of the ice sequence right then and then for each of the hidden Markov model that we construct for each class, right will be able to find the hidden Markov model for each segment.",
            "That produce the highest likelihood, and then we will say this.",
            "The label for each segment is corresponding to the hidden Markov model that produce the highest likelihood.",
            "So this gives us the label for each segment, right?",
            "Once we know the label for each segment, then we'll be able to to get the label for the back.",
            "The level of the back is simply corresponding to the label of the segment that produce the highest probability.",
            "So that's it.",
            "So they too.",
            "So the the master is actually pretty straightforward.",
            "OK, so why?"
        ],
        [
            "I want to show some experiment so we evaluate our own MSR on two benchmark data, set.",
            "The first one is the secret plus inside and this one have seven expression.",
            "So this is a multi class classification problem and it has 327 sequence and 118 subjects.",
            "So this is the first database that way but this one is a more post database.",
            "The second is the UN PC MIC master shoulder Print expression database.",
            "And this database is a binary classification plan on that notepad, right?",
            "And it has close to 150 sequence, 25 subjects, and for both datasets we perform we perform live one subject out cross validation during this experiment.",
            "And here are some results here.",
            "So this is for."
        ],
        [
            "The C Key plus database and here you see the seven expressions 7 expression and this is the recognition accuracy and this is the F1 score.",
            "As you can see overall for average is 98.5%, so the recognition accuracy is pretty good and this is a good and in particular for a couple of expressions.",
            "For example for angry and the sadness will achieve 100% recognition accuracy.",
            "So you need to think for this in Contacts.",
            "This result is achieved without pre segmentation.",
            "Right without prescription?",
            "And there's also.",
            "So we were pretty happy with this result.",
            "And for this database is printed base.",
            "It is a more challenging because this most spontaneous natural database and we still achieve accuracy about 85%, not as high as here.",
            "And and I've won skarphol .78 and so this is the performance of our MSR on these two benchmarking set.",
            "Now we want to compare the performance of our MSR against state of the art."
        ],
        [
            "So this is a comparison of our method with otherwise on the PIN database and this is our method and this is anonymizer.",
            "Recent vessel that's also use multi instance learning method for sequence based expression classification.",
            "So you can see that you know for our method we achieved with 85% accuracy and this matter is 83% so we're close to almost 2% improvement over this method that used similar similar technique.",
            "Under these are additional methods that he also has also been applied to this database.",
            "OK, but these methods, what we call traditional method because these methods require pre segmentation.",
            "Even though they require pre segmentation, their performance is still lower than our our method.",
            "So this term St.",
            "The the accuracy and robustness of our method OK."
        ],
        [
            "Friendly.",
            "Our MSR not all you can perform a classification of the sequence, but it can also.",
            "And then if I the segments that most likely content the expression.",
            "So for example, this is a sequence OK and this is represent different segments.",
            "As you can see from the probability here, we will be able to infer the starting and ending of the expression.",
            "For example, here is pretty neutral OK and here you can begin to see.",
            "Probably these close to .5 you'll see the on site of the hyper smell right then when you come here.",
            "This segment you can see.",
            "Maybe it is reached the APAC OK and the probabilities that right now is very high 95%.",
            "So here you basically said from here to here.",
            "Probably represented starting and ending.",
            "This is maybe the the on site.",
            "This is represented a pack free, so this can tell you over the long sequence from which frame the expression starts and at which frame is to achieve the APAC appear.",
            "So this is the result of.",
            "So this is another function of our method.",
            "So in summary."
        ],
        [
            "OK, so we propose a MSR for sequence based expression classification and this measure is based on combining multi instance learning with hidden Markov model and 80 can perform expression sequence level expression recognition without pre approval sequence segmentation and only require sequence level label and it can identify the expression label for each sequence as well as localized the expression segments within the sequence.",
            "And we evaluate its performance on two benchmark data set and it will show its good performance and it outperformed state of the art.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you so this is a joint work with Trillian Wolfe, one and.",
                    "label": 0
                },
                {
                    "sent": "OK, so this work is about.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Facial expression recognition from the image sequence.",
                    "label": 0
                },
                {
                    "sent": "And here are briefly give some motivation about risk related work.",
                    "label": 0
                },
                {
                    "sent": "Alright, for facial expression recognition there typically there are two approach 1 instead of frame based approach where you recognize each facial expression from each image.",
                    "label": 1
                },
                {
                    "sent": "But the frame based approaches require preprocessing.",
                    "label": 0
                },
                {
                    "sent": "So the preprocessing you have to identify the APAC free right.",
                    "label": 0
                },
                {
                    "sent": "You need to provide the label of the apex from during training and during testing.",
                    "label": 0
                },
                {
                    "sent": "You also need to identify the APAC frame.",
                    "label": 0
                },
                {
                    "sent": "So this is the.",
                    "label": 0
                },
                {
                    "sent": "Frame based, the second approach is a sequence based where you try to recognize the expression from a sequence of images.",
                    "label": 0
                },
                {
                    "sent": "So here you want to take advantage of the temple information in the in the in the image and like the frame based, it also require a preprocessing.",
                    "label": 0
                },
                {
                    "sent": "So basically it required to identify the segment in the sequence that contain the expression right?",
                    "label": 0
                },
                {
                    "sent": "You have to wonder, know the beginning and the duration of the of the expression.",
                    "label": 0
                },
                {
                    "sent": "So so so this is also required preprocessing.",
                    "label": 0
                },
                {
                    "sent": "So both of this work.",
                    "label": 0
                },
                {
                    "sent": "You require the preprocessing and the preprocessing.",
                    "label": 0
                },
                {
                    "sent": "Typically a time consuming and they are.",
                    "label": 0
                },
                {
                    "sent": "They are typically done manually, so this is some limitation with the existing approach.",
                    "label": 0
                },
                {
                    "sent": "So the goal of this research is proposed sequence based on facial expression recognition where we do not need pre segmentation which directly reckon recognize the expression from the whole sequence.",
                    "label": 0
                },
                {
                    "sent": "And we only need sequence level label.",
                    "label": 0
                },
                {
                    "sent": "No segment level label and we can identify the label for each sequence as well as identify the label.",
                    "label": 0
                },
                {
                    "sent": "The segments that contained expression.",
                    "label": 0
                },
                {
                    "sent": "So this is the main feature of this work.",
                    "label": 0
                },
                {
                    "sent": "And how can we achieve.",
                    "label": 0
                },
                {
                    "sent": "This is because.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we combine multi instant learning with hidden Markov model.",
                    "label": 1
                },
                {
                    "sent": "So this is the reason that we can achieve this features.",
                    "label": 0
                },
                {
                    "sent": "So first I want to give a quick very quick introduction about multi instance learning.",
                    "label": 0
                },
                {
                    "sent": "So multi.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Needs to Learn is a special kind of for supervised learning where you do not have label for each sample and you only have label for what they call the backs.",
                    "label": 0
                },
                {
                    "sent": "So the samples the samples are divided into bags OK and and there are two for binary classification there are two type back when is called a positive back and positive guide by consist of the samples are also referred to as instance and at least one sample.",
                    "label": 0
                },
                {
                    "sent": "In the in, the positive back must be positive, so this is a requirement for the negative back all the samples all the instance must be negative, so this is the the requirement for for the banks and the ones you have the.",
                    "label": 0
                },
                {
                    "sent": "These are basically we divided into the negative bags and positive bags.",
                    "label": 0
                },
                {
                    "sent": "And do the training through training all your high for you so that you have the bags you have their backs and you have the label of the packs, but they do not have the label for the samples in the packs and your goal is still constructor classifier to classify the sample in the back using only the label of the backs.",
                    "label": 0
                },
                {
                    "sent": "So this is achieved by by by maximize the conditional log likelihood of the bike labels.",
                    "label": 1
                },
                {
                    "sent": "The bag labels, so this is a conjoint conditional likelihood of the back labels and live with.",
                    "label": 0
                },
                {
                    "sent": "For binary classification you have this is a joint for the positive back joint likelihood for the positive back and this is joint log likelihood for the negative back.",
                    "label": 0
                },
                {
                    "sent": "So you put together and then you do maximize this object function.",
                    "label": 0
                },
                {
                    "sent": "Then you will be able to do.",
                    "label": 0
                },
                {
                    "sent": "Compute estimate the parameter of the classifier that can classify each sample.",
                    "label": 0
                },
                {
                    "sent": "So this is the basic idea.",
                    "label": 0
                },
                {
                    "sent": "OK, so once you have, once you have the once you finish the training during testing or you have these are back, you have a back and you also have the instance in the back.",
                    "label": 0
                },
                {
                    "sent": "So what you do first is you use this classifier which is the instance classifier to classify each sample in the back.",
                    "label": 0
                },
                {
                    "sent": "Once you have the label for each sample in the back then you will be able to infer the back label.",
                    "label": 1
                },
                {
                    "sent": "And the bike label is the label that correspond to the sample with the liquid correspond to the label of the sample with the highest probability.",
                    "label": 0
                },
                {
                    "sent": "So this is the basic idea of multi instance learning.",
                    "label": 0
                },
                {
                    "sent": "And now we want to use this idea for for expression sequence based expression recognition.",
                    "label": 0
                },
                {
                    "sent": "So here.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We want to compare multi instance learning with clean market model and here are the three major steps to achieve our goal.",
                    "label": 0
                },
                {
                    "sent": "The first step is a construct backs, so we have to because we use multi instance learning scheme right?",
                    "label": 0
                },
                {
                    "sent": "We need to construct bags and we also need to construct the instance the samples for each bag and here we treat each sequence as a bag and we have.",
                    "label": 1
                },
                {
                    "sent": "Then we divide.",
                    "label": 1
                },
                {
                    "sent": "This is because the sequence into segments, so each segment become a instance of the back.",
                    "label": 0
                },
                {
                    "sent": "And then within the you know then we did this multi instance learning framework.",
                    "label": 0
                },
                {
                    "sent": "We train a hidden Markov model because each instance is still a subsequence right?",
                    "label": 0
                },
                {
                    "sent": "So we we perform we propose to use hidden Markov model to do classification.",
                    "label": 0
                },
                {
                    "sent": "So we train a market model that to perform classification of each instance.",
                    "label": 0
                },
                {
                    "sent": "Super steps during testing.",
                    "label": 0
                },
                {
                    "sent": "Given a query sequence, we first divide it into segments.",
                    "label": 1
                },
                {
                    "sent": "Then we use this hidden Markov model to classify each segment and then given the label of each segment, will be able to determine the label for the back which is the label for the sequence.",
                    "label": 0
                },
                {
                    "sent": "So this is the major three steps, so now I'm going to go over the three steps that give more details about each step first.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Never used to construct the backs and the instance, so once we have a image during the training right, we have a sequence right?",
                    "label": 0
                },
                {
                    "sent": "We divided into sequence into positive sequence and the negative sequence.",
                    "label": 0
                },
                {
                    "sent": "Coded sequence corresponding called.",
                    "label": 0
                },
                {
                    "sent": "This is the positive sequence and each positive sequence Rep Linda went back, went back right.",
                    "label": 0
                },
                {
                    "sent": "Each policy is A1 back.",
                    "label": 0
                },
                {
                    "sent": "And each negative sequences another bag.",
                    "label": 1
                },
                {
                    "sent": "So so this is first, we produce the positive bags and natural bags.",
                    "label": 0
                },
                {
                    "sent": "The second step is for each back both the positive back and neck back with divide each of these sequence into segments into segments.",
                    "label": 1
                },
                {
                    "sent": "And here we use this in CART algorithm to automatically segment each sequence into different segments.",
                    "label": 0
                },
                {
                    "sent": "And this is basically is a graph cut algorithm or version of graph cut algorithm to segment the sequence in two different.",
                    "label": 0
                },
                {
                    "sent": "Segments and each of these segments segments constitute the sample with the instance of the vehicle.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the first step, so we finished the finisher, constructed back and the instance.",
                    "label": 0
                },
                {
                    "sent": "The second step is we trained jointly trained.",
                    "label": 0
                },
                {
                    "sent": "The multi multi use to learning and critical model.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here given training sequence as I so for each training sequence right we have the.",
                    "label": 1
                },
                {
                    "sent": "We have each label as well.",
                    "label": 0
                },
                {
                    "sent": "We also have for each segment of the first step we have this segment.",
                    "label": 0
                },
                {
                    "sent": "So each ISI is a bag and IJ represents a segment which is the instance right?",
                    "label": 0
                },
                {
                    "sent": "So now the goal here is is the trend of Markov model classifier using within the multi instance learning framework.",
                    "label": 1
                },
                {
                    "sent": "By maximizing the conditional log likelihood of the black labels.",
                    "label": 1
                },
                {
                    "sent": "So by maximizing the conditional electrical log likelihood of the back labels will be able to estimate the parameters of the hidden Markov model.",
                    "label": 0
                },
                {
                    "sent": "OK, and then this is capital through gradient descent method and with the method that we used to perform this gradient descent is the VFD S method.",
                    "label": 0
                },
                {
                    "sent": "So this after this one we will have hidden Markov model that can classify each segment and doing under under.",
                    "label": 0
                },
                {
                    "sent": "If we have multiple class multiple class then we train we do one meal mark model for each class.",
                    "label": 0
                },
                {
                    "sent": "OK sometimes we are not just binary right?",
                    "label": 0
                },
                {
                    "sent": "For some expression we have multiple express classes.",
                    "label": 1
                },
                {
                    "sent": "So we need to train multiple such model for each class.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Done doing testing, the idea is pretty simple.",
                    "label": 0
                },
                {
                    "sent": "During testing we are given a sequence, right?",
                    "label": 0
                },
                {
                    "sent": "The goal here is a given sequence, but we do not know where is the expression.",
                    "label": 0
                },
                {
                    "sent": "So we want to what we want to determine its label Expression label.",
                    "label": 0
                },
                {
                    "sent": "So first we have to divide the sequence into segments.",
                    "label": 0
                },
                {
                    "sent": "So we produce this age which is a segment of the ice sequence right then and then for each of the hidden Markov model that we construct for each class, right will be able to find the hidden Markov model for each segment.",
                    "label": 0
                },
                {
                    "sent": "That produce the highest likelihood, and then we will say this.",
                    "label": 0
                },
                {
                    "sent": "The label for each segment is corresponding to the hidden Markov model that produce the highest likelihood.",
                    "label": 0
                },
                {
                    "sent": "So this gives us the label for each segment, right?",
                    "label": 0
                },
                {
                    "sent": "Once we know the label for each segment, then we'll be able to to get the label for the back.",
                    "label": 1
                },
                {
                    "sent": "The level of the back is simply corresponding to the label of the segment that produce the highest probability.",
                    "label": 0
                },
                {
                    "sent": "So that's it.",
                    "label": 0
                },
                {
                    "sent": "So they too.",
                    "label": 0
                },
                {
                    "sent": "So the the master is actually pretty straightforward.",
                    "label": 0
                },
                {
                    "sent": "OK, so why?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I want to show some experiment so we evaluate our own MSR on two benchmark data, set.",
                    "label": 0
                },
                {
                    "sent": "The first one is the secret plus inside and this one have seven expression.",
                    "label": 0
                },
                {
                    "sent": "So this is a multi class classification problem and it has 327 sequence and 118 subjects.",
                    "label": 0
                },
                {
                    "sent": "So this is the first database that way but this one is a more post database.",
                    "label": 0
                },
                {
                    "sent": "The second is the UN PC MIC master shoulder Print expression database.",
                    "label": 0
                },
                {
                    "sent": "And this database is a binary classification plan on that notepad, right?",
                    "label": 0
                },
                {
                    "sent": "And it has close to 150 sequence, 25 subjects, and for both datasets we perform we perform live one subject out cross validation during this experiment.",
                    "label": 1
                },
                {
                    "sent": "And here are some results here.",
                    "label": 0
                },
                {
                    "sent": "So this is for.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The C Key plus database and here you see the seven expressions 7 expression and this is the recognition accuracy and this is the F1 score.",
                    "label": 0
                },
                {
                    "sent": "As you can see overall for average is 98.5%, so the recognition accuracy is pretty good and this is a good and in particular for a couple of expressions.",
                    "label": 0
                },
                {
                    "sent": "For example for angry and the sadness will achieve 100% recognition accuracy.",
                    "label": 0
                },
                {
                    "sent": "So you need to think for this in Contacts.",
                    "label": 0
                },
                {
                    "sent": "This result is achieved without pre segmentation.",
                    "label": 0
                },
                {
                    "sent": "Right without prescription?",
                    "label": 0
                },
                {
                    "sent": "And there's also.",
                    "label": 0
                },
                {
                    "sent": "So we were pretty happy with this result.",
                    "label": 0
                },
                {
                    "sent": "And for this database is printed base.",
                    "label": 0
                },
                {
                    "sent": "It is a more challenging because this most spontaneous natural database and we still achieve accuracy about 85%, not as high as here.",
                    "label": 0
                },
                {
                    "sent": "And and I've won skarphol .78 and so this is the performance of our MSR on these two benchmarking set.",
                    "label": 0
                },
                {
                    "sent": "Now we want to compare the performance of our MSR against state of the art.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is a comparison of our method with otherwise on the PIN database and this is our method and this is anonymizer.",
                    "label": 0
                },
                {
                    "sent": "Recent vessel that's also use multi instance learning method for sequence based expression classification.",
                    "label": 0
                },
                {
                    "sent": "So you can see that you know for our method we achieved with 85% accuracy and this matter is 83% so we're close to almost 2% improvement over this method that used similar similar technique.",
                    "label": 0
                },
                {
                    "sent": "Under these are additional methods that he also has also been applied to this database.",
                    "label": 0
                },
                {
                    "sent": "OK, but these methods, what we call traditional method because these methods require pre segmentation.",
                    "label": 0
                },
                {
                    "sent": "Even though they require pre segmentation, their performance is still lower than our our method.",
                    "label": 0
                },
                {
                    "sent": "So this term St.",
                    "label": 0
                },
                {
                    "sent": "The the accuracy and robustness of our method OK.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Friendly.",
                    "label": 0
                },
                {
                    "sent": "Our MSR not all you can perform a classification of the sequence, but it can also.",
                    "label": 0
                },
                {
                    "sent": "And then if I the segments that most likely content the expression.",
                    "label": 0
                },
                {
                    "sent": "So for example, this is a sequence OK and this is represent different segments.",
                    "label": 0
                },
                {
                    "sent": "As you can see from the probability here, we will be able to infer the starting and ending of the expression.",
                    "label": 0
                },
                {
                    "sent": "For example, here is pretty neutral OK and here you can begin to see.",
                    "label": 0
                },
                {
                    "sent": "Probably these close to .5 you'll see the on site of the hyper smell right then when you come here.",
                    "label": 0
                },
                {
                    "sent": "This segment you can see.",
                    "label": 0
                },
                {
                    "sent": "Maybe it is reached the APAC OK and the probabilities that right now is very high 95%.",
                    "label": 0
                },
                {
                    "sent": "So here you basically said from here to here.",
                    "label": 0
                },
                {
                    "sent": "Probably represented starting and ending.",
                    "label": 0
                },
                {
                    "sent": "This is maybe the the on site.",
                    "label": 0
                },
                {
                    "sent": "This is represented a pack free, so this can tell you over the long sequence from which frame the expression starts and at which frame is to achieve the APAC appear.",
                    "label": 0
                },
                {
                    "sent": "So this is the result of.",
                    "label": 0
                },
                {
                    "sent": "So this is another function of our method.",
                    "label": 0
                },
                {
                    "sent": "So in summary.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we propose a MSR for sequence based expression classification and this measure is based on combining multi instance learning with hidden Markov model and 80 can perform expression sequence level expression recognition without pre approval sequence segmentation and only require sequence level label and it can identify the expression label for each sequence as well as localized the expression segments within the sequence.",
                    "label": 1
                },
                {
                    "sent": "And we evaluate its performance on two benchmark data set and it will show its good performance and it outperformed state of the art.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}