{
    "id": "lisyxrbbzv5yf7xyxztl3un6lpuntwla",
    "title": "On the Estimation of alpha-Divergences",
    "info": {
        "author": [
            "Barnab\u00e1s P\u00f3czos, Machine Learning Department, School of Computer Science, Carnegie Mellon University"
        ],
        "published": "May 6, 2011",
        "recorded": "April 2011",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/aistats2011_poczes_estimation/",
    "segmentation": [
        [
            "Thanks so this is a joint work with the Jeff Schneider and I will continue the previous presentation.",
            "I will talk about divergences, how to estimate divergences.",
            "Vendor distributions are continuous and they can be multidimensional.",
            "So if you cannot see the slides for some reason at the back, then you just go to."
        ],
        [
            "I represent that should be there.",
            "Grocery.",
            "Edited version, you just have to remember this link here."
        ],
        [
            "So the brief outline is that first I will define divergences and explain that basic properties, and I will tell you why this is important in machine learning.",
            "After that I will show the estimator that we proposed to estimate the versions is and then I will try to convince you both theoretical results.",
            "Both numerical experiments, that this is indeed a good divisions estimator.",
            "So."
        ],
        [
            "So, but at the divergences we have two distributions P&Q and we have some random samples both from P an from Q and using these samples we want to estimate how far the PMQ from each other.",
            "So this is probably the most famous divisions which is this?",
            "So this is the callback clever diversions, which is defined by this quantity here, and this is non negative and this is zero if and only if P = Q, but but that's the only good property because it's not symmetric and there is no triangle inequality for this.",
            "So this is not really a metric.",
            "But there are other divergences."
        ],
        [
            "Such as studies, divergences, and rainy diversions, they are defined by these quantities.",
            "So observe this common terms in both and they are generalization of the kullback Leibler divergent in that sense that they have a parameter Alpha, and if R4 converges to one, then they will converge to the kullback.",
            "Leiber divergent.",
            "Our goal in this paper we are built to estimate these divergences."
        ],
        [
            "So why this is important in machine learning?",
            "So there can be several situation when you are interested how far is to distribution from each other, and there's a special case they can be used for mutual information estimation as well.",
            "So mutual information is nothing else, just the diversions between the joint distribution and the product of the marginals.",
            "And there are many, many applications for estimate for mutual information from dependence estimation, feature selection via independent component analysis to fMRI.",
            "Data processing and many more."
        ],
        [
            "And there's another interesting application that I would like to discuss with you.",
            "This is how to generalize many machine learning algorithms to the space of distributions in the sense that you know many machine learning algorithms works on finite dimensional feature vectors that you say you have the objects, and for each object you have some finite dimensional feature vectors on your machine.",
            "Learning algorithms work using this feature vectors and using this feature vectors.",
            "You can do clustering, classification or whatever you want, but we imagine a different scenario here that for each of you consider each object as a distribution and you sample from these distributions and then you will have some samples and then using these samples you want to calculate how far these distributions are from each other and using that as a metric you can do say clustering or if you have labels for these objects you can do classification as well.",
            "So for example, imagine the situation that you have.",
            "D people and you just.",
            "Say measured at blood pressure several times and you are interested how far the distribution of for each individual guy, how far these distributions are from each other.",
            "So these are the people you take some measurements and using these measurements using these samples you want to calculate how far these distributions are from each other and then you can cluster them or classify.",
            "Or you can do many machine other machine learning algorithms in this way.",
            "And there are other applications.",
            "So for example you can cross the gene expressions based on their expressions or you can cluster size or galaxies based on their Spectra or images.",
            "If you have a feature reference representation for these images and you can use that distributions to cluster images and so on.",
            "So after this introduction, our goal is to estimate this quantity which."
        ],
        [
            "I will just call the diversions using an samples from P&M samples from Q, and then we can plug these D. Divergent into the formulas of the Rainy Day version so that service diversions.",
            "And then we will have an estimator for deranian service diversions and limit.",
            "If offer is close to one, then we can approximate the kullback.",
            "Leiber divergent as well OK?",
            "That's our goal.",
            "Question."
        ],
        [
            "And how can we do that?",
            "So ideas?",
            "So how would you do?",
            "So the most obvious is that you estimate the densities 1st and you plug these densities into the formulas, so you can use histograms or kernel density estimation.",
            "K nearest neighbor density estimations and so on.",
            "The problem is that the density estimation is very difficult problem, especially in high dimension.",
            "So for example, if the dimension is larger than three, you don't want to do histogram based estimators there.",
            "If you use kernel density estimation, it you have to do some cross validation to tune the benefits.",
            "You have to be careful how to choose how to choose the appropriate kernel and so on.",
            "And the other problem is that the density is annoyance parameter.",
            "Here we are not interested in the density.",
            "We are interested in the divergences, so we want to.",
            "Estimate directly the divergent, not the densities.",
            "Another option would be to use density ratio estimations and use that as well, but this is also again, it's not a direct approach and it's computationally very expensive, so we want a direct method to estimate these divergences."
        ],
        [
            "And the estimator will based on K nearest neighbor density estimation.",
            "But there will be significant differences, so the K nearest neighbor density estimators operate in this way that you have some sample points denoted by this blue dots and you can fix a key so you 2, can you calculate the second nearest neighbor that's that point, and you do know this distance by role and after a bet you calculate that probability.",
            "Empirical probability of this more.",
            "This is of course just K / N -- 1 because you have K points in this bowl and then minus one point source together.",
            "If you don't calculate this guy and you divide this boy by the volume of this problem and that's it.",
            "The question how good is this estimator?",
            "And there's a theorem which says that if K goes to Infinity then this will be a consistent density estimator.",
            "We will use this density estimator, but we will fix.",
            "KK won't go to Infinity with the sample size, so we won't have a consistent density estimator in this case.",
            "And there's another theorem which says that if instead of estimating P, you estimate 1 / P using this expression, then it will converge to a random variable which has 1 / P mean and this much variance.",
            "So you can also observe if that case should go to Infinity to.",
            "For this variance to disappear, but we will fix K OK."
        ],
        [
            "And this is how our estimator will work, that for each sample point.",
            "For the blue guys fixed case, say three, we calculate its nearest neighbor.",
            "Let's throw three among the blue points and we also calculate it.",
            "Certainly arrest neighbor among the red points.",
            "The samples from the other distribution.",
            "This is then buy new and we use density estimators based on this and we plug this.",
            "So the the the vergence formula of the D diversions and then and then we would get this expression.",
            "OK, so you just use the density estimator, unplug the densities into the divisions expression, then you would get this formula.",
            "But again, because we fixed K, it's not a consistent estimator.",
            "But Interestingly, if we multiply this term by this guy here, then we will prove that this is a consistent estimator, so this multiplicative bias does not depend on P or Q. OK, so our goal is to prove that this is a consistent divergent estimation."
        ],
        [
            "So this is our first theoretical result about."
        ],
        [
            "The consistency.",
            "If the sample size N&N goes to Infinity, then the expected value of the estimator is the quantity that we want to estimate.",
            "So the estimator is asymptotically unbiased and also it's L2 consistent in the sense that the squared deviation in expected value converges to zero as well.",
            "For this, we need a few conditions to hold.",
            "The simplest conditions are these ones that P&Q should be bounded away from zero.",
            "They should be bounded above.",
            "They should be uniformly continuous.",
            "Densities and key should be as large as that this condition holds, so you fix an Alpha and K should be this large and we also have some conditions on the domain of the densities.",
            "Namely, if you put an arbitrary smaller bowl, this is the domain and you calculate the volume of this border.",
            "The intersection of this board and the domain and you divide this by the by the volume of the bowl.",
            "Then this should be bounded above bounded away from zero, so this is not a good domain.",
            "But this is a good one where we have we can prove consistency.",
            "So let's."
        ],
        [
            "See why the division's estimator is consistent.",
            "For this we will use.",
            "Certain properties of normalized key resonator.",
            "Distance is so row K is the K nearest neighbors from X1.",
            "They put this to the power D and multiply this by and minus one and we do this because its distribution function is nice so it has a closed form distribution and it's basically looks like looks like a binomial distribution.",
            "So what is this distribution function?",
            "You just plug this, you ask what is the probability that this guy is less than you?",
            "And this probability is nothing else, just the probability that the.",
            "Case, nearest neighbor distance is less than this rnu quantity, which is defined by this.",
            "And if nothing else, just key or more elements from the sample points are in this ball, and it's a binomial distribution.",
            "The other."
        ],
        [
            "With property that we can use that in the limit, this binomial distribution converges to analog distribution.",
            "Oops.",
            "That is, if you normalize this K nearest neighbor distances by this quantity, then you can prove that this random variable converges to this random variable, which has a long distribution with this mean and with this variance.",
            "And let's see how can we use this to prove."
        ],
        [
            "The estimate is asymptotically unbiased, so we want to prove this is our estimator.",
            "We want to prove that this is a synthetic Llambias, so we divide both sides by this quantity and we want to prove that the expected value of this term is this expression here, where this B is just this expression.",
            "So this right on the right hand side of this equation, that is, this term can be written in this form.",
            "OK, and you can observe that these terms here are nothing else, just the 1 minus Alpha and Alpha minus one moments of these normalized K nearest neighbor distances and we just discussed that we know the.",
            "That in the limit, these normalized nearest neighbor distances are they have a long distribution and there are closed form expressions for these moments."
        ],
        [
            "So let's do that.",
            "We want to calculate this limit here.",
            "If we can move this limit into the expectations, then we know that in the limit these guys have along distribution and we are calculating the 1 minus Alpha and Alpha minus one moment and we know that those moments they're just these gamma functions and in this way we can really prove that the expected value of our estimator is what we wanted to yes.",
            "So the estimator is asymptotically unbiased, all we need here is.",
            "That if we have a random variable which is converges to another variable in distribution, then it's gone.",
            "Math moments are converging as well, and that's also it's very simple."
        ],
        [
            "The little problem that is not true.",
            "So you can easily construct examples then density functions, so converge to.",
            "So this F1F2 and so on.",
            "F3 density functions converge to this density F. Um, but still age density has so big tier that they expected value doesn't even exist.",
            "So for example they are like cash distributions, but this F has expected value finite, so you can converge weakly in a way that the expected values are not converging.",
            "So to prove this, we need stronger assumptions.",
            "We have to prove that the theories of these random variables are asymptotically uniformly integrable.",
            "And unfortunately this increases the page ranks alittle, so it's not that simple to prove this, but."
        ],
        [
            "You can do.",
            "So after this I would like to show so."
        ],
        [
            "Numerical experiments.",
            "So here we had two dimensional normal distributions.",
            "We calculated the divergent Sandrini diversions.",
            "Using different sample size and here you can see five independent experiments and you can see how these experiments converge to the true quantity.",
            "This is the true versions.",
            "And so this is for the range of versions.",
            "This is a true quantity and this is how these estimators converge to the true."
        ],
        [
            "We repeat this in five dimensions using 5 dimensional beta distributions.",
            "You can also observe that as we increase the sample size, the estimators converge to the true."
        ],
        [
            "Quantity you can also use this to estimate mutual information in a way that you have samples from.",
            "D dimensional.",
            "Distribution and the divergences.",
            "Nothing else, just.",
            "The.",
            "The mutual information is nothing else, just the divergents of this joint distribution and the product of the marginals.",
            "So what we do, we just randomly shuffle these marginals and then we will have samples from the product of the marginals and we calculate the divisions between these two SAMP."
        ],
        [
            "And in this example, I just demonstrate that we have this rotated uniform distribution.",
            "We wanted to calculate the mutual information between the.",
            "Marginal variables you can see the original samples points from the joint distribution and this red circles add the samples from the product of the marginals.",
            "That is, if we randomly.",
            "Shuffle the marginal variables and you can see that the divergences.",
            "So the mutual information estimated mutual information is converging to the true quantity."
        ],
        [
            "And finally, in this last demonstration I would like to demonstrate that you can use this to do manifold learning, low dimensional embedding on the space of distributions as well, that in this example we had one dimension algorithms with different mean and variance parameters.",
            "And here you can see.",
            "We randomly generated 1000 sample points from each Gaussian and here you can see that embedded picture where we calculated the diversions between them and then use multidimensional scaling for the embedding.",
            "And what you can observe that this embedding preserve the structure of these Gaussians in the way that if you move along this line then this is how the variance is changing for the fixed mean and along this curve you can see how the.",
            "I mean is changing and we had another example where we did the same experiments on noisy 2 dimensional assigned functions, so these were this is 1 example from a noisy sine function.",
            "This is another one where the difference is that we just change the frequency between these examples.",
            "We had many many noisy sine functions like this and they were.",
            "They defect.",
            "By having different frequencies and we calculated the divisions between these two dimensional distribution and used again multidimensional scaling and you can see that the embed link preserved this one dimensional structure that they differed only by the frequency."
        ],
        [
            "So as a conclusion which showed that this diversions estimator is a consistent division submitted for Alpha divergences, it's a direct estimate that we don't use.",
            "Density estimation, and if you would like to see more applications, please come to our Learning Workshop poster.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thanks so this is a joint work with the Jeff Schneider and I will continue the previous presentation.",
                    "label": 1
                },
                {
                    "sent": "I will talk about divergences, how to estimate divergences.",
                    "label": 0
                },
                {
                    "sent": "Vendor distributions are continuous and they can be multidimensional.",
                    "label": 0
                },
                {
                    "sent": "So if you cannot see the slides for some reason at the back, then you just go to.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I represent that should be there.",
                    "label": 0
                },
                {
                    "sent": "Grocery.",
                    "label": 0
                },
                {
                    "sent": "Edited version, you just have to remember this link here.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the brief outline is that first I will define divergences and explain that basic properties, and I will tell you why this is important in machine learning.",
                    "label": 1
                },
                {
                    "sent": "After that I will show the estimator that we proposed to estimate the versions is and then I will try to convince you both theoretical results.",
                    "label": 1
                },
                {
                    "sent": "Both numerical experiments, that this is indeed a good divisions estimator.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, but at the divergences we have two distributions P&Q and we have some random samples both from P an from Q and using these samples we want to estimate how far the PMQ from each other.",
                    "label": 0
                },
                {
                    "sent": "So this is probably the most famous divisions which is this?",
                    "label": 0
                },
                {
                    "sent": "So this is the callback clever diversions, which is defined by this quantity here, and this is non negative and this is zero if and only if P = Q, but but that's the only good property because it's not symmetric and there is no triangle inequality for this.",
                    "label": 1
                },
                {
                    "sent": "So this is not really a metric.",
                    "label": 0
                },
                {
                    "sent": "But there are other divergences.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Such as studies, divergences, and rainy diversions, they are defined by these quantities.",
                    "label": 0
                },
                {
                    "sent": "So observe this common terms in both and they are generalization of the kullback Leibler divergent in that sense that they have a parameter Alpha, and if R4 converges to one, then they will converge to the kullback.",
                    "label": 0
                },
                {
                    "sent": "Leiber divergent.",
                    "label": 0
                },
                {
                    "sent": "Our goal in this paper we are built to estimate these divergences.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So why this is important in machine learning?",
                    "label": 0
                },
                {
                    "sent": "So there can be several situation when you are interested how far is to distribution from each other, and there's a special case they can be used for mutual information estimation as well.",
                    "label": 0
                },
                {
                    "sent": "So mutual information is nothing else, just the diversions between the joint distribution and the product of the marginals.",
                    "label": 0
                },
                {
                    "sent": "And there are many, many applications for estimate for mutual information from dependence estimation, feature selection via independent component analysis to fMRI.",
                    "label": 1
                },
                {
                    "sent": "Data processing and many more.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there's another interesting application that I would like to discuss with you.",
                    "label": 0
                },
                {
                    "sent": "This is how to generalize many machine learning algorithms to the space of distributions in the sense that you know many machine learning algorithms works on finite dimensional feature vectors that you say you have the objects, and for each object you have some finite dimensional feature vectors on your machine.",
                    "label": 0
                },
                {
                    "sent": "Learning algorithms work using this feature vectors and using this feature vectors.",
                    "label": 1
                },
                {
                    "sent": "You can do clustering, classification or whatever you want, but we imagine a different scenario here that for each of you consider each object as a distribution and you sample from these distributions and then you will have some samples and then using these samples you want to calculate how far these distributions are from each other and using that as a metric you can do say clustering or if you have labels for these objects you can do classification as well.",
                    "label": 1
                },
                {
                    "sent": "So for example, imagine the situation that you have.",
                    "label": 0
                },
                {
                    "sent": "D people and you just.",
                    "label": 0
                },
                {
                    "sent": "Say measured at blood pressure several times and you are interested how far the distribution of for each individual guy, how far these distributions are from each other.",
                    "label": 0
                },
                {
                    "sent": "So these are the people you take some measurements and using these measurements using these samples you want to calculate how far these distributions are from each other and then you can cluster them or classify.",
                    "label": 0
                },
                {
                    "sent": "Or you can do many machine other machine learning algorithms in this way.",
                    "label": 0
                },
                {
                    "sent": "And there are other applications.",
                    "label": 0
                },
                {
                    "sent": "So for example you can cross the gene expressions based on their expressions or you can cluster size or galaxies based on their Spectra or images.",
                    "label": 1
                },
                {
                    "sent": "If you have a feature reference representation for these images and you can use that distributions to cluster images and so on.",
                    "label": 0
                },
                {
                    "sent": "So after this introduction, our goal is to estimate this quantity which.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I will just call the diversions using an samples from P&M samples from Q, and then we can plug these D. Divergent into the formulas of the Rainy Day version so that service diversions.",
                    "label": 0
                },
                {
                    "sent": "And then we will have an estimator for deranian service diversions and limit.",
                    "label": 0
                },
                {
                    "sent": "If offer is close to one, then we can approximate the kullback.",
                    "label": 0
                },
                {
                    "sent": "Leiber divergent as well OK?",
                    "label": 0
                },
                {
                    "sent": "That's our goal.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And how can we do that?",
                    "label": 1
                },
                {
                    "sent": "So ideas?",
                    "label": 0
                },
                {
                    "sent": "So how would you do?",
                    "label": 0
                },
                {
                    "sent": "So the most obvious is that you estimate the densities 1st and you plug these densities into the formulas, so you can use histograms or kernel density estimation.",
                    "label": 0
                },
                {
                    "sent": "K nearest neighbor density estimations and so on.",
                    "label": 1
                },
                {
                    "sent": "The problem is that the density estimation is very difficult problem, especially in high dimension.",
                    "label": 0
                },
                {
                    "sent": "So for example, if the dimension is larger than three, you don't want to do histogram based estimators there.",
                    "label": 1
                },
                {
                    "sent": "If you use kernel density estimation, it you have to do some cross validation to tune the benefits.",
                    "label": 1
                },
                {
                    "sent": "You have to be careful how to choose how to choose the appropriate kernel and so on.",
                    "label": 0
                },
                {
                    "sent": "And the other problem is that the density is annoyance parameter.",
                    "label": 0
                },
                {
                    "sent": "Here we are not interested in the density.",
                    "label": 1
                },
                {
                    "sent": "We are interested in the divergences, so we want to.",
                    "label": 0
                },
                {
                    "sent": "Estimate directly the divergent, not the densities.",
                    "label": 0
                },
                {
                    "sent": "Another option would be to use density ratio estimations and use that as well, but this is also again, it's not a direct approach and it's computationally very expensive, so we want a direct method to estimate these divergences.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the estimator will based on K nearest neighbor density estimation.",
                    "label": 0
                },
                {
                    "sent": "But there will be significant differences, so the K nearest neighbor density estimators operate in this way that you have some sample points denoted by this blue dots and you can fix a key so you 2, can you calculate the second nearest neighbor that's that point, and you do know this distance by role and after a bet you calculate that probability.",
                    "label": 0
                },
                {
                    "sent": "Empirical probability of this more.",
                    "label": 0
                },
                {
                    "sent": "This is of course just K / N -- 1 because you have K points in this bowl and then minus one point source together.",
                    "label": 0
                },
                {
                    "sent": "If you don't calculate this guy and you divide this boy by the volume of this problem and that's it.",
                    "label": 0
                },
                {
                    "sent": "The question how good is this estimator?",
                    "label": 1
                },
                {
                    "sent": "And there's a theorem which says that if K goes to Infinity then this will be a consistent density estimator.",
                    "label": 0
                },
                {
                    "sent": "We will use this density estimator, but we will fix.",
                    "label": 0
                },
                {
                    "sent": "KK won't go to Infinity with the sample size, so we won't have a consistent density estimator in this case.",
                    "label": 0
                },
                {
                    "sent": "And there's another theorem which says that if instead of estimating P, you estimate 1 / P using this expression, then it will converge to a random variable which has 1 / P mean and this much variance.",
                    "label": 0
                },
                {
                    "sent": "So you can also observe if that case should go to Infinity to.",
                    "label": 0
                },
                {
                    "sent": "For this variance to disappear, but we will fix K OK.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is how our estimator will work, that for each sample point.",
                    "label": 0
                },
                {
                    "sent": "For the blue guys fixed case, say three, we calculate its nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "Let's throw three among the blue points and we also calculate it.",
                    "label": 0
                },
                {
                    "sent": "Certainly arrest neighbor among the red points.",
                    "label": 0
                },
                {
                    "sent": "The samples from the other distribution.",
                    "label": 0
                },
                {
                    "sent": "This is then buy new and we use density estimators based on this and we plug this.",
                    "label": 0
                },
                {
                    "sent": "So the the the vergence formula of the D diversions and then and then we would get this expression.",
                    "label": 0
                },
                {
                    "sent": "OK, so you just use the density estimator, unplug the densities into the divisions expression, then you would get this formula.",
                    "label": 0
                },
                {
                    "sent": "But again, because we fixed K, it's not a consistent estimator.",
                    "label": 0
                },
                {
                    "sent": "But Interestingly, if we multiply this term by this guy here, then we will prove that this is a consistent estimator, so this multiplicative bias does not depend on P or Q. OK, so our goal is to prove that this is a consistent divergent estimation.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is our first theoretical result about.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The consistency.",
                    "label": 0
                },
                {
                    "sent": "If the sample size N&N goes to Infinity, then the expected value of the estimator is the quantity that we want to estimate.",
                    "label": 0
                },
                {
                    "sent": "So the estimator is asymptotically unbiased and also it's L2 consistent in the sense that the squared deviation in expected value converges to zero as well.",
                    "label": 0
                },
                {
                    "sent": "For this, we need a few conditions to hold.",
                    "label": 0
                },
                {
                    "sent": "The simplest conditions are these ones that P&Q should be bounded away from zero.",
                    "label": 0
                },
                {
                    "sent": "They should be bounded above.",
                    "label": 0
                },
                {
                    "sent": "They should be uniformly continuous.",
                    "label": 1
                },
                {
                    "sent": "Densities and key should be as large as that this condition holds, so you fix an Alpha and K should be this large and we also have some conditions on the domain of the densities.",
                    "label": 0
                },
                {
                    "sent": "Namely, if you put an arbitrary smaller bowl, this is the domain and you calculate the volume of this border.",
                    "label": 0
                },
                {
                    "sent": "The intersection of this board and the domain and you divide this by the by the volume of the bowl.",
                    "label": 0
                },
                {
                    "sent": "Then this should be bounded above bounded away from zero, so this is not a good domain.",
                    "label": 1
                },
                {
                    "sent": "But this is a good one where we have we can prove consistency.",
                    "label": 0
                },
                {
                    "sent": "So let's.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "See why the division's estimator is consistent.",
                    "label": 0
                },
                {
                    "sent": "For this we will use.",
                    "label": 0
                },
                {
                    "sent": "Certain properties of normalized key resonator.",
                    "label": 1
                },
                {
                    "sent": "Distance is so row K is the K nearest neighbors from X1.",
                    "label": 0
                },
                {
                    "sent": "They put this to the power D and multiply this by and minus one and we do this because its distribution function is nice so it has a closed form distribution and it's basically looks like looks like a binomial distribution.",
                    "label": 0
                },
                {
                    "sent": "So what is this distribution function?",
                    "label": 0
                },
                {
                    "sent": "You just plug this, you ask what is the probability that this guy is less than you?",
                    "label": 0
                },
                {
                    "sent": "And this probability is nothing else, just the probability that the.",
                    "label": 0
                },
                {
                    "sent": "Case, nearest neighbor distance is less than this rnu quantity, which is defined by this.",
                    "label": 0
                },
                {
                    "sent": "And if nothing else, just key or more elements from the sample points are in this ball, and it's a binomial distribution.",
                    "label": 0
                },
                {
                    "sent": "The other.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With property that we can use that in the limit, this binomial distribution converges to analog distribution.",
                    "label": 0
                },
                {
                    "sent": "Oops.",
                    "label": 0
                },
                {
                    "sent": "That is, if you normalize this K nearest neighbor distances by this quantity, then you can prove that this random variable converges to this random variable, which has a long distribution with this mean and with this variance.",
                    "label": 0
                },
                {
                    "sent": "And let's see how can we use this to prove.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The estimate is asymptotically unbiased, so we want to prove this is our estimator.",
                    "label": 1
                },
                {
                    "sent": "We want to prove that this is a synthetic Llambias, so we divide both sides by this quantity and we want to prove that the expected value of this term is this expression here, where this B is just this expression.",
                    "label": 0
                },
                {
                    "sent": "So this right on the right hand side of this equation, that is, this term can be written in this form.",
                    "label": 0
                },
                {
                    "sent": "OK, and you can observe that these terms here are nothing else, just the 1 minus Alpha and Alpha minus one moments of these normalized K nearest neighbor distances and we just discussed that we know the.",
                    "label": 0
                },
                {
                    "sent": "That in the limit, these normalized nearest neighbor distances are they have a long distribution and there are closed form expressions for these moments.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's do that.",
                    "label": 0
                },
                {
                    "sent": "We want to calculate this limit here.",
                    "label": 0
                },
                {
                    "sent": "If we can move this limit into the expectations, then we know that in the limit these guys have along distribution and we are calculating the 1 minus Alpha and Alpha minus one moment and we know that those moments they're just these gamma functions and in this way we can really prove that the expected value of our estimator is what we wanted to yes.",
                    "label": 0
                },
                {
                    "sent": "So the estimator is asymptotically unbiased, all we need here is.",
                    "label": 1
                },
                {
                    "sent": "That if we have a random variable which is converges to another variable in distribution, then it's gone.",
                    "label": 0
                },
                {
                    "sent": "Math moments are converging as well, and that's also it's very simple.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The little problem that is not true.",
                    "label": 1
                },
                {
                    "sent": "So you can easily construct examples then density functions, so converge to.",
                    "label": 0
                },
                {
                    "sent": "So this F1F2 and so on.",
                    "label": 0
                },
                {
                    "sent": "F3 density functions converge to this density F. Um, but still age density has so big tier that they expected value doesn't even exist.",
                    "label": 0
                },
                {
                    "sent": "So for example they are like cash distributions, but this F has expected value finite, so you can converge weakly in a way that the expected values are not converging.",
                    "label": 0
                },
                {
                    "sent": "So to prove this, we need stronger assumptions.",
                    "label": 1
                },
                {
                    "sent": "We have to prove that the theories of these random variables are asymptotically uniformly integrable.",
                    "label": 0
                },
                {
                    "sent": "And unfortunately this increases the page ranks alittle, so it's not that simple to prove this, but.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can do.",
                    "label": 0
                },
                {
                    "sent": "So after this I would like to show so.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Numerical experiments.",
                    "label": 0
                },
                {
                    "sent": "So here we had two dimensional normal distributions.",
                    "label": 0
                },
                {
                    "sent": "We calculated the divergent Sandrini diversions.",
                    "label": 0
                },
                {
                    "sent": "Using different sample size and here you can see five independent experiments and you can see how these experiments converge to the true quantity.",
                    "label": 0
                },
                {
                    "sent": "This is the true versions.",
                    "label": 0
                },
                {
                    "sent": "And so this is for the range of versions.",
                    "label": 0
                },
                {
                    "sent": "This is a true quantity and this is how these estimators converge to the true.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We repeat this in five dimensions using 5 dimensional beta distributions.",
                    "label": 0
                },
                {
                    "sent": "You can also observe that as we increase the sample size, the estimators converge to the true.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Quantity you can also use this to estimate mutual information in a way that you have samples from.",
                    "label": 1
                },
                {
                    "sent": "D dimensional.",
                    "label": 0
                },
                {
                    "sent": "Distribution and the divergences.",
                    "label": 0
                },
                {
                    "sent": "Nothing else, just.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "The mutual information is nothing else, just the divergents of this joint distribution and the product of the marginals.",
                    "label": 0
                },
                {
                    "sent": "So what we do, we just randomly shuffle these marginals and then we will have samples from the product of the marginals and we calculate the divisions between these two SAMP.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in this example, I just demonstrate that we have this rotated uniform distribution.",
                    "label": 1
                },
                {
                    "sent": "We wanted to calculate the mutual information between the.",
                    "label": 0
                },
                {
                    "sent": "Marginal variables you can see the original samples points from the joint distribution and this red circles add the samples from the product of the marginals.",
                    "label": 0
                },
                {
                    "sent": "That is, if we randomly.",
                    "label": 0
                },
                {
                    "sent": "Shuffle the marginal variables and you can see that the divergences.",
                    "label": 0
                },
                {
                    "sent": "So the mutual information estimated mutual information is converging to the true quantity.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finally, in this last demonstration I would like to demonstrate that you can use this to do manifold learning, low dimensional embedding on the space of distributions as well, that in this example we had one dimension algorithms with different mean and variance parameters.",
                    "label": 0
                },
                {
                    "sent": "And here you can see.",
                    "label": 0
                },
                {
                    "sent": "We randomly generated 1000 sample points from each Gaussian and here you can see that embedded picture where we calculated the diversions between them and then use multidimensional scaling for the embedding.",
                    "label": 0
                },
                {
                    "sent": "And what you can observe that this embedding preserve the structure of these Gaussians in the way that if you move along this line then this is how the variance is changing for the fixed mean and along this curve you can see how the.",
                    "label": 0
                },
                {
                    "sent": "I mean is changing and we had another example where we did the same experiments on noisy 2 dimensional assigned functions, so these were this is 1 example from a noisy sine function.",
                    "label": 0
                },
                {
                    "sent": "This is another one where the difference is that we just change the frequency between these examples.",
                    "label": 0
                },
                {
                    "sent": "We had many many noisy sine functions like this and they were.",
                    "label": 0
                },
                {
                    "sent": "They defect.",
                    "label": 0
                },
                {
                    "sent": "By having different frequencies and we calculated the divisions between these two dimensional distribution and used again multidimensional scaling and you can see that the embed link preserved this one dimensional structure that they differed only by the frequency.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as a conclusion which showed that this diversions estimator is a consistent division submitted for Alpha divergences, it's a direct estimate that we don't use.",
                    "label": 0
                },
                {
                    "sent": "Density estimation, and if you would like to see more applications, please come to our Learning Workshop poster.",
                    "label": 1
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}