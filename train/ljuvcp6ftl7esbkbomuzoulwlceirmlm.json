{
    "id": "ljuvcp6ftl7esbkbomuzoulwlceirmlm",
    "title": "Learning to Disambiguate Search Queries from Short Sessions",
    "info": {
        "author": [
            "Lilyana Mihalkova, University of Texas at Austin"
        ],
        "published": "Oct. 20, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Information Retrieval",
            "Top->Computer Science->Semantic Web",
            "Top->Computer Science->Web Search"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd09_mihalkova_ldsqss/",
    "segmentation": [
        [
            "Now come to learn something about how to disambiguate.",
            "Thanks everyone for coming.",
            "I am dilana Michelle Cuva and this is joint work with my former PhD advisor, Ray Mooney.",
            "When I was a grad student at the University of Texas at Austin, I graduated last month, so now I'm a D and postdoc at the universe."
        ],
        [
            "Maryland College Park.",
            "So I'll start with the problem that we're addressing in this work.",
            "So web query disambiguation is when a user goes to search engine and enters a potentially ambiguous search string, such as the word scrubs here, which could mean either a popular TV show popular in the US at least, and it could also mean it could also mean the kind of uniform that doctors were in hospitals, and the question is when the user enters that query.",
            "Which of these two meanings is the intent?"
        ],
        [
            "Did one, and not surprisingly, that's a very widely studied problem.",
            "I'm listing just a few references here, but there are a lot more and what seems to be a common approach behind this work is that typically these approaches build a user profile from long history of that particular users, so."
        ],
        [
            "Which is.",
            "So there are various concerns with that.",
            "One is privacy concerns so many of you may have heard about the AOL data release when some reporters from the New York Times identified one of the searchers just based on her searches and they went and interviewed her, wrote an article, and it became a big scandal.",
            "Then their papers like this one Googling, considered harmful that argued that the types of information recorded by search engines about users constitute a serious infraction on there.",
            "Privacy.",
            "There are also pragmatic concerns.",
            "So how to store and protect this data?",
            "How to identify users?"
        ],
        [
            "Across sessions and so on.",
            "So what we propose to do in this paper is to base personalization only on very short glimpses of user search activities that are captured.",
            "In brief, short term sessions, and I'd like to emphasize that we do not assume across session user identifiers, so we cannot figure out that the session of the this user today is.",
            "So this user today of the session is the same user as some session yesterday or so, and we cannot."
        ],
        [
            "So just to put a number on this, what do we mean by short-term session?",
            "This is what our test data looks like on the X axis which is in log scale.",
            "We have the number of search queries that were issued in the session before the ambiguous query was issued an on the Y axis we have how many sessions we have with that number.",
            "So we see that most of our test sessions the user issued three searches and then the 4th search was an ambiguous query and we're trying from this very small amount of information.",
            "To provide some personal."
        ],
        [
            "Nation.",
            "So a natural question is whether this is enough info for anything meaningful.",
            "So I have anecdotal evidence that it is these are two sessions of two different users.",
            "First one search for some radio station.",
            "Then for this crossing, which also turns out to be a radio station and then for scrubs and then the second person searched for Huntsville Hospital, then for eBay and then for Scrubs.",
            "And so we can guess that the first person is.",
            "A generic sort of user who is most likely interested in the TV show where's the?",
            "Second Searcher is likely trying to find medical uniforms, and in this case it turns out that we are right.",
            "The this was a Click to the TV show, where is the other one?"
        ],
        [
            "So Click to the uniform space.",
            "So the there's one paper as far as we are aware that this more closely related to what we're trying to do here.",
            "They also make an assumption of short sessions, but their approach is better suited for a specialized search engine, so they run their experiments on a search of computer science for computer science literature store.",
            "So it's very specific sort of search words.",
            "We would like something that will work with the general type."
        ],
        [
            "Search so the main challenge challenge then is to find a way to harness this small amount of potentially noisy information and be able to personalize the experience of users and the key to our approach is to exploit the relationships among sessions you."
        ],
        [
            "1000 queries, so here's a picture of how we imagine this.",
            "So this in the middle is the current session, and we're viewing it as being related to a whole bunch of other sessions that are similarly short, and we don't know whether or not well they're most likely not belonging to the same user.",
            "So what are these relationships?",
            "Well, we established the relationships by shared information such as queries or clicks that are themselves related.",
            "And how are queries or clicks related will very simply, querystring in URL's are related by sharing identical keywords or by being themselves identical and I so this allows us to establish various kinds of relationships between the sessions and then any two sessions can be related also in a variety of ways.",
            "And there's one more kind of relationship that we can observe.",
            "So for each ambiguous query there is a set of possible results that could be clicked, and this set.",
            "These results are also related among each other, because you know if the person picks one of the results, maybe they're not interested in the other one, so this establishes some sort of relationship."
        ],
        [
            "Among those so, to overcome the sparsity in any information in any individual session, we need a technique that can effectively combine this diverse, diverse set of relationships that exist among the among the sessions and at least to us, this sounded like a statistical relational learning problem, and if you're not familiar with the area, there's an excellent compilation of papers in this book.",
            "Is it bag it or it ask are we?",
            "Use one particular model called the Markov Logic Network, and."
        ],
        [
            "Explain why in just a little bit.",
            "So for the rest of this talk I'll give some background on Markov logic networks.",
            "Then I'll give the details of our model and I will present some experiments on real search engine data which was provided to us by Microsoft Research and now finish with."
        ],
        [
            "After work.",
            "So Markov logic networks were introduced by Richardson and Domingos, and quite simply they consist of a set of weighted 1st order formulas and the intuition behind this model is that the larger the weight of a clause, the greater the penalty for not satisfying a grounding of this clause.",
            "So we can view this clauses as some sort of relational features that can be then grounded for each particular instance of a problem.",
            "And you know these features can provide some, possibly also contradicting evidence that can then be integr."
        ],
        [
            "It is in the principled way and this is how this is done.",
            "So say we have set of unknown query atoms Q.",
            "And the set of evidence atoms E and we want to know what's the probability of the query atoms given the evidence.",
            "And this is the answer according to Merlin, if we just focused on the top part for now, we have an exponential over of the sum over all formulas in the model, where for each formula we are multiplying the weight of that formula Times the number of satisfied groundings of this formula on these truth assignments.",
            "And in the denominator is just a normalizing constant where we're summing.",
            "Over every possible value of Q, so it gives a principled way of."
        ],
        [
            "Calculating this.",
            "And an advantage of using imbalances that there are a lot of algorithms that have already been introduced in the literature, and they're also implemented in this alchemy software package, which is what we used.",
            "In particular, we used the weight learning algorithm called contrastive divergent, which was adapted for millions by Loudon Domingos, and we had to adapt the code so that it can deal with streaming data because our data was too large to fit in memory and for inference we use the MCC.",
            "Algorithm."
        ],
        [
            "Which gives very good performance.",
            "So this is what we want to do.",
            "We have an ambiguous query for which we somehow get a set of possible results.",
            "Maybe we get it from a search engine.",
            "Then we have our Markov logic network which assigns a probability to each of the possible results, and this is basically what is the probability that the user will be interested in clicking on that result.",
            "And then we use these probabilities to rerank or to rank the.",
            "Possible results.",
            "So where do we get this MLN 4 from?",
            "Well, we hand code a set of formulas that captured the irregularities in the domain and the challenge here is to define an effective set of relations and then once we have these formulas, we use the data to learn a set of weights from them.",
            "And the challenge here is this.",
            "This is very noisy data as you can imagine when people search on the web, they are not necessarily following any strict rules.",
            "So just to mention that you know this is a standard methodology for using ambulance to address a particular problem, just hand code some rules and learn some weights and it's very convenient, especially if you're comfortable with first order logic to do that, because you know if you have some idea about how this domain works, you can express that in the 1st order logic using some possibly contradictory rules, and then by learning the weights on these from the data you can.",
            "As you know, have a way of trading off these rules and can."
        ],
        [
            "Finding them.",
            "So these are the specific relationships between sessions that we use, so this is the in orange.",
            "We have the current session and decent purple our previous sessions, so they share various kinds of information such as cures between searches, keywords between clicks between clicks and searches between searches and clicks and also shared clicks.",
            "So it's very basic kinds of things that are shared that established relationships."
        ],
        [
            "Between Sessions and based on these relationships, we define the clauses in the Markov logic network.",
            "So the most important set probably are the collaborative clauses.",
            "So they say that the current user will pick the result that was chosen by some related sessions, and the sessions could be related either by shared click or by shared keywords in some way, and for every way you can share keywords we have a diff."
        ],
        [
            "Control.",
            "Then we have a popularity clause that says that the user will choose the result that was chosen by any previous session, regardless of whether it's related or not.",
            "So the effect of that is that the most popular result gets the highest probability of being clicked, and it may seem like a really stupid rule to have, but actually it's a very strong baseline, as you will see in the results.",
            "And by the way, I'm just giving you these in English, but if you're interested in seeing the actual formulations in first order logic, I invite you to look at our paper.",
            "I think it will be.",
            "It's much easier to present it like this at the top."
        ],
        [
            "And then we have these local clauses that say, well, the user will choose the result that shares some sort of information with something that the same user didn't.",
            "This current session and.",
            "Well, that doesn't do very well, not surprisingly, because the sessions that we have are very short and you know, it just doesn't."
        ],
        [
            "Very well.",
            "And then we have a balanced clause which says that if the user chooses one of the possible results, they won't choose another one.",
            "This sets up a competition among the possible results, and pragmatically, the reason why we have it is because it allows us to use the same set of weights across different size problems."
        ],
        [
            "So this was the clauses that we defined.",
            "So now I'll talk about the empirical evaluation.",
            "So the data was provided by MSR.",
            "It was collected on their search engine in May 2006, so we use the first 25 days for training the last six days for testing and any session that straddled the testing and training period, we removed it to avoid contamination, so there were two difficulties associated with using this data which were a little bit frustrating.",
            "So first of all.",
            "The data doesn't say which queries are ambiguous, and in fact the detecting ambiguity whether something is ambiguous or not.",
            "It's it's own research area we didn't want to go there.",
            "Instead, we use this heuristic.",
            "We just use the Dimas hierarchy of pages, and if for that particular query string there were at least two clicks somewhere where they fell in different demote categories, we said, well, OK, this is an ambiguous query.",
            "The other problem with the data was that it only lists the results were actually clicked by the user, but we don't know what the user saw on the web page, so that was probably more serious because we didn't know how to run the experiments.",
            "But So what we did was we assume that the user saw all the all the results that were clicked by someone somewhere in the data following that exact query string.",
            "So this is a source of noise that this kind of unfortunate, but we didn't."
        ],
        [
            "A better way to control it.",
            "So these are the models that we tested we used, we tried random re ranking then we have the popularity baseline that basically ranks everything based on the number of people in on the test data that picked that result following that query.",
            "Then we implemented some standard collaborative filtering baselines where basically the preferences of the set of the most closely related.",
            "Mostly some of the set of the most closely similar sessions are are taken into account.",
            "So based on how the similarities calculated, we have two varieties, the Pearson in the cosine, and then we experimented with these three combinations of the clauses and so the results I'll show today are just this first ones collaborative plus popularity plus balance.",
            "All the clauses without the local ones.",
            "The other ones are in the."
        ],
        [
            "Paper.",
            "We use two measures for evaluation.",
            "The area under the Arosi curve and also the area under the Precision recall curve.",
            "So the RC curve corresponds to mean average through negative rate and I won't go over this.",
            "But if you're interested we can talk about it at the poster and the area under precision called curve corresponds to the mean average precision.",
            "So in the paper we have both of these with.",
            "Here I'll focus on the error."
        ],
        [
            "See curve because I think it has a really nice in two different reputation.",
            "So if we assume that the user starts scanning the set of the page results from the top and stops as soon as she finds irrelevant result, the AUC arosi captures what proportion of the irrelevant results are not seen by the user.",
            "So that's I think nice, and so here's a picture.",
            "You know if this is the relevant result, we put it second.",
            "Well, what proportion came?"
        ],
        [
            "After it.",
            "So this is, this is what the results look like.",
            "Random does right around .5.",
            "That's another nice thing about the AUC.",
            "Arosi the collaborative Pearson baseline did as good as random using using cosine similarity improved a little bit, then the popularity baseline is really good.",
            "And this is what we get by combining various kinds of information in the MLN model.",
            "And, you know, this may not seem like very large differences.",
            "But they're all.",
            "So whenever you see a difference, it's highly statistically significant and we are very encouraged, actually by these results because the data is very noisy."
        ],
        [
            "So it's interesting to kind of analyze the results a little bit more based on the difficulty of disambiguating a particular query.",
            "So some queries are easier than others, and the way we measure this easiness is by the KL divergences between the distribution over possible results for the for the query and the uniform distribution.",
            "So if the if the possible results are distributed roughly uniformly, then that's a very difficult ambiguous query for the popularity baseline because there is no clear winner.",
            "Whereas if we have one clear winner just predicting based on popularity, we can."
        ],
        [
            "Get a good performance so.",
            "Here's this picture again.",
            "We see that the gap between our MLN model and the popularity baseline is the biggest in the most difficult levels.",
            "And they sort of merge towards the end.",
            "So this is the average case.",
            "What do I mean by that?",
            "So if a set of results are given the same rank score, we put the relevant results in the middle.",
            "So that's what would happen on average.",
            "Now, if you look at the worst case which is.",
            "If the set of results have the same rank score, but we put the relevant one at the bottom, then we see that the performance of popularity actually goes way down.",
            "So if it just doesn't give consistently good performance whereas the MLN maintains."
        ],
        [
            "It's performance.",
            "So some future directions.",
            "I think it would be more interesting.",
            "It would be interesting to incorporate more information into the models and the biggest challenge with this is to find a way to retrieve the relevant information quickly.",
            "It would be interesting to learn more nuanced models, so right now we learn one, wait for every kind of relationship.",
            "But since the relationships are established by shared information.",
            "You know it might be interesting to learn a separate weight for every kind of shared keyword.",
            "Well, we actually tried that and this doesn't give better results.",
            "So we think that it might be useful to try to cluster keywords based on how good they are at relating.",
            "Sessions and finally it would be interesting to try to.",
            "Incorporate some kind of more advanced measures of user similarity, such as you know there's a whole literature on similarity measures, or one thing that we thought about this to use the times of the user spent on the page as a indicator of how interested the user was in this page when real."
        ],
        [
            "Meeting people, so thank you and I'll be happy to answer questions.",
            "Yes.",
            "I'm wondering if you could stab Lish.",
            "Place.",
            "Seeing how far you are or how close you are to that is sort of.",
            "Yeah, so the literature with where people use long history.",
            "So I think what they tend to find is that what really helps you is you know that this person search for the searches for this and they click on that and the next time they search for this you give them that thing that they clicked on the previous time.",
            "So I don't know if that would be a fair thing to compare to.",
            "It would be interesting to know how, how well you can do, but then people are erratic in their behavior and also.",
            "For the same kind of information, neither various sources, so I'm really not sure how to get a good upper bound on how well you can do, and certainly you can do better than what we currently have.",
            "But yeah, I don't know how much better.",
            "Thanks again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now come to learn something about how to disambiguate.",
                    "label": 0
                },
                {
                    "sent": "Thanks everyone for coming.",
                    "label": 0
                },
                {
                    "sent": "I am dilana Michelle Cuva and this is joint work with my former PhD advisor, Ray Mooney.",
                    "label": 0
                },
                {
                    "sent": "When I was a grad student at the University of Texas at Austin, I graduated last month, so now I'm a D and postdoc at the universe.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Maryland College Park.",
                    "label": 0
                },
                {
                    "sent": "So I'll start with the problem that we're addressing in this work.",
                    "label": 0
                },
                {
                    "sent": "So web query disambiguation is when a user goes to search engine and enters a potentially ambiguous search string, such as the word scrubs here, which could mean either a popular TV show popular in the US at least, and it could also mean it could also mean the kind of uniform that doctors were in hospitals, and the question is when the user enters that query.",
                    "label": 1
                },
                {
                    "sent": "Which of these two meanings is the intent?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Did one, and not surprisingly, that's a very widely studied problem.",
                    "label": 0
                },
                {
                    "sent": "I'm listing just a few references here, but there are a lot more and what seems to be a common approach behind this work is that typically these approaches build a user profile from long history of that particular users, so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which is.",
                    "label": 0
                },
                {
                    "sent": "So there are various concerns with that.",
                    "label": 0
                },
                {
                    "sent": "One is privacy concerns so many of you may have heard about the AOL data release when some reporters from the New York Times identified one of the searchers just based on her searches and they went and interviewed her, wrote an article, and it became a big scandal.",
                    "label": 1
                },
                {
                    "sent": "Then their papers like this one Googling, considered harmful that argued that the types of information recorded by search engines about users constitute a serious infraction on there.",
                    "label": 0
                },
                {
                    "sent": "Privacy.",
                    "label": 0
                },
                {
                    "sent": "There are also pragmatic concerns.",
                    "label": 0
                },
                {
                    "sent": "So how to store and protect this data?",
                    "label": 0
                },
                {
                    "sent": "How to identify users?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Across sessions and so on.",
                    "label": 0
                },
                {
                    "sent": "So what we propose to do in this paper is to base personalization only on very short glimpses of user search activities that are captured.",
                    "label": 1
                },
                {
                    "sent": "In brief, short term sessions, and I'd like to emphasize that we do not assume across session user identifiers, so we cannot figure out that the session of the this user today is.",
                    "label": 0
                },
                {
                    "sent": "So this user today of the session is the same user as some session yesterday or so, and we cannot.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just to put a number on this, what do we mean by short-term session?",
                    "label": 0
                },
                {
                    "sent": "This is what our test data looks like on the X axis which is in log scale.",
                    "label": 0
                },
                {
                    "sent": "We have the number of search queries that were issued in the session before the ambiguous query was issued an on the Y axis we have how many sessions we have with that number.",
                    "label": 1
                },
                {
                    "sent": "So we see that most of our test sessions the user issued three searches and then the 4th search was an ambiguous query and we're trying from this very small amount of information.",
                    "label": 0
                },
                {
                    "sent": "To provide some personal.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nation.",
                    "label": 0
                },
                {
                    "sent": "So a natural question is whether this is enough info for anything meaningful.",
                    "label": 1
                },
                {
                    "sent": "So I have anecdotal evidence that it is these are two sessions of two different users.",
                    "label": 0
                },
                {
                    "sent": "First one search for some radio station.",
                    "label": 0
                },
                {
                    "sent": "Then for this crossing, which also turns out to be a radio station and then for scrubs and then the second person searched for Huntsville Hospital, then for eBay and then for Scrubs.",
                    "label": 0
                },
                {
                    "sent": "And so we can guess that the first person is.",
                    "label": 0
                },
                {
                    "sent": "A generic sort of user who is most likely interested in the TV show where's the?",
                    "label": 0
                },
                {
                    "sent": "Second Searcher is likely trying to find medical uniforms, and in this case it turns out that we are right.",
                    "label": 0
                },
                {
                    "sent": "The this was a Click to the TV show, where is the other one?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So Click to the uniform space.",
                    "label": 0
                },
                {
                    "sent": "So the there's one paper as far as we are aware that this more closely related to what we're trying to do here.",
                    "label": 0
                },
                {
                    "sent": "They also make an assumption of short sessions, but their approach is better suited for a specialized search engine, so they run their experiments on a search of computer science for computer science literature store.",
                    "label": 1
                },
                {
                    "sent": "So it's very specific sort of search words.",
                    "label": 0
                },
                {
                    "sent": "We would like something that will work with the general type.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Search so the main challenge challenge then is to find a way to harness this small amount of potentially noisy information and be able to personalize the experience of users and the key to our approach is to exploit the relationships among sessions you.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "1000 queries, so here's a picture of how we imagine this.",
                    "label": 0
                },
                {
                    "sent": "So this in the middle is the current session, and we're viewing it as being related to a whole bunch of other sessions that are similarly short, and we don't know whether or not well they're most likely not belonging to the same user.",
                    "label": 0
                },
                {
                    "sent": "So what are these relationships?",
                    "label": 0
                },
                {
                    "sent": "Well, we established the relationships by shared information such as queries or clicks that are themselves related.",
                    "label": 1
                },
                {
                    "sent": "And how are queries or clicks related will very simply, querystring in URL's are related by sharing identical keywords or by being themselves identical and I so this allows us to establish various kinds of relationships between the sessions and then any two sessions can be related also in a variety of ways.",
                    "label": 0
                },
                {
                    "sent": "And there's one more kind of relationship that we can observe.",
                    "label": 0
                },
                {
                    "sent": "So for each ambiguous query there is a set of possible results that could be clicked, and this set.",
                    "label": 0
                },
                {
                    "sent": "These results are also related among each other, because you know if the person picks one of the results, maybe they're not interested in the other one, so this establishes some sort of relationship.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Among those so, to overcome the sparsity in any information in any individual session, we need a technique that can effectively combine this diverse, diverse set of relationships that exist among the among the sessions and at least to us, this sounded like a statistical relational learning problem, and if you're not familiar with the area, there's an excellent compilation of papers in this book.",
                    "label": 1
                },
                {
                    "sent": "Is it bag it or it ask are we?",
                    "label": 1
                },
                {
                    "sent": "Use one particular model called the Markov Logic Network, and.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Explain why in just a little bit.",
                    "label": 0
                },
                {
                    "sent": "So for the rest of this talk I'll give some background on Markov logic networks.",
                    "label": 1
                },
                {
                    "sent": "Then I'll give the details of our model and I will present some experiments on real search engine data which was provided to us by Microsoft Research and now finish with.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "After work.",
                    "label": 0
                },
                {
                    "sent": "So Markov logic networks were introduced by Richardson and Domingos, and quite simply they consist of a set of weighted 1st order formulas and the intuition behind this model is that the larger the weight of a clause, the greater the penalty for not satisfying a grounding of this clause.",
                    "label": 1
                },
                {
                    "sent": "So we can view this clauses as some sort of relational features that can be then grounded for each particular instance of a problem.",
                    "label": 0
                },
                {
                    "sent": "And you know these features can provide some, possibly also contradicting evidence that can then be integr.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It is in the principled way and this is how this is done.",
                    "label": 0
                },
                {
                    "sent": "So say we have set of unknown query atoms Q.",
                    "label": 1
                },
                {
                    "sent": "And the set of evidence atoms E and we want to know what's the probability of the query atoms given the evidence.",
                    "label": 0
                },
                {
                    "sent": "And this is the answer according to Merlin, if we just focused on the top part for now, we have an exponential over of the sum over all formulas in the model, where for each formula we are multiplying the weight of that formula Times the number of satisfied groundings of this formula on these truth assignments.",
                    "label": 0
                },
                {
                    "sent": "And in the denominator is just a normalizing constant where we're summing.",
                    "label": 0
                },
                {
                    "sent": "Over every possible value of Q, so it gives a principled way of.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Calculating this.",
                    "label": 0
                },
                {
                    "sent": "And an advantage of using imbalances that there are a lot of algorithms that have already been introduced in the literature, and they're also implemented in this alchemy software package, which is what we used.",
                    "label": 1
                },
                {
                    "sent": "In particular, we used the weight learning algorithm called contrastive divergent, which was adapted for millions by Loudon Domingos, and we had to adapt the code so that it can deal with streaming data because our data was too large to fit in memory and for inference we use the MCC.",
                    "label": 0
                },
                {
                    "sent": "Algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which gives very good performance.",
                    "label": 0
                },
                {
                    "sent": "So this is what we want to do.",
                    "label": 0
                },
                {
                    "sent": "We have an ambiguous query for which we somehow get a set of possible results.",
                    "label": 0
                },
                {
                    "sent": "Maybe we get it from a search engine.",
                    "label": 0
                },
                {
                    "sent": "Then we have our Markov logic network which assigns a probability to each of the possible results, and this is basically what is the probability that the user will be interested in clicking on that result.",
                    "label": 0
                },
                {
                    "sent": "And then we use these probabilities to rerank or to rank the.",
                    "label": 0
                },
                {
                    "sent": "Possible results.",
                    "label": 0
                },
                {
                    "sent": "So where do we get this MLN 4 from?",
                    "label": 0
                },
                {
                    "sent": "Well, we hand code a set of formulas that captured the irregularities in the domain and the challenge here is to define an effective set of relations and then once we have these formulas, we use the data to learn a set of weights from them.",
                    "label": 1
                },
                {
                    "sent": "And the challenge here is this.",
                    "label": 0
                },
                {
                    "sent": "This is very noisy data as you can imagine when people search on the web, they are not necessarily following any strict rules.",
                    "label": 0
                },
                {
                    "sent": "So just to mention that you know this is a standard methodology for using ambulance to address a particular problem, just hand code some rules and learn some weights and it's very convenient, especially if you're comfortable with first order logic to do that, because you know if you have some idea about how this domain works, you can express that in the 1st order logic using some possibly contradictory rules, and then by learning the weights on these from the data you can.",
                    "label": 0
                },
                {
                    "sent": "As you know, have a way of trading off these rules and can.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Finding them.",
                    "label": 0
                },
                {
                    "sent": "So these are the specific relationships between sessions that we use, so this is the in orange.",
                    "label": 1
                },
                {
                    "sent": "We have the current session and decent purple our previous sessions, so they share various kinds of information such as cures between searches, keywords between clicks between clicks and searches between searches and clicks and also shared clicks.",
                    "label": 0
                },
                {
                    "sent": "So it's very basic kinds of things that are shared that established relationships.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Between Sessions and based on these relationships, we define the clauses in the Markov logic network.",
                    "label": 0
                },
                {
                    "sent": "So the most important set probably are the collaborative clauses.",
                    "label": 1
                },
                {
                    "sent": "So they say that the current user will pick the result that was chosen by some related sessions, and the sessions could be related either by shared click or by shared keywords in some way, and for every way you can share keywords we have a diff.",
                    "label": 1
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Control.",
                    "label": 0
                },
                {
                    "sent": "Then we have a popularity clause that says that the user will choose the result that was chosen by any previous session, regardless of whether it's related or not.",
                    "label": 1
                },
                {
                    "sent": "So the effect of that is that the most popular result gets the highest probability of being clicked, and it may seem like a really stupid rule to have, but actually it's a very strong baseline, as you will see in the results.",
                    "label": 0
                },
                {
                    "sent": "And by the way, I'm just giving you these in English, but if you're interested in seeing the actual formulations in first order logic, I invite you to look at our paper.",
                    "label": 0
                },
                {
                    "sent": "I think it will be.",
                    "label": 0
                },
                {
                    "sent": "It's much easier to present it like this at the top.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then we have these local clauses that say, well, the user will choose the result that shares some sort of information with something that the same user didn't.",
                    "label": 1
                },
                {
                    "sent": "This current session and.",
                    "label": 0
                },
                {
                    "sent": "Well, that doesn't do very well, not surprisingly, because the sessions that we have are very short and you know, it just doesn't.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very well.",
                    "label": 0
                },
                {
                    "sent": "And then we have a balanced clause which says that if the user chooses one of the possible results, they won't choose another one.",
                    "label": 1
                },
                {
                    "sent": "This sets up a competition among the possible results, and pragmatically, the reason why we have it is because it allows us to use the same set of weights across different size problems.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this was the clauses that we defined.",
                    "label": 0
                },
                {
                    "sent": "So now I'll talk about the empirical evaluation.",
                    "label": 1
                },
                {
                    "sent": "So the data was provided by MSR.",
                    "label": 1
                },
                {
                    "sent": "It was collected on their search engine in May 2006, so we use the first 25 days for training the last six days for testing and any session that straddled the testing and training period, we removed it to avoid contamination, so there were two difficulties associated with using this data which were a little bit frustrating.",
                    "label": 0
                },
                {
                    "sent": "So first of all.",
                    "label": 1
                },
                {
                    "sent": "The data doesn't say which queries are ambiguous, and in fact the detecting ambiguity whether something is ambiguous or not.",
                    "label": 1
                },
                {
                    "sent": "It's it's own research area we didn't want to go there.",
                    "label": 0
                },
                {
                    "sent": "Instead, we use this heuristic.",
                    "label": 0
                },
                {
                    "sent": "We just use the Dimas hierarchy of pages, and if for that particular query string there were at least two clicks somewhere where they fell in different demote categories, we said, well, OK, this is an ambiguous query.",
                    "label": 0
                },
                {
                    "sent": "The other problem with the data was that it only lists the results were actually clicked by the user, but we don't know what the user saw on the web page, so that was probably more serious because we didn't know how to run the experiments.",
                    "label": 0
                },
                {
                    "sent": "But So what we did was we assume that the user saw all the all the results that were clicked by someone somewhere in the data following that exact query string.",
                    "label": 1
                },
                {
                    "sent": "So this is a source of noise that this kind of unfortunate, but we didn't.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A better way to control it.",
                    "label": 0
                },
                {
                    "sent": "So these are the models that we tested we used, we tried random re ranking then we have the popularity baseline that basically ranks everything based on the number of people in on the test data that picked that result following that query.",
                    "label": 0
                },
                {
                    "sent": "Then we implemented some standard collaborative filtering baselines where basically the preferences of the set of the most closely related.",
                    "label": 1
                },
                {
                    "sent": "Mostly some of the set of the most closely similar sessions are are taken into account.",
                    "label": 0
                },
                {
                    "sent": "So based on how the similarities calculated, we have two varieties, the Pearson in the cosine, and then we experimented with these three combinations of the clauses and so the results I'll show today are just this first ones collaborative plus popularity plus balance.",
                    "label": 0
                },
                {
                    "sent": "All the clauses without the local ones.",
                    "label": 0
                },
                {
                    "sent": "The other ones are in the.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Paper.",
                    "label": 0
                },
                {
                    "sent": "We use two measures for evaluation.",
                    "label": 0
                },
                {
                    "sent": "The area under the Arosi curve and also the area under the Precision recall curve.",
                    "label": 1
                },
                {
                    "sent": "So the RC curve corresponds to mean average through negative rate and I won't go over this.",
                    "label": 0
                },
                {
                    "sent": "But if you're interested we can talk about it at the poster and the area under precision called curve corresponds to the mean average precision.",
                    "label": 0
                },
                {
                    "sent": "So in the paper we have both of these with.",
                    "label": 0
                },
                {
                    "sent": "Here I'll focus on the error.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "See curve because I think it has a really nice in two different reputation.",
                    "label": 0
                },
                {
                    "sent": "So if we assume that the user starts scanning the set of the page results from the top and stops as soon as she finds irrelevant result, the AUC arosi captures what proportion of the irrelevant results are not seen by the user.",
                    "label": 1
                },
                {
                    "sent": "So that's I think nice, and so here's a picture.",
                    "label": 0
                },
                {
                    "sent": "You know if this is the relevant result, we put it second.",
                    "label": 0
                },
                {
                    "sent": "Well, what proportion came?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "After it.",
                    "label": 0
                },
                {
                    "sent": "So this is, this is what the results look like.",
                    "label": 0
                },
                {
                    "sent": "Random does right around .5.",
                    "label": 0
                },
                {
                    "sent": "That's another nice thing about the AUC.",
                    "label": 0
                },
                {
                    "sent": "Arosi the collaborative Pearson baseline did as good as random using using cosine similarity improved a little bit, then the popularity baseline is really good.",
                    "label": 0
                },
                {
                    "sent": "And this is what we get by combining various kinds of information in the MLN model.",
                    "label": 0
                },
                {
                    "sent": "And, you know, this may not seem like very large differences.",
                    "label": 0
                },
                {
                    "sent": "But they're all.",
                    "label": 0
                },
                {
                    "sent": "So whenever you see a difference, it's highly statistically significant and we are very encouraged, actually by these results because the data is very noisy.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it's interesting to kind of analyze the results a little bit more based on the difficulty of disambiguating a particular query.",
                    "label": 0
                },
                {
                    "sent": "So some queries are easier than others, and the way we measure this easiness is by the KL divergences between the distribution over possible results for the for the query and the uniform distribution.",
                    "label": 1
                },
                {
                    "sent": "So if the if the possible results are distributed roughly uniformly, then that's a very difficult ambiguous query for the popularity baseline because there is no clear winner.",
                    "label": 0
                },
                {
                    "sent": "Whereas if we have one clear winner just predicting based on popularity, we can.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Get a good performance so.",
                    "label": 0
                },
                {
                    "sent": "Here's this picture again.",
                    "label": 0
                },
                {
                    "sent": "We see that the gap between our MLN model and the popularity baseline is the biggest in the most difficult levels.",
                    "label": 0
                },
                {
                    "sent": "And they sort of merge towards the end.",
                    "label": 0
                },
                {
                    "sent": "So this is the average case.",
                    "label": 0
                },
                {
                    "sent": "What do I mean by that?",
                    "label": 0
                },
                {
                    "sent": "So if a set of results are given the same rank score, we put the relevant results in the middle.",
                    "label": 0
                },
                {
                    "sent": "So that's what would happen on average.",
                    "label": 0
                },
                {
                    "sent": "Now, if you look at the worst case which is.",
                    "label": 1
                },
                {
                    "sent": "If the set of results have the same rank score, but we put the relevant one at the bottom, then we see that the performance of popularity actually goes way down.",
                    "label": 0
                },
                {
                    "sent": "So if it just doesn't give consistently good performance whereas the MLN maintains.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's performance.",
                    "label": 0
                },
                {
                    "sent": "So some future directions.",
                    "label": 0
                },
                {
                    "sent": "I think it would be more interesting.",
                    "label": 0
                },
                {
                    "sent": "It would be interesting to incorporate more information into the models and the biggest challenge with this is to find a way to retrieve the relevant information quickly.",
                    "label": 1
                },
                {
                    "sent": "It would be interesting to learn more nuanced models, so right now we learn one, wait for every kind of relationship.",
                    "label": 1
                },
                {
                    "sent": "But since the relationships are established by shared information.",
                    "label": 0
                },
                {
                    "sent": "You know it might be interesting to learn a separate weight for every kind of shared keyword.",
                    "label": 0
                },
                {
                    "sent": "Well, we actually tried that and this doesn't give better results.",
                    "label": 0
                },
                {
                    "sent": "So we think that it might be useful to try to cluster keywords based on how good they are at relating.",
                    "label": 0
                },
                {
                    "sent": "Sessions and finally it would be interesting to try to.",
                    "label": 1
                },
                {
                    "sent": "Incorporate some kind of more advanced measures of user similarity, such as you know there's a whole literature on similarity measures, or one thing that we thought about this to use the times of the user spent on the page as a indicator of how interested the user was in this page when real.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Meeting people, so thank you and I'll be happy to answer questions.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "I'm wondering if you could stab Lish.",
                    "label": 0
                },
                {
                    "sent": "Place.",
                    "label": 0
                },
                {
                    "sent": "Seeing how far you are or how close you are to that is sort of.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so the literature with where people use long history.",
                    "label": 0
                },
                {
                    "sent": "So I think what they tend to find is that what really helps you is you know that this person search for the searches for this and they click on that and the next time they search for this you give them that thing that they clicked on the previous time.",
                    "label": 0
                },
                {
                    "sent": "So I don't know if that would be a fair thing to compare to.",
                    "label": 0
                },
                {
                    "sent": "It would be interesting to know how, how well you can do, but then people are erratic in their behavior and also.",
                    "label": 0
                },
                {
                    "sent": "For the same kind of information, neither various sources, so I'm really not sure how to get a good upper bound on how well you can do, and certainly you can do better than what we currently have.",
                    "label": 0
                },
                {
                    "sent": "But yeah, I don't know how much better.",
                    "label": 0
                },
                {
                    "sent": "Thanks again.",
                    "label": 0
                }
            ]
        }
    }
}