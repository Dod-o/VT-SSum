{
    "id": "ri37xmcrbdrmvggsamgceldnw323hucv",
    "title": "REGO: Rank-based estimation of Renyi information using Euclidean graph optimization",
    "info": {
        "author": [
            "Barnab\u00e1s P\u00f3czos, Machine Learning Department, School of Computer Science, Carnegie Mellon University"
        ],
        "published": "June 14, 2010",
        "recorded": "May 2010",
        "category": [
            "Top->Mathematics->Graph Theory"
        ]
    },
    "url": "http://videolectures.net/aistats2010_poczos_rrbe/",
    "segmentation": [
        [
            "Today I'm going to talk about.",
            "Listen.",
            "Cool.",
            "Very cool.",
            "Quite.",
            "Just a few minutes.",
            "Death.",
            "Can you hear me?",
            "OK, so I'm going to talk about an algorithm for estimating dependence and mutual information and record its algorithm regular to refer this acronym.",
            "And because regular is ancient Hungarian word, it's so ancient that we don't really know what it means.",
            "But maybe it's something like magic song or pray for our guards or 4.",
            "The size of our anchor stores.",
            "And in many times this magic deer is involved in our songs so."
        ],
        [
            "Today I'm going to talk about dependence estimation and mutual information estimation.",
            "So first I will briefly tell you why dependence estimation is important in machine learning.",
            "And I will provide a very brief background on information theory and entropy, and after that I will talk about something totally different.",
            "I will talk about traveling salesman problems, minimum spanning trees, K nearest neighborhood graph and popular transformation and we will see if we marry these two different topics then we will be able to provide consistent, robust mutual information estimation algorithm."
        ],
        [
            "So first question, who cares about dependence?",
            "Why do we want to estimate dependence?",
            "And I don't have time to give a full description.",
            "I just want to mention that it's important in both supervised learning and unsupervised learning.",
            "So probably a good example to explain this is.",
            "If you consider or think about feature selection, so say in supervised learning.",
            "If you have lots of features you want to select those features with which you can predict the target value right?",
            "So which have some dependence with the target values and in unsupervised learning you want to know which features are dependent which are independent of each other or when you analyze stock markets.",
            "Then you might want to know which stock prices are dependent on each other and which are independent.",
            "And there are lots of other applications from boosting clustering.",
            "We are active learning to migrated data, processing, image registration, independent component analysis, independent subsystem notices and so on.",
            "So dependencies in important the next question."
        ],
        [
            "What is dependence?",
            "And here we will follow the definition of Alfred any who came up with the six axioms.",
            "He said that dependence is nothing else, just the function over random variables and it's always non negative.",
            "It's zero if and only if the marginal variables are independent from each other and it has maximum value.",
            "If there's a deterministic relationship between the marginals, it should be invariant to one to one transformation and should be invented.",
            "The permutation of the marginals.",
            "Chianti.",
            "I suggested this as a measure of information.",
            "This ioffer because it satisfied that six axioms and.",
            "Also, it's a generalization of the standard Shannon formation.",
            "Later we will need the rain is entropy defined by this quantity and we will see that it's easy to see that it's really a generalization of the Shannon entropy denoted by H and the rings and information is the generalization of the Shannon information.",
            "That is, if our converges to one, then the Rangers information converges to Shannon information and the same for the entropy."
        ],
        [
            "So let's just do the manual.",
            "The definition of the Shannon.",
            "Information can be sent over there.",
            "And Shannon, entropy is defined by this guy.",
            "OK, so now I can formalize our problem.",
            "We have an IID sample from a distribution.",
            "Using this sample.",
            "We want to estimate this quantity.",
            "Here the rain is information."
        ],
        [
            "So the next question, how can we do that?",
            "So one obvious approach would be that first we estimate the density is right with some density estimators using histograms of channel density estimators and we plug in those estimators into those integral formulas.",
            "But the problem with this approach that many times these estimators have tunable parameters.",
            "Then you need cross validation for model selection.",
            "So for example, if you use histograms then you have to join the bandwidth.",
            "We don't want to do that and the other problem that the density function is annoyance parameter.",
            "We are not interested in this case the DHA city so we don't really want to estimate that is the same thing that you want to estimate the mean or the variance is just the functional of the density.",
            "But you don't start estimating the density is.",
            "If you want to estimate the mean you just use the empirical mean.",
            "And here we want something similar.",
            "Direct approach.",
            "OK, and now I don't."
        ],
        [
            "About something different, travelling salesman problems.",
            "So Once Upon a time 50 years ago people were interested solving traveling salesman problem and they realize that it's a difficult problem.",
            "So they asked a different question.",
            "OK, we cannot solve this problem, but at least can we say something approximate approximately if we have lots of cities?",
            "And being without an intimacy as this question, if we have, we assume that the cities in the traveling salesman problem have uniform distributions on the unit box then.",
            "How the length of the traveling salesman problem behaves when we have lots of samples and they found that if you normalize oops, the traveling salesman problem by square root of N, then this will almost surely confessed to a positive number beta.",
            "And they got excited.",
            "And then they realize that, oh, it folds in higher dimension as well.",
            "But you have to normalize by a different factor here.",
            "And after that they got more excited and asked this question.",
            "What happens if instead of uniform distribution you have another underlying distribution with density and they realize that almost everything remains the change the same?",
            "We will have the same.",
            "Combat just except we will have this multiplicative factor here, but this data is the same as before that we had on uniform distribution.",
            "It doesn't depend on on the density.",
            "20 years later, people realized that this property holds not only for traveling salesman problem, but for minimum spanning trees, minimum matchings, and they even proved.",
            "Almost threw 40 nearest neighborhood after that and after that they asked a very weird question.",
            "What happens if you modify the Euclidean metric?",
            "Do this magic the Euclidean metric to the power of P, which is not a metric anymore, it's P is larger than a month.",
            "They didn't want to estimate entropy, they just asked this crazy question and they realize that everything remains almost the same, except that you have to replace this one to Pete.",
            "And now the question, how can we use this for entropy estimation?",
            "And just remember that the ranges entropy definition was this.",
            "So what we have to do, we just have to set P to that value.",
            "Then this equals offer and then we can estimate this guy and we will have a consistent almost surely.",
            "Consistent training."
        ],
        [
            "Destination Did I just summarized this on minimum spending freeze?",
            "So if you want to estimate ranges are entropy, you set P to D minus Alpha U.",
            "Choose your favorite graph optimization problem.",
            "Travelling salesman problem with minimum spanning tree or a nearest neighborhood graph.",
            "So this problem with this ugly metric and then you will have an estimator for the Angels.",
            "School.",
            "One problem it's very sensitive to outliers.",
            "So imagine that I move.",
            "This point, somewhere over there.",
            "Then, if you have a very long edge, it can be arbitrary wrong and totally it can totally ruin our statistics so."
        ],
        [
            "It's not robust to outliers.",
            "OK, we know how to estimate ranges of entropy, but we are not interested in that.",
            "We are interested in estimating the ranges in formation and we want a robust methods for that.",
            "But can we do?",
            "And here the invariant strict and a couple of transformations will save us.",
            "But we have to notice that if we have a random variable D dimensional random variable and we apply monotone transformation jeevanantham GD, then the information won't change.",
            "That is, the transformed variables that will have the same training formation as the original operable.",
            "OK, the other thing we have to notice that if the marginals of the transformed variables are uniform, then these terms are just one and they cancel out.",
            "We don't have these guys and what we got the transformed variable will have its mutual in renys information will be just the negative ranges entropy.",
            "OK.",
            "So now but."
        ],
        [
            "And it is a monotone transformation with which you can transform your random variable to uniform to everyone by random variable with uniform distribution.",
            "But is that?",
            "It's you know, from probability theory one over that if you have a random variable and it has a F CDF, then if you apply its own CDF to that random variable, you will get uniform distribution.",
            "And that's what we need, and that's quite popular transformation.",
            "You do this transformation for all marginal variables.",
            "OK, so we know that we had.",
            "We want plastic at this quantity and after popular transformation we just have to estimate the negentropy of the transformed variable and we know how to estimate ranges cantrip in.",
            "So we're almost done.",
            "One problem we don't know the CDF.",
            "The distribution function of the variables, but it's only a 1 dimensional problem.",
            "We can estimate that using the."
        ],
        [
            "Empirical distribution function and that is called empirical transformation.",
            "So what we do for each marginal so say for marginal G We just ordered the points and if a PT has wrong I, then the corresponding empirical distribution VI over N. Distribution function."
        ],
        [
            "And we do this transformation for or marginal.",
            "So if you have the point XT, then it's true.",
            "Popular transformation would be something like that, but.",
            "The value of the empirical copula transformation and it will place the points on a grid.",
            "Like here.",
            "OK."
        ],
        [
            "And so this is our algorithm.",
            "It's pointless to accept.",
            "First, we calculate the empirical.",
            "Popular Angie was empirical copula transformation and then on this empirically then on this transform points we use the previously introduced trainee entropy calculation and then."
        ],
        [
            "Our main result is this.",
            "That if the dimension is larger or at least three and a half is between 1/2 and one.",
            "And you use for entropy estimation.",
            "Thank salesman problem, minimum spanning trees, minimum matching, caners neighborhood graphs, then the previously introduced algorithm will converge almost surely to the Rangers information.",
            "So we don't have results for other Alpha values and we don't have theoretical result for the case Monday equals 2.",
            "Another thing that we have to note that we used ranks only in this case for estimating the range information so that they can message that using statistics only.",
            "You can consistently estimate the ranges in formation.",
            "You don't have to use the original values."
        ],
        [
            "Here is a bit on the robustness.",
            "If you have an outlier arbitrary.",
            "In this case, the effect of the outlier cannot be arbitrary.",
            "So if you have enough sample points, then the effect of an outlier X can be negligible.",
            "If you have enough sample one, it's not the case if you don't use popular transformation."
        ],
        [
            "OK, and finally I would like to show some.",
            "Numerical results, so here in this case.",
            "I had 10 dimensional uniform distribution.",
            "Of course the mutual information is zero in this case, and if you ski nearest neighborhood graphs for entropy estimation or minimum spanning trees for entropy estimation and use popular transformation, then you can see that the regular algorithm converge to the true mutual information.",
            "In this other case I used 10 dimensional Gaussian distribution.",
            "Here you can see the sample numbers.",
            "And you can see that for both methods, the minimum spanning tree algorithm using copula method and caners neighborhood algorithms using popular methods converge to the true mutual information and I would like to show an application."
        ],
        [
            "Independent subspace analysis.",
            "So this is a generalization of independent component analysis.",
            "I choose three 2 dimensional subspaces denoted by this AB&C and I sampled 2000 points independently from these subspaces and they were hidden for our algorithm.",
            "We just formed built a matrix S using these three.",
            "2 dimensional points, so they formed a 6 by 2000.",
            "Dimensional matrix and mixed these hidden points by a random 6 by 6 mixing matrix and the offset.",
            "This quantity X.",
            "And here you can see, but the algorithm offset and the task was that using this observation these 2000 points from this mixture we had to recover the original subspaces."
        ],
        [
            "And delegate invert like this that we were looking for a W so-called mixing matrix with which WAS that is WX, which is.",
            "Why would be the recovered subspaces and we wanted to minimize the mutual information.",
            "Between here, between the subspaces, and here I am trying to show a movie how our algorithm works.",
            "This matrix here is W * A.",
            "So when our algorithm works perfectly then this WAW times a matrix would be a block permutation matrix and these.",
            "Figure sure how I would algorithm trying to separate the subspaces.",
            "OK, so now it's starting to look for the best value, with which you can minimize the mutual information.",
            "Here you can see WA again.",
            "And these are the recovered subspaces.",
            "OK, and you can see that finally we got the block permutation matrix here and we could recover these subspaces, minimizing the mutual information."
        ],
        [
            "So the take home message is that.",
            "Using graph optimization methods and popular transformation, if you marry these two things, you can get efficient, robust, almost fully consistent ranges, mutual information estimation method.",
            "And finally I just want to show."
        ],
        [
            "An example for this particular song.",
            "Oh, and I translated the lyrics.",
            "The screen is too big.",
            "Thanks."
        ],
        [
            "Very interesting time.",
            "So you need to solve.",
            "How to correctly you have to solve these graph problems yet to get estimated he's got problems have certain computational exactly yes.",
            "Does it allow us some conclusion about the general difficulty of obtaining empirical estimates of the radius.",
            "So you don't want to use probably traveling salesman problem right?",
            "To solve this problem that you can use Skinner's neighborhood graphs or minimum spending please.",
            "And the complexity there, it's N squared or N log N and that's the computational complexity.",
            "It's another issue, how other question?",
            "First, the the estimation convex to the true values and we have some results for that as well, and we can discuss that offline if you abandon thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Today I'm going to talk about.",
                    "label": 0
                },
                {
                    "sent": "Listen.",
                    "label": 0
                },
                {
                    "sent": "Cool.",
                    "label": 0
                },
                {
                    "sent": "Very cool.",
                    "label": 0
                },
                {
                    "sent": "Quite.",
                    "label": 0
                },
                {
                    "sent": "Just a few minutes.",
                    "label": 0
                },
                {
                    "sent": "Death.",
                    "label": 0
                },
                {
                    "sent": "Can you hear me?",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm going to talk about an algorithm for estimating dependence and mutual information and record its algorithm regular to refer this acronym.",
                    "label": 0
                },
                {
                    "sent": "And because regular is ancient Hungarian word, it's so ancient that we don't really know what it means.",
                    "label": 0
                },
                {
                    "sent": "But maybe it's something like magic song or pray for our guards or 4.",
                    "label": 0
                },
                {
                    "sent": "The size of our anchor stores.",
                    "label": 0
                },
                {
                    "sent": "And in many times this magic deer is involved in our songs so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Today I'm going to talk about dependence estimation and mutual information estimation.",
                    "label": 1
                },
                {
                    "sent": "So first I will briefly tell you why dependence estimation is important in machine learning.",
                    "label": 1
                },
                {
                    "sent": "And I will provide a very brief background on information theory and entropy, and after that I will talk about something totally different.",
                    "label": 0
                },
                {
                    "sent": "I will talk about traveling salesman problems, minimum spanning trees, K nearest neighborhood graph and popular transformation and we will see if we marry these two different topics then we will be able to provide consistent, robust mutual information estimation algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first question, who cares about dependence?",
                    "label": 1
                },
                {
                    "sent": "Why do we want to estimate dependence?",
                    "label": 0
                },
                {
                    "sent": "And I don't have time to give a full description.",
                    "label": 0
                },
                {
                    "sent": "I just want to mention that it's important in both supervised learning and unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "So probably a good example to explain this is.",
                    "label": 1
                },
                {
                    "sent": "If you consider or think about feature selection, so say in supervised learning.",
                    "label": 0
                },
                {
                    "sent": "If you have lots of features you want to select those features with which you can predict the target value right?",
                    "label": 0
                },
                {
                    "sent": "So which have some dependence with the target values and in unsupervised learning you want to know which features are dependent which are independent of each other or when you analyze stock markets.",
                    "label": 0
                },
                {
                    "sent": "Then you might want to know which stock prices are dependent on each other and which are independent.",
                    "label": 1
                },
                {
                    "sent": "And there are lots of other applications from boosting clustering.",
                    "label": 0
                },
                {
                    "sent": "We are active learning to migrated data, processing, image registration, independent component analysis, independent subsystem notices and so on.",
                    "label": 1
                },
                {
                    "sent": "So dependencies in important the next question.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What is dependence?",
                    "label": 0
                },
                {
                    "sent": "And here we will follow the definition of Alfred any who came up with the six axioms.",
                    "label": 0
                },
                {
                    "sent": "He said that dependence is nothing else, just the function over random variables and it's always non negative.",
                    "label": 0
                },
                {
                    "sent": "It's zero if and only if the marginal variables are independent from each other and it has maximum value.",
                    "label": 0
                },
                {
                    "sent": "If there's a deterministic relationship between the marginals, it should be invariant to one to one transformation and should be invented.",
                    "label": 0
                },
                {
                    "sent": "The permutation of the marginals.",
                    "label": 0
                },
                {
                    "sent": "Chianti.",
                    "label": 0
                },
                {
                    "sent": "I suggested this as a measure of information.",
                    "label": 0
                },
                {
                    "sent": "This ioffer because it satisfied that six axioms and.",
                    "label": 0
                },
                {
                    "sent": "Also, it's a generalization of the standard Shannon formation.",
                    "label": 0
                },
                {
                    "sent": "Later we will need the rain is entropy defined by this quantity and we will see that it's easy to see that it's really a generalization of the Shannon entropy denoted by H and the rings and information is the generalization of the Shannon information.",
                    "label": 0
                },
                {
                    "sent": "That is, if our converges to one, then the Rangers information converges to Shannon information and the same for the entropy.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's just do the manual.",
                    "label": 0
                },
                {
                    "sent": "The definition of the Shannon.",
                    "label": 0
                },
                {
                    "sent": "Information can be sent over there.",
                    "label": 0
                },
                {
                    "sent": "And Shannon, entropy is defined by this guy.",
                    "label": 1
                },
                {
                    "sent": "OK, so now I can formalize our problem.",
                    "label": 0
                },
                {
                    "sent": "We have an IID sample from a distribution.",
                    "label": 0
                },
                {
                    "sent": "Using this sample.",
                    "label": 0
                },
                {
                    "sent": "We want to estimate this quantity.",
                    "label": 0
                },
                {
                    "sent": "Here the rain is information.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the next question, how can we do that?",
                    "label": 1
                },
                {
                    "sent": "So one obvious approach would be that first we estimate the density is right with some density estimators using histograms of channel density estimators and we plug in those estimators into those integral formulas.",
                    "label": 0
                },
                {
                    "sent": "But the problem with this approach that many times these estimators have tunable parameters.",
                    "label": 0
                },
                {
                    "sent": "Then you need cross validation for model selection.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you use histograms then you have to join the bandwidth.",
                    "label": 0
                },
                {
                    "sent": "We don't want to do that and the other problem that the density function is annoyance parameter.",
                    "label": 0
                },
                {
                    "sent": "We are not interested in this case the DHA city so we don't really want to estimate that is the same thing that you want to estimate the mean or the variance is just the functional of the density.",
                    "label": 0
                },
                {
                    "sent": "But you don't start estimating the density is.",
                    "label": 0
                },
                {
                    "sent": "If you want to estimate the mean you just use the empirical mean.",
                    "label": 0
                },
                {
                    "sent": "And here we want something similar.",
                    "label": 0
                },
                {
                    "sent": "Direct approach.",
                    "label": 0
                },
                {
                    "sent": "OK, and now I don't.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About something different, travelling salesman problems.",
                    "label": 0
                },
                {
                    "sent": "So Once Upon a time 50 years ago people were interested solving traveling salesman problem and they realize that it's a difficult problem.",
                    "label": 0
                },
                {
                    "sent": "So they asked a different question.",
                    "label": 0
                },
                {
                    "sent": "OK, we cannot solve this problem, but at least can we say something approximate approximately if we have lots of cities?",
                    "label": 0
                },
                {
                    "sent": "And being without an intimacy as this question, if we have, we assume that the cities in the traveling salesman problem have uniform distributions on the unit box then.",
                    "label": 0
                },
                {
                    "sent": "How the length of the traveling salesman problem behaves when we have lots of samples and they found that if you normalize oops, the traveling salesman problem by square root of N, then this will almost surely confessed to a positive number beta.",
                    "label": 0
                },
                {
                    "sent": "And they got excited.",
                    "label": 0
                },
                {
                    "sent": "And then they realize that, oh, it folds in higher dimension as well.",
                    "label": 0
                },
                {
                    "sent": "But you have to normalize by a different factor here.",
                    "label": 0
                },
                {
                    "sent": "And after that they got more excited and asked this question.",
                    "label": 0
                },
                {
                    "sent": "What happens if instead of uniform distribution you have another underlying distribution with density and they realize that almost everything remains the change the same?",
                    "label": 0
                },
                {
                    "sent": "We will have the same.",
                    "label": 0
                },
                {
                    "sent": "Combat just except we will have this multiplicative factor here, but this data is the same as before that we had on uniform distribution.",
                    "label": 0
                },
                {
                    "sent": "It doesn't depend on on the density.",
                    "label": 0
                },
                {
                    "sent": "20 years later, people realized that this property holds not only for traveling salesman problem, but for minimum spanning trees, minimum matchings, and they even proved.",
                    "label": 0
                },
                {
                    "sent": "Almost threw 40 nearest neighborhood after that and after that they asked a very weird question.",
                    "label": 0
                },
                {
                    "sent": "What happens if you modify the Euclidean metric?",
                    "label": 0
                },
                {
                    "sent": "Do this magic the Euclidean metric to the power of P, which is not a metric anymore, it's P is larger than a month.",
                    "label": 0
                },
                {
                    "sent": "They didn't want to estimate entropy, they just asked this crazy question and they realize that everything remains almost the same, except that you have to replace this one to Pete.",
                    "label": 0
                },
                {
                    "sent": "And now the question, how can we use this for entropy estimation?",
                    "label": 0
                },
                {
                    "sent": "And just remember that the ranges entropy definition was this.",
                    "label": 0
                },
                {
                    "sent": "So what we have to do, we just have to set P to that value.",
                    "label": 0
                },
                {
                    "sent": "Then this equals offer and then we can estimate this guy and we will have a consistent almost surely.",
                    "label": 0
                },
                {
                    "sent": "Consistent training.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Destination Did I just summarized this on minimum spending freeze?",
                    "label": 0
                },
                {
                    "sent": "So if you want to estimate ranges are entropy, you set P to D minus Alpha U.",
                    "label": 0
                },
                {
                    "sent": "Choose your favorite graph optimization problem.",
                    "label": 0
                },
                {
                    "sent": "Travelling salesman problem with minimum spanning tree or a nearest neighborhood graph.",
                    "label": 0
                },
                {
                    "sent": "So this problem with this ugly metric and then you will have an estimator for the Angels.",
                    "label": 0
                },
                {
                    "sent": "School.",
                    "label": 0
                },
                {
                    "sent": "One problem it's very sensitive to outliers.",
                    "label": 1
                },
                {
                    "sent": "So imagine that I move.",
                    "label": 0
                },
                {
                    "sent": "This point, somewhere over there.",
                    "label": 0
                },
                {
                    "sent": "Then, if you have a very long edge, it can be arbitrary wrong and totally it can totally ruin our statistics so.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's not robust to outliers.",
                    "label": 0
                },
                {
                    "sent": "OK, we know how to estimate ranges of entropy, but we are not interested in that.",
                    "label": 0
                },
                {
                    "sent": "We are interested in estimating the ranges in formation and we want a robust methods for that.",
                    "label": 0
                },
                {
                    "sent": "But can we do?",
                    "label": 0
                },
                {
                    "sent": "And here the invariant strict and a couple of transformations will save us.",
                    "label": 0
                },
                {
                    "sent": "But we have to notice that if we have a random variable D dimensional random variable and we apply monotone transformation jeevanantham GD, then the information won't change.",
                    "label": 0
                },
                {
                    "sent": "That is, the transformed variables that will have the same training formation as the original operable.",
                    "label": 0
                },
                {
                    "sent": "OK, the other thing we have to notice that if the marginals of the transformed variables are uniform, then these terms are just one and they cancel out.",
                    "label": 0
                },
                {
                    "sent": "We don't have these guys and what we got the transformed variable will have its mutual in renys information will be just the negative ranges entropy.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So now but.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And it is a monotone transformation with which you can transform your random variable to uniform to everyone by random variable with uniform distribution.",
                    "label": 1
                },
                {
                    "sent": "But is that?",
                    "label": 0
                },
                {
                    "sent": "It's you know, from probability theory one over that if you have a random variable and it has a F CDF, then if you apply its own CDF to that random variable, you will get uniform distribution.",
                    "label": 0
                },
                {
                    "sent": "And that's what we need, and that's quite popular transformation.",
                    "label": 0
                },
                {
                    "sent": "You do this transformation for all marginal variables.",
                    "label": 0
                },
                {
                    "sent": "OK, so we know that we had.",
                    "label": 0
                },
                {
                    "sent": "We want plastic at this quantity and after popular transformation we just have to estimate the negentropy of the transformed variable and we know how to estimate ranges cantrip in.",
                    "label": 0
                },
                {
                    "sent": "So we're almost done.",
                    "label": 0
                },
                {
                    "sent": "One problem we don't know the CDF.",
                    "label": 1
                },
                {
                    "sent": "The distribution function of the variables, but it's only a 1 dimensional problem.",
                    "label": 0
                },
                {
                    "sent": "We can estimate that using the.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Empirical distribution function and that is called empirical transformation.",
                    "label": 0
                },
                {
                    "sent": "So what we do for each marginal so say for marginal G We just ordered the points and if a PT has wrong I, then the corresponding empirical distribution VI over N. Distribution function.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we do this transformation for or marginal.",
                    "label": 0
                },
                {
                    "sent": "So if you have the point XT, then it's true.",
                    "label": 0
                },
                {
                    "sent": "Popular transformation would be something like that, but.",
                    "label": 0
                },
                {
                    "sent": "The value of the empirical copula transformation and it will place the points on a grid.",
                    "label": 1
                },
                {
                    "sent": "Like here.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so this is our algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's pointless to accept.",
                    "label": 0
                },
                {
                    "sent": "First, we calculate the empirical.",
                    "label": 0
                },
                {
                    "sent": "Popular Angie was empirical copula transformation and then on this empirically then on this transform points we use the previously introduced trainee entropy calculation and then.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our main result is this.",
                    "label": 0
                },
                {
                    "sent": "That if the dimension is larger or at least three and a half is between 1/2 and one.",
                    "label": 0
                },
                {
                    "sent": "And you use for entropy estimation.",
                    "label": 0
                },
                {
                    "sent": "Thank salesman problem, minimum spanning trees, minimum matching, caners neighborhood graphs, then the previously introduced algorithm will converge almost surely to the Rangers information.",
                    "label": 0
                },
                {
                    "sent": "So we don't have results for other Alpha values and we don't have theoretical result for the case Monday equals 2.",
                    "label": 1
                },
                {
                    "sent": "Another thing that we have to note that we used ranks only in this case for estimating the range information so that they can message that using statistics only.",
                    "label": 0
                },
                {
                    "sent": "You can consistently estimate the ranges in formation.",
                    "label": 0
                },
                {
                    "sent": "You don't have to use the original values.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is a bit on the robustness.",
                    "label": 0
                },
                {
                    "sent": "If you have an outlier arbitrary.",
                    "label": 0
                },
                {
                    "sent": "In this case, the effect of the outlier cannot be arbitrary.",
                    "label": 1
                },
                {
                    "sent": "So if you have enough sample points, then the effect of an outlier X can be negligible.",
                    "label": 0
                },
                {
                    "sent": "If you have enough sample one, it's not the case if you don't use popular transformation.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and finally I would like to show some.",
                    "label": 0
                },
                {
                    "sent": "Numerical results, so here in this case.",
                    "label": 0
                },
                {
                    "sent": "I had 10 dimensional uniform distribution.",
                    "label": 0
                },
                {
                    "sent": "Of course the mutual information is zero in this case, and if you ski nearest neighborhood graphs for entropy estimation or minimum spanning trees for entropy estimation and use popular transformation, then you can see that the regular algorithm converge to the true mutual information.",
                    "label": 0
                },
                {
                    "sent": "In this other case I used 10 dimensional Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "Here you can see the sample numbers.",
                    "label": 0
                },
                {
                    "sent": "And you can see that for both methods, the minimum spanning tree algorithm using copula method and caners neighborhood algorithms using popular methods converge to the true mutual information and I would like to show an application.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Independent subspace analysis.",
                    "label": 0
                },
                {
                    "sent": "So this is a generalization of independent component analysis.",
                    "label": 0
                },
                {
                    "sent": "I choose three 2 dimensional subspaces denoted by this AB&C and I sampled 2000 points independently from these subspaces and they were hidden for our algorithm.",
                    "label": 0
                },
                {
                    "sent": "We just formed built a matrix S using these three.",
                    "label": 0
                },
                {
                    "sent": "2 dimensional points, so they formed a 6 by 2000.",
                    "label": 0
                },
                {
                    "sent": "Dimensional matrix and mixed these hidden points by a random 6 by 6 mixing matrix and the offset.",
                    "label": 0
                },
                {
                    "sent": "This quantity X.",
                    "label": 0
                },
                {
                    "sent": "And here you can see, but the algorithm offset and the task was that using this observation these 2000 points from this mixture we had to recover the original subspaces.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And delegate invert like this that we were looking for a W so-called mixing matrix with which WAS that is WX, which is.",
                    "label": 0
                },
                {
                    "sent": "Why would be the recovered subspaces and we wanted to minimize the mutual information.",
                    "label": 0
                },
                {
                    "sent": "Between here, between the subspaces, and here I am trying to show a movie how our algorithm works.",
                    "label": 0
                },
                {
                    "sent": "This matrix here is W * A.",
                    "label": 0
                },
                {
                    "sent": "So when our algorithm works perfectly then this WAW times a matrix would be a block permutation matrix and these.",
                    "label": 1
                },
                {
                    "sent": "Figure sure how I would algorithm trying to separate the subspaces.",
                    "label": 0
                },
                {
                    "sent": "OK, so now it's starting to look for the best value, with which you can minimize the mutual information.",
                    "label": 0
                },
                {
                    "sent": "Here you can see WA again.",
                    "label": 0
                },
                {
                    "sent": "And these are the recovered subspaces.",
                    "label": 0
                },
                {
                    "sent": "OK, and you can see that finally we got the block permutation matrix here and we could recover these subspaces, minimizing the mutual information.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the take home message is that.",
                    "label": 0
                },
                {
                    "sent": "Using graph optimization methods and popular transformation, if you marry these two things, you can get efficient, robust, almost fully consistent ranges, mutual information estimation method.",
                    "label": 1
                },
                {
                    "sent": "And finally I just want to show.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An example for this particular song.",
                    "label": 0
                },
                {
                    "sent": "Oh, and I translated the lyrics.",
                    "label": 0
                },
                {
                    "sent": "The screen is too big.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very interesting time.",
                    "label": 0
                },
                {
                    "sent": "So you need to solve.",
                    "label": 0
                },
                {
                    "sent": "How to correctly you have to solve these graph problems yet to get estimated he's got problems have certain computational exactly yes.",
                    "label": 0
                },
                {
                    "sent": "Does it allow us some conclusion about the general difficulty of obtaining empirical estimates of the radius.",
                    "label": 0
                },
                {
                    "sent": "So you don't want to use probably traveling salesman problem right?",
                    "label": 0
                },
                {
                    "sent": "To solve this problem that you can use Skinner's neighborhood graphs or minimum spending please.",
                    "label": 0
                },
                {
                    "sent": "And the complexity there, it's N squared or N log N and that's the computational complexity.",
                    "label": 0
                },
                {
                    "sent": "It's another issue, how other question?",
                    "label": 0
                },
                {
                    "sent": "First, the the estimation convex to the true values and we have some results for that as well, and we can discuss that offline if you abandon thanks.",
                    "label": 0
                }
            ]
        }
    }
}