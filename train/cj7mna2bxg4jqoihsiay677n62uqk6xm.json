{
    "id": "cj7mna2bxg4jqoihsiay677n62uqk6xm",
    "title": "Sequential Event Prediction with Association Rules",
    "info": {
        "author": [
            "Cynthia Rudin, Sloan School of Management, Massachusetts Institute of Technology, MIT"
        ],
        "published": "Aug. 2, 2011",
        "recorded": "July 2011",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Machine Learning->Statistical Learning"
        ]
    },
    "url": "http://videolectures.net/colt2011_rudin_prediction/",
    "segmentation": [
        [
            "So this is work with Ben Letham unsolved celebrity June Cogan and David Madigan."
        ],
        [
            "So when I lived in New York City, I used to shop at an on line grocery store called Fresh Direct and I think like half of NYC uses this store and I used fresh track pretty much every week for over a year.",
            "So I developed a sort of working knowledge of their recommender system and that system is sort of central to the way that you know it's central to their storefront and the way that you interact with them.",
            "So it's simple."
        ],
        [
            "Rent.",
            "And the way that you."
        ],
        [
            "The way that it works is that you pick something and you put it in your basket and then knowing what you have in your basket, it recommends you a few things that it thinks you like.",
            "And this could be really convenient if you're like me and you make the Mail order the same sorts of ingredients to make the same sorts of recipes all the time, but fresh tracks, current recommender system.",
            "Let's just say it doesn't do that."
        ],
        [
            "So, for instance, here's there's.",
            "You can't see it, but there's a bunch of vegetables in the shopping cart.",
            "There's cauliflower and cucumbers and stuff like that, and it recommends toddler microwave meals.",
            "And the person who set this up with Gene, and he's never ordered anything resembling toddler food."
        ],
        [
            "And I was thinking, well, you know, why?",
            "Couldn't it just sort out that I make this lemon sesame chicken recipe pretty often so maybe half the time I buy chicken, I'm going to want."
        ],
        [
            "Lemon.",
            "And if I order chicken, sesame means I'm almost definitely going to want."
        ],
        [
            "Minton and it turns out that within the last year I lived in New York, I ordered Chicken Sesame 15 times and 13 of those 15 times I ordered lemon.",
            "So if it was estimating conditional probabilities, it should have been able to figure out that it auto recommend me Lemon, but clearly it wasn't doing."
        ],
        [
            "At.",
            "And so I figured there's gotta be a better way.",
            "Maybe I figured, you know, maybe I could build a better system, something that's really simple.",
            "But actually, you know, really makes sense.",
            "And what I really wanted to do is to fill in incomplete recipes.",
            "I wanted to look at subsets of my current basket, see what items I had often purchased with each of those subsets and then and then choose what to recommend based on the probability of me picking the various items.",
            "And now, of course, this is a very natural thing, and in fact people have been designing algorithms that find patterns and subsets of of data for years since 1993, when Agrawal Emilienne Skin Swami's original paper, an Association rule mining came out and this thing on the screen is precisely an example of an Association rule."
        ],
        [
            "And the apriori algorithm, which was the first popular algorithm designed to mine Association rules from large within large databases, is one of the top 10 algorithms in data mining, at least according to the ICM survey.",
            "And the Weather Day priority works and the way that a lot of other real mining algorithms work is that they look for rules that obey two conditions.",
            "The first condition is that the rules have to have item sets that have high enough support, and this means the items within the rule have to have appeared often enough in that."
        ],
        [
            "The base.",
            "So here the support of chicken and sesame's 15.",
            "I've ordered those together 15 times and support of chicken, sesame and lemon is 13 and those numbers 13 and 15 have to be above a predetermined cutoff threshold and rules that don't meet that support requirement.",
            "They're just thrown out.",
            "We don't use them.",
            "OK, so the second thing is that the rules have to have a high confidence, which is the empirical conditional probability.",
            "So the confidence of the rule implies B.",
            "It's just the empirical probability of B given a.",
            "And so my first try in a recommender system follows a very traditional outline, but with one small exception, which is that I'm only going to look at the support of the left."
        ],
        [
            "And the confidence, and not the support of the right."
        ],
        [
            "OK, so here's my first recommender system, which is based pretty much on a Prairie, so step one.",
            "Look at all rules implies B where the left hand side is a subset of items that's in allowed set A.",
            "And the support of a sufficiently large.",
            "And B is a single item.",
            "Now, the reason we have the allowed set is because we don't want to enumerate all subsets that this would be.",
            "That would be too big, so we have this allowed set which is for instance all sets.",
            "Size at most two items.",
            "OK, so Step 2 is to order the rules by the confidence and recommend the right hand sides of the top ranked rules.",
            "OK, so here's an example with a bunch of rules and the supports of the left or are are larger than some cutoff threshold, say 10.",
            "And the rank ordered by confidence.",
            "So my recommender system would recommend lemon mushrooms, peas and Peppers in that order."
        ],
        [
            "OK, So what can I say about this algorithm?",
            "The minimum support threshold is fantastic in that it essentially allows generalization.",
            "So predictions are made only when there are enough data.",
            "But there are other things about the minimum support threshold that we're just not very satisfying.",
            "And first using the hard threshold excludes what are called Nuggets, which are very powerful but rare rules, and there's no chance to use those rules.",
            "They just get they just get swept away.",
            "OK, and so the second thing I wasn't thrilled about ranking rules by the confidence alone, because if I had two rules, one with confidence .99 and support 10,000 so is really good reliable rule.",
            "It wouldn't get ranked above another rule whose support barely made it over the threshold and whose confidence is exactly 1, so I wasn't completely satisfied with the maximum confidence minimum support algorithm."
        ],
        [
            "And so I decided to try something else.",
            "Instead.",
            "I defined what I called the adjusted confidence, which is a Bayesian estimate of the conditional probability.",
            "So it's just the number of times I've ordered an be together divided by number of times I've ordered a + K, where K is a constant and K is 0.",
            "The adjusted confidence is just the confidence and the new algorithm."
        ],
        [
            "Looks like this.",
            "So it seems the other algorithm, except there's no support threshold.",
            "We consider all of the allowed rules, and we rank the rules by adjusted confidence instead of by confidence.",
            "OK, so here the recommendation order is different than the other one, so the before we had lemon first, then mushrooms, then peas and Peppers.",
            "But this is also a very simple, very natural algorithm."
        ],
        [
            "OK, and when and when K is greater than zero, this this algorithm actually alleviates the two problems that I mentioned earlier.",
            "With the other algorithm, Nuggets don't get thrown out and they could actually get used if they're powerful enough.",
            "And among rules with them, with similar confidence, it'll choose the rule with higher support.",
            "So among the two rules at the bottom will choose the one on the left.",
            "If case sufficiently large.",
            "And we can still generalize because larger K encourages, so it doesn't guarantee it encourages larger support.",
            "An formalizing and understanding the generalization part for both algorithms is the subject of the paper."
        ],
        [
            "OK, so essentially the bottom line is that we're trying to develop a framework for using Association rules and supervised learning.",
            "Now Association rules have been pretty extensively used in the data mining literature, so we know they're useful, and they're fantastic modeling tools because they produce such simple models and the models aren't linear models.",
            "You know, there's no linearity assumption.",
            "The assumption is that a probability can be estimated by one of the conditional probabilities.",
            "But even though Association rules have such a large literature in data mining, as far as I know, there's never been a real attempt at developing a supervised learning framework for them.",
            "And also the bounds and learning are usually large sample bounds, but for Association rules they wouldn't make a prediction when when there's not a large enough sample, so there's a new dimension in this work, which is can we generalize in the small sample in the small sample regime where the support threshold or the adjusted confidence is K parameter provides generalization.",
            "OK, and so we also define the problem of sequential event prediction.",
            "OK, this is the problem where there's a database full of event sequences.",
            "And for current sequence we need to use subsets of past events to predict a future event.",
            "And the current sequence is the customer's basket in the online grocery store.",
            "And so we nicknamed this problem, the online grocery store problem, and this problem actually comes up all the time.",
            "So for instance, we're using it in another work to try to predict sequences of symptoms for medical patients in a clinical trial.",
            "And now for sequential event prediction.",
            "It's not clear how you'd use other supervised learning methods like logistic regression, for instance, because it's because of the sequential nature of it.",
            "But you can sure use Association rules so it has a major advantage in this setting.",
            "OK, so so now that we have a framework for studying Association rules and we've defined sequential event prediction, we can.",
            "Develop generalization bounds that try to tell us what are the.",
            "What might be the important quantities in the learning process, and I can tell you what they are so there the size of the training set.",
            "Of course the size of the allowed set a.",
            "And then P min, which are the probabilities of the least probable items or item sets.",
            "And then the adjusted confidence is key parameter or the minimum support threshold data.",
            "OK, so this this slide is the talk in a nutshell.",
            "So Association rules for learning defining the problem of sequential event prediction and what are the important quantities in the in the learning process."
        ],
        [
            "OK, so the adjusted confidence algorithm is able to describe more of the variance in the data because there's no strict minimum support threshold.",
            "But for that same reason, the guarantees for the adjusted confidence algorithm are weaker, like the dependence on Delta is not as good, but the point is that you can still generalize even even with the adjusted confidence algorithm."
        ],
        [
            "OK, so this is the longer version of the Cold Paper, and the paper contains bounds for both classification and for the sequential event prediction problem.",
            "Since we're trying to establish Association rules and supervised learning, we figured we do it also for classification.",
            "And there are large sample bounce in small sample bounds.",
            "One bound is a uniform BC bound and the rest are based on algorithmic stability.",
            "And mostly I used a particular kind of stability called pointwise hypothesis stability, except for this one boundary I was able to use using uniform stability and there's some experiments.",
            "OK, So what I'm going to do now is define a learning problem for sequential event prediction and just give a couple of the bounds.",
            "And I'm just going to give results for sequential event prediction today and not classification."
        ],
        [
            "Alright, so we're going to we're going to.",
            "Have the customer iteratively put one item at a time into their basket, and then at each step the adjusted confidence algorithm has to recommend recommend one item so X is the set of all items, Z is a basket, it's just a subset of items within ordering associated with it, and then ZT is the teeth item in basket Z so that dot is a placeholder.",
            "Right now it's going to be replaced with I for the training basket.",
            "And then T sub Z is the number of items in the basket Z.",
            "And then S is that the training set of M random baskets and then FSC is the adjusted confidence so that thing down there is the adjusted confidence of the rule A implies B.",
            "So you want to remember that notation.",
            "'cause I'm going to be using it a lot."
        ],
        [
            "Alright, so I'm filling up my basket sequentially so I'm at a particular time T with a bunch of items in my basket.",
            "And I need to define a highest scoring correct rule as the highest scoring rule that recommends the correct the correct items ET plus one which is, which is what the customer is going to put into their basket next.",
            "And the left hand side of a high scoring correct role has to obey.",
            "It's OK, it has to obey that it's.",
            "It's it's a highest.",
            "OK, it's the highest scoring rule and.",
            "It has to have a so a has to be a subset of items that are in the current basket in the name has to be allowed so it's got to be in that set a. OK. And so we call it a plus.",
            "OK, in high scoring incorrect rule it's a high scoring rule that doesn't recommend the next item, so the right hand side that be minus it can't be easy plus one.",
            "And again, the left hand side has to be in allowed subset of items in the basket.",
            "And now we can define the loss function, which just counts the number of times that we make the wrong recommendation.",
            "It's just that the proportion of times that the highest scoring correct rule had a lower adjusted confidence than the highest scoring incorrect rule, and so we could set K are equal to K. But the point is that we don't have to like like if you want to compare of different algorithms or different choices of K using the same loss function.",
            "This allows you to do it.",
            "OK, so now."
        ],
        [
            "Do we have the loss?",
            "We can define the true error as the expectation of the of the loss over the distribution of random baskets."
        ],
        [
            "OK, so let's keep that 01 loss on the."
        ],
        [
            "Screen for a minute.",
            "Put it up there."
        ],
        [
            "And Noah, I'm just going to find a continuous proxy for the loss function that depends on this function C gamma, which is a simple upper bound for the 01 loss."
        ],
        [
            "And then from that we can define the empirical error as the average of the gamma loss over the training baskets.",
            "And of course we're trying to get an upper bound for the true loss in terms of the empirical error."
        ],
        [
            "OK, so I'm just going to state a few of the bounds in the paper, namely the large sample pointwise stability bound for sequential event prediction.",
            "And the small sample stability bound for the adjusted confidence algorithm.",
            "Also for sequential event prediction.",
            "And this stability uses the bounds of Biscayne Eliseev which are based on ideas of Deroin Wagner in the 70s.",
            "And basically the idea is you want about how much the algorithm changes when you remove one of the training points.",
            "One training example.",
            "So let me show you the first one."
        ],
        [
            "OK, so it says that with high probability the true error is bounded by the empirical error plus a function depending on an upper bound for the pointwise hypothesis stability beta, which is the average change in the loss at one of the training points when you when you remove that point from the training set.",
            "And here's the result, but it's a little difficult to digest on one slide, so let me break it down."
        ],
        [
            "OK, so there's beta.",
            "And the first thing to notice is that it depends on the size of the allowed set a.",
            "So actually the allowed site allowed set might contain left hand sides that have zero probability of occurring, so scripta just eliminates those guys.",
            "OK, so and of the ones that are actually probable, the ones that occur most rarely have probability piman A, so that's what that is down there."
        ],
        [
            "OK, and there are two terms in the bound.",
            "And one of the second term it goes away when K equals KR, so you get penalized when the algorithm isn't completely calibrated to the loss function.",
            "And if you look at the first term, the K actually helps you out in the denominator.",
            "So large K helps and keeping K close to KR helps."
        ],
        [
            "OK, and then when Kane Karabal 0 the algorithm ranks items using the confidence and then the loss function involves differences in comp."
        ],
        [
            "It's values and it ends up producing just to this, and so this one is much easier to digest."
        ],
        [
            "OK, so let's look at that small sample."
        ],
        [
            "Sound.",
            "And the bound looks bound.",
            "Looks like this the stars are there because the definitions.",
            "Are slightly different than what I gave, but but it's pretty similar.",
            "Alright, so again.",
            "So here at Q are the items that have a positive probability of being chosen and payment is the probability of the least probable of those items.",
            "So let me break the."
        ],
        [
            "Down again.",
            "OK, so there are two terms.",
            "The second one vanishes when K equals KR and the first term again has this nice dependence on K and the bound doesn't depend on the size of a. OK, so for that second term it's a mean with respect to a binomial distribution, so you can make an approximation to see what's going on."
        ],
        [
            "Which is that M&K are large and we're approximating by the mean of the binomial, and it simplifies pretty nicely without being too inaccurate when K is close to KR.",
            "So that's the essence of beta, and you can see again that K helps, but that case should be close to KR."
        ],
        [
            "Alright, so I've gone through a few of the balance and I think I'm just going to summarize now what I learned from doing."
        ],
        [
            "All of this.",
            "OK, so first sociation rules.",
            "We can get generalization guarantees in two ways.",
            "Large sample bounds that depend mainly on the sample size and small sample bounds that depend mainly on the support of rules.",
            "And the adjusted confidence algorithm doesn't have a strict support threshold yet.",
            "We can still use its weaker support guarantee to get a bound.",
            "And also we highlighted several quantities that might be important for the learning process."
        ],
        [
            "OK so I have.",
            "I have several projects now that are related to Association rules and or sequential event prediction and one of them is with Tyler McCormick and David Madigan and it's a Bayesian hierarchical model that uses Association rules for sequential event prediction and so here we're trying to predict symptoms of medical patients in a clinical trial like I mentioned.",
            "And then also my student Ben is is working on a method for sequential event prediction that uses supervised ranking and not Association rules.",
            "And also I'm starting to learn how to find rules in medium to large size databases and this is where the field of Association rules has really kind of grown up.",
            "But since I come from a different background, I'm going to try to take a fresh look at it."
        ],
        [
            "Alright, thank you.",
            "What is your definition of losses with respect to what I mean by random basket?",
            "Because yeah, so the best answer chosen IID from some distribution over to the ex crust pie, right?",
            "So we have a distribution of a basket.",
            "Yes, yes yes items no no.",
            "It's silver baskets, yeah, and this should be distribution free for any distribution over baskets.",
            "Yeah, in this size of the baskets, how does it vary mean?",
            "Let's have a fix.",
            "Each basket is a finite size, right?",
            "The baskets can be any size they like.",
            "Their their chosen from a distribution of her baskets.",
            "OK, so institution of a Sigma star was sorry this solution over all finite baskets.",
            "Yeah, it's a distribution over all finite ordered baskets.",
            "Yeah, OK, that's it.",
            "That's right, there are no more questions.",
            "Thanks speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is work with Ben Letham unsolved celebrity June Cogan and David Madigan.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So when I lived in New York City, I used to shop at an on line grocery store called Fresh Direct and I think like half of NYC uses this store and I used fresh track pretty much every week for over a year.",
                    "label": 0
                },
                {
                    "sent": "So I developed a sort of working knowledge of their recommender system and that system is sort of central to the way that you know it's central to their storefront and the way that you interact with them.",
                    "label": 0
                },
                {
                    "sent": "So it's simple.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rent.",
                    "label": 0
                },
                {
                    "sent": "And the way that you.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The way that it works is that you pick something and you put it in your basket and then knowing what you have in your basket, it recommends you a few things that it thinks you like.",
                    "label": 0
                },
                {
                    "sent": "And this could be really convenient if you're like me and you make the Mail order the same sorts of ingredients to make the same sorts of recipes all the time, but fresh tracks, current recommender system.",
                    "label": 0
                },
                {
                    "sent": "Let's just say it doesn't do that.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, for instance, here's there's.",
                    "label": 0
                },
                {
                    "sent": "You can't see it, but there's a bunch of vegetables in the shopping cart.",
                    "label": 0
                },
                {
                    "sent": "There's cauliflower and cucumbers and stuff like that, and it recommends toddler microwave meals.",
                    "label": 0
                },
                {
                    "sent": "And the person who set this up with Gene, and he's never ordered anything resembling toddler food.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I was thinking, well, you know, why?",
                    "label": 0
                },
                {
                    "sent": "Couldn't it just sort out that I make this lemon sesame chicken recipe pretty often so maybe half the time I buy chicken, I'm going to want.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lemon.",
                    "label": 0
                },
                {
                    "sent": "And if I order chicken, sesame means I'm almost definitely going to want.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Minton and it turns out that within the last year I lived in New York, I ordered Chicken Sesame 15 times and 13 of those 15 times I ordered lemon.",
                    "label": 0
                },
                {
                    "sent": "So if it was estimating conditional probabilities, it should have been able to figure out that it auto recommend me Lemon, but clearly it wasn't doing.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At.",
                    "label": 0
                },
                {
                    "sent": "And so I figured there's gotta be a better way.",
                    "label": 0
                },
                {
                    "sent": "Maybe I figured, you know, maybe I could build a better system, something that's really simple.",
                    "label": 0
                },
                {
                    "sent": "But actually, you know, really makes sense.",
                    "label": 0
                },
                {
                    "sent": "And what I really wanted to do is to fill in incomplete recipes.",
                    "label": 0
                },
                {
                    "sent": "I wanted to look at subsets of my current basket, see what items I had often purchased with each of those subsets and then and then choose what to recommend based on the probability of me picking the various items.",
                    "label": 0
                },
                {
                    "sent": "And now, of course, this is a very natural thing, and in fact people have been designing algorithms that find patterns and subsets of of data for years since 1993, when Agrawal Emilienne Skin Swami's original paper, an Association rule mining came out and this thing on the screen is precisely an example of an Association rule.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the apriori algorithm, which was the first popular algorithm designed to mine Association rules from large within large databases, is one of the top 10 algorithms in data mining, at least according to the ICM survey.",
                    "label": 0
                },
                {
                    "sent": "And the Weather Day priority works and the way that a lot of other real mining algorithms work is that they look for rules that obey two conditions.",
                    "label": 0
                },
                {
                    "sent": "The first condition is that the rules have to have item sets that have high enough support, and this means the items within the rule have to have appeared often enough in that.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The base.",
                    "label": 0
                },
                {
                    "sent": "So here the support of chicken and sesame's 15.",
                    "label": 0
                },
                {
                    "sent": "I've ordered those together 15 times and support of chicken, sesame and lemon is 13 and those numbers 13 and 15 have to be above a predetermined cutoff threshold and rules that don't meet that support requirement.",
                    "label": 0
                },
                {
                    "sent": "They're just thrown out.",
                    "label": 0
                },
                {
                    "sent": "We don't use them.",
                    "label": 0
                },
                {
                    "sent": "OK, so the second thing is that the rules have to have a high confidence, which is the empirical conditional probability.",
                    "label": 0
                },
                {
                    "sent": "So the confidence of the rule implies B.",
                    "label": 0
                },
                {
                    "sent": "It's just the empirical probability of B given a.",
                    "label": 0
                },
                {
                    "sent": "And so my first try in a recommender system follows a very traditional outline, but with one small exception, which is that I'm only going to look at the support of the left.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the confidence, and not the support of the right.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so here's my first recommender system, which is based pretty much on a Prairie, so step one.",
                    "label": 0
                },
                {
                    "sent": "Look at all rules implies B where the left hand side is a subset of items that's in allowed set A.",
                    "label": 1
                },
                {
                    "sent": "And the support of a sufficiently large.",
                    "label": 0
                },
                {
                    "sent": "And B is a single item.",
                    "label": 0
                },
                {
                    "sent": "Now, the reason we have the allowed set is because we don't want to enumerate all subsets that this would be.",
                    "label": 0
                },
                {
                    "sent": "That would be too big, so we have this allowed set which is for instance all sets.",
                    "label": 0
                },
                {
                    "sent": "Size at most two items.",
                    "label": 0
                },
                {
                    "sent": "OK, so Step 2 is to order the rules by the confidence and recommend the right hand sides of the top ranked rules.",
                    "label": 1
                },
                {
                    "sent": "OK, so here's an example with a bunch of rules and the supports of the left or are are larger than some cutoff threshold, say 10.",
                    "label": 0
                },
                {
                    "sent": "And the rank ordered by confidence.",
                    "label": 0
                },
                {
                    "sent": "So my recommender system would recommend lemon mushrooms, peas and Peppers in that order.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what can I say about this algorithm?",
                    "label": 0
                },
                {
                    "sent": "The minimum support threshold is fantastic in that it essentially allows generalization.",
                    "label": 1
                },
                {
                    "sent": "So predictions are made only when there are enough data.",
                    "label": 0
                },
                {
                    "sent": "But there are other things about the minimum support threshold that we're just not very satisfying.",
                    "label": 0
                },
                {
                    "sent": "And first using the hard threshold excludes what are called Nuggets, which are very powerful but rare rules, and there's no chance to use those rules.",
                    "label": 0
                },
                {
                    "sent": "They just get they just get swept away.",
                    "label": 0
                },
                {
                    "sent": "OK, and so the second thing I wasn't thrilled about ranking rules by the confidence alone, because if I had two rules, one with confidence .99 and support 10,000 so is really good reliable rule.",
                    "label": 1
                },
                {
                    "sent": "It wouldn't get ranked above another rule whose support barely made it over the threshold and whose confidence is exactly 1, so I wasn't completely satisfied with the maximum confidence minimum support algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so I decided to try something else.",
                    "label": 0
                },
                {
                    "sent": "Instead.",
                    "label": 0
                },
                {
                    "sent": "I defined what I called the adjusted confidence, which is a Bayesian estimate of the conditional probability.",
                    "label": 1
                },
                {
                    "sent": "So it's just the number of times I've ordered an be together divided by number of times I've ordered a + K, where K is a constant and K is 0.",
                    "label": 0
                },
                {
                    "sent": "The adjusted confidence is just the confidence and the new algorithm.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Looks like this.",
                    "label": 0
                },
                {
                    "sent": "So it seems the other algorithm, except there's no support threshold.",
                    "label": 0
                },
                {
                    "sent": "We consider all of the allowed rules, and we rank the rules by adjusted confidence instead of by confidence.",
                    "label": 1
                },
                {
                    "sent": "OK, so here the recommendation order is different than the other one, so the before we had lemon first, then mushrooms, then peas and Peppers.",
                    "label": 0
                },
                {
                    "sent": "But this is also a very simple, very natural algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and when and when K is greater than zero, this this algorithm actually alleviates the two problems that I mentioned earlier.",
                    "label": 0
                },
                {
                    "sent": "With the other algorithm, Nuggets don't get thrown out and they could actually get used if they're powerful enough.",
                    "label": 0
                },
                {
                    "sent": "And among rules with them, with similar confidence, it'll choose the rule with higher support.",
                    "label": 1
                },
                {
                    "sent": "So among the two rules at the bottom will choose the one on the left.",
                    "label": 0
                },
                {
                    "sent": "If case sufficiently large.",
                    "label": 1
                },
                {
                    "sent": "And we can still generalize because larger K encourages, so it doesn't guarantee it encourages larger support.",
                    "label": 0
                },
                {
                    "sent": "An formalizing and understanding the generalization part for both algorithms is the subject of the paper.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so essentially the bottom line is that we're trying to develop a framework for using Association rules and supervised learning.",
                    "label": 1
                },
                {
                    "sent": "Now Association rules have been pretty extensively used in the data mining literature, so we know they're useful, and they're fantastic modeling tools because they produce such simple models and the models aren't linear models.",
                    "label": 0
                },
                {
                    "sent": "You know, there's no linearity assumption.",
                    "label": 0
                },
                {
                    "sent": "The assumption is that a probability can be estimated by one of the conditional probabilities.",
                    "label": 0
                },
                {
                    "sent": "But even though Association rules have such a large literature in data mining, as far as I know, there's never been a real attempt at developing a supervised learning framework for them.",
                    "label": 0
                },
                {
                    "sent": "And also the bounds and learning are usually large sample bounds, but for Association rules they wouldn't make a prediction when when there's not a large enough sample, so there's a new dimension in this work, which is can we generalize in the small sample in the small sample regime where the support threshold or the adjusted confidence is K parameter provides generalization.",
                    "label": 0
                },
                {
                    "sent": "OK, and so we also define the problem of sequential event prediction.",
                    "label": 0
                },
                {
                    "sent": "OK, this is the problem where there's a database full of event sequences.",
                    "label": 0
                },
                {
                    "sent": "And for current sequence we need to use subsets of past events to predict a future event.",
                    "label": 0
                },
                {
                    "sent": "And the current sequence is the customer's basket in the online grocery store.",
                    "label": 0
                },
                {
                    "sent": "And so we nicknamed this problem, the online grocery store problem, and this problem actually comes up all the time.",
                    "label": 1
                },
                {
                    "sent": "So for instance, we're using it in another work to try to predict sequences of symptoms for medical patients in a clinical trial.",
                    "label": 0
                },
                {
                    "sent": "And now for sequential event prediction.",
                    "label": 0
                },
                {
                    "sent": "It's not clear how you'd use other supervised learning methods like logistic regression, for instance, because it's because of the sequential nature of it.",
                    "label": 0
                },
                {
                    "sent": "But you can sure use Association rules so it has a major advantage in this setting.",
                    "label": 0
                },
                {
                    "sent": "OK, so so now that we have a framework for studying Association rules and we've defined sequential event prediction, we can.",
                    "label": 0
                },
                {
                    "sent": "Develop generalization bounds that try to tell us what are the.",
                    "label": 1
                },
                {
                    "sent": "What might be the important quantities in the learning process, and I can tell you what they are so there the size of the training set.",
                    "label": 1
                },
                {
                    "sent": "Of course the size of the allowed set a.",
                    "label": 0
                },
                {
                    "sent": "And then P min, which are the probabilities of the least probable items or item sets.",
                    "label": 0
                },
                {
                    "sent": "And then the adjusted confidence is key parameter or the minimum support threshold data.",
                    "label": 0
                },
                {
                    "sent": "OK, so this this slide is the talk in a nutshell.",
                    "label": 0
                },
                {
                    "sent": "So Association rules for learning defining the problem of sequential event prediction and what are the important quantities in the in the learning process.",
                    "label": 1
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the adjusted confidence algorithm is able to describe more of the variance in the data because there's no strict minimum support threshold.",
                    "label": 0
                },
                {
                    "sent": "But for that same reason, the guarantees for the adjusted confidence algorithm are weaker, like the dependence on Delta is not as good, but the point is that you can still generalize even even with the adjusted confidence algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is the longer version of the Cold Paper, and the paper contains bounds for both classification and for the sequential event prediction problem.",
                    "label": 0
                },
                {
                    "sent": "Since we're trying to establish Association rules and supervised learning, we figured we do it also for classification.",
                    "label": 0
                },
                {
                    "sent": "And there are large sample bounce in small sample bounds.",
                    "label": 1
                },
                {
                    "sent": "One bound is a uniform BC bound and the rest are based on algorithmic stability.",
                    "label": 0
                },
                {
                    "sent": "And mostly I used a particular kind of stability called pointwise hypothesis stability, except for this one boundary I was able to use using uniform stability and there's some experiments.",
                    "label": 0
                },
                {
                    "sent": "OK, So what I'm going to do now is define a learning problem for sequential event prediction and just give a couple of the bounds.",
                    "label": 0
                },
                {
                    "sent": "And I'm just going to give results for sequential event prediction today and not classification.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so we're going to we're going to.",
                    "label": 0
                },
                {
                    "sent": "Have the customer iteratively put one item at a time into their basket, and then at each step the adjusted confidence algorithm has to recommend recommend one item so X is the set of all items, Z is a basket, it's just a subset of items within ordering associated with it, and then ZT is the teeth item in basket Z so that dot is a placeholder.",
                    "label": 1
                },
                {
                    "sent": "Right now it's going to be replaced with I for the training basket.",
                    "label": 1
                },
                {
                    "sent": "And then T sub Z is the number of items in the basket Z.",
                    "label": 0
                },
                {
                    "sent": "And then S is that the training set of M random baskets and then FSC is the adjusted confidence so that thing down there is the adjusted confidence of the rule A implies B.",
                    "label": 0
                },
                {
                    "sent": "So you want to remember that notation.",
                    "label": 0
                },
                {
                    "sent": "'cause I'm going to be using it a lot.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so I'm filling up my basket sequentially so I'm at a particular time T with a bunch of items in my basket.",
                    "label": 0
                },
                {
                    "sent": "And I need to define a highest scoring correct rule as the highest scoring rule that recommends the correct the correct items ET plus one which is, which is what the customer is going to put into their basket next.",
                    "label": 0
                },
                {
                    "sent": "And the left hand side of a high scoring correct role has to obey.",
                    "label": 0
                },
                {
                    "sent": "It's OK, it has to obey that it's.",
                    "label": 0
                },
                {
                    "sent": "It's it's a highest.",
                    "label": 0
                },
                {
                    "sent": "OK, it's the highest scoring rule and.",
                    "label": 0
                },
                {
                    "sent": "It has to have a so a has to be a subset of items that are in the current basket in the name has to be allowed so it's got to be in that set a. OK. And so we call it a plus.",
                    "label": 0
                },
                {
                    "sent": "OK, in high scoring incorrect rule it's a high scoring rule that doesn't recommend the next item, so the right hand side that be minus it can't be easy plus one.",
                    "label": 0
                },
                {
                    "sent": "And again, the left hand side has to be in allowed subset of items in the basket.",
                    "label": 0
                },
                {
                    "sent": "And now we can define the loss function, which just counts the number of times that we make the wrong recommendation.",
                    "label": 0
                },
                {
                    "sent": "It's just that the proportion of times that the highest scoring correct rule had a lower adjusted confidence than the highest scoring incorrect rule, and so we could set K are equal to K. But the point is that we don't have to like like if you want to compare of different algorithms or different choices of K using the same loss function.",
                    "label": 0
                },
                {
                    "sent": "This allows you to do it.",
                    "label": 0
                },
                {
                    "sent": "OK, so now.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do we have the loss?",
                    "label": 0
                },
                {
                    "sent": "We can define the true error as the expectation of the of the loss over the distribution of random baskets.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's keep that 01 loss on the.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Screen for a minute.",
                    "label": 0
                },
                {
                    "sent": "Put it up there.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And Noah, I'm just going to find a continuous proxy for the loss function that depends on this function C gamma, which is a simple upper bound for the 01 loss.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then from that we can define the empirical error as the average of the gamma loss over the training baskets.",
                    "label": 0
                },
                {
                    "sent": "And of course we're trying to get an upper bound for the true loss in terms of the empirical error.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I'm just going to state a few of the bounds in the paper, namely the large sample pointwise stability bound for sequential event prediction.",
                    "label": 1
                },
                {
                    "sent": "And the small sample stability bound for the adjusted confidence algorithm.",
                    "label": 0
                },
                {
                    "sent": "Also for sequential event prediction.",
                    "label": 0
                },
                {
                    "sent": "And this stability uses the bounds of Biscayne Eliseev which are based on ideas of Deroin Wagner in the 70s.",
                    "label": 0
                },
                {
                    "sent": "And basically the idea is you want about how much the algorithm changes when you remove one of the training points.",
                    "label": 0
                },
                {
                    "sent": "One training example.",
                    "label": 0
                },
                {
                    "sent": "So let me show you the first one.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so it says that with high probability the true error is bounded by the empirical error plus a function depending on an upper bound for the pointwise hypothesis stability beta, which is the average change in the loss at one of the training points when you when you remove that point from the training set.",
                    "label": 0
                },
                {
                    "sent": "And here's the result, but it's a little difficult to digest on one slide, so let me break it down.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so there's beta.",
                    "label": 0
                },
                {
                    "sent": "And the first thing to notice is that it depends on the size of the allowed set a.",
                    "label": 0
                },
                {
                    "sent": "So actually the allowed site allowed set might contain left hand sides that have zero probability of occurring, so scripta just eliminates those guys.",
                    "label": 0
                },
                {
                    "sent": "OK, so and of the ones that are actually probable, the ones that occur most rarely have probability piman A, so that's what that is down there.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and there are two terms in the bound.",
                    "label": 0
                },
                {
                    "sent": "And one of the second term it goes away when K equals KR, so you get penalized when the algorithm isn't completely calibrated to the loss function.",
                    "label": 0
                },
                {
                    "sent": "And if you look at the first term, the K actually helps you out in the denominator.",
                    "label": 0
                },
                {
                    "sent": "So large K helps and keeping K close to KR helps.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and then when Kane Karabal 0 the algorithm ranks items using the confidence and then the loss function involves differences in comp.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's values and it ends up producing just to this, and so this one is much easier to digest.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's look at that small sample.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sound.",
                    "label": 0
                },
                {
                    "sent": "And the bound looks bound.",
                    "label": 0
                },
                {
                    "sent": "Looks like this the stars are there because the definitions.",
                    "label": 0
                },
                {
                    "sent": "Are slightly different than what I gave, but but it's pretty similar.",
                    "label": 0
                },
                {
                    "sent": "Alright, so again.",
                    "label": 0
                },
                {
                    "sent": "So here at Q are the items that have a positive probability of being chosen and payment is the probability of the least probable of those items.",
                    "label": 0
                },
                {
                    "sent": "So let me break the.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Down again.",
                    "label": 0
                },
                {
                    "sent": "OK, so there are two terms.",
                    "label": 0
                },
                {
                    "sent": "The second one vanishes when K equals KR and the first term again has this nice dependence on K and the bound doesn't depend on the size of a. OK, so for that second term it's a mean with respect to a binomial distribution, so you can make an approximation to see what's going on.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which is that M&K are large and we're approximating by the mean of the binomial, and it simplifies pretty nicely without being too inaccurate when K is close to KR.",
                    "label": 0
                },
                {
                    "sent": "So that's the essence of beta, and you can see again that K helps, but that case should be close to KR.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so I've gone through a few of the balance and I think I'm just going to summarize now what I learned from doing.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "All of this.",
                    "label": 0
                },
                {
                    "sent": "OK, so first sociation rules.",
                    "label": 0
                },
                {
                    "sent": "We can get generalization guarantees in two ways.",
                    "label": 0
                },
                {
                    "sent": "Large sample bounds that depend mainly on the sample size and small sample bounds that depend mainly on the support of rules.",
                    "label": 1
                },
                {
                    "sent": "And the adjusted confidence algorithm doesn't have a strict support threshold yet.",
                    "label": 1
                },
                {
                    "sent": "We can still use its weaker support guarantee to get a bound.",
                    "label": 0
                },
                {
                    "sent": "And also we highlighted several quantities that might be important for the learning process.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so I have.",
                    "label": 0
                },
                {
                    "sent": "I have several projects now that are related to Association rules and or sequential event prediction and one of them is with Tyler McCormick and David Madigan and it's a Bayesian hierarchical model that uses Association rules for sequential event prediction and so here we're trying to predict symptoms of medical patients in a clinical trial like I mentioned.",
                    "label": 1
                },
                {
                    "sent": "And then also my student Ben is is working on a method for sequential event prediction that uses supervised ranking and not Association rules.",
                    "label": 0
                },
                {
                    "sent": "And also I'm starting to learn how to find rules in medium to large size databases and this is where the field of Association rules has really kind of grown up.",
                    "label": 0
                },
                {
                    "sent": "But since I come from a different background, I'm going to try to take a fresh look at it.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, thank you.",
                    "label": 0
                },
                {
                    "sent": "What is your definition of losses with respect to what I mean by random basket?",
                    "label": 0
                },
                {
                    "sent": "Because yeah, so the best answer chosen IID from some distribution over to the ex crust pie, right?",
                    "label": 0
                },
                {
                    "sent": "So we have a distribution of a basket.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes yes items no no.",
                    "label": 0
                },
                {
                    "sent": "It's silver baskets, yeah, and this should be distribution free for any distribution over baskets.",
                    "label": 0
                },
                {
                    "sent": "Yeah, in this size of the baskets, how does it vary mean?",
                    "label": 0
                },
                {
                    "sent": "Let's have a fix.",
                    "label": 0
                },
                {
                    "sent": "Each basket is a finite size, right?",
                    "label": 0
                },
                {
                    "sent": "The baskets can be any size they like.",
                    "label": 0
                },
                {
                    "sent": "Their their chosen from a distribution of her baskets.",
                    "label": 0
                },
                {
                    "sent": "OK, so institution of a Sigma star was sorry this solution over all finite baskets.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's a distribution over all finite ordered baskets.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, that's it.",
                    "label": 0
                },
                {
                    "sent": "That's right, there are no more questions.",
                    "label": 0
                },
                {
                    "sent": "Thanks speaker again.",
                    "label": 0
                }
            ]
        }
    }
}