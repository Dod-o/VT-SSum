{
    "id": "nwg6ky43tgz5srfvtdqwvbpogklngpfl",
    "title": "Unsupervised Object Discovery and Segmentation in Videos",
    "info": {
        "author": [
            "Samuel Schulter, Institute for Computer Graphics and Vision, Graz University of Technology"
        ],
        "published": "April 3, 2014",
        "recorded": "September 2013",
        "category": [
            "Top->Computer Science->Computer Vision",
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/bmvc2013_schulter_object_discovery/",
    "segmentation": [
        [
            "I'm going to present our work titled Unsupervised Object Discovery and Segmentation in Videos.",
            "Has already said this was joint work with Christian license from Microsoft Austria and Peter Rodent Auspicia from Gratz University of Technology."
        ],
        [
            "So what is unsupervised object discovery?",
            "Let's assume we are given a set of unlabeled images like these ones."
        ],
        [
            "The goal in unsupervised object discovery is to find common visual concepts across these sets of images."
        ],
        [
            "Like the bicycles."
        ],
        [
            "Cars in this example."
        ],
        [
            "So the typical approach to unsupervised object discovery builds on a collection of unlabeled still images and often topic modeling or clustering methods are used.",
            "And however, these methods always have to rely somehow on any form of prior information, like arbitrary or pre segmentations of images which act like.",
            "Initial object proposals or objectives measures.",
            "However, these also have to rely on some supervision to train these objective measures.",
            "Of course there are many other different priors which could be used, however, a reliable discovery of objects and unlabeled images.",
            "Without priors is typically quite difficult."
        ],
        [
            "So in this work we want to emphasize the use of videos instead of still images, because in videos we can exploit motion and motion is a very strong and physically valid prior for objects.",
            "So using videos instead of still images has many benefits.",
            "For example, we can do a simple motion segmentation to extract object proposals from the background.",
            "Videos typically show a high wearability of the object's appearance if they're moving, and we can easily access a huge amount of data if we go to online platforms like YouTube or video with you."
        ],
        [
            "So in this paper we redefine the task of unsupervised object discovery a little bit such that we are given a set of unlabeled videos instead of images.",
            "The goal still is the same.",
            "We try to discover the objects which are moving in the videos and we try to assign them semantic label."
        ],
        [
            "OK, at this point I want to give you a brief outline after the presentation.",
            "First I will show you our approach to answer Quest object discovering we do is I will give you an overview and show you the building blocks.",
            "And of course, at the end of the presentation I will show you our experimental results."
        ],
        [
            "OK, so here we see the outline of our approach, which consists of several building blocks we see."
        ],
        [
            "Start with a set of unlabeled and unordered videos.",
            "Then we."
        ],
        [
            "Can calculate optical flow based on the videos and with a simple CRF based motion segmentation we can get Motion proposes which act as potential objects which."
        ],
        [
            "We discovered in the videos and given these proposals we try to cluster them.",
            "We try to find or identify semantically similar groups.",
            "And then we try.",
            "We learn object models for each of the four clusters and we apply them to."
        ],
        [
            "The input video frames again."
        ],
        [
            "And this allows us to do a CRF based semantic segmentation in a second iteration where we include now the optical flow information.",
            "So the motion information, but also the output of the classifiers which are trained for each of the clusters.",
            "And this then gives the final output of our unsupervised approach.",
            "We get semantically segmented video frames.",
            "OK."
        ],
        [
            "So let's start with motion segmentation.",
            "As I said, we have a CRF based motion CRF based energy formulation.",
            "I don't want to go into detail here.",
            "You can read up the details of the energy formulation in the paper.",
            "The only intention I want to give here is that we say that large optical flow vectors should correspond to potential objects because objects are moving, so they have large optical flow vector and we use this."
        ],
        [
            "As well as our unary potentials in the motion segmentation.",
            "So here we see the optical flow."
        ],
        [
            "And here we see the result of the motion segmentation.",
            "And as we can see here.",
            "These motion segments somehow correspond to potential."
        ],
        [
            "Object proposals so Motion segment is somehow object proposes or we regard it as an object proposal.",
            "However, these proposals are of course typically quite noisy because the optical flow calculation can fail the motion segmentation can fail.",
            "Arbitrary, uninteresting objects could move in the videos.",
            "However, at this point we can again exploit the motion information which is inherently given in the videos.",
            "Because of if we assume we have video which captures the frames at let's say 50 frames per second, we can make quite strong assumptions on the objects which are captured in the videos because they should move smooth issues through space and time.",
            "So what we can do here is we can easily remove potential outliers.",
            "By simply looking at some statistics of the proposals, like the trajectories which move through space and time, and we do some simple curve align fitting to remove the obvious outliers.",
            "In this case, and I also want to mention here that this would not be possible if you have an unordered set of still images which have no relation."
        ],
        [
            "OK, so after having filtered these proposals we are left with some.",
            "Potential object proposals which are not bounding box annotated.",
            "Actually, in the unlabeled videos.",
            "And we now employ a standard.",
            "Clustering approach to identify semantically similar groups.",
            "So this is standard approach which was often used in unsupervised object discovery approaches and still images.",
            "Where you have a pre segmentation and then you simply do a clustering and so we calculate a feature vector for each of the remaining proposals.",
            "In our case, we use the standard bag of words description with the spatial pyramid based on tensift.",
            "And then we have to choose the number of categories K we want to discover an we apply a simple spectral clustering approach based on the squared distance.",
            "Please note that we can also employ K means or any other clustering algorithm.",
            "We can exchange the features or anything else."
        ],
        [
            "OK, now I want to briefly show you some clustering results.",
            "Here we see 6.",
            "Object proposals which could cost it to, let's say cluster one, and this obviously corresponds to the bicycle cluster at this.",
            "Here we see also that the motion segmentation actually do quite good job, because the bounding box is tightly bound the object."
        ],
        [
            "Here we see another result where OK, this cluster obviously corresponds to the cars cluster."
        ],
        [
            "Well, and based on these clusters we learn an object model for each of them and this then allows to also discover non moving objects in the videos because at this point now we always limited to having only proposals from objects which are moving.",
            "However, when we train an object model, we can.",
            "Identify.",
            "Objects also new videos which have no emotion, so static objects.",
            "For doing this we employ random forests, random forest classifier on two different abstraction levels.",
            "So first we apply a random forest on a super pixel level, which is most commonly used in semantic segmentation approaches where you apply.",
            "Patch based random forest on the pixel level on the Super pixel level.",
            "In our case we do it on a super pixel level.",
            "I will come to that later.",
            "But we also have a second random forests framework which is based on the forest framework to include some higher order information.",
            "So we train half forest on the object level to identify the objects in the cluster.",
            "OK."
        ],
        [
            "Here we see what happens if we apply these classifiers to the input video frames again.",
            "On the top we see the output of the superpixel level classifier.",
            "On the bottom we see the output of the object level classifier.",
            "So the different colors here indicate the different clusters and the grey or black colors indicate the background.",
            "And the intensity of these boxes here.",
            "These are super pixels correspond to how confident the classifier is about this prediction."
        ],
        [
            "So at this point I want to briefly recap what already happened and where we are now.",
            "So we had some input videos.",
            "We did motion segmentation to get object proposals.",
            "We tried to discover semantically similar objects.",
            "Given this object proposes and we train some classifiers and applied them on the input video frames.",
            "OK."
        ],
        [
            "Now we end the second iteration and we again go back to our CRF segmentation, but now we don't have a motion segmentation.",
            "We have a semantic segmentation.",
            "And we can now include not only the optical flow field, so the unary potentials based on the optical flow field."
        ],
        [
            "We can also include semantic information given by our learned classifiers.",
            "So."
        ],
        [
            "So.",
            "Now let's talk about this CRF segment and semantic segmentation a bit.",
            "So, OK, we are given a graph where the work is.",
            "This correspond to the Super pixels which are illustrated here in this little example.",
            "We simply do a super pixel as a regular grip due to a fast computation, so it is quite fast to compute.",
            "Please note that we can also go with you the pixel level, or we could use boundary aligned super pixels.",
            "However, we are focused on identifying the objects rather than on a pixel.",
            "Accurate segmentation of these objects.",
            "The edges in the graph are linked spatially and also temporally with a standard four or in the 3D case 6IN neighborhood.",
            "And the label space in for this graph.",
            "Is K plus one?",
            "So we have one background class and K object categories which we discovered.",
            "And this is 1 difference to the motion segmentation where we only had two classes to discover, so the foreground or background."
        ],
        [
            "OK, to define the unary potentials, we have three unary potentials.",
            "In the second iteration of the CRF we had as an emotion segmentation the unary potential of the optical flow vectors, so which?",
            "Ensures that objects which have high motion also correspond to objects.",
            "And we also include now the output of the classifiers as a unary potential.",
            "So the output of the classifier are normalized between zero and one, so this is a probabilistic output of the classifier for each of the Super pixels.",
            "So we have a standard unary potentials as, so just simply transformed the probability distribution to an into an energy.",
            "For the edges, we have contrast sensitive pairwise potentials which.",
            "Is based on the RGB color vectors but also on the optical flow vectors of course, so different motions should indicate boundaries of the objects.",
            "To minimize this energy formulation, we use the sum of graph cut and for details on how the energy model looks like Howdy.",
            "All the different unary potentials are weighted and combined.",
            "Please have a look at the data paper to find the detail."
        ],
        [
            "More important, the output of the CRF based semantic segmentation is not the same.",
            "Typically labeled video frames, so all of the input video frames get now labeled into semantic.",
            "Groups which we discovered.",
            "So this is 1 example for the car cluster."
        ],
        [
            "OK, so now let's come to our experiments.",
            "We did two experiments, one with video data to show the.",
            "The outcome of our unsupervised object discovery, of course, but we also did an experiment on still images where we try to apply our unsupervised trained object models.",
            "On still images in an object detection task, so the input for our framework are always the videos from online buhman which consists of 96 videos having more than 7000 frames.",
            "And it is also captured with a non static handheld camera which makes it more challenging."
        ],
        [
            "So let's start with the first experiment, which is unsupervised object discovery in video.",
            "So the main topic.",
            "The intention here is to show that we can successfully discover the objects which are captured in the videos.",
            "He uses accuracy measure purati, which is a standard measure for unsupervised object discovery or clustering.",
            "And as we don't have the ground truth and annotations on the pixel level, we can only, we only have it on the.",
            "So we only have weak ground truth data.",
            "So we can we iframe is, in our case correctly classified.",
            "If the largest segment.",
            "Is correctly labeled to the corresponding class."
        ],
        [
            "We evaluated different the different variants of our approach and evaluated different parts we have, and we also compared with rather standard baseline, which is the work of Russell and colleagues.",
            "However, this is a very similar pipeline, 2 hours.",
            "They also use some pre segmentations of the of the images and do and then do a simple clustering.",
            "Here we can see on the left side that we outperform this standard baseline.",
            "We also see that the outlier removal which is based on our.",
            "Um?",
            "On the motion assumptions having drastically increases the performance from 62% to 75 point 1%, and we also see that the combination of the two semantic classifiers gives the best result in our case.",
            "So on the right side we see a confusion matrix of the four different categories in this.",
            "In these videos, which are bicycle cars, pedestrians in street?"
        ],
        [
            "And we see that bicycles and pedestrians are got most often confused, which is somewhat clear because in the videos the bicycles are always written by persons."
        ],
        [
            "Here we see some qualitative results, and especially in the middle block we see that we also can discover non moving objects which are the parking cars in this case in the background or the one.",
            "Still sending person here.",
            "He"
        ],
        [
            "We see some qualitative results in a video format.",
            "Well again, the different colors correspond to the different objects."
        ],
        [
            "Igor is obviously, so let's come to our final experiment, which is recognition is still images, and here the intention is to show that our unsupervised trained object models so."
        ],
        [
            "This one here and in the overview figure generalize well to also still images, not only videos."
        ],
        [
            "OK, so as I said, we trained hard forests, which is a standard object detector and can be directly applied to steal images.",
            "We use two different datasets that you D pedestrian and they teach set cars data set.",
            "And we always compared three different models, so the first model is the unsupervised tough forests which is trained only with the unsupervised frames from the videos which get discovered.",
            "The second one is the supervised model which is trained on the bounding box aligned still images from the corresponding data set.",
            "And the third model is the combined model.",
            "Where we had both, we just merged both image sets."
        ],
        [
            "So here we see the results on the 2D pedestrian data set and we see that.",
            "Wow.",
            "That the unsupervised model actually get real.",
            "Some reasonable resource when we consider the fact that it is trained completely unsupervised.",
            "However, we also see that.",
            "That the supervised models is still better than the combined model, so we do don't win anything here with the additional unlabeled data.",
            "But the reason for this is that the pedestrian data set in the test set always only shows pedestrians from the side view, so the additional information from the pedestrians in the videos doesn't help the model actually so actually destroys the model somehow.",
            "How?"
        ],
        [
            "The behavior is very different too.",
            "In the DH set cars data set.",
            "Here, the combined model significantly outperforms the supervised model by a margin of 7% in average position.",
            "This is actually a motivating result because we have to consider that these additional data comes completely for free and it.",
            "Drastically improved the train model of the cars."
        ],
        [
            "OK, two.",
            "To conclude this talk, I briefly summarize the key aspects of our work.",
            "So what we did is unsupervised object discovery an videos.",
            "And we wanted to show that motion in the videos is a strong and physically valid object prior and we included both motion and then appearance cues in a joint CRF formulation to discover these objects.",
            "We showed that we can successfully discover objects in the videos of unlabeled videos and we also showed that our unsupervised trained models.",
            "Can even be even be applied to still images in an object detection task."
        ],
        [
            "Thank you for interest.",
            "You solve your final semantics here F. Just by standard graph cut.",
            "Because if I understand it correctly, it's now a multilabel problem, right?",
            "And I guess you have kinda.",
            "Contrast sensitive Potts model on it, and this is NP hard to solve, so you should apply some approximation methods Now.",
            "I mean, it is you can't solve it exactly by graph cut.",
            "It is not exactly solved by graph cut.",
            "Of course it is a multi label problem and we solve it with Alpha, better swap or with the Alpha expansion version of the high.",
            "It's nice results, but I wonder if you have video as input, why not attempt?",
            "3D reconstruction before trying to segment objects.",
            "Isn't that going to make your life much easier?",
            "Well, of course this.",
            "Could make life easier, but also the approach more difficult, but.",
            "Well, our intention was to to.",
            "Julie, unsupervised object discovery only with Judy video frames.",
            "So I mean the categories you've discovered so far are quite obvious categories in a way.",
            "So yeah, you kind of wonder what what other sorts of objects could you discover by this method?",
            "Or is it should actually, without going to 3D?",
            "This is the question also of the scalability of this approach to many different categories.",
            "Of course, in this work we only had four categories, which is rather simple an.",
            "Increasing this set I guess will breakdown the method so you have to come up with more elaborate.",
            "Informations of priors.",
            "So 3D reconstruction would definitely be one of them.",
            "But we have to try what?",
            "What this CFA segmentation can do and what the capability of learning classifier is if we have really more categories and of course the clustering approach also has to deal with the scalability of K, could you comment on the how do you?",
            "I mean when you showed this this image sequence with a moving car with the disc golf then there is a lot of ego motion.",
            "I think the camera is driving parallel to the car and there's a lot of ego motion in the background so.",
            "Where's the optic flow of the background gun?",
            "So I want.",
            "Also, if you say you said large flow vectors correspond to objects now if there was an object standing still but between the camera and the car it would have a larger motion.",
            "How do you get rid of?",
            "This is one point I forgot to mention.",
            "It is typically the case that the cameraman tracks the object, so is often the obvious the opposite case.",
            "So the object itself or the object of interest has no motion and the background has all the motion.",
            "So what we do is a simple background motion subtraction, so we just look at the so the assumption is that the background so the frame of one or the border of 1 frame shows typically background, which is the average D optical flow at this point and subtract it.",
            "So we do a background motion subtraction summer.",
            "So and then the assumption that the objects of interest have large optical flow vectors, again, well it."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to present our work titled Unsupervised Object Discovery and Segmentation in Videos.",
                    "label": 0
                },
                {
                    "sent": "Has already said this was joint work with Christian license from Microsoft Austria and Peter Rodent Auspicia from Gratz University of Technology.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what is unsupervised object discovery?",
                    "label": 0
                },
                {
                    "sent": "Let's assume we are given a set of unlabeled images like these ones.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The goal in unsupervised object discovery is to find common visual concepts across these sets of images.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like the bicycles.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cars in this example.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the typical approach to unsupervised object discovery builds on a collection of unlabeled still images and often topic modeling or clustering methods are used.",
                    "label": 1
                },
                {
                    "sent": "And however, these methods always have to rely somehow on any form of prior information, like arbitrary or pre segmentations of images which act like.",
                    "label": 0
                },
                {
                    "sent": "Initial object proposals or objectives measures.",
                    "label": 0
                },
                {
                    "sent": "However, these also have to rely on some supervision to train these objective measures.",
                    "label": 0
                },
                {
                    "sent": "Of course there are many other different priors which could be used, however, a reliable discovery of objects and unlabeled images.",
                    "label": 1
                },
                {
                    "sent": "Without priors is typically quite difficult.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this work we want to emphasize the use of videos instead of still images, because in videos we can exploit motion and motion is a very strong and physically valid prior for objects.",
                    "label": 1
                },
                {
                    "sent": "So using videos instead of still images has many benefits.",
                    "label": 0
                },
                {
                    "sent": "For example, we can do a simple motion segmentation to extract object proposals from the background.",
                    "label": 0
                },
                {
                    "sent": "Videos typically show a high wearability of the object's appearance if they're moving, and we can easily access a huge amount of data if we go to online platforms like YouTube or video with you.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this paper we redefine the task of unsupervised object discovery a little bit such that we are given a set of unlabeled videos instead of images.",
                    "label": 1
                },
                {
                    "sent": "The goal still is the same.",
                    "label": 0
                },
                {
                    "sent": "We try to discover the objects which are moving in the videos and we try to assign them semantic label.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, at this point I want to give you a brief outline after the presentation.",
                    "label": 0
                },
                {
                    "sent": "First I will show you our approach to answer Quest object discovering we do is I will give you an overview and show you the building blocks.",
                    "label": 1
                },
                {
                    "sent": "And of course, at the end of the presentation I will show you our experimental results.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here we see the outline of our approach, which consists of several building blocks we see.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Start with a set of unlabeled and unordered videos.",
                    "label": 0
                },
                {
                    "sent": "Then we.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can calculate optical flow based on the videos and with a simple CRF based motion segmentation we can get Motion proposes which act as potential objects which.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We discovered in the videos and given these proposals we try to cluster them.",
                    "label": 0
                },
                {
                    "sent": "We try to find or identify semantically similar groups.",
                    "label": 0
                },
                {
                    "sent": "And then we try.",
                    "label": 0
                },
                {
                    "sent": "We learn object models for each of the four clusters and we apply them to.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The input video frames again.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this allows us to do a CRF based semantic segmentation in a second iteration where we include now the optical flow information.",
                    "label": 0
                },
                {
                    "sent": "So the motion information, but also the output of the classifiers which are trained for each of the clusters.",
                    "label": 0
                },
                {
                    "sent": "And this then gives the final output of our unsupervised approach.",
                    "label": 0
                },
                {
                    "sent": "We get semantically segmented video frames.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's start with motion segmentation.",
                    "label": 1
                },
                {
                    "sent": "As I said, we have a CRF based motion CRF based energy formulation.",
                    "label": 0
                },
                {
                    "sent": "I don't want to go into detail here.",
                    "label": 0
                },
                {
                    "sent": "You can read up the details of the energy formulation in the paper.",
                    "label": 0
                },
                {
                    "sent": "The only intention I want to give here is that we say that large optical flow vectors should correspond to potential objects because objects are moving, so they have large optical flow vector and we use this.",
                    "label": 1
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As well as our unary potentials in the motion segmentation.",
                    "label": 0
                },
                {
                    "sent": "So here we see the optical flow.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here we see the result of the motion segmentation.",
                    "label": 1
                },
                {
                    "sent": "And as we can see here.",
                    "label": 0
                },
                {
                    "sent": "These motion segments somehow correspond to potential.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Object proposals so Motion segment is somehow object proposes or we regard it as an object proposal.",
                    "label": 1
                },
                {
                    "sent": "However, these proposals are of course typically quite noisy because the optical flow calculation can fail the motion segmentation can fail.",
                    "label": 0
                },
                {
                    "sent": "Arbitrary, uninteresting objects could move in the videos.",
                    "label": 0
                },
                {
                    "sent": "However, at this point we can again exploit the motion information which is inherently given in the videos.",
                    "label": 0
                },
                {
                    "sent": "Because of if we assume we have video which captures the frames at let's say 50 frames per second, we can make quite strong assumptions on the objects which are captured in the videos because they should move smooth issues through space and time.",
                    "label": 0
                },
                {
                    "sent": "So what we can do here is we can easily remove potential outliers.",
                    "label": 0
                },
                {
                    "sent": "By simply looking at some statistics of the proposals, like the trajectories which move through space and time, and we do some simple curve align fitting to remove the obvious outliers.",
                    "label": 0
                },
                {
                    "sent": "In this case, and I also want to mention here that this would not be possible if you have an unordered set of still images which have no relation.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so after having filtered these proposals we are left with some.",
                    "label": 0
                },
                {
                    "sent": "Potential object proposals which are not bounding box annotated.",
                    "label": 1
                },
                {
                    "sent": "Actually, in the unlabeled videos.",
                    "label": 0
                },
                {
                    "sent": "And we now employ a standard.",
                    "label": 0
                },
                {
                    "sent": "Clustering approach to identify semantically similar groups.",
                    "label": 0
                },
                {
                    "sent": "So this is standard approach which was often used in unsupervised object discovery approaches and still images.",
                    "label": 1
                },
                {
                    "sent": "Where you have a pre segmentation and then you simply do a clustering and so we calculate a feature vector for each of the remaining proposals.",
                    "label": 1
                },
                {
                    "sent": "In our case, we use the standard bag of words description with the spatial pyramid based on tensift.",
                    "label": 0
                },
                {
                    "sent": "And then we have to choose the number of categories K we want to discover an we apply a simple spectral clustering approach based on the squared distance.",
                    "label": 1
                },
                {
                    "sent": "Please note that we can also employ K means or any other clustering algorithm.",
                    "label": 0
                },
                {
                    "sent": "We can exchange the features or anything else.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now I want to briefly show you some clustering results.",
                    "label": 0
                },
                {
                    "sent": "Here we see 6.",
                    "label": 0
                },
                {
                    "sent": "Object proposals which could cost it to, let's say cluster one, and this obviously corresponds to the bicycle cluster at this.",
                    "label": 0
                },
                {
                    "sent": "Here we see also that the motion segmentation actually do quite good job, because the bounding box is tightly bound the object.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here we see another result where OK, this cluster obviously corresponds to the cars cluster.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, and based on these clusters we learn an object model for each of them and this then allows to also discover non moving objects in the videos because at this point now we always limited to having only proposals from objects which are moving.",
                    "label": 0
                },
                {
                    "sent": "However, when we train an object model, we can.",
                    "label": 0
                },
                {
                    "sent": "Identify.",
                    "label": 0
                },
                {
                    "sent": "Objects also new videos which have no emotion, so static objects.",
                    "label": 1
                },
                {
                    "sent": "For doing this we employ random forests, random forest classifier on two different abstraction levels.",
                    "label": 1
                },
                {
                    "sent": "So first we apply a random forest on a super pixel level, which is most commonly used in semantic segmentation approaches where you apply.",
                    "label": 0
                },
                {
                    "sent": "Patch based random forest on the pixel level on the Super pixel level.",
                    "label": 0
                },
                {
                    "sent": "In our case we do it on a super pixel level.",
                    "label": 0
                },
                {
                    "sent": "I will come to that later.",
                    "label": 1
                },
                {
                    "sent": "But we also have a second random forests framework which is based on the forest framework to include some higher order information.",
                    "label": 0
                },
                {
                    "sent": "So we train half forest on the object level to identify the objects in the cluster.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here we see what happens if we apply these classifiers to the input video frames again.",
                    "label": 0
                },
                {
                    "sent": "On the top we see the output of the superpixel level classifier.",
                    "label": 1
                },
                {
                    "sent": "On the bottom we see the output of the object level classifier.",
                    "label": 0
                },
                {
                    "sent": "So the different colors here indicate the different clusters and the grey or black colors indicate the background.",
                    "label": 0
                },
                {
                    "sent": "And the intensity of these boxes here.",
                    "label": 0
                },
                {
                    "sent": "These are super pixels correspond to how confident the classifier is about this prediction.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So at this point I want to briefly recap what already happened and where we are now.",
                    "label": 0
                },
                {
                    "sent": "So we had some input videos.",
                    "label": 0
                },
                {
                    "sent": "We did motion segmentation to get object proposals.",
                    "label": 0
                },
                {
                    "sent": "We tried to discover semantically similar objects.",
                    "label": 0
                },
                {
                    "sent": "Given this object proposes and we train some classifiers and applied them on the input video frames.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we end the second iteration and we again go back to our CRF segmentation, but now we don't have a motion segmentation.",
                    "label": 0
                },
                {
                    "sent": "We have a semantic segmentation.",
                    "label": 0
                },
                {
                    "sent": "And we can now include not only the optical flow field, so the unary potentials based on the optical flow field.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can also include semantic information given by our learned classifiers.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Now let's talk about this CRF segment and semantic segmentation a bit.",
                    "label": 1
                },
                {
                    "sent": "So, OK, we are given a graph where the work is.",
                    "label": 0
                },
                {
                    "sent": "This correspond to the Super pixels which are illustrated here in this little example.",
                    "label": 1
                },
                {
                    "sent": "We simply do a super pixel as a regular grip due to a fast computation, so it is quite fast to compute.",
                    "label": 0
                },
                {
                    "sent": "Please note that we can also go with you the pixel level, or we could use boundary aligned super pixels.",
                    "label": 0
                },
                {
                    "sent": "However, we are focused on identifying the objects rather than on a pixel.",
                    "label": 1
                },
                {
                    "sent": "Accurate segmentation of these objects.",
                    "label": 0
                },
                {
                    "sent": "The edges in the graph are linked spatially and also temporally with a standard four or in the 3D case 6IN neighborhood.",
                    "label": 0
                },
                {
                    "sent": "And the label space in for this graph.",
                    "label": 1
                },
                {
                    "sent": "Is K plus one?",
                    "label": 0
                },
                {
                    "sent": "So we have one background class and K object categories which we discovered.",
                    "label": 0
                },
                {
                    "sent": "And this is 1 difference to the motion segmentation where we only had two classes to discover, so the foreground or background.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, to define the unary potentials, we have three unary potentials.",
                    "label": 0
                },
                {
                    "sent": "In the second iteration of the CRF we had as an emotion segmentation the unary potential of the optical flow vectors, so which?",
                    "label": 1
                },
                {
                    "sent": "Ensures that objects which have high motion also correspond to objects.",
                    "label": 0
                },
                {
                    "sent": "And we also include now the output of the classifiers as a unary potential.",
                    "label": 0
                },
                {
                    "sent": "So the output of the classifier are normalized between zero and one, so this is a probabilistic output of the classifier for each of the Super pixels.",
                    "label": 0
                },
                {
                    "sent": "So we have a standard unary potentials as, so just simply transformed the probability distribution to an into an energy.",
                    "label": 0
                },
                {
                    "sent": "For the edges, we have contrast sensitive pairwise potentials which.",
                    "label": 0
                },
                {
                    "sent": "Is based on the RGB color vectors but also on the optical flow vectors of course, so different motions should indicate boundaries of the objects.",
                    "label": 0
                },
                {
                    "sent": "To minimize this energy formulation, we use the sum of graph cut and for details on how the energy model looks like Howdy.",
                    "label": 0
                },
                {
                    "sent": "All the different unary potentials are weighted and combined.",
                    "label": 0
                },
                {
                    "sent": "Please have a look at the data paper to find the detail.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "More important, the output of the CRF based semantic segmentation is not the same.",
                    "label": 1
                },
                {
                    "sent": "Typically labeled video frames, so all of the input video frames get now labeled into semantic.",
                    "label": 1
                },
                {
                    "sent": "Groups which we discovered.",
                    "label": 0
                },
                {
                    "sent": "So this is 1 example for the car cluster.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now let's come to our experiments.",
                    "label": 0
                },
                {
                    "sent": "We did two experiments, one with video data to show the.",
                    "label": 1
                },
                {
                    "sent": "The outcome of our unsupervised object discovery, of course, but we also did an experiment on still images where we try to apply our unsupervised trained object models.",
                    "label": 1
                },
                {
                    "sent": "On still images in an object detection task, so the input for our framework are always the videos from online buhman which consists of 96 videos having more than 7000 frames.",
                    "label": 1
                },
                {
                    "sent": "And it is also captured with a non static handheld camera which makes it more challenging.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's start with the first experiment, which is unsupervised object discovery in video.",
                    "label": 1
                },
                {
                    "sent": "So the main topic.",
                    "label": 0
                },
                {
                    "sent": "The intention here is to show that we can successfully discover the objects which are captured in the videos.",
                    "label": 0
                },
                {
                    "sent": "He uses accuracy measure purati, which is a standard measure for unsupervised object discovery or clustering.",
                    "label": 0
                },
                {
                    "sent": "And as we don't have the ground truth and annotations on the pixel level, we can only, we only have it on the.",
                    "label": 0
                },
                {
                    "sent": "So we only have weak ground truth data.",
                    "label": 0
                },
                {
                    "sent": "So we can we iframe is, in our case correctly classified.",
                    "label": 0
                },
                {
                    "sent": "If the largest segment.",
                    "label": 1
                },
                {
                    "sent": "Is correctly labeled to the corresponding class.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We evaluated different the different variants of our approach and evaluated different parts we have, and we also compared with rather standard baseline, which is the work of Russell and colleagues.",
                    "label": 0
                },
                {
                    "sent": "However, this is a very similar pipeline, 2 hours.",
                    "label": 0
                },
                {
                    "sent": "They also use some pre segmentations of the of the images and do and then do a simple clustering.",
                    "label": 0
                },
                {
                    "sent": "Here we can see on the left side that we outperform this standard baseline.",
                    "label": 0
                },
                {
                    "sent": "We also see that the outlier removal which is based on our.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "On the motion assumptions having drastically increases the performance from 62% to 75 point 1%, and we also see that the combination of the two semantic classifiers gives the best result in our case.",
                    "label": 0
                },
                {
                    "sent": "So on the right side we see a confusion matrix of the four different categories in this.",
                    "label": 1
                },
                {
                    "sent": "In these videos, which are bicycle cars, pedestrians in street?",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we see that bicycles and pedestrians are got most often confused, which is somewhat clear because in the videos the bicycles are always written by persons.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here we see some qualitative results, and especially in the middle block we see that we also can discover non moving objects which are the parking cars in this case in the background or the one.",
                    "label": 1
                },
                {
                    "sent": "Still sending person here.",
                    "label": 0
                },
                {
                    "sent": "He",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We see some qualitative results in a video format.",
                    "label": 0
                },
                {
                    "sent": "Well again, the different colors correspond to the different objects.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Igor is obviously, so let's come to our final experiment, which is recognition is still images, and here the intention is to show that our unsupervised trained object models so.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This one here and in the overview figure generalize well to also still images, not only videos.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so as I said, we trained hard forests, which is a standard object detector and can be directly applied to steal images.",
                    "label": 1
                },
                {
                    "sent": "We use two different datasets that you D pedestrian and they teach set cars data set.",
                    "label": 0
                },
                {
                    "sent": "And we always compared three different models, so the first model is the unsupervised tough forests which is trained only with the unsupervised frames from the videos which get discovered.",
                    "label": 0
                },
                {
                    "sent": "The second one is the supervised model which is trained on the bounding box aligned still images from the corresponding data set.",
                    "label": 0
                },
                {
                    "sent": "And the third model is the combined model.",
                    "label": 1
                },
                {
                    "sent": "Where we had both, we just merged both image sets.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here we see the results on the 2D pedestrian data set and we see that.",
                    "label": 0
                },
                {
                    "sent": "Wow.",
                    "label": 0
                },
                {
                    "sent": "That the unsupervised model actually get real.",
                    "label": 0
                },
                {
                    "sent": "Some reasonable resource when we consider the fact that it is trained completely unsupervised.",
                    "label": 0
                },
                {
                    "sent": "However, we also see that.",
                    "label": 0
                },
                {
                    "sent": "That the supervised models is still better than the combined model, so we do don't win anything here with the additional unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "But the reason for this is that the pedestrian data set in the test set always only shows pedestrians from the side view, so the additional information from the pedestrians in the videos doesn't help the model actually so actually destroys the model somehow.",
                    "label": 0
                },
                {
                    "sent": "How?",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The behavior is very different too.",
                    "label": 0
                },
                {
                    "sent": "In the DH set cars data set.",
                    "label": 0
                },
                {
                    "sent": "Here, the combined model significantly outperforms the supervised model by a margin of 7% in average position.",
                    "label": 1
                },
                {
                    "sent": "This is actually a motivating result because we have to consider that these additional data comes completely for free and it.",
                    "label": 0
                },
                {
                    "sent": "Drastically improved the train model of the cars.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, two.",
                    "label": 0
                },
                {
                    "sent": "To conclude this talk, I briefly summarize the key aspects of our work.",
                    "label": 0
                },
                {
                    "sent": "So what we did is unsupervised object discovery an videos.",
                    "label": 0
                },
                {
                    "sent": "And we wanted to show that motion in the videos is a strong and physically valid object prior and we included both motion and then appearance cues in a joint CRF formulation to discover these objects.",
                    "label": 1
                },
                {
                    "sent": "We showed that we can successfully discover objects in the videos of unlabeled videos and we also showed that our unsupervised trained models.",
                    "label": 1
                },
                {
                    "sent": "Can even be even be applied to still images in an object detection task.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you for interest.",
                    "label": 0
                },
                {
                    "sent": "You solve your final semantics here F. Just by standard graph cut.",
                    "label": 0
                },
                {
                    "sent": "Because if I understand it correctly, it's now a multilabel problem, right?",
                    "label": 0
                },
                {
                    "sent": "And I guess you have kinda.",
                    "label": 0
                },
                {
                    "sent": "Contrast sensitive Potts model on it, and this is NP hard to solve, so you should apply some approximation methods Now.",
                    "label": 0
                },
                {
                    "sent": "I mean, it is you can't solve it exactly by graph cut.",
                    "label": 0
                },
                {
                    "sent": "It is not exactly solved by graph cut.",
                    "label": 0
                },
                {
                    "sent": "Of course it is a multi label problem and we solve it with Alpha, better swap or with the Alpha expansion version of the high.",
                    "label": 0
                },
                {
                    "sent": "It's nice results, but I wonder if you have video as input, why not attempt?",
                    "label": 0
                },
                {
                    "sent": "3D reconstruction before trying to segment objects.",
                    "label": 0
                },
                {
                    "sent": "Isn't that going to make your life much easier?",
                    "label": 0
                },
                {
                    "sent": "Well, of course this.",
                    "label": 0
                },
                {
                    "sent": "Could make life easier, but also the approach more difficult, but.",
                    "label": 0
                },
                {
                    "sent": "Well, our intention was to to.",
                    "label": 0
                },
                {
                    "sent": "Julie, unsupervised object discovery only with Judy video frames.",
                    "label": 0
                },
                {
                    "sent": "So I mean the categories you've discovered so far are quite obvious categories in a way.",
                    "label": 0
                },
                {
                    "sent": "So yeah, you kind of wonder what what other sorts of objects could you discover by this method?",
                    "label": 0
                },
                {
                    "sent": "Or is it should actually, without going to 3D?",
                    "label": 0
                },
                {
                    "sent": "This is the question also of the scalability of this approach to many different categories.",
                    "label": 0
                },
                {
                    "sent": "Of course, in this work we only had four categories, which is rather simple an.",
                    "label": 0
                },
                {
                    "sent": "Increasing this set I guess will breakdown the method so you have to come up with more elaborate.",
                    "label": 0
                },
                {
                    "sent": "Informations of priors.",
                    "label": 0
                },
                {
                    "sent": "So 3D reconstruction would definitely be one of them.",
                    "label": 0
                },
                {
                    "sent": "But we have to try what?",
                    "label": 0
                },
                {
                    "sent": "What this CFA segmentation can do and what the capability of learning classifier is if we have really more categories and of course the clustering approach also has to deal with the scalability of K, could you comment on the how do you?",
                    "label": 0
                },
                {
                    "sent": "I mean when you showed this this image sequence with a moving car with the disc golf then there is a lot of ego motion.",
                    "label": 0
                },
                {
                    "sent": "I think the camera is driving parallel to the car and there's a lot of ego motion in the background so.",
                    "label": 0
                },
                {
                    "sent": "Where's the optic flow of the background gun?",
                    "label": 0
                },
                {
                    "sent": "So I want.",
                    "label": 0
                },
                {
                    "sent": "Also, if you say you said large flow vectors correspond to objects now if there was an object standing still but between the camera and the car it would have a larger motion.",
                    "label": 0
                },
                {
                    "sent": "How do you get rid of?",
                    "label": 0
                },
                {
                    "sent": "This is one point I forgot to mention.",
                    "label": 0
                },
                {
                    "sent": "It is typically the case that the cameraman tracks the object, so is often the obvious the opposite case.",
                    "label": 0
                },
                {
                    "sent": "So the object itself or the object of interest has no motion and the background has all the motion.",
                    "label": 0
                },
                {
                    "sent": "So what we do is a simple background motion subtraction, so we just look at the so the assumption is that the background so the frame of one or the border of 1 frame shows typically background, which is the average D optical flow at this point and subtract it.",
                    "label": 0
                },
                {
                    "sent": "So we do a background motion subtraction summer.",
                    "label": 0
                },
                {
                    "sent": "So and then the assumption that the objects of interest have large optical flow vectors, again, well it.",
                    "label": 0
                }
            ]
        }
    }
}