{
    "id": "6h6tbmem3yifoahn7r7c6gyl22lyi2hv",
    "title": "Ranking with Ordered Weighted Pairwise Classi\ufb01cation",
    "info": {
        "author": [
            "Nicolas Usunier, LIP6, Universit\u00e9 Pierre et Marie Curie - Paris 6"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_usunier_rwowpwc/",
    "segmentation": [
        [
            "OK, so hello so this the work presented is joint work with Debbie.",
            "Funny and true."
        ],
        [
            "Gallanari so I will first give a brief introduction to sorry.",
            "I will first make a brief introduction to learning to rank and then the.",
            "Describe our framework and show how we can build convex losses for ranking in very general setting and then prove a margin based generation analysis in our framework and then show experiments on an information retrieval data set."
        ],
        [
            "So we consider a very simple framework of learning drunk, so it's fairly classical.",
            "An example is an observation and supervision.",
            "So for each observation we have a set of candidates that we will have to order, and we focus on setting where we have binary relevance judgments.",
            "So that's the supervision is just the set of indexes of the relevant candidates, right?",
            "So we have the candidates are either relevant or irrelevant, and the supervision is the set of relevance."
        ],
        [
            "So we want to learn the score function so we function that assigns a score for each candidate.",
            "Then the system orders the candidates according to the by decreasing order of scores.",
            "And the goal is to learn the function with low ranking error, which means the ranking error only depends on the rank of the relevant documents the relevant."
        ],
        [
            "Candidates.",
            "And the ranking error may depend on the task at hand.",
            "So in our experiments, we will focus on information retrieval since it's very difficult problem of ranking.",
            "But more generally, our framework can also be used in multiclass classification and other problems."
        ],
        [
            "So we give us background pairwise approach so it can be described in three steps.",
            "First we make the pairwise comparisons, so we take relevant candidates and irrelevant candidates and we count an error if the relative scores do not match the relevant relev."
        ],
        [
            "And then the ranking error is given by aggregating the pairwise error.",
            "So basically if we take the mean of all of the pairwise errors that we just defined.",
            "Then it boils down to optimizing the mean rank of the relevant documents of the relevant elements.",
            "Now if we take the Max operator over the pairwise errors.",
            "It boils down to checking if the relevant documents are top ranked.",
            "So this is the two settings that exist actually in the pairwise setting.",
            "And after this aggregation, in fact, we can obtain convex losses for ranking by."
        ],
        [
            "Simply changing the 01 classification loss with convex classification loss.",
            "So if for instance we take the hinge loss.",
            "Then we obtain ranking SVM.",
            "When we use the mean operator to aggregates the losses or the multiclass SVM where we use when we use the Max operator to aggregate the losses and if we take the expansion loss we recover the Wrangells algorithm.",
            "So although this approach is very simple.",
            "The problem is that it's not suitable for tasks like information retrieval where."
        ],
        [
            "We require high precision on the top of the list.",
            "Typically if we take if we try to optimize the mean rank of the relevant elements, then it's not very informative for what happens in the top of the list and on the other hand the Max operator which just checks for the first position it's too aggressive and so it's not suitable because it's just forgets the."
        ],
        [
            "First positions.",
            "So it has been shown that the quality measures used in information retrieval are not correlated to the errors defined in the pairwise approach.",
            "So basically, in the context of information retrieval, there has been a lot of previous work which do not focus more on pairwise comparison, but more use listwise loss functions or smooth approximation or convex upper bounds on our metrics.",
            "So there is a lot of algorithms, so they work quite well, but we will show an alternative that can use directly the pairwise approach to optimize to optimize.",
            "Ranking errors that focus on the top of the list.",
            "So basically the contribution of the of the paper is an extension of the pairwise approach to a very general setting of franking which can be applied to learning to rank for information retrieval.",
            "So the basis is to define family of ranking error functions which."
        ],
        [
            "Not exactly the ones that are used as evaluation metrics in IR, but for which we can define very easily.",
            "Context upper bounds and in a very general setting.",
            "So we will show how using aggregators more general than the Max, the Max or the mean.",
            "We can actually start from the pairwise pairwise losses and then build an error that focus on the whole rank list.",
            "And we'll see how we can have margin base error bounds in our framework.",
            "So the basis of our framework is this definition of the ranking error function, so basically.",
            "Here it's the number of relevant elements, and we define the rank of a given relevant element as.",
            "The number of irrelevant elements that have a better score.",
            "So basically it's the position in the sorted list, but we ignore the relative ranks of the relevant elements.",
            "OK, so we simply count the number of irrelevant elements that have better scores and then we use function Phi to transform the actual rank of the relevant elements.",
            "Two loss value to an error value.",
            "So here the AA Jays are the increments of the error function here, so it's done so that when a relevant element is exactly at the end of the list.",
            "The error is 1 and one relevant element is before every irrelevant ones.",
            "The error is 0 and then the increments are decreasing.",
            "What does it mean?",
            "It means that the error function grows faster for small ranks and then it grows slowly for large wrong values.",
            "So basically the idea is that if we lose the rank at the beginning of the list, it costs more and then if we lose a rank at the end of the list.",
            "So optimizing this error metric will actually allow us to focus on the very beginning of the list and everything depends on the weights Alpha.",
            "It's a very general formula and basically they depending on the value of the weights, we can express various preferences between the beginning of the list and the whole list.",
            "So for instance."
        ],
        [
            "If we take the first weights at one and order the other weights at zero, we actually have an error which is 1.",
            "As soon as relevant element is not exactly top front, so it's the very aggressive error that is used in multiclass classification for instance.",
            "If we take the weights all equal.",
            "We then recover them in range.",
            "And by choosing weights that are in between the two extremes, we are able to provide error functions that will focus more more or less on the top of the list depending on what we want.",
            "So for instance we can optimize them in rank on the top percent of the list by setting the weights at 0 after certain after a certain rank.",
            "So we now give how to use the pairwise."
        ],
        [
            "Approach and how to aggregate the pairwise loss to recover.",
            "The error function that we just defined so that we have.",
            "We will have a convex upper bound on an arrow that can focus on the top of the list.",
            "The basis of our framework is to use the ordered weighted averaging operators, which are defined by yoga in 1988.",
            "So the overall idea is to have a set of weights which are positive and sum to one.",
            "And then it takes as input."
        ],
        [
            "Set of real values."
        ],
        [
            "We saw them by."
        ],
        [
            "Decreasing order.",
            "And then there is a weighted sum of the sort of the sorted values.",
            "So basically the JS weight is applied to the JS to the JS greatest value.",
            "So it's a."
        ],
        [
            "A class of operators that generalize the Max, the mean and actually also the minimum, but we won't talk about it here.",
            "So if you take the 1st way to be one and all of the weights at 0.",
            "Then the OWA is just exactly the Max and if."
        ],
        [
            "You take the mean.",
            "All weights equal, you recover the mean.",
            "Basically, how I will now show how we can use it to recover the error function.",
            "We have the two definitions.",
            "We have the definition of."
        ],
        [
            "The OWA operator and we have the definition of our function that transform a rank into an error value.",
            "So if we pick if we fix an example an pick a relevant element in for this example.",
            "We define the weights of the."
        ],
        [
            "The operator to be exactly the weight Alpha J's here, so we take the same weights in the OWA operator and.",
            "In the."
        ],
        [
            "Function.",
            "Then if we consider the set.",
            "Of pairwise losses here.",
            "And apply to it the OWA."
        ],
        [
            "Operator when we thought the values or the ones become become first and there are as many ones as the rank of the relevant elements."
        ],
        [
            "So when we make a weighted sum.",
            "We actually recover exactly the sum of the first KK."
        ],
        [
            "Wait, so basically the ordered weighted averaging of the pairwise loss is exactly.",
            "The the error function.",
            "No, we can use the same trick as in the pairwise classification to obtain convex upper bounds on the loss function."
        ],
        [
            "So we can write our error with OWA."
        ],
        [
            "Raiders and we use additional properties, so if we have an ordered weighted averaging operator with non increasing weights then it is convex and we are in the case of non increasing weights.",
            "By the definition of our error function because we said that the weights should be decreasing to focus on the top of the list."
        ],
        [
            "And so if Moreover we use a convex upper bound on the standards classification error, then the ordered weighted averaging of the loss of the convex loss is also a convex upper bound on the weighted average of the 01 error.",
            "So we have a very general way of designing convex loss functions for ranking.",
            "We just take any convex loss and.",
            "Aggregate them with an ordered weighted averaging copper."
        ],
        [
            "So for instance, if we consider the problem of learning linear scoring functions and we use the hinge loss so the brackets and plus it's the positive part."
        ],
        [
            "We use here we just obtain.",
            "A natural convex upper bound on.",
            "An error function that we defined before.",
            "OK."
        ],
        [
            "So.",
            "If now we go on with the regularised empirical risks.",
            "We just put a regularization factor and we obtain some very some well known special cases.",
            "If there is always one relevant element and we take the OWA operator as being the Max.",
            "And then we just recover the SVM for multiclass classification of common single.",
            "And if we take the OWA as being the mean then we recover the ranking SVM.",
            "But we also have many other new possibilities.",
            "For instance optimize them in rank of the relevant elements on the top paper, P percent of the list and just forget about the relevant elements that are at the bottom of the list.",
            "So it's basically a convex convex function, so many any optimization method can be used.",
            "In our experiments, we."
        ],
        [
            "We showed that it can actually be written as a structural SVM.",
            "And we can use existing algorithms such as the cutting plane or we used LINQ which is.",
            "An efficient solver which also allows to learn online.",
            "Although we didn't try to run online here.",
            "And in terms of complexity, it's about the same complexity as other structured output methods for ranking.",
            "So."
        ],
        [
            "In terms of margin based generalization."
        ],
        [
            "We have our error which is defined with ordered weighted averaging.",
            "We assume for simplicity that there is a single relevant element and that the number of irrelevant element is constant."
        ],
        [
            "In fact, the only important thing is that both of us are constants, but we may have several relevant."
        ],
        [
            "It's.",
            "So we assume that we have a data set with ID examples.",
            "And so there is."
        ],
        [
            "States that if we have a little margin penalty.",
            "The generalization error of our score function is bounded by.",
            "Sorry it doesn't here.",
            "There is the margin term.",
            "So it's the empirical error, but penalized with the margin term here gamma.",
            "And there is a complexity term here which depends on the margin and basically the complexity term decreases with the margin.",
            "So what it means is that it's for generalization.",
            "It should be a good thing to optimize the pairwise margin.",
            "OK, on the.",
            "On the training set, in order to have a smaller complexity term.",
            "So this is also."
        ],
        [
            "Appearance with the loss function with we used with the bad guys hinge loss so we carried out experiments on little 3.0 so we use the six digit.gov datasets.",
            "So there is a train validation and test sets for each datasets, about 50 to 150 queries, and 1000 document queries.",
            "And succeed features for the joint query document representation.",
            "So in terms of evaluation measures, we."
        ],
        [
            "Use the mean average precision, the indecision, the precision, but we will only show in the presentation at the map and the precision.",
            "But the other ones are in the paper.",
            "For hyperparameter selection, we use the best MLP on the validation set.",
            "And for the experiments I report here, we use the weights of the OWA operator, which are fixed to linearly decreasing.",
            "So basically since error metric, our error measures are not exactly those that are used in information retrieval, we have to set the weights so that they will lead to good performance when evaluating when evaluating with the true error metrics.",
            "So this one yields a good compromise between the beginning and the end of the list.",
            "But we also made experiments where the weights are chosen on the validation sets and."
        ],
        [
            "Drawings of paper.",
            "So basically, in terms of mean average precision.",
            "So the average precision for a given query is the mean of the precision at the rank of the relevant elements.",
            "So here we have the six datasets and we chose the baseline.",
            "Results are given on the Microsoft website.",
            "So basically we can see that in terms of MLP.",
            "The pairwise they ordered weighted pairwise classification approach with the one over unk weights, outperforms the algorithms on four out of six datasets.",
            "And it's worse on two datasets.",
            "But still since new algorithm tend to be much better than the other on all datasets, it's.",
            "It's fairly good and we also show the test performance in terms of precision at one.",
            "So here, so precision at one is the percentage of queries having the first document.",
            "Relevant, the first retrieve documents are relevant.",
            "So here on the five datasets we are at the level of the best little baseline.",
            "So basically with these weights we are able to optimize at the same time the precision and and the AP.",
            "And we have other results for N DCG."
        ],
        [
            "So the ordered weighted pairwise classification approach allows to define convex losses for ranking.",
            "So using.",
            "Proxy error functions, not directly those that are using IR.",
            "But generalizes the classical pairwise classification approach and allows to learn function with a high precision on the top of the list.",
            "We have generalization errors for the set of ranking errors we defined and state of the art results on little 3.0.",
            "So one of the restrictions of our framework is that it only deals with binary relevance judgments for now.",
            "So we need to extend it for multivalued relevance treatments.",
            "And we also would like to explore the possibility of learning the weights rather than choosing them as hyperparameters or.",
            "Things like that.",
            "Thank you.",
            "Few minutes for questions."
        ],
        [
            "Did you?",
            "Did you make experiments with the setting different weights?",
            "Yes, figure out whether this really has an influence.",
            "Yes indeed there are some in in the paper we actually tried to optimize, for instance the min rank on the top percent of the list by by varying the P. So basically what we obtain is that for relatively large values of PSA 2020%, ten percent 5%, this yields in general and improvement of maybe.",
            "But when we go to much like we try to optimize on the very first few documents, then it's not very good results and basically.",
            "For instance the.",
            "The varying with the one over rank the linearly decreasing weights are systematically much better than the normalized weights.",
            "And we also tried other schemes, but.",
            "So yes, there is an influence and it's consistent over all the old data sets.",
            "Can you say anything about efficiency or complexity of learning?",
            "Is it?",
            "Yes?",
            "It's rather fast in in our setting, so basically.",
            "For the optimization, so we use LINQ which is known as being an efficient solver for multiclass classification, but it extends naturally to the structured output setting, and basically it's makes iterations and each iteration costs basically the time to sort the items for query and.",
            "Since the data set are rather small, it all depends on the stopping criteria.",
            "But with a good stopping criteria it takes a few seconds for.",
            "Sorry.",
            "But it seemed like what you really wanted to minimize.",
            "The order dated 01 average, right?",
            "Did you report experimental results?",
            "No, we we didn't of for this particular error, we didn't show results, but this was the question.",
            "Yeah yeah no, but it would also be difficult, difficult to compare with other algorithms since they.",
            "They were all evaluated in terms of any P or N DCG, but it would be interesting.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so hello so this the work presented is joint work with Debbie.",
                    "label": 0
                },
                {
                    "sent": "Funny and true.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Gallanari so I will first give a brief introduction to sorry.",
                    "label": 0
                },
                {
                    "sent": "I will first make a brief introduction to learning to rank and then the.",
                    "label": 1
                },
                {
                    "sent": "Describe our framework and show how we can build convex losses for ranking in very general setting and then prove a margin based generation analysis in our framework and then show experiments on an information retrieval data set.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we consider a very simple framework of learning drunk, so it's fairly classical.",
                    "label": 0
                },
                {
                    "sent": "An example is an observation and supervision.",
                    "label": 1
                },
                {
                    "sent": "So for each observation we have a set of candidates that we will have to order, and we focus on setting where we have binary relevance judgments.",
                    "label": 0
                },
                {
                    "sent": "So that's the supervision is just the set of indexes of the relevant candidates, right?",
                    "label": 1
                },
                {
                    "sent": "So we have the candidates are either relevant or irrelevant, and the supervision is the set of relevance.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we want to learn the score function so we function that assigns a score for each candidate.",
                    "label": 0
                },
                {
                    "sent": "Then the system orders the candidates according to the by decreasing order of scores.",
                    "label": 0
                },
                {
                    "sent": "And the goal is to learn the function with low ranking error, which means the ranking error only depends on the rank of the relevant documents the relevant.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Candidates.",
                    "label": 0
                },
                {
                    "sent": "And the ranking error may depend on the task at hand.",
                    "label": 1
                },
                {
                    "sent": "So in our experiments, we will focus on information retrieval since it's very difficult problem of ranking.",
                    "label": 1
                },
                {
                    "sent": "But more generally, our framework can also be used in multiclass classification and other problems.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we give us background pairwise approach so it can be described in three steps.",
                    "label": 0
                },
                {
                    "sent": "First we make the pairwise comparisons, so we take relevant candidates and irrelevant candidates and we count an error if the relative scores do not match the relevant relev.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then the ranking error is given by aggregating the pairwise error.",
                    "label": 0
                },
                {
                    "sent": "So basically if we take the mean of all of the pairwise errors that we just defined.",
                    "label": 1
                },
                {
                    "sent": "Then it boils down to optimizing the mean rank of the relevant documents of the relevant elements.",
                    "label": 1
                },
                {
                    "sent": "Now if we take the Max operator over the pairwise errors.",
                    "label": 1
                },
                {
                    "sent": "It boils down to checking if the relevant documents are top ranked.",
                    "label": 1
                },
                {
                    "sent": "So this is the two settings that exist actually in the pairwise setting.",
                    "label": 0
                },
                {
                    "sent": "And after this aggregation, in fact, we can obtain convex losses for ranking by.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Simply changing the 01 classification loss with convex classification loss.",
                    "label": 0
                },
                {
                    "sent": "So if for instance we take the hinge loss.",
                    "label": 0
                },
                {
                    "sent": "Then we obtain ranking SVM.",
                    "label": 0
                },
                {
                    "sent": "When we use the mean operator to aggregates the losses or the multiclass SVM where we use when we use the Max operator to aggregate the losses and if we take the expansion loss we recover the Wrangells algorithm.",
                    "label": 0
                },
                {
                    "sent": "So although this approach is very simple.",
                    "label": 0
                },
                {
                    "sent": "The problem is that it's not suitable for tasks like information retrieval where.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We require high precision on the top of the list.",
                    "label": 0
                },
                {
                    "sent": "Typically if we take if we try to optimize the mean rank of the relevant elements, then it's not very informative for what happens in the top of the list and on the other hand the Max operator which just checks for the first position it's too aggressive and so it's not suitable because it's just forgets the.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First positions.",
                    "label": 0
                },
                {
                    "sent": "So it has been shown that the quality measures used in information retrieval are not correlated to the errors defined in the pairwise approach.",
                    "label": 0
                },
                {
                    "sent": "So basically, in the context of information retrieval, there has been a lot of previous work which do not focus more on pairwise comparison, but more use listwise loss functions or smooth approximation or convex upper bounds on our metrics.",
                    "label": 1
                },
                {
                    "sent": "So there is a lot of algorithms, so they work quite well, but we will show an alternative that can use directly the pairwise approach to optimize to optimize.",
                    "label": 0
                },
                {
                    "sent": "Ranking errors that focus on the top of the list.",
                    "label": 1
                },
                {
                    "sent": "So basically the contribution of the of the paper is an extension of the pairwise approach to a very general setting of franking which can be applied to learning to rank for information retrieval.",
                    "label": 1
                },
                {
                    "sent": "So the basis is to define family of ranking error functions which.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Not exactly the ones that are used as evaluation metrics in IR, but for which we can define very easily.",
                    "label": 0
                },
                {
                    "sent": "Context upper bounds and in a very general setting.",
                    "label": 0
                },
                {
                    "sent": "So we will show how using aggregators more general than the Max, the Max or the mean.",
                    "label": 0
                },
                {
                    "sent": "We can actually start from the pairwise pairwise losses and then build an error that focus on the whole rank list.",
                    "label": 0
                },
                {
                    "sent": "And we'll see how we can have margin base error bounds in our framework.",
                    "label": 0
                },
                {
                    "sent": "So the basis of our framework is this definition of the ranking error function, so basically.",
                    "label": 1
                },
                {
                    "sent": "Here it's the number of relevant elements, and we define the rank of a given relevant element as.",
                    "label": 0
                },
                {
                    "sent": "The number of irrelevant elements that have a better score.",
                    "label": 0
                },
                {
                    "sent": "So basically it's the position in the sorted list, but we ignore the relative ranks of the relevant elements.",
                    "label": 0
                },
                {
                    "sent": "OK, so we simply count the number of irrelevant elements that have better scores and then we use function Phi to transform the actual rank of the relevant elements.",
                    "label": 0
                },
                {
                    "sent": "Two loss value to an error value.",
                    "label": 0
                },
                {
                    "sent": "So here the AA Jays are the increments of the error function here, so it's done so that when a relevant element is exactly at the end of the list.",
                    "label": 0
                },
                {
                    "sent": "The error is 1 and one relevant element is before every irrelevant ones.",
                    "label": 0
                },
                {
                    "sent": "The error is 0 and then the increments are decreasing.",
                    "label": 1
                },
                {
                    "sent": "What does it mean?",
                    "label": 0
                },
                {
                    "sent": "It means that the error function grows faster for small ranks and then it grows slowly for large wrong values.",
                    "label": 0
                },
                {
                    "sent": "So basically the idea is that if we lose the rank at the beginning of the list, it costs more and then if we lose a rank at the end of the list.",
                    "label": 0
                },
                {
                    "sent": "So optimizing this error metric will actually allow us to focus on the very beginning of the list and everything depends on the weights Alpha.",
                    "label": 1
                },
                {
                    "sent": "It's a very general formula and basically they depending on the value of the weights, we can express various preferences between the beginning of the list and the whole list.",
                    "label": 0
                },
                {
                    "sent": "So for instance.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If we take the first weights at one and order the other weights at zero, we actually have an error which is 1.",
                    "label": 0
                },
                {
                    "sent": "As soon as relevant element is not exactly top front, so it's the very aggressive error that is used in multiclass classification for instance.",
                    "label": 0
                },
                {
                    "sent": "If we take the weights all equal.",
                    "label": 0
                },
                {
                    "sent": "We then recover them in range.",
                    "label": 0
                },
                {
                    "sent": "And by choosing weights that are in between the two extremes, we are able to provide error functions that will focus more more or less on the top of the list depending on what we want.",
                    "label": 0
                },
                {
                    "sent": "So for instance we can optimize them in rank on the top percent of the list by setting the weights at 0 after certain after a certain rank.",
                    "label": 1
                },
                {
                    "sent": "So we now give how to use the pairwise.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Approach and how to aggregate the pairwise loss to recover.",
                    "label": 0
                },
                {
                    "sent": "The error function that we just defined so that we have.",
                    "label": 0
                },
                {
                    "sent": "We will have a convex upper bound on an arrow that can focus on the top of the list.",
                    "label": 0
                },
                {
                    "sent": "The basis of our framework is to use the ordered weighted averaging operators, which are defined by yoga in 1988.",
                    "label": 0
                },
                {
                    "sent": "So the overall idea is to have a set of weights which are positive and sum to one.",
                    "label": 0
                },
                {
                    "sent": "And then it takes as input.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Set of real values.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We saw them by.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Decreasing order.",
                    "label": 0
                },
                {
                    "sent": "And then there is a weighted sum of the sort of the sorted values.",
                    "label": 0
                },
                {
                    "sent": "So basically the JS weight is applied to the JS to the JS greatest value.",
                    "label": 0
                },
                {
                    "sent": "So it's a.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A class of operators that generalize the Max, the mean and actually also the minimum, but we won't talk about it here.",
                    "label": 0
                },
                {
                    "sent": "So if you take the 1st way to be one and all of the weights at 0.",
                    "label": 0
                },
                {
                    "sent": "Then the OWA is just exactly the Max and if.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You take the mean.",
                    "label": 0
                },
                {
                    "sent": "All weights equal, you recover the mean.",
                    "label": 0
                },
                {
                    "sent": "Basically, how I will now show how we can use it to recover the error function.",
                    "label": 0
                },
                {
                    "sent": "We have the two definitions.",
                    "label": 0
                },
                {
                    "sent": "We have the definition of.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The OWA operator and we have the definition of our function that transform a rank into an error value.",
                    "label": 1
                },
                {
                    "sent": "So if we pick if we fix an example an pick a relevant element in for this example.",
                    "label": 1
                },
                {
                    "sent": "We define the weights of the.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The operator to be exactly the weight Alpha J's here, so we take the same weights in the OWA operator and.",
                    "label": 0
                },
                {
                    "sent": "In the.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Function.",
                    "label": 0
                },
                {
                    "sent": "Then if we consider the set.",
                    "label": 0
                },
                {
                    "sent": "Of pairwise losses here.",
                    "label": 0
                },
                {
                    "sent": "And apply to it the OWA.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Operator when we thought the values or the ones become become first and there are as many ones as the rank of the relevant elements.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So when we make a weighted sum.",
                    "label": 0
                },
                {
                    "sent": "We actually recover exactly the sum of the first KK.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Wait, so basically the ordered weighted averaging of the pairwise loss is exactly.",
                    "label": 0
                },
                {
                    "sent": "The the error function.",
                    "label": 0
                },
                {
                    "sent": "No, we can use the same trick as in the pairwise classification to obtain convex upper bounds on the loss function.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can write our error with OWA.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Raiders and we use additional properties, so if we have an ordered weighted averaging operator with non increasing weights then it is convex and we are in the case of non increasing weights.",
                    "label": 0
                },
                {
                    "sent": "By the definition of our error function because we said that the weights should be decreasing to focus on the top of the list.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so if Moreover we use a convex upper bound on the standards classification error, then the ordered weighted averaging of the loss of the convex loss is also a convex upper bound on the weighted average of the 01 error.",
                    "label": 1
                },
                {
                    "sent": "So we have a very general way of designing convex loss functions for ranking.",
                    "label": 0
                },
                {
                    "sent": "We just take any convex loss and.",
                    "label": 0
                },
                {
                    "sent": "Aggregate them with an ordered weighted averaging copper.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for instance, if we consider the problem of learning linear scoring functions and we use the hinge loss so the brackets and plus it's the positive part.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We use here we just obtain.",
                    "label": 0
                },
                {
                    "sent": "A natural convex upper bound on.",
                    "label": 1
                },
                {
                    "sent": "An error function that we defined before.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If now we go on with the regularised empirical risks.",
                    "label": 0
                },
                {
                    "sent": "We just put a regularization factor and we obtain some very some well known special cases.",
                    "label": 0
                },
                {
                    "sent": "If there is always one relevant element and we take the OWA operator as being the Max.",
                    "label": 0
                },
                {
                    "sent": "And then we just recover the SVM for multiclass classification of common single.",
                    "label": 1
                },
                {
                    "sent": "And if we take the OWA as being the mean then we recover the ranking SVM.",
                    "label": 0
                },
                {
                    "sent": "But we also have many other new possibilities.",
                    "label": 0
                },
                {
                    "sent": "For instance optimize them in rank of the relevant elements on the top paper, P percent of the list and just forget about the relevant elements that are at the bottom of the list.",
                    "label": 1
                },
                {
                    "sent": "So it's basically a convex convex function, so many any optimization method can be used.",
                    "label": 0
                },
                {
                    "sent": "In our experiments, we.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We showed that it can actually be written as a structural SVM.",
                    "label": 1
                },
                {
                    "sent": "And we can use existing algorithms such as the cutting plane or we used LINQ which is.",
                    "label": 0
                },
                {
                    "sent": "An efficient solver which also allows to learn online.",
                    "label": 0
                },
                {
                    "sent": "Although we didn't try to run online here.",
                    "label": 0
                },
                {
                    "sent": "And in terms of complexity, it's about the same complexity as other structured output methods for ranking.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In terms of margin based generalization.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have our error which is defined with ordered weighted averaging.",
                    "label": 0
                },
                {
                    "sent": "We assume for simplicity that there is a single relevant element and that the number of irrelevant element is constant.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In fact, the only important thing is that both of us are constants, but we may have several relevant.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "So we assume that we have a data set with ID examples.",
                    "label": 0
                },
                {
                    "sent": "And so there is.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "States that if we have a little margin penalty.",
                    "label": 0
                },
                {
                    "sent": "The generalization error of our score function is bounded by.",
                    "label": 0
                },
                {
                    "sent": "Sorry it doesn't here.",
                    "label": 0
                },
                {
                    "sent": "There is the margin term.",
                    "label": 0
                },
                {
                    "sent": "So it's the empirical error, but penalized with the margin term here gamma.",
                    "label": 0
                },
                {
                    "sent": "And there is a complexity term here which depends on the margin and basically the complexity term decreases with the margin.",
                    "label": 0
                },
                {
                    "sent": "So what it means is that it's for generalization.",
                    "label": 0
                },
                {
                    "sent": "It should be a good thing to optimize the pairwise margin.",
                    "label": 0
                },
                {
                    "sent": "OK, on the.",
                    "label": 0
                },
                {
                    "sent": "On the training set, in order to have a smaller complexity term.",
                    "label": 0
                },
                {
                    "sent": "So this is also.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Appearance with the loss function with we used with the bad guys hinge loss so we carried out experiments on little 3.0 so we use the six digit.gov datasets.",
                    "label": 0
                },
                {
                    "sent": "So there is a train validation and test sets for each datasets, about 50 to 150 queries, and 1000 document queries.",
                    "label": 1
                },
                {
                    "sent": "And succeed features for the joint query document representation.",
                    "label": 0
                },
                {
                    "sent": "So in terms of evaluation measures, we.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Use the mean average precision, the indecision, the precision, but we will only show in the presentation at the map and the precision.",
                    "label": 0
                },
                {
                    "sent": "But the other ones are in the paper.",
                    "label": 0
                },
                {
                    "sent": "For hyperparameter selection, we use the best MLP on the validation set.",
                    "label": 0
                },
                {
                    "sent": "And for the experiments I report here, we use the weights of the OWA operator, which are fixed to linearly decreasing.",
                    "label": 0
                },
                {
                    "sent": "So basically since error metric, our error measures are not exactly those that are used in information retrieval, we have to set the weights so that they will lead to good performance when evaluating when evaluating with the true error metrics.",
                    "label": 0
                },
                {
                    "sent": "So this one yields a good compromise between the beginning and the end of the list.",
                    "label": 0
                },
                {
                    "sent": "But we also made experiments where the weights are chosen on the validation sets and.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Drawings of paper.",
                    "label": 0
                },
                {
                    "sent": "So basically, in terms of mean average precision.",
                    "label": 0
                },
                {
                    "sent": "So the average precision for a given query is the mean of the precision at the rank of the relevant elements.",
                    "label": 0
                },
                {
                    "sent": "So here we have the six datasets and we chose the baseline.",
                    "label": 0
                },
                {
                    "sent": "Results are given on the Microsoft website.",
                    "label": 0
                },
                {
                    "sent": "So basically we can see that in terms of MLP.",
                    "label": 0
                },
                {
                    "sent": "The pairwise they ordered weighted pairwise classification approach with the one over unk weights, outperforms the algorithms on four out of six datasets.",
                    "label": 0
                },
                {
                    "sent": "And it's worse on two datasets.",
                    "label": 0
                },
                {
                    "sent": "But still since new algorithm tend to be much better than the other on all datasets, it's.",
                    "label": 0
                },
                {
                    "sent": "It's fairly good and we also show the test performance in terms of precision at one.",
                    "label": 1
                },
                {
                    "sent": "So here, so precision at one is the percentage of queries having the first document.",
                    "label": 1
                },
                {
                    "sent": "Relevant, the first retrieve documents are relevant.",
                    "label": 0
                },
                {
                    "sent": "So here on the five datasets we are at the level of the best little baseline.",
                    "label": 0
                },
                {
                    "sent": "So basically with these weights we are able to optimize at the same time the precision and and the AP.",
                    "label": 0
                },
                {
                    "sent": "And we have other results for N DCG.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the ordered weighted pairwise classification approach allows to define convex losses for ranking.",
                    "label": 1
                },
                {
                    "sent": "So using.",
                    "label": 0
                },
                {
                    "sent": "Proxy error functions, not directly those that are using IR.",
                    "label": 0
                },
                {
                    "sent": "But generalizes the classical pairwise classification approach and allows to learn function with a high precision on the top of the list.",
                    "label": 1
                },
                {
                    "sent": "We have generalization errors for the set of ranking errors we defined and state of the art results on little 3.0.",
                    "label": 0
                },
                {
                    "sent": "So one of the restrictions of our framework is that it only deals with binary relevance judgments for now.",
                    "label": 0
                },
                {
                    "sent": "So we need to extend it for multivalued relevance treatments.",
                    "label": 0
                },
                {
                    "sent": "And we also would like to explore the possibility of learning the weights rather than choosing them as hyperparameters or.",
                    "label": 0
                },
                {
                    "sent": "Things like that.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Few minutes for questions.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Did you?",
                    "label": 0
                },
                {
                    "sent": "Did you make experiments with the setting different weights?",
                    "label": 0
                },
                {
                    "sent": "Yes, figure out whether this really has an influence.",
                    "label": 0
                },
                {
                    "sent": "Yes indeed there are some in in the paper we actually tried to optimize, for instance the min rank on the top percent of the list by by varying the P. So basically what we obtain is that for relatively large values of PSA 2020%, ten percent 5%, this yields in general and improvement of maybe.",
                    "label": 0
                },
                {
                    "sent": "But when we go to much like we try to optimize on the very first few documents, then it's not very good results and basically.",
                    "label": 0
                },
                {
                    "sent": "For instance the.",
                    "label": 0
                },
                {
                    "sent": "The varying with the one over rank the linearly decreasing weights are systematically much better than the normalized weights.",
                    "label": 0
                },
                {
                    "sent": "And we also tried other schemes, but.",
                    "label": 0
                },
                {
                    "sent": "So yes, there is an influence and it's consistent over all the old data sets.",
                    "label": 0
                },
                {
                    "sent": "Can you say anything about efficiency or complexity of learning?",
                    "label": 0
                },
                {
                    "sent": "Is it?",
                    "label": 0
                },
                {
                    "sent": "Yes?",
                    "label": 0
                },
                {
                    "sent": "It's rather fast in in our setting, so basically.",
                    "label": 0
                },
                {
                    "sent": "For the optimization, so we use LINQ which is known as being an efficient solver for multiclass classification, but it extends naturally to the structured output setting, and basically it's makes iterations and each iteration costs basically the time to sort the items for query and.",
                    "label": 0
                },
                {
                    "sent": "Since the data set are rather small, it all depends on the stopping criteria.",
                    "label": 0
                },
                {
                    "sent": "But with a good stopping criteria it takes a few seconds for.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "But it seemed like what you really wanted to minimize.",
                    "label": 0
                },
                {
                    "sent": "The order dated 01 average, right?",
                    "label": 0
                },
                {
                    "sent": "Did you report experimental results?",
                    "label": 0
                },
                {
                    "sent": "No, we we didn't of for this particular error, we didn't show results, but this was the question.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah no, but it would also be difficult, difficult to compare with other algorithms since they.",
                    "label": 0
                },
                {
                    "sent": "They were all evaluated in terms of any P or N DCG, but it would be interesting.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}