{
    "id": "4dxsijza33qa3llnawxkevipvx3xqpce",
    "title": "ABC-Boost: Adaptive Base Class Boost for Multi-Class Classi\ufb01cation",
    "info": {
        "author": [
            "Ping Li, Department of Statistical Science, Cornell University"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Supervised Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_li_abcb/",
    "segmentation": [
        [
            "Thank you.",
            "So.",
            "Well, I just give this talk to a non machine learning audience and that's why I have.",
            "This slide basically tells me."
        ],
        [
            "You my zip code so it's a 10 class classification problem so.",
            "And they were."
        ],
        [
            "Interesting multiclass classifications.",
            "So we have X&Y's, so why take value between zero and K capital K -- 1?",
            "So if so, if the two is binary and we are interested in multiclass for example Q25 in search engines in many of the search engine papers."
        ],
        [
            "So one strategy for model class classifications who we first learn the class probabilities and notice that there are some to one.",
            "So there's only really came minus 1 degrees of freedom, and then I assign the class label according to the maximum probabilities.",
            "So this is one of the main strategies."
        ],
        [
            "And so we have a strategy.",
            "Now the next we need a model.",
            "So one popular models multinomial logit probability model and so we have this function F for each class.",
            "So in logistic regression this function F just a linear function of the features.",
            "And for multiclass the sum to zero constraints very."
        ],
        [
            "Commonly used and tilt any unique solutions because there's really only came minus 1 degrees of freedom.",
            "And to appreciate this constraint we can consider if you add a constant which functions.",
            "It's a probability it doesn't change so so therefore a natural thing to do to in order to uniquely identify the function values, we just assume that the sum of the functions equal to a constant, which is equivalent to sum to 0.",
            "So this is just a very popular assumptions, and now we have."
        ],
        [
            "Model we have probability model, so the next task is hard to learn while we use multinomial maximum likelihood.",
            "Suppose supposed probably support classes K, so likelihood just the same as a probability and we often like to work with the log likelihood or minimizing the negative log likelihood loss.",
            "Think about loss function, but it's basically the maximum likelihood so."
        ],
        [
            "We have one sample, so therefore we just write down the last function in adopt summation form.",
            "We use indicator function R so that to make for convenience, so are.",
            "Little R has only one of them is 1, the other S 0 to this double summation form.",
            "Just make it more convenient.",
            "To to to work with so."
        ],
        [
            "Now to better appreciate the sum to zero constraints.",
            "This is not in the paper.",
            "Suppose we do not use subzero constraints and you can show that you take the first derivative without.",
            "Suppose this function has no relation there, they are independent each other.",
            "Then you take derivatives.",
            "You can get first rotor and 2nd director and you look at the second derivative, the Hessians actually.",
            "The Hessian is actually a singular, which is obvious.",
            "To bother, I just want called particular example is singular and it should be a singular for general cases.",
            "So we do need the constraints."
        ],
        [
            "And some to zero constraint is a natural thing to do and another work around by Friedman and Trevor Hastie.",
            "And they they use diagonal approximations of the Hessians.",
            "So once you do the approximation, it's a it's a.",
            "It's nonsingular, so that's one way to work around.",
            "And we also use a diagonal approximations.",
            "But we use different derivatives.",
            "So now just continue the introduction."
        ],
        [
            "So the function gradient boosting.",
            "So instead of doing logistic regression, which is which is a function assuming linear function or the features function, gradient boosting and freedoms framework or travel history, their framework is to assume that the function is a submission of capital M terms.",
            "So with the two coefficient A and row, so H is usually assumed to be a regression tree, is weak and owner in this content and such that they do at every stage.",
            "So they optimize at every stage, so it's a greedy algorithm and so this is a function that they try to optimize by greedy fashion and."
        ],
        [
            "And freedom and approximate conductor's deepest desire in the function space by solving the least square problem.",
            "And so in order to those tips the same, we need the first relative, so they just.",
            "Approximate conducts deepest dissent in function space.",
            "By minimizing the mean square errors and then the online search for another coefficient, we need 2 coefficients, so there is a is determined by by me square.",
            "Solutions and another coefficients determined by line search and later is approximated by one step, Newton update."
        ],
        [
            "So this is a generic gradient boosting algorithm is for every every every iteration.",
            "So we compute the gradients and then we can do the regression software regression problem, then then the line search.",
            "Then add the model at the small model to the to the big model.",
            "So marked gradient boosting is a greater boosting plus regression trees."
        ],
        [
            "And so this is just some more.",
            "Another implementation of this algorithms.",
            "So basically they use geothermal energy, terminal regression tree.",
            "So there is another coefficients and so the user.",
            "The residuals response, then they build regression trees.",
            "Then instead of doing the full line search that they do one step neutral updates.",
            "So that's that's the algorithm is seems to be dreaming successful an among many others, and."
        ],
        [
            "And so since so several components of this algorithm that we start with this loss function we're familiar with, then compute the first relative to learn the structure of the trees and 2nd derivative to determine the value of the terminal nodes.",
            "Also, for the one step, neutral updates and they have those heuristic factor for considering only came minus 1 degrees freedom and the shrinkage factor to avoid overfitting.",
            "So it's interesting algorithm and."
        ],
        [
            "However, like about a year ago when I was working on something else related and I really what disturbing means that there is a constraint which is used in their papers except when the when the derivative derivatives and they actually get around this constraints so so so that motivated me to to work on this ABC Boost.",
            "So first the first task is to.",
            "Derive what is the true derivatives under these constraints.",
            "So with this constraint we only need came we only the K -- 1 values, so we can assume that the zero classes base class.",
            "Without loss of generality.",
            "And then we can."
        ],
        [
            "Derive the directors, the first derivative, 2nd order, assuming that Class 0 is the is a base class.",
            "So the derivations on the paper and the quest straightforward derivations, and so it's interesting.",
            "So now if you change the base class, you just change this this values here so.",
            "So now the so those are the two main.",
            "This is more like a true derivatives of this loss function and the previous result those directors are there.",
            "Maybe approximate derivatives or or in any sense.",
            "So this I consider more true derivatives.",
            "So now the next question is which base class to use.",
            "So at that time we had this crazy idea.",
            "So since we do boosting we do many stages anyway, so let's change the base class at each iteration.",
            "Can you explain the intuition behind the?",
            "Well, if you if you take derivatives with better function values.",
            "So then.",
            "And because you can, you can write the constraint or what is actually what's your question.",
            "Curious about the end result.",
            "Yeah, result.",
            "So this is.",
            "And go back to the Dominican Republic.",
            "Yeah, without the constraints, just the part without.",
            "There is another term, right?",
            "Yeah, there's another terms, yeah, so this so the the power without constraint just so without any of this zero terms, yeah then yeah.",
            "So the base class gets in here, so the derivation is actually straightforward, so it's a.",
            "The."
        ],
        [
            "OK, so now so now the the key idea is that, well, we do not have to train the base class, so let's.",
            "So let's just work with mass one classes and then get another class by.",
            "There's something there.",
            "Constraint then H boosting iteration adapted, choose base class to maximize the performance training performance.",
            "So now we still need the strategy.",
            "A concrete strategy for choose a base class.",
            "So now what do we do?",
            "Well, we we we should choose basic cost cutting the performance."
        ],
        [
            "Training loss at experimenting with many ideas.",
            "This is one of the idea.",
            "I think we have.",
            "This criticism is that we do exhaustive search.",
            "So at each step at least, that we try each class as base class and then look at the performance and choose the one that and minimize the training loss.",
            "So the only criticism is a computationally expensive but not too bad and then SDK is really large.",
            "Yeah, so so the the many other ideas.",
            "And which I compose my web page.",
            "Later so so.",
            "This is the.",
            "This is the exhaustive idea, so any questions.",
            "So it should be straightforward, yeah, so once once the idea is there is actually very, I believe it's very easy to understand the general idea.",
            "OK, so now we need a concrete algorithm to work with."
        ],
        [
            "Choose mark so with the ABC model plus marker ABC, ABC Boost plus Mark become ABC Mart and."
        ],
        [
            "So the only change we need to do is we need to replace the first derivatives with the.",
            "We need to replace the derivatives with the with the derivatives I derive, so that's easy.",
            "We don't need this K -- 1 / K which will be recovered naturally and we also need to another another loop so at every every iteration we do add another loop for choosing the base class.",
            "Of course we need to do something here and to store the base class so this is a very very small changes or the algorithms.",
            "And."
        ],
        [
            "OK, so now not another.",
            "The fun part is because it's small change so the fun prizes performance and I tried many data sets.",
            "There's two data set, poker and amnist.",
            "These two data sets I I did not include the paper because I haven't finished experiment that time.",
            "But now now almost done.",
            "So I just because I think they're very fun.",
            "Data sets are large and the interesting results.",
            "So the another data sets too.",
            "So the letter.",
            "Memories 10K means I just swap the training testing to save them at training time.",
            "That's what we're testing.",
            "Also, the error can be larger.",
            "Otherwise it's like everybody working with 1.4 one point 3 is kind of set.",
            "Yeah, it's interesting, but it may not be most interesting, so just swap the training, testing and and also faster so I can do more experiments so the poker is poker is extremely unbalanced data sets in June, unbalanced data sets and get a very very very nice very nice results achieved.",
            "So the other data sets are not as a regular letter data sets and a letter for cases as well for training testing.",
            "Yeah so so this."
        ],
        [
            "The standard data set, so this is the result.",
            "There's also.",
            "This is Amar result.",
            "And this ABC marked the relative improvements relative."
        ],
        [
            "This means the relative improvement, so it's like a 10 or 20% improvements.",
            "And if you compute P value when you have large."
        ],
        [
            "Data sets often get 0P values, so there's another advantage of working with larger data set.",
            "You get 0P values so so, so this is all those data sets I haven't posted anywhere, they just tell you guys here, so it's actually very interesting because I got like 7% errors.",
            "And the original owner of this data sets they actually achieve like 50% accuracies or many various doubts that they work on that for years, and so I suggested will talk with the Lib SVM guys and the candle agree to put this data sets on the on the web page and they work on it.",
            "Last month I checked within the accuracy is about 60% so 40% errors.",
            "So this is like 3% seven percent errors.",
            "So this is a good opportunity for improving SVM's.",
            "And maybe maybe the different kinds of ways to for multiclass.",
            "So yeah, but maybe I will check with them again just in this month they get better results.",
            "So but last time I checked together 40% errors.",
            "So this is 7% error.",
            "So, so it's, uh, yeah.",
            "OK.",
            "But, and I cannot hold the temptation telling you that I have another algorithm that also improved this by another 10%, so which I'm going to post soon so.",
            "And so it says to make it more interesting.",
            "So so now P."
        ],
        [
            "People may be wondering, so is this improvement due to my particular choice of parameters?",
            "So people like this is a common trick, but so well, let's to answer the question, so let's try.",
            "Let's try various things.",
            "Let's try a series of parameters from the shrinkage from .04 two point 1.",
            "I cannot do more than this, but this is the best I can do.",
            "Then I tried different terminal nodes from 420.",
            "So yeah, so and I try and 10,000 iterations at most and or the user determine it before that because of the machine accuracy is rich unless unless the.",
            "So now the next question is the improvement due to incompetent imitational mark.",
            "We also report the results by using Freedom Morgan.",
            "My implementation also freedom market algorithms.",
            "So is the computational efficiency issue while training slower but estimates faster because we do not have to test of another cost.",
            "So this is a result report in the paper.",
            "What it looks like is I tried a bunch of shrinkage factor, anti bunch of eternal terminal nodes so side-by-side comparisons.",
            "So this is Mark."
        ],
        [
            "And so this is my implementation, just so the this is a freedom as input result, download his program and run it and make sure everything is the same so we can see the very similar also diverse stable algorithm.",
            "First table results across so many parameters.",
            "This is a huge advantage.",
            "So this is an improvement is ABC, my improvements or 20% improvement in Apprentices 25 to 20% improvements for an attorney to 30 approved."
        ],
        [
            "For every compilations or choices or parameters, so this is, I think it's a stronger, stronger statement than improving.",
            "Just improve, so improve for every combinations of parameters.",
            "I couldn't try more, I didn't try our numbers.",
            "I don't know why, but I should.",
            "And so this is what I do."
        ],
        [
            "So now now people may be wondering why, what's better while the train is faster because we're working with that router, so the train is faster, considerably faster by go 10 to the minus 16 until the machine actually reached."
        ],
        [
            "Training is faster and so testing.",
            "Ann is that it is also better.",
            "Notice that I don't, I don't.",
            "I don't report this value.",
            "I report this values.",
            "Yeah, so I put the lowest.",
            "So so yeah."
        ],
        [
            "So this is 1 particular data set, so many data sets is always in this form, so it's enormous amount of work specially had to do everything myself.",
            "And and but I did many data sets.",
            "So for this one I haven't really completed everything here, so I just report one."
        ],
        [
            "Parameters I mean one shrinkage factors and and so the countertop data sets even though they own."
        ],
        [
            "Improvements in at 8.2%.",
            "But if we terminate earlier earlier than earlier than 5000 iterations, I get.",
            "I still get 20% improvements.",
            "And so.",
            "So this is."
        ],
        [
            "The cover type data sets yeah.",
            "So."
        ],
        [
            "So the conclusion is that ABC Boost formula formulated by user assuming based class and adaptive choose based classes each iterations and I see some considerable improvements.",
            "And if you're interested in the in the in the partition of the data like cover type, I think that's only data set with no standard.",
            "Yeah, it's a fairly large, but I did my own partition, but if you're interested I can give you the indexes so.",
            "How are you measuring the?",
            "Classification accuracy error?",
            "Yeah, so if it matches then matches to the standard.",
            "Well, yeah.",
            "Yeah, I don't do this confusion matrix and I didn't do that here.",
            "Number.",
            "Uh, for this data set, I don't, I don't.",
            "I don't see, yeah.",
            "Yeah, I see I don't have appeared to see any here and it normally this when I come here.",
            "The terminal here because of the machine accuracy reached.",
            "Yeah so I cannot go any further.",
            "Do you have any gift for your mom?",
            "I think for this kind of data sets, is the data sets.",
            "Is it fairly fairly?",
            "I think it's probably due to the data sets, yeah?",
            "No neighbor noises?",
            "Yeah yeah, that's a good question.",
            "I actually experimented by swapping the labels and with some other experiments, I do see overfitting.",
            "If you swap the labels yet.",
            "OK, can you comment about?",
            "What exactly the guys at the provided the proper data set?",
            "Did the poker?",
            "Yeah, so you mentioned that you have 40% error?",
            "That's what they.",
            "That's what the live stream guys told me.",
            "They have 40% error doing SVM.",
            "Yeah, that's what they're doing because I'm not especially best VM so I don't want to report any results that I'm not expecting expert.",
            "So they are the experts and they told me last month the error is about 40%.",
            "But they're still working on them, so maybe maybe not fix the problem and it's a Gemini unbalanced data set.",
            "And they say very small training, very large testing.",
            "I don't know.",
            "It's kind of not very.",
            "I don't know.",
            "Yeah it's a very strange data set.",
            "Question.",
            "OK, let's thank speaker.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Well, I just give this talk to a non machine learning audience and that's why I have.",
                    "label": 0
                },
                {
                    "sent": "This slide basically tells me.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You my zip code so it's a 10 class classification problem so.",
                    "label": 0
                },
                {
                    "sent": "And they were.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Interesting multiclass classifications.",
                    "label": 0
                },
                {
                    "sent": "So we have X&Y's, so why take value between zero and K capital K -- 1?",
                    "label": 0
                },
                {
                    "sent": "So if so, if the two is binary and we are interested in multiclass for example Q25 in search engines in many of the search engine papers.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one strategy for model class classifications who we first learn the class probabilities and notice that there are some to one.",
                    "label": 1
                },
                {
                    "sent": "So there's only really came minus 1 degrees of freedom, and then I assign the class label according to the maximum probabilities.",
                    "label": 1
                },
                {
                    "sent": "So this is one of the main strategies.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so we have a strategy.",
                    "label": 0
                },
                {
                    "sent": "Now the next we need a model.",
                    "label": 0
                },
                {
                    "sent": "So one popular models multinomial logit probability model and so we have this function F for each class.",
                    "label": 0
                },
                {
                    "sent": "So in logistic regression this function F just a linear function of the features.",
                    "label": 0
                },
                {
                    "sent": "And for multiclass the sum to zero constraints very.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Commonly used and tilt any unique solutions because there's really only came minus 1 degrees of freedom.",
                    "label": 0
                },
                {
                    "sent": "And to appreciate this constraint we can consider if you add a constant which functions.",
                    "label": 0
                },
                {
                    "sent": "It's a probability it doesn't change so so therefore a natural thing to do to in order to uniquely identify the function values, we just assume that the sum of the functions equal to a constant, which is equivalent to sum to 0.",
                    "label": 1
                },
                {
                    "sent": "So this is just a very popular assumptions, and now we have.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Model we have probability model, so the next task is hard to learn while we use multinomial maximum likelihood.",
                    "label": 1
                },
                {
                    "sent": "Suppose supposed probably support classes K, so likelihood just the same as a probability and we often like to work with the log likelihood or minimizing the negative log likelihood loss.",
                    "label": 1
                },
                {
                    "sent": "Think about loss function, but it's basically the maximum likelihood so.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have one sample, so therefore we just write down the last function in adopt summation form.",
                    "label": 0
                },
                {
                    "sent": "We use indicator function R so that to make for convenience, so are.",
                    "label": 0
                },
                {
                    "sent": "Little R has only one of them is 1, the other S 0 to this double summation form.",
                    "label": 0
                },
                {
                    "sent": "Just make it more convenient.",
                    "label": 0
                },
                {
                    "sent": "To to to work with so.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now to better appreciate the sum to zero constraints.",
                    "label": 0
                },
                {
                    "sent": "This is not in the paper.",
                    "label": 0
                },
                {
                    "sent": "Suppose we do not use subzero constraints and you can show that you take the first derivative without.",
                    "label": 0
                },
                {
                    "sent": "Suppose this function has no relation there, they are independent each other.",
                    "label": 0
                },
                {
                    "sent": "Then you take derivatives.",
                    "label": 0
                },
                {
                    "sent": "You can get first rotor and 2nd director and you look at the second derivative, the Hessians actually.",
                    "label": 0
                },
                {
                    "sent": "The Hessian is actually a singular, which is obvious.",
                    "label": 0
                },
                {
                    "sent": "To bother, I just want called particular example is singular and it should be a singular for general cases.",
                    "label": 0
                },
                {
                    "sent": "So we do need the constraints.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And some to zero constraint is a natural thing to do and another work around by Friedman and Trevor Hastie.",
                    "label": 0
                },
                {
                    "sent": "And they they use diagonal approximations of the Hessians.",
                    "label": 0
                },
                {
                    "sent": "So once you do the approximation, it's a it's a.",
                    "label": 0
                },
                {
                    "sent": "It's nonsingular, so that's one way to work around.",
                    "label": 0
                },
                {
                    "sent": "And we also use a diagonal approximations.",
                    "label": 0
                },
                {
                    "sent": "But we use different derivatives.",
                    "label": 0
                },
                {
                    "sent": "So now just continue the introduction.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the function gradient boosting.",
                    "label": 0
                },
                {
                    "sent": "So instead of doing logistic regression, which is which is a function assuming linear function or the features function, gradient boosting and freedoms framework or travel history, their framework is to assume that the function is a submission of capital M terms.",
                    "label": 1
                },
                {
                    "sent": "So with the two coefficient A and row, so H is usually assumed to be a regression tree, is weak and owner in this content and such that they do at every stage.",
                    "label": 1
                },
                {
                    "sent": "So they optimize at every stage, so it's a greedy algorithm and so this is a function that they try to optimize by greedy fashion and.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And freedom and approximate conductor's deepest desire in the function space by solving the least square problem.",
                    "label": 1
                },
                {
                    "sent": "And so in order to those tips the same, we need the first relative, so they just.",
                    "label": 0
                },
                {
                    "sent": "Approximate conducts deepest dissent in function space.",
                    "label": 0
                },
                {
                    "sent": "By minimizing the mean square errors and then the online search for another coefficient, we need 2 coefficients, so there is a is determined by by me square.",
                    "label": 0
                },
                {
                    "sent": "Solutions and another coefficients determined by line search and later is approximated by one step, Newton update.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is a generic gradient boosting algorithm is for every every every iteration.",
                    "label": 1
                },
                {
                    "sent": "So we compute the gradients and then we can do the regression software regression problem, then then the line search.",
                    "label": 0
                },
                {
                    "sent": "Then add the model at the small model to the to the big model.",
                    "label": 1
                },
                {
                    "sent": "So marked gradient boosting is a greater boosting plus regression trees.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so this is just some more.",
                    "label": 0
                },
                {
                    "sent": "Another implementation of this algorithms.",
                    "label": 0
                },
                {
                    "sent": "So basically they use geothermal energy, terminal regression tree.",
                    "label": 0
                },
                {
                    "sent": "So there is another coefficients and so the user.",
                    "label": 0
                },
                {
                    "sent": "The residuals response, then they build regression trees.",
                    "label": 0
                },
                {
                    "sent": "Then instead of doing the full line search that they do one step neutral updates.",
                    "label": 0
                },
                {
                    "sent": "So that's that's the algorithm is seems to be dreaming successful an among many others, and.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so since so several components of this algorithm that we start with this loss function we're familiar with, then compute the first relative to learn the structure of the trees and 2nd derivative to determine the value of the terminal nodes.",
                    "label": 1
                },
                {
                    "sent": "Also, for the one step, neutral updates and they have those heuristic factor for considering only came minus 1 degrees freedom and the shrinkage factor to avoid overfitting.",
                    "label": 0
                },
                {
                    "sent": "So it's interesting algorithm and.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "However, like about a year ago when I was working on something else related and I really what disturbing means that there is a constraint which is used in their papers except when the when the derivative derivatives and they actually get around this constraints so so so that motivated me to to work on this ABC Boost.",
                    "label": 0
                },
                {
                    "sent": "So first the first task is to.",
                    "label": 0
                },
                {
                    "sent": "Derive what is the true derivatives under these constraints.",
                    "label": 1
                },
                {
                    "sent": "So with this constraint we only need came we only the K -- 1 values, so we can assume that the zero classes base class.",
                    "label": 0
                },
                {
                    "sent": "Without loss of generality.",
                    "label": 0
                },
                {
                    "sent": "And then we can.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Derive the directors, the first derivative, 2nd order, assuming that Class 0 is the is a base class.",
                    "label": 0
                },
                {
                    "sent": "So the derivations on the paper and the quest straightforward derivations, and so it's interesting.",
                    "label": 0
                },
                {
                    "sent": "So now if you change the base class, you just change this this values here so.",
                    "label": 0
                },
                {
                    "sent": "So now the so those are the two main.",
                    "label": 0
                },
                {
                    "sent": "This is more like a true derivatives of this loss function and the previous result those directors are there.",
                    "label": 0
                },
                {
                    "sent": "Maybe approximate derivatives or or in any sense.",
                    "label": 0
                },
                {
                    "sent": "So this I consider more true derivatives.",
                    "label": 0
                },
                {
                    "sent": "So now the next question is which base class to use.",
                    "label": 0
                },
                {
                    "sent": "So at that time we had this crazy idea.",
                    "label": 0
                },
                {
                    "sent": "So since we do boosting we do many stages anyway, so let's change the base class at each iteration.",
                    "label": 0
                },
                {
                    "sent": "Can you explain the intuition behind the?",
                    "label": 0
                },
                {
                    "sent": "Well, if you if you take derivatives with better function values.",
                    "label": 0
                },
                {
                    "sent": "So then.",
                    "label": 0
                },
                {
                    "sent": "And because you can, you can write the constraint or what is actually what's your question.",
                    "label": 0
                },
                {
                    "sent": "Curious about the end result.",
                    "label": 0
                },
                {
                    "sent": "Yeah, result.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "And go back to the Dominican Republic.",
                    "label": 0
                },
                {
                    "sent": "Yeah, without the constraints, just the part without.",
                    "label": 0
                },
                {
                    "sent": "There is another term, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, there's another terms, yeah, so this so the the power without constraint just so without any of this zero terms, yeah then yeah.",
                    "label": 0
                },
                {
                    "sent": "So the base class gets in here, so the derivation is actually straightforward, so it's a.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now so now the the key idea is that, well, we do not have to train the base class, so let's.",
                    "label": 0
                },
                {
                    "sent": "So let's just work with mass one classes and then get another class by.",
                    "label": 0
                },
                {
                    "sent": "There's something there.",
                    "label": 0
                },
                {
                    "sent": "Constraint then H boosting iteration adapted, choose base class to maximize the performance training performance.",
                    "label": 0
                },
                {
                    "sent": "So now we still need the strategy.",
                    "label": 0
                },
                {
                    "sent": "A concrete strategy for choose a base class.",
                    "label": 0
                },
                {
                    "sent": "So now what do we do?",
                    "label": 0
                },
                {
                    "sent": "Well, we we we should choose basic cost cutting the performance.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Training loss at experimenting with many ideas.",
                    "label": 1
                },
                {
                    "sent": "This is one of the idea.",
                    "label": 0
                },
                {
                    "sent": "I think we have.",
                    "label": 0
                },
                {
                    "sent": "This criticism is that we do exhaustive search.",
                    "label": 1
                },
                {
                    "sent": "So at each step at least, that we try each class as base class and then look at the performance and choose the one that and minimize the training loss.",
                    "label": 1
                },
                {
                    "sent": "So the only criticism is a computationally expensive but not too bad and then SDK is really large.",
                    "label": 1
                },
                {
                    "sent": "Yeah, so so the the many other ideas.",
                    "label": 0
                },
                {
                    "sent": "And which I compose my web page.",
                    "label": 0
                },
                {
                    "sent": "Later so so.",
                    "label": 0
                },
                {
                    "sent": "This is the.",
                    "label": 0
                },
                {
                    "sent": "This is the exhaustive idea, so any questions.",
                    "label": 0
                },
                {
                    "sent": "So it should be straightforward, yeah, so once once the idea is there is actually very, I believe it's very easy to understand the general idea.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we need a concrete algorithm to work with.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Choose mark so with the ABC model plus marker ABC, ABC Boost plus Mark become ABC Mart and.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the only change we need to do is we need to replace the first derivatives with the.",
                    "label": 0
                },
                {
                    "sent": "We need to replace the derivatives with the with the derivatives I derive, so that's easy.",
                    "label": 0
                },
                {
                    "sent": "We don't need this K -- 1 / K which will be recovered naturally and we also need to another another loop so at every every iteration we do add another loop for choosing the base class.",
                    "label": 0
                },
                {
                    "sent": "Of course we need to do something here and to store the base class so this is a very very small changes or the algorithms.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now not another.",
                    "label": 0
                },
                {
                    "sent": "The fun part is because it's small change so the fun prizes performance and I tried many data sets.",
                    "label": 0
                },
                {
                    "sent": "There's two data set, poker and amnist.",
                    "label": 0
                },
                {
                    "sent": "These two data sets I I did not include the paper because I haven't finished experiment that time.",
                    "label": 0
                },
                {
                    "sent": "But now now almost done.",
                    "label": 0
                },
                {
                    "sent": "So I just because I think they're very fun.",
                    "label": 0
                },
                {
                    "sent": "Data sets are large and the interesting results.",
                    "label": 0
                },
                {
                    "sent": "So the another data sets too.",
                    "label": 0
                },
                {
                    "sent": "So the letter.",
                    "label": 0
                },
                {
                    "sent": "Memories 10K means I just swap the training testing to save them at training time.",
                    "label": 0
                },
                {
                    "sent": "That's what we're testing.",
                    "label": 0
                },
                {
                    "sent": "Also, the error can be larger.",
                    "label": 0
                },
                {
                    "sent": "Otherwise it's like everybody working with 1.4 one point 3 is kind of set.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's interesting, but it may not be most interesting, so just swap the training, testing and and also faster so I can do more experiments so the poker is poker is extremely unbalanced data sets in June, unbalanced data sets and get a very very very nice very nice results achieved.",
                    "label": 0
                },
                {
                    "sent": "So the other data sets are not as a regular letter data sets and a letter for cases as well for training testing.",
                    "label": 0
                },
                {
                    "sent": "Yeah so so this.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The standard data set, so this is the result.",
                    "label": 0
                },
                {
                    "sent": "There's also.",
                    "label": 0
                },
                {
                    "sent": "This is Amar result.",
                    "label": 0
                },
                {
                    "sent": "And this ABC marked the relative improvements relative.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This means the relative improvement, so it's like a 10 or 20% improvements.",
                    "label": 0
                },
                {
                    "sent": "And if you compute P value when you have large.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Data sets often get 0P values, so there's another advantage of working with larger data set.",
                    "label": 0
                },
                {
                    "sent": "You get 0P values so so, so this is all those data sets I haven't posted anywhere, they just tell you guys here, so it's actually very interesting because I got like 7% errors.",
                    "label": 0
                },
                {
                    "sent": "And the original owner of this data sets they actually achieve like 50% accuracies or many various doubts that they work on that for years, and so I suggested will talk with the Lib SVM guys and the candle agree to put this data sets on the on the web page and they work on it.",
                    "label": 0
                },
                {
                    "sent": "Last month I checked within the accuracy is about 60% so 40% errors.",
                    "label": 0
                },
                {
                    "sent": "So this is like 3% seven percent errors.",
                    "label": 0
                },
                {
                    "sent": "So this is a good opportunity for improving SVM's.",
                    "label": 0
                },
                {
                    "sent": "And maybe maybe the different kinds of ways to for multiclass.",
                    "label": 0
                },
                {
                    "sent": "So yeah, but maybe I will check with them again just in this month they get better results.",
                    "label": 0
                },
                {
                    "sent": "So but last time I checked together 40% errors.",
                    "label": 0
                },
                {
                    "sent": "So this is 7% error.",
                    "label": 0
                },
                {
                    "sent": "So, so it's, uh, yeah.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "But, and I cannot hold the temptation telling you that I have another algorithm that also improved this by another 10%, so which I'm going to post soon so.",
                    "label": 0
                },
                {
                    "sent": "And so it says to make it more interesting.",
                    "label": 0
                },
                {
                    "sent": "So so now P.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "People may be wondering, so is this improvement due to my particular choice of parameters?",
                    "label": 1
                },
                {
                    "sent": "So people like this is a common trick, but so well, let's to answer the question, so let's try.",
                    "label": 0
                },
                {
                    "sent": "Let's try various things.",
                    "label": 1
                },
                {
                    "sent": "Let's try a series of parameters from the shrinkage from .04 two point 1.",
                    "label": 0
                },
                {
                    "sent": "I cannot do more than this, but this is the best I can do.",
                    "label": 0
                },
                {
                    "sent": "Then I tried different terminal nodes from 420.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so and I try and 10,000 iterations at most and or the user determine it before that because of the machine accuracy is rich unless unless the.",
                    "label": 1
                },
                {
                    "sent": "So now the next question is the improvement due to incompetent imitational mark.",
                    "label": 1
                },
                {
                    "sent": "We also report the results by using Freedom Morgan.",
                    "label": 0
                },
                {
                    "sent": "My implementation also freedom market algorithms.",
                    "label": 0
                },
                {
                    "sent": "So is the computational efficiency issue while training slower but estimates faster because we do not have to test of another cost.",
                    "label": 0
                },
                {
                    "sent": "So this is a result report in the paper.",
                    "label": 0
                },
                {
                    "sent": "What it looks like is I tried a bunch of shrinkage factor, anti bunch of eternal terminal nodes so side-by-side comparisons.",
                    "label": 0
                },
                {
                    "sent": "So this is Mark.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so this is my implementation, just so the this is a freedom as input result, download his program and run it and make sure everything is the same so we can see the very similar also diverse stable algorithm.",
                    "label": 0
                },
                {
                    "sent": "First table results across so many parameters.",
                    "label": 0
                },
                {
                    "sent": "This is a huge advantage.",
                    "label": 0
                },
                {
                    "sent": "So this is an improvement is ABC, my improvements or 20% improvement in Apprentices 25 to 20% improvements for an attorney to 30 approved.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For every compilations or choices or parameters, so this is, I think it's a stronger, stronger statement than improving.",
                    "label": 0
                },
                {
                    "sent": "Just improve, so improve for every combinations of parameters.",
                    "label": 0
                },
                {
                    "sent": "I couldn't try more, I didn't try our numbers.",
                    "label": 0
                },
                {
                    "sent": "I don't know why, but I should.",
                    "label": 0
                },
                {
                    "sent": "And so this is what I do.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now now people may be wondering why, what's better while the train is faster because we're working with that router, so the train is faster, considerably faster by go 10 to the minus 16 until the machine actually reached.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Training is faster and so testing.",
                    "label": 0
                },
                {
                    "sent": "Ann is that it is also better.",
                    "label": 0
                },
                {
                    "sent": "Notice that I don't, I don't.",
                    "label": 0
                },
                {
                    "sent": "I don't report this value.",
                    "label": 0
                },
                {
                    "sent": "I report this values.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I put the lowest.",
                    "label": 0
                },
                {
                    "sent": "So so yeah.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is 1 particular data set, so many data sets is always in this form, so it's enormous amount of work specially had to do everything myself.",
                    "label": 0
                },
                {
                    "sent": "And and but I did many data sets.",
                    "label": 0
                },
                {
                    "sent": "So for this one I haven't really completed everything here, so I just report one.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Parameters I mean one shrinkage factors and and so the countertop data sets even though they own.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Improvements in at 8.2%.",
                    "label": 0
                },
                {
                    "sent": "But if we terminate earlier earlier than earlier than 5000 iterations, I get.",
                    "label": 0
                },
                {
                    "sent": "I still get 20% improvements.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The cover type data sets yeah.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the conclusion is that ABC Boost formula formulated by user assuming based class and adaptive choose based classes each iterations and I see some considerable improvements.",
                    "label": 0
                },
                {
                    "sent": "And if you're interested in the in the in the partition of the data like cover type, I think that's only data set with no standard.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's a fairly large, but I did my own partition, but if you're interested I can give you the indexes so.",
                    "label": 0
                },
                {
                    "sent": "How are you measuring the?",
                    "label": 0
                },
                {
                    "sent": "Classification accuracy error?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so if it matches then matches to the standard.",
                    "label": 0
                },
                {
                    "sent": "Well, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I don't do this confusion matrix and I didn't do that here.",
                    "label": 0
                },
                {
                    "sent": "Number.",
                    "label": 0
                },
                {
                    "sent": "Uh, for this data set, I don't, I don't.",
                    "label": 0
                },
                {
                    "sent": "I don't see, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I see I don't have appeared to see any here and it normally this when I come here.",
                    "label": 0
                },
                {
                    "sent": "The terminal here because of the machine accuracy reached.",
                    "label": 0
                },
                {
                    "sent": "Yeah so I cannot go any further.",
                    "label": 0
                },
                {
                    "sent": "Do you have any gift for your mom?",
                    "label": 0
                },
                {
                    "sent": "I think for this kind of data sets, is the data sets.",
                    "label": 0
                },
                {
                    "sent": "Is it fairly fairly?",
                    "label": 0
                },
                {
                    "sent": "I think it's probably due to the data sets, yeah?",
                    "label": 0
                },
                {
                    "sent": "No neighbor noises?",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, that's a good question.",
                    "label": 0
                },
                {
                    "sent": "I actually experimented by swapping the labels and with some other experiments, I do see overfitting.",
                    "label": 0
                },
                {
                    "sent": "If you swap the labels yet.",
                    "label": 0
                },
                {
                    "sent": "OK, can you comment about?",
                    "label": 0
                },
                {
                    "sent": "What exactly the guys at the provided the proper data set?",
                    "label": 0
                },
                {
                    "sent": "Did the poker?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so you mentioned that you have 40% error?",
                    "label": 0
                },
                {
                    "sent": "That's what they.",
                    "label": 0
                },
                {
                    "sent": "That's what the live stream guys told me.",
                    "label": 0
                },
                {
                    "sent": "They have 40% error doing SVM.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's what they're doing because I'm not especially best VM so I don't want to report any results that I'm not expecting expert.",
                    "label": 0
                },
                {
                    "sent": "So they are the experts and they told me last month the error is about 40%.",
                    "label": 0
                },
                {
                    "sent": "But they're still working on them, so maybe maybe not fix the problem and it's a Gemini unbalanced data set.",
                    "label": 0
                },
                {
                    "sent": "And they say very small training, very large testing.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "It's kind of not very.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "Yeah it's a very strange data set.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "OK, let's thank speaker.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}