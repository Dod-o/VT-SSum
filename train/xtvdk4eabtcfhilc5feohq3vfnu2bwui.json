{
    "id": "xtvdk4eabtcfhilc5feohq3vfnu2bwui",
    "title": "Convex Sparse Methods for Feature Hierarchies",
    "info": {
        "author": [
            "Francis R. Bach, INRIA - SIERRA project-team"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Feature Selection",
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_bach_csmf/",
    "segmentation": [
        [
            "OK, so thank you for the introduction, so I have to admit I was quite the first 100 to be invited, but then a bit intrigued.",
            "Or do you really want to hear what I have to say?",
            "Or do you want to force me and torture myth?",
            "We want full day of a deep learning.",
            "So as you will see I will not have any deep.",
            "I will have some deep material, but not with deep learning so.",
            "Another another."
        ],
        [
            "Title B, also learning with kernels, is not dead.",
            "So people want us to die, but we are still fighting a bit.",
            "And if you have seen the talk by."
        ],
        [
            "In that got this, learning kernels themselves is not dead yet, so this is at least one example.",
            "I think we're learning.",
            "The kernel is fruitful.",
            "And to make really sure that this is not deep, you can also rephrase the talk's."
        ],
        [
            "Seeing some smart shallow learning so I do believe this is possible and my goal is this talk is to convince you coming soon up that."
        ],
        [
            "So we start first by giving a small review of supervised learning, comparing egg to norms and L1 norms.",
            "Then I will go quickly over the multiple kernel learning framework and then to the hierarchical version, which I think adapted to this workshop on future jockeys.",
            "So essentially mine will be built by hand, while the ones which are used in deep learning are learned from scratch.",
            "But in a sense we do the same thing and I built from scratch and you learn from scratch."
        ],
        [
            "OK, so let's take this simple regular setting when you want to predict why some wise given some X an half of machine learning is considering something similar to that where you minimize negative function, which is the sum of a data fitting term like that and plus some regularizer.",
            "Anne, this simple framework has led to a lot of work in statistics machine learning annually you have a few issues which are very important.",
            "First we have the loss.",
            "OK?",
            "Which loss should I use for which type of data?",
            "So this is depend on the Y. the Y viable an.",
            "So if you want to talk in a deep workshop and help people understand you call that energy.",
            "But it is pretty much the same.",
            "And then the other issue is which function space do you use and which number?",
            "Sorry, I know.",
            "I know it's not the same, it's.",
            "What you mean by three?",
            "Sure, sure, but inactive network shallow framework is the same.",
            "So and also another issue which I think is more important to me is which function space should you use?",
            "And this corresponds to in our framework which Hilbert space an in this community, more like which architecture should you use.",
            "But the issues at the same.",
            "Why should you use for prediction?",
            "OK, so I will focus more.",
            "We consider the last business being solved OK for example by young stock this morning and I will consider Molly on the mostly on the function space."
        ],
        [
            "So here.",
            "In the usual statistics framework, you have two camp.",
            "OK, so why why you want to regularize is is clear and I won't go too much into details.",
            "We want to avoid overfitting.",
            "And the goal.",
            "So everything, you have two main too many games.",
            "We have the Hilbert camp with esentially L2 norm, so Hilbert norms and this leads to these nonlinear kernel methods which have been hearing a lot for the last 20 years.",
            "Maybe too much for people in this audience an."
        ],
        [
            "The other camp is a is more recent is acampo sparsity inducing norms to 1st you go back from nonlinear to linear, so you always predict with a linear function W transpose X and the main example is the element norm, which which uses some of the absolute values.",
            "And here the cool thing is that you do two things at the price of 1.",
            "First you regularize, so you avoid overfitting and you learn the model.",
            "So it's really cool.",
            "You do two things for the price of 1.",
            "And I will try to show you some.",
            "Advances in this topic of sparsity inducing norms, but before before I do so."
        ],
        [
            "Let me review, methods.",
            "Think may be good for you, which we haven't used them for the last 10 years.",
            "So it also we will point out what's good and bad about can methods.",
            "It was true and wrong about kernel methods, so here essentially can methods they start out by building a set of features so it will be implicit, but still we are linear in those sets of features.",
            "There's nothing magic is still linear learning, but on the specific set of features an if you penalized by the L2 norm of the white, the weight vector.",
            "So here's the small 2 minutes to sum.",
            "This is a server, the squares of the components of W. Then the nice thing is that."
        ],
        [
            "The Representor theorem, which tells you that whatever the dimension of Phi is, even if you have many, many features you know in advance that you solution will be expanded as a linear combination of your input data points.",
            "OK, so this is a well known from the from the 70s.",
            "And now if you take back this expression so you know in advance is going to be a platform, so put that form back into the objective function and you get back the usual kernel trick, namely that everything depends only on the kernel matrix of that products, so they see the main.",
            "The main tool that we use in the in kernel methods.",
            "So the good thing is that at the end when you solve that."
        ],
        [
            "It becomes independent from the dimension of the features.",
            "You can have infinitely many features.",
            "You can still run it, and everything will depend only on the number of data points.",
            "So here I can see immediately the weapons of the neural network community saying that N square is useless because an when is 1 million, we can never never learn, but there have been a lot of work trying to be used at N squared to something which is closer to Earth.",
            "So here I put 10 square just to make sure that it includes everything, but you could do.",
            "You could do a bit less.",
            "What is important is this is independent from P, at least if you can compute the kernel in constant time.",
            "OK, so of course if the computer kernel you have to eliminate all the features and then take that product is kind of useless because you still have this over Penn Square P to do it.",
            "So the classical examples I think I won't explain those two.",
            "I think to me these kernels on structured data is something which I think kernels have been very good at and maybe deep learning should also focus on those types of data like several graphs.",
            "For mathematics, for example, is something that we would like to do in color methods and which is not not a lot done in this, at least that I know of and maybe.",
            "On this is, you know.",
            "Sure, but when I when I when I mean structured data is like when you want to put it on proteins.",
            "OK for example then you have to design a good set of features and this is where I think you're not so good at and I don't know.",
            "I'm not aware of any work from the neural network community in that setting.",
            "Maybe I'm wrong, but OK, I'm wrong.",
            "I'm good.",
            "I hope you'll be the first one is OK, so let's talk about that.",
            "I think to me when you have structure, kernels are really, really important and is a leading.",
            "Community has led to a lot of improvements in terms of pure performance."
        ],
        [
            "OK so but this is a good.",
            "The good part is that you have implicit nonlinearities with high dimensionality and the negative part to me, really is the fact that at the end you get linear combination of basis vectors.",
            "Fine, but you get a bunch of alphas, but can you really explain why it works?",
            "In a sense?",
            "Why is a prediction so good or so bad if you don't like them at the end?",
            "We have alphas.",
            "It's hard to see what the machine does at Test time.",
            "OK, so this is similar problems of interoperability.",
            "And also I think we've been learning a bit about this really high dimension.",
            "OK, we say we learn infinite dimensional feature space, but I think this is not true at all."
        ],
        [
            "And this is a simple reason.",
            "Why is the simple Zenos paradox so they know if they know wants to catch back the toys he has to do half of the the distance between himself, the toys, then another half the Ralph, then another Alpha.",
            "So we have to do infinitely many steps.",
            "So you might think that it will never catch the doctor list.",
            "But what's important is that all those steps are smaller and smaller, and the service is convergent.",
            "You see the same thing for the methods.",
            "You can explain the kernels in a lot of basis functions.",
            "But the wait is decaying very quickly, so essentially you have a number of features that go with the number of data points, but very very slowly.",
            "So to me this is not actually really infinite dimension, which is something which is growing, but not that fast.",
            "So this is just on the abstract self inflicted bashing on."
        ],
        [
            "Words.",
            "Let's look at the other game right now so the other camp is.",
            "You do like same thing.",
            "The sum of a data fitting term like that.",
            "So now we go back to linear methods.",
            "So I think we should push back a bit on what we achieved here.",
            "We have spent 10 years doing kernels and now we for the last two or three years we all we all go back to linear methods.",
            "Do we actually want to do that or will problems linear?",
            "I don't.",
            "I don't think so.",
            "And they already think that having too.",
            "Enumerate all the features which you have to do when you do anymore.",
            "Norms is kind of a step backward and I think both neural networks and kernels will agree in that respect is that nonlinearities are very important in practice and removing them I think it's on the long run.",
            "What won't work.",
            "So this is called the LASSO or the basic pursuit.",
            "If you have a square loss.",
            "OK, so here we get the cool thing is that when you optimize that you get some zeros and I won't explain you what."
        ],
        [
            "Now you get back to this setting where everyone knows will lead to what I call interpretable model.",
            "So of course positive sparsity does not mean interpret ability is some other proxy proxy for that.",
            "Birthday to noncontrolling, presently with very large spaces.",
            "So first which ones run faster?",
            "OK, because Actinometer leads to convex smooth optimization problems which are kind of nice, but L1 norms will lead to a nonsmooth problems, and it might be more difficult to optimize the nonsmooth function and people have shown in the last five years that this is not the case, and if I have time."
        ],
        [
            "I will show you the usual like placenta toys against the Goshen hair and since it's funny I will still do it.",
            "So Laplace was in favor of L1 norm and Gauss was in favor of L2 norms, so the L1 knobs looks lower.",
            "So This is why it's a lesson toys.",
            "But as you know the the toys wins OK even though it looks it looks lower and the reason the good stuff about that is that Laplace is French so it's a good thing and even.",
            "Even better, Le Plus thought that economic superior, which is even even better.",
            "So here the reasons why it it does it is it is faster than L2 norms is that you can use the fact that you get a spot solutions.",
            "OK, so you use it.",
            "You know in advance that you're going to get a lot of zeros so people can use that to design fast algorithms.",
            "So don't think that L1 is slower than L2L1 can be made can be made a lot faster than ever.",
            "So this is in terms."
        ],
        [
            "For algorithms, so in terms of our theory, I think those are the two main recent contributions which are very important in the domain of sparse methods.",
            "So the first one is 1 where theory has been late with respect to practice.",
            "So people have been using L1 norm for the last 10 or 20 years or 15 years, but we didn't knew we did not know whether it was working or not.",
            "What we getting the correct model.",
            "So we were all selling that we will get the correct model in at the end of the optimization.",
            "In fact those guys all shown that this is this is not always the case.",
            "OK, so I didn't put the condition because this is not the main topic of the talk.",
            "And the other good results is that.",
            "People have shown that under certain conditions that you could learn with exponentially many features.",
            "So this goes back to the what Joshua said this morning.",
            "That shallow is bad because you have exponentially exponentially many features by say, who cares.",
            "We can still learn.",
            "You can still learn from that.",
            "So of course we have assumptions.",
            "OK, those are our many assumptions hidden in this exponentially many features.",
            "Relevant.",
            "Sure, sure yeah.",
            "Good point.",
            "So of course all the hidden assumption is that although you have exponentially many features to choose from, only a few are relevant for your learning task.",
            "So you see where specific comes in.",
            "So this is a."
        ],
        [
            "So still, so let's say you believe theory.",
            "And that you would like to learn from exponentially many features.",
            "Let's take N being 100.",
            "So theory somewhat predicts that you could run from 10 to the 100 features.",
            "Where?",
            "How would you run such an algorithm?",
            "So this is for me, where a place where theories in advance theory somewhat predict that you could learn, but nobody can.",
            "You can not.",
            "If you were to think about that many features, that's kind of hard.",
            "OK, you need some time to enumerate the features and what I claim in this talk is that.",
            "If you were, if people want to achieve this bound in practice, you need to have a way of generating these features in the jockey or in recursive way in the sense you need a computer program to generate the features.",
            "Otherwise we never be able to even think about that many features.",
            "Try to think about 10 to the 80 features.",
            "OK, try and if you find a way by the end of this dog should come back to me because this will be quite impressed.",
            "The only way is to say that you are aggressive way.",
            "This is all the subsets of the given set, for example, so you have instructor save you can.",
            "Design it you can.",
            "You can think about it.",
            "You can learn from it and this will be the topic of the other two."
        ],
        [
            "So sometimes the focus of it is where I think is not so different from what people do in the deep learning.",
            "So in my case I will build the Rocky by hand, whereas in deep learning you will only be the structure, but their features inside the jockey will be learn from data, so it's not to miss not totally opposite."
        ],
        [
            "So how does it work?",
            "So I will present quickly as a multiple kernel learning framework."
        ],
        [
            "OK so here.",
            "In multiple kernel learning, so we assume that we have a bunch of kernels.",
            "OK, so each kernel corresponds to a one.",
            "To one set of features, so the kernel the kernels are indexed by V. And we assume that.",
            "So if you look at an 8 or the features, this is equivalent in kernel space to summing the kernels.",
            "This is a very simple and useful notion.",
            "If you catenate equivalent to solving the kernels like that.",
            "So now on this concatenation, then you have one WVU.",
            "TV and now.",
            "But when you will learn on those set of kernels, you have two ways of regularising with the first one is to use the square or Delta.",
            "Norm is simply using the sum of the kernels and the other one is to penalize by L1 norm.",
            "So putting the square there is not.",
            "It does not make a difference.",
            "So if you use an L1 norm, this will induce sparsity as a group level and we have shown with girl that this corresponds to learning context combination.",
            "So if you have heard the talk by Acarina Curtis at the main conference.",
            "I think this is a thing that she missed.",
            "Innocence MCL is a spark methods, at least when you restrict to positive combinations of matrices.",
            "So since this is a sparse method, you might expect it to work when you have a sparse problem, namely if you have a lot of kernels and you assume that many of them are just noise.",
            "OK, so this is, at least to me explanation why MKL might might not work if you have a lot of kernels and you want to select a few of them, then it's going to work.",
            "But if you take 20 Journal which has been hand designed to work.",
            "It should not work because L2 is that setting.",
            "L2 norms are more appropriate, so this is not the topic of the talk.",
            "So now the goal will be to have to do kernel learning on the specific."
        ],
        [
            "Case and here the goal will be to put all those kernels so the server the Cavi Inagaki.",
            "So here let me give me an example which was my motivation for this work is a one of nonlinear variable selection, so take this score, the Gaussian and over Colonel.",
            "So essentially this allows you to learn any function on the Q variables and if you take that this is a product of sums and you can invert the sum of the product and you get a son over all possible subsets.",
            "Of the regular gas and kernel on on that subset.",
            "So here I have an instance of a kernel which I can compute easily, which is which is an expansion of not infinitely many, but exponentially many, many kernels.",
            "So if Q is 100, you have two to the 100 kernels over there.",
            "So now the goal is to do what is to learn a sparse convex combination over all those, all those kernels, and what I claim it as this will do exactly what I want, which is namely no linear variable selection.",
            "To do linear variable selection, you can always fix yourself great.",
            "Treat yourself to only subsets of size 1 becausw linear functions at the next property of being linear functions, whereas if you want to do nonlinear selection, you want to be able to consider higher order interactions.",
            "OK, all nonlinear functions are not.",
            "Some of simple functions.",
            "You might need to have a higher order interaction, and essentially if you want to be what we like to be universally consistent, to be able to adapt to all possible set of features.",
            "You need at one point to consider at least I thesis all the possible, all the possible subsets.",
            "OK, so here are start from Q variables to Q might be big and I end up by trying to learn from 2 kernels, so it's kind of exploding blowing up the representation."
        ],
        [
            "So how are we going to do that?",
            "So if you want to be sparse.",
            "An you penalized by the L1 norm?",
            "OK then you will have a lot of zeros, but those deals will be scattered all over the place and this requires 2 being being able to enumerate or your kernels at least once, so this will be to the P in terms of complexity or to the cube.",
            "So this cannot be used and what we will do is to use the fact that we have a lot of kernels, but I'm not in the flat structure, they are, they are in a yucky."
        ],
        [
            "So how does it work with subsets here?",
            "From now on, I will focus on the subsets because it season made many examples I have worked on, so the set of subsets, even though with the natural directed acyclic graph.",
            "So if every subset points to its immediate bigger subsets.",
            "So this is an example for subsets of size of 1234 and what we will use is Cherokee to West tricked the choice of the kernel.",
            "Essentially we always make sure.",
            "That we select the kernel only if we have selected all the ancestors before.",
            "So in terms of nonlinear variable selection, it means that you will include higher order interaction only if you have already included all those smaller ones.",
            "So this is a common constraint in variable selection.",
            "So here we will.",
            "We will fix the problem and now the goal is can we learn in polynomial time from that exponentially many kernels in a convex way.",
            "OK, so this is a.",
            "This can be achieved by choosing a particular norm."
        ],
        [
            "I won't go into the details of the norm because I don't have enough time, but the take home message that we can do it by imposing some constraints on the problem, we are able to.",
            "We are able to learn over this exponentially.",
            "Many kernels in a political time.",
            "So here's the catch is that we restrict alot the set of kernels.",
            "OK, we're not searching over the two to the to the P subsets of that, we only search over the subsets which are connected to the source 'cause IT system.",
            "So maybe the deep learning framework.",
            "If this is the the top of the rocky you first use the layers which are closer to the end.",
            "And maybe try to go a bit further than the further down the.",
            "Done the Yaqui so this may be a disease.",
            "May have a possible use in deep learning.",
            "Once you have learn all your features."
        ],
        [
            "So now what we can prove it, which I won't go into into today is you have a polynomial time algorithm for this norm which will be able to be applicable for larger, large values of P. You know exactly when you are consistent in the sense you know exactly when you're going to select the good kernels.",
            "You can have this nice killing between PQ&N, so here P is a size of the dog, so P is huge OK, P essentially is to the Q and we can achieve a rate of N equals low Q which is the right for linear variable selection and we can achieve for nonlinear variable selection and we've applied it to variable selection.",
            "Which I will show you quickly, so this is a big number.",
            "Go OK."
        ],
        [
            "So this is a slide that Coyner showed that the main conference.",
            "But this is the bottom part where results are good.",
            "So this is where essentially doing nonlinear variable selection does help, as opposed to using L2 norm.",
            "So you see the simple example where L1 norm does bit add to norm, especially in cases where those problems are known to be nonlinear.",
            "An hard and this is where the ashkal ash cave does work.",
            "So this is to me an example where.",
            "So essentially what we did, we do the problem.",
            "OK, we put we left it in a huge huge feature space and by doing.",
            "Clever or subtle things in this big space for able to learn, so is the case.",
            "We go from Q very small to the Q and are still able able to learn."
        ],
        [
            "So we can extend that to other kernels and this one.",
            "In fact, the main motivation for this work is worth to apply it to images like what two countries or two kernels build on strings.",
            "So this is a nice application of of kernels is too all strings and usually those kernels will count the number of common substrings.",
            "So at the end, if you learn with the search kernels where you get, you get a bunch of alphas.",
            "But what you really want to do is to get which other substrings which are important for my problem.",
            "So essentially you want to be able to interpret your method and by doing L1 on the set of substrings, you will be able to do so.",
            "And of course a set of substrings is of course embedded in a nice tree, so you could be you could apply this type of technique to this this problem.",
            "OK, so."
        ],
        [
            "The rest look at the conclusion of discussion about this topic, so here.",
            "To me it's not because you're shallow that you're stupid.",
            "OK, so essentially what I've tried to show you is that.",
            "But you think a flat architecture with, well, you know you have a lot of features.",
            "You guys still under certain circumstances, being able to learn both in theory and both algorithmically and to me.",
            "This is really where there is a misunderstanding between the two, the two approaches.",
            "You may have a lot of features, but if you regularize you can still deal with them.",
            "OK, so this is to be very important that it's not because you have a lot of features that you know that you cannot learn.",
            "If you do constrain the set of functions within those features, you can still learn.",
            "I think it is to me one of them is an understanding."
        ],
        [
            "Now let's try to compare the two.",
            "OK, so I think this is a I think really important to me that to see what is good and bad in the in the two, the two types of techniques.",
            "So first and this we start with the nice things and I will finish with the less nice things.",
            "First, we all agree that nonlinearities are important.",
            "OK, so neural networks are non linear, kernel methods are nonlinear.",
            "So I think that is very important.",
            "Also, even even nicer multitask learning is important.",
            "Anthony Neural networks and deep learning has been in advance in that for that.",
            "So if you do that for the last 1015 years to integers, so neural networks are inherently multi task and but it ask has entered the convex framework only five years ago.",
            "So I think we were poor late were late here, so I think this is to me a good contribution important from new networks known as going to the let's go to the smaller.",
            "Play sing by.",
            "Maybe you have some misunderstanding that William so problems are not convex.",
            "So I do believe so problems are not convex.",
            "If you give me a problem I'm almost is not convex, but the goal, at least in my work, is to find a way to lift it in a place where it becomes convex, OK, and hear what I try to show you is that by going by lifting in two by building by hand all those features I've made the problem convex, or at least I've made the problem easy enough.",
            "To be able to be sold by context methods here the catch is that you need a lot.",
            "A lot of features to be able to use convex methods, but if you're smart enough in the way you optimize, you can you can deal with that.",
            "So I think problems are not convex, but it's not.",
            "It's not.",
            "It's not always a problem and he seems to me.",
            "Well, the two communities I've been turning their back at each other OK, so let's say in the convex path framework we take simpler and simpler problems and show a nicer nicer bounds without further and further from a real real real real applications.",
            "And in the other direction, people think in this community I've been using more and more complex architecture.",
            "And which become more and more harder and harder to explain.",
            "And I think really people should turn around and see whether you can get something out from the other from the other side.",
            "Essentially, I believe that we find myself in the convex case.",
            "We should really try to look whether you're either way of explaining why you methods work in practice and in exchange.",
            "Maybe it would be a nice idea to try to clean was what works in deep learning and what does not work.",
            "OK, the sequence of step and I'm pretty sure that some of them are very, very important and some of them are not so important, so being able to extract what makes it work, I think is quite a quite important another at the final world, I think that.",
            "At the end, we know we all know that we cannot learn directly from the data.",
            "There will be some some intermediate layers such that we need to use for prediction anthomy just whether you want to learn engineer or sample those other ones.",
            "So in deep learning you learn, you learn the people, learn the.",
            "The features in what I what I have presented today, people engineer the features, but you could see.",
            "Imagine that you could sample some of those and these have been recent work.",
            "OK, all these compressed sensing craziness is about that.",
            "If you sample sufficiently many features, but do learn the last layer, then people have been been able to learn and get good predictions.",
            "So to me it is really the issue.",
            "Do you want to learn engineer or center the features?",
            "Thank you for your attention.",
            "Only consider a future.",
            "Yep.",
            "Sure, so in that case I include all the audiences as well, so I would in your case I would get it.",
            "But I will need to include all of it sensors, so I need to include too many to many of those and this is in fact correct that we do it also in the lab trying to be able to catch a future without having to include not audience sisters but just the path to the ancestor.",
            "There's an exponential number of ancestors.",
            "Sure, sure, but these days you will never have enough data points too.",
            "If you want to include the higher order interaction then you need a lot of data points to support it, so usually so in the rocky.",
            "So here there is.",
            "OK the.",
            "Gotta catch here so."
        ],
        [
            "So that's a good way to catch here that, OK, you have a lot of potentially a lot of features, but of course to be able to get to the end of the rocky you would need tend to be huge.",
            "OK, so in practice, if the ad is not is not that big, you stopped after four or five 5 levels, we never go deeper than the five or six, but if you work to have to go bigger, you would need to go to go further.",
            "So this is one point I think you said it when you talk about really do with which is that basically have the guests coming through your choice between stacking more layers and keeping each layer or reasonable size or just having two layers for making the middle layer exponential.",
            "Theoretical results in the circuit theory that distributed through this morning basically tell you that right.",
            "You can this most Boolean functions, for example required exponential number of in terms if you want to implement them to come into layers.",
            "But if you allow yourself to add a folder with some video components in becoming an example.",
            "So I agree with this other question is you know you said there are practical ways to garner.",
            "So two layer systems within special features.",
            "But it's in some ways.",
            "There's somewhat weak conditions of features and the features that are sort of linear combinations of subsets of features that already exists, and in the end we do in SVM the actual number of features you have is just another training samples.",
            "You had it right because you run through your models and then the number.",
            "Sure, it's even less.",
            "It's even less than that.",
            "It is even a lot less than a number of data points.",
            "You can prove it's like log in or smaller in or smaller class quieter than in the best cases, so it's even worse than having.",
            "At most, any feature is having a login or small power event features, but still you need agree you need to have exponentially many features and my point is that you can still learn from those.",
            "In practice.",
            "Problems.",
            "Pixel Pictures at the same time.",
            "You come up with some sort of, exponentially, you know, some sort of.",
            "Way of generating exponential number of features that will solve the vision problem, but that way because it.",
            "Yeah what you're doing is sort of dimensions, so right so I know you always say that committed the template matching, but it's a bit more than that.",
            "Here I have OK if you wish template from infinitely many templates you wish.",
            "But it's not just template matching.",
            "You have more than that in current methods.",
            "No dad OK?",
            "Here you have to be honest that if you want to deal directly with kernels, you need to bid at.",
            "I think by now a case by hand, an intermediate representation like sift, OK if you want to do end to end learning, agree that is going to be tough for us to do that, but do you actually need that in practice, as in your case, can you take stuff on pixels 'cause pixels are natural, but the point you will have to put something into your deep learning architecture.",
            "You select pixels in my key to select.",
            "I'll do a bit more work.",
            "Can I use a more refined feature?",
            "But you know, in every place where you use learning, you have to add a feature so you stop with no prior knowledge or little edge, and I put more prior knowledge into into the system.",
            "So the main difference is I'm willing to use prior knowledge.",
            "Different way prepare knowledge right input it by combining a small set of variables like we do in commercial Nets or engine rebuilding in front of functions.",
            "Just two different ways of prior knowledge for different situations will have advantages, sure, but we are willing to use parallel.",
            "It's been engineered by clever people accept using Sift really to be the problem because it actually makes a problem too simple in some cases.",
            "Should be.",
            "So you are learning all these Filipino.",
            "There should be.",
            "It depends if your goal is to produce human intelligence, yes, but if your goal is to have to solve computer vision to solve urban formatics, maybe you don't.",
            "Always need to build features from scratch."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so thank you for the introduction, so I have to admit I was quite the first 100 to be invited, but then a bit intrigued.",
                    "label": 0
                },
                {
                    "sent": "Or do you really want to hear what I have to say?",
                    "label": 0
                },
                {
                    "sent": "Or do you want to force me and torture myth?",
                    "label": 0
                },
                {
                    "sent": "We want full day of a deep learning.",
                    "label": 0
                },
                {
                    "sent": "So as you will see I will not have any deep.",
                    "label": 0
                },
                {
                    "sent": "I will have some deep material, but not with deep learning so.",
                    "label": 0
                },
                {
                    "sent": "Another another.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Title B, also learning with kernels, is not dead.",
                    "label": 1
                },
                {
                    "sent": "So people want us to die, but we are still fighting a bit.",
                    "label": 0
                },
                {
                    "sent": "And if you have seen the talk by.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In that got this, learning kernels themselves is not dead yet, so this is at least one example.",
                    "label": 1
                },
                {
                    "sent": "I think we're learning.",
                    "label": 0
                },
                {
                    "sent": "The kernel is fruitful.",
                    "label": 0
                },
                {
                    "sent": "And to make really sure that this is not deep, you can also rephrase the talk's.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Seeing some smart shallow learning so I do believe this is possible and my goal is this talk is to convince you coming soon up that.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we start first by giving a small review of supervised learning, comparing egg to norms and L1 norms.",
                    "label": 1
                },
                {
                    "sent": "Then I will go quickly over the multiple kernel learning framework and then to the hierarchical version, which I think adapted to this workshop on future jockeys.",
                    "label": 1
                },
                {
                    "sent": "So essentially mine will be built by hand, while the ones which are used in deep learning are learned from scratch.",
                    "label": 0
                },
                {
                    "sent": "But in a sense we do the same thing and I built from scratch and you learn from scratch.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's take this simple regular setting when you want to predict why some wise given some X an half of machine learning is considering something similar to that where you minimize negative function, which is the sum of a data fitting term like that and plus some regularizer.",
                    "label": 0
                },
                {
                    "sent": "Anne, this simple framework has led to a lot of work in statistics machine learning annually you have a few issues which are very important.",
                    "label": 0
                },
                {
                    "sent": "First we have the loss.",
                    "label": 0
                },
                {
                    "sent": "OK?",
                    "label": 0
                },
                {
                    "sent": "Which loss should I use for which type of data?",
                    "label": 0
                },
                {
                    "sent": "So this is depend on the Y. the Y viable an.",
                    "label": 0
                },
                {
                    "sent": "So if you want to talk in a deep workshop and help people understand you call that energy.",
                    "label": 0
                },
                {
                    "sent": "But it is pretty much the same.",
                    "label": 0
                },
                {
                    "sent": "And then the other issue is which function space do you use and which number?",
                    "label": 1
                },
                {
                    "sent": "Sorry, I know.",
                    "label": 0
                },
                {
                    "sent": "I know it's not the same, it's.",
                    "label": 0
                },
                {
                    "sent": "What you mean by three?",
                    "label": 0
                },
                {
                    "sent": "Sure, sure, but inactive network shallow framework is the same.",
                    "label": 1
                },
                {
                    "sent": "So and also another issue which I think is more important to me is which function space should you use?",
                    "label": 0
                },
                {
                    "sent": "And this corresponds to in our framework which Hilbert space an in this community, more like which architecture should you use.",
                    "label": 0
                },
                {
                    "sent": "But the issues at the same.",
                    "label": 0
                },
                {
                    "sent": "Why should you use for prediction?",
                    "label": 0
                },
                {
                    "sent": "OK, so I will focus more.",
                    "label": 0
                },
                {
                    "sent": "We consider the last business being solved OK for example by young stock this morning and I will consider Molly on the mostly on the function space.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here.",
                    "label": 0
                },
                {
                    "sent": "In the usual statistics framework, you have two camp.",
                    "label": 0
                },
                {
                    "sent": "OK, so why why you want to regularize is is clear and I won't go too much into details.",
                    "label": 0
                },
                {
                    "sent": "We want to avoid overfitting.",
                    "label": 1
                },
                {
                    "sent": "And the goal.",
                    "label": 1
                },
                {
                    "sent": "So everything, you have two main too many games.",
                    "label": 0
                },
                {
                    "sent": "We have the Hilbert camp with esentially L2 norm, so Hilbert norms and this leads to these nonlinear kernel methods which have been hearing a lot for the last 20 years.",
                    "label": 0
                },
                {
                    "sent": "Maybe too much for people in this audience an.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The other camp is a is more recent is acampo sparsity inducing norms to 1st you go back from nonlinear to linear, so you always predict with a linear function W transpose X and the main example is the element norm, which which uses some of the absolute values.",
                    "label": 1
                },
                {
                    "sent": "And here the cool thing is that you do two things at the price of 1.",
                    "label": 1
                },
                {
                    "sent": "First you regularize, so you avoid overfitting and you learn the model.",
                    "label": 0
                },
                {
                    "sent": "So it's really cool.",
                    "label": 0
                },
                {
                    "sent": "You do two things for the price of 1.",
                    "label": 0
                },
                {
                    "sent": "And I will try to show you some.",
                    "label": 0
                },
                {
                    "sent": "Advances in this topic of sparsity inducing norms, but before before I do so.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me review, methods.",
                    "label": 0
                },
                {
                    "sent": "Think may be good for you, which we haven't used them for the last 10 years.",
                    "label": 0
                },
                {
                    "sent": "So it also we will point out what's good and bad about can methods.",
                    "label": 0
                },
                {
                    "sent": "It was true and wrong about kernel methods, so here essentially can methods they start out by building a set of features so it will be implicit, but still we are linear in those sets of features.",
                    "label": 0
                },
                {
                    "sent": "There's nothing magic is still linear learning, but on the specific set of features an if you penalized by the L2 norm of the white, the weight vector.",
                    "label": 0
                },
                {
                    "sent": "So here's the small 2 minutes to sum.",
                    "label": 0
                },
                {
                    "sent": "This is a server, the squares of the components of W. Then the nice thing is that.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The Representor theorem, which tells you that whatever the dimension of Phi is, even if you have many, many features you know in advance that you solution will be expanded as a linear combination of your input data points.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a well known from the from the 70s.",
                    "label": 0
                },
                {
                    "sent": "And now if you take back this expression so you know in advance is going to be a platform, so put that form back into the objective function and you get back the usual kernel trick, namely that everything depends only on the kernel matrix of that products, so they see the main.",
                    "label": 0
                },
                {
                    "sent": "The main tool that we use in the in kernel methods.",
                    "label": 0
                },
                {
                    "sent": "So the good thing is that at the end when you solve that.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It becomes independent from the dimension of the features.",
                    "label": 0
                },
                {
                    "sent": "You can have infinitely many features.",
                    "label": 0
                },
                {
                    "sent": "You can still run it, and everything will depend only on the number of data points.",
                    "label": 0
                },
                {
                    "sent": "So here I can see immediately the weapons of the neural network community saying that N square is useless because an when is 1 million, we can never never learn, but there have been a lot of work trying to be used at N squared to something which is closer to Earth.",
                    "label": 0
                },
                {
                    "sent": "So here I put 10 square just to make sure that it includes everything, but you could do.",
                    "label": 0
                },
                {
                    "sent": "You could do a bit less.",
                    "label": 0
                },
                {
                    "sent": "What is important is this is independent from P, at least if you can compute the kernel in constant time.",
                    "label": 0
                },
                {
                    "sent": "OK, so of course if the computer kernel you have to eliminate all the features and then take that product is kind of useless because you still have this over Penn Square P to do it.",
                    "label": 0
                },
                {
                    "sent": "So the classical examples I think I won't explain those two.",
                    "label": 0
                },
                {
                    "sent": "I think to me these kernels on structured data is something which I think kernels have been very good at and maybe deep learning should also focus on those types of data like several graphs.",
                    "label": 0
                },
                {
                    "sent": "For mathematics, for example, is something that we would like to do in color methods and which is not not a lot done in this, at least that I know of and maybe.",
                    "label": 0
                },
                {
                    "sent": "On this is, you know.",
                    "label": 0
                },
                {
                    "sent": "Sure, but when I when I when I mean structured data is like when you want to put it on proteins.",
                    "label": 0
                },
                {
                    "sent": "OK for example then you have to design a good set of features and this is where I think you're not so good at and I don't know.",
                    "label": 0
                },
                {
                    "sent": "I'm not aware of any work from the neural network community in that setting.",
                    "label": 0
                },
                {
                    "sent": "Maybe I'm wrong, but OK, I'm wrong.",
                    "label": 0
                },
                {
                    "sent": "I'm good.",
                    "label": 0
                },
                {
                    "sent": "I hope you'll be the first one is OK, so let's talk about that.",
                    "label": 0
                },
                {
                    "sent": "I think to me when you have structure, kernels are really, really important and is a leading.",
                    "label": 0
                },
                {
                    "sent": "Community has led to a lot of improvements in terms of pure performance.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so but this is a good.",
                    "label": 0
                },
                {
                    "sent": "The good part is that you have implicit nonlinearities with high dimensionality and the negative part to me, really is the fact that at the end you get linear combination of basis vectors.",
                    "label": 0
                },
                {
                    "sent": "Fine, but you get a bunch of alphas, but can you really explain why it works?",
                    "label": 0
                },
                {
                    "sent": "In a sense?",
                    "label": 0
                },
                {
                    "sent": "Why is a prediction so good or so bad if you don't like them at the end?",
                    "label": 0
                },
                {
                    "sent": "We have alphas.",
                    "label": 0
                },
                {
                    "sent": "It's hard to see what the machine does at Test time.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is similar problems of interoperability.",
                    "label": 0
                },
                {
                    "sent": "And also I think we've been learning a bit about this really high dimension.",
                    "label": 0
                },
                {
                    "sent": "OK, we say we learn infinite dimensional feature space, but I think this is not true at all.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is a simple reason.",
                    "label": 0
                },
                {
                    "sent": "Why is the simple Zenos paradox so they know if they know wants to catch back the toys he has to do half of the the distance between himself, the toys, then another half the Ralph, then another Alpha.",
                    "label": 0
                },
                {
                    "sent": "So we have to do infinitely many steps.",
                    "label": 0
                },
                {
                    "sent": "So you might think that it will never catch the doctor list.",
                    "label": 0
                },
                {
                    "sent": "But what's important is that all those steps are smaller and smaller, and the service is convergent.",
                    "label": 1
                },
                {
                    "sent": "You see the same thing for the methods.",
                    "label": 0
                },
                {
                    "sent": "You can explain the kernels in a lot of basis functions.",
                    "label": 0
                },
                {
                    "sent": "But the wait is decaying very quickly, so essentially you have a number of features that go with the number of data points, but very very slowly.",
                    "label": 1
                },
                {
                    "sent": "So to me this is not actually really infinite dimension, which is something which is growing, but not that fast.",
                    "label": 0
                },
                {
                    "sent": "So this is just on the abstract self inflicted bashing on.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Words.",
                    "label": 0
                },
                {
                    "sent": "Let's look at the other game right now so the other camp is.",
                    "label": 0
                },
                {
                    "sent": "You do like same thing.",
                    "label": 0
                },
                {
                    "sent": "The sum of a data fitting term like that.",
                    "label": 0
                },
                {
                    "sent": "So now we go back to linear methods.",
                    "label": 0
                },
                {
                    "sent": "So I think we should push back a bit on what we achieved here.",
                    "label": 0
                },
                {
                    "sent": "We have spent 10 years doing kernels and now we for the last two or three years we all we all go back to linear methods.",
                    "label": 0
                },
                {
                    "sent": "Do we actually want to do that or will problems linear?",
                    "label": 0
                },
                {
                    "sent": "I don't.",
                    "label": 0
                },
                {
                    "sent": "I don't think so.",
                    "label": 0
                },
                {
                    "sent": "And they already think that having too.",
                    "label": 0
                },
                {
                    "sent": "Enumerate all the features which you have to do when you do anymore.",
                    "label": 0
                },
                {
                    "sent": "Norms is kind of a step backward and I think both neural networks and kernels will agree in that respect is that nonlinearities are very important in practice and removing them I think it's on the long run.",
                    "label": 0
                },
                {
                    "sent": "What won't work.",
                    "label": 0
                },
                {
                    "sent": "So this is called the LASSO or the basic pursuit.",
                    "label": 0
                },
                {
                    "sent": "If you have a square loss.",
                    "label": 0
                },
                {
                    "sent": "OK, so here we get the cool thing is that when you optimize that you get some zeros and I won't explain you what.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now you get back to this setting where everyone knows will lead to what I call interpretable model.",
                    "label": 0
                },
                {
                    "sent": "So of course positive sparsity does not mean interpret ability is some other proxy proxy for that.",
                    "label": 0
                },
                {
                    "sent": "Birthday to noncontrolling, presently with very large spaces.",
                    "label": 0
                },
                {
                    "sent": "So first which ones run faster?",
                    "label": 0
                },
                {
                    "sent": "OK, because Actinometer leads to convex smooth optimization problems which are kind of nice, but L1 norms will lead to a nonsmooth problems, and it might be more difficult to optimize the nonsmooth function and people have shown in the last five years that this is not the case, and if I have time.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I will show you the usual like placenta toys against the Goshen hair and since it's funny I will still do it.",
                    "label": 0
                },
                {
                    "sent": "So Laplace was in favor of L1 norm and Gauss was in favor of L2 norms, so the L1 knobs looks lower.",
                    "label": 0
                },
                {
                    "sent": "So This is why it's a lesson toys.",
                    "label": 0
                },
                {
                    "sent": "But as you know the the toys wins OK even though it looks it looks lower and the reason the good stuff about that is that Laplace is French so it's a good thing and even.",
                    "label": 0
                },
                {
                    "sent": "Even better, Le Plus thought that economic superior, which is even even better.",
                    "label": 0
                },
                {
                    "sent": "So here the reasons why it it does it is it is faster than L2 norms is that you can use the fact that you get a spot solutions.",
                    "label": 0
                },
                {
                    "sent": "OK, so you use it.",
                    "label": 0
                },
                {
                    "sent": "You know in advance that you're going to get a lot of zeros so people can use that to design fast algorithms.",
                    "label": 0
                },
                {
                    "sent": "So don't think that L1 is slower than L2L1 can be made can be made a lot faster than ever.",
                    "label": 0
                },
                {
                    "sent": "So this is in terms.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For algorithms, so in terms of our theory, I think those are the two main recent contributions which are very important in the domain of sparse methods.",
                    "label": 1
                },
                {
                    "sent": "So the first one is 1 where theory has been late with respect to practice.",
                    "label": 0
                },
                {
                    "sent": "So people have been using L1 norm for the last 10 or 20 years or 15 years, but we didn't knew we did not know whether it was working or not.",
                    "label": 0
                },
                {
                    "sent": "What we getting the correct model.",
                    "label": 0
                },
                {
                    "sent": "So we were all selling that we will get the correct model in at the end of the optimization.",
                    "label": 0
                },
                {
                    "sent": "In fact those guys all shown that this is this is not always the case.",
                    "label": 0
                },
                {
                    "sent": "OK, so I didn't put the condition because this is not the main topic of the talk.",
                    "label": 0
                },
                {
                    "sent": "And the other good results is that.",
                    "label": 0
                },
                {
                    "sent": "People have shown that under certain conditions that you could learn with exponentially many features.",
                    "label": 0
                },
                {
                    "sent": "So this goes back to the what Joshua said this morning.",
                    "label": 0
                },
                {
                    "sent": "That shallow is bad because you have exponentially exponentially many features by say, who cares.",
                    "label": 0
                },
                {
                    "sent": "We can still learn.",
                    "label": 0
                },
                {
                    "sent": "You can still learn from that.",
                    "label": 0
                },
                {
                    "sent": "So of course we have assumptions.",
                    "label": 1
                },
                {
                    "sent": "OK, those are our many assumptions hidden in this exponentially many features.",
                    "label": 0
                },
                {
                    "sent": "Relevant.",
                    "label": 0
                },
                {
                    "sent": "Sure, sure yeah.",
                    "label": 0
                },
                {
                    "sent": "Good point.",
                    "label": 0
                },
                {
                    "sent": "So of course all the hidden assumption is that although you have exponentially many features to choose from, only a few are relevant for your learning task.",
                    "label": 0
                },
                {
                    "sent": "So you see where specific comes in.",
                    "label": 0
                },
                {
                    "sent": "So this is a.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So still, so let's say you believe theory.",
                    "label": 0
                },
                {
                    "sent": "And that you would like to learn from exponentially many features.",
                    "label": 1
                },
                {
                    "sent": "Let's take N being 100.",
                    "label": 0
                },
                {
                    "sent": "So theory somewhat predicts that you could run from 10 to the 100 features.",
                    "label": 0
                },
                {
                    "sent": "Where?",
                    "label": 0
                },
                {
                    "sent": "How would you run such an algorithm?",
                    "label": 0
                },
                {
                    "sent": "So this is for me, where a place where theories in advance theory somewhat predict that you could learn, but nobody can.",
                    "label": 0
                },
                {
                    "sent": "You can not.",
                    "label": 0
                },
                {
                    "sent": "If you were to think about that many features, that's kind of hard.",
                    "label": 0
                },
                {
                    "sent": "OK, you need some time to enumerate the features and what I claim in this talk is that.",
                    "label": 0
                },
                {
                    "sent": "If you were, if people want to achieve this bound in practice, you need to have a way of generating these features in the jockey or in recursive way in the sense you need a computer program to generate the features.",
                    "label": 0
                },
                {
                    "sent": "Otherwise we never be able to even think about that many features.",
                    "label": 0
                },
                {
                    "sent": "Try to think about 10 to the 80 features.",
                    "label": 0
                },
                {
                    "sent": "OK, try and if you find a way by the end of this dog should come back to me because this will be quite impressed.",
                    "label": 0
                },
                {
                    "sent": "The only way is to say that you are aggressive way.",
                    "label": 0
                },
                {
                    "sent": "This is all the subsets of the given set, for example, so you have instructor save you can.",
                    "label": 0
                },
                {
                    "sent": "Design it you can.",
                    "label": 0
                },
                {
                    "sent": "You can think about it.",
                    "label": 1
                },
                {
                    "sent": "You can learn from it and this will be the topic of the other two.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So sometimes the focus of it is where I think is not so different from what people do in the deep learning.",
                    "label": 0
                },
                {
                    "sent": "So in my case I will build the Rocky by hand, whereas in deep learning you will only be the structure, but their features inside the jockey will be learn from data, so it's not to miss not totally opposite.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how does it work?",
                    "label": 0
                },
                {
                    "sent": "So I will present quickly as a multiple kernel learning framework.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so here.",
                    "label": 0
                },
                {
                    "sent": "In multiple kernel learning, so we assume that we have a bunch of kernels.",
                    "label": 1
                },
                {
                    "sent": "OK, so each kernel corresponds to a one.",
                    "label": 1
                },
                {
                    "sent": "To one set of features, so the kernel the kernels are indexed by V. And we assume that.",
                    "label": 1
                },
                {
                    "sent": "So if you look at an 8 or the features, this is equivalent in kernel space to summing the kernels.",
                    "label": 0
                },
                {
                    "sent": "This is a very simple and useful notion.",
                    "label": 0
                },
                {
                    "sent": "If you catenate equivalent to solving the kernels like that.",
                    "label": 0
                },
                {
                    "sent": "So now on this concatenation, then you have one WVU.",
                    "label": 0
                },
                {
                    "sent": "TV and now.",
                    "label": 0
                },
                {
                    "sent": "But when you will learn on those set of kernels, you have two ways of regularising with the first one is to use the square or Delta.",
                    "label": 1
                },
                {
                    "sent": "Norm is simply using the sum of the kernels and the other one is to penalize by L1 norm.",
                    "label": 1
                },
                {
                    "sent": "So putting the square there is not.",
                    "label": 0
                },
                {
                    "sent": "It does not make a difference.",
                    "label": 0
                },
                {
                    "sent": "So if you use an L1 norm, this will induce sparsity as a group level and we have shown with girl that this corresponds to learning context combination.",
                    "label": 0
                },
                {
                    "sent": "So if you have heard the talk by Acarina Curtis at the main conference.",
                    "label": 0
                },
                {
                    "sent": "I think this is a thing that she missed.",
                    "label": 0
                },
                {
                    "sent": "Innocence MCL is a spark methods, at least when you restrict to positive combinations of matrices.",
                    "label": 1
                },
                {
                    "sent": "So since this is a sparse method, you might expect it to work when you have a sparse problem, namely if you have a lot of kernels and you assume that many of them are just noise.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is, at least to me explanation why MKL might might not work if you have a lot of kernels and you want to select a few of them, then it's going to work.",
                    "label": 0
                },
                {
                    "sent": "But if you take 20 Journal which has been hand designed to work.",
                    "label": 0
                },
                {
                    "sent": "It should not work because L2 is that setting.",
                    "label": 0
                },
                {
                    "sent": "L2 norms are more appropriate, so this is not the topic of the talk.",
                    "label": 0
                },
                {
                    "sent": "So now the goal will be to have to do kernel learning on the specific.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Case and here the goal will be to put all those kernels so the server the Cavi Inagaki.",
                    "label": 0
                },
                {
                    "sent": "So here let me give me an example which was my motivation for this work is a one of nonlinear variable selection, so take this score, the Gaussian and over Colonel.",
                    "label": 0
                },
                {
                    "sent": "So essentially this allows you to learn any function on the Q variables and if you take that this is a product of sums and you can invert the sum of the product and you get a son over all possible subsets.",
                    "label": 0
                },
                {
                    "sent": "Of the regular gas and kernel on on that subset.",
                    "label": 0
                },
                {
                    "sent": "So here I have an instance of a kernel which I can compute easily, which is which is an expansion of not infinitely many, but exponentially many, many kernels.",
                    "label": 1
                },
                {
                    "sent": "So if Q is 100, you have two to the 100 kernels over there.",
                    "label": 0
                },
                {
                    "sent": "So now the goal is to do what is to learn a sparse convex combination over all those, all those kernels, and what I claim it as this will do exactly what I want, which is namely no linear variable selection.",
                    "label": 0
                },
                {
                    "sent": "To do linear variable selection, you can always fix yourself great.",
                    "label": 1
                },
                {
                    "sent": "Treat yourself to only subsets of size 1 becausw linear functions at the next property of being linear functions, whereas if you want to do nonlinear selection, you want to be able to consider higher order interactions.",
                    "label": 0
                },
                {
                    "sent": "OK, all nonlinear functions are not.",
                    "label": 0
                },
                {
                    "sent": "Some of simple functions.",
                    "label": 0
                },
                {
                    "sent": "You might need to have a higher order interaction, and essentially if you want to be what we like to be universally consistent, to be able to adapt to all possible set of features.",
                    "label": 0
                },
                {
                    "sent": "You need at one point to consider at least I thesis all the possible, all the possible subsets.",
                    "label": 0
                },
                {
                    "sent": "OK, so here are start from Q variables to Q might be big and I end up by trying to learn from 2 kernels, so it's kind of exploding blowing up the representation.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how are we going to do that?",
                    "label": 0
                },
                {
                    "sent": "So if you want to be sparse.",
                    "label": 0
                },
                {
                    "sent": "An you penalized by the L1 norm?",
                    "label": 0
                },
                {
                    "sent": "OK then you will have a lot of zeros, but those deals will be scattered all over the place and this requires 2 being being able to enumerate or your kernels at least once, so this will be to the P in terms of complexity or to the cube.",
                    "label": 0
                },
                {
                    "sent": "So this cannot be used and what we will do is to use the fact that we have a lot of kernels, but I'm not in the flat structure, they are, they are in a yucky.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how does it work with subsets here?",
                    "label": 0
                },
                {
                    "sent": "From now on, I will focus on the subsets because it season made many examples I have worked on, so the set of subsets, even though with the natural directed acyclic graph.",
                    "label": 1
                },
                {
                    "sent": "So if every subset points to its immediate bigger subsets.",
                    "label": 0
                },
                {
                    "sent": "So this is an example for subsets of size of 1234 and what we will use is Cherokee to West tricked the choice of the kernel.",
                    "label": 0
                },
                {
                    "sent": "Essentially we always make sure.",
                    "label": 1
                },
                {
                    "sent": "That we select the kernel only if we have selected all the ancestors before.",
                    "label": 0
                },
                {
                    "sent": "So in terms of nonlinear variable selection, it means that you will include higher order interaction only if you have already included all those smaller ones.",
                    "label": 0
                },
                {
                    "sent": "So this is a common constraint in variable selection.",
                    "label": 0
                },
                {
                    "sent": "So here we will.",
                    "label": 0
                },
                {
                    "sent": "We will fix the problem and now the goal is can we learn in polynomial time from that exponentially many kernels in a convex way.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a.",
                    "label": 0
                },
                {
                    "sent": "This can be achieved by choosing a particular norm.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I won't go into the details of the norm because I don't have enough time, but the take home message that we can do it by imposing some constraints on the problem, we are able to.",
                    "label": 0
                },
                {
                    "sent": "We are able to learn over this exponentially.",
                    "label": 0
                },
                {
                    "sent": "Many kernels in a political time.",
                    "label": 0
                },
                {
                    "sent": "So here's the catch is that we restrict alot the set of kernels.",
                    "label": 1
                },
                {
                    "sent": "OK, we're not searching over the two to the to the P subsets of that, we only search over the subsets which are connected to the source 'cause IT system.",
                    "label": 0
                },
                {
                    "sent": "So maybe the deep learning framework.",
                    "label": 1
                },
                {
                    "sent": "If this is the the top of the rocky you first use the layers which are closer to the end.",
                    "label": 0
                },
                {
                    "sent": "And maybe try to go a bit further than the further down the.",
                    "label": 0
                },
                {
                    "sent": "Done the Yaqui so this may be a disease.",
                    "label": 0
                },
                {
                    "sent": "May have a possible use in deep learning.",
                    "label": 0
                },
                {
                    "sent": "Once you have learn all your features.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now what we can prove it, which I won't go into into today is you have a polynomial time algorithm for this norm which will be able to be applicable for larger, large values of P. You know exactly when you are consistent in the sense you know exactly when you're going to select the good kernels.",
                    "label": 1
                },
                {
                    "sent": "You can have this nice killing between PQ&N, so here P is a size of the dog, so P is huge OK, P essentially is to the Q and we can achieve a rate of N equals low Q which is the right for linear variable selection and we can achieve for nonlinear variable selection and we've applied it to variable selection.",
                    "label": 0
                },
                {
                    "sent": "Which I will show you quickly, so this is a big number.",
                    "label": 0
                },
                {
                    "sent": "Go OK.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is a slide that Coyner showed that the main conference.",
                    "label": 0
                },
                {
                    "sent": "But this is the bottom part where results are good.",
                    "label": 0
                },
                {
                    "sent": "So this is where essentially doing nonlinear variable selection does help, as opposed to using L2 norm.",
                    "label": 0
                },
                {
                    "sent": "So you see the simple example where L1 norm does bit add to norm, especially in cases where those problems are known to be nonlinear.",
                    "label": 0
                },
                {
                    "sent": "An hard and this is where the ashkal ash cave does work.",
                    "label": 0
                },
                {
                    "sent": "So this is to me an example where.",
                    "label": 0
                },
                {
                    "sent": "So essentially what we did, we do the problem.",
                    "label": 0
                },
                {
                    "sent": "OK, we put we left it in a huge huge feature space and by doing.",
                    "label": 0
                },
                {
                    "sent": "Clever or subtle things in this big space for able to learn, so is the case.",
                    "label": 0
                },
                {
                    "sent": "We go from Q very small to the Q and are still able able to learn.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we can extend that to other kernels and this one.",
                    "label": 1
                },
                {
                    "sent": "In fact, the main motivation for this work is worth to apply it to images like what two countries or two kernels build on strings.",
                    "label": 0
                },
                {
                    "sent": "So this is a nice application of of kernels is too all strings and usually those kernels will count the number of common substrings.",
                    "label": 0
                },
                {
                    "sent": "So at the end, if you learn with the search kernels where you get, you get a bunch of alphas.",
                    "label": 0
                },
                {
                    "sent": "But what you really want to do is to get which other substrings which are important for my problem.",
                    "label": 0
                },
                {
                    "sent": "So essentially you want to be able to interpret your method and by doing L1 on the set of substrings, you will be able to do so.",
                    "label": 0
                },
                {
                    "sent": "And of course a set of substrings is of course embedded in a nice tree, so you could be you could apply this type of technique to this this problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The rest look at the conclusion of discussion about this topic, so here.",
                    "label": 0
                },
                {
                    "sent": "To me it's not because you're shallow that you're stupid.",
                    "label": 0
                },
                {
                    "sent": "OK, so essentially what I've tried to show you is that.",
                    "label": 0
                },
                {
                    "sent": "But you think a flat architecture with, well, you know you have a lot of features.",
                    "label": 1
                },
                {
                    "sent": "You guys still under certain circumstances, being able to learn both in theory and both algorithmically and to me.",
                    "label": 0
                },
                {
                    "sent": "This is really where there is a misunderstanding between the two, the two approaches.",
                    "label": 0
                },
                {
                    "sent": "You may have a lot of features, but if you regularize you can still deal with them.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is to be very important that it's not because you have a lot of features that you know that you cannot learn.",
                    "label": 0
                },
                {
                    "sent": "If you do constrain the set of functions within those features, you can still learn.",
                    "label": 0
                },
                {
                    "sent": "I think it is to me one of them is an understanding.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let's try to compare the two.",
                    "label": 0
                },
                {
                    "sent": "OK, so I think this is a I think really important to me that to see what is good and bad in the in the two, the two types of techniques.",
                    "label": 0
                },
                {
                    "sent": "So first and this we start with the nice things and I will finish with the less nice things.",
                    "label": 0
                },
                {
                    "sent": "First, we all agree that nonlinearities are important.",
                    "label": 1
                },
                {
                    "sent": "OK, so neural networks are non linear, kernel methods are nonlinear.",
                    "label": 0
                },
                {
                    "sent": "So I think that is very important.",
                    "label": 0
                },
                {
                    "sent": "Also, even even nicer multitask learning is important.",
                    "label": 1
                },
                {
                    "sent": "Anthony Neural networks and deep learning has been in advance in that for that.",
                    "label": 0
                },
                {
                    "sent": "So if you do that for the last 1015 years to integers, so neural networks are inherently multi task and but it ask has entered the convex framework only five years ago.",
                    "label": 0
                },
                {
                    "sent": "So I think we were poor late were late here, so I think this is to me a good contribution important from new networks known as going to the let's go to the smaller.",
                    "label": 0
                },
                {
                    "sent": "Play sing by.",
                    "label": 1
                },
                {
                    "sent": "Maybe you have some misunderstanding that William so problems are not convex.",
                    "label": 0
                },
                {
                    "sent": "So I do believe so problems are not convex.",
                    "label": 0
                },
                {
                    "sent": "If you give me a problem I'm almost is not convex, but the goal, at least in my work, is to find a way to lift it in a place where it becomes convex, OK, and hear what I try to show you is that by going by lifting in two by building by hand all those features I've made the problem convex, or at least I've made the problem easy enough.",
                    "label": 0
                },
                {
                    "sent": "To be able to be sold by context methods here the catch is that you need a lot.",
                    "label": 0
                },
                {
                    "sent": "A lot of features to be able to use convex methods, but if you're smart enough in the way you optimize, you can you can deal with that.",
                    "label": 0
                },
                {
                    "sent": "So I think problems are not convex, but it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "It's not always a problem and he seems to me.",
                    "label": 0
                },
                {
                    "sent": "Well, the two communities I've been turning their back at each other OK, so let's say in the convex path framework we take simpler and simpler problems and show a nicer nicer bounds without further and further from a real real real real applications.",
                    "label": 0
                },
                {
                    "sent": "And in the other direction, people think in this community I've been using more and more complex architecture.",
                    "label": 0
                },
                {
                    "sent": "And which become more and more harder and harder to explain.",
                    "label": 0
                },
                {
                    "sent": "And I think really people should turn around and see whether you can get something out from the other from the other side.",
                    "label": 0
                },
                {
                    "sent": "Essentially, I believe that we find myself in the convex case.",
                    "label": 0
                },
                {
                    "sent": "We should really try to look whether you're either way of explaining why you methods work in practice and in exchange.",
                    "label": 0
                },
                {
                    "sent": "Maybe it would be a nice idea to try to clean was what works in deep learning and what does not work.",
                    "label": 0
                },
                {
                    "sent": "OK, the sequence of step and I'm pretty sure that some of them are very, very important and some of them are not so important, so being able to extract what makes it work, I think is quite a quite important another at the final world, I think that.",
                    "label": 0
                },
                {
                    "sent": "At the end, we know we all know that we cannot learn directly from the data.",
                    "label": 0
                },
                {
                    "sent": "There will be some some intermediate layers such that we need to use for prediction anthomy just whether you want to learn engineer or sample those other ones.",
                    "label": 0
                },
                {
                    "sent": "So in deep learning you learn, you learn the people, learn the.",
                    "label": 0
                },
                {
                    "sent": "The features in what I what I have presented today, people engineer the features, but you could see.",
                    "label": 0
                },
                {
                    "sent": "Imagine that you could sample some of those and these have been recent work.",
                    "label": 0
                },
                {
                    "sent": "OK, all these compressed sensing craziness is about that.",
                    "label": 0
                },
                {
                    "sent": "If you sample sufficiently many features, but do learn the last layer, then people have been been able to learn and get good predictions.",
                    "label": 0
                },
                {
                    "sent": "So to me it is really the issue.",
                    "label": 0
                },
                {
                    "sent": "Do you want to learn engineer or center the features?",
                    "label": 0
                },
                {
                    "sent": "Thank you for your attention.",
                    "label": 0
                },
                {
                    "sent": "Only consider a future.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Sure, so in that case I include all the audiences as well, so I would in your case I would get it.",
                    "label": 0
                },
                {
                    "sent": "But I will need to include all of it sensors, so I need to include too many to many of those and this is in fact correct that we do it also in the lab trying to be able to catch a future without having to include not audience sisters but just the path to the ancestor.",
                    "label": 0
                },
                {
                    "sent": "There's an exponential number of ancestors.",
                    "label": 0
                },
                {
                    "sent": "Sure, sure, but these days you will never have enough data points too.",
                    "label": 0
                },
                {
                    "sent": "If you want to include the higher order interaction then you need a lot of data points to support it, so usually so in the rocky.",
                    "label": 0
                },
                {
                    "sent": "So here there is.",
                    "label": 0
                },
                {
                    "sent": "OK the.",
                    "label": 0
                },
                {
                    "sent": "Gotta catch here so.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's a good way to catch here that, OK, you have a lot of potentially a lot of features, but of course to be able to get to the end of the rocky you would need tend to be huge.",
                    "label": 0
                },
                {
                    "sent": "OK, so in practice, if the ad is not is not that big, you stopped after four or five 5 levels, we never go deeper than the five or six, but if you work to have to go bigger, you would need to go to go further.",
                    "label": 0
                },
                {
                    "sent": "So this is one point I think you said it when you talk about really do with which is that basically have the guests coming through your choice between stacking more layers and keeping each layer or reasonable size or just having two layers for making the middle layer exponential.",
                    "label": 0
                },
                {
                    "sent": "Theoretical results in the circuit theory that distributed through this morning basically tell you that right.",
                    "label": 0
                },
                {
                    "sent": "You can this most Boolean functions, for example required exponential number of in terms if you want to implement them to come into layers.",
                    "label": 0
                },
                {
                    "sent": "But if you allow yourself to add a folder with some video components in becoming an example.",
                    "label": 0
                },
                {
                    "sent": "So I agree with this other question is you know you said there are practical ways to garner.",
                    "label": 0
                },
                {
                    "sent": "So two layer systems within special features.",
                    "label": 0
                },
                {
                    "sent": "But it's in some ways.",
                    "label": 0
                },
                {
                    "sent": "There's somewhat weak conditions of features and the features that are sort of linear combinations of subsets of features that already exists, and in the end we do in SVM the actual number of features you have is just another training samples.",
                    "label": 0
                },
                {
                    "sent": "You had it right because you run through your models and then the number.",
                    "label": 0
                },
                {
                    "sent": "Sure, it's even less.",
                    "label": 0
                },
                {
                    "sent": "It's even less than that.",
                    "label": 0
                },
                {
                    "sent": "It is even a lot less than a number of data points.",
                    "label": 0
                },
                {
                    "sent": "You can prove it's like log in or smaller in or smaller class quieter than in the best cases, so it's even worse than having.",
                    "label": 0
                },
                {
                    "sent": "At most, any feature is having a login or small power event features, but still you need agree you need to have exponentially many features and my point is that you can still learn from those.",
                    "label": 0
                },
                {
                    "sent": "In practice.",
                    "label": 0
                },
                {
                    "sent": "Problems.",
                    "label": 0
                },
                {
                    "sent": "Pixel Pictures at the same time.",
                    "label": 0
                },
                {
                    "sent": "You come up with some sort of, exponentially, you know, some sort of.",
                    "label": 0
                },
                {
                    "sent": "Way of generating exponential number of features that will solve the vision problem, but that way because it.",
                    "label": 0
                },
                {
                    "sent": "Yeah what you're doing is sort of dimensions, so right so I know you always say that committed the template matching, but it's a bit more than that.",
                    "label": 0
                },
                {
                    "sent": "Here I have OK if you wish template from infinitely many templates you wish.",
                    "label": 0
                },
                {
                    "sent": "But it's not just template matching.",
                    "label": 0
                },
                {
                    "sent": "You have more than that in current methods.",
                    "label": 0
                },
                {
                    "sent": "No dad OK?",
                    "label": 0
                },
                {
                    "sent": "Here you have to be honest that if you want to deal directly with kernels, you need to bid at.",
                    "label": 0
                },
                {
                    "sent": "I think by now a case by hand, an intermediate representation like sift, OK if you want to do end to end learning, agree that is going to be tough for us to do that, but do you actually need that in practice, as in your case, can you take stuff on pixels 'cause pixels are natural, but the point you will have to put something into your deep learning architecture.",
                    "label": 0
                },
                {
                    "sent": "You select pixels in my key to select.",
                    "label": 0
                },
                {
                    "sent": "I'll do a bit more work.",
                    "label": 0
                },
                {
                    "sent": "Can I use a more refined feature?",
                    "label": 0
                },
                {
                    "sent": "But you know, in every place where you use learning, you have to add a feature so you stop with no prior knowledge or little edge, and I put more prior knowledge into into the system.",
                    "label": 0
                },
                {
                    "sent": "So the main difference is I'm willing to use prior knowledge.",
                    "label": 0
                },
                {
                    "sent": "Different way prepare knowledge right input it by combining a small set of variables like we do in commercial Nets or engine rebuilding in front of functions.",
                    "label": 0
                },
                {
                    "sent": "Just two different ways of prior knowledge for different situations will have advantages, sure, but we are willing to use parallel.",
                    "label": 0
                },
                {
                    "sent": "It's been engineered by clever people accept using Sift really to be the problem because it actually makes a problem too simple in some cases.",
                    "label": 0
                },
                {
                    "sent": "Should be.",
                    "label": 0
                },
                {
                    "sent": "So you are learning all these Filipino.",
                    "label": 0
                },
                {
                    "sent": "There should be.",
                    "label": 0
                },
                {
                    "sent": "It depends if your goal is to produce human intelligence, yes, but if your goal is to have to solve computer vision to solve urban formatics, maybe you don't.",
                    "label": 0
                },
                {
                    "sent": "Always need to build features from scratch.",
                    "label": 0
                }
            ]
        }
    }
}