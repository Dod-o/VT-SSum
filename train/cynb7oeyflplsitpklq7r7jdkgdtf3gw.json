{
    "id": "cynb7oeyflplsitpklq7r7jdkgdtf3gw",
    "title": "Expectation-prior PAC-Bayes Bounds for SVMs",
    "info": {
        "author": [
            "Shiliang Sun, Department of Computer Science and Technology, East China Normal University"
        ],
        "published": "April 14, 2010",
        "recorded": "March 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods->Support Vector Machines"
        ]
    },
    "url": "http://videolectures.net/pacbayesian_sun_eppb/",
    "segmentation": [
        [
            "Cover 3 theorems based on."
        ],
        [
            "The expectation prize and then briefly mention a simple insights for training algorithms using the generalization bounds of that.",
            "Emily will present some empirical work and some related research.",
            "The motivation is."
        ],
        [
            "We would like to design a good prior that is without seeing and data, and the idea is to using some expectation over the data distribution.",
            "Here a good price is that this prior can probably lead to tighter bounds just hopefully.",
            "So the setting here is is binary classification and the labels are minus 1 + 1 linear threshold classifiers is a considered.",
            "There is no bias be actually this notice serious limitation.",
            "We can put the bias into W by the augmented representation."
        ],
        [
            "First of all, we define important very important variable for this expectation.",
            "Prior this WP also commissioned by joint is just the expectation of the label Way Times 5X will.",
            "FedEx is the mapping of the of the input vector X in feature space by a proper kernel.",
            "So as a special case of this WP, let's look at this figure.",
            "It's in this case we have two classes.",
            "They have equal probabilities.",
            "Now WP is just the just the line linking the means of this, both positive and negative classes.",
            "So if we have a hyperplane perpendicular to this.",
            "WP probably we can get a good classifier, at least in this in this figure."
        ],
        [
            "The problem is that we cannot.",
            "We cannot exactly calculate this expectation because we don't know the distribution generally.",
            "But we can use the empirical estimate of the expectation to approximate it.",
            "The idea is to use the user training data of examples.",
            "We define this empirical estimate WPH, just as the average would have examples."
        ],
        [
            "You know, here is the first theorem is.",
            "We assume we have a single prior the price is causing centered at 8 WP and it is isotropic with identity variance.",
            "The posterior is also goes in a very important case and.",
            "And but here we will assume that the center of this, the mean of this posterior causing is mute times.",
            "WLW is normalized, but for the prior we didn't assume WP is is normalized vector becausw.",
            "It would induce some difficulty in derive a nice looking bond, we just do it there it is just WP.",
            "For this theorem there is another variable which is.",
            "The capital capital R, which is the Supreme of this of the norm of the FedEx.",
            "For some kernels this Supreme can be easily computed, such As for the ghosting is equal 2 to one.",
            "So this theorem states that with high probability that care divergent between the empirical and true heir of the Gibbs classifier is bounded by this this quantity for this country.",
            "We note here this term is the distance between UW tailed and eat at WP Hat.",
            "This is a term related to the probability of the number of training examples."
        ],
        [
            "This proof is is simple, is quite straightforward.",
            "Let's let's show it the first thing we need to do is to bound the code awareness.",
            "Because these two distributions are very simple Gaussian distributions, we can write down the code versions.",
            "And it involves WP because we don't know it.",
            "We would like to approximate by WP Hat.",
            "Therefore we add WP Hat and at the same time subtract it and then buy some simple algebra.",
            "We get this.",
            "Formula this inequality where we also use the culture Schwarz inequality here to maximize the inner product between 2 arrival variables.",
            "OK, now it seems that it is sufficient to bound this this.",
            "This term WP Head and WP."
        ],
        [
            "This this can be easily done because we we can use the existing result, making use of the Macdiarmid inequality.",
            "I think everybody here knew this, so the result is that with high probability, WP and is empirical estimate, their distance is bounded by this by this quantity."
        ],
        [
            "So for shorthand notation, we define be at.",
            "This suggests that the quality and define a as the distance between UW tail and each WP hat.",
            "Then we can get this this formula for which is the bound for the kill diversions.",
            "Then we use general theorem.",
            "This this should be according to John Langford and Seeger, bound just we replace.",
            "Data were true.",
            "We replace that with that over to here, then combine these two formulas for on fire.",
            "We can gather the bond.",
            "This is this is what we need for this theorem.",
            "Sometimes we."
        ],
        [
            "I would like to use multiple priors, so this is almost a direct extension of the single expectation prior bound for this.",
            "For this case because we use multiple priors, we we will.",
            "We will induce some additional penalty here.",
            "Look at the theorem.",
            "The additional penalty is Longwell over page a web page.",
            "It is the probability to choose prior PJ, so PJ is.",
            "So of course, in Gaussian distribution along the direction of WP and there is a scalar liturgy, and I think this proof of this theorem is where simpler, just by combining the single expectation private and union bound.",
            "So we don't talk about that.",
            "Let's consider another situation that is."
        ],
        [
            "If the number of the multiple price goes to Infinity in this case, we cannot use the multiple expectation prior.",
            "We have to change our assumption for the Gaussian prior.",
            "For this case, we assume that the prior is is elongated, causing the variance in the direction of WP is to square, but in other directions perpendicular to this WP it is still one is still one.",
            "And the posterior is still the same.",
            "The same goes in, is centered at Mu Dubled."
        ],
        [
            "So the proof of this theory theorem is just.",
            "Is the main procedure is to bound.",
            "Kelkoo P. Unfortunately this is this can be done very easily because there is some existing result 2.",
            "To characterize the kill divergences between two single Gaussian distributions for our setting, we can write down this key out divergences the first line.",
            "Two clumsy terms in in the bracket.",
            "We would like to simplify these two terms and just buy algebra and we can.",
            "We can get this formula Six.",
            "Here we we look, we see a very familiar term which is the distance between you double tiered and eat WP.",
            "So in this case we can still use the old trick.",
            "We plus.",
            "Eat WP hat and subtract it W hat using the same the same procedure."
        ],
        [
            "Compound this term.",
            "Phone so this is a this is a.",
            "So this is the result by making use of the bound full system and we can get the bound for care divergent skew copy.",
            "And I think this theorem is is basically proved because we can."
        ],
        [
            "Just use the general theorem and mix them up.",
            "We can get this final.",
            "A final bond for the for this.",
            "Expectation prior based bond."
        ],
        [
            "I guess just to be how briefly review the main bonds than \u00a33.00 for this expectation.",
            "Prior beast like baseball for linear threshold classifiers, we're not only interested in deriving some bounds, we are also interested in Eustis bounce to train our classifiers.",
            "We've found common term in this through bonds is the distance between UW tell Dan each WP hat, so if this term is smaller.",
            "The bone is is also smaller, so we can.",
            "We can use this inside to train as well.",
            "The idea is to replace W norm in the original SVM classifiers with with W -- 8 WP hat now and then we add the hinge loss and we can train algorithm SVM algorithms.",
            "OK, this this offer me I I'm a little present some empirical work thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cover 3 theorems based on.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The expectation prize and then briefly mention a simple insights for training algorithms using the generalization bounds of that.",
                    "label": 1
                },
                {
                    "sent": "Emily will present some empirical work and some related research.",
                    "label": 0
                },
                {
                    "sent": "The motivation is.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We would like to design a good prior that is without seeing and data, and the idea is to using some expectation over the data distribution.",
                    "label": 1
                },
                {
                    "sent": "Here a good price is that this prior can probably lead to tighter bounds just hopefully.",
                    "label": 1
                },
                {
                    "sent": "So the setting here is is binary classification and the labels are minus 1 + 1 linear threshold classifiers is a considered.",
                    "label": 0
                },
                {
                    "sent": "There is no bias be actually this notice serious limitation.",
                    "label": 0
                },
                {
                    "sent": "We can put the bias into W by the augmented representation.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First of all, we define important very important variable for this expectation.",
                    "label": 0
                },
                {
                    "sent": "Prior this WP also commissioned by joint is just the expectation of the label Way Times 5X will.",
                    "label": 0
                },
                {
                    "sent": "FedEx is the mapping of the of the input vector X in feature space by a proper kernel.",
                    "label": 0
                },
                {
                    "sent": "So as a special case of this WP, let's look at this figure.",
                    "label": 0
                },
                {
                    "sent": "It's in this case we have two classes.",
                    "label": 0
                },
                {
                    "sent": "They have equal probabilities.",
                    "label": 0
                },
                {
                    "sent": "Now WP is just the just the line linking the means of this, both positive and negative classes.",
                    "label": 0
                },
                {
                    "sent": "So if we have a hyperplane perpendicular to this.",
                    "label": 0
                },
                {
                    "sent": "WP probably we can get a good classifier, at least in this in this figure.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The problem is that we cannot.",
                    "label": 0
                },
                {
                    "sent": "We cannot exactly calculate this expectation because we don't know the distribution generally.",
                    "label": 0
                },
                {
                    "sent": "But we can use the empirical estimate of the expectation to approximate it.",
                    "label": 0
                },
                {
                    "sent": "The idea is to use the user training data of examples.",
                    "label": 0
                },
                {
                    "sent": "We define this empirical estimate WPH, just as the average would have examples.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You know, here is the first theorem is.",
                    "label": 0
                },
                {
                    "sent": "We assume we have a single prior the price is causing centered at 8 WP and it is isotropic with identity variance.",
                    "label": 0
                },
                {
                    "sent": "The posterior is also goes in a very important case and.",
                    "label": 1
                },
                {
                    "sent": "And but here we will assume that the center of this, the mean of this posterior causing is mute times.",
                    "label": 0
                },
                {
                    "sent": "WLW is normalized, but for the prior we didn't assume WP is is normalized vector becausw.",
                    "label": 0
                },
                {
                    "sent": "It would induce some difficulty in derive a nice looking bond, we just do it there it is just WP.",
                    "label": 0
                },
                {
                    "sent": "For this theorem there is another variable which is.",
                    "label": 0
                },
                {
                    "sent": "The capital capital R, which is the Supreme of this of the norm of the FedEx.",
                    "label": 1
                },
                {
                    "sent": "For some kernels this Supreme can be easily computed, such As for the ghosting is equal 2 to one.",
                    "label": 0
                },
                {
                    "sent": "So this theorem states that with high probability that care divergent between the empirical and true heir of the Gibbs classifier is bounded by this this quantity for this country.",
                    "label": 0
                },
                {
                    "sent": "We note here this term is the distance between UW tailed and eat at WP Hat.",
                    "label": 0
                },
                {
                    "sent": "This is a term related to the probability of the number of training examples.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This proof is is simple, is quite straightforward.",
                    "label": 0
                },
                {
                    "sent": "Let's let's show it the first thing we need to do is to bound the code awareness.",
                    "label": 1
                },
                {
                    "sent": "Because these two distributions are very simple Gaussian distributions, we can write down the code versions.",
                    "label": 0
                },
                {
                    "sent": "And it involves WP because we don't know it.",
                    "label": 0
                },
                {
                    "sent": "We would like to approximate by WP Hat.",
                    "label": 0
                },
                {
                    "sent": "Therefore we add WP Hat and at the same time subtract it and then buy some simple algebra.",
                    "label": 0
                },
                {
                    "sent": "We get this.",
                    "label": 0
                },
                {
                    "sent": "Formula this inequality where we also use the culture Schwarz inequality here to maximize the inner product between 2 arrival variables.",
                    "label": 0
                },
                {
                    "sent": "OK, now it seems that it is sufficient to bound this this.",
                    "label": 1
                },
                {
                    "sent": "This term WP Head and WP.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This this can be easily done because we we can use the existing result, making use of the Macdiarmid inequality.",
                    "label": 0
                },
                {
                    "sent": "I think everybody here knew this, so the result is that with high probability, WP and is empirical estimate, their distance is bounded by this by this quantity.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for shorthand notation, we define be at.",
                    "label": 0
                },
                {
                    "sent": "This suggests that the quality and define a as the distance between UW tail and each WP hat.",
                    "label": 0
                },
                {
                    "sent": "Then we can get this this formula for which is the bound for the kill diversions.",
                    "label": 0
                },
                {
                    "sent": "Then we use general theorem.",
                    "label": 0
                },
                {
                    "sent": "This this should be according to John Langford and Seeger, bound just we replace.",
                    "label": 0
                },
                {
                    "sent": "Data were true.",
                    "label": 0
                },
                {
                    "sent": "We replace that with that over to here, then combine these two formulas for on fire.",
                    "label": 0
                },
                {
                    "sent": "We can gather the bond.",
                    "label": 0
                },
                {
                    "sent": "This is this is what we need for this theorem.",
                    "label": 0
                },
                {
                    "sent": "Sometimes we.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I would like to use multiple priors, so this is almost a direct extension of the single expectation prior bound for this.",
                    "label": 0
                },
                {
                    "sent": "For this case because we use multiple priors, we we will.",
                    "label": 0
                },
                {
                    "sent": "We will induce some additional penalty here.",
                    "label": 0
                },
                {
                    "sent": "Look at the theorem.",
                    "label": 0
                },
                {
                    "sent": "The additional penalty is Longwell over page a web page.",
                    "label": 0
                },
                {
                    "sent": "It is the probability to choose prior PJ, so PJ is.",
                    "label": 0
                },
                {
                    "sent": "So of course, in Gaussian distribution along the direction of WP and there is a scalar liturgy, and I think this proof of this theorem is where simpler, just by combining the single expectation private and union bound.",
                    "label": 0
                },
                {
                    "sent": "So we don't talk about that.",
                    "label": 0
                },
                {
                    "sent": "Let's consider another situation that is.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If the number of the multiple price goes to Infinity in this case, we cannot use the multiple expectation prior.",
                    "label": 0
                },
                {
                    "sent": "We have to change our assumption for the Gaussian prior.",
                    "label": 1
                },
                {
                    "sent": "For this case, we assume that the prior is is elongated, causing the variance in the direction of WP is to square, but in other directions perpendicular to this WP it is still one is still one.",
                    "label": 0
                },
                {
                    "sent": "And the posterior is still the same.",
                    "label": 1
                },
                {
                    "sent": "The same goes in, is centered at Mu Dubled.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the proof of this theory theorem is just.",
                    "label": 0
                },
                {
                    "sent": "Is the main procedure is to bound.",
                    "label": 0
                },
                {
                    "sent": "Kelkoo P. Unfortunately this is this can be done very easily because there is some existing result 2.",
                    "label": 0
                },
                {
                    "sent": "To characterize the kill divergences between two single Gaussian distributions for our setting, we can write down this key out divergences the first line.",
                    "label": 0
                },
                {
                    "sent": "Two clumsy terms in in the bracket.",
                    "label": 0
                },
                {
                    "sent": "We would like to simplify these two terms and just buy algebra and we can.",
                    "label": 0
                },
                {
                    "sent": "We can get this formula Six.",
                    "label": 0
                },
                {
                    "sent": "Here we we look, we see a very familiar term which is the distance between you double tiered and eat WP.",
                    "label": 0
                },
                {
                    "sent": "So in this case we can still use the old trick.",
                    "label": 0
                },
                {
                    "sent": "We plus.",
                    "label": 0
                },
                {
                    "sent": "Eat WP hat and subtract it W hat using the same the same procedure.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Compound this term.",
                    "label": 0
                },
                {
                    "sent": "Phone so this is a this is a.",
                    "label": 0
                },
                {
                    "sent": "So this is the result by making use of the bound full system and we can get the bound for care divergent skew copy.",
                    "label": 0
                },
                {
                    "sent": "And I think this theorem is is basically proved because we can.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just use the general theorem and mix them up.",
                    "label": 0
                },
                {
                    "sent": "We can get this final.",
                    "label": 0
                },
                {
                    "sent": "A final bond for the for this.",
                    "label": 0
                },
                {
                    "sent": "Expectation prior based bond.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I guess just to be how briefly review the main bonds than \u00a33.00 for this expectation.",
                    "label": 0
                },
                {
                    "sent": "Prior beast like baseball for linear threshold classifiers, we're not only interested in deriving some bounds, we are also interested in Eustis bounce to train our classifiers.",
                    "label": 0
                },
                {
                    "sent": "We've found common term in this through bonds is the distance between UW tell Dan each WP hat, so if this term is smaller.",
                    "label": 0
                },
                {
                    "sent": "The bone is is also smaller, so we can.",
                    "label": 0
                },
                {
                    "sent": "We can use this inside to train as well.",
                    "label": 0
                },
                {
                    "sent": "The idea is to replace W norm in the original SVM classifiers with with W -- 8 WP hat now and then we add the hinge loss and we can train algorithm SVM algorithms.",
                    "label": 1
                },
                {
                    "sent": "OK, this this offer me I I'm a little present some empirical work thanks.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}