{
    "id": "7n64fa6e7aljx6lm6zm7xtgq437t5eht",
    "title": "Bad Universal Priors and Notions of Optimality",
    "info": {
        "author": [
            "Jan Leike, College of Engineering and Computer Science, Australian National University"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_leike_universal_priors/",
    "segmentation": [
        [
            "So the."
        ],
        [
            "The setting of this work is general reinforcement learning, where we don't make any assumptions about on our environment, except that it's computable.",
            "And that's a very weak assumption.",
            "So generally that's it's really hard to do anything.",
            "So to make our lives easier, we're going to assume that we have an infinite amount of computation time available.",
            "Um?",
            "So."
        ],
        [
            "One of the things that we could do.",
            "So one idea is you take the set of all computable environments that might be stochastic, and then you take a prior over these environments and that gives you a Bayesian mixture.",
            "An then so in the prior that you choose is a universal prior, meaning that environments that have a short program will have a higher probability.",
            "And then you maximize the expected rewards in the basean mixture that I just defined.",
            "So this is known as the agent I see.",
            "So the question that we want to look at in this talk is or in this work is is actually optimal.",
            "And I have to warn you up front I'll be result."
        ],
        [
            "They're going to be entirely negative, so.",
            "So first let's review the optimality notions that are known from the literature.",
            "So first, I'm actually is known to be pretty optimal, meaning that there is no policy that is at least as good in all environments and strictly better in at least one.",
            "The next optimality says that axes balanced pre optimal, which is the same as maximal liquid intelligence and that means that the if I look at how much value my policy gets in all environments and I waved at by the universal prior.",
            "Then there's my AXI.",
            "Is the policy that scores highest?",
            "And that this notion depends on the prior that they choose.",
            "And Lastly, I sees self optimizing, meaning that if there is a policy that is strongly asymptotically optimal in the sense that the value of that policy converges to the optimal value value, then this will also hold for AC.",
            "So in this work we prove that in fact every policy is pretty optimal.",
            "So pretty optimality is not really a useful notion of optimality.",
            "In this setting.",
            "The balance preacher optimality is highly depending on the Universal Turing machine that gives me my universal prior.",
            "So if I choose a different universal prior, I will have.",
            "I will my left.",
            "Her intelligence might be very low.",
            "So this this notion is this notion of optimality is highly subjective.",
            "And Lastly the self optimizing theorem doesn't apply to our setting because there is no strongly asymptotically optimal policies.",
            "So this is really depressing.",
            "There's really no nontrivial non subjective notions from optimality left.",
            "Um?",
            "So what does that mean?",
            "So.",
            "In particular, the question would I?"
        ],
        [
            "He works, so say I'm walking down the street one day and I'll find a halting Oracle just lying on the street and they go home and I build.",
            "I see so well will it work well it maximize rewards.",
            "We don't have a formal argument.",
            "There will work, but we know that if we add extra exploration on top of the Bayesian mixture then I can achieve a non trivial non subjective notion of optimality.",
            "So we might just conclude that.",
            "You don't explore enough to really get rid of all the priors biases, which is in contrast to the sequence prediction case where the bias of the prior asymptotically goes away.",
            "So the answer to this question is, well, it will maximize rewards in the Bajan way, but you get a strong bias by the prior that doesn't go away so.",
            "It really depends on how good your priors."
        ],
        [
            "So yes, if you want to know more, come to my poster and I'll tell you all about it.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The setting of this work is general reinforcement learning, where we don't make any assumptions about on our environment, except that it's computable.",
                    "label": 0
                },
                {
                    "sent": "And that's a very weak assumption.",
                    "label": 0
                },
                {
                    "sent": "So generally that's it's really hard to do anything.",
                    "label": 0
                },
                {
                    "sent": "So to make our lives easier, we're going to assume that we have an infinite amount of computation time available.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One of the things that we could do.",
                    "label": 0
                },
                {
                    "sent": "So one idea is you take the set of all computable environments that might be stochastic, and then you take a prior over these environments and that gives you a Bayesian mixture.",
                    "label": 1
                },
                {
                    "sent": "An then so in the prior that you choose is a universal prior, meaning that environments that have a short program will have a higher probability.",
                    "label": 0
                },
                {
                    "sent": "And then you maximize the expected rewards in the basean mixture that I just defined.",
                    "label": 0
                },
                {
                    "sent": "So this is known as the agent I see.",
                    "label": 0
                },
                {
                    "sent": "So the question that we want to look at in this talk is or in this work is is actually optimal.",
                    "label": 0
                },
                {
                    "sent": "And I have to warn you up front I'll be result.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "They're going to be entirely negative, so.",
                    "label": 0
                },
                {
                    "sent": "So first let's review the optimality notions that are known from the literature.",
                    "label": 0
                },
                {
                    "sent": "So first, I'm actually is known to be pretty optimal, meaning that there is no policy that is at least as good in all environments and strictly better in at least one.",
                    "label": 0
                },
                {
                    "sent": "The next optimality says that axes balanced pre optimal, which is the same as maximal liquid intelligence and that means that the if I look at how much value my policy gets in all environments and I waved at by the universal prior.",
                    "label": 0
                },
                {
                    "sent": "Then there's my AXI.",
                    "label": 0
                },
                {
                    "sent": "Is the policy that scores highest?",
                    "label": 0
                },
                {
                    "sent": "And that this notion depends on the prior that they choose.",
                    "label": 0
                },
                {
                    "sent": "And Lastly, I sees self optimizing, meaning that if there is a policy that is strongly asymptotically optimal in the sense that the value of that policy converges to the optimal value value, then this will also hold for AC.",
                    "label": 0
                },
                {
                    "sent": "So in this work we prove that in fact every policy is pretty optimal.",
                    "label": 0
                },
                {
                    "sent": "So pretty optimality is not really a useful notion of optimality.",
                    "label": 0
                },
                {
                    "sent": "In this setting.",
                    "label": 0
                },
                {
                    "sent": "The balance preacher optimality is highly depending on the Universal Turing machine that gives me my universal prior.",
                    "label": 1
                },
                {
                    "sent": "So if I choose a different universal prior, I will have.",
                    "label": 0
                },
                {
                    "sent": "I will my left.",
                    "label": 0
                },
                {
                    "sent": "Her intelligence might be very low.",
                    "label": 0
                },
                {
                    "sent": "So this this notion is this notion of optimality is highly subjective.",
                    "label": 0
                },
                {
                    "sent": "And Lastly the self optimizing theorem doesn't apply to our setting because there is no strongly asymptotically optimal policies.",
                    "label": 0
                },
                {
                    "sent": "So this is really depressing.",
                    "label": 0
                },
                {
                    "sent": "There's really no nontrivial non subjective notions from optimality left.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So what does that mean?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In particular, the question would I?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "He works, so say I'm walking down the street one day and I'll find a halting Oracle just lying on the street and they go home and I build.",
                    "label": 0
                },
                {
                    "sent": "I see so well will it work well it maximize rewards.",
                    "label": 0
                },
                {
                    "sent": "We don't have a formal argument.",
                    "label": 1
                },
                {
                    "sent": "There will work, but we know that if we add extra exploration on top of the Bayesian mixture then I can achieve a non trivial non subjective notion of optimality.",
                    "label": 0
                },
                {
                    "sent": "So we might just conclude that.",
                    "label": 0
                },
                {
                    "sent": "You don't explore enough to really get rid of all the priors biases, which is in contrast to the sequence prediction case where the bias of the prior asymptotically goes away.",
                    "label": 0
                },
                {
                    "sent": "So the answer to this question is, well, it will maximize rewards in the Bajan way, but you get a strong bias by the prior that doesn't go away so.",
                    "label": 0
                },
                {
                    "sent": "It really depends on how good your priors.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So yes, if you want to know more, come to my poster and I'll tell you all about it.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}