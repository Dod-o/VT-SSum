{
    "id": "dt6qflttjj7b6zzk3xzhc45n3bubowva",
    "title": "Learning Sparse Representations of High Dimensional Data on Large Scale Dictionaries",
    "info": {
        "author": [
            "Zhen James Xiang, Princeton University"
        ],
        "published": "Jan. 25, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/nips2011_xiang_dictionaries/",
    "segmentation": [
        [
            "OK, I will just get started.",
            "I'm James shell.",
            "This is joint work with how and my advisor Professor Peter Ramage.",
            "So today I'm going to talk about how to learn sparse representation more efficiently for large scale problems."
        ],
        [
            "The talk is going through his three parts.",
            "First of all, I'm going to introduce the problem, specifically the computational challenge of learning large scale of learning.",
            "Large scale sparse representations, and then I'm going to talk about two methods to make this process significantly faster.",
            "One of them is lasso screening, the other one is hierarchical dictionary, so now."
        ],
        [
            "Now let's get started with the problem.",
            "While we interested in sparse representations, well, we have heard a lot of great things about sparse representation, but in the context of this talk, I'd like to emphasize that sparse representation is a very effective way of discovering the hidden structuring data.",
            "So in this formulation we have the vector X and we're trying to represent this vector as the linear combination of the column vectors in B.",
            "So this column vectors we call them code words.",
            "And B is called a dictionary."
        ],
        [
            "This is a very familiar formulation because when B contains Fourier basis, this is not involved Fourier analysis."
        ],
        [
            "The same thing goes for wavelet analysis, but in a lot of machine learning applications our high dimensional data such as these ones usually have inherent structure.",
            "It's also very important to have a sparse representation that respects such inherent structure of data."
        ],
        [
            "So for example, when data are distributed on the non non linear non linear low dimensional manifold, the dictionary B should contain code words that are anchor points from the same manifold and the sparse representation just selects only a few code words that are most relevant in representing X."
        ],
        [
            "And sparsity plays a key role here, because we can extend this to the matrix formulation an we can ask the question what's the dictionary B?",
            "That will give us the sparsest or the simplest representation of this data.",
            "So by doing this we are learning a dictionary that's adapted to the data."
        ],
        [
            "Well, how can we do this?",
            "We can formulate this optimization problem which minimizes this Fidelity term regularised by the L1 norm.",
            "So this is a convex convex approximation."
        ],
        [
            "Well, this problem is very difficult to solve, by the way.",
            "This is also called the sparse coding for."
        ],
        [
            "Duration, and he has a lot of applications, but this problem is not jointly convex in BMW.",
            "So a lot of state of the arts use coordinate dissent or block owning the decent methods and what?",
            "And a very common method is to iteratively update WNB so when we update W we have to solve a lost soul problem which basically is a L1 regularised linear programming problem and we have to solve this for each data point and when we update B we have to solve a constraint.",
            "Least squares problem and this has to be iterated for many times."
        ],
        [
            "So now let's take a look at some examples just to get a sense of the scales we're dealing with in real world applications.",
            "So one of the example in which we can use this is face recognition.",
            "So here the idea is to represent a face image as the sparse linear combination of other training phases.",
            "And here I'd like to emphasize it's generally recommended in these publications to use 30 to 40 training subjects.",
            "Training images per subject.",
            "So that means the dictionary size.",
            "Is 30 to 40 times the number of subjects.",
            "That means even if we're building a small face recognition system for small communities such as Princeton University, we're talking about dictionary size of nearly half a million."
        ],
        [
            "And the second application is image restoration.",
            "So here the idea is to learn a sparse dictionary.",
            "Model for the natural path for the patches in natural images.",
            "So here I'd like to emphasize that we usually have nearly millions of data points.",
            "Their image patches sampled from natural images, or even video sequences, and we have to get a satisfactory dictionary.",
            "We have to run hundreds of iterations, so that means if we use the formulation on the last slide, then that sparse regression problem has to be solved for many, many times, and it's probably clear from this point that.",
            "Buying better computers doesn't solve the problem because the scale of the problem that we are interested in is constantly growing."
        ],
        [
            "So actually addressing these computational challenges from a algorithm standpoint is the active research field.",
            "If you just look around this year's NIPS conference, you will find some very related works, and I encourage you to look at these posters.",
            "So back to us."
        ],
        [
            "Our talk, our philosophy of addressing this problem, is to break large scale sparse representation problem into smaller scale problems and our first talk about lasso screening.",
            "So this solves the last server problem much faster before I even go into the."
        ],
        [
            "How about we just take some face images from a typical face recognition data set and see how fast it is to solve just one answer problem.",
            "So here we are representing a face onto a Dictionary of other faces, and I'm very in the dictionary size.",
            "Different color bars correspond to three different vessel servers I tried, so as you can see here that I'm required to solve just one answer problem quickly.",
            "Growth of the with the dictionary size and add dictionary size of 2048.",
            "Takes nearly 15 seconds to solve, just to one as a problem, so that's to recognize just one testing phase and we're talking about a very small toy face recognition system.",
            "And if we want to do dictionary iteration and etc, then this computation time."
        ],
        [
            "Really adds up.",
            "So I'm going to introduce a very efficient method to deal with this is called also screening, so as long as you can write your problem in this standard formulation in which you represent vector X as the sparse linear combination of BI, so be.",
            "I call them code words.",
            "Sometimes they refer to as Regressors.",
            "Then you can use the screening test so the idea of the screening test is to take a code will be I and apply this test and if BI satisfy this test then we can know for sure that the optimum coefficient in the optimal solution WI will be 0.",
            "Therefore we can reject or discard the."
        ],
        [
            "So you can imagine this as an actual send screen instead of with a lot of code words, and by applying this screen.",
            "Hopefully you can discard.",
            "A large percentage of the code words and you only have to solve the lesser problem on the remaining code words, and you're guaranteed to have the exact same solution because these coefficients are guaranteed to be 0."
        ],
        [
            "To make more sense of this, let's look at a specific screening test.",
            "So this is already in the literature called save test.",
            "In our paper, we call it ST1, so this test is very simple.",
            "It just calculates the correlation between X&BI and if it's certain and certain, and if it's smaller than a certain threshold, then we throw away by.",
            "So this is so.",
            "This is the online test, only requires 2 passes through the data.",
            "It's very efficient."
        ],
        [
            "So we just apply this test on that lasso problem.",
            "I just showed you of the dictionary size 2048 and see how it performs.",
            "Well disappointing it doesn't reject anything.",
            "We can make it reject something by increasing the Lambda value, which makes the lasso problem easier, but generally it's not very effective.",
            "Well, the good news is that in our paper we have some newer tests that are more."
        ],
        [
            "For example, we have these tests AST two that can reject almost 1/3 of the code word and make lots of problem two point 2.5."
        ],
        [
            "Times faster we have this better test ST3 that can make it 7.7 times faster.",
            "And after the NIPS submission."
        ],
        [
            "Deadline we have found this even valid test called Dome test that rejects up to 70% of the code words and makes the law so problems 21 times faster isn't that great."
        ],
        [
            "And the better news is that these tests are very fast, so the time for running these tests less than one millisecond.",
            "So that means within this total time I'm plotting here, less than 1% expand on actually doing the test."
        ],
        [
            "And it's not surprising because these tests are very simple, so this is all the recipe you need to implement these tests.",
            "Everything is in closed form expression.",
            "There is no iterative procedure, no parameter estimating or anything like that.",
            "So basically you can write 10 lines of code to implement this test, and then you can combine that Taylor Swift code with any of your favorite lasso solvers and achieve a significant speedup."
        ],
        [
            "And this can be done on very large scale datasets because we don't need to fit all the data into memory.",
            "It just requires 2 passes through the data and the memory footprint is as small as three code words.",
            "OK, so you might ask what?"
        ],
        [
            "The catch here?",
            "Well, let's read the fine print.",
            "We actually don't claim that these newer tests are always very powerful.",
            "Our theoretical analysis in the paper actually shows that they're only powerful when Lambda Max is Raj and what is Lambda Max.",
            "Lambda Max is the maximum correlation between X and your code words, so that means it's helpful to at least have some code words in the dictionary that's very correlated with X.",
            "But Fortunately this is actually a common situation in real world.",
            "Datasets so this is discussed in this paper, so they discussed the phenomenon that in real world datasets, data points usually have high correlations, so this is actually a bad situation for traditional compressive sensing, but they give some theoretical justifications of why this still YY.",
            "Applying sparse representation is still a good idea, and it's certainly a good situation for our screening test."
        ],
        [
            "In this setting."
        ],
        [
            "So now let me go into more details of the math behind these tests.",
            "These tests are not designed ad hoc, they have some optimization theories that go behind them, and so I should give a warning that this slide is going to get a little bit technical, but just for this one slide.",
            "So to derive this test we look at the dual problem of the lasso problem, and in this dual space we are optimizing the position of this dual variable cost data."
        ],
        [
            "And Theta."
        ],
        [
            "Is constrained by this hyperplane constraints, which says that thing has to be uncertain side of the hyperplane whose normal vectors RBI?"
        ],
        [
            "And we have developed developed this core rejection test, which is a consequence of a primal dual relationship called complementary slackness.",
            "What it says is that if the optimum dual solution Theta~ is not on the boundary of any specific hyperplane, then the code word corresponding to the hyperplane will have zero coefficients.",
            "But of course we cannot apply this test directly because that would imply we have already solved."
        ],
        [
            "The last cell problem.",
            "But what we can do is we can bound data~ within some certain region and in our test in the paper we have bound this Theta~ within some spherical region as showing this blue sphere here.",
            "And once we have done that we can develop these tasks using very simple geometric relationship that are characterized by this picture here.",
            "So that's as much as I would go into right now.",
            "I would be happy to talk about this in more detail."
        ],
        [
            "The other poster.",
            "So just to sum up, just to summarize the second part, we have talked about the LASSO screening test, and although the performance is dependent on some parameters such as Lambda and Lambda Max, this usually has a very good performance in the real applications that we are interested in.",
            "So we're going to switch gears and talk about hierarchical dictionary.",
            "So this is an efficient algorithm framework to learn tree structured multilayer."
        ],
        [
            "Dictionaries.",
            "So the idea here is to grow this dictionary in a tree structure and we enforce a structured sparsity constraints which says that the coefficient of a certain code word can only be non zero if its parent has non zero coefficients.",
            "So that means if the first layer 4 code words has only two zero coefficients as indicated by the black circle here, then in the second layer we only have to solve the sparse representation for this 8 Gray circles.",
            "So this reduces the computation.",
            "And it gives us a tree struck."
        ],
        [
            "A dictionary such as this.",
            "This illustrating example, so here we are learning a two layer structure dictionary on a bunch of images of the same object, rotating around a circle.",
            "So here as you can see, this two layer a dictionary almost gives you a multiresolution representation if you will, because this first 2 training images they have very similar orientations, so they have similar activation in the first layer, but they are further differentiated in the in the second layer, so this is actually not.",
            "A new idea, such a structure dictionary is appreciated in in the literature for some time."
        ],
        [
            "And our work builds on this work, especially the proximal methods paper, but our."
        ],
        [
            "The contribution here is that we have introduced the random projections to control the information flow and to learn this dictionary layer by layer much faster.",
            "So here the idea is instead of learning sparse representation on this data X, we use random projection matrix to sample the data and learn a much smaller scale sparse representation and for the first layer we can do this for very few random projections since we're only interested in the rough representation.",
            "And ah."
        ],
        [
            "As we go on."
        ],
        [
            "The previous layers can help the coding of later layers, which uses more random projections to learn a sparse representation that contains more details.",
            "So this is the basic gist of the idea.",
            "There are more details in the paper.",
            "For example, we have some analysis on the property of random projections."
        ],
        [
            "So I just jump through the evaluation since, unlike Lasso screening, will no longer to guarantee to have the exact same solution.",
            "So what we do is to evaluate the efficiency as well as the quality of the coding which is evaluated by the classification accuracy of SVM classifier using the sparse coding weights and the bottom line is that our methods can achieve the same quality of coding.",
            "Within much last time."
        ],
        [
            "So just to wrap up, learning sparse representation is hard, but there is hope, so we hope our theoretical results an algorithm framework in the paper can effectively make headway on addressing this challenge.",
            "We have talked about Lasso screening an hierarchical dictionary learning, and you can find more information on my website, so I'd like to thank all my collaborators and thank you very much for your attention, and I look forward to talk to you at the other post."
        ],
        [
            "OK, thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, I will just get started.",
                    "label": 0
                },
                {
                    "sent": "I'm James shell.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with how and my advisor Professor Peter Ramage.",
                    "label": 0
                },
                {
                    "sent": "So today I'm going to talk about how to learn sparse representation more efficiently for large scale problems.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The talk is going through his three parts.",
                    "label": 0
                },
                {
                    "sent": "First of all, I'm going to introduce the problem, specifically the computational challenge of learning large scale of learning.",
                    "label": 1
                },
                {
                    "sent": "Large scale sparse representations, and then I'm going to talk about two methods to make this process significantly faster.",
                    "label": 0
                },
                {
                    "sent": "One of them is lasso screening, the other one is hierarchical dictionary, so now.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's get started with the problem.",
                    "label": 0
                },
                {
                    "sent": "While we interested in sparse representations, well, we have heard a lot of great things about sparse representation, but in the context of this talk, I'd like to emphasize that sparse representation is a very effective way of discovering the hidden structuring data.",
                    "label": 0
                },
                {
                    "sent": "So in this formulation we have the vector X and we're trying to represent this vector as the linear combination of the column vectors in B.",
                    "label": 0
                },
                {
                    "sent": "So this column vectors we call them code words.",
                    "label": 0
                },
                {
                    "sent": "And B is called a dictionary.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is a very familiar formulation because when B contains Fourier basis, this is not involved Fourier analysis.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The same thing goes for wavelet analysis, but in a lot of machine learning applications our high dimensional data such as these ones usually have inherent structure.",
                    "label": 0
                },
                {
                    "sent": "It's also very important to have a sparse representation that respects such inherent structure of data.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for example, when data are distributed on the non non linear non linear low dimensional manifold, the dictionary B should contain code words that are anchor points from the same manifold and the sparse representation just selects only a few code words that are most relevant in representing X.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And sparsity plays a key role here, because we can extend this to the matrix formulation an we can ask the question what's the dictionary B?",
                    "label": 0
                },
                {
                    "sent": "That will give us the sparsest or the simplest representation of this data.",
                    "label": 0
                },
                {
                    "sent": "So by doing this we are learning a dictionary that's adapted to the data.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, how can we do this?",
                    "label": 0
                },
                {
                    "sent": "We can formulate this optimization problem which minimizes this Fidelity term regularised by the L1 norm.",
                    "label": 0
                },
                {
                    "sent": "So this is a convex convex approximation.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, this problem is very difficult to solve, by the way.",
                    "label": 0
                },
                {
                    "sent": "This is also called the sparse coding for.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Duration, and he has a lot of applications, but this problem is not jointly convex in BMW.",
                    "label": 0
                },
                {
                    "sent": "So a lot of state of the arts use coordinate dissent or block owning the decent methods and what?",
                    "label": 0
                },
                {
                    "sent": "And a very common method is to iteratively update WNB so when we update W we have to solve a lost soul problem which basically is a L1 regularised linear programming problem and we have to solve this for each data point and when we update B we have to solve a constraint.",
                    "label": 0
                },
                {
                    "sent": "Least squares problem and this has to be iterated for many times.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now let's take a look at some examples just to get a sense of the scales we're dealing with in real world applications.",
                    "label": 0
                },
                {
                    "sent": "So one of the example in which we can use this is face recognition.",
                    "label": 0
                },
                {
                    "sent": "So here the idea is to represent a face image as the sparse linear combination of other training phases.",
                    "label": 0
                },
                {
                    "sent": "And here I'd like to emphasize it's generally recommended in these publications to use 30 to 40 training subjects.",
                    "label": 0
                },
                {
                    "sent": "Training images per subject.",
                    "label": 0
                },
                {
                    "sent": "So that means the dictionary size.",
                    "label": 0
                },
                {
                    "sent": "Is 30 to 40 times the number of subjects.",
                    "label": 1
                },
                {
                    "sent": "That means even if we're building a small face recognition system for small communities such as Princeton University, we're talking about dictionary size of nearly half a million.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the second application is image restoration.",
                    "label": 0
                },
                {
                    "sent": "So here the idea is to learn a sparse dictionary.",
                    "label": 0
                },
                {
                    "sent": "Model for the natural path for the patches in natural images.",
                    "label": 0
                },
                {
                    "sent": "So here I'd like to emphasize that we usually have nearly millions of data points.",
                    "label": 1
                },
                {
                    "sent": "Their image patches sampled from natural images, or even video sequences, and we have to get a satisfactory dictionary.",
                    "label": 0
                },
                {
                    "sent": "We have to run hundreds of iterations, so that means if we use the formulation on the last slide, then that sparse regression problem has to be solved for many, many times, and it's probably clear from this point that.",
                    "label": 0
                },
                {
                    "sent": "Buying better computers doesn't solve the problem because the scale of the problem that we are interested in is constantly growing.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So actually addressing these computational challenges from a algorithm standpoint is the active research field.",
                    "label": 0
                },
                {
                    "sent": "If you just look around this year's NIPS conference, you will find some very related works, and I encourage you to look at these posters.",
                    "label": 0
                },
                {
                    "sent": "So back to us.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our talk, our philosophy of addressing this problem, is to break large scale sparse representation problem into smaller scale problems and our first talk about lasso screening.",
                    "label": 0
                },
                {
                    "sent": "So this solves the last server problem much faster before I even go into the.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How about we just take some face images from a typical face recognition data set and see how fast it is to solve just one answer problem.",
                    "label": 0
                },
                {
                    "sent": "So here we are representing a face onto a Dictionary of other faces, and I'm very in the dictionary size.",
                    "label": 0
                },
                {
                    "sent": "Different color bars correspond to three different vessel servers I tried, so as you can see here that I'm required to solve just one answer problem quickly.",
                    "label": 0
                },
                {
                    "sent": "Growth of the with the dictionary size and add dictionary size of 2048.",
                    "label": 0
                },
                {
                    "sent": "Takes nearly 15 seconds to solve, just to one as a problem, so that's to recognize just one testing phase and we're talking about a very small toy face recognition system.",
                    "label": 0
                },
                {
                    "sent": "And if we want to do dictionary iteration and etc, then this computation time.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Really adds up.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to introduce a very efficient method to deal with this is called also screening, so as long as you can write your problem in this standard formulation in which you represent vector X as the sparse linear combination of BI, so be.",
                    "label": 0
                },
                {
                    "sent": "I call them code words.",
                    "label": 0
                },
                {
                    "sent": "Sometimes they refer to as Regressors.",
                    "label": 0
                },
                {
                    "sent": "Then you can use the screening test so the idea of the screening test is to take a code will be I and apply this test and if BI satisfy this test then we can know for sure that the optimum coefficient in the optimal solution WI will be 0.",
                    "label": 1
                },
                {
                    "sent": "Therefore we can reject or discard the.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you can imagine this as an actual send screen instead of with a lot of code words, and by applying this screen.",
                    "label": 0
                },
                {
                    "sent": "Hopefully you can discard.",
                    "label": 0
                },
                {
                    "sent": "A large percentage of the code words and you only have to solve the lesser problem on the remaining code words, and you're guaranteed to have the exact same solution because these coefficients are guaranteed to be 0.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To make more sense of this, let's look at a specific screening test.",
                    "label": 0
                },
                {
                    "sent": "So this is already in the literature called save test.",
                    "label": 0
                },
                {
                    "sent": "In our paper, we call it ST1, so this test is very simple.",
                    "label": 0
                },
                {
                    "sent": "It just calculates the correlation between X&BI and if it's certain and certain, and if it's smaller than a certain threshold, then we throw away by.",
                    "label": 0
                },
                {
                    "sent": "So this is so.",
                    "label": 0
                },
                {
                    "sent": "This is the online test, only requires 2 passes through the data.",
                    "label": 1
                },
                {
                    "sent": "It's very efficient.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we just apply this test on that lasso problem.",
                    "label": 0
                },
                {
                    "sent": "I just showed you of the dictionary size 2048 and see how it performs.",
                    "label": 0
                },
                {
                    "sent": "Well disappointing it doesn't reject anything.",
                    "label": 0
                },
                {
                    "sent": "We can make it reject something by increasing the Lambda value, which makes the lasso problem easier, but generally it's not very effective.",
                    "label": 0
                },
                {
                    "sent": "Well, the good news is that in our paper we have some newer tests that are more.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For example, we have these tests AST two that can reject almost 1/3 of the code word and make lots of problem two point 2.5.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Times faster we have this better test ST3 that can make it 7.7 times faster.",
                    "label": 0
                },
                {
                    "sent": "And after the NIPS submission.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Deadline we have found this even valid test called Dome test that rejects up to 70% of the code words and makes the law so problems 21 times faster isn't that great.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the better news is that these tests are very fast, so the time for running these tests less than one millisecond.",
                    "label": 0
                },
                {
                    "sent": "So that means within this total time I'm plotting here, less than 1% expand on actually doing the test.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it's not surprising because these tests are very simple, so this is all the recipe you need to implement these tests.",
                    "label": 0
                },
                {
                    "sent": "Everything is in closed form expression.",
                    "label": 0
                },
                {
                    "sent": "There is no iterative procedure, no parameter estimating or anything like that.",
                    "label": 0
                },
                {
                    "sent": "So basically you can write 10 lines of code to implement this test, and then you can combine that Taylor Swift code with any of your favorite lasso solvers and achieve a significant speedup.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this can be done on very large scale datasets because we don't need to fit all the data into memory.",
                    "label": 0
                },
                {
                    "sent": "It just requires 2 passes through the data and the memory footprint is as small as three code words.",
                    "label": 0
                },
                {
                    "sent": "OK, so you might ask what?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The catch here?",
                    "label": 0
                },
                {
                    "sent": "Well, let's read the fine print.",
                    "label": 1
                },
                {
                    "sent": "We actually don't claim that these newer tests are always very powerful.",
                    "label": 1
                },
                {
                    "sent": "Our theoretical analysis in the paper actually shows that they're only powerful when Lambda Max is Raj and what is Lambda Max.",
                    "label": 0
                },
                {
                    "sent": "Lambda Max is the maximum correlation between X and your code words, so that means it's helpful to at least have some code words in the dictionary that's very correlated with X.",
                    "label": 1
                },
                {
                    "sent": "But Fortunately this is actually a common situation in real world.",
                    "label": 0
                },
                {
                    "sent": "Datasets so this is discussed in this paper, so they discussed the phenomenon that in real world datasets, data points usually have high correlations, so this is actually a bad situation for traditional compressive sensing, but they give some theoretical justifications of why this still YY.",
                    "label": 0
                },
                {
                    "sent": "Applying sparse representation is still a good idea, and it's certainly a good situation for our screening test.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this setting.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now let me go into more details of the math behind these tests.",
                    "label": 1
                },
                {
                    "sent": "These tests are not designed ad hoc, they have some optimization theories that go behind them, and so I should give a warning that this slide is going to get a little bit technical, but just for this one slide.",
                    "label": 0
                },
                {
                    "sent": "So to derive this test we look at the dual problem of the lasso problem, and in this dual space we are optimizing the position of this dual variable cost data.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And Theta.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is constrained by this hyperplane constraints, which says that thing has to be uncertain side of the hyperplane whose normal vectors RBI?",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we have developed developed this core rejection test, which is a consequence of a primal dual relationship called complementary slackness.",
                    "label": 0
                },
                {
                    "sent": "What it says is that if the optimum dual solution Theta~ is not on the boundary of any specific hyperplane, then the code word corresponding to the hyperplane will have zero coefficients.",
                    "label": 0
                },
                {
                    "sent": "But of course we cannot apply this test directly because that would imply we have already solved.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The last cell problem.",
                    "label": 0
                },
                {
                    "sent": "But what we can do is we can bound data~ within some certain region and in our test in the paper we have bound this Theta~ within some spherical region as showing this blue sphere here.",
                    "label": 0
                },
                {
                    "sent": "And once we have done that we can develop these tasks using very simple geometric relationship that are characterized by this picture here.",
                    "label": 0
                },
                {
                    "sent": "So that's as much as I would go into right now.",
                    "label": 0
                },
                {
                    "sent": "I would be happy to talk about this in more detail.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The other poster.",
                    "label": 0
                },
                {
                    "sent": "So just to sum up, just to summarize the second part, we have talked about the LASSO screening test, and although the performance is dependent on some parameters such as Lambda and Lambda Max, this usually has a very good performance in the real applications that we are interested in.",
                    "label": 0
                },
                {
                    "sent": "So we're going to switch gears and talk about hierarchical dictionary.",
                    "label": 0
                },
                {
                    "sent": "So this is an efficient algorithm framework to learn tree structured multilayer.",
                    "label": 1
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dictionaries.",
                    "label": 0
                },
                {
                    "sent": "So the idea here is to grow this dictionary in a tree structure and we enforce a structured sparsity constraints which says that the coefficient of a certain code word can only be non zero if its parent has non zero coefficients.",
                    "label": 0
                },
                {
                    "sent": "So that means if the first layer 4 code words has only two zero coefficients as indicated by the black circle here, then in the second layer we only have to solve the sparse representation for this 8 Gray circles.",
                    "label": 0
                },
                {
                    "sent": "So this reduces the computation.",
                    "label": 0
                },
                {
                    "sent": "And it gives us a tree struck.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A dictionary such as this.",
                    "label": 0
                },
                {
                    "sent": "This illustrating example, so here we are learning a two layer structure dictionary on a bunch of images of the same object, rotating around a circle.",
                    "label": 0
                },
                {
                    "sent": "So here as you can see, this two layer a dictionary almost gives you a multiresolution representation if you will, because this first 2 training images they have very similar orientations, so they have similar activation in the first layer, but they are further differentiated in the in the second layer, so this is actually not.",
                    "label": 0
                },
                {
                    "sent": "A new idea, such a structure dictionary is appreciated in in the literature for some time.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And our work builds on this work, especially the proximal methods paper, but our.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The contribution here is that we have introduced the random projections to control the information flow and to learn this dictionary layer by layer much faster.",
                    "label": 0
                },
                {
                    "sent": "So here the idea is instead of learning sparse representation on this data X, we use random projection matrix to sample the data and learn a much smaller scale sparse representation and for the first layer we can do this for very few random projections since we're only interested in the rough representation.",
                    "label": 0
                },
                {
                    "sent": "And ah.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As we go on.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The previous layers can help the coding of later layers, which uses more random projections to learn a sparse representation that contains more details.",
                    "label": 1
                },
                {
                    "sent": "So this is the basic gist of the idea.",
                    "label": 0
                },
                {
                    "sent": "There are more details in the paper.",
                    "label": 0
                },
                {
                    "sent": "For example, we have some analysis on the property of random projections.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I just jump through the evaluation since, unlike Lasso screening, will no longer to guarantee to have the exact same solution.",
                    "label": 0
                },
                {
                    "sent": "So what we do is to evaluate the efficiency as well as the quality of the coding which is evaluated by the classification accuracy of SVM classifier using the sparse coding weights and the bottom line is that our methods can achieve the same quality of coding.",
                    "label": 0
                },
                {
                    "sent": "Within much last time.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just to wrap up, learning sparse representation is hard, but there is hope, so we hope our theoretical results an algorithm framework in the paper can effectively make headway on addressing this challenge.",
                    "label": 0
                },
                {
                    "sent": "We have talked about Lasso screening an hierarchical dictionary learning, and you can find more information on my website, so I'd like to thank all my collaborators and thank you very much for your attention, and I look forward to talk to you at the other post.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}