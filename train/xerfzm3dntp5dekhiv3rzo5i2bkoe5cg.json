{
    "id": "xerfzm3dntp5dekhiv3rzo5i2bkoe5cg",
    "title": "BoltzRank: Learning to Maximize Expected Ranking Gain",
    "info": {
        "author": [
            "Maksims Volkovs, Department of Computer Science, University of Toronto"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Search Engines",
            "Top->Computer Science->Text Mining",
            "Top->Computer Science->Information Retrieval"
        ]
    },
    "url": "http://videolectures.net/icml09_volkovs_blmerg/",
    "segmentation": [
        [
            "OK so hi everyone my name is Maximus bogus and today I'm going to present the new learning trying method code both rank and so the joint work with."
        ],
        [
            "So first limit on the problem we're trying to solve.",
            "So in a typical ranking problem we're given end queries as input, where each query has a set of documents D and a set of relevance is L, where documents are query dependent features.",
            "In RP, an relevance is typically take small integer values, where the higher the value do more relevant.",
            "Responding document is to the query, so our goal is to sort the documents and out with them in order of relevance to the query.",
            "So our typical strategy which most people taken, which we will also take, is to create a scoring function which has some tunable parameters an which takes as input sets of documents and outputs the score for every document in that set.",
            "The scores are then used to sort the documents and output the ranked list."
        ],
        [
            "So the order between the scores and the relevance is typically judged by NCG.",
            "Once we sort the documents according to scores, we get this ranked list and and this is combines this rank list with relevance.",
            "Levels and is given here, so in this form T is a truncation constant, which means there were only interested in top T of the output documents.",
            "Real J is the relevance of the document at position J starting from the top and log one plus Jays.",
            "The discount function, which again emphasizes the outmost importance of the top ranked documents.",
            "Finally end Qi is the normalizing constant which ensures that energy is always between zero and one an, which allows us to meaningfully compare.",
            "And this is across different queries with different document sizes, so there's a problem with this function in that the sum is over the documents in sorted order and therefore the gradients of energy with respect to F are not smooth and we can't use any standard gradient descent to learn F. So in order to optimize this, we need a good approximation."
        ],
        [
            "So there are a number of previous methods that have been developed to solve this problem, and they can be divided into three categories.",
            "So the first category is the individual and individual methods map features of documents straight to scores without considering any relative information.",
            "The second group is the pairwise pairwise methods minimize pairwise misclassification probabilities where the target probability is given by.",
            "The relevance is in the last group at the list wise.",
            "Where a list of documents I used as learning instances and some listwise loss function is minimized.",
            "So we also choose to work in a listwise domain becausw.",
            "Since energy is a function of lists, we believe there is the most optimal way to solve this."
        ],
        [
            "This problem.",
            "So to the restaurant.",
            "Knowledge all current methods have these two disadvantages.",
            "Well, the first one is that the energy is not included in the learning.",
            "Objective, so either some approximation or a different function is optimized instead, which we believe is not the best way to solve this problem, since in the end you are interested in maximizing and CG.",
            "The second disadvantage is that higher order document interactions are not explored, so a scoring function is always the function of a single document which causes all the relative constraints to be loss."
        ],
        [
            "So our method aims to solve both of those problems, and here's a brief outline of how we plan to do this, so we'll use a scoring function which depends on both individual and pairwise potentials, and the pairwise potentials will allow us to enforce the relative constraints at the best time.",
            "So then, once we have the scoring function, we can get this distribution over all possible document rankings and use this distribution to get the expectation over.",
            "I objective function, which in this case will be NCG.",
            "Once we have this expectation, we can maximize the respect to the scoring function.",
            "So now."
        ],
        [
            "In more detail.",
            "So here's the scoring function, and it consists of the two potential.",
            "The individual potential fire and the pairwise potential size, so this pairwise potential allows us to enforce these.",
            "Learn 2nd order constraints at inference time.",
            "In our experiments, show that adding this potential is useful, which means that this is relative constraints.",
            "I useful for ranking.",
            "Now, once we have the score, the scoring function, we can."
        ],
        [
            "Get the scores from the scoring function to a sort of dark documents and we can define the compatibility between the scores an any ranking through this energy function, which is just a mean of the differences between ranks and scores.",
            "So here G is any sign preserving function, an M is the total number of documents in D, so in this forum this an energy is the lack of compatibility between R&S.",
            "And we can see this if we consider two documents DJ&DDK.",
            "So if the J is ranked higher than decay in R, then.",
            "RJ or rank of Document J is going to be higher than RK and their difference will be greater than zero.",
            "If this is consistent with the score is then SJ minus SK will be less than then zero, and if it's not consistent and SG MSS key will be greater than zero, thus the large values of energy means that the order is given by scores are not compatible with those given by R, and Vice vice versa.",
            "So now once we have this energy.",
            "We can then give the conditional distribution of any ranking given the scores by simply exponentiating an.",
            "Norma."
        ],
        [
            "Rising once we have this conditional distribution, we can get the expectation of the energy under this distribution by simply summing over all rankings and multiplying the energy of each.",
            "Ranking by its conditional probability under the scores.",
            "So this sum is over exponentially.",
            "Many ranking assignments and we cannot compute it.",
            "So instead we use Monte Carlo estimate an instead of summing over all possible rankings.",
            "We will have a ranking samples that are Q and we will estimate this expectation by only summing over rankings that are in this ranking sample set.",
            "So here is given here."
        ],
        [
            "So this objective function allows us to include any CG at any truncation level without any approximations.",
            "It also allows us to optimize any other functions such as map, but there's a problem and the CG is very severe function in that Sgt places all its emphasis only on the top T documents and other potentially useful lower and documents are not considered an.",
            "We can see this, especially if we can.",
            "Kind of consider and G at one where it will only change once the top document change changes.",
            "So to solve this we can combine the expected error message with a less severe function and one of the options is to combine it with KL between the true rank distribution given by the relevance is and the predicted."
        ],
        [
            "This distribution given by the scores.",
            "So here is the scale.",
            "An so simple respect to the rank sample set.",
            "Then once we have the scale, the final objective becomes the expected energy minus the KL scaled by this Lambda pile parameter and the gradients of this objective function in respect to F and are smooth and we can use this straightforward gradient descent to maximize this.",
            "So too."
        ],
        [
            "Investigate the usefulness of adding this extra Cal term.",
            "We tried various Lambda parameters and we plotted energy values and we tried a Lambda starting from one which is only an SG-2 Lambda 0, which is only KL and we see that always the best.",
            "The best performances is a combination of the two.",
            "Furthermore, we see that when Lambda is 0, which is only KL, we get this significantly worse performance, which means that.",
            "An SG still contains very useful for ranking information and should not be discarded.",
            "So now more on the ranking sample sets it's since."
        ],
        [
            "Learning to rank the data set generally very, very big.",
            "Who sampling at training time is very expensive, so we don't want to do that, so instead will precompute samples for every query and we will use those fixed samples throughout learning.",
            "So one way of doing that is by swapping.",
            "Relevance is or various documents and then sorting to get the NCG values.",
            "So here is 1 example of such samples, so here we have 100 samples and it's an anesthesia at 5 and we see that by swapping documents with high and low low and low or high and high we can get a full range of energy values from very low.",
            "Here to talk to.",
            "Almost perfect.",
            "This is not the only procedure that can be used, and any other procedure that gives you a full range of NCG values can be applied here.",
            "So now."
        ],
        [
            "More on the experiments that with it, so we use two datasets.",
            "The first data set was the which is limited data set, which 106 Grayson about 16,000 query document pairs.",
            "It also had three relevance is 01 and two were zero men that the document is not relevant and do men there is highly relevant and there are 45 features per document, so the features were the standard IR features such as term, frequency and output of the M25.",
            "King function, the second data set, is the TTD, 2004.",
            "It has 75 queries but 75,000 query document.",
            "There's an had binary relevance is an about 64 features per document, and there are only about 1100 relevant documents, so we subsample the training data to speed up learning.",
            "We took out about 70% of the irrelevant documents from the training data and will have the validation and test sets as is.",
            "Both of their sets also come with with five sixty 2020 splits for training, validation and test.",
            "So we tested all the methods and we come computed the averages of the five test folds.",
            "Finally, both datasets are part of the new learning to rank or liter data set, which is publicly available on the Microsoft website.",
            "So two to test how well our method does."
        ],
        [
            "Used 1 hidden layer neural Nets for individual and pairwise purple potentials.",
            "Input to the pairwise potential was a con con cat nation of features of the two documents and we fix the sample set to 100 as we found that no significant gain could be obtained by going hard and then that and there are also two versions of both rank.",
            "One was without the pairwise.",
            "Potential in which recalled bothering one and one was with the Pirates potential, which is both blank two.",
            "And we did that to investigate the usefulness of the pairwise potentials.",
            "Finally, we compared our methods with this state of the art on both an SG and map so."
        ],
        [
            "Here's what we got, so the first half of the table is.",
            "Assume Med and the second half of the table is the TTD eighty 2004.",
            "So we see that on the which is you you met both rank two does significantly better than all the baselines, and generally does significantly better than bothering one, except for an SG at 4 on the TT 2004 with similar story where bowling two does significantly better than all the baselines except for SG at one.",
            "And thus it is significantly better than both rank one.",
            "So this shows that the pairwise potentials are useful for learning to rank, at least experimentally."
        ],
        [
            "Finally, what we learned from this work.",
            "So the first thing is that optimizing proper objective function does improve performance, but one has to be careful in that if the objective function is very severe, then some formal smoothing might need to be added.",
            "The second thing is that by estimating this distribution over ranking so we can optimize any permutation based objective function, but just maximizing its expectation under this distribution.",
            "And the last thing is that.",
            "These relative constraints seem to be useful for.",
            "Learning to rank and should be further explored.",
            "And this is it.",
            "Thank you."
        ],
        [
            "Time for several questions.",
            "Yep.",
            "I wasn't really sure that you got this distinction between.",
            "That was until this was proved that we made in the very beginning.",
            "You said that you pursue a list at least was approved, but when I look at your objective functions then eventually you also do a pairwise comparison.",
            "Will distinguish listwise from pairwise in that in the pairwise you can update as scoring function with only pair of documents, whether it's with the list wise.",
            "You need a full list of documents to make a single update to the scoring function, so we have those those pairwise contests innocence, but we need a full list of documents to make 1.",
            "One single gradient I sent you know scoring function, so that was the way which is.",
            "Yes.",
            "Your sample size you can get back.",
            "There will be vengeance.",
            "Know especially when the energy is used with males with a very small translocation.",
            "Constant is very hard to get a good sample of it because the sample the NSG value will only change when you change the very top document and so you can't really have many samples in a sense because it's only a function of the very first document.",
            "So you need this extra term.",
            "In, when the energy is used with."
        ],
        [
            "Very large central truncation constant then then maybe a sample said that's quite big, might be enough, but once a very small trend translation or constant you have to use the actual term.",
            "So I had a question which is do you think you could generalize the framework too?",
            "If you had pairwise observations?",
            "Because in a lot of settings you just know this is preferred to this, but you don't necessarily have these absolute score levels, is that?",
            "Yeah, since we only need the difference in the score is then if we have this this pairwise so one is just higher than the other.",
            "I can say at least by how much then we can just add that into scoring function and do it in the same way as we don't need any absolute, just the relative difference.",
            "Barbie you beginning of your talk.",
            "You talked about Listwise methods and one of the disadvantages was that.",
            "The only.",
            "Approximations, for example in DCG, but then later in your talk you you add in this KL.",
            "Peace to the smooth.",
            "You rejected so you can do smooth gradient descent.",
            "So it's a comparison between what you finally end up with and.",
            "These other methods, but within the comparison.",
            "So one of them, I guess the slides are gone, so one of them was a pairwise method and generally at least that I know of at least wise do considerably better than than pairwise method, so this pairwise misclassifications generally do considerably worse.",
            "Ankle sore throat or something like that which is listwise?",
            "Will compare to.",
            "I think one of them was list net which is also at least wise, and the fact that we added and CG seems to help because we can do considerably better than the list net.",
            "So it does seem that energy contains some useful information.",
            "An Lambda Rang also explores this because it also adjusts the bears by this and it's the G factor, so that seemed that has this useful for.",
            "Ranking extra info, but you have to be careful in terms of equal B2 severe optimizing.",
            "New questions."
        ],
        [
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so hi everyone my name is Maximus bogus and today I'm going to present the new learning trying method code both rank and so the joint work with.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first limit on the problem we're trying to solve.",
                    "label": 0
                },
                {
                    "sent": "So in a typical ranking problem we're given end queries as input, where each query has a set of documents D and a set of relevance is L, where documents are query dependent features.",
                    "label": 1
                },
                {
                    "sent": "In RP, an relevance is typically take small integer values, where the higher the value do more relevant.",
                    "label": 1
                },
                {
                    "sent": "Responding document is to the query, so our goal is to sort the documents and out with them in order of relevance to the query.",
                    "label": 0
                },
                {
                    "sent": "So our typical strategy which most people taken, which we will also take, is to create a scoring function which has some tunable parameters an which takes as input sets of documents and outputs the score for every document in that set.",
                    "label": 0
                },
                {
                    "sent": "The scores are then used to sort the documents and output the ranked list.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the order between the scores and the relevance is typically judged by NCG.",
                    "label": 1
                },
                {
                    "sent": "Once we sort the documents according to scores, we get this ranked list and and this is combines this rank list with relevance.",
                    "label": 1
                },
                {
                    "sent": "Levels and is given here, so in this form T is a truncation constant, which means there were only interested in top T of the output documents.",
                    "label": 0
                },
                {
                    "sent": "Real J is the relevance of the document at position J starting from the top and log one plus Jays.",
                    "label": 0
                },
                {
                    "sent": "The discount function, which again emphasizes the outmost importance of the top ranked documents.",
                    "label": 0
                },
                {
                    "sent": "Finally end Qi is the normalizing constant which ensures that energy is always between zero and one an, which allows us to meaningfully compare.",
                    "label": 0
                },
                {
                    "sent": "And this is across different queries with different document sizes, so there's a problem with this function in that the sum is over the documents in sorted order and therefore the gradients of energy with respect to F are not smooth and we can't use any standard gradient descent to learn F. So in order to optimize this, we need a good approximation.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there are a number of previous methods that have been developed to solve this problem, and they can be divided into three categories.",
                    "label": 0
                },
                {
                    "sent": "So the first category is the individual and individual methods map features of documents straight to scores without considering any relative information.",
                    "label": 1
                },
                {
                    "sent": "The second group is the pairwise pairwise methods minimize pairwise misclassification probabilities where the target probability is given by.",
                    "label": 1
                },
                {
                    "sent": "The relevance is in the last group at the list wise.",
                    "label": 0
                },
                {
                    "sent": "Where a list of documents I used as learning instances and some listwise loss function is minimized.",
                    "label": 1
                },
                {
                    "sent": "So we also choose to work in a listwise domain becausw.",
                    "label": 0
                },
                {
                    "sent": "Since energy is a function of lists, we believe there is the most optimal way to solve this.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This problem.",
                    "label": 0
                },
                {
                    "sent": "So to the restaurant.",
                    "label": 0
                },
                {
                    "sent": "Knowledge all current methods have these two disadvantages.",
                    "label": 1
                },
                {
                    "sent": "Well, the first one is that the energy is not included in the learning.",
                    "label": 1
                },
                {
                    "sent": "Objective, so either some approximation or a different function is optimized instead, which we believe is not the best way to solve this problem, since in the end you are interested in maximizing and CG.",
                    "label": 0
                },
                {
                    "sent": "The second disadvantage is that higher order document interactions are not explored, so a scoring function is always the function of a single document which causes all the relative constraints to be loss.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our method aims to solve both of those problems, and here's a brief outline of how we plan to do this, so we'll use a scoring function which depends on both individual and pairwise potentials, and the pairwise potentials will allow us to enforce the relative constraints at the best time.",
                    "label": 0
                },
                {
                    "sent": "So then, once we have the scoring function, we can get this distribution over all possible document rankings and use this distribution to get the expectation over.",
                    "label": 1
                },
                {
                    "sent": "I objective function, which in this case will be NCG.",
                    "label": 1
                },
                {
                    "sent": "Once we have this expectation, we can maximize the respect to the scoring function.",
                    "label": 0
                },
                {
                    "sent": "So now.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In more detail.",
                    "label": 0
                },
                {
                    "sent": "So here's the scoring function, and it consists of the two potential.",
                    "label": 0
                },
                {
                    "sent": "The individual potential fire and the pairwise potential size, so this pairwise potential allows us to enforce these.",
                    "label": 0
                },
                {
                    "sent": "Learn 2nd order constraints at inference time.",
                    "label": 1
                },
                {
                    "sent": "In our experiments, show that adding this potential is useful, which means that this is relative constraints.",
                    "label": 0
                },
                {
                    "sent": "I useful for ranking.",
                    "label": 1
                },
                {
                    "sent": "Now, once we have the score, the scoring function, we can.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Get the scores from the scoring function to a sort of dark documents and we can define the compatibility between the scores an any ranking through this energy function, which is just a mean of the differences between ranks and scores.",
                    "label": 0
                },
                {
                    "sent": "So here G is any sign preserving function, an M is the total number of documents in D, so in this forum this an energy is the lack of compatibility between R&S.",
                    "label": 1
                },
                {
                    "sent": "And we can see this if we consider two documents DJ&DDK.",
                    "label": 0
                },
                {
                    "sent": "So if the J is ranked higher than decay in R, then.",
                    "label": 0
                },
                {
                    "sent": "RJ or rank of Document J is going to be higher than RK and their difference will be greater than zero.",
                    "label": 0
                },
                {
                    "sent": "If this is consistent with the score is then SJ minus SK will be less than then zero, and if it's not consistent and SG MSS key will be greater than zero, thus the large values of energy means that the order is given by scores are not compatible with those given by R, and Vice vice versa.",
                    "label": 0
                },
                {
                    "sent": "So now once we have this energy.",
                    "label": 1
                },
                {
                    "sent": "We can then give the conditional distribution of any ranking given the scores by simply exponentiating an.",
                    "label": 0
                },
                {
                    "sent": "Norma.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Rising once we have this conditional distribution, we can get the expectation of the energy under this distribution by simply summing over all rankings and multiplying the energy of each.",
                    "label": 0
                },
                {
                    "sent": "Ranking by its conditional probability under the scores.",
                    "label": 0
                },
                {
                    "sent": "So this sum is over exponentially.",
                    "label": 1
                },
                {
                    "sent": "Many ranking assignments and we cannot compute it.",
                    "label": 0
                },
                {
                    "sent": "So instead we use Monte Carlo estimate an instead of summing over all possible rankings.",
                    "label": 0
                },
                {
                    "sent": "We will have a ranking samples that are Q and we will estimate this expectation by only summing over rankings that are in this ranking sample set.",
                    "label": 0
                },
                {
                    "sent": "So here is given here.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this objective function allows us to include any CG at any truncation level without any approximations.",
                    "label": 0
                },
                {
                    "sent": "It also allows us to optimize any other functions such as map, but there's a problem and the CG is very severe function in that Sgt places all its emphasis only on the top T documents and other potentially useful lower and documents are not considered an.",
                    "label": 1
                },
                {
                    "sent": "We can see this, especially if we can.",
                    "label": 0
                },
                {
                    "sent": "Kind of consider and G at one where it will only change once the top document change changes.",
                    "label": 0
                },
                {
                    "sent": "So to solve this we can combine the expected error message with a less severe function and one of the options is to combine it with KL between the true rank distribution given by the relevance is and the predicted.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This distribution given by the scores.",
                    "label": 0
                },
                {
                    "sent": "So here is the scale.",
                    "label": 0
                },
                {
                    "sent": "An so simple respect to the rank sample set.",
                    "label": 0
                },
                {
                    "sent": "Then once we have the scale, the final objective becomes the expected energy minus the KL scaled by this Lambda pile parameter and the gradients of this objective function in respect to F and are smooth and we can use this straightforward gradient descent to maximize this.",
                    "label": 1
                },
                {
                    "sent": "So too.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Investigate the usefulness of adding this extra Cal term.",
                    "label": 0
                },
                {
                    "sent": "We tried various Lambda parameters and we plotted energy values and we tried a Lambda starting from one which is only an SG-2 Lambda 0, which is only KL and we see that always the best.",
                    "label": 0
                },
                {
                    "sent": "The best performances is a combination of the two.",
                    "label": 0
                },
                {
                    "sent": "Furthermore, we see that when Lambda is 0, which is only KL, we get this significantly worse performance, which means that.",
                    "label": 0
                },
                {
                    "sent": "An SG still contains very useful for ranking information and should not be discarded.",
                    "label": 0
                },
                {
                    "sent": "So now more on the ranking sample sets it's since.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learning to rank the data set generally very, very big.",
                    "label": 0
                },
                {
                    "sent": "Who sampling at training time is very expensive, so we don't want to do that, so instead will precompute samples for every query and we will use those fixed samples throughout learning.",
                    "label": 0
                },
                {
                    "sent": "So one way of doing that is by swapping.",
                    "label": 0
                },
                {
                    "sent": "Relevance is or various documents and then sorting to get the NCG values.",
                    "label": 0
                },
                {
                    "sent": "So here is 1 example of such samples, so here we have 100 samples and it's an anesthesia at 5 and we see that by swapping documents with high and low low and low or high and high we can get a full range of energy values from very low.",
                    "label": 0
                },
                {
                    "sent": "Here to talk to.",
                    "label": 0
                },
                {
                    "sent": "Almost perfect.",
                    "label": 0
                },
                {
                    "sent": "This is not the only procedure that can be used, and any other procedure that gives you a full range of NCG values can be applied here.",
                    "label": 0
                },
                {
                    "sent": "So now.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "More on the experiments that with it, so we use two datasets.",
                    "label": 0
                },
                {
                    "sent": "The first data set was the which is limited data set, which 106 Grayson about 16,000 query document pairs.",
                    "label": 0
                },
                {
                    "sent": "It also had three relevance is 01 and two were zero men that the document is not relevant and do men there is highly relevant and there are 45 features per document, so the features were the standard IR features such as term, frequency and output of the M25.",
                    "label": 0
                },
                {
                    "sent": "King function, the second data set, is the TTD, 2004.",
                    "label": 0
                },
                {
                    "sent": "It has 75 queries but 75,000 query document.",
                    "label": 1
                },
                {
                    "sent": "There's an had binary relevance is an about 64 features per document, and there are only about 1100 relevant documents, so we subsample the training data to speed up learning.",
                    "label": 1
                },
                {
                    "sent": "We took out about 70% of the irrelevant documents from the training data and will have the validation and test sets as is.",
                    "label": 1
                },
                {
                    "sent": "Both of their sets also come with with five sixty 2020 splits for training, validation and test.",
                    "label": 0
                },
                {
                    "sent": "So we tested all the methods and we come computed the averages of the five test folds.",
                    "label": 1
                },
                {
                    "sent": "Finally, both datasets are part of the new learning to rank or liter data set, which is publicly available on the Microsoft website.",
                    "label": 0
                },
                {
                    "sent": "So two to test how well our method does.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Used 1 hidden layer neural Nets for individual and pairwise purple potentials.",
                    "label": 1
                },
                {
                    "sent": "Input to the pairwise potential was a con con cat nation of features of the two documents and we fix the sample set to 100 as we found that no significant gain could be obtained by going hard and then that and there are also two versions of both rank.",
                    "label": 1
                },
                {
                    "sent": "One was without the pairwise.",
                    "label": 0
                },
                {
                    "sent": "Potential in which recalled bothering one and one was with the Pirates potential, which is both blank two.",
                    "label": 0
                },
                {
                    "sent": "And we did that to investigate the usefulness of the pairwise potentials.",
                    "label": 0
                },
                {
                    "sent": "Finally, we compared our methods with this state of the art on both an SG and map so.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's what we got, so the first half of the table is.",
                    "label": 0
                },
                {
                    "sent": "Assume Med and the second half of the table is the TTD eighty 2004.",
                    "label": 0
                },
                {
                    "sent": "So we see that on the which is you you met both rank two does significantly better than all the baselines, and generally does significantly better than bothering one, except for an SG at 4 on the TT 2004 with similar story where bowling two does significantly better than all the baselines except for SG at one.",
                    "label": 0
                },
                {
                    "sent": "And thus it is significantly better than both rank one.",
                    "label": 0
                },
                {
                    "sent": "So this shows that the pairwise potentials are useful for learning to rank, at least experimentally.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Finally, what we learned from this work.",
                    "label": 0
                },
                {
                    "sent": "So the first thing is that optimizing proper objective function does improve performance, but one has to be careful in that if the objective function is very severe, then some formal smoothing might need to be added.",
                    "label": 1
                },
                {
                    "sent": "The second thing is that by estimating this distribution over ranking so we can optimize any permutation based objective function, but just maximizing its expectation under this distribution.",
                    "label": 0
                },
                {
                    "sent": "And the last thing is that.",
                    "label": 0
                },
                {
                    "sent": "These relative constraints seem to be useful for.",
                    "label": 0
                },
                {
                    "sent": "Learning to rank and should be further explored.",
                    "label": 0
                },
                {
                    "sent": "And this is it.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Time for several questions.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "I wasn't really sure that you got this distinction between.",
                    "label": 0
                },
                {
                    "sent": "That was until this was proved that we made in the very beginning.",
                    "label": 0
                },
                {
                    "sent": "You said that you pursue a list at least was approved, but when I look at your objective functions then eventually you also do a pairwise comparison.",
                    "label": 0
                },
                {
                    "sent": "Will distinguish listwise from pairwise in that in the pairwise you can update as scoring function with only pair of documents, whether it's with the list wise.",
                    "label": 0
                },
                {
                    "sent": "You need a full list of documents to make a single update to the scoring function, so we have those those pairwise contests innocence, but we need a full list of documents to make 1.",
                    "label": 0
                },
                {
                    "sent": "One single gradient I sent you know scoring function, so that was the way which is.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Your sample size you can get back.",
                    "label": 0
                },
                {
                    "sent": "There will be vengeance.",
                    "label": 0
                },
                {
                    "sent": "Know especially when the energy is used with males with a very small translocation.",
                    "label": 0
                },
                {
                    "sent": "Constant is very hard to get a good sample of it because the sample the NSG value will only change when you change the very top document and so you can't really have many samples in a sense because it's only a function of the very first document.",
                    "label": 0
                },
                {
                    "sent": "So you need this extra term.",
                    "label": 0
                },
                {
                    "sent": "In, when the energy is used with.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very large central truncation constant then then maybe a sample said that's quite big, might be enough, but once a very small trend translation or constant you have to use the actual term.",
                    "label": 0
                },
                {
                    "sent": "So I had a question which is do you think you could generalize the framework too?",
                    "label": 0
                },
                {
                    "sent": "If you had pairwise observations?",
                    "label": 0
                },
                {
                    "sent": "Because in a lot of settings you just know this is preferred to this, but you don't necessarily have these absolute score levels, is that?",
                    "label": 0
                },
                {
                    "sent": "Yeah, since we only need the difference in the score is then if we have this this pairwise so one is just higher than the other.",
                    "label": 0
                },
                {
                    "sent": "I can say at least by how much then we can just add that into scoring function and do it in the same way as we don't need any absolute, just the relative difference.",
                    "label": 0
                },
                {
                    "sent": "Barbie you beginning of your talk.",
                    "label": 0
                },
                {
                    "sent": "You talked about Listwise methods and one of the disadvantages was that.",
                    "label": 0
                },
                {
                    "sent": "The only.",
                    "label": 0
                },
                {
                    "sent": "Approximations, for example in DCG, but then later in your talk you you add in this KL.",
                    "label": 0
                },
                {
                    "sent": "Peace to the smooth.",
                    "label": 0
                },
                {
                    "sent": "You rejected so you can do smooth gradient descent.",
                    "label": 0
                },
                {
                    "sent": "So it's a comparison between what you finally end up with and.",
                    "label": 0
                },
                {
                    "sent": "These other methods, but within the comparison.",
                    "label": 0
                },
                {
                    "sent": "So one of them, I guess the slides are gone, so one of them was a pairwise method and generally at least that I know of at least wise do considerably better than than pairwise method, so this pairwise misclassifications generally do considerably worse.",
                    "label": 0
                },
                {
                    "sent": "Ankle sore throat or something like that which is listwise?",
                    "label": 0
                },
                {
                    "sent": "Will compare to.",
                    "label": 0
                },
                {
                    "sent": "I think one of them was list net which is also at least wise, and the fact that we added and CG seems to help because we can do considerably better than the list net.",
                    "label": 0
                },
                {
                    "sent": "So it does seem that energy contains some useful information.",
                    "label": 0
                },
                {
                    "sent": "An Lambda Rang also explores this because it also adjusts the bears by this and it's the G factor, so that seemed that has this useful for.",
                    "label": 0
                },
                {
                    "sent": "Ranking extra info, but you have to be careful in terms of equal B2 severe optimizing.",
                    "label": 0
                },
                {
                    "sent": "New questions.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}