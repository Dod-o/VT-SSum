{
    "id": "ggunprgupdriok7ycn7fl2wihd5vi255",
    "title": "Learning generative texture models with extended Fields-of-Experts",
    "info": {
        "author": [
            "Nicolas Heess, University of Edinburgh"
        ],
        "published": "Dec. 1, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Computer Vision",
            "Top->Computer Science->Image Analysis"
        ]
    },
    "url": "http://videolectures.net/bmvc09_heess_lgtm/",
    "segmentation": [
        [
            "My talk is entitled Learning generative texture models with extended field of experts.",
            "My intro."
        ],
        [
            "This is really generated, that is, probabilistic models of natural image structure and how to learn them in an unsupervised way from data.",
            "Here I'm going to look at a particular class of probabilistic models.",
            "The field of expert framework proposed by Rotten Black in 2005 and this is the field of experts have a lot of interesting properties with when it comes to modeling natural image structure.",
            "It's a continuous or continuous valued high order MRF.",
            "That is fully parametric in all parameters can be learned from data the FOV has originally been introduced as a model for generic image structure and applied for instance to image denoising or simple inpainting tasks.",
            "Here ever I'm going to look at it as a model for specific image reduction, in particular image texture.",
            "There are two reasons why I'm interested in modeling textures.",
            "The first is that texture is obviously an important aspect of natural images, and if we want to model natural images well, then we better get texture, right?",
            "So, considering for instance, the image on the left, then we can probably describe that well by describing the textures in the individual regions and then composing them together to obtain the full image.",
            "The other reason why I'm interested in this task is that texture synthesis might tell us something about what kind of structure and model can and cannot learn, so it's an interesting."
        ],
        [
            "Best case.",
            "Now basically in a natural what I'm going to do is I'm going to focus on learning generative models of textures, such as the one here.",
            "Abroad adds texture and I'm going to demonstrate that if we use the FOE with its standard formulation."
        ],
        [
            "Standard formulation, although it's apparently good model for generic image structure, we get very poor results.",
            "And then going to disk."
        ],
        [
            "Drive an extension that if it's trending the right way, it produces much better samples or learn Spanish.",
            "Much better models which can in some cases be equivalent to what we can achieve with nonparametric methods.",
            "So the outline is as follows.",
            "I'll start by introducing the field of experts."
        ],
        [
            "And then describe the extension and what the difference between these two is an.",
            "I'm going to evaluate this extended formulation on two tasks, both involving basically learning generative texture models and compare it to the standard model and the non parametric method and I'll conclude with a discussion."
        ],
        [
            "OK, so the field of expert is a high order MRF with potential is defined in terms of the responses of linear filters.",
            "So if we focus on the simplest case, 1st and fo E with a single expert then we can write the density over our image X in terms of a product of creeks where we have one Creek centered at each pixel and the click potential here is defined in terms of a response of a linear filter at that pixel.",
            "Here's the dot product between the filter vector and the.",
            "Image Patch centered at Pixely, which is transformed by the expert nonlinearity 5.",
            "Anne.",
            "So this Murph is obviously homogeneous and the click size is effectively determined by the support of the filter.",
            "Now, in the general case, we certainly don't have only a single filter or a single expert, but M experts.",
            "So the density in this case is simply again at each pixel we have the product basically of M different experts, where each expert is defined in terms of its filter and the expert nonlinearity.",
            "Now.",
            "I'm."
        ],
        [
            "One of the interesting properties of the field of expert is that it can be fully learned from data that includes, in particular the filter weights, but also the parameters of the expert non linearity.",
            "Standard formulation rough in black use the student T the simplified student potential which is shown here.",
            "It has a single parameter we and depending on that scale parameter we get different shapes of that potential.",
            "Using this potential function, we can write the density over the image in the familiar form in terms of an energy function.",
            "This potential function is motivated by the statistics of natural images in particular.",
            "Highly non Gaussian response marginals of linear filters applied to natural images, and it seems to work well if we try to model generic image structure and want to do for instance image denoising.",
            "But it should be noted is that if we use that potential our global density over the image space will always be unimodal and this is potentially a significant limitation Now SMH."
        ],
        [
            "Tentative so this is the original F potential, so now tentatively propose to use a somewhat modified form which introduces 2 new parameters A&B.",
            "These two parameters bias alot of flexibility, in particular depending on the choice of parameter A we can obtain.",
            "Unimodal for the red or the pink curve here potentials or bimodal ones?",
            "If a is negative, so this is the red and the green and the blue curve here in the background.",
            "This is just the student potential for comparison.",
            "Now.",
            "So we add two parameters, but this buys us a lot in terms of the global density parameters for experts.",
            "But this by the bodies as a lot in terms of the global density over the image that this model can describe.",
            "And to illustrate this, let's."
        ],
        [
            "Focus on the most simple case where exr images only two dimensional with that don't need to worry about translation invariants."
        ],
        [
            "So this is our image here 2 dimensional and basically each.",
            "In the two dimensional case, each filter of the filter off an expert corresponds to the direction the X One X2 plane and its expert nonlinearity effectively constrains the density in the direction of that filter according."
        ],
        [
            "Adding to that nonlinearity and by multiplying together."
        ],
        [
            "Of these experts, we can shape our global density flexibly.",
            "However, as you probably see, no matter how many experts we add."
        ],
        [
            "It will always be unimodal.",
            "Picture is very different for the bimodal.",
            "In the bimodal potential.",
            "In this case, even with only two experts, we can obtain rather complex and especially multimodal density over the image."
        ],
        [
            "OK, so this seems to be crucial for getting good texture models and what I'm going to do in the rest of the talk is I'm going to show basically results that hopefully demonstrate this empirically.",
            "Specifically, I'm going to learn generative models of a set of products and two other textures.",
            "They were chosen to include basically a set of more regular textures, and then another set of more irregular and potentially more complicated to learn textures.",
            "And basically.",
            "There are two tasks that."
        ],
        [
            "Going to look at it texture synthesis task where we train models on these for these textures and then draw samples from them compared to the originals and in texture inpainting task in which would basically require the trained models to fill in a hole in the original texture image.",
            "I'm going to use that to compare the model to a non parametric method by efforts and I'm also going to consider as a baseline model the Gaussian FOV which has a similar form to the original FOV, but it's analytically tractable.",
            "An much simpler.",
            "But we know that it's not a good model of image structure.",
            "Yeah, so the models in this task in this in the rest of the talk have either 9 or 15 experts.",
            "The filter size is always 7 * 7.",
            "For each texture, we train all the MoD."
        ],
        [
            "It's on the same data set.",
            "OK, so I don't want to say too much about learning here.",
            "I'm happy to come back to this at the end of the task talk, sorry.",
            "But just one thing.",
            "So the problem with learning FOS is that we can't.",
            "We can derive the gradient, but we can't compute it exactly because it involves an expectation over the model distribution, which is, which is intractable.",
            "One of the standard ways of approximating the likelihood gradient is contrastive divergent, which is also used both in black.",
            "But this doesn't work for the model that we're considering here.",
            "As an alternative, we use a somewhat more sophisticated method proposed by Telemann in 2008 in the context of learning restricted Boltzmann machine and the important thing is that this method seems to fare much better when it comes to proxy Banting multimodal distributions with this crucial for learning good by FOE models.",
            "OK, coming to the."
        ],
        [
            "Results so again we have a set of textures.",
            "We train generative models or basic models on these textures, and we sample from them.",
            "The top row shows parts of the original texture image and the 2nd and 3rd row shows samples drawn from the models trained on these textures.",
            "And as we can see, neither the FOV nor the GF, we captured the relevant structure.",
            "In particularly FOV doesn't seem to do any better than the GSO.",
            "Although the latter is much simpler."
        ],
        [
            "The picture is very different for the buyer.",
            "For me it learns good models of.",
            "And all the four textures, and in some cases the samples are actually quite hard to distinguish from the originals.",
            "Now, obviously sampling is a stochastic process, so we didn't."
        ],
        [
            "They're only like 1 sample from each model, but many of them and then used some normalized cross correlation based similarity score to compare these samples with the original textures.",
            "Details are in the paper, but basically what we find is that for all for all textures on average the similarity is much higher for the by FOV than it is for the other two models, and also the variance is considerably smaller, so these are the results for the four different textures, and the biophilia is the.",
            "The green one.",
            "Now an incur."
        ],
        [
            "Sing the IT doesn't work only for the.",
            "For more regular and S, presumably more simpler textures, but also for the somewhat more irregular ones we can get pretty good results.",
            "Anne."
        ],
        [
            "It's also interesting to note that all the Fe models do actually learn a parameters that are smaller than zero or have at least some experts that have a parameter smaller than zero, indicating that some of these expert nonlinearities do actually become bimodal.",
            "OK."
        ],
        [
            "To compare the results also to a non parametric method which typically do better than the probabilistic ones, we considered also a texture inpainting task where basically we have parts of the original texture image and we cut a 50 * 50 pixel hold into that image and require the model to fill that in an we use the buyer foie and also the method proposed by efforts in lung."
        ],
        [
            "We use efforts in this method, then unsurprisingly, we get pretty good results.",
            "Then expect anything else perhaps."
        ],
        [
            "More surprisingly, the results with the Biophilia are really not any worse than again, we repeated this several times using basically several times for each frame and also using different frames, and on average the by Fe certainly doesn't do any worse.",
            "In fact, slightly better than the other method, but I wouldn't interpret much into that difference.",
            "It doesn't seem to do any worse."
        ],
        [
            "OK, so let me just summarize So what I've I've been looking at the FOV model as a generative model for specific image structure, in particular textures and the standard formulation of the foie seems to be quite limited in this case, although it's a good generative model for generic image structure, or at least it does well in denoising and inpainting costs, using somewhat more flexible expert nonlinearity, we obtain a much more powerful model.",
            "It has considerably more flexibility with respect to what kind of global density it can learn.",
            "It does very well on these texture synthesis task with performance, at least for the texture is considered equivalent to a non parametric method, but the difference is that the description is really compact and it's just a few small filters and more importantly a well defined probabilistic model and we can use that probabilistic model for instance as a component of a more comprehensive model of natural images.",
            "One thing we are currently looking at from the model for that can deal with multiple differently textured image regions that could then for instance also be applied to.",
            "A segmentation task.",
            "Finally, it's interesting to note that the results are not inconsistent with the good performance of the FOE S8 generic model as a model for generic image structure, because when we train the FOV on natural images, what it seems to be doing is primarily model that.",
            "It seems that it's primarily modeling piecewise smoothness.",
            "As has been pointed out, but two other papers, and to model that we probably don't need this multimodality, but the unit model PDF works just fine.",
            "If we want to model however, more interesting stuff, then we do actually need something that allows us to very selectively assign probability mass to those spaces in those areas in image space, they do actually correspond to valid texture images."
        ],
        [
            "Thanks.",
            "Right, so basically the evaluation very slightly between the two tasks for the synthesis tasks.",
            "It it's certainly true out what you're saying and what we did there is, we basically correlated the site.",
            "Build a smaller small Patch.",
            "Have a slide here.",
            "A small Patch from the model and correlated that with the whole texture image and took the maximum over the image for the basically for the inpainting task, where we basically have constraints as to how the texture the generated texture should be aligned with respect to the.",
            "With respect to the boundary, right?",
            "Because these constraints are given by the boundary, we just took the.",
            "The normalized cross correlation at that particular so without any shifts or something.",
            "I'm not claiming this is the perfect method for evaluating this, really.",
            "What one would like to do is, but it is to compute the log likelihood, which is unfortunately rather difficult for these undirected models.",
            "It was mainly to give us a.",
            "Some rough measure as to what the.",
            "As to what the quality of a large number of samples is on average, OK, it can't.",
            "It can't copy the original training sample in this case, and the filters are much smaller and I can generate basically images of arbitrary size from that model.",
            "So in this case it can't copy, just it's not a pet Patch based method.",
            "Basically, where you have filters that fill the full Patch and you could potentially just do some sort of autoencoder that reproduces the training data, the filters are considerably smaller than.",
            "Basically, the image that I can generate from that there's something I'm still looking at, so they are very sparse, typically sparser than the filters that you get with the regular.",
            "When you train this definitely on natural images, The thing is that I did some experiment experiments with basically 1 dimensional toy data like square wave patterns or things like that.",
            "In this case, if you choose the number of filters right, so for instance, or square wave, you can construct basically filters that.",
            "Give you the right thing and then if you use the right number of filters, the model actually learns those filters and they can be kind of interpreted in some sense.",
            "Or you see why they work together with the corresponding nonlinearities.",
            "So I would expect that even.",
            "If they might be interpretable in principle without choosing the number of filters right, they are unlikely to be like obviously interpretable is that.",
            "This makes sense, but yeah, it's certainly interesting, especially the question whether you could potentially design these 88 priority instead of learning them.",
            "Probably prefer to do it in terms of some latent variables that can, for instance introduce some non stationarity.",
            "Into the image, but I I don't think so.",
            "Basically the bottom line I think at the moment is that if you make what the model is supposed to learn to complicate it, and especially if there are strong nonstationarity's in the image, it's not going to do well.",
            "And this might be a learning or an optimization problem.",
            "So I wouldn't want to do it with a preprocessing step.",
            "I would like to model it basically, although I've got some ideas how to do that, perhaps to do texture gradients or things like that but?",
            "I haven't really looked into those yet."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My talk is entitled Learning generative texture models with extended field of experts.",
                    "label": 0
                },
                {
                    "sent": "My intro.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is really generated, that is, probabilistic models of natural image structure and how to learn them in an unsupervised way from data.",
                    "label": 1
                },
                {
                    "sent": "Here I'm going to look at a particular class of probabilistic models.",
                    "label": 1
                },
                {
                    "sent": "The field of expert framework proposed by Rotten Black in 2005 and this is the field of experts have a lot of interesting properties with when it comes to modeling natural image structure.",
                    "label": 0
                },
                {
                    "sent": "It's a continuous or continuous valued high order MRF.",
                    "label": 0
                },
                {
                    "sent": "That is fully parametric in all parameters can be learned from data the FOV has originally been introduced as a model for generic image structure and applied for instance to image denoising or simple inpainting tasks.",
                    "label": 1
                },
                {
                    "sent": "Here ever I'm going to look at it as a model for specific image reduction, in particular image texture.",
                    "label": 0
                },
                {
                    "sent": "There are two reasons why I'm interested in modeling textures.",
                    "label": 1
                },
                {
                    "sent": "The first is that texture is obviously an important aspect of natural images, and if we want to model natural images well, then we better get texture, right?",
                    "label": 0
                },
                {
                    "sent": "So, considering for instance, the image on the left, then we can probably describe that well by describing the textures in the individual regions and then composing them together to obtain the full image.",
                    "label": 0
                },
                {
                    "sent": "The other reason why I'm interested in this task is that texture synthesis might tell us something about what kind of structure and model can and cannot learn, so it's an interesting.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Best case.",
                    "label": 0
                },
                {
                    "sent": "Now basically in a natural what I'm going to do is I'm going to focus on learning generative models of textures, such as the one here.",
                    "label": 1
                },
                {
                    "sent": "Abroad adds texture and I'm going to demonstrate that if we use the FOE with its standard formulation.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Standard formulation, although it's apparently good model for generic image structure, we get very poor results.",
                    "label": 0
                },
                {
                    "sent": "And then going to disk.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Drive an extension that if it's trending the right way, it produces much better samples or learn Spanish.",
                    "label": 0
                },
                {
                    "sent": "Much better models which can in some cases be equivalent to what we can achieve with nonparametric methods.",
                    "label": 0
                },
                {
                    "sent": "So the outline is as follows.",
                    "label": 0
                },
                {
                    "sent": "I'll start by introducing the field of experts.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then describe the extension and what the difference between these two is an.",
                    "label": 0
                },
                {
                    "sent": "I'm going to evaluate this extended formulation on two tasks, both involving basically learning generative texture models and compare it to the standard model and the non parametric method and I'll conclude with a discussion.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the field of expert is a high order MRF with potential is defined in terms of the responses of linear filters.",
                    "label": 1
                },
                {
                    "sent": "So if we focus on the simplest case, 1st and fo E with a single expert then we can write the density over our image X in terms of a product of creeks where we have one Creek centered at each pixel and the click potential here is defined in terms of a response of a linear filter at that pixel.",
                    "label": 0
                },
                {
                    "sent": "Here's the dot product between the filter vector and the.",
                    "label": 1
                },
                {
                    "sent": "Image Patch centered at Pixely, which is transformed by the expert nonlinearity 5.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So this Murph is obviously homogeneous and the click size is effectively determined by the support of the filter.",
                    "label": 0
                },
                {
                    "sent": "Now, in the general case, we certainly don't have only a single filter or a single expert, but M experts.",
                    "label": 0
                },
                {
                    "sent": "So the density in this case is simply again at each pixel we have the product basically of M different experts, where each expert is defined in terms of its filter and the expert nonlinearity.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One of the interesting properties of the field of expert is that it can be fully learned from data that includes, in particular the filter weights, but also the parameters of the expert non linearity.",
                    "label": 0
                },
                {
                    "sent": "Standard formulation rough in black use the student T the simplified student potential which is shown here.",
                    "label": 0
                },
                {
                    "sent": "It has a single parameter we and depending on that scale parameter we get different shapes of that potential.",
                    "label": 0
                },
                {
                    "sent": "Using this potential function, we can write the density over the image in the familiar form in terms of an energy function.",
                    "label": 0
                },
                {
                    "sent": "This potential function is motivated by the statistics of natural images in particular.",
                    "label": 0
                },
                {
                    "sent": "Highly non Gaussian response marginals of linear filters applied to natural images, and it seems to work well if we try to model generic image structure and want to do for instance image denoising.",
                    "label": 0
                },
                {
                    "sent": "But it should be noted is that if we use that potential our global density over the image space will always be unimodal and this is potentially a significant limitation Now SMH.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tentative so this is the original F potential, so now tentatively propose to use a somewhat modified form which introduces 2 new parameters A&B.",
                    "label": 0
                },
                {
                    "sent": "These two parameters bias alot of flexibility, in particular depending on the choice of parameter A we can obtain.",
                    "label": 0
                },
                {
                    "sent": "Unimodal for the red or the pink curve here potentials or bimodal ones?",
                    "label": 0
                },
                {
                    "sent": "If a is negative, so this is the red and the green and the blue curve here in the background.",
                    "label": 0
                },
                {
                    "sent": "This is just the student potential for comparison.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "So we add two parameters, but this buys us a lot in terms of the global density parameters for experts.",
                    "label": 0
                },
                {
                    "sent": "But this by the bodies as a lot in terms of the global density over the image that this model can describe.",
                    "label": 0
                },
                {
                    "sent": "And to illustrate this, let's.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Focus on the most simple case where exr images only two dimensional with that don't need to worry about translation invariants.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is our image here 2 dimensional and basically each.",
                    "label": 0
                },
                {
                    "sent": "In the two dimensional case, each filter of the filter off an expert corresponds to the direction the X One X2 plane and its expert nonlinearity effectively constrains the density in the direction of that filter according.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Adding to that nonlinearity and by multiplying together.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of these experts, we can shape our global density flexibly.",
                    "label": 0
                },
                {
                    "sent": "However, as you probably see, no matter how many experts we add.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It will always be unimodal.",
                    "label": 0
                },
                {
                    "sent": "Picture is very different for the bimodal.",
                    "label": 0
                },
                {
                    "sent": "In the bimodal potential.",
                    "label": 0
                },
                {
                    "sent": "In this case, even with only two experts, we can obtain rather complex and especially multimodal density over the image.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this seems to be crucial for getting good texture models and what I'm going to do in the rest of the talk is I'm going to show basically results that hopefully demonstrate this empirically.",
                    "label": 0
                },
                {
                    "sent": "Specifically, I'm going to learn generative models of a set of products and two other textures.",
                    "label": 0
                },
                {
                    "sent": "They were chosen to include basically a set of more regular textures, and then another set of more irregular and potentially more complicated to learn textures.",
                    "label": 0
                },
                {
                    "sent": "And basically.",
                    "label": 0
                },
                {
                    "sent": "There are two tasks that.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Going to look at it texture synthesis task where we train models on these for these textures and then draw samples from them compared to the originals and in texture inpainting task in which would basically require the trained models to fill in a hole in the original texture image.",
                    "label": 1
                },
                {
                    "sent": "I'm going to use that to compare the model to a non parametric method by efforts and I'm also going to consider as a baseline model the Gaussian FOV which has a similar form to the original FOV, but it's analytically tractable.",
                    "label": 0
                },
                {
                    "sent": "An much simpler.",
                    "label": 0
                },
                {
                    "sent": "But we know that it's not a good model of image structure.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so the models in this task in this in the rest of the talk have either 9 or 15 experts.",
                    "label": 0
                },
                {
                    "sent": "The filter size is always 7 * 7.",
                    "label": 1
                },
                {
                    "sent": "For each texture, we train all the MoD.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's on the same data set.",
                    "label": 0
                },
                {
                    "sent": "OK, so I don't want to say too much about learning here.",
                    "label": 0
                },
                {
                    "sent": "I'm happy to come back to this at the end of the task talk, sorry.",
                    "label": 1
                },
                {
                    "sent": "But just one thing.",
                    "label": 0
                },
                {
                    "sent": "So the problem with learning FOS is that we can't.",
                    "label": 1
                },
                {
                    "sent": "We can derive the gradient, but we can't compute it exactly because it involves an expectation over the model distribution, which is, which is intractable.",
                    "label": 1
                },
                {
                    "sent": "One of the standard ways of approximating the likelihood gradient is contrastive divergent, which is also used both in black.",
                    "label": 0
                },
                {
                    "sent": "But this doesn't work for the model that we're considering here.",
                    "label": 0
                },
                {
                    "sent": "As an alternative, we use a somewhat more sophisticated method proposed by Telemann in 2008 in the context of learning restricted Boltzmann machine and the important thing is that this method seems to fare much better when it comes to proxy Banting multimodal distributions with this crucial for learning good by FOE models.",
                    "label": 1
                },
                {
                    "sent": "OK, coming to the.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Results so again we have a set of textures.",
                    "label": 0
                },
                {
                    "sent": "We train generative models or basic models on these textures, and we sample from them.",
                    "label": 0
                },
                {
                    "sent": "The top row shows parts of the original texture image and the 2nd and 3rd row shows samples drawn from the models trained on these textures.",
                    "label": 1
                },
                {
                    "sent": "And as we can see, neither the FOV nor the GF, we captured the relevant structure.",
                    "label": 0
                },
                {
                    "sent": "In particularly FOV doesn't seem to do any better than the GSO.",
                    "label": 0
                },
                {
                    "sent": "Although the latter is much simpler.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The picture is very different for the buyer.",
                    "label": 0
                },
                {
                    "sent": "For me it learns good models of.",
                    "label": 0
                },
                {
                    "sent": "And all the four textures, and in some cases the samples are actually quite hard to distinguish from the originals.",
                    "label": 1
                },
                {
                    "sent": "Now, obviously sampling is a stochastic process, so we didn't.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They're only like 1 sample from each model, but many of them and then used some normalized cross correlation based similarity score to compare these samples with the original textures.",
                    "label": 0
                },
                {
                    "sent": "Details are in the paper, but basically what we find is that for all for all textures on average the similarity is much higher for the by FOV than it is for the other two models, and also the variance is considerably smaller, so these are the results for the four different textures, and the biophilia is the.",
                    "label": 0
                },
                {
                    "sent": "The green one.",
                    "label": 0
                },
                {
                    "sent": "Now an incur.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sing the IT doesn't work only for the.",
                    "label": 0
                },
                {
                    "sent": "For more regular and S, presumably more simpler textures, but also for the somewhat more irregular ones we can get pretty good results.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's also interesting to note that all the Fe models do actually learn a parameters that are smaller than zero or have at least some experts that have a parameter smaller than zero, indicating that some of these expert nonlinearities do actually become bimodal.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To compare the results also to a non parametric method which typically do better than the probabilistic ones, we considered also a texture inpainting task where basically we have parts of the original texture image and we cut a 50 * 50 pixel hold into that image and require the model to fill that in an we use the buyer foie and also the method proposed by efforts in lung.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We use efforts in this method, then unsurprisingly, we get pretty good results.",
                    "label": 0
                },
                {
                    "sent": "Then expect anything else perhaps.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More surprisingly, the results with the Biophilia are really not any worse than again, we repeated this several times using basically several times for each frame and also using different frames, and on average the by Fe certainly doesn't do any worse.",
                    "label": 0
                },
                {
                    "sent": "In fact, slightly better than the other method, but I wouldn't interpret much into that difference.",
                    "label": 0
                },
                {
                    "sent": "It doesn't seem to do any worse.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let me just summarize So what I've I've been looking at the FOV model as a generative model for specific image structure, in particular textures and the standard formulation of the foie seems to be quite limited in this case, although it's a good generative model for generic image structure, or at least it does well in denoising and inpainting costs, using somewhat more flexible expert nonlinearity, we obtain a much more powerful model.",
                    "label": 0
                },
                {
                    "sent": "It has considerably more flexibility with respect to what kind of global density it can learn.",
                    "label": 0
                },
                {
                    "sent": "It does very well on these texture synthesis task with performance, at least for the texture is considered equivalent to a non parametric method, but the difference is that the description is really compact and it's just a few small filters and more importantly a well defined probabilistic model and we can use that probabilistic model for instance as a component of a more comprehensive model of natural images.",
                    "label": 1
                },
                {
                    "sent": "One thing we are currently looking at from the model for that can deal with multiple differently textured image regions that could then for instance also be applied to.",
                    "label": 0
                },
                {
                    "sent": "A segmentation task.",
                    "label": 1
                },
                {
                    "sent": "Finally, it's interesting to note that the results are not inconsistent with the good performance of the FOE S8 generic model as a model for generic image structure, because when we train the FOV on natural images, what it seems to be doing is primarily model that.",
                    "label": 1
                },
                {
                    "sent": "It seems that it's primarily modeling piecewise smoothness.",
                    "label": 0
                },
                {
                    "sent": "As has been pointed out, but two other papers, and to model that we probably don't need this multimodality, but the unit model PDF works just fine.",
                    "label": 0
                },
                {
                    "sent": "If we want to model however, more interesting stuff, then we do actually need something that allows us to very selectively assign probability mass to those spaces in those areas in image space, they do actually correspond to valid texture images.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "Right, so basically the evaluation very slightly between the two tasks for the synthesis tasks.",
                    "label": 0
                },
                {
                    "sent": "It it's certainly true out what you're saying and what we did there is, we basically correlated the site.",
                    "label": 0
                },
                {
                    "sent": "Build a smaller small Patch.",
                    "label": 0
                },
                {
                    "sent": "Have a slide here.",
                    "label": 0
                },
                {
                    "sent": "A small Patch from the model and correlated that with the whole texture image and took the maximum over the image for the basically for the inpainting task, where we basically have constraints as to how the texture the generated texture should be aligned with respect to the.",
                    "label": 0
                },
                {
                    "sent": "With respect to the boundary, right?",
                    "label": 0
                },
                {
                    "sent": "Because these constraints are given by the boundary, we just took the.",
                    "label": 0
                },
                {
                    "sent": "The normalized cross correlation at that particular so without any shifts or something.",
                    "label": 0
                },
                {
                    "sent": "I'm not claiming this is the perfect method for evaluating this, really.",
                    "label": 0
                },
                {
                    "sent": "What one would like to do is, but it is to compute the log likelihood, which is unfortunately rather difficult for these undirected models.",
                    "label": 0
                },
                {
                    "sent": "It was mainly to give us a.",
                    "label": 0
                },
                {
                    "sent": "Some rough measure as to what the.",
                    "label": 0
                },
                {
                    "sent": "As to what the quality of a large number of samples is on average, OK, it can't.",
                    "label": 0
                },
                {
                    "sent": "It can't copy the original training sample in this case, and the filters are much smaller and I can generate basically images of arbitrary size from that model.",
                    "label": 0
                },
                {
                    "sent": "So in this case it can't copy, just it's not a pet Patch based method.",
                    "label": 0
                },
                {
                    "sent": "Basically, where you have filters that fill the full Patch and you could potentially just do some sort of autoencoder that reproduces the training data, the filters are considerably smaller than.",
                    "label": 0
                },
                {
                    "sent": "Basically, the image that I can generate from that there's something I'm still looking at, so they are very sparse, typically sparser than the filters that you get with the regular.",
                    "label": 0
                },
                {
                    "sent": "When you train this definitely on natural images, The thing is that I did some experiment experiments with basically 1 dimensional toy data like square wave patterns or things like that.",
                    "label": 0
                },
                {
                    "sent": "In this case, if you choose the number of filters right, so for instance, or square wave, you can construct basically filters that.",
                    "label": 0
                },
                {
                    "sent": "Give you the right thing and then if you use the right number of filters, the model actually learns those filters and they can be kind of interpreted in some sense.",
                    "label": 0
                },
                {
                    "sent": "Or you see why they work together with the corresponding nonlinearities.",
                    "label": 0
                },
                {
                    "sent": "So I would expect that even.",
                    "label": 0
                },
                {
                    "sent": "If they might be interpretable in principle without choosing the number of filters right, they are unlikely to be like obviously interpretable is that.",
                    "label": 0
                },
                {
                    "sent": "This makes sense, but yeah, it's certainly interesting, especially the question whether you could potentially design these 88 priority instead of learning them.",
                    "label": 0
                },
                {
                    "sent": "Probably prefer to do it in terms of some latent variables that can, for instance introduce some non stationarity.",
                    "label": 0
                },
                {
                    "sent": "Into the image, but I I don't think so.",
                    "label": 0
                },
                {
                    "sent": "Basically the bottom line I think at the moment is that if you make what the model is supposed to learn to complicate it, and especially if there are strong nonstationarity's in the image, it's not going to do well.",
                    "label": 0
                },
                {
                    "sent": "And this might be a learning or an optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So I wouldn't want to do it with a preprocessing step.",
                    "label": 0
                },
                {
                    "sent": "I would like to model it basically, although I've got some ideas how to do that, perhaps to do texture gradients or things like that but?",
                    "label": 0
                },
                {
                    "sent": "I haven't really looked into those yet.",
                    "label": 0
                }
            ]
        }
    }
}