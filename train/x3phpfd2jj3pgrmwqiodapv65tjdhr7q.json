{
    "id": "x3phpfd2jj3pgrmwqiodapv65tjdhr7q",
    "title": "Approaching Textual Entailment with LFG and FrameNet Frames",
    "info": {
        "author": [
            "Aljoscha Burchardt, Saarland University"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "April 2006",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/pcw06_burchardt_atelf/",
    "segmentation": [
        [
            "Temptation that Apple is approaching.",
            "Excellent payment there with energy and frame it Ranger and I'm not sure good character is presented.",
            "With a native tongue, can you hear me?",
            "Somehow hoops so the outline of my talk.",
            "I'm first going to introduce frame semantics because this is the semantic framework we will use.",
            "We were using for our system.",
            "Then I will introduce our system, which we deliberately called a baseline system for approximately textual entailment because we think that this is the beginning of interesting adventure rather than the end or whatever.",
            "You can think of.",
            "Then I will walk you through."
        ],
        [
            "One example from the current artei task and give the results and briefly conclude."
        ],
        [
            "Frame semantics is a lexical semantics classification.",
            "It classifieds predicates in their argument structure.",
            "A-frame represents a prototypical situation.",
            "Like commercial transaction, theft, awareness, and so on, and a set of roles identifies the participants of propositions involved and what's best.",
            "As you see on the bottom of the page, there's this Berkeley Framework Project, and they define 600 frames with almost 10,000 lexicalization's and lots of annotated example sense that we can draw.",
            "From this resource."
        ],
        [
            "Um?",
            "To give you an example, here are some examples sentence that I could categorized as the frame Commerce by a commercial situation for somebody by something.",
            "Here we have the semantic role seller, buyer, goods and money.",
            "And then we have different sentences that then I have highlighted the respective role in the sentence like BMW Bought Rover from British Aerospace or Rover was bought by BMW.",
            "Which financing by MW which acquired Rover and BMW's purchase of Rover.",
            "And here you see.",
            "A number of linguistic normalization are categorized under the same frame.",
            "So here we have different voice.",
            "We have active BMW, bored Rover or the passive Construction Rover was bought by BMW.",
            "We have verbs like acquire we have to now purchase and we have finally.",
            "Trivially different lexicalization's we have by and acquired and purchase and all in with the uniform.",
            "Analysis, like instance of the same frame with the roads in the right way.",
            "So now why do we want to use friend semantics for the artei task when we focus on these lexical semantic classes and the role based argument structures, we can talk about semantic similarity at very high."
        ],
        [
            "Lobby of abstractions.",
            "So we can disregard for a moment they pitfalls of deep semantics such as negation, modality, and quantification, but of course we have to model them.",
            "But so our approach is to model them.",
            "The deeper semantics, aspects on demand.",
            "So we have shown it for the treatment of modality and we could do it for other.",
            "Things as well.",
            "So now the main ingredients of our system are a very fine grained LFG based syntactic analysis.",
            "We use a wide coverage passing system with a high quality probabilistic disambiguation with module.",
            "Then we introduce."
        ],
        [
            "Frame semantic protection.",
            "We extend it with word, net senses and consumer concepts and the final step which has been shown in a number of systems today is we compute structural and semantic overlap of the two graphs we get for the text and for the hypothesis, and OK the larger the overlap, the more probable entailment.",
            "This.",
            "So now.",
            "A little more colorful and a little more deeper.",
            "Look into the components of our systems so we have the linguistic and."
        ],
        [
            "This then we compute the semantic overlap between text and hypothesis, and finally we make a statistical decision as to whether entailment holds or not.",
            "Anne.",
            "As I said, we analyzed texts and hypothesis in terms of LFG pass with a frame semantic and word net enriched projection and when we had those two analysis for Texan hypothesis, then we compute what we call the match graph.",
            "This is we compute the overlapping parts of both.",
            "We record very different aspects, different kinds of matching measures.",
            "There's just different aspects of similarity.",
            "I will talk about this later.",
            "And then we extract features from it on various levels, lexical, syntactic, semantic and overlap measures, and train a model and classify the data.",
            "And I would mainly talk about the upper half today.",
            "When for the first now over, concentrate on the green part.",
            "This is the linguistic components."
        ],
        [
            "Here we go.",
            "So.",
            "We have the exactly passing systems assistant.",
            "Then we have two systems for automatic frame assignment, which are called Fred and Detour, and we have one system called Rosie for automatic role assignments.",
            "These are stand alone and then we use an external Wordnet based word sense disambiguation system, namely the one by Ted Peters, and that has been or Ted Peterson has been mentioned a few times today, and the existing mappings to sumo ontology and from these three inputs.",
            "We generate our central semantic representation.",
            "This is an app structure graph with frame semantics projection.",
            "And then.",
            "Once we have it, we can extend and refine this representation in a rule based manner.",
            "Anne.",
            "We have done it by introducing named entities and location information we get from the LFG passing systems and we introduced this into frame semantic representations.",
            "We did some coreference treatment that we get the variables right if we have.",
            "There like an office or.",
            "Acquisitions and so on.",
            "And finally then we introduced some modality information.",
            "So we look at matching and not matching modality.",
            "I cannot go into the details here, but and at this place you can match much more deep information into the frame.",
            "Analysis and.",
            "Now."
        ],
        [
            "To get a little more into the into an example that you understand what I'm talking about, look at this example pair.",
            "So I chose a short example for presentation, which does not mean that we cannot treat longer examples, so this is the pair 716.",
            "The text is in 1983 ARC equals MC.",
            "He directed his first full-time feature and the hypothesis is akiko's.",
            "Maggie directed the film.",
            "And the entitlement is.",
            "True, right?",
            "So first we."
        ],
        [
            "LFG pass and here we have got the LFG structures and we need not discuss the details.",
            "I think for now so the only thing I have highlighted down there is that the FG tells us that Kaurismaki Orocovis Maggie is a proper name list and information.",
            "We will integrate later.",
            "And then we have."
        ],
        [
            "Our automatic frame annotation systems, and as I said, they are standalone system, so they they're not.",
            "Um?",
            "They do not depend on the LFG pastor.",
            "They work with the Collins pass.",
            "And here for archives Monkey directed his own first full length feature.",
            "We get the center of frame behind the scenes, which is evoked by directed, and this frame is talking about film business.",
            "It has roles like artist, place, time, production and score and the artist role has been assigned to our Kickers Mickey and this is right.",
            "The production role has not been assigned to his first full length feature, which is sad, but that's real life in semantic analysis, so it's a hard task and we will see later how we can repair this in our multi layer approach.",
            "And then for first we get the frame ordinal numbers that should be self explaining.",
            "And sadly, again, for feature we get the frame distinctiveness.",
            "So this is a wrong word sense disambiguation.",
            "So this is the wrong sense of feature, as you can guess.",
            "But again later on we will see that.",
            "Our approach can live with that.",
            "These two frames this frame and roll were generated by the statistical systems and the other two frames were generated by award net.",
            "This is a rule based system we have devised for cases where friend it doesn't have enough coverage because word NET has much more coverage and then we can.",
            "We can assign more frames."
        ],
        [
            "The analysis for the hypothesis akiko's market directly to film is perfect.",
            "Here we have the frame behind the scenes.",
            "Again, the artist is Arkose marquee.",
            "The production is a film and the film itself evokes the frame behind the scene as well.",
            "Because it's also a word from the film business.",
            "And now.",
            "For the first time I will show you the final representation.",
            "How this is integrated with the electric F structure for the hypothesis."
        ],
        [
            "So this is our final representation.",
            "On the left you see the LGF structure, so direct is the main predicate.",
            "The deep object is film and the deep subject is akiko's marquee.",
            "We have the frame behind the scenes and behind the scenes again we've seen before.",
            "And here we have edited the people frame becausw we saw in the LFG structure that.",
            "Is that the one?",
            "Akiko's makiza is a proper name and then I have to hurry up."
        ],
        [
            "So I will not talk about so we have.",
            "Then we match Texan hypothesis and birech regarding different aspects of similarity syntax based semantic base.",
            "We have a weak matching condition where we use word, NET and coreference links to bring things together and now if you look at the.",
            "Analysis For the text and we disregard."
        ],
        [
            "This wrongly assigned frame distinctiveness for a moment.",
            "And here I show you the hypothesis, then we see.",
            "That the upper parts are almost the same except for this in 1983 and feature and film are found to be related by Wordnet, and both are grammatically related.",
            "Being the deep object of direct.",
            "And so we get a perfect match for this example.",
            "Then we extract features and I don't have time to go."
        ],
        [
            "To the syntax, semantic features and how much how large the overlapping clusters are.",
            "And finally.",
            "Our results were fear in the in the middle.",
            "Say we were best at summarization and information retrieval because these are natural tasks for friend semantics.",
            "So we more or less model rather aboutness here than hard factivity.",
            "We have a good approximation for semantic similarity, so our deep LFG syntactic analysis and integrated with the shallow lexical frame semantics and the other resources gives us a good system that talks about semantic similarity.",
            "We need to model more as other groups as you said today, or some groups of a perfect modeling dissimilarity, so we should cooperate in modeling these aspects.",
            "And we have started to integrate deeper modeling, but for the future we have more plans to go further into this direction.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Temptation that Apple is approaching.",
                    "label": 0
                },
                {
                    "sent": "Excellent payment there with energy and frame it Ranger and I'm not sure good character is presented.",
                    "label": 0
                },
                {
                    "sent": "With a native tongue, can you hear me?",
                    "label": 0
                },
                {
                    "sent": "Somehow hoops so the outline of my talk.",
                    "label": 0
                },
                {
                    "sent": "I'm first going to introduce frame semantics because this is the semantic framework we will use.",
                    "label": 0
                },
                {
                    "sent": "We were using for our system.",
                    "label": 0
                },
                {
                    "sent": "Then I will introduce our system, which we deliberately called a baseline system for approximately textual entailment because we think that this is the beginning of interesting adventure rather than the end or whatever.",
                    "label": 0
                },
                {
                    "sent": "You can think of.",
                    "label": 0
                },
                {
                    "sent": "Then I will walk you through.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One example from the current artei task and give the results and briefly conclude.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Frame semantics is a lexical semantics classification.",
                    "label": 0
                },
                {
                    "sent": "It classifieds predicates in their argument structure.",
                    "label": 1
                },
                {
                    "sent": "A-frame represents a prototypical situation.",
                    "label": 1
                },
                {
                    "sent": "Like commercial transaction, theft, awareness, and so on, and a set of roles identifies the participants of propositions involved and what's best.",
                    "label": 1
                },
                {
                    "sent": "As you see on the bottom of the page, there's this Berkeley Framework Project, and they define 600 frames with almost 10,000 lexicalization's and lots of annotated example sense that we can draw.",
                    "label": 0
                },
                {
                    "sent": "From this resource.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "To give you an example, here are some examples sentence that I could categorized as the frame Commerce by a commercial situation for somebody by something.",
                    "label": 0
                },
                {
                    "sent": "Here we have the semantic role seller, buyer, goods and money.",
                    "label": 0
                },
                {
                    "sent": "And then we have different sentences that then I have highlighted the respective role in the sentence like BMW Bought Rover from British Aerospace or Rover was bought by BMW.",
                    "label": 1
                },
                {
                    "sent": "Which financing by MW which acquired Rover and BMW's purchase of Rover.",
                    "label": 0
                },
                {
                    "sent": "And here you see.",
                    "label": 0
                },
                {
                    "sent": "A number of linguistic normalization are categorized under the same frame.",
                    "label": 0
                },
                {
                    "sent": "So here we have different voice.",
                    "label": 0
                },
                {
                    "sent": "We have active BMW, bored Rover or the passive Construction Rover was bought by BMW.",
                    "label": 0
                },
                {
                    "sent": "We have verbs like acquire we have to now purchase and we have finally.",
                    "label": 0
                },
                {
                    "sent": "Trivially different lexicalization's we have by and acquired and purchase and all in with the uniform.",
                    "label": 0
                },
                {
                    "sent": "Analysis, like instance of the same frame with the roads in the right way.",
                    "label": 0
                },
                {
                    "sent": "So now why do we want to use friend semantics for the artei task when we focus on these lexical semantic classes and the role based argument structures, we can talk about semantic similarity at very high.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Lobby of abstractions.",
                    "label": 0
                },
                {
                    "sent": "So we can disregard for a moment they pitfalls of deep semantics such as negation, modality, and quantification, but of course we have to model them.",
                    "label": 1
                },
                {
                    "sent": "But so our approach is to model them.",
                    "label": 1
                },
                {
                    "sent": "The deeper semantics, aspects on demand.",
                    "label": 1
                },
                {
                    "sent": "So we have shown it for the treatment of modality and we could do it for other.",
                    "label": 1
                },
                {
                    "sent": "Things as well.",
                    "label": 0
                },
                {
                    "sent": "So now the main ingredients of our system are a very fine grained LFG based syntactic analysis.",
                    "label": 0
                },
                {
                    "sent": "We use a wide coverage passing system with a high quality probabilistic disambiguation with module.",
                    "label": 0
                },
                {
                    "sent": "Then we introduce.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Frame semantic protection.",
                    "label": 0
                },
                {
                    "sent": "We extend it with word, net senses and consumer concepts and the final step which has been shown in a number of systems today is we compute structural and semantic overlap of the two graphs we get for the text and for the hypothesis, and OK the larger the overlap, the more probable entailment.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "So now.",
                    "label": 0
                },
                {
                    "sent": "A little more colorful and a little more deeper.",
                    "label": 0
                },
                {
                    "sent": "Look into the components of our systems so we have the linguistic and.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This then we compute the semantic overlap between text and hypothesis, and finally we make a statistical decision as to whether entailment holds or not.",
                    "label": 1
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "As I said, we analyzed texts and hypothesis in terms of LFG pass with a frame semantic and word net enriched projection and when we had those two analysis for Texan hypothesis, then we compute what we call the match graph.",
                    "label": 0
                },
                {
                    "sent": "This is we compute the overlapping parts of both.",
                    "label": 0
                },
                {
                    "sent": "We record very different aspects, different kinds of matching measures.",
                    "label": 0
                },
                {
                    "sent": "There's just different aspects of similarity.",
                    "label": 1
                },
                {
                    "sent": "I will talk about this later.",
                    "label": 0
                },
                {
                    "sent": "And then we extract features from it on various levels, lexical, syntactic, semantic and overlap measures, and train a model and classify the data.",
                    "label": 1
                },
                {
                    "sent": "And I would mainly talk about the upper half today.",
                    "label": 0
                },
                {
                    "sent": "When for the first now over, concentrate on the green part.",
                    "label": 0
                },
                {
                    "sent": "This is the linguistic components.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here we go.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We have the exactly passing systems assistant.",
                    "label": 0
                },
                {
                    "sent": "Then we have two systems for automatic frame assignment, which are called Fred and Detour, and we have one system called Rosie for automatic role assignments.",
                    "label": 0
                },
                {
                    "sent": "These are stand alone and then we use an external Wordnet based word sense disambiguation system, namely the one by Ted Peters, and that has been or Ted Peterson has been mentioned a few times today, and the existing mappings to sumo ontology and from these three inputs.",
                    "label": 0
                },
                {
                    "sent": "We generate our central semantic representation.",
                    "label": 0
                },
                {
                    "sent": "This is an app structure graph with frame semantics projection.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "Once we have it, we can extend and refine this representation in a rule based manner.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "We have done it by introducing named entities and location information we get from the LFG passing systems and we introduced this into frame semantic representations.",
                    "label": 0
                },
                {
                    "sent": "We did some coreference treatment that we get the variables right if we have.",
                    "label": 0
                },
                {
                    "sent": "There like an office or.",
                    "label": 0
                },
                {
                    "sent": "Acquisitions and so on.",
                    "label": 0
                },
                {
                    "sent": "And finally then we introduced some modality information.",
                    "label": 0
                },
                {
                    "sent": "So we look at matching and not matching modality.",
                    "label": 0
                },
                {
                    "sent": "I cannot go into the details here, but and at this place you can match much more deep information into the frame.",
                    "label": 0
                },
                {
                    "sent": "Analysis and.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To get a little more into the into an example that you understand what I'm talking about, look at this example pair.",
                    "label": 0
                },
                {
                    "sent": "So I chose a short example for presentation, which does not mean that we cannot treat longer examples, so this is the pair 716.",
                    "label": 0
                },
                {
                    "sent": "The text is in 1983 ARC equals MC.",
                    "label": 1
                },
                {
                    "sent": "He directed his first full-time feature and the hypothesis is akiko's.",
                    "label": 1
                },
                {
                    "sent": "Maggie directed the film.",
                    "label": 0
                },
                {
                    "sent": "And the entitlement is.",
                    "label": 0
                },
                {
                    "sent": "True, right?",
                    "label": 0
                },
                {
                    "sent": "So first we.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "LFG pass and here we have got the LFG structures and we need not discuss the details.",
                    "label": 0
                },
                {
                    "sent": "I think for now so the only thing I have highlighted down there is that the FG tells us that Kaurismaki Orocovis Maggie is a proper name list and information.",
                    "label": 0
                },
                {
                    "sent": "We will integrate later.",
                    "label": 0
                },
                {
                    "sent": "And then we have.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our automatic frame annotation systems, and as I said, they are standalone system, so they they're not.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "They do not depend on the LFG pastor.",
                    "label": 0
                },
                {
                    "sent": "They work with the Collins pass.",
                    "label": 0
                },
                {
                    "sent": "And here for archives Monkey directed his own first full length feature.",
                    "label": 0
                },
                {
                    "sent": "We get the center of frame behind the scenes, which is evoked by directed, and this frame is talking about film business.",
                    "label": 0
                },
                {
                    "sent": "It has roles like artist, place, time, production and score and the artist role has been assigned to our Kickers Mickey and this is right.",
                    "label": 0
                },
                {
                    "sent": "The production role has not been assigned to his first full length feature, which is sad, but that's real life in semantic analysis, so it's a hard task and we will see later how we can repair this in our multi layer approach.",
                    "label": 0
                },
                {
                    "sent": "And then for first we get the frame ordinal numbers that should be self explaining.",
                    "label": 0
                },
                {
                    "sent": "And sadly, again, for feature we get the frame distinctiveness.",
                    "label": 0
                },
                {
                    "sent": "So this is a wrong word sense disambiguation.",
                    "label": 0
                },
                {
                    "sent": "So this is the wrong sense of feature, as you can guess.",
                    "label": 0
                },
                {
                    "sent": "But again later on we will see that.",
                    "label": 0
                },
                {
                    "sent": "Our approach can live with that.",
                    "label": 0
                },
                {
                    "sent": "These two frames this frame and roll were generated by the statistical systems and the other two frames were generated by award net.",
                    "label": 0
                },
                {
                    "sent": "This is a rule based system we have devised for cases where friend it doesn't have enough coverage because word NET has much more coverage and then we can.",
                    "label": 0
                },
                {
                    "sent": "We can assign more frames.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The analysis for the hypothesis akiko's market directly to film is perfect.",
                    "label": 0
                },
                {
                    "sent": "Here we have the frame behind the scenes.",
                    "label": 0
                },
                {
                    "sent": "Again, the artist is Arkose marquee.",
                    "label": 0
                },
                {
                    "sent": "The production is a film and the film itself evokes the frame behind the scene as well.",
                    "label": 1
                },
                {
                    "sent": "Because it's also a word from the film business.",
                    "label": 0
                },
                {
                    "sent": "And now.",
                    "label": 0
                },
                {
                    "sent": "For the first time I will show you the final representation.",
                    "label": 0
                },
                {
                    "sent": "How this is integrated with the electric F structure for the hypothesis.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is our final representation.",
                    "label": 0
                },
                {
                    "sent": "On the left you see the LGF structure, so direct is the main predicate.",
                    "label": 0
                },
                {
                    "sent": "The deep object is film and the deep subject is akiko's marquee.",
                    "label": 0
                },
                {
                    "sent": "We have the frame behind the scenes and behind the scenes again we've seen before.",
                    "label": 0
                },
                {
                    "sent": "And here we have edited the people frame becausw we saw in the LFG structure that.",
                    "label": 0
                },
                {
                    "sent": "Is that the one?",
                    "label": 0
                },
                {
                    "sent": "Akiko's makiza is a proper name and then I have to hurry up.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I will not talk about so we have.",
                    "label": 0
                },
                {
                    "sent": "Then we match Texan hypothesis and birech regarding different aspects of similarity syntax based semantic base.",
                    "label": 1
                },
                {
                    "sent": "We have a weak matching condition where we use word, NET and coreference links to bring things together and now if you look at the.",
                    "label": 0
                },
                {
                    "sent": "Analysis For the text and we disregard.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This wrongly assigned frame distinctiveness for a moment.",
                    "label": 0
                },
                {
                    "sent": "And here I show you the hypothesis, then we see.",
                    "label": 0
                },
                {
                    "sent": "That the upper parts are almost the same except for this in 1983 and feature and film are found to be related by Wordnet, and both are grammatically related.",
                    "label": 0
                },
                {
                    "sent": "Being the deep object of direct.",
                    "label": 0
                },
                {
                    "sent": "And so we get a perfect match for this example.",
                    "label": 0
                },
                {
                    "sent": "Then we extract features and I don't have time to go.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To the syntax, semantic features and how much how large the overlapping clusters are.",
                    "label": 0
                },
                {
                    "sent": "And finally.",
                    "label": 0
                },
                {
                    "sent": "Our results were fear in the in the middle.",
                    "label": 0
                },
                {
                    "sent": "Say we were best at summarization and information retrieval because these are natural tasks for friend semantics.",
                    "label": 0
                },
                {
                    "sent": "So we more or less model rather aboutness here than hard factivity.",
                    "label": 0
                },
                {
                    "sent": "We have a good approximation for semantic similarity, so our deep LFG syntactic analysis and integrated with the shallow lexical frame semantics and the other resources gives us a good system that talks about semantic similarity.",
                    "label": 0
                },
                {
                    "sent": "We need to model more as other groups as you said today, or some groups of a perfect modeling dissimilarity, so we should cooperate in modeling these aspects.",
                    "label": 0
                },
                {
                    "sent": "And we have started to integrate deeper modeling, but for the future we have more plans to go further into this direction.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}