{
    "id": "pjckzuqwespcqlmxz4vyrzdcahyfu42b",
    "title": "Sparse Adaptive Dirichlet-Multinomial-like Processes",
    "info": {
        "author": [
            "Tor Lattimore, DeepMind Technologies Limited"
        ],
        "published": "Aug. 9, 2013",
        "recorded": "June 2013",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/colt2013_lattimore_sparse/",
    "segmentation": [
        [
            "OK, so this is."
        ],
        [
            "Compression problems, so we just compressing sequences of data and we have a problem here because the alphabet size is assumed to be extremely large compared to the length of our sequence.",
            "So you have some very big alphabet, maybe sets of English words and you have some short sequence of those and you want to compress it.",
            "And the goal is to make an online algorithm and here we make some assumptions, which is just the ID and there of course lots of applications and some obvious examples."
        ],
        [
            "OK, so it's based on the Dirichlet multinomial model, which I guess most people know.",
            "So here you have a sort of a frequentist estimator of your predicting the next symbol.",
            "So we have deer of XN plus one equals.",
            "I given some previous history is basically just the frequentist estimate plus some fictitious counts, which is your Alpha I so let's some fictitious counts of the number of times these observed symbol.",
            "I should just add and there are a few choices for this Alpha I so the plus and Katie lots of people will know.",
            "And then there is perks prior, which gives a much much lower weight to unseen symbols, and Haldane is the frequentist estimate.",
            "And then in this work Marcus basically optimized this parameter Alpha.",
            "So how should you choose Gamma Alpha optimally to get the best result?",
            "And here M is the number of symbols that you see.",
            "So this could be much like much, much smaller than the whole alphabet in typically will be."
        ],
        [
            "OK, so the main contribution is a GNU estimator which is very closely related to the original model and it's very simple.",
            "It's online, it's very fast.",
            "It works well in practice for for small, M for medium M and for large M. And it has some nice properties like not occurring symbols in use their redundancy.",
            "So constant sequence has constant relevancy.",
            "OK, and it's very competitive with some slightly slower based methods."
        ],
        [
            "OK, So what is the model?",
            "It's very similar to the display model, so if you have observed simbli already then you assign just the frequentist estimator.",
            "But of course sign some beta value to the unseen symbols.",
            "And if you haven't seen it, then you assign some weight which is proportional debater and these W's are just a prior on the unseen symbols, so typically that would be chosen to be uniform.",
            "There are finitely many symbols.",
            "And it's very similar to the Dursley model.",
            "But instead of having the sum, it's just cases.",
            "So if you haven't seen it then you do something slightly different to what you have seen it.",
            "And the big benefit you get here is it's much easier to analyze."
        ],
        [
            "OK, so how do we measure performance?",
            "Well, it's just redundancy, so the code length is just minus log of the probability and then the redundancy is the difference between the code length that your estimator produces and the code length that the optimal one produces with respect to the empirical estimates.",
            "OK so a cheers.",
            "The entropy of the impaired customer."
        ],
        [
            "OK, and then what Marcus did was just analytically compute the optimal value of beta for for his model.",
            "So here you have this M on 2L N an on M and just some sanity check.",
            "So if M is really large then that means we've seen lots of new symbols.",
            "We expect beta to be really large and it is.",
            "On the other hand, if M is very small then we have feature is very small as well, which is also what we want.",
            "We want to give very low probability to new symbols."
        ],
        [
            "OK, so then he proves a redundancy bound.",
            "And so you can see it consists of three terms and the first one and the third one I read the constant or quite small.",
            "So the interesting term is the middle one.",
            "And the important thing here is this only depends on the symbols that you see.",
            "Yeah, so you only some over these symbols in a, which is the set of symbols that you observe.",
            "Whereas in the right hand side you have the redundancy bound for the KT estimator which depends on D, which we've assumed to be really really large.",
            "So it's very common that.",
            "Marcus is redundancy bound will be much much smaller and not depend on D at all.",
            "OK, and you can extend the continuous or countable alphabets some efforts."
        ],
        [
            "OK, so some slight problem.",
            "So beta depends on M which you don't get to observe until you see the whole sequence is not online.",
            "So if you want it to be online then you do sort of the obvious thing.",
            "So instead of an you have T and instead of M you have the number of symbol times the number of distinct symbols you've seen up to time T. And then to get rid of the problem where T equals.",
            "So you just add 1 to T, so this is quite natural adaptive variable and this works.",
            "You get the same redundancy bounds except for some constants."
        ],
        [
            "And actually, this works so.",
            "This is comparing it to the Katie Estimator and the perks and what you'll see is here on the X axis is the number of total symbols that you observe, and if that's very small than the Katie estimated does very badly because it assigns.",
            "Quite a high probability to observing you symbols, but perks does very well and then going over to the right perks does really badly because there are lots of new symbols and perks.",
            "Doesn't believe this and Katie does well.",
            "But this new estimated as well the whole way across it's it's very good in practice as well.",
            "Thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Compression problems, so we just compressing sequences of data and we have a problem here because the alphabet size is assumed to be extremely large compared to the length of our sequence.",
                    "label": 0
                },
                {
                    "sent": "So you have some very big alphabet, maybe sets of English words and you have some short sequence of those and you want to compress it.",
                    "label": 1
                },
                {
                    "sent": "And the goal is to make an online algorithm and here we make some assumptions, which is just the ID and there of course lots of applications and some obvious examples.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so it's based on the Dirichlet multinomial model, which I guess most people know.",
                    "label": 0
                },
                {
                    "sent": "So here you have a sort of a frequentist estimator of your predicting the next symbol.",
                    "label": 0
                },
                {
                    "sent": "So we have deer of XN plus one equals.",
                    "label": 0
                },
                {
                    "sent": "I given some previous history is basically just the frequentist estimate plus some fictitious counts, which is your Alpha I so let's some fictitious counts of the number of times these observed symbol.",
                    "label": 1
                },
                {
                    "sent": "I should just add and there are a few choices for this Alpha I so the plus and Katie lots of people will know.",
                    "label": 0
                },
                {
                    "sent": "And then there is perks prior, which gives a much much lower weight to unseen symbols, and Haldane is the frequentist estimate.",
                    "label": 0
                },
                {
                    "sent": "And then in this work Marcus basically optimized this parameter Alpha.",
                    "label": 0
                },
                {
                    "sent": "So how should you choose Gamma Alpha optimally to get the best result?",
                    "label": 0
                },
                {
                    "sent": "And here M is the number of symbols that you see.",
                    "label": 1
                },
                {
                    "sent": "So this could be much like much, much smaller than the whole alphabet in typically will be.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the main contribution is a GNU estimator which is very closely related to the original model and it's very simple.",
                    "label": 1
                },
                {
                    "sent": "It's online, it's very fast.",
                    "label": 0
                },
                {
                    "sent": "It works well in practice for for small, M for medium M and for large M. And it has some nice properties like not occurring symbols in use their redundancy.",
                    "label": 1
                },
                {
                    "sent": "So constant sequence has constant relevancy.",
                    "label": 0
                },
                {
                    "sent": "OK, and it's very competitive with some slightly slower based methods.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what is the model?",
                    "label": 0
                },
                {
                    "sent": "It's very similar to the display model, so if you have observed simbli already then you assign just the frequentist estimator.",
                    "label": 0
                },
                {
                    "sent": "But of course sign some beta value to the unseen symbols.",
                    "label": 0
                },
                {
                    "sent": "And if you haven't seen it, then you assign some weight which is proportional debater and these W's are just a prior on the unseen symbols, so typically that would be chosen to be uniform.",
                    "label": 0
                },
                {
                    "sent": "There are finitely many symbols.",
                    "label": 0
                },
                {
                    "sent": "And it's very similar to the Dursley model.",
                    "label": 0
                },
                {
                    "sent": "But instead of having the sum, it's just cases.",
                    "label": 1
                },
                {
                    "sent": "So if you haven't seen it then you do something slightly different to what you have seen it.",
                    "label": 0
                },
                {
                    "sent": "And the big benefit you get here is it's much easier to analyze.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so how do we measure performance?",
                    "label": 0
                },
                {
                    "sent": "Well, it's just redundancy, so the code length is just minus log of the probability and then the redundancy is the difference between the code length that your estimator produces and the code length that the optimal one produces with respect to the empirical estimates.",
                    "label": 0
                },
                {
                    "sent": "OK so a cheers.",
                    "label": 0
                },
                {
                    "sent": "The entropy of the impaired customer.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and then what Marcus did was just analytically compute the optimal value of beta for for his model.",
                    "label": 0
                },
                {
                    "sent": "So here you have this M on 2L N an on M and just some sanity check.",
                    "label": 0
                },
                {
                    "sent": "So if M is really large then that means we've seen lots of new symbols.",
                    "label": 1
                },
                {
                    "sent": "We expect beta to be really large and it is.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if M is very small then we have feature is very small as well, which is also what we want.",
                    "label": 0
                },
                {
                    "sent": "We want to give very low probability to new symbols.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so then he proves a redundancy bound.",
                    "label": 0
                },
                {
                    "sent": "And so you can see it consists of three terms and the first one and the third one I read the constant or quite small.",
                    "label": 0
                },
                {
                    "sent": "So the interesting term is the middle one.",
                    "label": 0
                },
                {
                    "sent": "And the important thing here is this only depends on the symbols that you see.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so you only some over these symbols in a, which is the set of symbols that you observe.",
                    "label": 0
                },
                {
                    "sent": "Whereas in the right hand side you have the redundancy bound for the KT estimator which depends on D, which we've assumed to be really really large.",
                    "label": 0
                },
                {
                    "sent": "So it's very common that.",
                    "label": 0
                },
                {
                    "sent": "Marcus is redundancy bound will be much much smaller and not depend on D at all.",
                    "label": 0
                },
                {
                    "sent": "OK, and you can extend the continuous or countable alphabets some efforts.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so some slight problem.",
                    "label": 0
                },
                {
                    "sent": "So beta depends on M which you don't get to observe until you see the whole sequence is not online.",
                    "label": 1
                },
                {
                    "sent": "So if you want it to be online then you do sort of the obvious thing.",
                    "label": 1
                },
                {
                    "sent": "So instead of an you have T and instead of M you have the number of symbol times the number of distinct symbols you've seen up to time T. And then to get rid of the problem where T equals.",
                    "label": 0
                },
                {
                    "sent": "So you just add 1 to T, so this is quite natural adaptive variable and this works.",
                    "label": 0
                },
                {
                    "sent": "You get the same redundancy bounds except for some constants.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And actually, this works so.",
                    "label": 0
                },
                {
                    "sent": "This is comparing it to the Katie Estimator and the perks and what you'll see is here on the X axis is the number of total symbols that you observe, and if that's very small than the Katie estimated does very badly because it assigns.",
                    "label": 0
                },
                {
                    "sent": "Quite a high probability to observing you symbols, but perks does very well and then going over to the right perks does really badly because there are lots of new symbols and perks.",
                    "label": 0
                },
                {
                    "sent": "Doesn't believe this and Katie does well.",
                    "label": 0
                },
                {
                    "sent": "But this new estimated as well the whole way across it's it's very good in practice as well.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                }
            ]
        }
    }
}