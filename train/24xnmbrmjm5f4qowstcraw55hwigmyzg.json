{
    "id": "24xnmbrmjm5f4qowstcraw55hwigmyzg",
    "title": "Reinforcement learning objectives constrain the cognitive map",
    "info": {
        "author": [
            "Kimberly Stachenfeld, Princeton Neuroscience Institute, Princeton University"
        ],
        "published": "July 28, 2015",
        "recorded": "June 2015",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Artificial Intelligence",
            "Top->Social Sciences->Economics",
            "Top->Social Sciences->Psychology",
            "Top->Medicine->Neuroscience",
            "Top->Technology->Engineering->Electrical Engineering->Control Engineering"
        ]
    },
    "url": "http://videolectures.net/rldm2015_stachenfeld_cognitive_map/",
    "segmentation": [
        [
            "Good afternoon, my name is Kimberly Stack Infeld.",
            "I'm a graduate student in Matt Botvinnik's lab at Princeton and I've been working on this project in collaboration with Matt and with Sam Gershman who's at MIT.",
            "The theme of this talk is reinforces how reinforcement learning objectives constrain the cognitive map.",
            "1st, I'll go over some things neuro scientists know about how space is represented in the brain.",
            "Then I'll talk about the model and the reinforcement.",
            "Learning ideas that we used to inform it.",
            "And at the end I'll present kind of a flurry of simulations and data that we predict and explain with our model."
        ],
        [
            "We know that there exists these cells called place cells in the hippocampus.",
            "And these playstyles have firing that's specific to one location.",
            "So in the figure at the top right we see the animals actual trajectory through a real room in black, and then in red we see the locations where a cell fired, so you can see that it's sort of limited to the range in the bottom of the room.",
            "Fire is also sensitive to other aspects of experience and to reward in the environment, and that's why we're interested in it.",
            "Place cells also show this predicted precession effect where after the animals been trained, cells that are upcoming will start firing in advance of when they're actually reached."
        ],
        [
            "Grid cells are found in enter rhino cortex.",
            "For grid cells, firing is periodic with respect to space, so here you can see this grid cell has multiple firing fields that are arranged in this nice hexagonal tessellation."
        ],
        [
            "So now to go into the reinforcement learning side of it, animals want to maximize value.",
            "The most general objective of navigation under a given policy is therefore to visit states maximizing value.",
            "Here we define the value of a state to be the total discounted expected reward that you'll receive over your future from that state on.",
            "And you can compute that with the infinite sum of the expected reward from your current state on all your future states.",
            "We also include this discount term which is often included in the Bellman equation to encode the intuition that immediate reward matters more than very distantly received reward."
        ],
        [
            "So for a given policy, the value function of a given state can be neatly decomposed into the dot product of two separate terms.",
            "In red we see the expected number of times each state in the environment will be visited from your given starting location.",
            "Is called the successor representation, and we see the reward vector.",
            "So the total expected discounted reward will be the expected discounted number of times you reach each state and the reward you get when you're there.",
            "And this decomposition was introduced by Peter Diane in 1993, and it's called the successor representation because you're representing your state as a statistic over your future states or your successors.",
            "This representation can be learned incrementally with TD learning and because space and reward or independent terms, you can quickly recompute value in cases where the reward is changing at a faster timescale than your policy can be updated.",
            "It's important to note that we're considering the successor representation as a representation under a given static policy.",
            "That is, we assume that the ASR is handed a policy and that it's incorporated in some larger policy learning loop and not learning it in and of itself."
        ],
        [
            "The main thrust of our model is that the hippocampus is a cognitive map is realizing the successor representation.",
            "So to paint a picture of what those equations will mean, let's go through exactly what the spatial representation would look like if it's realized by neurons in the hippocampus.",
            "So we have our little rat friend and he starts out in the middle of this linear track an in his hippocampus.",
            "He's got a population of cells and we want to decide how to put them to work to represent space."
        ],
        [
            "One entirely reasonable way to do it would be for each, read each circle there to just encode some location in space, and only fire when they sell it.",
            "When the animal is there."
        ],
        [
            "And that's what this would look like.",
            "The animals in the center, the maze to the cell that encodes the center of the maze will be firing."
        ],
        [
            "A really similar representation would be this indicator bump.",
            "We're now spaces encoded continuously and the animals location is encoded by where the center of this bump is.",
            "Population could still be decoded to find out the animal's precise location."
        ],
        [
            "The successor representation is different in that now instead we're representing a predictive statistic over locations instead of current location.",
            "That means that different locations that predict similar locations should invoke really similar population vectors in the neuronal population.",
            "Here, the firing rate of each cell is equal to the expected discounted number of times that it will be visited.",
            "Since the rat starts out in the center of the maze and we discount so that immediate states count more, we see that the firing is maximal where that is and kind of decays.",
            "If you get further and further away from that location."
        ],
        [
            "So before we claimed that computing that value computation, is this really easy linear operation and it can rapidly account for changes in where the reward is placed.",
            "So let's go through what that would look like.",
            "Let's say we put the reward this fruit loop at the end of the maze.",
            "And now."
        ],
        [
            "One word vector looks like this is just a one where the fruit loop is in a zero everywhere else.",
            "To compute value, we multiply them together elementwise, and then sum up the the multiplied vector.",
            "And we get that this value is .1 three and I included numbers in the successor representation.",
            "So you can check my math and make sure it's working.",
            "Now if we just remove the move the reward over one, we see that we get a new value because now the reward is being scaled by a different element in the successor representation.",
            "So because the reward is closer rightly, we see a higher value for that central state.",
            "If we."
        ],
        [
            "Instead, just leave the reward where it is, but increase the magnitude of it.",
            "We can once again compute value by multiplying our reward vector with the precompiled successor representation and quickly get a new value.",
            "And to make things really."
        ],
        [
            "Crazy, let's add to rewards now and once again we can do the same multiplication of reward vector with precompiled successor representation and get this new value estimate.",
            "So if the reward is left in the same place for a really long time, we would expect the animal to develop this policy where it always heads in the direction of reward.",
            "Um?",
            "Because an his successor representation depends on policy, so it too will become changed once the animal updates his policy sufficiently.",
            "However, since this is a slower incremental update process, we expect this initial value estimate to still be a pretty good guess before the policy has had time to update, or if the reward kind of stays in the same place, but maybe increases in magnitude."
        ],
        [
            "So now we'll do it quick data Blitz where I just throw lots of data, actually with minimal detail.",
            "And if you want to go over in more detail later, you should definitely come to my poster and we'll talk about it."
        ],
        [
            "First, we'll look at simulated and recorded place fields in a 2D environment.",
            "As you can see, place fields have been recorded by lots and lots of researchers, and they're very well documented.",
            "A Canonical place field has this circular field that kind of gradually decays as you get away from the center of the field.",
            "It's important to note to hear that we switched gears a little bit and now instead of plotting what the entire population of cells is doing at a single location, applauding what one cell would do at a bunch of locations.",
            "So each of these square plots here is a receptive field.",
            "It's what each cell does for lots of locations, and this kind of data is easier to get from these experiments.",
            "In our simulated data we we used a random walk policy with no directional biases.",
            "Here too we're plotting receptive fields, not population vectors.",
            "So what's being shown is the expected number of times that central state will be visited from any given starting location, rather than the expected number of times.",
            "All of these locations would be given from a central starting location.",
            "Kind of a subtle distinction, and the main point to be made here is that these guys look similar.",
            "It's kind of a nice sanity check that this generates just the Canonical place else."
        ],
        [
            "Now we'll go into some more nuanced phenomena.",
            "One thing that's been notice about place cells, as they tend to swell to fit the enclosure, and they're constrained by boundaries, so firing rate doesn't drop off as a function of the Euclidean distance from the center of the field, but rather as a function of your distance along some actually traversable path so you can see here in this plot from an older paper that the cells don't really on the other side of the barrier you don't see firing.",
            "We see the same thing too in our simulation by adding barriers across which the animal can't transition, and this makes sense because states on the other side of the barrier aren't actually predicted by the current location, the animal would have to take a longer path."
        ],
        [
            "It's also been shown that when an animal has been trained to move preferentially in One Direction down a 1 dimensional track, you see place fields that gradually skew opposite the direction of travel.",
            "That is, as the animal is going down the track, the cell will start firing earlier and earlier as the animals been trained to do it.",
            "We see the same in our simulations compared to the no bias condition.",
            "Since these cell since once the policy has been learned in which it's constantly moving in One Direction, then that cell can be predicted from earlier on and it will start to skew more and more as the bias is increased."
        ],
        [
            "We use this also to explain the fact that the clustering of place cells near rewarded location has been observed.",
            "This figure by Halep at all shows that the percentage of place cells that were recorded firing above baseline is larger in the recorded segment of an annular maze than in the other segments.",
            "In this experiment, a rat is swimming around in this opaque solution in a circular maze, and rats don't like swimming.",
            "They find it exhausting.",
            "So the reward is this hidden platform that it can't see and just learn about when it gets there.",
            "In our simulation we see the same thing and this results because the firing fields of cells that are near the reward or kind of or on the path to the reward will start to expand their firing fields.",
            "Since once the policy is learned in the animals always heading towards the reward, it knows that these states are predicted.",
            "And so we see the same effect as well."
        ],
        [
            "And one more result to bring it back to behavior.",
            "It's been shown that fear learning is more effective if the animal has had a chance to wander around its enclosure for a little bit.",
            "So in this experiment the animal is exposed to a new Chamber and it's still an it's delivered a shock and animals don't like shocks, so that has a very large negative value.",
            "And the spirit learning is more effective.",
            "The animal can wander around the room first rather than receiving the shock as soon as it gets into the room.",
            "So you can see this blue line.",
            "Here is the to.",
            "The leftmost is the amount of freezing that they show when they've been exposed to the room and in red.",
            "This is when they've not been pre exposed and it's much less.",
            "This is also hippocampal independent and you can see when the hippocampus has been lesioned.",
            "This effect is much less pronounced.",
            "This makes a lot of sense if you consider the temporal dynamics of learning the successor representation.",
            "If you aren't given time to learn how states predict each other, then this negative reward that's received can't effectively diffused other States and it can't be amplified by the actual amount that the animal expects to visit these other locations.",
            "So here we kind of plot the total value over the room are proxy for how much, how effective fear conditioning should be in green on top as a function of pre exposure time, and you can see as the animal learns the successor representation, the negative value that's measured will be greater.",
            "We simulated lesioning the hippocampus by lowering the learning rate on this successor representation, and of course it takes much longer to learn and the effect is much less pronounced.",
            "So by sampling at the beginning and end of pre exposure with the Legion and Legion cases we show the same barplot and replicate the finding."
        ],
        [
            "In conclusion, what did we do?",
            "We explained why predictive coding in the hippocampus is useful for reinforcement learning and how these constraints can cause place fields to naturally arise.",
            "We showed how this representation of space explains the shape of place fields in certain environments and is useful for reward computations.",
            "And we also employed a sufficiently general model that can apply to these generic non spatial tasks."
        ],
        [
            "So for the stuff that I didn't cover here but is on my poster, if you guys are interested in it, we have an eigenvector model of grid cells where we take the eigen decomposition of the successor representation and show that these look a lot like grid cells.",
            "And here are some simulated grid cells in a bunch of different environments.",
            "We talk about how space is represented hierarchically.",
            "If you consider a successor representation over multiple discounts and how this leads itself to be a natural substrate for hierarchical reinforcement learning.",
            "And we talked about an experiment showing predictive representations for non spatial tasks that's been performed by somebody else in my lab."
        ],
        [
            "I'd like to thank my collaborators Matt Botvinnik and Sam Gershman, who are brilliant and awesome to work with.",
            "I would also like to thank the other members of Botvinnik Lab and other labs in Princeton who gave me a lot of really helpful feedback on this project, and also specifically this presentation.",
            "The funding agencies who gave us money to think about stuff and thank you guys for listening to me talk NRL DM for inviting me."
        ],
        [
            "Are there any questions?",
            "Hey, this is a very interesting idea.",
            "I guess I'm a little sceptical.",
            "That this the local nature of place fields, even in situations where animals behaving very deterministically is consistent, you'll either have to assume.",
            "Very unrealistic levels of discounting and or randomness in the policy to get feels that look.",
            "Localist under this kind of assumption, or that the biases that you're.",
            "Modeling being much larger than the rather subtle biases in the experimental data, like the asymmetry, right?",
            "You would imagine in your model without, you know under realistic discount factors or policy deterministic determinism, you get huge biases that aren't really.",
            "Yeah, I I so I largely agree with you.",
            "I mean, I guess a range of discounts is kind of representing the hippocampus.",
            "You could consider like less spatial specificity then we show is also shown elsewhere in the hippocampus more.",
            "Importantly though, I think this.",
            "To whatever extent this does explain place selectivity, I don't think it explains the activity of all of them.",
            "Like a lot of place cells seem to just kind of like pop up as soon as the animal enters the room and doesn't really seem to depend on the experience and wandering around as much.",
            "So I think a lot of the cases where we don't wear like we don't have the same baseline effect might just be explained by other place else.",
            "Not doing this in the event that this model is right for another population of them.",
            "Does that address your question?",
            "The same question again, but just expressed differently, so it seems like so.",
            "Firstly, I think it's a cool idea and.",
            "And I think the grid cells does great as well.",
            "The I was, it seems like the fundamental principle behind the idea requires you to have global scope in the successor place, else 'cause if you haven't got global scope, you can't, you can't.",
            "Compute the value function so so like if I'm a long way away from from this place, I still have to have information about in this place all about where the reward is.",
            "I have, yeah, sorry, so that seem and that on the on the firing I require falls off exponentially.",
            "I just don't know how.",
            "How can you do that robustly with spikes over?",
            "Because like I'm either going out one spike or no space.",
            "How can we get exponentially smaller spikes over global scope?",
            "It just seems I mean, I think it's the same question as a family is really.",
            "Yeah again, yeah, that's that's a good point I think.",
            "So for problems with larger scale, even though it's an exponentially decaying discount if you have a really large discount, one that's pretty close to one, you can get a very long horizon with it, and that's like I like that about this model that you can kind of arbitrarily tune the the scale of how far in advance you want to care about.",
            "As for encoding it in.",
            "As for like the reliability of spiking coding on resolution like on for suchlike.",
            "Sparse problems I guess.",
            "I'm not sure.",
            "I would think that in the event of maybe a larger discount then you've got a larger mean firing rate because it doesn't get us quite as low, and maybe that would help, but that's a good point we should think about that more.",
            "So there are pretty cool idea.",
            "I was wondering, so there's a bunch of other specially tuned cell types in campus and vicinity like border cells, head direction cells and they have previously been used to explain, for example, the behavior of place else near boundaries.",
            "So was wondering if your account contradicts these accounts or.",
            "Yeah, so it.",
            "It kind of ignores the the input to this cells.",
            "At this point we so the border cells and head direction cells are almost certainly really important for making place cells and for allowing place to be in Ferd.",
            "And that's a really interesting and really important problem is is how the animal can actually get this representation of the topology.",
            "The space from its observation, how it can infer location.",
            "We kind of assume that happens a little bit downstream, and we're more considered or more considering the thing that the map is optimized for at downstream of its map rather than how the map is built itself.",
            "But it's a important part of the problem.",
            "It doesn't.",
            "I think it doesn't contradict that it more just relies on that happening.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good afternoon, my name is Kimberly Stack Infeld.",
                    "label": 0
                },
                {
                    "sent": "I'm a graduate student in Matt Botvinnik's lab at Princeton and I've been working on this project in collaboration with Matt and with Sam Gershman who's at MIT.",
                    "label": 0
                },
                {
                    "sent": "The theme of this talk is reinforces how reinforcement learning objectives constrain the cognitive map.",
                    "label": 1
                },
                {
                    "sent": "1st, I'll go over some things neuro scientists know about how space is represented in the brain.",
                    "label": 0
                },
                {
                    "sent": "Then I'll talk about the model and the reinforcement.",
                    "label": 0
                },
                {
                    "sent": "Learning ideas that we used to inform it.",
                    "label": 0
                },
                {
                    "sent": "And at the end I'll present kind of a flurry of simulations and data that we predict and explain with our model.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We know that there exists these cells called place cells in the hippocampus.",
                    "label": 0
                },
                {
                    "sent": "And these playstyles have firing that's specific to one location.",
                    "label": 1
                },
                {
                    "sent": "So in the figure at the top right we see the animals actual trajectory through a real room in black, and then in red we see the locations where a cell fired, so you can see that it's sort of limited to the range in the bottom of the room.",
                    "label": 0
                },
                {
                    "sent": "Fire is also sensitive to other aspects of experience and to reward in the environment, and that's why we're interested in it.",
                    "label": 1
                },
                {
                    "sent": "Place cells also show this predicted precession effect where after the animals been trained, cells that are upcoming will start firing in advance of when they're actually reached.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Grid cells are found in enter rhino cortex.",
                    "label": 0
                },
                {
                    "sent": "For grid cells, firing is periodic with respect to space, so here you can see this grid cell has multiple firing fields that are arranged in this nice hexagonal tessellation.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now to go into the reinforcement learning side of it, animals want to maximize value.",
                    "label": 1
                },
                {
                    "sent": "The most general objective of navigation under a given policy is therefore to visit states maximizing value.",
                    "label": 1
                },
                {
                    "sent": "Here we define the value of a state to be the total discounted expected reward that you'll receive over your future from that state on.",
                    "label": 0
                },
                {
                    "sent": "And you can compute that with the infinite sum of the expected reward from your current state on all your future states.",
                    "label": 0
                },
                {
                    "sent": "We also include this discount term which is often included in the Bellman equation to encode the intuition that immediate reward matters more than very distantly received reward.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for a given policy, the value function of a given state can be neatly decomposed into the dot product of two separate terms.",
                    "label": 0
                },
                {
                    "sent": "In red we see the expected number of times each state in the environment will be visited from your given starting location.",
                    "label": 0
                },
                {
                    "sent": "Is called the successor representation, and we see the reward vector.",
                    "label": 0
                },
                {
                    "sent": "So the total expected discounted reward will be the expected discounted number of times you reach each state and the reward you get when you're there.",
                    "label": 0
                },
                {
                    "sent": "And this decomposition was introduced by Peter Diane in 1993, and it's called the successor representation because you're representing your state as a statistic over your future states or your successors.",
                    "label": 0
                },
                {
                    "sent": "This representation can be learned incrementally with TD learning and because space and reward or independent terms, you can quickly recompute value in cases where the reward is changing at a faster timescale than your policy can be updated.",
                    "label": 1
                },
                {
                    "sent": "It's important to note that we're considering the successor representation as a representation under a given static policy.",
                    "label": 0
                },
                {
                    "sent": "That is, we assume that the ASR is handed a policy and that it's incorporated in some larger policy learning loop and not learning it in and of itself.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The main thrust of our model is that the hippocampus is a cognitive map is realizing the successor representation.",
                    "label": 0
                },
                {
                    "sent": "So to paint a picture of what those equations will mean, let's go through exactly what the spatial representation would look like if it's realized by neurons in the hippocampus.",
                    "label": 0
                },
                {
                    "sent": "So we have our little rat friend and he starts out in the middle of this linear track an in his hippocampus.",
                    "label": 0
                },
                {
                    "sent": "He's got a population of cells and we want to decide how to put them to work to represent space.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One entirely reasonable way to do it would be for each, read each circle there to just encode some location in space, and only fire when they sell it.",
                    "label": 0
                },
                {
                    "sent": "When the animal is there.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's what this would look like.",
                    "label": 0
                },
                {
                    "sent": "The animals in the center, the maze to the cell that encodes the center of the maze will be firing.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A really similar representation would be this indicator bump.",
                    "label": 0
                },
                {
                    "sent": "We're now spaces encoded continuously and the animals location is encoded by where the center of this bump is.",
                    "label": 0
                },
                {
                    "sent": "Population could still be decoded to find out the animal's precise location.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The successor representation is different in that now instead we're representing a predictive statistic over locations instead of current location.",
                    "label": 0
                },
                {
                    "sent": "That means that different locations that predict similar locations should invoke really similar population vectors in the neuronal population.",
                    "label": 0
                },
                {
                    "sent": "Here, the firing rate of each cell is equal to the expected discounted number of times that it will be visited.",
                    "label": 0
                },
                {
                    "sent": "Since the rat starts out in the center of the maze and we discount so that immediate states count more, we see that the firing is maximal where that is and kind of decays.",
                    "label": 0
                },
                {
                    "sent": "If you get further and further away from that location.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So before we claimed that computing that value computation, is this really easy linear operation and it can rapidly account for changes in where the reward is placed.",
                    "label": 0
                },
                {
                    "sent": "So let's go through what that would look like.",
                    "label": 0
                },
                {
                    "sent": "Let's say we put the reward this fruit loop at the end of the maze.",
                    "label": 0
                },
                {
                    "sent": "And now.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One word vector looks like this is just a one where the fruit loop is in a zero everywhere else.",
                    "label": 0
                },
                {
                    "sent": "To compute value, we multiply them together elementwise, and then sum up the the multiplied vector.",
                    "label": 0
                },
                {
                    "sent": "And we get that this value is .1 three and I included numbers in the successor representation.",
                    "label": 0
                },
                {
                    "sent": "So you can check my math and make sure it's working.",
                    "label": 0
                },
                {
                    "sent": "Now if we just remove the move the reward over one, we see that we get a new value because now the reward is being scaled by a different element in the successor representation.",
                    "label": 0
                },
                {
                    "sent": "So because the reward is closer rightly, we see a higher value for that central state.",
                    "label": 0
                },
                {
                    "sent": "If we.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Instead, just leave the reward where it is, but increase the magnitude of it.",
                    "label": 0
                },
                {
                    "sent": "We can once again compute value by multiplying our reward vector with the precompiled successor representation and quickly get a new value.",
                    "label": 0
                },
                {
                    "sent": "And to make things really.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Crazy, let's add to rewards now and once again we can do the same multiplication of reward vector with precompiled successor representation and get this new value estimate.",
                    "label": 0
                },
                {
                    "sent": "So if the reward is left in the same place for a really long time, we would expect the animal to develop this policy where it always heads in the direction of reward.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Because an his successor representation depends on policy, so it too will become changed once the animal updates his policy sufficiently.",
                    "label": 0
                },
                {
                    "sent": "However, since this is a slower incremental update process, we expect this initial value estimate to still be a pretty good guess before the policy has had time to update, or if the reward kind of stays in the same place, but maybe increases in magnitude.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we'll do it quick data Blitz where I just throw lots of data, actually with minimal detail.",
                    "label": 0
                },
                {
                    "sent": "And if you want to go over in more detail later, you should definitely come to my poster and we'll talk about it.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First, we'll look at simulated and recorded place fields in a 2D environment.",
                    "label": 0
                },
                {
                    "sent": "As you can see, place fields have been recorded by lots and lots of researchers, and they're very well documented.",
                    "label": 0
                },
                {
                    "sent": "A Canonical place field has this circular field that kind of gradually decays as you get away from the center of the field.",
                    "label": 0
                },
                {
                    "sent": "It's important to note to hear that we switched gears a little bit and now instead of plotting what the entire population of cells is doing at a single location, applauding what one cell would do at a bunch of locations.",
                    "label": 0
                },
                {
                    "sent": "So each of these square plots here is a receptive field.",
                    "label": 0
                },
                {
                    "sent": "It's what each cell does for lots of locations, and this kind of data is easier to get from these experiments.",
                    "label": 0
                },
                {
                    "sent": "In our simulated data we we used a random walk policy with no directional biases.",
                    "label": 0
                },
                {
                    "sent": "Here too we're plotting receptive fields, not population vectors.",
                    "label": 0
                },
                {
                    "sent": "So what's being shown is the expected number of times that central state will be visited from any given starting location, rather than the expected number of times.",
                    "label": 0
                },
                {
                    "sent": "All of these locations would be given from a central starting location.",
                    "label": 0
                },
                {
                    "sent": "Kind of a subtle distinction, and the main point to be made here is that these guys look similar.",
                    "label": 0
                },
                {
                    "sent": "It's kind of a nice sanity check that this generates just the Canonical place else.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we'll go into some more nuanced phenomena.",
                    "label": 0
                },
                {
                    "sent": "One thing that's been notice about place cells, as they tend to swell to fit the enclosure, and they're constrained by boundaries, so firing rate doesn't drop off as a function of the Euclidean distance from the center of the field, but rather as a function of your distance along some actually traversable path so you can see here in this plot from an older paper that the cells don't really on the other side of the barrier you don't see firing.",
                    "label": 0
                },
                {
                    "sent": "We see the same thing too in our simulation by adding barriers across which the animal can't transition, and this makes sense because states on the other side of the barrier aren't actually predicted by the current location, the animal would have to take a longer path.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's also been shown that when an animal has been trained to move preferentially in One Direction down a 1 dimensional track, you see place fields that gradually skew opposite the direction of travel.",
                    "label": 0
                },
                {
                    "sent": "That is, as the animal is going down the track, the cell will start firing earlier and earlier as the animals been trained to do it.",
                    "label": 0
                },
                {
                    "sent": "We see the same in our simulations compared to the no bias condition.",
                    "label": 0
                },
                {
                    "sent": "Since these cell since once the policy has been learned in which it's constantly moving in One Direction, then that cell can be predicted from earlier on and it will start to skew more and more as the bias is increased.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We use this also to explain the fact that the clustering of place cells near rewarded location has been observed.",
                    "label": 0
                },
                {
                    "sent": "This figure by Halep at all shows that the percentage of place cells that were recorded firing above baseline is larger in the recorded segment of an annular maze than in the other segments.",
                    "label": 0
                },
                {
                    "sent": "In this experiment, a rat is swimming around in this opaque solution in a circular maze, and rats don't like swimming.",
                    "label": 0
                },
                {
                    "sent": "They find it exhausting.",
                    "label": 0
                },
                {
                    "sent": "So the reward is this hidden platform that it can't see and just learn about when it gets there.",
                    "label": 0
                },
                {
                    "sent": "In our simulation we see the same thing and this results because the firing fields of cells that are near the reward or kind of or on the path to the reward will start to expand their firing fields.",
                    "label": 0
                },
                {
                    "sent": "Since once the policy is learned in the animals always heading towards the reward, it knows that these states are predicted.",
                    "label": 0
                },
                {
                    "sent": "And so we see the same effect as well.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And one more result to bring it back to behavior.",
                    "label": 0
                },
                {
                    "sent": "It's been shown that fear learning is more effective if the animal has had a chance to wander around its enclosure for a little bit.",
                    "label": 0
                },
                {
                    "sent": "So in this experiment the animal is exposed to a new Chamber and it's still an it's delivered a shock and animals don't like shocks, so that has a very large negative value.",
                    "label": 0
                },
                {
                    "sent": "And the spirit learning is more effective.",
                    "label": 0
                },
                {
                    "sent": "The animal can wander around the room first rather than receiving the shock as soon as it gets into the room.",
                    "label": 0
                },
                {
                    "sent": "So you can see this blue line.",
                    "label": 0
                },
                {
                    "sent": "Here is the to.",
                    "label": 0
                },
                {
                    "sent": "The leftmost is the amount of freezing that they show when they've been exposed to the room and in red.",
                    "label": 0
                },
                {
                    "sent": "This is when they've not been pre exposed and it's much less.",
                    "label": 0
                },
                {
                    "sent": "This is also hippocampal independent and you can see when the hippocampus has been lesioned.",
                    "label": 0
                },
                {
                    "sent": "This effect is much less pronounced.",
                    "label": 0
                },
                {
                    "sent": "This makes a lot of sense if you consider the temporal dynamics of learning the successor representation.",
                    "label": 0
                },
                {
                    "sent": "If you aren't given time to learn how states predict each other, then this negative reward that's received can't effectively diffused other States and it can't be amplified by the actual amount that the animal expects to visit these other locations.",
                    "label": 0
                },
                {
                    "sent": "So here we kind of plot the total value over the room are proxy for how much, how effective fear conditioning should be in green on top as a function of pre exposure time, and you can see as the animal learns the successor representation, the negative value that's measured will be greater.",
                    "label": 0
                },
                {
                    "sent": "We simulated lesioning the hippocampus by lowering the learning rate on this successor representation, and of course it takes much longer to learn and the effect is much less pronounced.",
                    "label": 0
                },
                {
                    "sent": "So by sampling at the beginning and end of pre exposure with the Legion and Legion cases we show the same barplot and replicate the finding.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In conclusion, what did we do?",
                    "label": 0
                },
                {
                    "sent": "We explained why predictive coding in the hippocampus is useful for reinforcement learning and how these constraints can cause place fields to naturally arise.",
                    "label": 0
                },
                {
                    "sent": "We showed how this representation of space explains the shape of place fields in certain environments and is useful for reward computations.",
                    "label": 0
                },
                {
                    "sent": "And we also employed a sufficiently general model that can apply to these generic non spatial tasks.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for the stuff that I didn't cover here but is on my poster, if you guys are interested in it, we have an eigenvector model of grid cells where we take the eigen decomposition of the successor representation and show that these look a lot like grid cells.",
                    "label": 0
                },
                {
                    "sent": "And here are some simulated grid cells in a bunch of different environments.",
                    "label": 1
                },
                {
                    "sent": "We talk about how space is represented hierarchically.",
                    "label": 0
                },
                {
                    "sent": "If you consider a successor representation over multiple discounts and how this leads itself to be a natural substrate for hierarchical reinforcement learning.",
                    "label": 1
                },
                {
                    "sent": "And we talked about an experiment showing predictive representations for non spatial tasks that's been performed by somebody else in my lab.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'd like to thank my collaborators Matt Botvinnik and Sam Gershman, who are brilliant and awesome to work with.",
                    "label": 0
                },
                {
                    "sent": "I would also like to thank the other members of Botvinnik Lab and other labs in Princeton who gave me a lot of really helpful feedback on this project, and also specifically this presentation.",
                    "label": 0
                },
                {
                    "sent": "The funding agencies who gave us money to think about stuff and thank you guys for listening to me talk NRL DM for inviting me.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are there any questions?",
                    "label": 0
                },
                {
                    "sent": "Hey, this is a very interesting idea.",
                    "label": 0
                },
                {
                    "sent": "I guess I'm a little sceptical.",
                    "label": 0
                },
                {
                    "sent": "That this the local nature of place fields, even in situations where animals behaving very deterministically is consistent, you'll either have to assume.",
                    "label": 0
                },
                {
                    "sent": "Very unrealistic levels of discounting and or randomness in the policy to get feels that look.",
                    "label": 0
                },
                {
                    "sent": "Localist under this kind of assumption, or that the biases that you're.",
                    "label": 0
                },
                {
                    "sent": "Modeling being much larger than the rather subtle biases in the experimental data, like the asymmetry, right?",
                    "label": 0
                },
                {
                    "sent": "You would imagine in your model without, you know under realistic discount factors or policy deterministic determinism, you get huge biases that aren't really.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I I so I largely agree with you.",
                    "label": 0
                },
                {
                    "sent": "I mean, I guess a range of discounts is kind of representing the hippocampus.",
                    "label": 0
                },
                {
                    "sent": "You could consider like less spatial specificity then we show is also shown elsewhere in the hippocampus more.",
                    "label": 0
                },
                {
                    "sent": "Importantly though, I think this.",
                    "label": 0
                },
                {
                    "sent": "To whatever extent this does explain place selectivity, I don't think it explains the activity of all of them.",
                    "label": 0
                },
                {
                    "sent": "Like a lot of place cells seem to just kind of like pop up as soon as the animal enters the room and doesn't really seem to depend on the experience and wandering around as much.",
                    "label": 0
                },
                {
                    "sent": "So I think a lot of the cases where we don't wear like we don't have the same baseline effect might just be explained by other place else.",
                    "label": 0
                },
                {
                    "sent": "Not doing this in the event that this model is right for another population of them.",
                    "label": 0
                },
                {
                    "sent": "Does that address your question?",
                    "label": 0
                },
                {
                    "sent": "The same question again, but just expressed differently, so it seems like so.",
                    "label": 0
                },
                {
                    "sent": "Firstly, I think it's a cool idea and.",
                    "label": 0
                },
                {
                    "sent": "And I think the grid cells does great as well.",
                    "label": 0
                },
                {
                    "sent": "The I was, it seems like the fundamental principle behind the idea requires you to have global scope in the successor place, else 'cause if you haven't got global scope, you can't, you can't.",
                    "label": 0
                },
                {
                    "sent": "Compute the value function so so like if I'm a long way away from from this place, I still have to have information about in this place all about where the reward is.",
                    "label": 0
                },
                {
                    "sent": "I have, yeah, sorry, so that seem and that on the on the firing I require falls off exponentially.",
                    "label": 0
                },
                {
                    "sent": "I just don't know how.",
                    "label": 0
                },
                {
                    "sent": "How can you do that robustly with spikes over?",
                    "label": 0
                },
                {
                    "sent": "Because like I'm either going out one spike or no space.",
                    "label": 0
                },
                {
                    "sent": "How can we get exponentially smaller spikes over global scope?",
                    "label": 0
                },
                {
                    "sent": "It just seems I mean, I think it's the same question as a family is really.",
                    "label": 0
                },
                {
                    "sent": "Yeah again, yeah, that's that's a good point I think.",
                    "label": 0
                },
                {
                    "sent": "So for problems with larger scale, even though it's an exponentially decaying discount if you have a really large discount, one that's pretty close to one, you can get a very long horizon with it, and that's like I like that about this model that you can kind of arbitrarily tune the the scale of how far in advance you want to care about.",
                    "label": 0
                },
                {
                    "sent": "As for encoding it in.",
                    "label": 0
                },
                {
                    "sent": "As for like the reliability of spiking coding on resolution like on for suchlike.",
                    "label": 0
                },
                {
                    "sent": "Sparse problems I guess.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure.",
                    "label": 0
                },
                {
                    "sent": "I would think that in the event of maybe a larger discount then you've got a larger mean firing rate because it doesn't get us quite as low, and maybe that would help, but that's a good point we should think about that more.",
                    "label": 0
                },
                {
                    "sent": "So there are pretty cool idea.",
                    "label": 0
                },
                {
                    "sent": "I was wondering, so there's a bunch of other specially tuned cell types in campus and vicinity like border cells, head direction cells and they have previously been used to explain, for example, the behavior of place else near boundaries.",
                    "label": 0
                },
                {
                    "sent": "So was wondering if your account contradicts these accounts or.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so it.",
                    "label": 0
                },
                {
                    "sent": "It kind of ignores the the input to this cells.",
                    "label": 0
                },
                {
                    "sent": "At this point we so the border cells and head direction cells are almost certainly really important for making place cells and for allowing place to be in Ferd.",
                    "label": 0
                },
                {
                    "sent": "And that's a really interesting and really important problem is is how the animal can actually get this representation of the topology.",
                    "label": 0
                },
                {
                    "sent": "The space from its observation, how it can infer location.",
                    "label": 0
                },
                {
                    "sent": "We kind of assume that happens a little bit downstream, and we're more considered or more considering the thing that the map is optimized for at downstream of its map rather than how the map is built itself.",
                    "label": 0
                },
                {
                    "sent": "But it's a important part of the problem.",
                    "label": 0
                },
                {
                    "sent": "It doesn't.",
                    "label": 0
                },
                {
                    "sent": "I think it doesn't contradict that it more just relies on that happening.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}