{
    "id": "oovtf4u4hwpf4uf26pbfezlf6dfru6rp",
    "title": "Scalable Structured Low Rank Matrix Optimization Problems",
    "info": {
        "author": [
            "Marco Signoretto, KU Leuven"
        ],
        "published": "Aug. 26, 2013",
        "recorded": "July 2013",
        "category": [
            "Top->Computer Science->Compressed Sensing",
            "Top->Computer Science->Machine Learning->Regularization",
            "Top->Computer Science->Optimization Methods",
            "Top->Computer Science->Machine Learning->Kernel Methods->Support Vector Machines"
        ]
    },
    "url": "http://videolectures.net/roks2013_signoretto_optimization/",
    "segmentation": [
        [
            "So this talk is not going to be about tensors, but again about structured low rank matrix optimization problems.",
            "And this is joint work.",
            "We've Balkan, chevra Nanyang cycles."
        ],
        [
            "So this is the outline of my talk.",
            "I will first start by talking about general setting in which I put into the context of learning a class of structured low rank matrix optimization problem.",
            "Then I'll talk about specific problem formulation and application to system identification with missing data, which is pretty much what Constantine was talking about earlier.",
            "But our approach is somewhat different.",
            "Then talk about solutions, strategies, an discuss reformulations of the problem and present some experiments."
        ],
        [
            "Alright, so."
        ],
        [
            "I would like I would like to start from from scratch.",
            "The problem of finding a model from observational data."
        ],
        [
            "Within the context of structuralism, Isation Varia is to construct nested subsets of increasingly convex complex iPod.",
            "This is F."
        ],
        [
            "In practice, we do so, but by defining a set which is define iponan.",
            "The level set of a penalty function Omega.",
            "For each of these sets, we solve infect penalized empirical discrimination problem in which in fact parimeter penalty parameter Lambda is 1 to one with this parameter that I have earlier and then at the end we find we pick the model that realises complexity.",
            "Fidelity trade off.",
            "And this is our final final model.",
            "So within this framework and actually also with in other frameworks of learning from examples, it is quite crucial to design the structure of this nested sequence publicly, and this amount in fact that choosing the penalty penalty function.",
            "And there's been quite some work in the last two decades, in effect coming out with appropriate penalties in."
        ],
        [
            "The contest of structured sparsity.",
            "Or if you want structure inducing penalties, the simplest case is perhaps the L1 penalty, which has been popularized by the socola Sue and the prior knowledge is that parameter vector should be sparse, and then we solve this convex problem.",
            "This is I guess you are already familiar with.",
            "It.",
            "VL one or the one norm?",
            "An other setting where structured sparsity arises within multi task learning.",
            "And then massive until was talking about this yesterday.",
            "The idea is that we have a capital T parameter vectors which relates to learning tasks.",
            "And then there's a comment.",
            "So then the line assumption is that there's a common dictionary.",
            "Basically, there's a bulk of common features that are shared across this different learning tasks.",
            "In this case, it has been proposed to consider as a penalty the nuclear norm, which is a proxy for rank function.",
            "And it's defined as a sum of singular values, so this was also already discussed in in previous talks during the workshop.",
            "These are."
        ],
        [
            "Are somewhat simple penalty as you can see they are defined based upon norms.",
            "It turns out that there's quite some interest in more complex penalty that involve a composition of functions, so we have, for instance, enorme composed with linear transformation.",
            "This is the case for instance in the so called fused lasso, in which we basically assume that the parameter to be learn should have small number of changes.",
            "So we have, let's say capital P entries and we want to observe as a small number of changes as possible.",
            "And basically we can encode this structure by means of a matrix and require that the after being linearly transformed by a the parameter vector is sparse.",
            "So basically the sparsity here is to be understood after linear transformation.",
            "In this setting, an upper case concerning the nuclear norm this time.",
            "Is when we have a linear transformation of a parameter matrix which is in fact encoded by these two matrices A&B and this has been considered quite extensively in multitask learning, collaborative filtering and also in infecting system identification so."
        ],
        [
            "And these problems are usually more difficult to to solve an and the problem I'm going to talk about it is precisely we belongs precisely to this to this class of problem with composite penalties."
        ],
        [
            "So.",
            "The goal is pretty much the same as in constant in stock.",
            "We want to learn from observational data and metrics that in addition to being low rank as entered partitioning to known disjoint groups.",
            "So here I'm considering not general linear transformation, but the specific class.",
            "This is the type of problem formulation we consider we have here in empirical risk and a penalty which is a composite spectral penalty spectral because, well, it's a nuclear norm.",
            "So it depends upon the singular values of the matrix obtained by transforming the parameter W. This linear operator, which we call a mutation, and I'm going to give some more details in a minute as I, as I mentioned already, we use the nuclear norm.",
            "As a proxy for the rank, so this makes this problem convex, assuming that this term is also convex, which is usually the case and can be turned into an SDP.",
            "But we don't want to do this because we want to exploit the structure and deal with large scale problems.",
            "Anne.",
            "So."
        ],
        [
            "What is this simulation I was talking about?",
            "As I mentioned, we are, we assume we are assuming that we have entries partition into disjoint sets, so they form a partition and we can define a membership function that Maps entries into the appropriate set indexed by hell and then we have a mutation operator which I call forward to be distinguished with backward which is the joint.",
            "So the adjoint of this operator I will call it.",
            "Backward an and what this does is to map a parameter vector into a structure metrics depending upon the membership function.",
            "So if you're lost into the formulas, you can look at this simple picture here.",
            "Yeah, I'm assuming I have two dimensional parameter vector with just two entries and IMAP these entries into two different sets that forms a partition.",
            "So you see the entries in red and the entries in yellow.",
            "So these are in two different sets.",
            "That forms a partition.",
            "So this is a type of structured I'm I'm interested in."
        ],
        [
            "And then one possible application is system identification.",
            "General idea is to find a dynamical model from observational input output signals and the nuclear norm has been used extensively in the last, say one decade.",
            "It has been motivated by well known subspace properties and constant is already discussed.",
            "This a bit an also not just nuclear norm but wait in nuclear norm in the sense that.",
            "Anne.",
            "I introduced earlier, so we have weights and this weights in the context of system identification are related to instrumental variables.",
            "And typically one observes improvement but modest relatively modest improvement with respect to classical subspace based algorithms that were developed in the 80s.",
            "On the other end, the nuclear norm has been shown to be quite effective for completion problems.",
            "By now it is it has been used quite extensively.",
            "It appeared in many papers and the idea is that when we have missing input and output.",
            "System identification problem.",
            "We can essentially use a structural rank metrics optimization problem as I introduced it earlier to infect learning dynamical model under under missing data.",
            "So basically reconstruct input and output an in a second stage.",
            "We can find system matrices by means of simple algebraic steps."
        ],
        [
            "So this is where this arises from.",
            "So, or at least a specific instance.",
            "Let's assume that we have an underlying state space model with order and sybex where answer backs is in fact the dimensionality of a state vector.",
            "It turns out that under certain conditions we can form.",
            "So this is the block ankle metrics which was already described by constantly.",
            "So I'm not going to talk about it.",
            "This is an ankle operator applied to Vector now.",
            "Sorry, actually it's it's a matrix.",
            "In fact, in the case where we have multiple outputs or inputs.",
            "Ann, but you can think of it as a vectorized version of it.",
            "So basically we are interested in this linear operator that Maps this yuan Y into concatenation of block Hankel matrices and turns out that under this generating mechanism the rank of this matrix now is equal to the rank of the input ankle block anchor metrics plus.",
            "Order of assistance.",
            "So the dimensionality of a state vector and we can effect rely on this in order to derive formulation for system identification.",
            "This is being considered."
        ],
        [
            "Bayou ET al.",
            "Recently they saw this type of.",
            "Problem which basically tries to reconstruct a set of inputs and outputs based upon sampling.",
            "So we have observed measurements from inputs and outputs and we try to map them into the.",
            "Inputs and output to be found provided that.",
            "So I mean using as prior that essentially the generating mechanism should be as simple as possible, so the dynamics should be such that the states we underline state vector is small essentially, and as you can see, this is basically a structured low rank matrix optimization problem in the sense I described earlier.",
            "In particular, this operator is basically a mutation now in in the sense of my definition that I gave earlier."
        ],
        [
            "So now how do we this is effect a specific application to give you an idea on why we should be interested in this?",
            "Now I'm going to talk about solutions strategies."
        ],
        [
            "Starting from perhaps the simplest setting, which is a simple, simplest class of problem which are sorry of solution strategies which approximal algorithms.",
            "In case you're not familiar with this already, let's suppose that we have.",
            "We want to solve to minimize the convex function.",
            "This is smooth and this might be not differentiable.",
            "Then we can define the proximity operator of this.",
            "Penalty term which is again not might not be differentiable and this proximity operator is the argument that minimizes this functional here which is given by the sum of this penalty plus a quadratic regularizer."
        ],
        [
            "Why we are interested in this?",
            "Because we can now derive a very simple scheme forward backward splitting, which is perhaps the simplest algorithm within the class of proxamol solution strategies.",
            "What we do basically is to compute to do a gradient step in the smooth bar and apply the prox operator on the outcome.",
            "There is simple there is."
        ],
        [
            "To code it, Skype is scalable and it can be accelerated to reach their optimal rate of convergence in the sense of of Nesterov on."
        ],
        [
            "Ever and it should be stressed with the CPU time, so the practical time required by solving such a problem depends on the global iteration complexity.",
            "So basically how difficult it is in fact to compute this prox operator turns out."
        ],
        [
            "For the case of nuclear norm, this is simple to be done in order to compute the prox operator at a given matrix X, we just compute the SVD and basically apply singular value software showing.",
            "So this is quite simple.",
            "For the case of composite nuclear norm, on the other hand, the setting is not as simple, so in general this type of penalty is not proximate approximable in the sense that one needs to solve an iterative scheme to come up with a solution of.",
            "As I mean to to find effects, the procs of a given.",
            "Metrics of a given parameter and turns out that."
        ],
        [
            "Even though this needs to be solved iteratively, we could under certain conditions still reach the optimal rate of convergence, but it's been quite some interest lately in in proving such results an.",
            "And also in some special cases we can still essentially compute this prox operator on with a finite number of simple steps.",
            "More precisely, if.",
            "They partition is based is made out of Singleton.",
            "So basically the parameter vector has the same dimension of a matrix.",
            "Then this works.",
            "So what we have to do is simply to compute the singular value of software showing operator.",
            "So essentially an SVD of this term here and then apply their joint or backward operator as I call it earlier of a mutation.",
            "But again this is this applies only for very special cases which are not so much.",
            "Interesting I would say."
        ],
        [
            "And I just want to mention our simple disease to be implemented in practice.",
            "This is Matlab codes to implement forward operator using linear indexing.",
            "This is just to show you that basically you can really code this strategy within few lines based upon again linear indexing within Matlab.",
            "Of course when you deal with a certain specific type of structures.",
            "So for instance.",
            "Anchor matrices then you would like to do it in a different way for the sake of efficiency, But this is very general.",
            "It applies to any partition of entries and it can be code very, very quickly.",
            "All."
        ],
        [
            "Right so.",
            "We are not entirely satisfied with solving the prox operator iteratively an on, especially because we can rely on well known strategies like the alternating direction method of multiplier.",
            "So it turns out that people have been considering a constraint with formulation which has a separable objective function and basically we want to do so again because we can rely on well known.",
            "And solvers like the alternating direction method of multiplier, which was in effect introduced in the 70s.",
            "It as root in the 50s and recently it's witnessed a renewed interest precisely because of machine learning application with large scale problems.",
            "Anne and Douglas Rachford, which is to some extent equivalent.",
            "This is also another way to solve this type of pro."
        ],
        [
            "For this specific case where we have nuclear norm, one is again to consider.",
            "At each iteration a singular value decomposition which was also considering the first talk of this session.",
            "AM."
        ],
        [
            "And the fact that the Adm is well started allows us to use essentially state of the art.",
            "Tolerances, and so I mean way in fact, to tune the parameters within this solution strategy.",
            "So essentially we can use adaptive tolerances and we have recipes to tune the augmented Lagrangian parameter in a clever way so that we essentially converge into a relatively small number of iteration.",
            "This has been considered at least for the case of system identification by Lou Italian in this recent paper.",
            "An the fact that we need at each iteration to compute an SVD might still be a problem.",
            "So what we consider instead."
        ],
        [
            "What we have considered instead is a reformulation which is non convex and it's based on a variational formulation of a nuclear norm.",
            "So this problem is now known."
        ],
        [
            "Mix due to this nonlinear equality constraint.",
            "An on the other entities move, so it's the objective is differentiable."
        ],
        [
            "An ants on top of that, we can constrain the size of the factors.",
            "So basically we can consider thing factors an save A lot of para meters.",
            "So basically reduce the number of parameters in victimization."
        ],
        [
            "Problem.",
            "It turns out, on top of vets that it involved a problem is nonconvex, at least for certain specific problems such as completion.",
            "One can show that the solution of this not well of actually not this problem, but related problem an essentially is corresponds to the solution of the nuclear norm based problem.",
            "So once we found und, the resulting matrix would solve also.",
            "And a problem where we use the nuclear norm.",
            "This holds at least it has been proved in this in this paper.",
            "For the case of completion.",
            "Here we have a different problem.",
            "Butts empirical evidence is that this still holds at least under certain conditions and."
        ],
        [
            "I will talk about it in a few slides.",
            "So all this works in practice.",
            "Again, we want to tackle this this optimization problem.",
            "We formulate an augmented Lagrangian an basically this is a man iteration.",
            "So we have to iteratively solve this into the para meters.",
            "So the original parameter, the factors and the dual variable.",
            "So Lambda sub mu is Lagrangian, with you being the Lagrangian parameter, the.",
            "Meet the environmental degradation.",
            "Turns out that if our empirical risk is quadratic function, then these three steps corresponds to solve a system of linear equation, so it's very easy to code this in fact."
        ],
        [
            "An let's now see how it works.",
            "So again we consider this this type of problem formulation within the context of system identification.",
            "We generate random inputs.",
            "We have a state.",
            "Well, consider randomly generated stable states based model.",
            "Anne.",
            "We were given order an we corrupt the outputs with Gaussian noise.",
            "We've given variance we fix.",
            "Notice that we fix their regularization parameter because our goal is in fact here not to see how it was.",
            "This works in terms of.",
            "The capability of finding the underlying dynamical system, but rather we want to characterize the solution of this nonconvex problem with respect to the solution of a convex one.",
            "So we potentially have something that works better by effect constraining the size of the matrices.",
            "We have something that is more suitable for large scale problem, so therefore the question is.",
            "Can we still get the same solution of a convex problem?"
        ],
        [
            "That we consider originally based upon the nuclear norm.",
            "So these are the metrics and considering in in doing the valuation, I'm looking at the attained value of objective function visibility that is basically a well this constraint is satisfied.",
            "Model fit is basically the distance we respect to the underlying model.",
            "Ann and CPU time.",
            "So model fits.",
            "You should consider again that here I'm not searching over the parameter space.",
            "So the idea is to see whether we get the same result in terms of model fit with respect to the nuclear norm problem rather than obtaining the best possible model fit which would follow from searching over parameter space via Sacra Solid Dacian.",
            "So here again, I want to stress I'm set, I'm fixing the parameters.",
            "So."
        ],
        [
            "When we first perform the experiments, we got very, very good results, which was kind of impressive.",
            "So we get basically from 5 to 7 time faster algorithm and we always get up to numerical precision.",
            "The same objective, same objective value, and the same.",
            "Feasibility so this is surprising because I want to stress that here I'm not restricting the size of the matrices, so I'm considering full factors and so even though we do not compute SVD, it is somewhat strange that we get such such an improvement."
        ],
        [
            "I should mention that the SVD based is in fact the implementation that is publicly available from the authors of this paper.",
            "And yesterday free is is our proposal.",
            "Without infecting this setting, at least constraining the sizes of a matrix turns out."
        ],
        [
            "However, by inspection of this SVD based algorithm that the reason why we get such an improvement is that they revealed or consider SVD instead of the economy size.",
            "So once you use the economy size basically here.",
            "So basically what I'm saying here is that improvement is essentially not due to the goodness of our approach here, but rather I mean if it's a minor problem with the coding of the SVD based approach."
        ],
        [
            "We never end.",
            "We can still constrain the metrics factors and we get quite a considerable improvement in this case, and so notice that.",
            "So this is the size of matrices and considering.",
            "CPU time is his second, the SVD.",
            "Econ is now the same as the algorithm consider earlier, but with economy size this is with full matrices, so we still get an improvement and this is by considering only 22 columns, so the underlying generating system is rank 18.",
            "I believe in this case.",
            "So basically we just allow for some extra room.",
            "Essentially we get again to the global optimum of up to numerical precision.",
            "They of convex problem even though we are dealing with a non convex formulation.",
            "And this works, but it works up to a certain noise level.",
            "Then it breaks down, breaks down, meaning that we if the noise is high enough, then we get basically stuck into a local minimum.",
            "I should also mention that these are in fact average results over 20 Monte Carlo iterations, so basically.",
            "Yeah, this is not the one shot result, but it's it's run over multiple realization.",
            "Not only it it breaks down in the sense that it gets stuck into a local minimum, but also as you can see, the difference is very small now.",
            "So essentially we get a compatible CPU time.",
            "The reason for this is essentially that we get an erratic behavior of a soldier."
        ],
        [
            "This gets even worse as soon as the noise gets quiet quiet.",
            "I so in a sense this is not entirely satisfactory for sure, even though I should mention that we still get quite an improvement even without constraining the size of the factors."
        ],
        [
            "So I'm going to conclude here what I presented is a class of structure low rank learning problems, in which I consider that the structure is encoded by what I call mutation, which is a specific type of linear operator.",
            "I gave application to system identification with missing data, and I talked about different solution strategies and in particular in over a non convex reformulation based on a certain variational.",
            "Factorization of a nuclear norm.",
            "There are of course a number of open problems.",
            "One is to further understand how this works in practice, especially, what is the role of a noise in getting close or not to the solution of origonal convex problem?",
            "Another interesting Avenue of research is to further exploit the structure of mutation, which are in fact the subset of the set of linear operators.",
            "So therefore it is expected that we can improve by relying on the specific nature of this of these operators, and then what I the reason why I looked into this in the 1st place was not within the context of system identification, but rather in the context of.",
            "Machine learning type of problems where one uses a certain composite penalty to impose a certain structure over para meters.",
            "For instance, in the context of multitask learning.",
            "So I believe that this could be used.",
            "This idea could be used to impose.",
            "Certain structure.",
            "Over parameters in general learning problems.",
            "And this Sir concludes my talk.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this talk is not going to be about tensors, but again about structured low rank matrix optimization problems.",
                    "label": 1
                },
                {
                    "sent": "And this is joint work.",
                    "label": 0
                },
                {
                    "sent": "We've Balkan, chevra Nanyang cycles.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the outline of my talk.",
                    "label": 0
                },
                {
                    "sent": "I will first start by talking about general setting in which I put into the context of learning a class of structured low rank matrix optimization problem.",
                    "label": 1
                },
                {
                    "sent": "Then I'll talk about specific problem formulation and application to system identification with missing data, which is pretty much what Constantine was talking about earlier.",
                    "label": 1
                },
                {
                    "sent": "But our approach is somewhat different.",
                    "label": 0
                },
                {
                    "sent": "Then talk about solutions, strategies, an discuss reformulations of the problem and present some experiments.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I would like I would like to start from from scratch.",
                    "label": 0
                },
                {
                    "sent": "The problem of finding a model from observational data.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Within the context of structuralism, Isation Varia is to construct nested subsets of increasingly convex complex iPod.",
                    "label": 0
                },
                {
                    "sent": "This is F.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In practice, we do so, but by defining a set which is define iponan.",
                    "label": 0
                },
                {
                    "sent": "The level set of a penalty function Omega.",
                    "label": 0
                },
                {
                    "sent": "For each of these sets, we solve infect penalized empirical discrimination problem in which in fact parimeter penalty parameter Lambda is 1 to one with this parameter that I have earlier and then at the end we find we pick the model that realises complexity.",
                    "label": 0
                },
                {
                    "sent": "Fidelity trade off.",
                    "label": 0
                },
                {
                    "sent": "And this is our final final model.",
                    "label": 0
                },
                {
                    "sent": "So within this framework and actually also with in other frameworks of learning from examples, it is quite crucial to design the structure of this nested sequence publicly, and this amount in fact that choosing the penalty penalty function.",
                    "label": 0
                },
                {
                    "sent": "And there's been quite some work in the last two decades, in effect coming out with appropriate penalties in.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The contest of structured sparsity.",
                    "label": 0
                },
                {
                    "sent": "Or if you want structure inducing penalties, the simplest case is perhaps the L1 penalty, which has been popularized by the socola Sue and the prior knowledge is that parameter vector should be sparse, and then we solve this convex problem.",
                    "label": 0
                },
                {
                    "sent": "This is I guess you are already familiar with.",
                    "label": 0
                },
                {
                    "sent": "It.",
                    "label": 0
                },
                {
                    "sent": "VL one or the one norm?",
                    "label": 0
                },
                {
                    "sent": "An other setting where structured sparsity arises within multi task learning.",
                    "label": 0
                },
                {
                    "sent": "And then massive until was talking about this yesterday.",
                    "label": 0
                },
                {
                    "sent": "The idea is that we have a capital T parameter vectors which relates to learning tasks.",
                    "label": 0
                },
                {
                    "sent": "And then there's a comment.",
                    "label": 0
                },
                {
                    "sent": "So then the line assumption is that there's a common dictionary.",
                    "label": 0
                },
                {
                    "sent": "Basically, there's a bulk of common features that are shared across this different learning tasks.",
                    "label": 0
                },
                {
                    "sent": "In this case, it has been proposed to consider as a penalty the nuclear norm, which is a proxy for rank function.",
                    "label": 0
                },
                {
                    "sent": "And it's defined as a sum of singular values, so this was also already discussed in in previous talks during the workshop.",
                    "label": 0
                },
                {
                    "sent": "These are.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are somewhat simple penalty as you can see they are defined based upon norms.",
                    "label": 0
                },
                {
                    "sent": "It turns out that there's quite some interest in more complex penalty that involve a composition of functions, so we have, for instance, enorme composed with linear transformation.",
                    "label": 0
                },
                {
                    "sent": "This is the case for instance in the so called fused lasso, in which we basically assume that the parameter to be learn should have small number of changes.",
                    "label": 0
                },
                {
                    "sent": "So we have, let's say capital P entries and we want to observe as a small number of changes as possible.",
                    "label": 0
                },
                {
                    "sent": "And basically we can encode this structure by means of a matrix and require that the after being linearly transformed by a the parameter vector is sparse.",
                    "label": 0
                },
                {
                    "sent": "So basically the sparsity here is to be understood after linear transformation.",
                    "label": 0
                },
                {
                    "sent": "In this setting, an upper case concerning the nuclear norm this time.",
                    "label": 0
                },
                {
                    "sent": "Is when we have a linear transformation of a parameter matrix which is in fact encoded by these two matrices A&B and this has been considered quite extensively in multitask learning, collaborative filtering and also in infecting system identification so.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And these problems are usually more difficult to to solve an and the problem I'm going to talk about it is precisely we belongs precisely to this to this class of problem with composite penalties.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The goal is pretty much the same as in constant in stock.",
                    "label": 0
                },
                {
                    "sent": "We want to learn from observational data and metrics that in addition to being low rank as entered partitioning to known disjoint groups.",
                    "label": 1
                },
                {
                    "sent": "So here I'm considering not general linear transformation, but the specific class.",
                    "label": 0
                },
                {
                    "sent": "This is the type of problem formulation we consider we have here in empirical risk and a penalty which is a composite spectral penalty spectral because, well, it's a nuclear norm.",
                    "label": 0
                },
                {
                    "sent": "So it depends upon the singular values of the matrix obtained by transforming the parameter W. This linear operator, which we call a mutation, and I'm going to give some more details in a minute as I, as I mentioned already, we use the nuclear norm.",
                    "label": 0
                },
                {
                    "sent": "As a proxy for the rank, so this makes this problem convex, assuming that this term is also convex, which is usually the case and can be turned into an SDP.",
                    "label": 0
                },
                {
                    "sent": "But we don't want to do this because we want to exploit the structure and deal with large scale problems.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What is this simulation I was talking about?",
                    "label": 0
                },
                {
                    "sent": "As I mentioned, we are, we assume we are assuming that we have entries partition into disjoint sets, so they form a partition and we can define a membership function that Maps entries into the appropriate set indexed by hell and then we have a mutation operator which I call forward to be distinguished with backward which is the joint.",
                    "label": 0
                },
                {
                    "sent": "So the adjoint of this operator I will call it.",
                    "label": 0
                },
                {
                    "sent": "Backward an and what this does is to map a parameter vector into a structure metrics depending upon the membership function.",
                    "label": 0
                },
                {
                    "sent": "So if you're lost into the formulas, you can look at this simple picture here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'm assuming I have two dimensional parameter vector with just two entries and IMAP these entries into two different sets that forms a partition.",
                    "label": 0
                },
                {
                    "sent": "So you see the entries in red and the entries in yellow.",
                    "label": 0
                },
                {
                    "sent": "So these are in two different sets.",
                    "label": 0
                },
                {
                    "sent": "That forms a partition.",
                    "label": 0
                },
                {
                    "sent": "So this is a type of structured I'm I'm interested in.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then one possible application is system identification.",
                    "label": 0
                },
                {
                    "sent": "General idea is to find a dynamical model from observational input output signals and the nuclear norm has been used extensively in the last, say one decade.",
                    "label": 0
                },
                {
                    "sent": "It has been motivated by well known subspace properties and constant is already discussed.",
                    "label": 0
                },
                {
                    "sent": "This a bit an also not just nuclear norm but wait in nuclear norm in the sense that.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "I introduced earlier, so we have weights and this weights in the context of system identification are related to instrumental variables.",
                    "label": 0
                },
                {
                    "sent": "And typically one observes improvement but modest relatively modest improvement with respect to classical subspace based algorithms that were developed in the 80s.",
                    "label": 0
                },
                {
                    "sent": "On the other end, the nuclear norm has been shown to be quite effective for completion problems.",
                    "label": 0
                },
                {
                    "sent": "By now it is it has been used quite extensively.",
                    "label": 0
                },
                {
                    "sent": "It appeared in many papers and the idea is that when we have missing input and output.",
                    "label": 0
                },
                {
                    "sent": "System identification problem.",
                    "label": 0
                },
                {
                    "sent": "We can essentially use a structural rank metrics optimization problem as I introduced it earlier to infect learning dynamical model under under missing data.",
                    "label": 0
                },
                {
                    "sent": "So basically reconstruct input and output an in a second stage.",
                    "label": 0
                },
                {
                    "sent": "We can find system matrices by means of simple algebraic steps.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is where this arises from.",
                    "label": 0
                },
                {
                    "sent": "So, or at least a specific instance.",
                    "label": 0
                },
                {
                    "sent": "Let's assume that we have an underlying state space model with order and sybex where answer backs is in fact the dimensionality of a state vector.",
                    "label": 0
                },
                {
                    "sent": "It turns out that under certain conditions we can form.",
                    "label": 0
                },
                {
                    "sent": "So this is the block ankle metrics which was already described by constantly.",
                    "label": 0
                },
                {
                    "sent": "So I'm not going to talk about it.",
                    "label": 0
                },
                {
                    "sent": "This is an ankle operator applied to Vector now.",
                    "label": 0
                },
                {
                    "sent": "Sorry, actually it's it's a matrix.",
                    "label": 0
                },
                {
                    "sent": "In fact, in the case where we have multiple outputs or inputs.",
                    "label": 0
                },
                {
                    "sent": "Ann, but you can think of it as a vectorized version of it.",
                    "label": 0
                },
                {
                    "sent": "So basically we are interested in this linear operator that Maps this yuan Y into concatenation of block Hankel matrices and turns out that under this generating mechanism the rank of this matrix now is equal to the rank of the input ankle block anchor metrics plus.",
                    "label": 0
                },
                {
                    "sent": "Order of assistance.",
                    "label": 0
                },
                {
                    "sent": "So the dimensionality of a state vector and we can effect rely on this in order to derive formulation for system identification.",
                    "label": 0
                },
                {
                    "sent": "This is being considered.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bayou ET al.",
                    "label": 0
                },
                {
                    "sent": "Recently they saw this type of.",
                    "label": 0
                },
                {
                    "sent": "Problem which basically tries to reconstruct a set of inputs and outputs based upon sampling.",
                    "label": 0
                },
                {
                    "sent": "So we have observed measurements from inputs and outputs and we try to map them into the.",
                    "label": 0
                },
                {
                    "sent": "Inputs and output to be found provided that.",
                    "label": 0
                },
                {
                    "sent": "So I mean using as prior that essentially the generating mechanism should be as simple as possible, so the dynamics should be such that the states we underline state vector is small essentially, and as you can see, this is basically a structured low rank matrix optimization problem in the sense I described earlier.",
                    "label": 0
                },
                {
                    "sent": "In particular, this operator is basically a mutation now in in the sense of my definition that I gave earlier.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now how do we this is effect a specific application to give you an idea on why we should be interested in this?",
                    "label": 0
                },
                {
                    "sent": "Now I'm going to talk about solutions strategies.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Starting from perhaps the simplest setting, which is a simple, simplest class of problem which are sorry of solution strategies which approximal algorithms.",
                    "label": 0
                },
                {
                    "sent": "In case you're not familiar with this already, let's suppose that we have.",
                    "label": 0
                },
                {
                    "sent": "We want to solve to minimize the convex function.",
                    "label": 0
                },
                {
                    "sent": "This is smooth and this might be not differentiable.",
                    "label": 0
                },
                {
                    "sent": "Then we can define the proximity operator of this.",
                    "label": 0
                },
                {
                    "sent": "Penalty term which is again not might not be differentiable and this proximity operator is the argument that minimizes this functional here which is given by the sum of this penalty plus a quadratic regularizer.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Why we are interested in this?",
                    "label": 0
                },
                {
                    "sent": "Because we can now derive a very simple scheme forward backward splitting, which is perhaps the simplest algorithm within the class of proxamol solution strategies.",
                    "label": 0
                },
                {
                    "sent": "What we do basically is to compute to do a gradient step in the smooth bar and apply the prox operator on the outcome.",
                    "label": 0
                },
                {
                    "sent": "There is simple there is.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To code it, Skype is scalable and it can be accelerated to reach their optimal rate of convergence in the sense of of Nesterov on.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ever and it should be stressed with the CPU time, so the practical time required by solving such a problem depends on the global iteration complexity.",
                    "label": 0
                },
                {
                    "sent": "So basically how difficult it is in fact to compute this prox operator turns out.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the case of nuclear norm, this is simple to be done in order to compute the prox operator at a given matrix X, we just compute the SVD and basically apply singular value software showing.",
                    "label": 0
                },
                {
                    "sent": "So this is quite simple.",
                    "label": 0
                },
                {
                    "sent": "For the case of composite nuclear norm, on the other hand, the setting is not as simple, so in general this type of penalty is not proximate approximable in the sense that one needs to solve an iterative scheme to come up with a solution of.",
                    "label": 0
                },
                {
                    "sent": "As I mean to to find effects, the procs of a given.",
                    "label": 0
                },
                {
                    "sent": "Metrics of a given parameter and turns out that.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Even though this needs to be solved iteratively, we could under certain conditions still reach the optimal rate of convergence, but it's been quite some interest lately in in proving such results an.",
                    "label": 0
                },
                {
                    "sent": "And also in some special cases we can still essentially compute this prox operator on with a finite number of simple steps.",
                    "label": 0
                },
                {
                    "sent": "More precisely, if.",
                    "label": 0
                },
                {
                    "sent": "They partition is based is made out of Singleton.",
                    "label": 0
                },
                {
                    "sent": "So basically the parameter vector has the same dimension of a matrix.",
                    "label": 0
                },
                {
                    "sent": "Then this works.",
                    "label": 0
                },
                {
                    "sent": "So what we have to do is simply to compute the singular value of software showing operator.",
                    "label": 0
                },
                {
                    "sent": "So essentially an SVD of this term here and then apply their joint or backward operator as I call it earlier of a mutation.",
                    "label": 0
                },
                {
                    "sent": "But again this is this applies only for very special cases which are not so much.",
                    "label": 0
                },
                {
                    "sent": "Interesting I would say.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I just want to mention our simple disease to be implemented in practice.",
                    "label": 0
                },
                {
                    "sent": "This is Matlab codes to implement forward operator using linear indexing.",
                    "label": 0
                },
                {
                    "sent": "This is just to show you that basically you can really code this strategy within few lines based upon again linear indexing within Matlab.",
                    "label": 0
                },
                {
                    "sent": "Of course when you deal with a certain specific type of structures.",
                    "label": 0
                },
                {
                    "sent": "So for instance.",
                    "label": 0
                },
                {
                    "sent": "Anchor matrices then you would like to do it in a different way for the sake of efficiency, But this is very general.",
                    "label": 0
                },
                {
                    "sent": "It applies to any partition of entries and it can be code very, very quickly.",
                    "label": 0
                },
                {
                    "sent": "All.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "We are not entirely satisfied with solving the prox operator iteratively an on, especially because we can rely on well known strategies like the alternating direction method of multiplier.",
                    "label": 0
                },
                {
                    "sent": "So it turns out that people have been considering a constraint with formulation which has a separable objective function and basically we want to do so again because we can rely on well known.",
                    "label": 0
                },
                {
                    "sent": "And solvers like the alternating direction method of multiplier, which was in effect introduced in the 70s.",
                    "label": 0
                },
                {
                    "sent": "It as root in the 50s and recently it's witnessed a renewed interest precisely because of machine learning application with large scale problems.",
                    "label": 0
                },
                {
                    "sent": "Anne and Douglas Rachford, which is to some extent equivalent.",
                    "label": 0
                },
                {
                    "sent": "This is also another way to solve this type of pro.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For this specific case where we have nuclear norm, one is again to consider.",
                    "label": 0
                },
                {
                    "sent": "At each iteration a singular value decomposition which was also considering the first talk of this session.",
                    "label": 0
                },
                {
                    "sent": "AM.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the fact that the Adm is well started allows us to use essentially state of the art.",
                    "label": 0
                },
                {
                    "sent": "Tolerances, and so I mean way in fact, to tune the parameters within this solution strategy.",
                    "label": 0
                },
                {
                    "sent": "So essentially we can use adaptive tolerances and we have recipes to tune the augmented Lagrangian parameter in a clever way so that we essentially converge into a relatively small number of iteration.",
                    "label": 0
                },
                {
                    "sent": "This has been considered at least for the case of system identification by Lou Italian in this recent paper.",
                    "label": 0
                },
                {
                    "sent": "An the fact that we need at each iteration to compute an SVD might still be a problem.",
                    "label": 0
                },
                {
                    "sent": "So what we consider instead.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we have considered instead is a reformulation which is non convex and it's based on a variational formulation of a nuclear norm.",
                    "label": 0
                },
                {
                    "sent": "So this problem is now known.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mix due to this nonlinear equality constraint.",
                    "label": 0
                },
                {
                    "sent": "An on the other entities move, so it's the objective is differentiable.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An ants on top of that, we can constrain the size of the factors.",
                    "label": 0
                },
                {
                    "sent": "So basically we can consider thing factors an save A lot of para meters.",
                    "label": 0
                },
                {
                    "sent": "So basically reduce the number of parameters in victimization.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem.",
                    "label": 0
                },
                {
                    "sent": "It turns out, on top of vets that it involved a problem is nonconvex, at least for certain specific problems such as completion.",
                    "label": 0
                },
                {
                    "sent": "One can show that the solution of this not well of actually not this problem, but related problem an essentially is corresponds to the solution of the nuclear norm based problem.",
                    "label": 0
                },
                {
                    "sent": "So once we found und, the resulting matrix would solve also.",
                    "label": 0
                },
                {
                    "sent": "And a problem where we use the nuclear norm.",
                    "label": 0
                },
                {
                    "sent": "This holds at least it has been proved in this in this paper.",
                    "label": 0
                },
                {
                    "sent": "For the case of completion.",
                    "label": 0
                },
                {
                    "sent": "Here we have a different problem.",
                    "label": 0
                },
                {
                    "sent": "Butts empirical evidence is that this still holds at least under certain conditions and.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I will talk about it in a few slides.",
                    "label": 0
                },
                {
                    "sent": "So all this works in practice.",
                    "label": 0
                },
                {
                    "sent": "Again, we want to tackle this this optimization problem.",
                    "label": 0
                },
                {
                    "sent": "We formulate an augmented Lagrangian an basically this is a man iteration.",
                    "label": 0
                },
                {
                    "sent": "So we have to iteratively solve this into the para meters.",
                    "label": 0
                },
                {
                    "sent": "So the original parameter, the factors and the dual variable.",
                    "label": 0
                },
                {
                    "sent": "So Lambda sub mu is Lagrangian, with you being the Lagrangian parameter, the.",
                    "label": 0
                },
                {
                    "sent": "Meet the environmental degradation.",
                    "label": 0
                },
                {
                    "sent": "Turns out that if our empirical risk is quadratic function, then these three steps corresponds to solve a system of linear equation, so it's very easy to code this in fact.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An let's now see how it works.",
                    "label": 0
                },
                {
                    "sent": "So again we consider this this type of problem formulation within the context of system identification.",
                    "label": 0
                },
                {
                    "sent": "We generate random inputs.",
                    "label": 0
                },
                {
                    "sent": "We have a state.",
                    "label": 0
                },
                {
                    "sent": "Well, consider randomly generated stable states based model.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "We were given order an we corrupt the outputs with Gaussian noise.",
                    "label": 0
                },
                {
                    "sent": "We've given variance we fix.",
                    "label": 0
                },
                {
                    "sent": "Notice that we fix their regularization parameter because our goal is in fact here not to see how it was.",
                    "label": 0
                },
                {
                    "sent": "This works in terms of.",
                    "label": 0
                },
                {
                    "sent": "The capability of finding the underlying dynamical system, but rather we want to characterize the solution of this nonconvex problem with respect to the solution of a convex one.",
                    "label": 0
                },
                {
                    "sent": "So we potentially have something that works better by effect constraining the size of the matrices.",
                    "label": 0
                },
                {
                    "sent": "We have something that is more suitable for large scale problem, so therefore the question is.",
                    "label": 0
                },
                {
                    "sent": "Can we still get the same solution of a convex problem?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That we consider originally based upon the nuclear norm.",
                    "label": 0
                },
                {
                    "sent": "So these are the metrics and considering in in doing the valuation, I'm looking at the attained value of objective function visibility that is basically a well this constraint is satisfied.",
                    "label": 0
                },
                {
                    "sent": "Model fit is basically the distance we respect to the underlying model.",
                    "label": 0
                },
                {
                    "sent": "Ann and CPU time.",
                    "label": 0
                },
                {
                    "sent": "So model fits.",
                    "label": 0
                },
                {
                    "sent": "You should consider again that here I'm not searching over the parameter space.",
                    "label": 0
                },
                {
                    "sent": "So the idea is to see whether we get the same result in terms of model fit with respect to the nuclear norm problem rather than obtaining the best possible model fit which would follow from searching over parameter space via Sacra Solid Dacian.",
                    "label": 0
                },
                {
                    "sent": "So here again, I want to stress I'm set, I'm fixing the parameters.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When we first perform the experiments, we got very, very good results, which was kind of impressive.",
                    "label": 0
                },
                {
                    "sent": "So we get basically from 5 to 7 time faster algorithm and we always get up to numerical precision.",
                    "label": 0
                },
                {
                    "sent": "The same objective, same objective value, and the same.",
                    "label": 0
                },
                {
                    "sent": "Feasibility so this is surprising because I want to stress that here I'm not restricting the size of the matrices, so I'm considering full factors and so even though we do not compute SVD, it is somewhat strange that we get such such an improvement.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I should mention that the SVD based is in fact the implementation that is publicly available from the authors of this paper.",
                    "label": 0
                },
                {
                    "sent": "And yesterday free is is our proposal.",
                    "label": 0
                },
                {
                    "sent": "Without infecting this setting, at least constraining the sizes of a matrix turns out.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, by inspection of this SVD based algorithm that the reason why we get such an improvement is that they revealed or consider SVD instead of the economy size.",
                    "label": 0
                },
                {
                    "sent": "So once you use the economy size basically here.",
                    "label": 0
                },
                {
                    "sent": "So basically what I'm saying here is that improvement is essentially not due to the goodness of our approach here, but rather I mean if it's a minor problem with the coding of the SVD based approach.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We never end.",
                    "label": 0
                },
                {
                    "sent": "We can still constrain the metrics factors and we get quite a considerable improvement in this case, and so notice that.",
                    "label": 0
                },
                {
                    "sent": "So this is the size of matrices and considering.",
                    "label": 0
                },
                {
                    "sent": "CPU time is his second, the SVD.",
                    "label": 0
                },
                {
                    "sent": "Econ is now the same as the algorithm consider earlier, but with economy size this is with full matrices, so we still get an improvement and this is by considering only 22 columns, so the underlying generating system is rank 18.",
                    "label": 0
                },
                {
                    "sent": "I believe in this case.",
                    "label": 0
                },
                {
                    "sent": "So basically we just allow for some extra room.",
                    "label": 0
                },
                {
                    "sent": "Essentially we get again to the global optimum of up to numerical precision.",
                    "label": 0
                },
                {
                    "sent": "They of convex problem even though we are dealing with a non convex formulation.",
                    "label": 0
                },
                {
                    "sent": "And this works, but it works up to a certain noise level.",
                    "label": 0
                },
                {
                    "sent": "Then it breaks down, breaks down, meaning that we if the noise is high enough, then we get basically stuck into a local minimum.",
                    "label": 0
                },
                {
                    "sent": "I should also mention that these are in fact average results over 20 Monte Carlo iterations, so basically.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is not the one shot result, but it's it's run over multiple realization.",
                    "label": 0
                },
                {
                    "sent": "Not only it it breaks down in the sense that it gets stuck into a local minimum, but also as you can see, the difference is very small now.",
                    "label": 0
                },
                {
                    "sent": "So essentially we get a compatible CPU time.",
                    "label": 0
                },
                {
                    "sent": "The reason for this is essentially that we get an erratic behavior of a soldier.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This gets even worse as soon as the noise gets quiet quiet.",
                    "label": 0
                },
                {
                    "sent": "I so in a sense this is not entirely satisfactory for sure, even though I should mention that we still get quite an improvement even without constraining the size of the factors.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to conclude here what I presented is a class of structure low rank learning problems, in which I consider that the structure is encoded by what I call mutation, which is a specific type of linear operator.",
                    "label": 0
                },
                {
                    "sent": "I gave application to system identification with missing data, and I talked about different solution strategies and in particular in over a non convex reformulation based on a certain variational.",
                    "label": 0
                },
                {
                    "sent": "Factorization of a nuclear norm.",
                    "label": 0
                },
                {
                    "sent": "There are of course a number of open problems.",
                    "label": 0
                },
                {
                    "sent": "One is to further understand how this works in practice, especially, what is the role of a noise in getting close or not to the solution of origonal convex problem?",
                    "label": 0
                },
                {
                    "sent": "Another interesting Avenue of research is to further exploit the structure of mutation, which are in fact the subset of the set of linear operators.",
                    "label": 0
                },
                {
                    "sent": "So therefore it is expected that we can improve by relying on the specific nature of this of these operators, and then what I the reason why I looked into this in the 1st place was not within the context of system identification, but rather in the context of.",
                    "label": 0
                },
                {
                    "sent": "Machine learning type of problems where one uses a certain composite penalty to impose a certain structure over para meters.",
                    "label": 0
                },
                {
                    "sent": "For instance, in the context of multitask learning.",
                    "label": 0
                },
                {
                    "sent": "So I believe that this could be used.",
                    "label": 0
                },
                {
                    "sent": "This idea could be used to impose.",
                    "label": 0
                },
                {
                    "sent": "Certain structure.",
                    "label": 0
                },
                {
                    "sent": "Over parameters in general learning problems.",
                    "label": 0
                },
                {
                    "sent": "And this Sir concludes my talk.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}