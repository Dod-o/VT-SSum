{
    "id": "et3oziqbgpp6sopmvxsggovokluw5nap",
    "title": "Near-Optimal Herding",
    "info": {
        "author": [
            "Samira Samadi, Department of Computer Science, University of British Columbia"
        ],
        "published": "July 15, 2014",
        "recorded": "June 2014",
        "category": [
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->Unsupervised Learning",
            "Top->Computer Science->Machine Learning->Supervised Learning",
            "Top->Computer Science->Machine Learning->Statistical Learning",
            "Top->Computer Science->Machine Learning->On-line Learning"
        ]
    },
    "url": "http://videolectures.net/colt2014_samadi_herding/",
    "segmentation": [
        [
            "I want to talk about our new algorithm, which we call near optimal herding.",
            "This is a joint work with Nick Harvey from University of British Columbia."
        ],
        [
            "So at the beginning I want to give you an intuition of where this name is coming from.",
            "So in our work we define a very fundamental question that asks how can we do an efficient sampling.",
            "So we call this problem sampling problem and we analyze any possible solution for it.",
            "And it turns out that inheriting which is an algorithm of recent interest in machine learning community gives a solution to our problem.",
            "So our goal is to rigorously analyze this problem and in this direction we are able to give a near optimal solution for it."
        ],
        [
            "OK, So what is the question?",
            "The machine learning community asks?",
            "Suppose that you have a set of samples from a model and you want to generate more samples from it.",
            "So the classical approach is to use maximum likelihood estimation to infer permits of the model.",
            "And once you have estimated the model, Now you can draw more samples from it.",
            "But this maximum likelihood estimation step is expensive, so we want something better.",
            "And here is where herding comes, so it avoids this difficulty by directly generating samples.",
            "OK, let's get back to our goal.",
            "So we have a set of samples and we want to generate more samples.",
            "We formalize it as sampling problem.",
            "So we are given a set X in Rd with mean mu and we want to generate an infinite sequence of points from X such that for every T, the Euclidean distance between the empirical mean of the first T points and the original mean, you would always be bounded by Alpha over TO Alpha.",
            "Here is a quantity that can depend on.",
            "Geometry of X and dimension of the space, but it shouldn't depend on T. So we call this term error and our goal is to find the best Alpha as a function of D that can fits here.",
            "Let's see an example of how different sampling algorithms can work.",
            "Suppose that we have 12 set of points with their with their mean at the center and a simple sampling algorithm can work like this."
        ],
        [
            "I choose this point as X1."
        ],
        [
            "Mew would be at the same place.",
            "That"
        ],
        [
            "Jason Point as X2."
        ],
        [
            "Mew with."
        ],
        [
            "Two year.",
            "And."
        ],
        [
            "The same path."
        ],
        [
            "So after getting 5 samples.",
            "The distance between the empirical mean and the original mean would be something like this, which is big.",
            "But if we do our sampling in a more clever way."
        ],
        [
            "I choose the same point as X."
        ],
        [
            "1."
        ],
        [
            "Meal would be at the same place, but this time I choose."
        ],
        [
            "It'll point to X1 as X2."
        ],
        [
            "So mu."
        ],
        [
            "It goes to here and."
        ],
        [
            "So on."
        ],
        [
            "So first."
        ],
        [
            "Extra."
        ],
        [
            "Re explore"
        ],
        [
            "Or"
        ],
        [
            "Expire."
        ],
        [
            "Five and after."
        ],
        [
            "Adding five samples in this new algorithm, the distance between the empirical mean and the original mean is much smaller."
        ],
        [
            "Let's first set the assumptions that I'm going to use throughout the paper we.",
            "We assume that we have a set of points X in Rd with mean mu.",
            "So this is the convex Hull of set X.",
            "And also we assume that said X lies in the Euclidean ball of radius Delta.",
            "And also let R be the radius of the biggest ball that is contained inside the convex Hull effects.",
            "So in our scenario, we assume that set X is finite with size N. And also for simplicity we set mu to be equal to 0 and we also assume that the two norm of all the points in X is smaller than one.",
            "OK, so.",
            "I first want to give you a big picture of our main results.",
            "So if we change the previous result setting to our geometrical setting, it can be seen that the error for hurting self sampling problem and it gives Alpha smaller than D under some assumptions that I will talk later.",
            "We proved that any algorithm that solves sampling problem cannot do better than Alpha Square root of T. Furthermore, we propose an algorithm that solves sampling problem with Alpha smaller than a square root of the sum log factors of N. So if we assume that the set X is not too big, then I can ignore lock factors here and our algorithm is getting the optimal."
        ],
        [
            "OK, remember that I defined error to be the two norm of.",
            "All the empirical means.",
            "As a function of T and if I have a solution to sampling problem, it guarantees that this error is always a smaller than Alpha over T, so I can redefine error as supermom of all the partial sums for all the values T and so now on I ignore about this tea and talk about her.",
            "Like this?",
            "OK, So what is the most natural idea for sampling?",
            "This is random sampling.",
            "It says get IID uniform samples from set X.",
            "Standard analysis showed that the error for random sampling decreases by ratio 1 / sqrt T. So it's a very simple algorithm, but unfortunately it is not solving our sampling problem because in sampling problem we want the error to decrease by ratio 1 / T. The next natural idea is kind of a greedy algorithm, and that is what's hurting is doing, so it's these two lines of code.",
            "It maintains a way to assure W, and at each step it.",
            "Computes the maximum of the inner product and set it to be the next sample X = 1.",
            "This code may remind some of you of the perceptron perceptron boundedness theorem was first introduced by block and live in in 1970, and it says that for a big class of algorithms that do a kind of sampling and have a common property, you can say that their error is always bounded by Alpha.",
            "So it's not difficult to see that hurting also belongs to that class of algorithms, and so perceptron bounded.",
            "This implies that the error for hurting is also bounded by Alpha.",
            "Alpha is.",
            "You see, at the top Alpha is this quantity that can depend on geometry of X and the dimension, and but it cannot depend on T. So it's a constant.",
            "OK, but unfortunately the since this perceptron boundedness is a difficult and very general theorem.",
            "The proof is not constructive and it doesn't give any explicit bound on Alpha.",
            "But if we analyze hurting algorithms specifically, it has been shown that error for hurting is bound."
        ],
        [
            "Did buy.",
            "The two norm of W not plus 1 / R and remember that our was the radius of the biggest ball that is contained inside the convex Hull effects.",
            "And W not is the initial way to actor that is chosen by the algorithm, so it's usually set to be 0 and mu.",
            "So since the two norm of W not is low order, I can ignore it and say that error for hurting is order of 1 / R. So generally speaking, these are can get very tiny and run over arc and gets very big.",
            "But if we assume that our body which in our scenario is the convex Hull, affects is somehow nicer round, which is precisely defined by John's position, then it guarantees that 1 / R is smaller than D. So we say that the body is in John's position if the biggest ellipsoid that is contained inside our body is a factor of the unit ball.",
            "As an example, this body is not in John's position.",
            "But this part is."
        ],
        [
            "So under these assumptions, we have that 1 / R is smaller than D and it implies that error for hurting is bounded by D and this is the D factor that I mentioned earlier.",
            "So so far we have seen that."
        ],
        [
            "Running gives a solution to sampling problem with Alpha.",
            "Order of the OK Now what is our approach for solving sampling problem?",
            "So our insight is that sampling problem is connected to ideas in discrepancy theory.",
            "And by building this connection between sampling problem and discrepancy theory, we would be able to.",
            "Use the world structured theorems and results in this field.",
            "OK, so we introduce permutation problem an instead of solving sampling problem, we focus on solving permutation problem.",
            "And.",
            "The interesting fact is that permutation problem is connected to some famous theorems in discrepancy.",
            "So remember that I defined sampling problem as following.",
            "We were given the set X with the same assumptions and we want to find an infinite sequence of points in X such that.",
            "The Super mom of.",
            "All the partial sums would always be bounded by Alpha.",
            "Now we define permutation problem as follows.",
            "We are given the set the same set X.",
            "This time we want to find an ordering of elements of X such that the Super Bowl of all the partial sums over disordering is always bounded by Alpha.",
            "It's not difficult to see that if you have a solution to permutation problem, it gives you a solution to sampling problem with the same paraments Alpha.",
            "So now on we just focus on permutation problem."
        ],
        [
            "OK.",
            "So.",
            "Uh.",
            "One of the theorems that is very connected to permutation problem is Astinus lemma.",
            "It says that if you are given the set X with the same assumptions, then there is an ordering of elements of X such that for every T the two norm of all the partial sums over this ordering is always bounded by the dimension D. So is Sinus lemma tell you that you can get D for the permutation problem and it has been a major open question since 1931.",
            "If you can improve this D 2 sqrt T and if it's true then it gives you a solution to sampling problem with Alpha equal to square root of T. So we do not solve this open question unfortunately.",
            "But we give another algorithm that does not exactly get the square root of T but gets very close to it.",
            "OK, so for those of you who are not familiar with discrepancy, I should give you more details to explain our algorithm so."
        ],
        [
            "Let BP and BQ be unit balls of Norm P&Q in Rd.",
            "We define St of PPNBQST is coming from this dynast constant in a few seconds you will see the relationship.",
            "And is defined as the smallest Lambda such that for every sequence of points in week MBQ with mean equal to zero, there is an ordering of these elements such that all the partial sums for is for.",
            "This ordering is bounded by Lambda.",
            "So."
        ],
        [
            "To remind you of what is sinus, similar lemma is saying it says that given this at X we can always find an ordering of elements of X such that.",
            "This partial sums is always bounded by T. So."
        ],
        [
            "Sinus lemma implies that St of B2 and B2 is bounded by D."
        ],
        [
            "The next thing that we define is SV of BP, and BQ SV is aggravation of sine vectors.",
            "And is defined as the smallest Lambda such that for every sequence of points in BQ there is this plus 1 -- 1 such that this signed vector is always bounded by Lambda.",
            "So in other words, SV is saying that.",
            "Divide your set X into 2.",
            "Two sets such that when you.",
            "Compute the sum of elements of the first set.",
            "It shouldn't be very far from the sum of the elements of the other set.",
            "As a simple."
        ],
        [
            "Sample suppose that we have a set of points in our space and their approximate copies.",
            "So the best way to choose this plus one minus ones too.",
            "Have a small Lambda here for this specific set of points is."
        ],
        [
            "Set plus one to the black points or the other way."
        ],
        [
            "Round and minus one to their copies."
        ],
        [
            "The last thing."
        ],
        [
            "That we need is SS of PPNBQSS is abbreviation of science series, so SS is exactly defined.",
            "Uh.",
            "As SV by the difference that we have a quantifier for all K in N. So in SV we wanted to have a control over the sign Victor, but in SS we want to have a control over all the signed partial sums.",
            "So SS in away is a harder version of SV.",
            "So our intuition is that giving you bound anesti is harder than giving you bound on SS, and it's harder than giving abound on SP.",
            "So in order to.",
            "Have our algorithm.",
            "We start with SV.",
            "We give a bound for SV and then we improve it to bound for St finally.",
            "OK, so."
        ],
        [
            "This new setting.",
            "What is our goal?",
            "Our goal is to prove that St of B2 and B2 is bounded by the square root of the log factors of N. We start by SV.",
            "One can use partial coloring lemma, which is a recent breakthrough in discrepancy theory.",
            "To show that SV of B Infinity and B2 is bounded by log and so it is already known.",
            "Afterward, we show an algorithmic reduction from SS to SV and it enables us to show that SS of B, Infinity, and V2 is bounded by some log factors event and this extra log factors that you see is.",
            "The cost of the algorithmic reduction.",
            "So there is a classic theorem, Chobanian theorem, that connects SS&ST, but it's not constructive, so we give an algorithmic version of Chobanian to show that St is always bounded by SS.",
            "And finally, by applying some norm inequality's we get we get our final result.",
            "So I my final note is that these quantities SVS is an SDR well, as studied in discrepancy theory.",
            "Just to give you an example, big feel approved at FB Infinity and be one is bounded by two and also it has been a major open question if you can prove that SV of B Infinity and B2 is order of 1.",
            "So this is the end of my talk.",
            "Thanks everyone for listening.",
            "And if you have any questions I'm happy."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I want to talk about our new algorithm, which we call near optimal herding.",
                    "label": 0
                },
                {
                    "sent": "This is a joint work with Nick Harvey from University of British Columbia.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So at the beginning I want to give you an intuition of where this name is coming from.",
                    "label": 0
                },
                {
                    "sent": "So in our work we define a very fundamental question that asks how can we do an efficient sampling.",
                    "label": 0
                },
                {
                    "sent": "So we call this problem sampling problem and we analyze any possible solution for it.",
                    "label": 1
                },
                {
                    "sent": "And it turns out that inheriting which is an algorithm of recent interest in machine learning community gives a solution to our problem.",
                    "label": 0
                },
                {
                    "sent": "So our goal is to rigorously analyze this problem and in this direction we are able to give a near optimal solution for it.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what is the question?",
                    "label": 1
                },
                {
                    "sent": "The machine learning community asks?",
                    "label": 0
                },
                {
                    "sent": "Suppose that you have a set of samples from a model and you want to generate more samples from it.",
                    "label": 0
                },
                {
                    "sent": "So the classical approach is to use maximum likelihood estimation to infer permits of the model.",
                    "label": 0
                },
                {
                    "sent": "And once you have estimated the model, Now you can draw more samples from it.",
                    "label": 0
                },
                {
                    "sent": "But this maximum likelihood estimation step is expensive, so we want something better.",
                    "label": 0
                },
                {
                    "sent": "And here is where herding comes, so it avoids this difficulty by directly generating samples.",
                    "label": 0
                },
                {
                    "sent": "OK, let's get back to our goal.",
                    "label": 0
                },
                {
                    "sent": "So we have a set of samples and we want to generate more samples.",
                    "label": 1
                },
                {
                    "sent": "We formalize it as sampling problem.",
                    "label": 0
                },
                {
                    "sent": "So we are given a set X in Rd with mean mu and we want to generate an infinite sequence of points from X such that for every T, the Euclidean distance between the empirical mean of the first T points and the original mean, you would always be bounded by Alpha over TO Alpha.",
                    "label": 0
                },
                {
                    "sent": "Here is a quantity that can depend on.",
                    "label": 0
                },
                {
                    "sent": "Geometry of X and dimension of the space, but it shouldn't depend on T. So we call this term error and our goal is to find the best Alpha as a function of D that can fits here.",
                    "label": 0
                },
                {
                    "sent": "Let's see an example of how different sampling algorithms can work.",
                    "label": 0
                },
                {
                    "sent": "Suppose that we have 12 set of points with their with their mean at the center and a simple sampling algorithm can work like this.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I choose this point as X1.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mew would be at the same place.",
                    "label": 0
                },
                {
                    "sent": "That",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Jason Point as X2.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mew with.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two year.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The same path.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So after getting 5 samples.",
                    "label": 0
                },
                {
                    "sent": "The distance between the empirical mean and the original mean would be something like this, which is big.",
                    "label": 0
                },
                {
                    "sent": "But if we do our sampling in a more clever way.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I choose the same point as X.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "1.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Meal would be at the same place, but this time I choose.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It'll point to X1 as X2.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So mu.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It goes to here and.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So on.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Extra.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Re explore",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Expire.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Five and after.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Adding five samples in this new algorithm, the distance between the empirical mean and the original mean is much smaller.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's first set the assumptions that I'm going to use throughout the paper we.",
                    "label": 0
                },
                {
                    "sent": "We assume that we have a set of points X in Rd with mean mu.",
                    "label": 1
                },
                {
                    "sent": "So this is the convex Hull of set X.",
                    "label": 0
                },
                {
                    "sent": "And also we assume that said X lies in the Euclidean ball of radius Delta.",
                    "label": 0
                },
                {
                    "sent": "And also let R be the radius of the biggest ball that is contained inside the convex Hull effects.",
                    "label": 0
                },
                {
                    "sent": "So in our scenario, we assume that set X is finite with size N. And also for simplicity we set mu to be equal to 0 and we also assume that the two norm of all the points in X is smaller than one.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "I first want to give you a big picture of our main results.",
                    "label": 0
                },
                {
                    "sent": "So if we change the previous result setting to our geometrical setting, it can be seen that the error for hurting self sampling problem and it gives Alpha smaller than D under some assumptions that I will talk later.",
                    "label": 0
                },
                {
                    "sent": "We proved that any algorithm that solves sampling problem cannot do better than Alpha Square root of T. Furthermore, we propose an algorithm that solves sampling problem with Alpha smaller than a square root of the sum log factors of N. So if we assume that the set X is not too big, then I can ignore lock factors here and our algorithm is getting the optimal.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, remember that I defined error to be the two norm of.",
                    "label": 0
                },
                {
                    "sent": "All the empirical means.",
                    "label": 0
                },
                {
                    "sent": "As a function of T and if I have a solution to sampling problem, it guarantees that this error is always a smaller than Alpha over T, so I can redefine error as supermom of all the partial sums for all the values T and so now on I ignore about this tea and talk about her.",
                    "label": 0
                },
                {
                    "sent": "Like this?",
                    "label": 0
                },
                {
                    "sent": "OK, So what is the most natural idea for sampling?",
                    "label": 1
                },
                {
                    "sent": "This is random sampling.",
                    "label": 0
                },
                {
                    "sent": "It says get IID uniform samples from set X.",
                    "label": 0
                },
                {
                    "sent": "Standard analysis showed that the error for random sampling decreases by ratio 1 / sqrt T. So it's a very simple algorithm, but unfortunately it is not solving our sampling problem because in sampling problem we want the error to decrease by ratio 1 / T. The next natural idea is kind of a greedy algorithm, and that is what's hurting is doing, so it's these two lines of code.",
                    "label": 0
                },
                {
                    "sent": "It maintains a way to assure W, and at each step it.",
                    "label": 0
                },
                {
                    "sent": "Computes the maximum of the inner product and set it to be the next sample X = 1.",
                    "label": 0
                },
                {
                    "sent": "This code may remind some of you of the perceptron perceptron boundedness theorem was first introduced by block and live in in 1970, and it says that for a big class of algorithms that do a kind of sampling and have a common property, you can say that their error is always bounded by Alpha.",
                    "label": 0
                },
                {
                    "sent": "So it's not difficult to see that hurting also belongs to that class of algorithms, and so perceptron bounded.",
                    "label": 0
                },
                {
                    "sent": "This implies that the error for hurting is also bounded by Alpha.",
                    "label": 0
                },
                {
                    "sent": "Alpha is.",
                    "label": 0
                },
                {
                    "sent": "You see, at the top Alpha is this quantity that can depend on geometry of X and the dimension, and but it cannot depend on T. So it's a constant.",
                    "label": 0
                },
                {
                    "sent": "OK, but unfortunately the since this perceptron boundedness is a difficult and very general theorem.",
                    "label": 0
                },
                {
                    "sent": "The proof is not constructive and it doesn't give any explicit bound on Alpha.",
                    "label": 0
                },
                {
                    "sent": "But if we analyze hurting algorithms specifically, it has been shown that error for hurting is bound.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Did buy.",
                    "label": 0
                },
                {
                    "sent": "The two norm of W not plus 1 / R and remember that our was the radius of the biggest ball that is contained inside the convex Hull effects.",
                    "label": 0
                },
                {
                    "sent": "And W not is the initial way to actor that is chosen by the algorithm, so it's usually set to be 0 and mu.",
                    "label": 0
                },
                {
                    "sent": "So since the two norm of W not is low order, I can ignore it and say that error for hurting is order of 1 / R. So generally speaking, these are can get very tiny and run over arc and gets very big.",
                    "label": 0
                },
                {
                    "sent": "But if we assume that our body which in our scenario is the convex Hull, affects is somehow nicer round, which is precisely defined by John's position, then it guarantees that 1 / R is smaller than D. So we say that the body is in John's position if the biggest ellipsoid that is contained inside our body is a factor of the unit ball.",
                    "label": 0
                },
                {
                    "sent": "As an example, this body is not in John's position.",
                    "label": 0
                },
                {
                    "sent": "But this part is.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So under these assumptions, we have that 1 / R is smaller than D and it implies that error for hurting is bounded by D and this is the D factor that I mentioned earlier.",
                    "label": 0
                },
                {
                    "sent": "So so far we have seen that.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Running gives a solution to sampling problem with Alpha.",
                    "label": 0
                },
                {
                    "sent": "Order of the OK Now what is our approach for solving sampling problem?",
                    "label": 0
                },
                {
                    "sent": "So our insight is that sampling problem is connected to ideas in discrepancy theory.",
                    "label": 0
                },
                {
                    "sent": "And by building this connection between sampling problem and discrepancy theory, we would be able to.",
                    "label": 0
                },
                {
                    "sent": "Use the world structured theorems and results in this field.",
                    "label": 0
                },
                {
                    "sent": "OK, so we introduce permutation problem an instead of solving sampling problem, we focus on solving permutation problem.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The interesting fact is that permutation problem is connected to some famous theorems in discrepancy.",
                    "label": 0
                },
                {
                    "sent": "So remember that I defined sampling problem as following.",
                    "label": 0
                },
                {
                    "sent": "We were given the set X with the same assumptions and we want to find an infinite sequence of points in X such that.",
                    "label": 0
                },
                {
                    "sent": "The Super mom of.",
                    "label": 0
                },
                {
                    "sent": "All the partial sums would always be bounded by Alpha.",
                    "label": 0
                },
                {
                    "sent": "Now we define permutation problem as follows.",
                    "label": 0
                },
                {
                    "sent": "We are given the set the same set X.",
                    "label": 0
                },
                {
                    "sent": "This time we want to find an ordering of elements of X such that the Super Bowl of all the partial sums over disordering is always bounded by Alpha.",
                    "label": 0
                },
                {
                    "sent": "It's not difficult to see that if you have a solution to permutation problem, it gives you a solution to sampling problem with the same paraments Alpha.",
                    "label": 0
                },
                {
                    "sent": "So now on we just focus on permutation problem.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "One of the theorems that is very connected to permutation problem is Astinus lemma.",
                    "label": 0
                },
                {
                    "sent": "It says that if you are given the set X with the same assumptions, then there is an ordering of elements of X such that for every T the two norm of all the partial sums over this ordering is always bounded by the dimension D. So is Sinus lemma tell you that you can get D for the permutation problem and it has been a major open question since 1931.",
                    "label": 0
                },
                {
                    "sent": "If you can improve this D 2 sqrt T and if it's true then it gives you a solution to sampling problem with Alpha equal to square root of T. So we do not solve this open question unfortunately.",
                    "label": 0
                },
                {
                    "sent": "But we give another algorithm that does not exactly get the square root of T but gets very close to it.",
                    "label": 0
                },
                {
                    "sent": "OK, so for those of you who are not familiar with discrepancy, I should give you more details to explain our algorithm so.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let BP and BQ be unit balls of Norm P&Q in Rd.",
                    "label": 0
                },
                {
                    "sent": "We define St of PPNBQST is coming from this dynast constant in a few seconds you will see the relationship.",
                    "label": 0
                },
                {
                    "sent": "And is defined as the smallest Lambda such that for every sequence of points in week MBQ with mean equal to zero, there is an ordering of these elements such that all the partial sums for is for.",
                    "label": 0
                },
                {
                    "sent": "This ordering is bounded by Lambda.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To remind you of what is sinus, similar lemma is saying it says that given this at X we can always find an ordering of elements of X such that.",
                    "label": 0
                },
                {
                    "sent": "This partial sums is always bounded by T. So.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sinus lemma implies that St of B2 and B2 is bounded by D.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The next thing that we define is SV of BP, and BQ SV is aggravation of sine vectors.",
                    "label": 0
                },
                {
                    "sent": "And is defined as the smallest Lambda such that for every sequence of points in BQ there is this plus 1 -- 1 such that this signed vector is always bounded by Lambda.",
                    "label": 0
                },
                {
                    "sent": "So in other words, SV is saying that.",
                    "label": 0
                },
                {
                    "sent": "Divide your set X into 2.",
                    "label": 0
                },
                {
                    "sent": "Two sets such that when you.",
                    "label": 0
                },
                {
                    "sent": "Compute the sum of elements of the first set.",
                    "label": 0
                },
                {
                    "sent": "It shouldn't be very far from the sum of the elements of the other set.",
                    "label": 0
                },
                {
                    "sent": "As a simple.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sample suppose that we have a set of points in our space and their approximate copies.",
                    "label": 0
                },
                {
                    "sent": "So the best way to choose this plus one minus ones too.",
                    "label": 0
                },
                {
                    "sent": "Have a small Lambda here for this specific set of points is.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Set plus one to the black points or the other way.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Round and minus one to their copies.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The last thing.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That we need is SS of PPNBQSS is abbreviation of science series, so SS is exactly defined.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "As SV by the difference that we have a quantifier for all K in N. So in SV we wanted to have a control over the sign Victor, but in SS we want to have a control over all the signed partial sums.",
                    "label": 0
                },
                {
                    "sent": "So SS in away is a harder version of SV.",
                    "label": 0
                },
                {
                    "sent": "So our intuition is that giving you bound anesti is harder than giving you bound on SS, and it's harder than giving abound on SP.",
                    "label": 0
                },
                {
                    "sent": "So in order to.",
                    "label": 0
                },
                {
                    "sent": "Have our algorithm.",
                    "label": 0
                },
                {
                    "sent": "We start with SV.",
                    "label": 0
                },
                {
                    "sent": "We give a bound for SV and then we improve it to bound for St finally.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This new setting.",
                    "label": 0
                },
                {
                    "sent": "What is our goal?",
                    "label": 0
                },
                {
                    "sent": "Our goal is to prove that St of B2 and B2 is bounded by the square root of the log factors of N. We start by SV.",
                    "label": 0
                },
                {
                    "sent": "One can use partial coloring lemma, which is a recent breakthrough in discrepancy theory.",
                    "label": 0
                },
                {
                    "sent": "To show that SV of B Infinity and B2 is bounded by log and so it is already known.",
                    "label": 0
                },
                {
                    "sent": "Afterward, we show an algorithmic reduction from SS to SV and it enables us to show that SS of B, Infinity, and V2 is bounded by some log factors event and this extra log factors that you see is.",
                    "label": 0
                },
                {
                    "sent": "The cost of the algorithmic reduction.",
                    "label": 0
                },
                {
                    "sent": "So there is a classic theorem, Chobanian theorem, that connects SS&ST, but it's not constructive, so we give an algorithmic version of Chobanian to show that St is always bounded by SS.",
                    "label": 0
                },
                {
                    "sent": "And finally, by applying some norm inequality's we get we get our final result.",
                    "label": 0
                },
                {
                    "sent": "So I my final note is that these quantities SVS is an SDR well, as studied in discrepancy theory.",
                    "label": 0
                },
                {
                    "sent": "Just to give you an example, big feel approved at FB Infinity and be one is bounded by two and also it has been a major open question if you can prove that SV of B Infinity and B2 is order of 1.",
                    "label": 0
                },
                {
                    "sent": "So this is the end of my talk.",
                    "label": 1
                },
                {
                    "sent": "Thanks everyone for listening.",
                    "label": 0
                },
                {
                    "sent": "And if you have any questions I'm happy.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}