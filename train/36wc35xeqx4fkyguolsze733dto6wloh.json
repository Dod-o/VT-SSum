{
    "id": "36wc35xeqx4fkyguolsze733dto6wloh",
    "title": "Patterns in sets of points: the myriad virtues of eigenproblems",
    "info": {
        "author": [
            "Tijl De Bie, Department of Engineering Mathematics, University of Bristol"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "October 2005",
        "category": [
            "Top->Computer Science->Machine Learning->Pattern Recognition"
        ]
    },
    "url": "http://videolectures.net/aop05_bie_mve/",
    "segmentation": [
        [
            "So maybe you remember my talk on on Saturday or I started with explaining.",
            "Potential framework for pattern analysis pattern discovery pattern matching and then I applied it to some discrete optimization problems for pattern discovery and some continuous optimization problems for patterns in sets of points.",
            "One of them was a discraft problem which is convex special case of a convex optimization problem.",
            "And then I also discussed one adding value problem.",
            "Today I will zoom in on these eigenvalue problems a bit because they provide a large toolbox of.",
            "Different algorithms to solve?",
            "So this is the outline of the talk.",
            "I will first."
        ],
        [
            "I give a few general facts about eigenvalue problems, why it is so useful, why it's interesting, and what you can do with it.",
            "And then the main focus will be about finding relations between two paired datasets.",
            "So for example, you have a set of French text and you have a set of English text that are translations and what is in common, so that's what I mean.",
            "We have paired data set and I will discuss several ways to find such relations relations using PCA, PLS CCA.",
            "And then I will show how this fits in a regularization context.",
            "Now you can extend it to more than just.",
            "Two datasets.",
            "OK, so just to refresh your mind, this was the example I used to."
        ],
        [
            "Discuss all the continuous optimization problems in my previous talk, so it consists of measurements over 454 months around Lake Shasta area and was measured or these weather conditions or six weather conditions.",
            "Annotation will search that.",
            "I used data matrix X and every roll of this data matrix consists of 1.",
            "One of these months, so, containing these six variables, so we have.",
            "In other words, 454 rows and six columns, and the question we asked ourselves, can we summarize the data in a lower dimensional space instead of this examination space?",
            "Can we reduce it to say 2 dimensional space?",
            "And the way the way to do it is you can look for directions represented by a weight vector W Sir jet if you."
        ],
        [
            "Check the data on these directions.",
            "We get a very large variance.",
            "If you would get a small variance, it means there is not much information in this direction, so we can discard a small variance directions.",
            "So this is the variance of projection of X on W, so the projection transpose times projection itself and I'll be cause just maximizing this would yield an infinitely large weight vector W. We have to impose a constraint that says that the square norm of W is smaller than or equal to 1.",
            "So this was in the end the optimization problem to solve, and it was very easy to solve using Lagrangian.",
            "Equating the gradients with respect to W. 20 and results on that given by this.",
            "So this is an eigenvalue problem.",
            "It's a matrix symmetric matrix times a vector that has to be equal to the vector times scalar.",
            "This color is calls.",
            "They added value and the vector isn't called the eigenvector and the directions of large variance or the eigenvectors corresponding to the large eigenvalues and the subsequent ones corresponds to each time a smaller variance.",
            "So if you take the two largest eigenvalues and the corresponding eigenvalue."
        ],
        [
            "I mean projected data on this 2 dimensional space spanned by these two eigenvectors and what we got was this.",
            "In the two dimensional summary of the data and 76% of the variance was captured, so that's quite a lot for only 1/3 of the dimensions, and then we found out that the weight factors actually have a semantic meaning, so they really represent something about the data alarge value along this component means batch weather because a lot of rain and quite cold and a large value along this component means.",
            "Quite windy weather, so we decompose the weather types in terms of nice sunny normal rainy an windy or not windy.",
            "So the question we want to answer now is not just to summarize the data, But suppose we are given two datasets represented by X&YY."
        ],
        [
            "Consists also of vectors denoted by lower case Y.",
            "Every row is such a vector.",
            "So we have two such datasets and every row of X corresponds to one row of Y.",
            "So the number of roles I have is the same and we want to find common one common influence that determines to some extent X&Y as well.",
            "So for example, with the example I started, you have a series of French texts and translations in English, and then what is in common between these two datasets is the meaning of the text, not not the actual words, But the meaning of the text, so that's very interesting if you can.",
            "Actually, device what is the meaning of the text and get rid of all the representational details.",
            "Another example would be if one data set consists of pictures and.",
            "The other data set associated with it corresponds to the captions corresponding to these pictures, so then you can find out which which.",
            "Words, for example in the text, corresponds to which features in the pictures and in this way maybe you can do text retrieval using text retrieval of pictures.",
            "So in the end the goal is to find semantically semantic meaning in common between two datasets X&Y."
        ],
        [
            "There are several ways to do this, and I will go over a few of these ways.",
            "Feel these approaches and they will all be based on an angle value problem.",
            "The first one is a very simple extension of principle.",
            "Component analysis will just take one slide and the 2nd and the third one or more interesting that are based on maximizing covariance between projections and both of the datasets are maximizing the correlation.",
            "So.",
            "I just want to give some general information about eigenvalue problems, the kind of eigenvalues."
        ],
        [
            "Promise I will talk about our generalized eigenvalue problems.",
            "They call it becausw on the right hand side we can also have a matrix, so we want to find the eigenvectors fee or generalized eigenvectors such that if you multiply them on the right with a on the left with a, then you get Lambda times another matrix B times that factor again.",
            "Some properties that you may know is that for symmetric AMB the item vectors are real and the eigenvalues are real, so we don't have to care about that.",
            "Then again, vectors corresponding to different eigenvalues and they are orthogonal in the B metric, which can be represented in this.",
            "So I and J indicate that we are talking about different adding vectors corresponding to eigenvalues, llama and llama J.",
            "The way to see to see it is it's quite easy.",
            "OK, I will not go over the details of this little proof.",
            "And then a very important fact of about eigenvalue problems.",
            "It is faster to compute.",
            "So to compute the dominant eigen eigenvector corresponding to the largest eigenvalue, it takes a quadratic number of steps given A&B of dimensionality D. And fast."
        ],
        [
            "Approximations exist if the matrices are sparse or if you don't need an exact solution.",
            "Then I just some terminology which may be useful.",
            "Maybe you remembered it in PCA problem.",
            "This matrix was of importance.",
            "We actually computed the eigen values and eigen vectors of this matrix.",
            "This is called the sample covariance matrix.",
            "Usually this normalized by the number of data points.",
            "But I will ignore this normalization.",
            "So what it is in fact it is the sum of the other products of all the data points.",
            "Then we have the sample cross covariance matrix between X&X&Y between two datasets given by this an.",
            "I'm using these matrices.",
            "It's easy to compute the variance along a weight vector WX just project X&WX in this way and multiply it with its transpose.",
            "Get this for the covariance, very similar."
        ],
        [
            "OK, so now.",
            "We know all this.",
            "I can go on to show how to find actual correspondences between different datasets between different datasets using these three different approaches.",
            "The data was things that I meant."
        ],
        [
            "Which is kind of difficult to represent, even if you divide it in two, because I will divide the six dimensional data set in two parts, but I would like to give 2 dimensional representation.",
            "So what I did is instead of using these six measurements, I only use two measurements for each data set, namely the temperature and dew point I use in one data set.",
            "So every.",
            "Every month in this data set for response to one point in the dew point versus temperature plots, and then two other measurements, the precipitation and the inflow to Lake Shasta I put in the Y matrix.",
            "So here every point corresponds to influence precipitation and I will try to find during the presentation an underlying common factor that explains some features of the temperature and dew point and at the same time of."
        ],
        [
            "Septation an inflow.",
            "OK, so these are summary questions you want to answer.",
            "So the first alternate."
        ],
        [
            "If it's a very naive, naive way of doing it, but who knows, maybe it works.",
            "Now let's just concatenate the two matrices.",
            "We just put them next to each other, and then what we have is an matrix of 454, four rows for all the months, all the measurements and four columns cause X&Y.",
            "Each have two columns and then we basically just do principal component analysis on this combined matrix.",
            "This is the covariance of this combined matrix which contains the covariance.",
            "The variance of the sample variance.",
            "Of X and the sample covariance of Y with X as well.",
            "So again, subject to this normal strength or the combined weight factor and then we get this eigenvalue problem, which is essentially the same angle value problem we saw before.",
            "When I discussed the PCA on just one datasets and results in fact this division in X&Y, you can do it after your do PCA so.",
            "It's not a very elegant coupling, very elegant way of finding common factors, but let's see what it does so.",
            "These are again the two datasets."
        ],
        [
            "2 dimensional datasets and what I showed here.",
            "These red lines or the principle components.",
            "The first principle components of the two datasets, so there is no coupling at all between these two directions.",
            "They were just computed on both datasets separately.",
            "If you do a couple PC's I.",
            "Call it in stock and you see that it only slightly shifts a bit, so tilted a bit.",
            "This is a bit lower here, and the same here.",
            "So what you see is that the variance within the two datasets themselves is not so important, because at the same time you want to maximize the covariance between the two between the projections on this direction and projection on this direction.",
            "So in that sense it's fit bias towards the direction with a large covariance between projection.",
            "Here in the projection here.",
            "I have to say that also because we have a coupling now the direction directionality is of importance now, so that's why."
        ],
        [
            "I wanna order.",
            "So as I said, also the covariance is now more important.",
            "Maybe it's better to just look at the covariance itself without looking at the variance within the data, because this is kind of hard to interpret.",
            "What exactly it means.",
            "So."
        ],
        [
            "The second alternative is known as partial least squares, and it only maximizes the covariance between the projection of the data set X on WX projection of data set Y on the Wii.",
            "And so this covariance can be written in this form, which is sometimes called the Rayleigh quotient so.",
            "Is Justin.",
            "Some of our.",
            "So this is the covariance, not normalized.",
            "So for WX large it would become very large.",
            "Do the same for WY, so we normalize it with the norm of WY and within normal WX.",
            "So you see that if we are multiply WX with the constants, this objective does not change and the same for WY, 'cause it would just cancels.",
            "On the one hand is nice because we don't want WX to become infinitely larger, safer WY, but on the other hand, it's annoying because we have an infinite number of solutions in this case.",
            "So, but it's very easy to solve.",
            "You can just fix the normal value X to one, then fix it on the norm of WI also to one.",
            "So this is the optimization problem we want to solve.",
            "End result will.",
            "The resulting WXW I will maximize the covariance of the projections on them.",
            "So we can solve this."
        ],
        [
            "When using Lagrangian, we have two constraints, so we have two LaGrange multipliers, Lambda one and Lambda 2.",
            "We compute the gradient respective WXWY and then we get these two equations and they look a bit like an eigenvalue problem, but they are not exactly yet because we have two these two LaGrange multipliers.",
            "So it's like we have two eigenvalues.",
            "Yeah, but it's actually possible to show that they are equal to each other at the optimum.",
            "By multiplying this equation to the left by WX transpose and this one byw transpose, when you see that this and this.",
            "These two left hand sides become equal to each other.",
            "They were just the transpose of each other.",
            "An here we have value X transpose times X which is equal to 1 and here it is also equal to 1 S. Using this sequence of equations you can see that Lambda one Lambda, two must be equal to each other, so we replace it by Lambda and the result is just one simple adding value problem.",
            "Written less like this.",
            "So.",
            "Solving this gives you to wait factors for the two spaces, and if you project the data on these two weight factors, you have the maximum covariance you can achieve over all possible weight factors.",
            "Um?"
        ],
        [
            "I just want to show someone alternative derivation because it will be convenient afterwards and with this alternative derivation you immediately only have 1 LaGrange multiplier and the way to do it is by just using this constraint instead of the normal strength and WXMWY separately.",
            "Then this is the Lagrangian.",
            "With only one LaGrange multiplier and you immediately.",
            "Gets the eigenvalue problem as a result.",
            "So let me just give a few properties.",
            "So on."
        ],
        [
            "Of course you have an eigenvector for each different possible value of Lambda.",
            "An egg and vectors corresponding to.",
            "Different values of Lambda.",
            "So say eigenvector value XINWXJ&WYNWYJ.",
            "Each time you have WXLWY and satisfy certain interesting properties, namely.",
            "You can show that WXI is orthogonal to WXJ.",
            "There should be a transpose here and here.",
            "Also, the same holds for Wii and Wii J.",
            "And the projections are orthogonal or uncorrelated.",
            "So this means that the different directions at a different weight vectors you'll find.",
            "For the largest covariance direction for the second largest covariance direction and so on, they were talking to each other, so you get a nice orthogonal subspace if you pick a few of them.",
            "So to see what it gives on the data, I repeated a couple PCA results and here is the result for."
        ],
        [
            "Glass problem you see that?",
            "Even more than in a couple PCA case.",
            "This direction is shifted a bit, so this is a bit lower.",
            "This bit higher and similarly here.",
            "So.",
            "I don't know, maybe it makes sense that you have less rain if temperature is larger, but on the other hand it's not exactly the case because it would you point is larger than you have more chance on rain.",
            "So in some sense it's not optimal.",
            "It's still a bit hard to interpret, and probably one of the reasons maybe that PCA maximizes the variance with this scale dependent.",
            "Maybe the Geo point and the temperature should be expressed in a different scale or similarly for the inflow or precipitation.",
            "Imagine, for example, that one of the two datasets, say X contains 1."
        ],
        [
            "Parameter which is expressed in a distance in meters and another variable which is expressed in degrees Celsius.",
            "Then it doesn't really make sense to talk about variances because it depends on the scale you're using and there is no way to normalize dinner in a fairway.",
            "So that's a drawback of this method.",
            "So the third alternative is probably."
        ],
        [
            "Intuitively, by far the best, which tries to find directions such that if you project the data on them, you get a maximal correlation.",
            "An result will be scale independent, so it doesn't matter if you have different.",
            "Scales at all, so again the correlation can be expressed as a Rayleigh quotient, so this is a covariance between the projection on WXW why this is.",
            "The variance or the standard deviation after taking the square root?",
            "In the attic space, this is a standard deviation of Oxford Square roots.",
            "In the wide space, and together this yields the correlation.",
            "Again, we have to fix the scale because again, if you multiply the value equity fixed constants, the objective remains the same.",
            "So to obtain just one solution we have to fix the scale.",
            "I now immediately take the easy way, we just want constraints.",
            "Which is this constraint, and this constraint is sufficient to fix the scale of both WX&WY.",
            "So this is the optimization problem you want to solve.",
            "We can write the Lagrangian with LaGrange multiplier.",
            "Lambda writes 1/2 just for convenience because afterwards is a bit easier than an if you create the gradients with respect to WX&WY to 0 again, you get this as a result.",
            "So now you see that you have on the right hand side.",
            "Matrix differ."
        ],
        [
            "From the identity, with here covariance of X covariance of Y as well.",
            "So in this sense it is normalized.",
            "The data is normalized automatically.",
            "So I repeated the eigenvalue problem here and again we can find some properties for directions from different eigenvectors."
        ],
        [
            "And in this case the different weight factors corresponding to different diagonals or not orthogonal, but the projections on them they run correlated.",
            "Which can be expressed in this way.",
            "So which is another nice property?",
            "It's different from orthogonality, of course.",
            "Probably in many cases it's more interesting because it means that projections on different weight vectors corresponding to different eigenvalues actually contain really different information.",
            "They're unrelated with previous.",
            "W axis so if you look at the result I repeated PLS result again and this is the Canonical correlation result.",
            "So now we really shifted quite a lot but."
        ],
        [
            "Here is quite easy to attach an interpretation to the result because.",
            "The."
        ],
        [
            "More we are to the right on this line.",
            "The larger the temperature mine minus two point is.",
            "Because the temperature is larger, the Geo point is smaller, and as you may know, Geo Point actually is a measure of how much moisture moisture is in the air and it is expressed in an attempt as a temperature and if it real temperature drops below the dew point, which is the temperature and it starts raining.",
            "So in fact temperature minus zero point is a very good indication of how likely it is going to rain today and you can see indeed.",
            "Here this components.",
            "It only measures the precipitation exactly is so.",
            "The more we are in that direction, the less rain we have, the more we are in this direction, the larger the temperature minus the jewel point is, so the less likely is going to rain.",
            "So indeed, apparently the first DCA component is something that tells you whether it's raining or not.",
            "So it is possible to attach an interpretation to this much more easily.",
            "So this is."
        ],
        [
            "The summary of all the different different eigenvalue problems at the top is just PC on the different datasets separately.",
            "I did not really discuss it because.",
            "Nothing special, this was PCA on the combined datasets.",
            "You can see that then we already incorporate some covariance between the two datasets.",
            "PLS ignores the variance within the data sets itself and PCA even compensates for it.",
            "So it says OK if you along a certain direction.",
            "If you find a large covariance it doesn't count as much.",
            "If you also have a large variance within the data set itself.",
            "So in that sense, the importance of the variance within the datasets themselves decreases if you go down and.",
            "In this table and.",
            "One important fact is that the projections on the weight factors.",
            "Um?",
            "In the first 3 cases, they change if you change the scale.",
            "So if you can rotate the data and the projections on resulting weight factors will remain the same, so essentially nothing changes about the solution.",
            "But if you use another invertible transformation besides rotation, then the result will be different.",
            "That was basically what I said about.",
            "If you use data containing different express in different measures, for example it sells using meters, but for CCA it is invariant with respect to any invertible transformation.",
            "So."
        ],
        [
            "Summarize the advantages of PCA.",
            "MPLS are in fact they are robust against noise because in many cases you can assume that noise is small.",
            "If it's large, you cannot do anything anyway, so you have to assume it is.",
            "It is small.",
            "And by only taking into account large variances, they they're capable to deal with it.",
            "On the other hand, we have seen at CCA is in fact more easy to understand, and results are easier to interpret.",
            "Also, maybe there is something in between we can do such that it's still interpretable, but on the other hand we have some better noise robustness and then I will show Now how this can be done within a regularization perspective.",
            "So the goal of regularization is to take PCA, adapted a bit just to make it more noise, robust, but not to lose its properties.",
            "That is easy to interpret.",
            "And then maybe Europe?"
        ],
        [
            "Amber from the lecture I gave on Saturday that regularization often involves the upper bounding or the minimization of a normal overweight factor, so that's what we're going to do.",
            "You will add a cost for a large weight vector WX, and for a large phase factor, WY."
        ],
        [
            "So if this was the CCA optimization problem with your covariance of the projections and with this.",
            "Equated to want to fix the scale.",
            "And then we add a regularization term that is essentially the sum of the norms of WX of WY times constants that we can choose.",
            "So then the optimum becomes this with just at the optimization problem becomes this or just this is added.",
            "So we want to maximize this, which means that we want to find because there is a - We want to find solutions with a large correlation, but at the same time with a small norm of WX&WY.",
            "If we solve this in the same way with LaGrange multipliers and.",
            "It's on."
        ],
        [
            "Then we get this as a result, or the only difference with the CCA in value problem is that we have minus gamma times the identity matrix here and here.",
            "So that's the only the only difference.",
            "Now what is what is."
        ],
        [
            "First thing about this optimization problem is that it actually gives an overview on all the other adding value problems I discussed until now.",
            "In fact, by varying gamma you interpolate between the extreme case of doing PCA on X&Y separately, so no coupling at all.",
            "An on the other hand, CCA Canonical correlation analysis.",
            "You can see this as so gamma has to be positive.",
            "I didn't say that, but gamma is always positive.",
            "To get a reasonable good regularization effect.",
            "So for gamma, really large.",
            "These covariance matrices they don't matter, so you can just drop them in the limit.",
            "What you get then is this, after some reorganization, which is indeed in the end PC on.",
            "An ex separately and why separately?",
            "If gamma decreases now and it decreases the certain extent that Lambda becomes equal to minus one, then.",
            "OK, so Lambda becomes equal to 1 -- 1, so we can bring this.",
            "To the other side of the equation of the equality sign, an identity, times identity to this site and then we get this as a result, which is if you remember the PCA on the couple data set XY.",
            "Then if you decrease gamma regularization parameter even more until love now becomes equal to 0, then you get these results because Lambda is 0.",
            "So this is just come.",
            "If you do it even more than you get something like this so it's not equal to anything now.",
            "Still a continuous parameter which is regularised CCA, as many people know better than in this form.",
            "And then she equated to zero you just get Canonical correlation analysis.",
            "Just the diagonal just disappears.",
            "So that's kind of a nice way of interpreting all these different angle value problems.",
            "That's regularize versions of Canonical correlation analysis.",
            "So.",
            "What is GIFs?",
            "If you're very gamma, I start here in these pictures with a fairly large gamma."
        ],
        [
            "Which is just VCM.",
            "Two different datasets separately, and now if we decrease the regularization, you see that slowly the directions evolve towards the CCA directions.",
            "So this one CCA solution."
        ],
        [
            "Now, So what can we do with this regularization?",
            "How is it useful?",
            "Well, even if you are just interested in finding correlations and not anything like, you're not interested in the variance itself.",
            "Sometimes it is beneficial to regularize, and this is the case if you have few data or lots of noise or high dimensionality.",
            "Because Daniel fact you don't have enough information to learn from, so you need to restrict the search space here looking over.",
            "So to show why to show that this is the case, I trained.",
            "I learned WX&WY 2 weight factors just 12 of the month, so not all the 454, but just 12 of them.",
            "And then I evaluate what the correlation is.",
            "Of the test points to the remaining 442 data points.",
            "With these weight factors, learning on the 12 ones.",
            "So effects I trained on a small test set an evaluates the correlation on on the test set.",
            "And then I did this for varying gamma 4.",
            "So this is gamma very large, very strong.",
            "Regularization is for gamma really small, so at Infinity we have CCA and you see that.",
            "At the.",
            "Correlation on the training set.",
            "So indeed the correlation on the training set is maximal for CCA case.",
            "Very much in that direction, because CCA exactly maximizes the correlation, so of course it has to be maximal for CCA.",
            "But on the other hand, if you look at the correlation on the test set, then the maximum is not achieved for CCA case because of the noise and the side of the training set, which is very small.",
            "But at some non 0 value of the regularization parameter right here.",
            "I will not go into ways to find an optimal value of the regularization parameter, but there are several ways to do this.",
            "OK, and also on the 1st."
        ],
        [
            "Yeah, I said that I would show a way through Fisher discriminant analysis and rich aggression more based on eigenvalue problem.",
            "Well, actually CCAR regularize CCA is rich regression.",
            "Or you can see that's rich aggression with a more dimensional datasets.",
            "Why so serious?",
            "Let's let's take the datasets.",
            "Why to be 1 dimensional?",
            "So then why this factor is just a scalar and the scale doesn't matter.",
            "So let's just fix it to one.",
            "So we equate W. Why to one?",
            "And then this equation, the regularised CCA equation reduces to this.",
            "So why is just a vector just one dimensional?",
            "Then after reordering this bit.",
            "The first role if tries to this.",
            "WX has to be equal to.",
            "His expression, which is proportional to this, and maybe you remember this equation is exactly the rich regression equation where this parameter is replaced by the proportion of gamma and Lambda.",
            "And Lambda you can compute it in this way, but it's not so important what exactly Lambda is, just a constant sort of form is exactly identical to rich aggression, so this means that rich aggression is the same, as regularised CCA between a multi dimensional data set X and a one dimensional data set Y.",
            "Now maybe some of you know multi multiple discriminant analysis?",
            "Sorry.",
            "OK, OK, let me first say that if Y is binary, so plus one and minus one only, then since we saw that rich regression and Fisher discriminant analysis or actually also almost the same except for in Fisher discriminant analysis, why is binary you have exactly the same case?",
            "The same thing here first."
        ],
        [
            "Commitment analysis is also a special case of CCA or regularised CCA.",
            "Now sometimes it is.",
            "It is useful.",
            "To do something like Fisher discriminant analysis, if you have more than just two classes more than just a binary classification problem, and then in that case you can represent the classes of the data points using matrix like this.",
            "So for every class you have points.",
            "For every class you have a column for every data points.",
            "We have a one in one of the columns depending on depending on which class the data points belongs so.",
            "This can be summarized in a matrix like this for all the data points are organized such that once in the same class occurs consecutively.",
            "So A1C1 means a column of ones of C1 ones basically, and then you just have to censor it.",
            "That's not so important, and now you can do multiple discriminant analysis using the data matrix X on the one hand data matrix, matrix Y label matrix, on the other hand, and what you get then.",
            "ISM.",
            "If you get you get directions in the data SpaceX search that the different clusters corresponding to these different columns or as widely separated as possible.",
            "So in that sense multiple discriminant analysis can be formulated as a Canonical correlation analysis problem.",
            "So just for everything was about two datasets and X data set and a wide data set.",
            "Maybe things can be done as well if you have more than two datasets, so you could have."
        ],
        [
            "Just the French and English corpus, but also a German and Italian and so on.",
            "And surely you can you can get better results and you can be better at this extracting semantics if you can do this.",
            "In a good way, so I will show an easy way to Excel."
        ],
        [
            "And is that have some nice interpretations.",
            "So what we had if we just had two datasets and we maximize this for XI, was equal to an ex data set XJ2Y data set with all summation because we only had two datasets.",
            "And then a constraint that says that the sum of the two covariances within the two datasets had to be equal to 1.",
            "Now it's easy to extend this, namely, just summing over all pairs of datasets, and here summing over all the datasets individually.",
            "So this is just intuitive way of extending it is not very well motivated.",
            "Of course, whether or some nice interpretations.",
            "If you know that.",
            "I do normal square norm of the sum of all the projections of the data XI on their corresponding weight factor is equal to this.",
            "This sum which you see here plus this which is this some which is fixed.",
            "So it's constant anyway and you see that instead of maximizing this with respect to this constraint, we can also maximize this with respect to the same constraint.",
            "So resulting optimization problem is this and what that exactly is is OK, so we have.",
            "Say K datasets so.",
            "XI from X1 up to XK.",
            "And then let's project each of these on their weight vector.",
            "So then we have XI times Wii.",
            "And then this projection is a vector of length equal to the number of data points you have in each of the datasets.",
            "So I represented these factors here by all these small arrows.",
            "Now what you want to maximize is the norm of the sum of all these arrows.",
            "Which is this long arrow for the square norm of this long arrow subject to the constraints that the sum of the squares of each of the individual norms is constant equal to 1.",
            "The way to the best way to achieve this is to have them all in the same direction.",
            "So in the end you try to stretch all these arrows as much as possible such that they are.",
            "As similar to each other as possible, which again amounts to finding at direction in each of the data spaces such that they capture a very similar semantic meaning in these data datasets.",
            "Resulting eigenvalue problem is given by this, so it look."
        ],
        [
            "Maybe a bit complicated, but in fact it's not really these.",
            "This right hand side matrix just contains covariance matrix on the agonal and on the left you have all the cross covariance matrices.",
            "So I gave one geometric interpretation of what is exactly doing.",
            "Another interpretation is a probabilistic one.",
            "So imagine you have you assume a model, a noisy model for the data which is as follows data.",
            "Each data set XI is generated using some underlying components, so you assume there are some components generating this data.",
            "Plus some noise, so the components are affected by noise and then they are transformed in some way using matrix WI inverse.",
            "So we can write it just in this way, which says that if we transform the matrix, the data XI matrix WI, we just see the individual components plus some noise.",
            "Now this eigenvalue problem multiway CCA if you want to call it that way is in fact a maximum likelihood estimator for.",
            "This matrix is WI under certain assumptions.",
            "So this is another way to see that indeed, because this C is equal for all the eyes for all the datasets, it is a way to extract a common underlying factor underlying each of these datasets.",
            "Again, you can regularize this and very often is needed if you have."
        ],
        [
            "I love data and is done in a very similar way and very similar properties hold if you vary gamma from really large to small.",
            "OK."
        ],
        [
            "So let me conclude.",
            "So I think at least I try to show that I can value problems are very powerful and you can do many things with it, even though I only talked about one class of problems, namely finding connections between datasets.",
            "Or noise dimensionality reduction in the PCA case, but you can do many other things with eigenvalue problems that are quite active.",
            "Research domains recently, for example, you can do spectral clustering, which means doing clustering using just an eigenvalue problem.",
            "On on a matrix that is computed based on the data and apparently also classification and regression.",
            "Fisher discriminant analysis and rich regression can be seen within the same framework.",
            "If you want to.",
            "One thing that is very interesting about all these algorithms is that they are.",
            "They can be expressed completely in terms of inner products.",
            "And as you will see in the next lecture, I think this helps you in making them all nonlinear so that you can.",
            "Find really weird shape.",
            "So I mean, if you have a non linear data set you can really easily find a structure within it.",
            "OK, so that concludes my lecture.",
            "Question.",
            "Why is partially square?",
            "Actually.",
            "Honestly, I don't know.",
            "I don't know.",
            "I think it might be because I only presented part of partial least squares.",
            "Usually this is done in several steps.",
            "So what I showed is an eigenvalue problem and you can solve for all the eigenvectors and all the eigenvalues.",
            "If you want all at once.",
            "But usually they use partially squares to do regression an they do it in a much more complicated way than just in one eigenvalue step, just using one eigenvalue problem and they deflate and so on.",
            "And I think that's why author deflation they only use part of the remaining, they only use the remaining variance in the data.",
            "To do PLS again so it looks like they always release crash problem only on only a part of the data still.",
            "Maybe that's not the answer.",
            "Yes.",
            "You mentioned that you did.",
            "Yeah, yeah.",
            "Bristol.",
            "What about models?",
            "That example needs to factorization probabilistic speciales in these types of models that Starbucks parser construction sometimes and.",
            "Linear algebra.",
            "Yeah, I wanted to talk only about adding value problems because it is anger.",
            "Problems are computationally very well studied and so you can just pick any eigenvalue algorithm from the shelf and use it immediately.",
            "So I think that's the strength of this kind of algorithms.",
            "I agree that of course in many cases they don't.",
            "They never result in sparsity, for example, which is something that is very often desirable, but indeed then you have to resort to different methods.",
            "That's true."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So maybe you remember my talk on on Saturday or I started with explaining.",
                    "label": 0
                },
                {
                    "sent": "Potential framework for pattern analysis pattern discovery pattern matching and then I applied it to some discrete optimization problems for pattern discovery and some continuous optimization problems for patterns in sets of points.",
                    "label": 1
                },
                {
                    "sent": "One of them was a discraft problem which is convex special case of a convex optimization problem.",
                    "label": 0
                },
                {
                    "sent": "And then I also discussed one adding value problem.",
                    "label": 0
                },
                {
                    "sent": "Today I will zoom in on these eigenvalue problems a bit because they provide a large toolbox of.",
                    "label": 0
                },
                {
                    "sent": "Different algorithms to solve?",
                    "label": 0
                },
                {
                    "sent": "So this is the outline of the talk.",
                    "label": 0
                },
                {
                    "sent": "I will first.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I give a few general facts about eigenvalue problems, why it is so useful, why it's interesting, and what you can do with it.",
                    "label": 0
                },
                {
                    "sent": "And then the main focus will be about finding relations between two paired datasets.",
                    "label": 1
                },
                {
                    "sent": "So for example, you have a set of French text and you have a set of English text that are translations and what is in common, so that's what I mean.",
                    "label": 1
                },
                {
                    "sent": "We have paired data set and I will discuss several ways to find such relations relations using PCA, PLS CCA.",
                    "label": 0
                },
                {
                    "sent": "And then I will show how this fits in a regularization context.",
                    "label": 0
                },
                {
                    "sent": "Now you can extend it to more than just.",
                    "label": 0
                },
                {
                    "sent": "Two datasets.",
                    "label": 0
                },
                {
                    "sent": "OK, so just to refresh your mind, this was the example I used to.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Discuss all the continuous optimization problems in my previous talk, so it consists of measurements over 454 months around Lake Shasta area and was measured or these weather conditions or six weather conditions.",
                    "label": 0
                },
                {
                    "sent": "Annotation will search that.",
                    "label": 0
                },
                {
                    "sent": "I used data matrix X and every roll of this data matrix consists of 1.",
                    "label": 0
                },
                {
                    "sent": "One of these months, so, containing these six variables, so we have.",
                    "label": 0
                },
                {
                    "sent": "In other words, 454 rows and six columns, and the question we asked ourselves, can we summarize the data in a lower dimensional space instead of this examination space?",
                    "label": 0
                },
                {
                    "sent": "Can we reduce it to say 2 dimensional space?",
                    "label": 0
                },
                {
                    "sent": "And the way the way to do it is you can look for directions represented by a weight vector W Sir jet if you.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Check the data on these directions.",
                    "label": 0
                },
                {
                    "sent": "We get a very large variance.",
                    "label": 0
                },
                {
                    "sent": "If you would get a small variance, it means there is not much information in this direction, so we can discard a small variance directions.",
                    "label": 0
                },
                {
                    "sent": "So this is the variance of projection of X on W, so the projection transpose times projection itself and I'll be cause just maximizing this would yield an infinitely large weight vector W. We have to impose a constraint that says that the square norm of W is smaller than or equal to 1.",
                    "label": 0
                },
                {
                    "sent": "So this was in the end the optimization problem to solve, and it was very easy to solve using Lagrangian.",
                    "label": 0
                },
                {
                    "sent": "Equating the gradients with respect to W. 20 and results on that given by this.",
                    "label": 0
                },
                {
                    "sent": "So this is an eigenvalue problem.",
                    "label": 0
                },
                {
                    "sent": "It's a matrix symmetric matrix times a vector that has to be equal to the vector times scalar.",
                    "label": 0
                },
                {
                    "sent": "This color is calls.",
                    "label": 0
                },
                {
                    "sent": "They added value and the vector isn't called the eigenvector and the directions of large variance or the eigenvectors corresponding to the large eigenvalues and the subsequent ones corresponds to each time a smaller variance.",
                    "label": 0
                },
                {
                    "sent": "So if you take the two largest eigenvalues and the corresponding eigenvalue.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I mean projected data on this 2 dimensional space spanned by these two eigenvectors and what we got was this.",
                    "label": 0
                },
                {
                    "sent": "In the two dimensional summary of the data and 76% of the variance was captured, so that's quite a lot for only 1/3 of the dimensions, and then we found out that the weight factors actually have a semantic meaning, so they really represent something about the data alarge value along this component means batch weather because a lot of rain and quite cold and a large value along this component means.",
                    "label": 0
                },
                {
                    "sent": "Quite windy weather, so we decompose the weather types in terms of nice sunny normal rainy an windy or not windy.",
                    "label": 0
                },
                {
                    "sent": "So the question we want to answer now is not just to summarize the data, But suppose we are given two datasets represented by X&YY.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Consists also of vectors denoted by lower case Y.",
                    "label": 0
                },
                {
                    "sent": "Every row is such a vector.",
                    "label": 0
                },
                {
                    "sent": "So we have two such datasets and every row of X corresponds to one row of Y.",
                    "label": 0
                },
                {
                    "sent": "So the number of roles I have is the same and we want to find common one common influence that determines to some extent X&Y as well.",
                    "label": 0
                },
                {
                    "sent": "So for example, with the example I started, you have a series of French texts and translations in English, and then what is in common between these two datasets is the meaning of the text, not not the actual words, But the meaning of the text, so that's very interesting if you can.",
                    "label": 0
                },
                {
                    "sent": "Actually, device what is the meaning of the text and get rid of all the representational details.",
                    "label": 0
                },
                {
                    "sent": "Another example would be if one data set consists of pictures and.",
                    "label": 0
                },
                {
                    "sent": "The other data set associated with it corresponds to the captions corresponding to these pictures, so then you can find out which which.",
                    "label": 0
                },
                {
                    "sent": "Words, for example in the text, corresponds to which features in the pictures and in this way maybe you can do text retrieval using text retrieval of pictures.",
                    "label": 0
                },
                {
                    "sent": "So in the end the goal is to find semantically semantic meaning in common between two datasets X&Y.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are several ways to do this, and I will go over a few of these ways.",
                    "label": 0
                },
                {
                    "sent": "Feel these approaches and they will all be based on an angle value problem.",
                    "label": 0
                },
                {
                    "sent": "The first one is a very simple extension of principle.",
                    "label": 0
                },
                {
                    "sent": "Component analysis will just take one slide and the 2nd and the third one or more interesting that are based on maximizing covariance between projections and both of the datasets are maximizing the correlation.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I just want to give some general information about eigenvalue problems, the kind of eigenvalues.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Promise I will talk about our generalized eigenvalue problems.",
                    "label": 1
                },
                {
                    "sent": "They call it becausw on the right hand side we can also have a matrix, so we want to find the eigenvectors fee or generalized eigenvectors such that if you multiply them on the right with a on the left with a, then you get Lambda times another matrix B times that factor again.",
                    "label": 0
                },
                {
                    "sent": "Some properties that you may know is that for symmetric AMB the item vectors are real and the eigenvalues are real, so we don't have to care about that.",
                    "label": 0
                },
                {
                    "sent": "Then again, vectors corresponding to different eigenvalues and they are orthogonal in the B metric, which can be represented in this.",
                    "label": 0
                },
                {
                    "sent": "So I and J indicate that we are talking about different adding vectors corresponding to eigenvalues, llama and llama J.",
                    "label": 0
                },
                {
                    "sent": "The way to see to see it is it's quite easy.",
                    "label": 0
                },
                {
                    "sent": "OK, I will not go over the details of this little proof.",
                    "label": 1
                },
                {
                    "sent": "And then a very important fact of about eigenvalue problems.",
                    "label": 0
                },
                {
                    "sent": "It is faster to compute.",
                    "label": 0
                },
                {
                    "sent": "So to compute the dominant eigen eigenvector corresponding to the largest eigenvalue, it takes a quadratic number of steps given A&B of dimensionality D. And fast.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Approximations exist if the matrices are sparse or if you don't need an exact solution.",
                    "label": 0
                },
                {
                    "sent": "Then I just some terminology which may be useful.",
                    "label": 0
                },
                {
                    "sent": "Maybe you remembered it in PCA problem.",
                    "label": 0
                },
                {
                    "sent": "This matrix was of importance.",
                    "label": 0
                },
                {
                    "sent": "We actually computed the eigen values and eigen vectors of this matrix.",
                    "label": 0
                },
                {
                    "sent": "This is called the sample covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "Usually this normalized by the number of data points.",
                    "label": 0
                },
                {
                    "sent": "But I will ignore this normalization.",
                    "label": 0
                },
                {
                    "sent": "So what it is in fact it is the sum of the other products of all the data points.",
                    "label": 0
                },
                {
                    "sent": "Then we have the sample cross covariance matrix between X&X&Y between two datasets given by this an.",
                    "label": 0
                },
                {
                    "sent": "I'm using these matrices.",
                    "label": 0
                },
                {
                    "sent": "It's easy to compute the variance along a weight vector WX just project X&WX in this way and multiply it with its transpose.",
                    "label": 0
                },
                {
                    "sent": "Get this for the covariance, very similar.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now.",
                    "label": 0
                },
                {
                    "sent": "We know all this.",
                    "label": 0
                },
                {
                    "sent": "I can go on to show how to find actual correspondences between different datasets between different datasets using these three different approaches.",
                    "label": 0
                },
                {
                    "sent": "The data was things that I meant.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which is kind of difficult to represent, even if you divide it in two, because I will divide the six dimensional data set in two parts, but I would like to give 2 dimensional representation.",
                    "label": 0
                },
                {
                    "sent": "So what I did is instead of using these six measurements, I only use two measurements for each data set, namely the temperature and dew point I use in one data set.",
                    "label": 0
                },
                {
                    "sent": "So every.",
                    "label": 0
                },
                {
                    "sent": "Every month in this data set for response to one point in the dew point versus temperature plots, and then two other measurements, the precipitation and the inflow to Lake Shasta I put in the Y matrix.",
                    "label": 0
                },
                {
                    "sent": "So here every point corresponds to influence precipitation and I will try to find during the presentation an underlying common factor that explains some features of the temperature and dew point and at the same time of.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Septation an inflow.",
                    "label": 0
                },
                {
                    "sent": "OK, so these are summary questions you want to answer.",
                    "label": 0
                },
                {
                    "sent": "So the first alternate.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If it's a very naive, naive way of doing it, but who knows, maybe it works.",
                    "label": 0
                },
                {
                    "sent": "Now let's just concatenate the two matrices.",
                    "label": 0
                },
                {
                    "sent": "We just put them next to each other, and then what we have is an matrix of 454, four rows for all the months, all the measurements and four columns cause X&Y.",
                    "label": 0
                },
                {
                    "sent": "Each have two columns and then we basically just do principal component analysis on this combined matrix.",
                    "label": 0
                },
                {
                    "sent": "This is the covariance of this combined matrix which contains the covariance.",
                    "label": 0
                },
                {
                    "sent": "The variance of the sample variance.",
                    "label": 0
                },
                {
                    "sent": "Of X and the sample covariance of Y with X as well.",
                    "label": 0
                },
                {
                    "sent": "So again, subject to this normal strength or the combined weight factor and then we get this eigenvalue problem, which is essentially the same angle value problem we saw before.",
                    "label": 0
                },
                {
                    "sent": "When I discussed the PCA on just one datasets and results in fact this division in X&Y, you can do it after your do PCA so.",
                    "label": 0
                },
                {
                    "sent": "It's not a very elegant coupling, very elegant way of finding common factors, but let's see what it does so.",
                    "label": 0
                },
                {
                    "sent": "These are again the two datasets.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "2 dimensional datasets and what I showed here.",
                    "label": 0
                },
                {
                    "sent": "These red lines or the principle components.",
                    "label": 0
                },
                {
                    "sent": "The first principle components of the two datasets, so there is no coupling at all between these two directions.",
                    "label": 0
                },
                {
                    "sent": "They were just computed on both datasets separately.",
                    "label": 0
                },
                {
                    "sent": "If you do a couple PC's I.",
                    "label": 0
                },
                {
                    "sent": "Call it in stock and you see that it only slightly shifts a bit, so tilted a bit.",
                    "label": 0
                },
                {
                    "sent": "This is a bit lower here, and the same here.",
                    "label": 0
                },
                {
                    "sent": "So what you see is that the variance within the two datasets themselves is not so important, because at the same time you want to maximize the covariance between the two between the projections on this direction and projection on this direction.",
                    "label": 0
                },
                {
                    "sent": "So in that sense it's fit bias towards the direction with a large covariance between projection.",
                    "label": 0
                },
                {
                    "sent": "Here in the projection here.",
                    "label": 0
                },
                {
                    "sent": "I have to say that also because we have a coupling now the direction directionality is of importance now, so that's why.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I wanna order.",
                    "label": 0
                },
                {
                    "sent": "So as I said, also the covariance is now more important.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's better to just look at the covariance itself without looking at the variance within the data, because this is kind of hard to interpret.",
                    "label": 0
                },
                {
                    "sent": "What exactly it means.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The second alternative is known as partial least squares, and it only maximizes the covariance between the projection of the data set X on WX projection of data set Y on the Wii.",
                    "label": 1
                },
                {
                    "sent": "And so this covariance can be written in this form, which is sometimes called the Rayleigh quotient so.",
                    "label": 0
                },
                {
                    "sent": "Is Justin.",
                    "label": 0
                },
                {
                    "sent": "Some of our.",
                    "label": 0
                },
                {
                    "sent": "So this is the covariance, not normalized.",
                    "label": 0
                },
                {
                    "sent": "So for WX large it would become very large.",
                    "label": 0
                },
                {
                    "sent": "Do the same for WY, so we normalize it with the norm of WY and within normal WX.",
                    "label": 0
                },
                {
                    "sent": "So you see that if we are multiply WX with the constants, this objective does not change and the same for WY, 'cause it would just cancels.",
                    "label": 0
                },
                {
                    "sent": "On the one hand is nice because we don't want WX to become infinitely larger, safer WY, but on the other hand, it's annoying because we have an infinite number of solutions in this case.",
                    "label": 0
                },
                {
                    "sent": "So, but it's very easy to solve.",
                    "label": 0
                },
                {
                    "sent": "You can just fix the normal value X to one, then fix it on the norm of WI also to one.",
                    "label": 0
                },
                {
                    "sent": "So this is the optimization problem we want to solve.",
                    "label": 0
                },
                {
                    "sent": "End result will.",
                    "label": 0
                },
                {
                    "sent": "The resulting WXW I will maximize the covariance of the projections on them.",
                    "label": 0
                },
                {
                    "sent": "So we can solve this.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When using Lagrangian, we have two constraints, so we have two LaGrange multipliers, Lambda one and Lambda 2.",
                    "label": 0
                },
                {
                    "sent": "We compute the gradient respective WXWY and then we get these two equations and they look a bit like an eigenvalue problem, but they are not exactly yet because we have two these two LaGrange multipliers.",
                    "label": 0
                },
                {
                    "sent": "So it's like we have two eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but it's actually possible to show that they are equal to each other at the optimum.",
                    "label": 0
                },
                {
                    "sent": "By multiplying this equation to the left by WX transpose and this one byw transpose, when you see that this and this.",
                    "label": 0
                },
                {
                    "sent": "These two left hand sides become equal to each other.",
                    "label": 0
                },
                {
                    "sent": "They were just the transpose of each other.",
                    "label": 0
                },
                {
                    "sent": "An here we have value X transpose times X which is equal to 1 and here it is also equal to 1 S. Using this sequence of equations you can see that Lambda one Lambda, two must be equal to each other, so we replace it by Lambda and the result is just one simple adding value problem.",
                    "label": 0
                },
                {
                    "sent": "Written less like this.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Solving this gives you to wait factors for the two spaces, and if you project the data on these two weight factors, you have the maximum covariance you can achieve over all possible weight factors.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I just want to show someone alternative derivation because it will be convenient afterwards and with this alternative derivation you immediately only have 1 LaGrange multiplier and the way to do it is by just using this constraint instead of the normal strength and WXMWY separately.",
                    "label": 0
                },
                {
                    "sent": "Then this is the Lagrangian.",
                    "label": 0
                },
                {
                    "sent": "With only one LaGrange multiplier and you immediately.",
                    "label": 0
                },
                {
                    "sent": "Gets the eigenvalue problem as a result.",
                    "label": 0
                },
                {
                    "sent": "So let me just give a few properties.",
                    "label": 0
                },
                {
                    "sent": "So on.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of course you have an eigenvector for each different possible value of Lambda.",
                    "label": 0
                },
                {
                    "sent": "An egg and vectors corresponding to.",
                    "label": 0
                },
                {
                    "sent": "Different values of Lambda.",
                    "label": 0
                },
                {
                    "sent": "So say eigenvector value XINWXJ&WYNWYJ.",
                    "label": 0
                },
                {
                    "sent": "Each time you have WXLWY and satisfy certain interesting properties, namely.",
                    "label": 0
                },
                {
                    "sent": "You can show that WXI is orthogonal to WXJ.",
                    "label": 0
                },
                {
                    "sent": "There should be a transpose here and here.",
                    "label": 0
                },
                {
                    "sent": "Also, the same holds for Wii and Wii J.",
                    "label": 0
                },
                {
                    "sent": "And the projections are orthogonal or uncorrelated.",
                    "label": 0
                },
                {
                    "sent": "So this means that the different directions at a different weight vectors you'll find.",
                    "label": 0
                },
                {
                    "sent": "For the largest covariance direction for the second largest covariance direction and so on, they were talking to each other, so you get a nice orthogonal subspace if you pick a few of them.",
                    "label": 0
                },
                {
                    "sent": "So to see what it gives on the data, I repeated a couple PCA results and here is the result for.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Glass problem you see that?",
                    "label": 0
                },
                {
                    "sent": "Even more than in a couple PCA case.",
                    "label": 0
                },
                {
                    "sent": "This direction is shifted a bit, so this is a bit lower.",
                    "label": 0
                },
                {
                    "sent": "This bit higher and similarly here.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I don't know, maybe it makes sense that you have less rain if temperature is larger, but on the other hand it's not exactly the case because it would you point is larger than you have more chance on rain.",
                    "label": 0
                },
                {
                    "sent": "So in some sense it's not optimal.",
                    "label": 0
                },
                {
                    "sent": "It's still a bit hard to interpret, and probably one of the reasons maybe that PCA maximizes the variance with this scale dependent.",
                    "label": 0
                },
                {
                    "sent": "Maybe the Geo point and the temperature should be expressed in a different scale or similarly for the inflow or precipitation.",
                    "label": 0
                },
                {
                    "sent": "Imagine, for example, that one of the two datasets, say X contains 1.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Parameter which is expressed in a distance in meters and another variable which is expressed in degrees Celsius.",
                    "label": 0
                },
                {
                    "sent": "Then it doesn't really make sense to talk about variances because it depends on the scale you're using and there is no way to normalize dinner in a fairway.",
                    "label": 0
                },
                {
                    "sent": "So that's a drawback of this method.",
                    "label": 0
                },
                {
                    "sent": "So the third alternative is probably.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Intuitively, by far the best, which tries to find directions such that if you project the data on them, you get a maximal correlation.",
                    "label": 0
                },
                {
                    "sent": "An result will be scale independent, so it doesn't matter if you have different.",
                    "label": 0
                },
                {
                    "sent": "Scales at all, so again the correlation can be expressed as a Rayleigh quotient, so this is a covariance between the projection on WXW why this is.",
                    "label": 0
                },
                {
                    "sent": "The variance or the standard deviation after taking the square root?",
                    "label": 0
                },
                {
                    "sent": "In the attic space, this is a standard deviation of Oxford Square roots.",
                    "label": 0
                },
                {
                    "sent": "In the wide space, and together this yields the correlation.",
                    "label": 0
                },
                {
                    "sent": "Again, we have to fix the scale because again, if you multiply the value equity fixed constants, the objective remains the same.",
                    "label": 0
                },
                {
                    "sent": "So to obtain just one solution we have to fix the scale.",
                    "label": 0
                },
                {
                    "sent": "I now immediately take the easy way, we just want constraints.",
                    "label": 0
                },
                {
                    "sent": "Which is this constraint, and this constraint is sufficient to fix the scale of both WX&WY.",
                    "label": 0
                },
                {
                    "sent": "So this is the optimization problem you want to solve.",
                    "label": 0
                },
                {
                    "sent": "We can write the Lagrangian with LaGrange multiplier.",
                    "label": 0
                },
                {
                    "sent": "Lambda writes 1/2 just for convenience because afterwards is a bit easier than an if you create the gradients with respect to WX&WY to 0 again, you get this as a result.",
                    "label": 0
                },
                {
                    "sent": "So now you see that you have on the right hand side.",
                    "label": 0
                },
                {
                    "sent": "Matrix differ.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From the identity, with here covariance of X covariance of Y as well.",
                    "label": 0
                },
                {
                    "sent": "So in this sense it is normalized.",
                    "label": 0
                },
                {
                    "sent": "The data is normalized automatically.",
                    "label": 0
                },
                {
                    "sent": "So I repeated the eigenvalue problem here and again we can find some properties for directions from different eigenvectors.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in this case the different weight factors corresponding to different diagonals or not orthogonal, but the projections on them they run correlated.",
                    "label": 0
                },
                {
                    "sent": "Which can be expressed in this way.",
                    "label": 0
                },
                {
                    "sent": "So which is another nice property?",
                    "label": 0
                },
                {
                    "sent": "It's different from orthogonality, of course.",
                    "label": 0
                },
                {
                    "sent": "Probably in many cases it's more interesting because it means that projections on different weight vectors corresponding to different eigenvalues actually contain really different information.",
                    "label": 0
                },
                {
                    "sent": "They're unrelated with previous.",
                    "label": 0
                },
                {
                    "sent": "W axis so if you look at the result I repeated PLS result again and this is the Canonical correlation result.",
                    "label": 0
                },
                {
                    "sent": "So now we really shifted quite a lot but.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is quite easy to attach an interpretation to the result because.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More we are to the right on this line.",
                    "label": 0
                },
                {
                    "sent": "The larger the temperature mine minus two point is.",
                    "label": 0
                },
                {
                    "sent": "Because the temperature is larger, the Geo point is smaller, and as you may know, Geo Point actually is a measure of how much moisture moisture is in the air and it is expressed in an attempt as a temperature and if it real temperature drops below the dew point, which is the temperature and it starts raining.",
                    "label": 0
                },
                {
                    "sent": "So in fact temperature minus zero point is a very good indication of how likely it is going to rain today and you can see indeed.",
                    "label": 0
                },
                {
                    "sent": "Here this components.",
                    "label": 0
                },
                {
                    "sent": "It only measures the precipitation exactly is so.",
                    "label": 0
                },
                {
                    "sent": "The more we are in that direction, the less rain we have, the more we are in this direction, the larger the temperature minus the jewel point is, so the less likely is going to rain.",
                    "label": 0
                },
                {
                    "sent": "So indeed, apparently the first DCA component is something that tells you whether it's raining or not.",
                    "label": 0
                },
                {
                    "sent": "So it is possible to attach an interpretation to this much more easily.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The summary of all the different different eigenvalue problems at the top is just PC on the different datasets separately.",
                    "label": 0
                },
                {
                    "sent": "I did not really discuss it because.",
                    "label": 0
                },
                {
                    "sent": "Nothing special, this was PCA on the combined datasets.",
                    "label": 0
                },
                {
                    "sent": "You can see that then we already incorporate some covariance between the two datasets.",
                    "label": 0
                },
                {
                    "sent": "PLS ignores the variance within the data sets itself and PCA even compensates for it.",
                    "label": 0
                },
                {
                    "sent": "So it says OK if you along a certain direction.",
                    "label": 0
                },
                {
                    "sent": "If you find a large covariance it doesn't count as much.",
                    "label": 0
                },
                {
                    "sent": "If you also have a large variance within the data set itself.",
                    "label": 0
                },
                {
                    "sent": "So in that sense, the importance of the variance within the datasets themselves decreases if you go down and.",
                    "label": 0
                },
                {
                    "sent": "In this table and.",
                    "label": 0
                },
                {
                    "sent": "One important fact is that the projections on the weight factors.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "In the first 3 cases, they change if you change the scale.",
                    "label": 0
                },
                {
                    "sent": "So if you can rotate the data and the projections on resulting weight factors will remain the same, so essentially nothing changes about the solution.",
                    "label": 0
                },
                {
                    "sent": "But if you use another invertible transformation besides rotation, then the result will be different.",
                    "label": 0
                },
                {
                    "sent": "That was basically what I said about.",
                    "label": 0
                },
                {
                    "sent": "If you use data containing different express in different measures, for example it sells using meters, but for CCA it is invariant with respect to any invertible transformation.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Summarize the advantages of PCA.",
                    "label": 0
                },
                {
                    "sent": "MPLS are in fact they are robust against noise because in many cases you can assume that noise is small.",
                    "label": 0
                },
                {
                    "sent": "If it's large, you cannot do anything anyway, so you have to assume it is.",
                    "label": 0
                },
                {
                    "sent": "It is small.",
                    "label": 0
                },
                {
                    "sent": "And by only taking into account large variances, they they're capable to deal with it.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, we have seen at CCA is in fact more easy to understand, and results are easier to interpret.",
                    "label": 0
                },
                {
                    "sent": "Also, maybe there is something in between we can do such that it's still interpretable, but on the other hand we have some better noise robustness and then I will show Now how this can be done within a regularization perspective.",
                    "label": 0
                },
                {
                    "sent": "So the goal of regularization is to take PCA, adapted a bit just to make it more noise, robust, but not to lose its properties.",
                    "label": 0
                },
                {
                    "sent": "That is easy to interpret.",
                    "label": 0
                },
                {
                    "sent": "And then maybe Europe?",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Amber from the lecture I gave on Saturday that regularization often involves the upper bounding or the minimization of a normal overweight factor, so that's what we're going to do.",
                    "label": 0
                },
                {
                    "sent": "You will add a cost for a large weight vector WX, and for a large phase factor, WY.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if this was the CCA optimization problem with your covariance of the projections and with this.",
                    "label": 0
                },
                {
                    "sent": "Equated to want to fix the scale.",
                    "label": 0
                },
                {
                    "sent": "And then we add a regularization term that is essentially the sum of the norms of WX of WY times constants that we can choose.",
                    "label": 0
                },
                {
                    "sent": "So then the optimum becomes this with just at the optimization problem becomes this or just this is added.",
                    "label": 0
                },
                {
                    "sent": "So we want to maximize this, which means that we want to find because there is a - We want to find solutions with a large correlation, but at the same time with a small norm of WX&WY.",
                    "label": 0
                },
                {
                    "sent": "If we solve this in the same way with LaGrange multipliers and.",
                    "label": 0
                },
                {
                    "sent": "It's on.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we get this as a result, or the only difference with the CCA in value problem is that we have minus gamma times the identity matrix here and here.",
                    "label": 0
                },
                {
                    "sent": "So that's the only the only difference.",
                    "label": 0
                },
                {
                    "sent": "Now what is what is.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First thing about this optimization problem is that it actually gives an overview on all the other adding value problems I discussed until now.",
                    "label": 0
                },
                {
                    "sent": "In fact, by varying gamma you interpolate between the extreme case of doing PCA on X&Y separately, so no coupling at all.",
                    "label": 0
                },
                {
                    "sent": "An on the other hand, CCA Canonical correlation analysis.",
                    "label": 0
                },
                {
                    "sent": "You can see this as so gamma has to be positive.",
                    "label": 0
                },
                {
                    "sent": "I didn't say that, but gamma is always positive.",
                    "label": 0
                },
                {
                    "sent": "To get a reasonable good regularization effect.",
                    "label": 0
                },
                {
                    "sent": "So for gamma, really large.",
                    "label": 0
                },
                {
                    "sent": "These covariance matrices they don't matter, so you can just drop them in the limit.",
                    "label": 0
                },
                {
                    "sent": "What you get then is this, after some reorganization, which is indeed in the end PC on.",
                    "label": 0
                },
                {
                    "sent": "An ex separately and why separately?",
                    "label": 0
                },
                {
                    "sent": "If gamma decreases now and it decreases the certain extent that Lambda becomes equal to minus one, then.",
                    "label": 0
                },
                {
                    "sent": "OK, so Lambda becomes equal to 1 -- 1, so we can bring this.",
                    "label": 0
                },
                {
                    "sent": "To the other side of the equation of the equality sign, an identity, times identity to this site and then we get this as a result, which is if you remember the PCA on the couple data set XY.",
                    "label": 0
                },
                {
                    "sent": "Then if you decrease gamma regularization parameter even more until love now becomes equal to 0, then you get these results because Lambda is 0.",
                    "label": 0
                },
                {
                    "sent": "So this is just come.",
                    "label": 0
                },
                {
                    "sent": "If you do it even more than you get something like this so it's not equal to anything now.",
                    "label": 0
                },
                {
                    "sent": "Still a continuous parameter which is regularised CCA, as many people know better than in this form.",
                    "label": 0
                },
                {
                    "sent": "And then she equated to zero you just get Canonical correlation analysis.",
                    "label": 0
                },
                {
                    "sent": "Just the diagonal just disappears.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of a nice way of interpreting all these different angle value problems.",
                    "label": 0
                },
                {
                    "sent": "That's regularize versions of Canonical correlation analysis.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What is GIFs?",
                    "label": 0
                },
                {
                    "sent": "If you're very gamma, I start here in these pictures with a fairly large gamma.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which is just VCM.",
                    "label": 0
                },
                {
                    "sent": "Two different datasets separately, and now if we decrease the regularization, you see that slowly the directions evolve towards the CCA directions.",
                    "label": 0
                },
                {
                    "sent": "So this one CCA solution.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, So what can we do with this regularization?",
                    "label": 0
                },
                {
                    "sent": "How is it useful?",
                    "label": 0
                },
                {
                    "sent": "Well, even if you are just interested in finding correlations and not anything like, you're not interested in the variance itself.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it is beneficial to regularize, and this is the case if you have few data or lots of noise or high dimensionality.",
                    "label": 0
                },
                {
                    "sent": "Because Daniel fact you don't have enough information to learn from, so you need to restrict the search space here looking over.",
                    "label": 0
                },
                {
                    "sent": "So to show why to show that this is the case, I trained.",
                    "label": 0
                },
                {
                    "sent": "I learned WX&WY 2 weight factors just 12 of the month, so not all the 454, but just 12 of them.",
                    "label": 0
                },
                {
                    "sent": "And then I evaluate what the correlation is.",
                    "label": 0
                },
                {
                    "sent": "Of the test points to the remaining 442 data points.",
                    "label": 0
                },
                {
                    "sent": "With these weight factors, learning on the 12 ones.",
                    "label": 0
                },
                {
                    "sent": "So effects I trained on a small test set an evaluates the correlation on on the test set.",
                    "label": 0
                },
                {
                    "sent": "And then I did this for varying gamma 4.",
                    "label": 0
                },
                {
                    "sent": "So this is gamma very large, very strong.",
                    "label": 0
                },
                {
                    "sent": "Regularization is for gamma really small, so at Infinity we have CCA and you see that.",
                    "label": 0
                },
                {
                    "sent": "At the.",
                    "label": 0
                },
                {
                    "sent": "Correlation on the training set.",
                    "label": 0
                },
                {
                    "sent": "So indeed the correlation on the training set is maximal for CCA case.",
                    "label": 0
                },
                {
                    "sent": "Very much in that direction, because CCA exactly maximizes the correlation, so of course it has to be maximal for CCA.",
                    "label": 0
                },
                {
                    "sent": "But on the other hand, if you look at the correlation on the test set, then the maximum is not achieved for CCA case because of the noise and the side of the training set, which is very small.",
                    "label": 0
                },
                {
                    "sent": "But at some non 0 value of the regularization parameter right here.",
                    "label": 0
                },
                {
                    "sent": "I will not go into ways to find an optimal value of the regularization parameter, but there are several ways to do this.",
                    "label": 0
                },
                {
                    "sent": "OK, and also on the 1st.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, I said that I would show a way through Fisher discriminant analysis and rich aggression more based on eigenvalue problem.",
                    "label": 0
                },
                {
                    "sent": "Well, actually CCAR regularize CCA is rich regression.",
                    "label": 0
                },
                {
                    "sent": "Or you can see that's rich aggression with a more dimensional datasets.",
                    "label": 0
                },
                {
                    "sent": "Why so serious?",
                    "label": 0
                },
                {
                    "sent": "Let's let's take the datasets.",
                    "label": 0
                },
                {
                    "sent": "Why to be 1 dimensional?",
                    "label": 0
                },
                {
                    "sent": "So then why this factor is just a scalar and the scale doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "So let's just fix it to one.",
                    "label": 0
                },
                {
                    "sent": "So we equate W. Why to one?",
                    "label": 0
                },
                {
                    "sent": "And then this equation, the regularised CCA equation reduces to this.",
                    "label": 0
                },
                {
                    "sent": "So why is just a vector just one dimensional?",
                    "label": 0
                },
                {
                    "sent": "Then after reordering this bit.",
                    "label": 0
                },
                {
                    "sent": "The first role if tries to this.",
                    "label": 0
                },
                {
                    "sent": "WX has to be equal to.",
                    "label": 0
                },
                {
                    "sent": "His expression, which is proportional to this, and maybe you remember this equation is exactly the rich regression equation where this parameter is replaced by the proportion of gamma and Lambda.",
                    "label": 0
                },
                {
                    "sent": "And Lambda you can compute it in this way, but it's not so important what exactly Lambda is, just a constant sort of form is exactly identical to rich aggression, so this means that rich aggression is the same, as regularised CCA between a multi dimensional data set X and a one dimensional data set Y.",
                    "label": 0
                },
                {
                    "sent": "Now maybe some of you know multi multiple discriminant analysis?",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "OK, OK, let me first say that if Y is binary, so plus one and minus one only, then since we saw that rich regression and Fisher discriminant analysis or actually also almost the same except for in Fisher discriminant analysis, why is binary you have exactly the same case?",
                    "label": 0
                },
                {
                    "sent": "The same thing here first.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Commitment analysis is also a special case of CCA or regularised CCA.",
                    "label": 0
                },
                {
                    "sent": "Now sometimes it is.",
                    "label": 0
                },
                {
                    "sent": "It is useful.",
                    "label": 0
                },
                {
                    "sent": "To do something like Fisher discriminant analysis, if you have more than just two classes more than just a binary classification problem, and then in that case you can represent the classes of the data points using matrix like this.",
                    "label": 0
                },
                {
                    "sent": "So for every class you have points.",
                    "label": 0
                },
                {
                    "sent": "For every class you have a column for every data points.",
                    "label": 0
                },
                {
                    "sent": "We have a one in one of the columns depending on depending on which class the data points belongs so.",
                    "label": 0
                },
                {
                    "sent": "This can be summarized in a matrix like this for all the data points are organized such that once in the same class occurs consecutively.",
                    "label": 0
                },
                {
                    "sent": "So A1C1 means a column of ones of C1 ones basically, and then you just have to censor it.",
                    "label": 0
                },
                {
                    "sent": "That's not so important, and now you can do multiple discriminant analysis using the data matrix X on the one hand data matrix, matrix Y label matrix, on the other hand, and what you get then.",
                    "label": 0
                },
                {
                    "sent": "ISM.",
                    "label": 0
                },
                {
                    "sent": "If you get you get directions in the data SpaceX search that the different clusters corresponding to these different columns or as widely separated as possible.",
                    "label": 0
                },
                {
                    "sent": "So in that sense multiple discriminant analysis can be formulated as a Canonical correlation analysis problem.",
                    "label": 0
                },
                {
                    "sent": "So just for everything was about two datasets and X data set and a wide data set.",
                    "label": 0
                },
                {
                    "sent": "Maybe things can be done as well if you have more than two datasets, so you could have.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just the French and English corpus, but also a German and Italian and so on.",
                    "label": 0
                },
                {
                    "sent": "And surely you can you can get better results and you can be better at this extracting semantics if you can do this.",
                    "label": 0
                },
                {
                    "sent": "In a good way, so I will show an easy way to Excel.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And is that have some nice interpretations.",
                    "label": 0
                },
                {
                    "sent": "So what we had if we just had two datasets and we maximize this for XI, was equal to an ex data set XJ2Y data set with all summation because we only had two datasets.",
                    "label": 0
                },
                {
                    "sent": "And then a constraint that says that the sum of the two covariances within the two datasets had to be equal to 1.",
                    "label": 0
                },
                {
                    "sent": "Now it's easy to extend this, namely, just summing over all pairs of datasets, and here summing over all the datasets individually.",
                    "label": 0
                },
                {
                    "sent": "So this is just intuitive way of extending it is not very well motivated.",
                    "label": 0
                },
                {
                    "sent": "Of course, whether or some nice interpretations.",
                    "label": 0
                },
                {
                    "sent": "If you know that.",
                    "label": 0
                },
                {
                    "sent": "I do normal square norm of the sum of all the projections of the data XI on their corresponding weight factor is equal to this.",
                    "label": 0
                },
                {
                    "sent": "This sum which you see here plus this which is this some which is fixed.",
                    "label": 0
                },
                {
                    "sent": "So it's constant anyway and you see that instead of maximizing this with respect to this constraint, we can also maximize this with respect to the same constraint.",
                    "label": 0
                },
                {
                    "sent": "So resulting optimization problem is this and what that exactly is is OK, so we have.",
                    "label": 0
                },
                {
                    "sent": "Say K datasets so.",
                    "label": 0
                },
                {
                    "sent": "XI from X1 up to XK.",
                    "label": 0
                },
                {
                    "sent": "And then let's project each of these on their weight vector.",
                    "label": 0
                },
                {
                    "sent": "So then we have XI times Wii.",
                    "label": 0
                },
                {
                    "sent": "And then this projection is a vector of length equal to the number of data points you have in each of the datasets.",
                    "label": 0
                },
                {
                    "sent": "So I represented these factors here by all these small arrows.",
                    "label": 0
                },
                {
                    "sent": "Now what you want to maximize is the norm of the sum of all these arrows.",
                    "label": 0
                },
                {
                    "sent": "Which is this long arrow for the square norm of this long arrow subject to the constraints that the sum of the squares of each of the individual norms is constant equal to 1.",
                    "label": 0
                },
                {
                    "sent": "The way to the best way to achieve this is to have them all in the same direction.",
                    "label": 0
                },
                {
                    "sent": "So in the end you try to stretch all these arrows as much as possible such that they are.",
                    "label": 0
                },
                {
                    "sent": "As similar to each other as possible, which again amounts to finding at direction in each of the data spaces such that they capture a very similar semantic meaning in these data datasets.",
                    "label": 0
                },
                {
                    "sent": "Resulting eigenvalue problem is given by this, so it look.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maybe a bit complicated, but in fact it's not really these.",
                    "label": 0
                },
                {
                    "sent": "This right hand side matrix just contains covariance matrix on the agonal and on the left you have all the cross covariance matrices.",
                    "label": 0
                },
                {
                    "sent": "So I gave one geometric interpretation of what is exactly doing.",
                    "label": 0
                },
                {
                    "sent": "Another interpretation is a probabilistic one.",
                    "label": 0
                },
                {
                    "sent": "So imagine you have you assume a model, a noisy model for the data which is as follows data.",
                    "label": 0
                },
                {
                    "sent": "Each data set XI is generated using some underlying components, so you assume there are some components generating this data.",
                    "label": 0
                },
                {
                    "sent": "Plus some noise, so the components are affected by noise and then they are transformed in some way using matrix WI inverse.",
                    "label": 0
                },
                {
                    "sent": "So we can write it just in this way, which says that if we transform the matrix, the data XI matrix WI, we just see the individual components plus some noise.",
                    "label": 0
                },
                {
                    "sent": "Now this eigenvalue problem multiway CCA if you want to call it that way is in fact a maximum likelihood estimator for.",
                    "label": 0
                },
                {
                    "sent": "This matrix is WI under certain assumptions.",
                    "label": 0
                },
                {
                    "sent": "So this is another way to see that indeed, because this C is equal for all the eyes for all the datasets, it is a way to extract a common underlying factor underlying each of these datasets.",
                    "label": 0
                },
                {
                    "sent": "Again, you can regularize this and very often is needed if you have.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I love data and is done in a very similar way and very similar properties hold if you vary gamma from really large to small.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me conclude.",
                    "label": 0
                },
                {
                    "sent": "So I think at least I try to show that I can value problems are very powerful and you can do many things with it, even though I only talked about one class of problems, namely finding connections between datasets.",
                    "label": 0
                },
                {
                    "sent": "Or noise dimensionality reduction in the PCA case, but you can do many other things with eigenvalue problems that are quite active.",
                    "label": 0
                },
                {
                    "sent": "Research domains recently, for example, you can do spectral clustering, which means doing clustering using just an eigenvalue problem.",
                    "label": 0
                },
                {
                    "sent": "On on a matrix that is computed based on the data and apparently also classification and regression.",
                    "label": 0
                },
                {
                    "sent": "Fisher discriminant analysis and rich regression can be seen within the same framework.",
                    "label": 0
                },
                {
                    "sent": "If you want to.",
                    "label": 0
                },
                {
                    "sent": "One thing that is very interesting about all these algorithms is that they are.",
                    "label": 0
                },
                {
                    "sent": "They can be expressed completely in terms of inner products.",
                    "label": 0
                },
                {
                    "sent": "And as you will see in the next lecture, I think this helps you in making them all nonlinear so that you can.",
                    "label": 0
                },
                {
                    "sent": "Find really weird shape.",
                    "label": 0
                },
                {
                    "sent": "So I mean, if you have a non linear data set you can really easily find a structure within it.",
                    "label": 0
                },
                {
                    "sent": "OK, so that concludes my lecture.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Why is partially square?",
                    "label": 0
                },
                {
                    "sent": "Actually.",
                    "label": 0
                },
                {
                    "sent": "Honestly, I don't know.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "I think it might be because I only presented part of partial least squares.",
                    "label": 0
                },
                {
                    "sent": "Usually this is done in several steps.",
                    "label": 0
                },
                {
                    "sent": "So what I showed is an eigenvalue problem and you can solve for all the eigenvectors and all the eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "If you want all at once.",
                    "label": 0
                },
                {
                    "sent": "But usually they use partially squares to do regression an they do it in a much more complicated way than just in one eigenvalue step, just using one eigenvalue problem and they deflate and so on.",
                    "label": 0
                },
                {
                    "sent": "And I think that's why author deflation they only use part of the remaining, they only use the remaining variance in the data.",
                    "label": 0
                },
                {
                    "sent": "To do PLS again so it looks like they always release crash problem only on only a part of the data still.",
                    "label": 0
                },
                {
                    "sent": "Maybe that's not the answer.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "You mentioned that you did.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Bristol.",
                    "label": 0
                },
                {
                    "sent": "What about models?",
                    "label": 0
                },
                {
                    "sent": "That example needs to factorization probabilistic speciales in these types of models that Starbucks parser construction sometimes and.",
                    "label": 0
                },
                {
                    "sent": "Linear algebra.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I wanted to talk only about adding value problems because it is anger.",
                    "label": 0
                },
                {
                    "sent": "Problems are computationally very well studied and so you can just pick any eigenvalue algorithm from the shelf and use it immediately.",
                    "label": 0
                },
                {
                    "sent": "So I think that's the strength of this kind of algorithms.",
                    "label": 0
                },
                {
                    "sent": "I agree that of course in many cases they don't.",
                    "label": 0
                },
                {
                    "sent": "They never result in sparsity, for example, which is something that is very often desirable, but indeed then you have to resort to different methods.",
                    "label": 0
                },
                {
                    "sent": "That's true.",
                    "label": 0
                }
            ]
        }
    }
}