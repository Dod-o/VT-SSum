{
    "id": "556dxkiiwkgk4khcqd44xz7fdradhzr3",
    "title": "Iterative Learning of Weighted Rule Sets for Greedy Search",
    "info": {
        "author": [
            "Yuehua Xu, Oregon State University"
        ],
        "published": "Nov. 15, 2010",
        "recorded": "May 2010",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icaps2010_xu_greedysearch/",
    "segmentation": [
        [
            "Hi, sorry about dinner.",
            "I'm your heart and I'm going to present my work with Anna and Son Group.",
            "They are iterative learning of weighted rulesets for greedy search."
        ],
        [
            "So we are considering the terministic strips planning and aims to learn efficient planners from gaming target domain so.",
            "In our we consider the learning problems for learning efficient planners.",
            "Here we can see their training.",
            "Data are represented, is really sorry.",
            "Training data represents their prior planning experiments for our target domain and each of their training.",
            "Example, user planning problem with a solution trajectory and."
        ],
        [
            "Since we want to learn efficient planner based on this solve the problems.",
            "The first question is that what type of efficient planner?"
        ],
        [
            "We want to learn so we can see the greedy search where we come approach to achieve efficiency in greedy search assumes greedy control knowledge that can be evaluated on all search nodes and starting from the initial.",
            "We can select their best note according to the Greek language and we will continue this process until I go is found and after the goal is found, we can backtrack to get this solution trajectory.",
            "As we can see their performance of greedy search or depends on their goodness of their control knowledge.",
            "If they control only is good, we can find the solution very quickly, but when it's not accurate enough it often fails.",
            "Then in there.",
            "Framework of greedy starts.",
            "The problem of learning.",
            "Patient planners convert to learning good control knowledge that can guide the greedy search.",
            "Now our question comes to how to represent the control knowledge and also how to learn the cultural knowledge."
        ],
        [
            "The one major problem approaches before, while major prior approaches to learn such control, EG is to learn decision needs of action selection rules.",
            "As we can see, these actions are actions, words, count and reference.",
            "Some good, controlling from control knowledge.",
            "But however, in many domains there it has observed that these learned policies are often imperfect perfect and could result in poor performance.",
            "Here we address 3 reasons for this line cover."
        ],
        [
            "The first problem.",
            "For Pryor no.",
            "No approach for Ruby star.",
            "Sorry for reaction.",
            "For their role based policies is that each decision of their policy is made by a single rule in their nest.",
            "What we have multiple rules in the policy will each decision is made by only, I think, so we are wondering whether we can use many rules and let them vote to make their decision in machine learning.",
            "It has shown that usually their voting will help."
        ],
        [
            "And the second problem we have found for the anonymous approaches for robust policy is that these learning approaches.",
            "Often they must forces forced policies to copy their reactions and sequence scheming in their training data.",
            "For example here we have an example action sequence DY 2.",
            "And in the planning domains, there are often many good orderings of these actions that are not included in the training set there.",
            "Here.",
            "Suppose A1 and A2.",
            "The there are no essential difference between them, and then it's not reasonable to restrict.",
            "One must be ranked must be selected compared to a two.",
            "It would be confusing for their learning problem so.",
            "Um?",
            "This many good are ordering of these actions this problem.",
            "It will make make their learning problem more difficult."
        ],
        [
            "And the third problem, we have observed that he's an early learning approaches.",
            "Their unit don't come there doesn't consider the actual search performance of the normal policy.",
            "So giving those planning problems with solutions.",
            "Noney approaches were extract training data.",
            "And then run learner to learn their policies.",
            "However, they learn to control knowledge will never be tested.",
            "So we don't know whether they were good or not.",
            "They will work well or not."
        ],
        [
            "All these three problems has motivates us to study.",
            "Some other approaches and other type of control knowledge.",
            "I'm going to present our work that solves all these three problems.",
            "So for the first problem as.",
            "We are going.",
            "I'm going to introduce a new form of control knowledge that allows multiple rules to vote.",
            "So each of them providing its parents and for the ambiguity in their training data, we are going to make your use of their passion, order, plan and defend their learning goal and solve forcing greedy search to remain consistent with their passion order plants.",
            "So instead of sit to copy arbitrary action sequence.",
            "Also, we are going to give me an algorithm that can tightly integrate learning with search."
        ],
        [
            "So there is a remainder of it."
        ],
        [
            "Talk so I will first introduce the representation of our control learning."
        ],
        [
            "So here again.",
            "I give some examples of the actions and exchange rules so they are used previously in to represent the reactive policies.",
            "It has shown that the learners that can learn these good actions and exchange rules cheese.",
            "Then how can we use this rules in our more robust way?"
        ],
        [
            "We proposed.",
            "To use it in the form of which rule steps so there waiting rulesets were consists of a set of rules, each of them has its own weight, so each possible rule.",
            "Depends are binary feature state go action cheaper?",
            "For example, if our suggests action A instead, As for Gyorgy, then we assign South it our values 1.",
            "Otherwise we'll set our values also.",
            "In this way we can view them as features of state transitions and we can evaluate these features on those statuses transitions.",
            "Then we defend our ranking function in the base as a linear combination of these features.",
            "Here, if it's done in a combination of their features.",
            "If I SDA, then by doing this.",
            "Their framework of greedy search, the rank of our each transition is the sum of weights of words that suggest suggest that transition unless it can be used to guide agresearch games.",
            "The information which node should we select in the greedy search process?"
        ],
        [
            "And after introducing the representation of this control knowledge, then we will need to.",
            "Study their approach approaches to learning to learn this control knowledge before introduce that I will first.",
            "Introduce ranking problems and rank boost on which our learning algorithm will be based."
        ],
        [
            "Our traditional ranking problem.",
            "It has input as a set of ordered pairs, so here are for example appear you are you a pair of you.",
            "We indicate that we should be ranked higher than you.",
            "The output is the ranking function that can minimize this, misdirect pairs and to solve this problem.",
            "Rank Booster is a algorithm to solve it, and it learns linear ranking functions."
        ],
        [
            "Rank boost it this algorithm.",
            "It assumes a weak learner that can return a feature with nontrivial performance.",
            "Here in this figure, as we can the input to their weak learner is a set of pairs, and the output is our ranking feature.",
            "If I with nontrivial performance and rank boost, will actually iterate to iteratively create new features.",
            "And no new weights and each iteration after a new feature is not ranked, boost were assigned a weight and it will adjust to such that it works in the next generation.",
            "It will focus on those pairs that were incorrectly ranked, so each it will continue doing this process and always focusing on those incorrectly ranked pairs.",
            "And this algorithm has very strong theoretical guarantees."
        ],
        [
            "Nextier again we are more detailed.",
            "Example rank boost.",
            "In this figure, each pair of points ranking instance initially all pairs have the same weight.",
            "The Rock Post Lamp post them 'cause there we can order to non ranking feature here the pairs in this circle is there pairs ranked correctly by their feet?"
        ],
        [
            "So for those pills that were not correctly ranked by it, like Boost were increase their weight in their next iteration into our focus.",
            "More on those incorrectly ranked pairs, and this process will continue.",
            "The next rank, the next non official link.",
            "I'm emphasize there Miss ranked players.",
            "And then it will continue to learn our ranking feed."
        ],
        [
            "Shows finally the non ranking function would be on in your combination of all these ranking fields features and the based on the rank boost algorithm."
        ],
        [
            "We extended to allow for prior knowledge.",
            "Here are it's.",
            "In this figure we can see that compared to their previous, the original rank boost algorithm.",
            "This extension allows our initial ranking function as input and then the initial discussion of these pairs will be decided based on the initial ranking function and finally, after learning the output ranking function would be the linear combination of learner features plus the initial ranking function.",
            "It has has similar theoretical properties as rank boost and later I will introduce our when I introduce our learning algorithm we can see that this extension is more useful because we need to incorporate prior knowledge."
        ],
        [
            "So I'm going to introduce their our boosting style algorithm for learning rules and weights."
        ],
        [
            "Back to the planning problem.",
            "As I have introduced, we have the training data as planning problems with solution trajectories.",
            "We consider greedy search and want to learn linear ranking functions that guide greedy search.",
            "There are a record that.",
            "When we mentioned then our problem for their prior approaches on learning reactive policy, one problem is that their force than our policy copies operatories action sequence.",
            "So here in order to.",
            "Be more robust.",
            "We convert their sequential plans in their training data to partially ordered."
        ],
        [
            "Price so in our learning problem Now in this example.",
            "Or read notes indicates they already know.",
            "They indicate their solutions included in the packet order plans.",
            "So there could be multiple solutions.",
            "Then Argo, I used to know.",
            "Now with the rule set so that greedy search according to the control knowledge that is our linear ranking function could it could remain consistent with their passion order plan.",
            "That is when we perform the greedy search, we can always find one solution included in their passion order plan."
        ],
        [
            "Honey, in order to learn such a weighted ruleset, we propose an interpreter learning algorithm.",
            "Idea is to iteratively formulate ranking problems based on the current ranking function, and.",
            "Initially we were have some.",
            "Proin initially we will have an initial ranking function.",
            "For example, maybe they relax planets heuristic and then we were performing equity search.",
            "For example for the initial load.",
            "For as we can see, for all the actions that could be applied on the initial or node we were decide which nodes are good, which nodes are bad based on the passion order plan and we were constructing the learning algorithm, learning problems so that these red nodes, the good nodes will be ranked higher than those blue nodes.",
            "At this process, continue.",
            "So for example here and there.",
            "Cheatham arsenic to their highest ranked node and the highest rank node, which is included in their pasture order plan and then we were a constructor.",
            "More training instances and dispersal processes were continued finally after we.",
            "Since we know the planets of the game in training data, so finally it's guaranteed within a number of steps and it will construct our training set.",
            "We were 'cause there are be prior algorithm and.",
            "Learn a new ranking function.",
            "If this rank function can solve their current problem, that is it.",
            "Come find our solution.",
            "Pass consistent with the partial order plan.",
            "Then the learning will terminate.",
            "Otherwise do this greedy search again, and since it cannot solve this problem, it must make some mistakes before, so this time it may find another pass on this pass.",
            "It will construct much.",
            "Some new training instances, and again we were augmented training set and continue learning a new updated ranking function.",
            "So we will continue doing this process so iteratively construct the training problem and then cause there are prior to corrected ranked.",
            "This pairs so that it can guide the greedy search."
        ],
        [
            "So we have also studied the convergence properties of this learning algorithm.",
            "Under certain assumptions, each call to our price is guaranteed to terminate with offended of amount of time with the ranking notes and also the number of calls to the upper bound.",
            "It cause their their size over the partial or plan.",
            "It would be bounded, so the number of nodes on the partial plan would be bonded, and to our knowledge this is the first convergence with that.",
            "On Saturday work most proud work done to even check whether it converges."
        ],
        [
            "Next I will present our experiment and result."
        ],
        [
            "So we have selected domains from some approaches so that we can compare our results to them and we evaluate different learning approaches on these domains there are ITI is our internal learning algorithm and the IPR prize in an iterative version which only generated one training set.",
            "It won't do it.",
            "It won't.",
            "I repeatedly greedy search, so and are the third one is a prior prior work on learning.",
            "Who is provided features so it doesn't involve feature learning and also there are prior work on.",
            "Learning rule based policies and also we compare these to the FS relax planners, holistic used in greedy search."
        ],
        [
            "So here are the results.",
            "The first, the first column RPL at indicates there perform their results from greedy search using the relax penance holistic.",
            "First number in each cell indicates the number of problems solved in these domains and the second number indicate the medium.",
            "Planets of this solved problems.",
            "For example, appear solves starting problems in Brooks World and the medium plans Nancy is around 3000, and this that doesn't seem good and the second column there for the first 4 domains we don't have.",
            "Similarly, we don't have policies so we cannot compare, but for the last two.",
            "For the next three domains there, because in this work they used polishing at different way.",
            "Here we evaluated by performing the greedy search and we observe that it cannot solve any problem and the third column indicates the private work on learning weights.",
            "For guiding greedy search and their happy proud nitrogen Mercer.",
            "And I TR is a learning method as we can see compared to.",
            "Then part unlearned policies.",
            "The prior work that known policies they waited.",
            "Ruleset is much more effective, so it solves all problems in blocksworld and it solves.",
            "Before examining the philosopher proud redact policy that has been learned cannot solve any problem.",
            "But here it serves all of them with very good solution and we can also observe that our approach is it improves on prior approaches anwit learning because here we are learning both features and weights.",
            "And then ask when we compare the last column and there are people there we can see.",
            "The iterative approach has improved improvement compared to their non iterative approach in the Bronx.",
            "Born in this all the same problems but the planets has been reduced and indie ports it serves much many more problems in general.",
            "It works well."
        ],
        [
            "But in the paper we have given more results.",
            "So for the iterative learning algorithm.",
            "So we have more observe here and there.",
            "First problem we have observed is that as a learning goes on, we may fail to introduce new features.",
            "So one possible reason is that our rule fails to adequately explore the space of their possible rules and also maybe our language of representing.",
            "Their rules, it's limited and we have also observed that as we learn more rules as we increase the number of iterations, when we learn more rules, sometimes they pass planning performance, get worse.",
            "So this requires us to study more on the learning approaches."
        ],
        [
            "And this observations has motivated us for some future directions.",
            "For example, we can consider more powerful with learning algorithm or more powerful control knowledge.",
            "So we want to find improve the convergence with bounds because our analysis is kind of worst case Anna size, so it's possible to improve it and we also want to understand the fundamental limitations on learning for greedy search.",
            "So can one characterize when it's when it's possible to practically compile search of ways we learning because we are performing a greedy search then?",
            "When we learn our good ranking function when we learn to nature then we can avoid a search when we are trying to solve a problem.",
            "So all this directions seems seem interesting to us."
        ],
        [
            "Yeah, thank you.",
            "Caramel can you go back to the experimental?"
        ],
        [
            "Yeah, thank you so full blocks world for example.",
            "What was the train set in terms of instances and what was the test set?",
            "So here are for their password password with package and philosophy I used as they are.",
            "Problems provided on the IPC website so they have 50 problems or 48 problems.",
            "I use their first 15 as training program and there's I mean I use the first 15 problems because they are usually simpler and training problems and then the Earth problems.",
            "Testing for Brooks word.",
            "I use the generator provided.",
            "The general generator and I choose their para meters.",
            "I increase the parameters too.",
            "Cancel their difficulty with problems because for training problems I need some simpler problem and then for testing I use some larger para meters and four DPO driven, again free.",
            "So I think there are only.",
            "15 or 20 problems provided on their website, so I use it the first 15 as training problems.",
            "Then I use the generator to generate more problems as testing.",
            "Hi so my question is if you've tried using this in a more sophisticated search, not just following the policy one step at a time, like you had some previous work in your group on using it with look ahead for example.",
            "So have you tried doing that?",
            "Now for I have some prior work on learning in beam search.",
            "So which is more general than greedy search?",
            "So in that work their features are.",
            "Capturing information about each state, so here their features are capturing this state action pairs.",
            "So I was trying to extend it to two game search with larger B Morris, but somehow they perform a standard seem good so.",
            "So I guess this needs more this need more investigation.",
            "OK."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi, sorry about dinner.",
                    "label": 0
                },
                {
                    "sent": "I'm your heart and I'm going to present my work with Anna and Son Group.",
                    "label": 0
                },
                {
                    "sent": "They are iterative learning of weighted rulesets for greedy search.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we are considering the terministic strips planning and aims to learn efficient planners from gaming target domain so.",
                    "label": 0
                },
                {
                    "sent": "In our we consider the learning problems for learning efficient planners.",
                    "label": 0
                },
                {
                    "sent": "Here we can see their training.",
                    "label": 0
                },
                {
                    "sent": "Data are represented, is really sorry.",
                    "label": 0
                },
                {
                    "sent": "Training data represents their prior planning experiments for our target domain and each of their training.",
                    "label": 0
                },
                {
                    "sent": "Example, user planning problem with a solution trajectory and.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Since we want to learn efficient planner based on this solve the problems.",
                    "label": 0
                },
                {
                    "sent": "The first question is that what type of efficient planner?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We want to learn so we can see the greedy search where we come approach to achieve efficiency in greedy search assumes greedy control knowledge that can be evaluated on all search nodes and starting from the initial.",
                    "label": 0
                },
                {
                    "sent": "We can select their best note according to the Greek language and we will continue this process until I go is found and after the goal is found, we can backtrack to get this solution trajectory.",
                    "label": 0
                },
                {
                    "sent": "As we can see their performance of greedy search or depends on their goodness of their control knowledge.",
                    "label": 0
                },
                {
                    "sent": "If they control only is good, we can find the solution very quickly, but when it's not accurate enough it often fails.",
                    "label": 0
                },
                {
                    "sent": "Then in there.",
                    "label": 0
                },
                {
                    "sent": "Framework of greedy starts.",
                    "label": 0
                },
                {
                    "sent": "The problem of learning.",
                    "label": 0
                },
                {
                    "sent": "Patient planners convert to learning good control knowledge that can guide the greedy search.",
                    "label": 0
                },
                {
                    "sent": "Now our question comes to how to represent the control knowledge and also how to learn the cultural knowledge.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The one major problem approaches before, while major prior approaches to learn such control, EG is to learn decision needs of action selection rules.",
                    "label": 0
                },
                {
                    "sent": "As we can see, these actions are actions, words, count and reference.",
                    "label": 0
                },
                {
                    "sent": "Some good, controlling from control knowledge.",
                    "label": 0
                },
                {
                    "sent": "But however, in many domains there it has observed that these learned policies are often imperfect perfect and could result in poor performance.",
                    "label": 1
                },
                {
                    "sent": "Here we address 3 reasons for this line cover.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The first problem.",
                    "label": 0
                },
                {
                    "sent": "For Pryor no.",
                    "label": 0
                },
                {
                    "sent": "No approach for Ruby star.",
                    "label": 0
                },
                {
                    "sent": "Sorry for reaction.",
                    "label": 0
                },
                {
                    "sent": "For their role based policies is that each decision of their policy is made by a single rule in their nest.",
                    "label": 1
                },
                {
                    "sent": "What we have multiple rules in the policy will each decision is made by only, I think, so we are wondering whether we can use many rules and let them vote to make their decision in machine learning.",
                    "label": 1
                },
                {
                    "sent": "It has shown that usually their voting will help.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the second problem we have found for the anonymous approaches for robust policy is that these learning approaches.",
                    "label": 0
                },
                {
                    "sent": "Often they must forces forced policies to copy their reactions and sequence scheming in their training data.",
                    "label": 0
                },
                {
                    "sent": "For example here we have an example action sequence DY 2.",
                    "label": 0
                },
                {
                    "sent": "And in the planning domains, there are often many good orderings of these actions that are not included in the training set there.",
                    "label": 1
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "Suppose A1 and A2.",
                    "label": 0
                },
                {
                    "sent": "The there are no essential difference between them, and then it's not reasonable to restrict.",
                    "label": 0
                },
                {
                    "sent": "One must be ranked must be selected compared to a two.",
                    "label": 0
                },
                {
                    "sent": "It would be confusing for their learning problem so.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "This many good are ordering of these actions this problem.",
                    "label": 0
                },
                {
                    "sent": "It will make make their learning problem more difficult.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the third problem, we have observed that he's an early learning approaches.",
                    "label": 0
                },
                {
                    "sent": "Their unit don't come there doesn't consider the actual search performance of the normal policy.",
                    "label": 1
                },
                {
                    "sent": "So giving those planning problems with solutions.",
                    "label": 0
                },
                {
                    "sent": "Noney approaches were extract training data.",
                    "label": 0
                },
                {
                    "sent": "And then run learner to learn their policies.",
                    "label": 0
                },
                {
                    "sent": "However, they learn to control knowledge will never be tested.",
                    "label": 0
                },
                {
                    "sent": "So we don't know whether they were good or not.",
                    "label": 0
                },
                {
                    "sent": "They will work well or not.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "All these three problems has motivates us to study.",
                    "label": 0
                },
                {
                    "sent": "Some other approaches and other type of control knowledge.",
                    "label": 0
                },
                {
                    "sent": "I'm going to present our work that solves all these three problems.",
                    "label": 1
                },
                {
                    "sent": "So for the first problem as.",
                    "label": 0
                },
                {
                    "sent": "We are going.",
                    "label": 0
                },
                {
                    "sent": "I'm going to introduce a new form of control knowledge that allows multiple rules to vote.",
                    "label": 1
                },
                {
                    "sent": "So each of them providing its parents and for the ambiguity in their training data, we are going to make your use of their passion, order, plan and defend their learning goal and solve forcing greedy search to remain consistent with their passion order plants.",
                    "label": 1
                },
                {
                    "sent": "So instead of sit to copy arbitrary action sequence.",
                    "label": 0
                },
                {
                    "sent": "Also, we are going to give me an algorithm that can tightly integrate learning with search.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there is a remainder of it.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Talk so I will first introduce the representation of our control learning.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here again.",
                    "label": 0
                },
                {
                    "sent": "I give some examples of the actions and exchange rules so they are used previously in to represent the reactive policies.",
                    "label": 0
                },
                {
                    "sent": "It has shown that the learners that can learn these good actions and exchange rules cheese.",
                    "label": 1
                },
                {
                    "sent": "Then how can we use this rules in our more robust way?",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We proposed.",
                    "label": 0
                },
                {
                    "sent": "To use it in the form of which rule steps so there waiting rulesets were consists of a set of rules, each of them has its own weight, so each possible rule.",
                    "label": 1
                },
                {
                    "sent": "Depends are binary feature state go action cheaper?",
                    "label": 0
                },
                {
                    "sent": "For example, if our suggests action A instead, As for Gyorgy, then we assign South it our values 1.",
                    "label": 0
                },
                {
                    "sent": "Otherwise we'll set our values also.",
                    "label": 0
                },
                {
                    "sent": "In this way we can view them as features of state transitions and we can evaluate these features on those statuses transitions.",
                    "label": 1
                },
                {
                    "sent": "Then we defend our ranking function in the base as a linear combination of these features.",
                    "label": 1
                },
                {
                    "sent": "Here, if it's done in a combination of their features.",
                    "label": 0
                },
                {
                    "sent": "If I SDA, then by doing this.",
                    "label": 0
                },
                {
                    "sent": "Their framework of greedy search, the rank of our each transition is the sum of weights of words that suggest suggest that transition unless it can be used to guide agresearch games.",
                    "label": 1
                },
                {
                    "sent": "The information which node should we select in the greedy search process?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And after introducing the representation of this control knowledge, then we will need to.",
                    "label": 0
                },
                {
                    "sent": "Study their approach approaches to learning to learn this control knowledge before introduce that I will first.",
                    "label": 0
                },
                {
                    "sent": "Introduce ranking problems and rank boost on which our learning algorithm will be based.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our traditional ranking problem.",
                    "label": 0
                },
                {
                    "sent": "It has input as a set of ordered pairs, so here are for example appear you are you a pair of you.",
                    "label": 1
                },
                {
                    "sent": "We indicate that we should be ranked higher than you.",
                    "label": 1
                },
                {
                    "sent": "The output is the ranking function that can minimize this, misdirect pairs and to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "Rank Booster is a algorithm to solve it, and it learns linear ranking functions.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Rank boost it this algorithm.",
                    "label": 0
                },
                {
                    "sent": "It assumes a weak learner that can return a feature with nontrivial performance.",
                    "label": 1
                },
                {
                    "sent": "Here in this figure, as we can the input to their weak learner is a set of pairs, and the output is our ranking feature.",
                    "label": 1
                },
                {
                    "sent": "If I with nontrivial performance and rank boost, will actually iterate to iteratively create new features.",
                    "label": 0
                },
                {
                    "sent": "And no new weights and each iteration after a new feature is not ranked, boost were assigned a weight and it will adjust to such that it works in the next generation.",
                    "label": 0
                },
                {
                    "sent": "It will focus on those pairs that were incorrectly ranked, so each it will continue doing this process and always focusing on those incorrectly ranked pairs.",
                    "label": 1
                },
                {
                    "sent": "And this algorithm has very strong theoretical guarantees.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nextier again we are more detailed.",
                    "label": 0
                },
                {
                    "sent": "Example rank boost.",
                    "label": 0
                },
                {
                    "sent": "In this figure, each pair of points ranking instance initially all pairs have the same weight.",
                    "label": 1
                },
                {
                    "sent": "The Rock Post Lamp post them 'cause there we can order to non ranking feature here the pairs in this circle is there pairs ranked correctly by their feet?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for those pills that were not correctly ranked by it, like Boost were increase their weight in their next iteration into our focus.",
                    "label": 0
                },
                {
                    "sent": "More on those incorrectly ranked pairs, and this process will continue.",
                    "label": 1
                },
                {
                    "sent": "The next rank, the next non official link.",
                    "label": 0
                },
                {
                    "sent": "I'm emphasize there Miss ranked players.",
                    "label": 0
                },
                {
                    "sent": "And then it will continue to learn our ranking feed.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Shows finally the non ranking function would be on in your combination of all these ranking fields features and the based on the rank boost algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We extended to allow for prior knowledge.",
                    "label": 1
                },
                {
                    "sent": "Here are it's.",
                    "label": 0
                },
                {
                    "sent": "In this figure we can see that compared to their previous, the original rank boost algorithm.",
                    "label": 0
                },
                {
                    "sent": "This extension allows our initial ranking function as input and then the initial discussion of these pairs will be decided based on the initial ranking function and finally, after learning the output ranking function would be the linear combination of learner features plus the initial ranking function.",
                    "label": 0
                },
                {
                    "sent": "It has has similar theoretical properties as rank boost and later I will introduce our when I introduce our learning algorithm we can see that this extension is more useful because we need to incorporate prior knowledge.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to introduce their our boosting style algorithm for learning rules and weights.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Back to the planning problem.",
                    "label": 0
                },
                {
                    "sent": "As I have introduced, we have the training data as planning problems with solution trajectories.",
                    "label": 0
                },
                {
                    "sent": "We consider greedy search and want to learn linear ranking functions that guide greedy search.",
                    "label": 1
                },
                {
                    "sent": "There are a record that.",
                    "label": 0
                },
                {
                    "sent": "When we mentioned then our problem for their prior approaches on learning reactive policy, one problem is that their force than our policy copies operatories action sequence.",
                    "label": 0
                },
                {
                    "sent": "So here in order to.",
                    "label": 0
                },
                {
                    "sent": "Be more robust.",
                    "label": 1
                },
                {
                    "sent": "We convert their sequential plans in their training data to partially ordered.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Price so in our learning problem Now in this example.",
                    "label": 0
                },
                {
                    "sent": "Or read notes indicates they already know.",
                    "label": 0
                },
                {
                    "sent": "They indicate their solutions included in the packet order plans.",
                    "label": 0
                },
                {
                    "sent": "So there could be multiple solutions.",
                    "label": 0
                },
                {
                    "sent": "Then Argo, I used to know.",
                    "label": 0
                },
                {
                    "sent": "Now with the rule set so that greedy search according to the control knowledge that is our linear ranking function could it could remain consistent with their passion order plan.",
                    "label": 1
                },
                {
                    "sent": "That is when we perform the greedy search, we can always find one solution included in their passion order plan.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Honey, in order to learn such a weighted ruleset, we propose an interpreter learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "Idea is to iteratively formulate ranking problems based on the current ranking function, and.",
                    "label": 1
                },
                {
                    "sent": "Initially we were have some.",
                    "label": 0
                },
                {
                    "sent": "Proin initially we will have an initial ranking function.",
                    "label": 0
                },
                {
                    "sent": "For example, maybe they relax planets heuristic and then we were performing equity search.",
                    "label": 0
                },
                {
                    "sent": "For example for the initial load.",
                    "label": 0
                },
                {
                    "sent": "For as we can see, for all the actions that could be applied on the initial or node we were decide which nodes are good, which nodes are bad based on the passion order plan and we were constructing the learning algorithm, learning problems so that these red nodes, the good nodes will be ranked higher than those blue nodes.",
                    "label": 0
                },
                {
                    "sent": "At this process, continue.",
                    "label": 0
                },
                {
                    "sent": "So for example here and there.",
                    "label": 0
                },
                {
                    "sent": "Cheatham arsenic to their highest ranked node and the highest rank node, which is included in their pasture order plan and then we were a constructor.",
                    "label": 0
                },
                {
                    "sent": "More training instances and dispersal processes were continued finally after we.",
                    "label": 0
                },
                {
                    "sent": "Since we know the planets of the game in training data, so finally it's guaranteed within a number of steps and it will construct our training set.",
                    "label": 0
                },
                {
                    "sent": "We were 'cause there are be prior algorithm and.",
                    "label": 0
                },
                {
                    "sent": "Learn a new ranking function.",
                    "label": 0
                },
                {
                    "sent": "If this rank function can solve their current problem, that is it.",
                    "label": 0
                },
                {
                    "sent": "Come find our solution.",
                    "label": 0
                },
                {
                    "sent": "Pass consistent with the partial order plan.",
                    "label": 0
                },
                {
                    "sent": "Then the learning will terminate.",
                    "label": 0
                },
                {
                    "sent": "Otherwise do this greedy search again, and since it cannot solve this problem, it must make some mistakes before, so this time it may find another pass on this pass.",
                    "label": 0
                },
                {
                    "sent": "It will construct much.",
                    "label": 1
                },
                {
                    "sent": "Some new training instances, and again we were augmented training set and continue learning a new updated ranking function.",
                    "label": 0
                },
                {
                    "sent": "So we will continue doing this process so iteratively construct the training problem and then cause there are prior to corrected ranked.",
                    "label": 0
                },
                {
                    "sent": "This pairs so that it can guide the greedy search.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we have also studied the convergence properties of this learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "Under certain assumptions, each call to our price is guaranteed to terminate with offended of amount of time with the ranking notes and also the number of calls to the upper bound.",
                    "label": 1
                },
                {
                    "sent": "It cause their their size over the partial or plan.",
                    "label": 0
                },
                {
                    "sent": "It would be bounded, so the number of nodes on the partial plan would be bonded, and to our knowledge this is the first convergence with that.",
                    "label": 1
                },
                {
                    "sent": "On Saturday work most proud work done to even check whether it converges.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Next I will present our experiment and result.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we have selected domains from some approaches so that we can compare our results to them and we evaluate different learning approaches on these domains there are ITI is our internal learning algorithm and the IPR prize in an iterative version which only generated one training set.",
                    "label": 0
                },
                {
                    "sent": "It won't do it.",
                    "label": 0
                },
                {
                    "sent": "It won't.",
                    "label": 0
                },
                {
                    "sent": "I repeatedly greedy search, so and are the third one is a prior prior work on learning.",
                    "label": 1
                },
                {
                    "sent": "Who is provided features so it doesn't involve feature learning and also there are prior work on.",
                    "label": 0
                },
                {
                    "sent": "Learning rule based policies and also we compare these to the FS relax planners, holistic used in greedy search.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here are the results.",
                    "label": 0
                },
                {
                    "sent": "The first, the first column RPL at indicates there perform their results from greedy search using the relax penance holistic.",
                    "label": 0
                },
                {
                    "sent": "First number in each cell indicates the number of problems solved in these domains and the second number indicate the medium.",
                    "label": 0
                },
                {
                    "sent": "Planets of this solved problems.",
                    "label": 0
                },
                {
                    "sent": "For example, appear solves starting problems in Brooks World and the medium plans Nancy is around 3000, and this that doesn't seem good and the second column there for the first 4 domains we don't have.",
                    "label": 0
                },
                {
                    "sent": "Similarly, we don't have policies so we cannot compare, but for the last two.",
                    "label": 0
                },
                {
                    "sent": "For the next three domains there, because in this work they used polishing at different way.",
                    "label": 0
                },
                {
                    "sent": "Here we evaluated by performing the greedy search and we observe that it cannot solve any problem and the third column indicates the private work on learning weights.",
                    "label": 0
                },
                {
                    "sent": "For guiding greedy search and their happy proud nitrogen Mercer.",
                    "label": 0
                },
                {
                    "sent": "And I TR is a learning method as we can see compared to.",
                    "label": 0
                },
                {
                    "sent": "Then part unlearned policies.",
                    "label": 0
                },
                {
                    "sent": "The prior work that known policies they waited.",
                    "label": 0
                },
                {
                    "sent": "Ruleset is much more effective, so it solves all problems in blocksworld and it solves.",
                    "label": 1
                },
                {
                    "sent": "Before examining the philosopher proud redact policy that has been learned cannot solve any problem.",
                    "label": 0
                },
                {
                    "sent": "But here it serves all of them with very good solution and we can also observe that our approach is it improves on prior approaches anwit learning because here we are learning both features and weights.",
                    "label": 1
                },
                {
                    "sent": "And then ask when we compare the last column and there are people there we can see.",
                    "label": 0
                },
                {
                    "sent": "The iterative approach has improved improvement compared to their non iterative approach in the Bronx.",
                    "label": 0
                },
                {
                    "sent": "Born in this all the same problems but the planets has been reduced and indie ports it serves much many more problems in general.",
                    "label": 0
                },
                {
                    "sent": "It works well.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But in the paper we have given more results.",
                    "label": 0
                },
                {
                    "sent": "So for the iterative learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "So we have more observe here and there.",
                    "label": 0
                },
                {
                    "sent": "First problem we have observed is that as a learning goes on, we may fail to introduce new features.",
                    "label": 1
                },
                {
                    "sent": "So one possible reason is that our rule fails to adequately explore the space of their possible rules and also maybe our language of representing.",
                    "label": 1
                },
                {
                    "sent": "Their rules, it's limited and we have also observed that as we learn more rules as we increase the number of iterations, when we learn more rules, sometimes they pass planning performance, get worse.",
                    "label": 1
                },
                {
                    "sent": "So this requires us to study more on the learning approaches.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this observations has motivated us for some future directions.",
                    "label": 0
                },
                {
                    "sent": "For example, we can consider more powerful with learning algorithm or more powerful control knowledge.",
                    "label": 1
                },
                {
                    "sent": "So we want to find improve the convergence with bounds because our analysis is kind of worst case Anna size, so it's possible to improve it and we also want to understand the fundamental limitations on learning for greedy search.",
                    "label": 1
                },
                {
                    "sent": "So can one characterize when it's when it's possible to practically compile search of ways we learning because we are performing a greedy search then?",
                    "label": 0
                },
                {
                    "sent": "When we learn our good ranking function when we learn to nature then we can avoid a search when we are trying to solve a problem.",
                    "label": 0
                },
                {
                    "sent": "So all this directions seems seem interesting to us.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, thank you.",
                    "label": 0
                },
                {
                    "sent": "Caramel can you go back to the experimental?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, thank you so full blocks world for example.",
                    "label": 0
                },
                {
                    "sent": "What was the train set in terms of instances and what was the test set?",
                    "label": 0
                },
                {
                    "sent": "So here are for their password password with package and philosophy I used as they are.",
                    "label": 0
                },
                {
                    "sent": "Problems provided on the IPC website so they have 50 problems or 48 problems.",
                    "label": 0
                },
                {
                    "sent": "I use their first 15 as training program and there's I mean I use the first 15 problems because they are usually simpler and training problems and then the Earth problems.",
                    "label": 0
                },
                {
                    "sent": "Testing for Brooks word.",
                    "label": 0
                },
                {
                    "sent": "I use the generator provided.",
                    "label": 0
                },
                {
                    "sent": "The general generator and I choose their para meters.",
                    "label": 0
                },
                {
                    "sent": "I increase the parameters too.",
                    "label": 0
                },
                {
                    "sent": "Cancel their difficulty with problems because for training problems I need some simpler problem and then for testing I use some larger para meters and four DPO driven, again free.",
                    "label": 0
                },
                {
                    "sent": "So I think there are only.",
                    "label": 0
                },
                {
                    "sent": "15 or 20 problems provided on their website, so I use it the first 15 as training problems.",
                    "label": 1
                },
                {
                    "sent": "Then I use the generator to generate more problems as testing.",
                    "label": 0
                },
                {
                    "sent": "Hi so my question is if you've tried using this in a more sophisticated search, not just following the policy one step at a time, like you had some previous work in your group on using it with look ahead for example.",
                    "label": 1
                },
                {
                    "sent": "So have you tried doing that?",
                    "label": 1
                },
                {
                    "sent": "Now for I have some prior work on learning in beam search.",
                    "label": 1
                },
                {
                    "sent": "So which is more general than greedy search?",
                    "label": 1
                },
                {
                    "sent": "So in that work their features are.",
                    "label": 0
                },
                {
                    "sent": "Capturing information about each state, so here their features are capturing this state action pairs.",
                    "label": 0
                },
                {
                    "sent": "So I was trying to extend it to two game search with larger B Morris, but somehow they perform a standard seem good so.",
                    "label": 0
                },
                {
                    "sent": "So I guess this needs more this need more investigation.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        }
    }
}