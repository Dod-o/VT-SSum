{
    "id": "jsbpk2y6wevxemsb6yinvxomyvxdppjl",
    "title": "Online MKL for Structured Prediction",
    "info": {
        "author": [
            "Andr\u00e9 F. T. Martins, Language Technologies Institute, Carnegie Mellon University"
        ],
        "published": "Jan. 12, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Structured Output",
            "Top->Computer Science->Machine Learning->On-line Learning"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2010_martins_oms/",
    "segmentation": [
        [
            "So we are interested in computing account iconic combination of kernels in the name Cal setting.",
            "So the idea is to."
        ],
        [
            "We have several kernels K1 to KM and you want to come up with a new kernel that is a combination of the existing kernels and you want to embed these into the learning problem."
        ],
        [
            "The first approach is for these used second order cone programming and similar techniques, but unfortunately they could not scale very well and so."
        ],
        [
            "So.",
            "So more recently there have been some wrapper based methods that scale better.",
            "But so these methods require having to repeatedly solving an SVM.",
            "So they're quite looping over this, and this is sometimes a disadvantage because in some in some settings in, for example, if you have lots of data or instruction prediction.",
            "You don't actually manage to guarantee very good accuracy in solving each SVM.",
            "So."
        ],
        [
            "But we are going to propose is an online algorithm that alternates between stochastic gradient steps and possible steps."
        ],
        [
            "And this algorithm is very flexible.",
            "It works for all kinds of convex losses.",
            "We are going specifically to tackle structure prediction.",
            "With this it's compatible with the many different chemical formulations.",
            "Parson on sparse.",
            "It's also good for Lasuen variants of group lawsuits and so forth."
        ],
        [
            "We're going to show some convergence and realization bounds."
        ],
        [
            "Some experiments in actual structure prediction problems."
        ],
        [
            "OK, so let's start."
        ],
        [
            "By describing our structured prediction setting, we have an input set X.",
            "For example, sequences and then output set Y.",
            "For example, labelings for those sequences.",
            "And."
        ],
        [
            "The assumption here is that why is a very large and structured set.",
            "So for example for sequence labeling.",
            "So each element in this output set is going to be a possible labeling of the sequence, and you have exponentially many of those."
        ],
        [
            "So we're going to assume a supervised learning scenario.",
            "We have a data set of IID points and our goal."
        ],
        [
            "All is to learn the compatibility function.",
            "F that Maps pairs of elements in X&Y to some real value and is used to make a new prediction.",
            "So given a new point X, we're going to predict yhat as the output that maximizes this compatibility function.",
            "So if you have some Hilbert space."
        ],
        [
            "It's H andsome kernel K we are."
        ],
        [
            "Need to consider linear classifiers that can be written as the inner product that you were space of, a parameter vector title."
        ],
        [
            "And feature a joint feature map feature Vector Phi of X&Y&Y.",
            "If you have multiple kernels.",
            "We actually discussed sponse to a direct sum of Hubert spaces of smaller Hubert Spaces, H1 to HM1 for each kernel and the computed function can be written as a sum of smaller inner product."
        ],
        [
            "OK, so let me start by stating the main assumption that is usually made in structured prediction in order to make the code decoding problem tractable, you need to come up with.",
            "Some sort of assumption, and this is usually the assumption that is taken that you decompose.",
            "They can decompose the output in two parts.",
            "So essentially we designed our features to be Sam's over over parts for some definition of parts.",
            "If you have a Markov network, these parts can correspond to assignments in vertices and edges.",
            "Of course you can go to clicks in general, and So what we have is in this example I'm representing the joint feature map.",
            "As something defined on the vertices and some features defined on the edges, where each of them."
        ],
        [
            "Our region as well as connector products of features that only depends on the point sites and in the output side at some position in the network.",
            "So if you if we translate this to kernels, this is what we obtain essentially kernel between two pairs of points XY and X prime.",
            "Y prime can be written as a sum for all nodes in each in each of the graphs and some over all edges in both graphs.",
            "Of multiplication of kernels.",
            "So the case depends on the inputs and the L depends on the output.",
            "So there are several iPods to learn the kernel here.",
            "We cannot learn either of these.",
            "Component kernels here we're going to take the simplest assumption, which is we're going to fix this to be just a connector Delta.",
            "So this is only one if the two labeling the two labels are the same.",
            "We do something similar here, but we allow for a constant beta zero that we are going to learn.",
            "We fix.",
            "This cannot be cause and so it can be taken out and we focus on learning this part.",
            "So this is the kernel that you are going to learn here.",
            "So again, we see that you have M kernels KV-12 KVM and you want to learn these coefficients.",
            "OK, so let's talk about."
        ],
        [
            "Learning, we suppose that you have some cost function that qualifies the cost of predicting Y hats when the true output is why?"
        ],
        [
            "We define some convex loss function, for example, the structural image loss.",
            "So this is just a generalization of the binary hinge loss."
        ],
        [
            "We define a regularizer.",
            "And sweet."
        ],
        [
            "And by minimizing the regularise empirical risk.",
            "So if you have to."
        ],
        [
            "There is a shun.",
            "This is very standard.",
            "We just recovered the standard SVM setting where we are using a sum of kernels."
        ],
        [
            "And talking about algorithms to solve these problems.",
            "Some of the fastest to dates are online algorithms.",
            "That's you know have bigger one to one over epsilon.",
            "They require.",
            "Uh oh, bigger one over epsilon iterations like Pegasus and do a good job at training SVM in a logical scenario."
        ],
        [
            "So in general, we're going to think about LPQ norms.",
            "So this is defined, so we assume that there is some block structure in the parameters and we define.",
            "This design is defined as taking the P norm that it's block and then taking the Q norm of the vector of norms."
        ],
        [
            "In sports MCL, the underlying norm that is being used as regularizer is the L2 one norm.",
            "Actually, the square of death."
        ],
        [
            "So in sports in KL just reminds we are constraining these coefficients to be non negative and to sum to one.",
            "And in non sparse MCL.",
            "On another project that have been recently proposed, we constraint instead the P norm of beta to be one or at less than one.",
            "This doesn't matter.",
            "And this corresponds to using an L and L2 Q norm instead of an L2 one regularizer regularizer.",
            "For quick, you relax with this way."
        ],
        [
            "So in general you are interested and we're going to show an algorithm that can angle can handle more general regularizers that can be written as a sum of smaller regularizers.",
            "So this can handle any case where the regularization consists in Assam for the equal 22 J off some weight.",
            "Some wait times, regularizers attacks.",
            "Which of these forms?"
        ],
        [
            "OK, so this is the algorithm that you propose."
        ],
        [
            "It's an online algorithm, it's work."
        ],
        [
            "In rounds at this round, it picks a training pair, XTYT.",
            "It computes a subgradient, and to do that, this corresponds to do some sort of inference.",
            "So if we."
        ],
        [
            "We are working with SVM's.",
            "This means that you need to do map inference if you're using an S SRF loss.",
            "These correspond to marginal inference and then we take a subgradient step.",
            "Right?",
            "Aft."
        ],
        [
            "So that for each regularizer that we have, we compute approximal.",
            "We do approximal step.",
            "Which means that we have to solve this problem.",
            "This is quite efficient for the block normal advertisers that you have just seen, including the squared L1 norm.",
            "Although even with the square there and so there, there are some explanation about that in the paper."
        ],
        [
            "And finally, there is an optional projection step.",
            "Whenever you know abounds in the normal for optimal parameters, and often you can come up with those bounds.",
            "That's essentially guarantees that we stay in the in the in the physical region."
        ],
        [
            "So this can speed up the algorithm.",
            "OK, let's talk about convergence and generalization bounds.",
            "We assume that your losses, Lipschitz with some constant G on the physical region, so this is Delta ball with radius radius, gamma and with."
        ],
        [
            "Probability we get convergence in bigger one over epsilon squared rounds there are.",
            "There are some constants here and you actually are going to quantify these constants for sparse MCL."
        ],
        [
            "In addition, if the loss or the regularizers are strongly convex, this can be improved to over epsilon.",
            "If we exclude arhythmic terms.",
            "Anne.",
            "Digitalization."
        ],
        [
            "These are the same order and this is also."
        ],
        [
            "List using standard techniques in online learning.",
            "Also some fundamental result."
        ],
        [
            "In the in proximity theory.",
            "OK, so for the specific case of space of sparse MKL.",
            "Using either the SVM or the CRF losses, and assuming that the kernels are bounded, we can actually quantify this Lipschitz constant, and these bounds gamma, and so you obtain something like this and you can see that."
        ],
        [
            "It depends linearly in the number of kernels, which is not so good, but it's what this algorithm allows for.",
            "OK, so let's see some experiments.",
            "The first one is in sequence labeling."
        ],
        [
            "In an OCR data set, so this is we have a sequence of characters of images of characters and the goal is to predict what is the character using the correlation between consecutive characters.",
            "So user bigram model for this."
        ],
        [
            "And our first attempt is just combining linear, quadratic and the Gaussian kernel, so this is."
        ],
        [
            "So each kernel performs individually.",
            "You can see that the linear kernel is pretty bad.",
            "We can do much better with the quadratic or Gaussian kernel, the average."
        ],
        [
            "Doesn't help much.",
            "It's doing worse than the quadratic kernel if you do."
        ],
        [
            "Well, using this algorithm we actually come up with significant improvement.",
            "The second exper."
        ],
        [
            "Attempts to.",
            "So if you have a really large scale problem with many examples, it's not the case of this data set.",
            "It's pretty expensive to store the kernel matrix or to do kernel computations, so this is we need to do quadratic computations with respect to the the number of week to sample size and so.",
            "So it's known that if you have explicit features, this problem doesn't exist.",
            "In most of the cases.",
            "So the idea here is to combine explicit features with sparse kernels.",
            "So this is what motivates us to consider the combination of a linear kernels and B1 spline kernels, so here, so these are.",
            "Similar to RBF kernels, but truncate is so this text list looks like a triangle and the idea is that we use a very small width for the triangle, which means that you are essentially selecting just a few neighbors of each point.",
            "And so each of these kernel."
        ],
        [
            "Individual performs quite bad, but."
        ],
        [
            "You take the average we manage to get a very large improvement.",
            "And this is good because if you have lots of data, this is not so expensive.",
            "By doing MKL we can even do better.",
            "So it's interesting that."
        ],
        [
            "We in a much faster runtime can achieve an accuracy which is not that far from what you get with dense kernels.",
            "OK, so just a small note about comparing this algorithm with existing wrapper based methods."
        ],
        [
            "We compared with the Gauss cycle cycle method that alternates between training SVM and obtaining the kernel coefficients and also simple MKL which is a gradient based method."
        ],
        [
            "And this is the results that we obtain.",
            "So the wrapper based methods managed to top 10 good convergence, but it can take a lot of time until they get to the to the close to the optimal solution.",
            "On the other hands with MCL.",
            "With this algorithm for MCL, we managed to."
        ],
        [
            "So to get close to the optimum in only 20 or 30 passes over the data, so this is competitive with what you can get with the standard SVM or CRF."
        ],
        [
            "OK, so we also tried."
        ],
        [
            "Independency parsing I'm not coming into details about these models here.",
            "But the idea is that you have a sentence like this and the goal is to predict these dependencies structure.",
            "So predicting went to predict these links here and these links are constrained to overall define."
        ],
        [
            "A tree.",
            "We find 507 feature templates.",
            "So these are just linear kernels.",
            "And in this example we did not."
        ],
        [
            "Outperform SVM, but we did a good job at identifying which feature templates are relevant for this task.",
            "So in this example."
        ],
        [
            "We are using a two stage architecture where we first use MKL to identify relevant feature templates.",
            "Then we select only the ones that are found relevant and then you run and."
        ],
        [
            "SVM on those, and by doing that you can.",
            "Actually you can get a similar accuracy, not much worse some."
        ],
        [
            "Is better.",
            "With a much compact model.",
            "So."
        ],
        [
            "To conclude, we have proposed a method for kernel learning and feature template selection that works in structured prediction.",
            "So this is a new class of online proximal algorithms that is very flexible.",
            "It has convergence and generalization bounds.",
            "There is some related work that we'd like to mention, so this is a very interesting recent work by G and colleagues, which is also an online method for MCL.",
            "It differs from from our approach in the sense that they essentially it boils down to performing dual coordinate descent, so block coordinates in the tool.",
            "It seems to be slightly less flexible in our algorithm because it's limited to Lt regularization, so it does not allow for some of regularizers and Q's to be larger than one, and there's a lot of related work in structured sparsity.",
            "I'm just mentioning quite a few works here.",
            "OK, that's all that I."
        ],
        [
            "Thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we are interested in computing account iconic combination of kernels in the name Cal setting.",
                    "label": 0
                },
                {
                    "sent": "So the idea is to.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have several kernels K1 to KM and you want to come up with a new kernel that is a combination of the existing kernels and you want to embed these into the learning problem.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The first approach is for these used second order cone programming and similar techniques, but unfortunately they could not scale very well and so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So more recently there have been some wrapper based methods that scale better.",
                    "label": 0
                },
                {
                    "sent": "But so these methods require having to repeatedly solving an SVM.",
                    "label": 1
                },
                {
                    "sent": "So they're quite looping over this, and this is sometimes a disadvantage because in some in some settings in, for example, if you have lots of data or instruction prediction.",
                    "label": 0
                },
                {
                    "sent": "You don't actually manage to guarantee very good accuracy in solving each SVM.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But we are going to propose is an online algorithm that alternates between stochastic gradient steps and possible steps.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this algorithm is very flexible.",
                    "label": 0
                },
                {
                    "sent": "It works for all kinds of convex losses.",
                    "label": 0
                },
                {
                    "sent": "We are going specifically to tackle structure prediction.",
                    "label": 0
                },
                {
                    "sent": "With this it's compatible with the many different chemical formulations.",
                    "label": 0
                },
                {
                    "sent": "Parson on sparse.",
                    "label": 0
                },
                {
                    "sent": "It's also good for Lasuen variants of group lawsuits and so forth.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're going to show some convergence and realization bounds.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some experiments in actual structure prediction problems.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's start.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "By describing our structured prediction setting, we have an input set X.",
                    "label": 1
                },
                {
                    "sent": "For example, sequences and then output set Y.",
                    "label": 0
                },
                {
                    "sent": "For example, labelings for those sequences.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The assumption here is that why is a very large and structured set.",
                    "label": 0
                },
                {
                    "sent": "So for example for sequence labeling.",
                    "label": 0
                },
                {
                    "sent": "So each element in this output set is going to be a possible labeling of the sequence, and you have exponentially many of those.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we're going to assume a supervised learning scenario.",
                    "label": 0
                },
                {
                    "sent": "We have a data set of IID points and our goal.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All is to learn the compatibility function.",
                    "label": 0
                },
                {
                    "sent": "F that Maps pairs of elements in X&Y to some real value and is used to make a new prediction.",
                    "label": 0
                },
                {
                    "sent": "So given a new point X, we're going to predict yhat as the output that maximizes this compatibility function.",
                    "label": 0
                },
                {
                    "sent": "So if you have some Hilbert space.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's H andsome kernel K we are.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Need to consider linear classifiers that can be written as the inner product that you were space of, a parameter vector title.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And feature a joint feature map feature Vector Phi of X&Y&Y.",
                    "label": 0
                },
                {
                    "sent": "If you have multiple kernels.",
                    "label": 0
                },
                {
                    "sent": "We actually discussed sponse to a direct sum of Hubert spaces of smaller Hubert Spaces, H1 to HM1 for each kernel and the computed function can be written as a sum of smaller inner product.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let me start by stating the main assumption that is usually made in structured prediction in order to make the code decoding problem tractable, you need to come up with.",
                    "label": 0
                },
                {
                    "sent": "Some sort of assumption, and this is usually the assumption that is taken that you decompose.",
                    "label": 0
                },
                {
                    "sent": "They can decompose the output in two parts.",
                    "label": 0
                },
                {
                    "sent": "So essentially we designed our features to be Sam's over over parts for some definition of parts.",
                    "label": 0
                },
                {
                    "sent": "If you have a Markov network, these parts can correspond to assignments in vertices and edges.",
                    "label": 0
                },
                {
                    "sent": "Of course you can go to clicks in general, and So what we have is in this example I'm representing the joint feature map.",
                    "label": 0
                },
                {
                    "sent": "As something defined on the vertices and some features defined on the edges, where each of them.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our region as well as connector products of features that only depends on the point sites and in the output side at some position in the network.",
                    "label": 0
                },
                {
                    "sent": "So if you if we translate this to kernels, this is what we obtain essentially kernel between two pairs of points XY and X prime.",
                    "label": 0
                },
                {
                    "sent": "Y prime can be written as a sum for all nodes in each in each of the graphs and some over all edges in both graphs.",
                    "label": 0
                },
                {
                    "sent": "Of multiplication of kernels.",
                    "label": 0
                },
                {
                    "sent": "So the case depends on the inputs and the L depends on the output.",
                    "label": 0
                },
                {
                    "sent": "So there are several iPods to learn the kernel here.",
                    "label": 0
                },
                {
                    "sent": "We cannot learn either of these.",
                    "label": 0
                },
                {
                    "sent": "Component kernels here we're going to take the simplest assumption, which is we're going to fix this to be just a connector Delta.",
                    "label": 0
                },
                {
                    "sent": "So this is only one if the two labeling the two labels are the same.",
                    "label": 0
                },
                {
                    "sent": "We do something similar here, but we allow for a constant beta zero that we are going to learn.",
                    "label": 0
                },
                {
                    "sent": "We fix.",
                    "label": 0
                },
                {
                    "sent": "This cannot be cause and so it can be taken out and we focus on learning this part.",
                    "label": 0
                },
                {
                    "sent": "So this is the kernel that you are going to learn here.",
                    "label": 0
                },
                {
                    "sent": "So again, we see that you have M kernels KV-12 KVM and you want to learn these coefficients.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's talk about.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learning, we suppose that you have some cost function that qualifies the cost of predicting Y hats when the true output is why?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We define some convex loss function, for example, the structural image loss.",
                    "label": 0
                },
                {
                    "sent": "So this is just a generalization of the binary hinge loss.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We define a regularizer.",
                    "label": 0
                },
                {
                    "sent": "And sweet.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And by minimizing the regularise empirical risk.",
                    "label": 0
                },
                {
                    "sent": "So if you have to.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is a shun.",
                    "label": 0
                },
                {
                    "sent": "This is very standard.",
                    "label": 0
                },
                {
                    "sent": "We just recovered the standard SVM setting where we are using a sum of kernels.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And talking about algorithms to solve these problems.",
                    "label": 0
                },
                {
                    "sent": "Some of the fastest to dates are online algorithms.",
                    "label": 0
                },
                {
                    "sent": "That's you know have bigger one to one over epsilon.",
                    "label": 0
                },
                {
                    "sent": "They require.",
                    "label": 0
                },
                {
                    "sent": "Uh oh, bigger one over epsilon iterations like Pegasus and do a good job at training SVM in a logical scenario.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in general, we're going to think about LPQ norms.",
                    "label": 0
                },
                {
                    "sent": "So this is defined, so we assume that there is some block structure in the parameters and we define.",
                    "label": 0
                },
                {
                    "sent": "This design is defined as taking the P norm that it's block and then taking the Q norm of the vector of norms.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In sports MCL, the underlying norm that is being used as regularizer is the L2 one norm.",
                    "label": 0
                },
                {
                    "sent": "Actually, the square of death.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in sports in KL just reminds we are constraining these coefficients to be non negative and to sum to one.",
                    "label": 0
                },
                {
                    "sent": "And in non sparse MCL.",
                    "label": 0
                },
                {
                    "sent": "On another project that have been recently proposed, we constraint instead the P norm of beta to be one or at less than one.",
                    "label": 0
                },
                {
                    "sent": "This doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "And this corresponds to using an L and L2 Q norm instead of an L2 one regularizer regularizer.",
                    "label": 0
                },
                {
                    "sent": "For quick, you relax with this way.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in general you are interested and we're going to show an algorithm that can angle can handle more general regularizers that can be written as a sum of smaller regularizers.",
                    "label": 0
                },
                {
                    "sent": "So this can handle any case where the regularization consists in Assam for the equal 22 J off some weight.",
                    "label": 0
                },
                {
                    "sent": "Some wait times, regularizers attacks.",
                    "label": 0
                },
                {
                    "sent": "Which of these forms?",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is the algorithm that you propose.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's an online algorithm, it's work.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In rounds at this round, it picks a training pair, XTYT.",
                    "label": 0
                },
                {
                    "sent": "It computes a subgradient, and to do that, this corresponds to do some sort of inference.",
                    "label": 0
                },
                {
                    "sent": "So if we.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We are working with SVM's.",
                    "label": 0
                },
                {
                    "sent": "This means that you need to do map inference if you're using an S SRF loss.",
                    "label": 0
                },
                {
                    "sent": "These correspond to marginal inference and then we take a subgradient step.",
                    "label": 1
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Aft.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that for each regularizer that we have, we compute approximal.",
                    "label": 0
                },
                {
                    "sent": "We do approximal step.",
                    "label": 0
                },
                {
                    "sent": "Which means that we have to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "This is quite efficient for the block normal advertisers that you have just seen, including the squared L1 norm.",
                    "label": 0
                },
                {
                    "sent": "Although even with the square there and so there, there are some explanation about that in the paper.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finally, there is an optional projection step.",
                    "label": 0
                },
                {
                    "sent": "Whenever you know abounds in the normal for optimal parameters, and often you can come up with those bounds.",
                    "label": 0
                },
                {
                    "sent": "That's essentially guarantees that we stay in the in the in the physical region.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this can speed up the algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, let's talk about convergence and generalization bounds.",
                    "label": 0
                },
                {
                    "sent": "We assume that your losses, Lipschitz with some constant G on the physical region, so this is Delta ball with radius radius, gamma and with.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Probability we get convergence in bigger one over epsilon squared rounds there are.",
                    "label": 0
                },
                {
                    "sent": "There are some constants here and you actually are going to quantify these constants for sparse MCL.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In addition, if the loss or the regularizers are strongly convex, this can be improved to over epsilon.",
                    "label": 0
                },
                {
                    "sent": "If we exclude arhythmic terms.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Digitalization.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are the same order and this is also.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "List using standard techniques in online learning.",
                    "label": 0
                },
                {
                    "sent": "Also some fundamental result.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the in proximity theory.",
                    "label": 0
                },
                {
                    "sent": "OK, so for the specific case of space of sparse MKL.",
                    "label": 0
                },
                {
                    "sent": "Using either the SVM or the CRF losses, and assuming that the kernels are bounded, we can actually quantify this Lipschitz constant, and these bounds gamma, and so you obtain something like this and you can see that.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It depends linearly in the number of kernels, which is not so good, but it's what this algorithm allows for.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's see some experiments.",
                    "label": 0
                },
                {
                    "sent": "The first one is in sequence labeling.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In an OCR data set, so this is we have a sequence of characters of images of characters and the goal is to predict what is the character using the correlation between consecutive characters.",
                    "label": 0
                },
                {
                    "sent": "So user bigram model for this.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And our first attempt is just combining linear, quadratic and the Gaussian kernel, so this is.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So each kernel performs individually.",
                    "label": 0
                },
                {
                    "sent": "You can see that the linear kernel is pretty bad.",
                    "label": 0
                },
                {
                    "sent": "We can do much better with the quadratic or Gaussian kernel, the average.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Doesn't help much.",
                    "label": 0
                },
                {
                    "sent": "It's doing worse than the quadratic kernel if you do.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, using this algorithm we actually come up with significant improvement.",
                    "label": 0
                },
                {
                    "sent": "The second exper.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Attempts to.",
                    "label": 0
                },
                {
                    "sent": "So if you have a really large scale problem with many examples, it's not the case of this data set.",
                    "label": 0
                },
                {
                    "sent": "It's pretty expensive to store the kernel matrix or to do kernel computations, so this is we need to do quadratic computations with respect to the the number of week to sample size and so.",
                    "label": 0
                },
                {
                    "sent": "So it's known that if you have explicit features, this problem doesn't exist.",
                    "label": 0
                },
                {
                    "sent": "In most of the cases.",
                    "label": 0
                },
                {
                    "sent": "So the idea here is to combine explicit features with sparse kernels.",
                    "label": 0
                },
                {
                    "sent": "So this is what motivates us to consider the combination of a linear kernels and B1 spline kernels, so here, so these are.",
                    "label": 0
                },
                {
                    "sent": "Similar to RBF kernels, but truncate is so this text list looks like a triangle and the idea is that we use a very small width for the triangle, which means that you are essentially selecting just a few neighbors of each point.",
                    "label": 0
                },
                {
                    "sent": "And so each of these kernel.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Individual performs quite bad, but.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You take the average we manage to get a very large improvement.",
                    "label": 0
                },
                {
                    "sent": "And this is good because if you have lots of data, this is not so expensive.",
                    "label": 0
                },
                {
                    "sent": "By doing MKL we can even do better.",
                    "label": 0
                },
                {
                    "sent": "So it's interesting that.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We in a much faster runtime can achieve an accuracy which is not that far from what you get with dense kernels.",
                    "label": 0
                },
                {
                    "sent": "OK, so just a small note about comparing this algorithm with existing wrapper based methods.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We compared with the Gauss cycle cycle method that alternates between training SVM and obtaining the kernel coefficients and also simple MKL which is a gradient based method.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is the results that we obtain.",
                    "label": 0
                },
                {
                    "sent": "So the wrapper based methods managed to top 10 good convergence, but it can take a lot of time until they get to the to the close to the optimal solution.",
                    "label": 1
                },
                {
                    "sent": "On the other hands with MCL.",
                    "label": 0
                },
                {
                    "sent": "With this algorithm for MCL, we managed to.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to get close to the optimum in only 20 or 30 passes over the data, so this is competitive with what you can get with the standard SVM or CRF.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we also tried.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Independency parsing I'm not coming into details about these models here.",
                    "label": 0
                },
                {
                    "sent": "But the idea is that you have a sentence like this and the goal is to predict these dependencies structure.",
                    "label": 0
                },
                {
                    "sent": "So predicting went to predict these links here and these links are constrained to overall define.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A tree.",
                    "label": 0
                },
                {
                    "sent": "We find 507 feature templates.",
                    "label": 0
                },
                {
                    "sent": "So these are just linear kernels.",
                    "label": 0
                },
                {
                    "sent": "And in this example we did not.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Outperform SVM, but we did a good job at identifying which feature templates are relevant for this task.",
                    "label": 0
                },
                {
                    "sent": "So in this example.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We are using a two stage architecture where we first use MKL to identify relevant feature templates.",
                    "label": 0
                },
                {
                    "sent": "Then we select only the ones that are found relevant and then you run and.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "SVM on those, and by doing that you can.",
                    "label": 0
                },
                {
                    "sent": "Actually you can get a similar accuracy, not much worse some.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is better.",
                    "label": 0
                },
                {
                    "sent": "With a much compact model.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To conclude, we have proposed a method for kernel learning and feature template selection that works in structured prediction.",
                    "label": 0
                },
                {
                    "sent": "So this is a new class of online proximal algorithms that is very flexible.",
                    "label": 0
                },
                {
                    "sent": "It has convergence and generalization bounds.",
                    "label": 0
                },
                {
                    "sent": "There is some related work that we'd like to mention, so this is a very interesting recent work by G and colleagues, which is also an online method for MCL.",
                    "label": 0
                },
                {
                    "sent": "It differs from from our approach in the sense that they essentially it boils down to performing dual coordinate descent, so block coordinates in the tool.",
                    "label": 0
                },
                {
                    "sent": "It seems to be slightly less flexible in our algorithm because it's limited to Lt regularization, so it does not allow for some of regularizers and Q's to be larger than one, and there's a lot of related work in structured sparsity.",
                    "label": 0
                },
                {
                    "sent": "I'm just mentioning quite a few works here.",
                    "label": 0
                },
                {
                    "sent": "OK, that's all that I.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}