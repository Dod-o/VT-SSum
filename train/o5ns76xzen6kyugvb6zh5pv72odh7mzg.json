{
    "id": "o5ns76xzen6kyugvb6zh5pv72odh7mzg",
    "title": "Estimation of Extreme Values and Associated Level Sets of a Regression Function via Selective Sampling",
    "info": {
        "author": [
            "Stanislav Minsker, Department of Mathematics, Duke University"
        ],
        "published": "Aug. 9, 2013",
        "recorded": "June 2013",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/colt2013_minsker_estimation/",
    "segmentation": [
        [
            "So I'll be talking about problem of global maximization based on.",
            "Based on noisy function values.",
            "How?"
        ],
        [
            "So let me first briefly outline the framework.",
            "Assume that we have.",
            "Random couple X&Y, where X leaves dimensional cube and Y is real valued and X&Y are connected.",
            "By the following model so.",
            "OK, so why is this noise observation of the function F taken at X and the noises?",
            "Some somewhere in the variable see so we can always write this case that affects the conditional expectation of Y given X and.",
            "The joint distribution of this couple will be denoted by P and the marginal distribution will be denoted by \u03c0 throughout the talk.",
            "So the goal of the problem that we want to solve is.",
            "Rather than vicious, I want first of all, we want to estimate.",
            "The global maximum of this function and check the node by MFF.",
            "And Secondly we want to estimate the level set which is associated with this maximum which is arguably even more important than estimating.",
            "The maximum itself is.",
            "This set of folex is.",
            "Where have affects is equal to MFF.",
            "And the way we're going to do this?",
            "Well, we're going to do this from.",
            "From a collection of data, so collection, effect size and why I switched somehow sampled.",
            "From distribution P. Yes, I want to know where my function achieves its maximum value, yes?",
            "Discretize in some way that you are only talking about this single maximum.",
            "No.",
            "So yes, maybe for this question I should say now that I do not want to assume that my function achieves its maximum single point and they don't want to assume any assumptions such as convexity for example so.",
            "The level set might be.",
            "Justin arbitrarily sets for sale.",
            "Normal level set of continuous function doesn't.",
            "The point does not have to be unique.",
            "With some slack, like all the full features are very close to maximum or just well, I will be talking about the estimation Hausdorff distance.",
            "So, so in this case we can avoid talking about these things, but I will present my results and then it will become more clear.",
            "OK, so since since I'm talking with active learning.",
            "Session I'm supposed to say something bad about persevering and belief, and So what is what is the way to collect to collect the data?",
            "So, of course, one way is just to collect the data in the ideal way.",
            "And then try to estimate the parameters of interest.",
            "But this is definitely not the most efficient way because lots of observationes are going to be useless and general belief in practice that.",
            "The cost of obtaining the data is actually concentrated in the wise, so if you want to evaluate the function then we might need to perform some experiments.",
            "And this in general.",
            "So in this case the number, the number of noise evaluations in the parameter."
        ],
        [
            "And so.",
            "Well, they said that they still learning.",
            "Maybe not the best idea and.",
            "In general, we want to select the points that are somehow most informative, which I don't define in the precise way.",
            "But general active algorithms try to relate the function at the most informative locations, so this is the main idea."
        ],
        [
            "Oh, what is the way to relate the function?",
            "The most informative locations?",
            "So one way is.",
            "Where to obtain the data?",
            "For example as follows.",
            "Assume that we sampled the observation sequentially and assume assume that support of the distribution is known or who might think.",
            "About the related multi.",
            "Well, turn bandits problem that you have this set of farms that is known in advance and you allowed at each step you're allowed to pull any army once.",
            "So another approach that is closer to what you do here is what I call the streaming mode.",
            "So in this case.",
            "I'm just assuming the sequence of observation data sampled from some modified distribution.",
            "Depends on the previous history.",
            "So the way to think about this is follows.",
            "That's why right accept reject here.",
            "So observe some sample and then I algorithm decides if we should accept it or if we should reject it.",
            "And this decision is based on some condition which depends on the previous observed data.",
            "And then after.",
            "After the point, XK is fixed with sample.",
            "We observe the noisy function value.",
            "From the conditional distribution, we assume that this value is independent.",
            "Given the given the locations.",
            "So there's been a lot of work on this problem during the last 60 years, and probably before, and this work has been done in different communities from different viewpoints, both in statistical community and seeing the benefits community.",
            "Township.",
            "What do you mean?",
            "In New York.",
            "I'm not sure I understand this last condition in the time is even all full access your.",
            "Well, my so.",
            "If if I fix the location then my values or say the noise is independent, so the noise variables independent.",
            "So if I fix my locations where a sample the function then it gets against independent random variables conditionally, conditionally, independent on the fixed locations, yes?",
            "So I mean there is nothing behind it.",
            "But the ones that you selected and 50 samples we wanted.",
            "No.",
            "Yes, extended.",
            "Condition.",
            "So yeah, there is nothing.",
            "Nothing special about this assumption."
        ],
        [
            "So this is just a small sample of work that has been done during the last 60 years.",
            "Those people who were involved in this work are here in the audience, so maybe I'll just try to give some important important.",
            "Summary of the work."
        ],
        [
            "This is so it was.",
            "Causes of essential.",
            "This too, that's important parameters for this problem are the regularity of the function that.",
            "In this work I just expressed in the simple way to solve the usual color condition and the second important assumption, the margin condition, which is also called the Super call flow noise assumption classification is richer and this just describes essentially describe the size of the neighborhoods of the maximum level set.",
            "So just to give."
        ],
        [
            "You have a better idea of how it looks like, so these are graphs.",
            "This line corresponds to the case where gamma is equal to 1.",
            "This corresponds to make call to four.",
            "This corresponds to gamma equal to 1/4 and intuitively so into this case should be called the simplest to identify.",
            "Because because in this case, so it is clear that the maximum is obtained at this point.",
            "While in this case it is not clear if the maximum that here, here, here.",
            "And the result.",
            "So under these two assumptions.",
            "There is a lower bound that that was proved in the."
        ],
        [
            "Ben is framework.",
            "Essentially it says that.",
            "We cannot estimate.",
            "The maximum of the function, the rate that would be faster as the number of noisy values to the power minus beta divided by beta plus one minus the product of between gamma.",
            "So this was done in the univariate case when the function is just, say the function 01.",
            "So.",
            "This result will be complemented by."
        ],
        [
            "Yeah, this is all will be complemented by the results on estimating the level sets as well, so I will present it in a couple of minutes and the main motivation for this work.",
            "Was to develop the algorithms that allow to achieve our goal that have minimal assumptions.",
            "On the structure of the function.",
            "So they said we do not want to assume the uniqueness of the maximum and we do not want to assume.",
            "We convexity for example and at the same time we want the algorithms to be adaptive with respect.",
            "To the structure of the problem, which can be the smoothness of the margin assumption.",
            "Or maybe something else?",
            "We do not want to know this parameters in advance.",
            "Oh another interesting direction.",
            "In the case when we are doing global maximization with some conditional global optimization with some implicit site conditions.",
            "For example, you might not know the support of the distribution, and one way to think about it is when distribution of X is supported from some low dimensional submanifold and we want to find the maximum of the function of this manifolds.",
            "So this question will be addressed in the medical section if I have time in the end."
        ],
        [
            "So let me present the first main result, which is the lower bound for estimating the level set of the maximum.",
            "The house or distance.",
            "So the theorem says that.",
            "For any estimator.",
            "Which is based on noisy function relations.",
            "Oh the rate at which maximum level set can be estimated is bounded.",
            "Global is bounded below by this quantity, so this thread.",
            "Parameter beta times gamma.",
            "So the product of smoothness and the margin constants is the main main quantity of the difference.",
            "The active framework from the passive framework and this product can be as large as D, so bitter type gamma is equal to D. Then we get the dimension trait.",
            "But once again this one is lower bound.",
            "So of course we have to support it with some upper bounds as well.",
            "It's only bought from below."
        ],
        [
            "OK, so now let me describe the algorithm that she allows to obtain or allow us to.",
            "The upper bounds to complement the previous results, so this algorithm is based on the confidence bands for the function and it works like this.",
            "So under the assumption that they specified."
        ],
        [
            "What to do first?",
            "So first we use some fraction of available label budgets and once again I assume that I fix the maximum number of noise evaluation that I want to make.",
            "So in the first step I construct.",
            "In this case, the piecewise constant estimator of my function, and I assume that this estimator has some theoretical properties of how well it approximates the true function in subnormal."
        ],
        [
            "So.",
            "So here it is assumed that I know this equality holds true and they assume that MK hat is.",
            "That's with maximum of of my estimator, which is of course easy to evaluate since it is just piecewise constant function."
        ],
        [
            "So what we do next.",
            "Next we reduce the search domain for the maximum and it is easy to do because if we just look carefully this picture so this so it is easy to see that.",
            "At the point where the estimator differs from its maximum value by more than two Delta K. With high probability, the supremum cannot be achieved, so we can just throw away these parts, and for this particular picture the only place where we should look for the global maximum.",
            "This red set.",
            "So on the next step will only requests the points which are concentrated in this thread subset."
        ],
        [
            "And we just iterate.",
            "This process will achieve the label label limit.",
            "And well, in the end we return the last.",
            "The last maximum, the maximum of the last estimator and the last active set.",
            "She will be our finalist."
        ],
        [
            "Measures so.",
            "Yes.",
            "It happens that your case Wi-Fi setup installation.",
            "Close the underestimate the maximum you did well under the assumption that I state it cannot happen, so I will outline these assumptions in the next slide.",
            "Size of the temple.",
            "Yes so.",
            "Yes, I will try to explain this.",
            "So the main assumptions, as I've already said, is horrible Arity.",
            "The margin condition.",
            "Then we make assumptions, annoys, chose can be relaxed in numerous ways, and for this result I assume that the support of my distribution, the whole unit cube.",
            "And the distribution has a density which is bounded above and below.",
            "So essentially this equivalent to uniform sampling."
        ],
        [
            "So the key assumption they put in separate slide in the following, so we assume.",
            "Essentially, the counterpart of the color condition.",
            "So if you take the best approximate if you take the best approximation of our function by, say by the higher basis."
        ],
        [
            "They want.",
            "This included the whole true, so we want different with bounded from below, and this assumption appeared recently in the statistical literature.",
            "For example, in the work feature Nick on the version of confidence bands against estimation, and essentially there is a lot of.",
            "A lot of work that's described.",
            "What type of functions satisfy this assumption?",
            "How restrictive it is.",
            "But I'm not going to spend time on this now.",
            "The intuition behind these assumptions is that function.",
            "For the function that satisfies this assumption, the true smoothness can be learned from the data."
        ],
        [
            "So whenever these conditions true, we have the following theorem for the following result for the described algorithm.",
            "So what we have if you want to estimate.",
            "Of absolute maximum up to precision.",
            "Epsilon and if you want.",
            "To estimate the level sets.",
            "In this way, so this condition just says that the estimate of the level set.",
            "It contains the true level set and it is contained in.",
            "Essentially some neighborhood over level set then.",
            "The number of noisy function measurements should satisfy this inequality.",
            "So once again in the red I highlighted.",
            "This difference, D minus.",
            "You might be the times gamma, so when beta times gamma is close to D then this rate becomes almost dimension free.",
            "Oh yeah, I think this is something that should be should be F yeah it should have.",
            "Yes, thank you.",
            "Yes, well.",
            "To be effective here I just.",
            "Oh so the best case.",
            "This race will be dimension free and."
        ],
        [
            "Just few comments.",
            "As already said, the key assumption is satisfied for many many functions.",
            "On the other hand, this algorithm is zero order, so it doesn't use any information about the derivatives, so it might not.",
            "Might not have good rates for.",
            "Smooth functions.",
            "So it's all set in the beginning.",
            "We assume that the support in the distribution is known and at least in practice we want to avoid this assumption.",
            "And this last part of my talk I will show what can be done in practice.",
            "So just use similar ideas to develop a slightly different algorithm.",
            "And to compute, so we compute something."
        ],
        [
            "OK, so this is the model.",
            "I consider two scenarios.",
            "In the first case.",
            "Oh apply the uniform distribution of the cube and the function.",
            "Adjust the distance to the origin and in the second case by uniform distribution on the unit sphere, 3 dimensional space and.",
            "The function is the distance of the pole.",
            "So once again, especially in the second case, support.",
            "So the distribution is not known to the algorithm in advance.",
            "And so here is how.",
            "Well, here's how."
        ],
        [
            "It works so in the first scenario we observe something like this."
        ],
        [
            "So each picture corresponds to a separate iteration and.",
            "The points of different colors correspond to the points that are obtained by the algorithm in different iterations.",
            "Well, that's what it does.",
            "So we see that the points come closer and closer and closer to the origin.",
            "And here is just the output.",
            "So the maximum is obtained at the origin, so we're pretty close to the origin.",
            "The estimated value is also pretty close to zero, and they just show the total number of functional relations and the total number of points that are processed during the algorithm.",
            "K for."
        ],
        [
            "Second scenario.",
            "It is I showed the similar picture."
        ],
        [
            "So this is my union sphere.",
            "And this is how the algorithm works.",
            "So this is the first iteration, the second generation, third iteration, 4th, 5th, etc etc."
        ],
        [
            "So she once again the points tend to tend to the pole and just provide the same same numbers.",
            "So once again, the true maximum is equal to 0 and we have pretty close to it.",
            "The number of function violations in this case is larger just because we use the larger value for the standard deviation of noise.",
            "And the total number of processed samples is a lot larger than the number of functional Asians.",
            "So."
        ],
        [
            "Thank you for attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'll be talking about problem of global maximization based on.",
                    "label": 0
                },
                {
                    "sent": "Based on noisy function values.",
                    "label": 0
                },
                {
                    "sent": "How?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me first briefly outline the framework.",
                    "label": 0
                },
                {
                    "sent": "Assume that we have.",
                    "label": 0
                },
                {
                    "sent": "Random couple X&Y, where X leaves dimensional cube and Y is real valued and X&Y are connected.",
                    "label": 0
                },
                {
                    "sent": "By the following model so.",
                    "label": 0
                },
                {
                    "sent": "OK, so why is this noise observation of the function F taken at X and the noises?",
                    "label": 0
                },
                {
                    "sent": "Some somewhere in the variable see so we can always write this case that affects the conditional expectation of Y given X and.",
                    "label": 0
                },
                {
                    "sent": "The joint distribution of this couple will be denoted by P and the marginal distribution will be denoted by \u03c0 throughout the talk.",
                    "label": 1
                },
                {
                    "sent": "So the goal of the problem that we want to solve is.",
                    "label": 0
                },
                {
                    "sent": "Rather than vicious, I want first of all, we want to estimate.",
                    "label": 0
                },
                {
                    "sent": "The global maximum of this function and check the node by MFF.",
                    "label": 0
                },
                {
                    "sent": "And Secondly we want to estimate the level set which is associated with this maximum which is arguably even more important than estimating.",
                    "label": 0
                },
                {
                    "sent": "The maximum itself is.",
                    "label": 0
                },
                {
                    "sent": "This set of folex is.",
                    "label": 0
                },
                {
                    "sent": "Where have affects is equal to MFF.",
                    "label": 0
                },
                {
                    "sent": "And the way we're going to do this?",
                    "label": 0
                },
                {
                    "sent": "Well, we're going to do this from.",
                    "label": 0
                },
                {
                    "sent": "From a collection of data, so collection, effect size and why I switched somehow sampled.",
                    "label": 0
                },
                {
                    "sent": "From distribution P. Yes, I want to know where my function achieves its maximum value, yes?",
                    "label": 0
                },
                {
                    "sent": "Discretize in some way that you are only talking about this single maximum.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "So yes, maybe for this question I should say now that I do not want to assume that my function achieves its maximum single point and they don't want to assume any assumptions such as convexity for example so.",
                    "label": 0
                },
                {
                    "sent": "The level set might be.",
                    "label": 0
                },
                {
                    "sent": "Justin arbitrarily sets for sale.",
                    "label": 0
                },
                {
                    "sent": "Normal level set of continuous function doesn't.",
                    "label": 0
                },
                {
                    "sent": "The point does not have to be unique.",
                    "label": 0
                },
                {
                    "sent": "With some slack, like all the full features are very close to maximum or just well, I will be talking about the estimation Hausdorff distance.",
                    "label": 0
                },
                {
                    "sent": "So, so in this case we can avoid talking about these things, but I will present my results and then it will become more clear.",
                    "label": 0
                },
                {
                    "sent": "OK, so since since I'm talking with active learning.",
                    "label": 0
                },
                {
                    "sent": "Session I'm supposed to say something bad about persevering and belief, and So what is what is the way to collect to collect the data?",
                    "label": 0
                },
                {
                    "sent": "So, of course, one way is just to collect the data in the ideal way.",
                    "label": 0
                },
                {
                    "sent": "And then try to estimate the parameters of interest.",
                    "label": 0
                },
                {
                    "sent": "But this is definitely not the most efficient way because lots of observationes are going to be useless and general belief in practice that.",
                    "label": 0
                },
                {
                    "sent": "The cost of obtaining the data is actually concentrated in the wise, so if you want to evaluate the function then we might need to perform some experiments.",
                    "label": 0
                },
                {
                    "sent": "And this in general.",
                    "label": 0
                },
                {
                    "sent": "So in this case the number, the number of noise evaluations in the parameter.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "Well, they said that they still learning.",
                    "label": 0
                },
                {
                    "sent": "Maybe not the best idea and.",
                    "label": 0
                },
                {
                    "sent": "In general, we want to select the points that are somehow most informative, which I don't define in the precise way.",
                    "label": 1
                },
                {
                    "sent": "But general active algorithms try to relate the function at the most informative locations, so this is the main idea.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Oh, what is the way to relate the function?",
                    "label": 0
                },
                {
                    "sent": "The most informative locations?",
                    "label": 0
                },
                {
                    "sent": "So one way is.",
                    "label": 0
                },
                {
                    "sent": "Where to obtain the data?",
                    "label": 0
                },
                {
                    "sent": "For example as follows.",
                    "label": 0
                },
                {
                    "sent": "Assume that we sampled the observation sequentially and assume assume that support of the distribution is known or who might think.",
                    "label": 0
                },
                {
                    "sent": "About the related multi.",
                    "label": 0
                },
                {
                    "sent": "Well, turn bandits problem that you have this set of farms that is known in advance and you allowed at each step you're allowed to pull any army once.",
                    "label": 0
                },
                {
                    "sent": "So another approach that is closer to what you do here is what I call the streaming mode.",
                    "label": 0
                },
                {
                    "sent": "So in this case.",
                    "label": 0
                },
                {
                    "sent": "I'm just assuming the sequence of observation data sampled from some modified distribution.",
                    "label": 1
                },
                {
                    "sent": "Depends on the previous history.",
                    "label": 0
                },
                {
                    "sent": "So the way to think about this is follows.",
                    "label": 0
                },
                {
                    "sent": "That's why right accept reject here.",
                    "label": 0
                },
                {
                    "sent": "So observe some sample and then I algorithm decides if we should accept it or if we should reject it.",
                    "label": 0
                },
                {
                    "sent": "And this decision is based on some condition which depends on the previous observed data.",
                    "label": 0
                },
                {
                    "sent": "And then after.",
                    "label": 0
                },
                {
                    "sent": "After the point, XK is fixed with sample.",
                    "label": 0
                },
                {
                    "sent": "We observe the noisy function value.",
                    "label": 0
                },
                {
                    "sent": "From the conditional distribution, we assume that this value is independent.",
                    "label": 1
                },
                {
                    "sent": "Given the given the locations.",
                    "label": 0
                },
                {
                    "sent": "So there's been a lot of work on this problem during the last 60 years, and probably before, and this work has been done in different communities from different viewpoints, both in statistical community and seeing the benefits community.",
                    "label": 0
                },
                {
                    "sent": "Township.",
                    "label": 0
                },
                {
                    "sent": "What do you mean?",
                    "label": 0
                },
                {
                    "sent": "In New York.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure I understand this last condition in the time is even all full access your.",
                    "label": 0
                },
                {
                    "sent": "Well, my so.",
                    "label": 0
                },
                {
                    "sent": "If if I fix the location then my values or say the noise is independent, so the noise variables independent.",
                    "label": 0
                },
                {
                    "sent": "So if I fix my locations where a sample the function then it gets against independent random variables conditionally, conditionally, independent on the fixed locations, yes?",
                    "label": 0
                },
                {
                    "sent": "So I mean there is nothing behind it.",
                    "label": 0
                },
                {
                    "sent": "But the ones that you selected and 50 samples we wanted.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Yes, extended.",
                    "label": 0
                },
                {
                    "sent": "Condition.",
                    "label": 0
                },
                {
                    "sent": "So yeah, there is nothing.",
                    "label": 0
                },
                {
                    "sent": "Nothing special about this assumption.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is just a small sample of work that has been done during the last 60 years.",
                    "label": 0
                },
                {
                    "sent": "Those people who were involved in this work are here in the audience, so maybe I'll just try to give some important important.",
                    "label": 0
                },
                {
                    "sent": "Summary of the work.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is so it was.",
                    "label": 0
                },
                {
                    "sent": "Causes of essential.",
                    "label": 0
                },
                {
                    "sent": "This too, that's important parameters for this problem are the regularity of the function that.",
                    "label": 1
                },
                {
                    "sent": "In this work I just expressed in the simple way to solve the usual color condition and the second important assumption, the margin condition, which is also called the Super call flow noise assumption classification is richer and this just describes essentially describe the size of the neighborhoods of the maximum level set.",
                    "label": 0
                },
                {
                    "sent": "So just to give.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You have a better idea of how it looks like, so these are graphs.",
                    "label": 0
                },
                {
                    "sent": "This line corresponds to the case where gamma is equal to 1.",
                    "label": 0
                },
                {
                    "sent": "This corresponds to make call to four.",
                    "label": 0
                },
                {
                    "sent": "This corresponds to gamma equal to 1/4 and intuitively so into this case should be called the simplest to identify.",
                    "label": 0
                },
                {
                    "sent": "Because because in this case, so it is clear that the maximum is obtained at this point.",
                    "label": 0
                },
                {
                    "sent": "While in this case it is not clear if the maximum that here, here, here.",
                    "label": 0
                },
                {
                    "sent": "And the result.",
                    "label": 0
                },
                {
                    "sent": "So under these two assumptions.",
                    "label": 0
                },
                {
                    "sent": "There is a lower bound that that was proved in the.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ben is framework.",
                    "label": 0
                },
                {
                    "sent": "Essentially it says that.",
                    "label": 0
                },
                {
                    "sent": "We cannot estimate.",
                    "label": 0
                },
                {
                    "sent": "The maximum of the function, the rate that would be faster as the number of noisy values to the power minus beta divided by beta plus one minus the product of between gamma.",
                    "label": 0
                },
                {
                    "sent": "So this was done in the univariate case when the function is just, say the function 01.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This result will be complemented by.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, this is all will be complemented by the results on estimating the level sets as well, so I will present it in a couple of minutes and the main motivation for this work.",
                    "label": 0
                },
                {
                    "sent": "Was to develop the algorithms that allow to achieve our goal that have minimal assumptions.",
                    "label": 1
                },
                {
                    "sent": "On the structure of the function.",
                    "label": 0
                },
                {
                    "sent": "So they said we do not want to assume the uniqueness of the maximum and we do not want to assume.",
                    "label": 0
                },
                {
                    "sent": "We convexity for example and at the same time we want the algorithms to be adaptive with respect.",
                    "label": 1
                },
                {
                    "sent": "To the structure of the problem, which can be the smoothness of the margin assumption.",
                    "label": 0
                },
                {
                    "sent": "Or maybe something else?",
                    "label": 0
                },
                {
                    "sent": "We do not want to know this parameters in advance.",
                    "label": 0
                },
                {
                    "sent": "Oh another interesting direction.",
                    "label": 0
                },
                {
                    "sent": "In the case when we are doing global maximization with some conditional global optimization with some implicit site conditions.",
                    "label": 0
                },
                {
                    "sent": "For example, you might not know the support of the distribution, and one way to think about it is when distribution of X is supported from some low dimensional submanifold and we want to find the maximum of the function of this manifolds.",
                    "label": 0
                },
                {
                    "sent": "So this question will be addressed in the medical section if I have time in the end.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me present the first main result, which is the lower bound for estimating the level set of the maximum.",
                    "label": 0
                },
                {
                    "sent": "The house or distance.",
                    "label": 0
                },
                {
                    "sent": "So the theorem says that.",
                    "label": 0
                },
                {
                    "sent": "For any estimator.",
                    "label": 0
                },
                {
                    "sent": "Which is based on noisy function relations.",
                    "label": 1
                },
                {
                    "sent": "Oh the rate at which maximum level set can be estimated is bounded.",
                    "label": 0
                },
                {
                    "sent": "Global is bounded below by this quantity, so this thread.",
                    "label": 0
                },
                {
                    "sent": "Parameter beta times gamma.",
                    "label": 0
                },
                {
                    "sent": "So the product of smoothness and the margin constants is the main main quantity of the difference.",
                    "label": 0
                },
                {
                    "sent": "The active framework from the passive framework and this product can be as large as D, so bitter type gamma is equal to D. Then we get the dimension trait.",
                    "label": 1
                },
                {
                    "sent": "But once again this one is lower bound.",
                    "label": 0
                },
                {
                    "sent": "So of course we have to support it with some upper bounds as well.",
                    "label": 0
                },
                {
                    "sent": "It's only bought from below.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now let me describe the algorithm that she allows to obtain or allow us to.",
                    "label": 0
                },
                {
                    "sent": "The upper bounds to complement the previous results, so this algorithm is based on the confidence bands for the function and it works like this.",
                    "label": 0
                },
                {
                    "sent": "So under the assumption that they specified.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What to do first?",
                    "label": 0
                },
                {
                    "sent": "So first we use some fraction of available label budgets and once again I assume that I fix the maximum number of noise evaluation that I want to make.",
                    "label": 0
                },
                {
                    "sent": "So in the first step I construct.",
                    "label": 0
                },
                {
                    "sent": "In this case, the piecewise constant estimator of my function, and I assume that this estimator has some theoretical properties of how well it approximates the true function in subnormal.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So here it is assumed that I know this equality holds true and they assume that MK hat is.",
                    "label": 0
                },
                {
                    "sent": "That's with maximum of of my estimator, which is of course easy to evaluate since it is just piecewise constant function.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we do next.",
                    "label": 0
                },
                {
                    "sent": "Next we reduce the search domain for the maximum and it is easy to do because if we just look carefully this picture so this so it is easy to see that.",
                    "label": 0
                },
                {
                    "sent": "At the point where the estimator differs from its maximum value by more than two Delta K. With high probability, the supremum cannot be achieved, so we can just throw away these parts, and for this particular picture the only place where we should look for the global maximum.",
                    "label": 1
                },
                {
                    "sent": "This red set.",
                    "label": 0
                },
                {
                    "sent": "So on the next step will only requests the points which are concentrated in this thread subset.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we just iterate.",
                    "label": 0
                },
                {
                    "sent": "This process will achieve the label label limit.",
                    "label": 0
                },
                {
                    "sent": "And well, in the end we return the last.",
                    "label": 0
                },
                {
                    "sent": "The last maximum, the maximum of the last estimator and the last active set.",
                    "label": 0
                },
                {
                    "sent": "She will be our finalist.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Measures so.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "It happens that your case Wi-Fi setup installation.",
                    "label": 0
                },
                {
                    "sent": "Close the underestimate the maximum you did well under the assumption that I state it cannot happen, so I will outline these assumptions in the next slide.",
                    "label": 0
                },
                {
                    "sent": "Size of the temple.",
                    "label": 0
                },
                {
                    "sent": "Yes so.",
                    "label": 0
                },
                {
                    "sent": "Yes, I will try to explain this.",
                    "label": 0
                },
                {
                    "sent": "So the main assumptions, as I've already said, is horrible Arity.",
                    "label": 0
                },
                {
                    "sent": "The margin condition.",
                    "label": 0
                },
                {
                    "sent": "Then we make assumptions, annoys, chose can be relaxed in numerous ways, and for this result I assume that the support of my distribution, the whole unit cube.",
                    "label": 0
                },
                {
                    "sent": "And the distribution has a density which is bounded above and below.",
                    "label": 0
                },
                {
                    "sent": "So essentially this equivalent to uniform sampling.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the key assumption they put in separate slide in the following, so we assume.",
                    "label": 0
                },
                {
                    "sent": "Essentially, the counterpart of the color condition.",
                    "label": 0
                },
                {
                    "sent": "So if you take the best approximate if you take the best approximation of our function by, say by the higher basis.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "They want.",
                    "label": 0
                },
                {
                    "sent": "This included the whole true, so we want different with bounded from below, and this assumption appeared recently in the statistical literature.",
                    "label": 0
                },
                {
                    "sent": "For example, in the work feature Nick on the version of confidence bands against estimation, and essentially there is a lot of.",
                    "label": 0
                },
                {
                    "sent": "A lot of work that's described.",
                    "label": 0
                },
                {
                    "sent": "What type of functions satisfy this assumption?",
                    "label": 0
                },
                {
                    "sent": "How restrictive it is.",
                    "label": 0
                },
                {
                    "sent": "But I'm not going to spend time on this now.",
                    "label": 0
                },
                {
                    "sent": "The intuition behind these assumptions is that function.",
                    "label": 0
                },
                {
                    "sent": "For the function that satisfies this assumption, the true smoothness can be learned from the data.",
                    "label": 1
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So whenever these conditions true, we have the following theorem for the following result for the described algorithm.",
                    "label": 0
                },
                {
                    "sent": "So what we have if you want to estimate.",
                    "label": 0
                },
                {
                    "sent": "Of absolute maximum up to precision.",
                    "label": 0
                },
                {
                    "sent": "Epsilon and if you want.",
                    "label": 0
                },
                {
                    "sent": "To estimate the level sets.",
                    "label": 0
                },
                {
                    "sent": "In this way, so this condition just says that the estimate of the level set.",
                    "label": 0
                },
                {
                    "sent": "It contains the true level set and it is contained in.",
                    "label": 0
                },
                {
                    "sent": "Essentially some neighborhood over level set then.",
                    "label": 0
                },
                {
                    "sent": "The number of noisy function measurements should satisfy this inequality.",
                    "label": 1
                },
                {
                    "sent": "So once again in the red I highlighted.",
                    "label": 0
                },
                {
                    "sent": "This difference, D minus.",
                    "label": 1
                },
                {
                    "sent": "You might be the times gamma, so when beta times gamma is close to D then this rate becomes almost dimension free.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, I think this is something that should be should be F yeah it should have.",
                    "label": 0
                },
                {
                    "sent": "Yes, thank you.",
                    "label": 0
                },
                {
                    "sent": "Yes, well.",
                    "label": 0
                },
                {
                    "sent": "To be effective here I just.",
                    "label": 0
                },
                {
                    "sent": "Oh so the best case.",
                    "label": 0
                },
                {
                    "sent": "This race will be dimension free and.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just few comments.",
                    "label": 0
                },
                {
                    "sent": "As already said, the key assumption is satisfied for many many functions.",
                    "label": 1
                },
                {
                    "sent": "On the other hand, this algorithm is zero order, so it doesn't use any information about the derivatives, so it might not.",
                    "label": 0
                },
                {
                    "sent": "Might not have good rates for.",
                    "label": 0
                },
                {
                    "sent": "Smooth functions.",
                    "label": 0
                },
                {
                    "sent": "So it's all set in the beginning.",
                    "label": 1
                },
                {
                    "sent": "We assume that the support in the distribution is known and at least in practice we want to avoid this assumption.",
                    "label": 0
                },
                {
                    "sent": "And this last part of my talk I will show what can be done in practice.",
                    "label": 0
                },
                {
                    "sent": "So just use similar ideas to develop a slightly different algorithm.",
                    "label": 1
                },
                {
                    "sent": "And to compute, so we compute something.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is the model.",
                    "label": 0
                },
                {
                    "sent": "I consider two scenarios.",
                    "label": 0
                },
                {
                    "sent": "In the first case.",
                    "label": 0
                },
                {
                    "sent": "Oh apply the uniform distribution of the cube and the function.",
                    "label": 0
                },
                {
                    "sent": "Adjust the distance to the origin and in the second case by uniform distribution on the unit sphere, 3 dimensional space and.",
                    "label": 0
                },
                {
                    "sent": "The function is the distance of the pole.",
                    "label": 0
                },
                {
                    "sent": "So once again, especially in the second case, support.",
                    "label": 0
                },
                {
                    "sent": "So the distribution is not known to the algorithm in advance.",
                    "label": 0
                },
                {
                    "sent": "And so here is how.",
                    "label": 0
                },
                {
                    "sent": "Well, here's how.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It works so in the first scenario we observe something like this.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So each picture corresponds to a separate iteration and.",
                    "label": 0
                },
                {
                    "sent": "The points of different colors correspond to the points that are obtained by the algorithm in different iterations.",
                    "label": 0
                },
                {
                    "sent": "Well, that's what it does.",
                    "label": 0
                },
                {
                    "sent": "So we see that the points come closer and closer and closer to the origin.",
                    "label": 0
                },
                {
                    "sent": "And here is just the output.",
                    "label": 0
                },
                {
                    "sent": "So the maximum is obtained at the origin, so we're pretty close to the origin.",
                    "label": 0
                },
                {
                    "sent": "The estimated value is also pretty close to zero, and they just show the total number of functional relations and the total number of points that are processed during the algorithm.",
                    "label": 0
                },
                {
                    "sent": "K for.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Second scenario.",
                    "label": 0
                },
                {
                    "sent": "It is I showed the similar picture.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is my union sphere.",
                    "label": 0
                },
                {
                    "sent": "And this is how the algorithm works.",
                    "label": 0
                },
                {
                    "sent": "So this is the first iteration, the second generation, third iteration, 4th, 5th, etc etc.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So she once again the points tend to tend to the pole and just provide the same same numbers.",
                    "label": 0
                },
                {
                    "sent": "So once again, the true maximum is equal to 0 and we have pretty close to it.",
                    "label": 0
                },
                {
                    "sent": "The number of function violations in this case is larger just because we use the larger value for the standard deviation of noise.",
                    "label": 1
                },
                {
                    "sent": "And the total number of processed samples is a lot larger than the number of functional Asians.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you for attention.",
                    "label": 0
                }
            ]
        }
    }
}