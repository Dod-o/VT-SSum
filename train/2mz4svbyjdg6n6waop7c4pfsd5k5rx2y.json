{
    "id": "2mz4svbyjdg6n6waop7c4pfsd5k5rx2y",
    "title": "Online Learning in The Manifold of Low-Rank Matrices",
    "info": {
        "author": [
            "Uri Shalit, School of Computer Science and Engineering, The Hebrew University of Jerusalem"
        ],
        "published": "March 25, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Manifold Learning"
        ]
    },
    "url": "http://videolectures.net/nips2010_shalit_olm/",
    "segmentation": [
        [
            "Hi so in many cases in machine learning it is useful."
        ],
        [
            "To have a matrix as your model, consider for example the case where you give me some document.",
            "Ann, you want you to find some document other documents similar to that query document so we can model the similarity between a pair of documents P&Q as the bilinear form.",
            "Too bad, OK, sorry bout that.",
            "OK, so if my problem is to model similarity between parafed documents, I can MoD a bilinear form.",
            "I have a document P&Q and model similarity by bilinear form P. Transfers WQ and the metrics W would be my model.",
            "Second example of matrix models, which we've seen a lot in this conference so far is a multi task learning so I can have a linear classifier for each task and just stack up all these classifier to four metric matrix model.",
            "A natural way to regularize such matrix models is using a low rank constraint.",
            "This could mean that I believe the data lies in some low dimensional space.",
            "Or maybe I believe that my tests are just a combination of a small number of latent tests.",
            "Another advantage of this low rank constraint is that lowering matrices have a lot less parameters and therefore they need a lot less memory, and they're much faster to compute.",
            "Anne.",
            "That allows me to tackle problems which are much larger than those that I could have tackled if I use the full rank model.",
            "Now in this work we present an online algorithm for learning low rank matrices because the problems which tackle are both high dimensional and large scale.",
            "Now."
        ],
        [
            "Now it's it's well known that the computation at enforcing alluring constraint is computationally hard.",
            "But an interesting observation is that the set of low rank matrices of Frank K actually forms a manifold embedded in Euclidean space.",
            "This observation gives us a new way to approach this problem, because now we can view online learning as following some path within this manifold, trying to minimize some stochastic loss function.",
            "Now from Minnesota population Theory, we know that the ideal path you'd want to follow is that of the geodesics that point in the direction of the gradient.",
            "But calculating these geodesics is also computationally hard.",
            "Now, a more straightforward way to approach this constraint optimization problem is just to do projected gradient right.",
            "You can take a gradient step and project back to the manifold.",
            "Now it turns out that projected gradient is just an approximation.",
            "Actually 2nd order approximation of the ideal path and the problem with projected gradient in this case is that it requires a singular value decomposition at each online step, which is pretty heavy computationally.",
            "Now, approximations that like projected gradient to the ideal path are known as retractions.",
            "And in our work we present a new retraction which also approximates the ideal path.",
            "But in the online case it's much faster to compute compared with an SVD.",
            "A retraction, computing retraction, realizing just some basic linear algebra operations along with a few computational tricks.",
            "No."
        ],
        [
            "Using this new attraction, we formulate our online algorithm, which we call the Loretta for a low rank retraction algorithm.",
            "The computational complexity of Loretta is linear in the number of model parameters.",
            "That means that we take full advantage of the memory and computational savings that low rank matrices can offer.",
            "And now we we experimented with the red on two different task.",
            "One of learning document similarity, the other of ranking labels for images taken from the image in that data set an.",
            "In both of these experiments, Loretta outperforms other methods which included the methods that optimize the factors of the dark matrices as well as online L2 one learning.",
            "And For more information about the experience about the of the method, you're all welcome to our poster tonight that W 23 thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi so in many cases in machine learning it is useful.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To have a matrix as your model, consider for example the case where you give me some document.",
                    "label": 0
                },
                {
                    "sent": "Ann, you want you to find some document other documents similar to that query document so we can model the similarity between a pair of documents P&Q as the bilinear form.",
                    "label": 0
                },
                {
                    "sent": "Too bad, OK, sorry bout that.",
                    "label": 0
                },
                {
                    "sent": "OK, so if my problem is to model similarity between parafed documents, I can MoD a bilinear form.",
                    "label": 0
                },
                {
                    "sent": "I have a document P&Q and model similarity by bilinear form P. Transfers WQ and the metrics W would be my model.",
                    "label": 0
                },
                {
                    "sent": "Second example of matrix models, which we've seen a lot in this conference so far is a multi task learning so I can have a linear classifier for each task and just stack up all these classifier to four metric matrix model.",
                    "label": 0
                },
                {
                    "sent": "A natural way to regularize such matrix models is using a low rank constraint.",
                    "label": 0
                },
                {
                    "sent": "This could mean that I believe the data lies in some low dimensional space.",
                    "label": 0
                },
                {
                    "sent": "Or maybe I believe that my tests are just a combination of a small number of latent tests.",
                    "label": 0
                },
                {
                    "sent": "Another advantage of this low rank constraint is that lowering matrices have a lot less parameters and therefore they need a lot less memory, and they're much faster to compute.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "That allows me to tackle problems which are much larger than those that I could have tackled if I use the full rank model.",
                    "label": 0
                },
                {
                    "sent": "Now in this work we present an online algorithm for learning low rank matrices because the problems which tackle are both high dimensional and large scale.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now it's it's well known that the computation at enforcing alluring constraint is computationally hard.",
                    "label": 1
                },
                {
                    "sent": "But an interesting observation is that the set of low rank matrices of Frank K actually forms a manifold embedded in Euclidean space.",
                    "label": 1
                },
                {
                    "sent": "This observation gives us a new way to approach this problem, because now we can view online learning as following some path within this manifold, trying to minimize some stochastic loss function.",
                    "label": 0
                },
                {
                    "sent": "Now from Minnesota population Theory, we know that the ideal path you'd want to follow is that of the geodesics that point in the direction of the gradient.",
                    "label": 0
                },
                {
                    "sent": "But calculating these geodesics is also computationally hard.",
                    "label": 0
                },
                {
                    "sent": "Now, a more straightforward way to approach this constraint optimization problem is just to do projected gradient right.",
                    "label": 0
                },
                {
                    "sent": "You can take a gradient step and project back to the manifold.",
                    "label": 0
                },
                {
                    "sent": "Now it turns out that projected gradient is just an approximation.",
                    "label": 0
                },
                {
                    "sent": "Actually 2nd order approximation of the ideal path and the problem with projected gradient in this case is that it requires a singular value decomposition at each online step, which is pretty heavy computationally.",
                    "label": 0
                },
                {
                    "sent": "Now, approximations that like projected gradient to the ideal path are known as retractions.",
                    "label": 1
                },
                {
                    "sent": "And in our work we present a new retraction which also approximates the ideal path.",
                    "label": 1
                },
                {
                    "sent": "But in the online case it's much faster to compute compared with an SVD.",
                    "label": 0
                },
                {
                    "sent": "A retraction, computing retraction, realizing just some basic linear algebra operations along with a few computational tricks.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Using this new attraction, we formulate our online algorithm, which we call the Loretta for a low rank retraction algorithm.",
                    "label": 0
                },
                {
                    "sent": "The computational complexity of Loretta is linear in the number of model parameters.",
                    "label": 1
                },
                {
                    "sent": "That means that we take full advantage of the memory and computational savings that low rank matrices can offer.",
                    "label": 1
                },
                {
                    "sent": "And now we we experimented with the red on two different task.",
                    "label": 0
                },
                {
                    "sent": "One of learning document similarity, the other of ranking labels for images taken from the image in that data set an.",
                    "label": 0
                },
                {
                    "sent": "In both of these experiments, Loretta outperforms other methods which included the methods that optimize the factors of the dark matrices as well as online L2 one learning.",
                    "label": 0
                },
                {
                    "sent": "And For more information about the experience about the of the method, you're all welcome to our poster tonight that W 23 thanks.",
                    "label": 0
                }
            ]
        }
    }
}