{
    "id": "ipytmoeskp3pj7dp6qxefdvqjb2ap2te",
    "title": "Laplace Maximum Margin Markov Networks",
    "info": {
        "author": [
            "Jun Zhu, Department of Computer Science and Technology, Tsinghua University"
        ],
        "published": "July 28, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Markov Processes"
        ]
    },
    "url": "http://videolectures.net/icml08_zhu_lmm/",
    "segmentation": [
        [
            "Our next talk is on LA Plus maximum margin Markov networks.",
            "The papers by Jun Zhu, Eric Xing, and Bosang and will be presented by June.",
            "OK, thank you.",
            "So is it is this talk.",
            "I will introduce less Max margin Mark Network so I'm doing and from Shanghai University.",
            "This work was done when I visit visited University and its joint work with Arash."
        ],
        [
            "And quotes on.",
            "So here's a content server causes this talk again.",
            "I will briefly introduce the problem of strike prediction and introduce some.",
            "Existing model is actually the maximum Mark network.",
            "The second part is Nova Framework, proposed to do baseline next magin learning, and we refer to present them better theorem and their average introduce the spare case of last smack smack smack network.",
            "Before concluding this talk about issues and empirical."
        ],
        [
            "Doubts.",
            "OK, let's consider that should initialize the complication problem.",
            "So here we take a set of inputs and so each sample is.",
            "A dimension feature vector and.",
            "The change order will have true level.",
            "So out of target to learn a mapping function.",
            "So we take a test input and assign a class label.",
            "So traditionally we use this rule so we have a score function.",
            "So you can you have different definition of this F function.",
            "So a common choice is the linear model.",
            "So your problem those is the logistic regression and support conversion, so they perform different edge method.",
            "Logistic regression is based on maximum alert for defamation.",
            "So you define likelihood model, sorry.",
            "Yeah, so we define a lot of like the model which is distribution and.",
            "That's where I'm performing.",
            "This might be learning it directly.",
            "Optimize the margin and subject to a set of constraints which to enforce that the model can predict correctly on the training data."
        ],
        [
            "So your Lucas is the problem more complicated, for example, employs typing.",
            "We have our input sequence of words or tokens, so our target is to assign appears tag to each each word.",
            "So this problem is.",
            "The character of different words potentially correlated together so.",
            "So General Mesa inefficient newscast by Chili's sample independent.",
            "Another example is the 2 dimensional image segmentation.",
            "So our input 2 dimensional and target target to assign a class level to each pixel.",
            "So again in this case this levels are correlated together.",
            "So for example, the memory pixels tend to have the same class level.",
            "So strike prediction is a framework to jointly do the prediction, so again we consider the supervised learning case.",
            "So our training samples have this form so excited can contain multiple XJ, so each extra is the D dimensional feature vector.",
            "So the age of production is.",
            "A set of single level so.",
            "So here I'll target you also learn a mapping function and for for incoming test test example run to assign out good good prediction."
        ],
        [
            "For context, the two popular models for straight production.",
            "This one is the contingent fees proposed by John Locke and his colleagues in 2001 and otherwise makes money market network.",
            "Proposed by a task so they they take two different learning learning methods.",
            "Sad for you based on just like legit logistic regression and maximum language mention we define conditional likelihood model.",
            "Which is also implemented family distribution.",
            "At the mystery, and is a generalization of SM to structure prediction case.",
            "So if you optimize the margin and also these constraint to enforce their model predicted correctly on the training data.",
            "So in this case because each production can have multiple single level, so we need a error function to augment the margin constraint.",
            "So from this model can be complicated because we could have a potential number of possible predictions.",
            "So turn back this model computationally efficient.",
            "We usually make some Markov assumptions, so for example in this chain graph, bottom property says that giving a random variables these two numbers are independent.",
            "So so when we define redefine these functions?",
            "We can only define the clickers in this graph.",
            "In this case, it's the signals of the edge."
        ],
        [
            "OK, so we have introduced some models for both classification and strike prediction.",
            "That's better there.",
            "They take two different learning from work.",
            "Why is the maximum likelihood estimation and otherwise mixed market learning?",
            "So let's compare these two framework.",
            "That's good, like about that dimension is the probabilistic method.",
            "It define a joint or conditional like the model.",
            "So the advantage is that it can be easily adapted to perform Bayesian learning.",
            "So we can consider each do some prior knowledge of consistent missing information so that the best final learning directly optimizing margin so it's.",
            "Somehow, like implicit probability interpretation.",
            "So if not have obvious how to how to do basic learning, how to ensure the priority concern missing data?",
            "So but learning has its advantages, so Windows there are some theoretical arguments about the prediction error as well, and then through network.",
            "So maybe you're wondering whether we can combine the merits of from two work.",
            "Well, there's some work I've already done for the classification problem.",
            "This is the framework proposed by the collapse in 1999, so basically perform basic learning so you learn a polity redistribution and for prediction you take the average.",
            "So this is a binary classification problem.",
            "So 2 pictures.",
            "The best posterior distribution you you solve this optimization problem.",
            "It's a character vergence to the this is the prior.",
            "So these constraints define a subspace of possible distribution.",
            "So better if this constraint.",
            "Are related to the margin.",
            "It is similar to set up as well.",
            "So this framework has a lot of nice property, so when I would like to mention that and we did actually as well with the spare case of this framework."
        ],
        [
            "So.",
            "So we are probably the first part of work to attend this virtuous strike code in case so we propose the.",
            "That's the Internet.",
            "It's it's a long title so.",
            "So, but here we we perform testing, learning and learn a party redistribution.",
            "So we solve this optimization problem.",
            "So better, this is regularised care divergences mu is.",
            "Max function.",
            "So its function is also called generalized maximum entropy.",
            "So to consider margin we define this subspace.",
            "So that if we ignore this average, these constraints are the same as those of the standard measurement mark network.",
            "So to show the basic idea so so before the end that we will choose the prior distribution subjective.",
            "Then we see some data, so we re add some constraint because we want to make a correct prediction on the same data so.",
            "This constraint is a linear function of PW, so we define our plans, distribute space so this is inequality implies that we only one up space is feasible.",
            "So when we see more and more data, we add more and more constraint.",
            "So the intersection intersection of all the halfspaces defined this piece of subspace.",
            "So Max entropy principle says that we should choose this one.",
            "At the the small small care divergent to the prior.",
            "So that's the best idea.",
            "So to solve this problem, we will note that this is a convex function of PW and also we assume you is a convex function.",
            "So there was this.",
            "This constraint is a linear of PW, so is defined a convex set.",
            "So this is a correct program, so we can define a library function and applies a calculus of variation and get."
        ],
        [
            "This is a test results.",
            "So the general solution of this problem is of this form.",
            "This is prior and this part can be seen as a language model because it related to the data.",
            "So there is a part in functional normalization factor to make the whole thing is the probability distribution.",
            "So these parameters are achieved by solving this problem.",
            "So you size the convex conjugate of new function.",
            "So here's some basic design of complex conjugate.",
            "This is the definition so far is atomic functions, and by size conjugate.",
            "So here's some here to spare case.",
            "For the care divergent, we have the conjugate is lock party function, so it's not spread.",
            "While we have this part here.",
            "So also for L1 norm.",
            "Function we have.",
            "This is the controller is just a set of box constraint."
        ],
        [
            "So as I have mentioned to me, different works.",
            "That's why we have the special case of MDD.",
            "So here we have a similar results.",
            "If we make some assumption up front, we can reduce to the standard M3, and so this is a theory.",
            "Supposed this is a linear model and the prior stand normal.",
            "So MU functions just right now so have.",
            "The party real history is also a normal with the ship Jimmy, so he knows the variance.",
            "The correct metric doesn't change is also identical metric.",
            "So that you know that your problem is this one.",
            "So this is the same as the standard M3.",
            "So if you use the posterior to do prediction, you can get the same particular SM three and so are from country called the standard M streaming.",
            "So this is good, but it would be better if we can do a different additional thing.",
            "So actually makes sense that can have at least three advantages."
        ],
        [
            "So here they are.",
            "So first.",
            "An excellent Internet has some packages and bound by the prediction error.",
            "Also it has a prior we can introduce some regularization fix, so just pass it.",
            "Also, because you perform best in learning, we can consider missing that of some some hidden strong."
        ],
        [
            "Sure.",
            "So in this talk I will cover the first 2, the third one is beyond the scope of this paper."
        ],
        [
            "So OK, so first, why is that going to the theater without upper bound?",
            "But because it makes sense that perform basic learning.",
            "Also, it's subsume observing, so we also call it the best management network.",
            "So this is basically results so.",
            "So this is our model, we devices margin and if you assume the function is bounded then we can have this powerful choose with provision at least one minus Delta.",
            "So basically says if you can find a distribution that have a small care divergent.",
            "Also make most of the training data have a light bound like margin.",
            "Then you can guarantee to more like to particular correctly on the."
        ],
        [
            "Teacher test data.",
            "So, so the second one is that supplier can be designed to induce sparsity effect.",
            "So, scientists is desirable property in machine learning set so many data may be high dimensional space or have some involving Iran features.",
            "Normal price is not so good because Windows at 1st and almost as the party is also normal.",
            "So the correct material doesn't change so it means it cannot distinguish relevant or irrelevant feature.",
            "So here we use the Laplace.",
            "This user graph issues.",
            "Compare these two different price this formula this mathematical formulas.",
            "So when I say representation we used is also called sticky mixture interpretation of lot of Laplace prior.",
            "So we first have a normal you have a high pressure on the variance so."
        ],
        [
            "OK, so.",
            "So let's first see how the Laplace priors affect the partial mean.",
            "So if you do the exact computation, we could get the part in function as this one, so it came as a component of data.",
            "Is a vector, so you take this directive you can get.",
            "You can get this this formula and you can take another approach to use the definition of Z.",
            "We also take the derivative.",
            "Get this one if you compare these two you get the partial me is a function of eater.",
            "So also in the M3 four M3 we can follow the same steps because it's a spare case with a standard prior then get this audio me so we know that there is a transformation businesses too.",
            "2 results.",
            "So if we put the key on this axis and possibly on this one, so you can enjoy this curve.",
            "The blue is for the stand normal M3 and so the other two are for the Laplace prior with different parameters.",
            "So you can see.",
            "Around the Rivera party.",
            "Reassuringly effect, that's the best idea, so let's see how to solve this."
        ],
        [
            "Problem.",
            "So remember we do these active computation.",
            "We get this body function and we get this do this can be hard because this quarter, here is the logarithm.",
            "Remember this this term depends on all the parameters.",
            "So return to rational Bayesian learning.",
            "So we use the hierarchical advantage and the River bound.",
            "Our problem is this one, so we do our math memorization so this step can be nice down cause.",
            "So we keep this part fixed.",
            "OK, actually, surprise normal.",
            "So this is standard M3 optimization problem so that we could PW fixed so we can have a closed form solution of you talk.",
            "So this can nicely done.",
            "So return to resolve these two step."
        ],
        [
            "OK.",
            "So this is this is a procedure, so we first start with you me and identity covariance of software through and update occurrence return to sources to step.",
            "So here's a detailed derivative."
        ],
        [
            "Yeah, this is basically the model and a better theory, so let's shoot some impairments.",
            "OK, so we can see if we compare with the standard EMS CRF will perform.",
            "And also with different norm Elterlein remember ranges gives sparse estimation.",
            "So."
        ],
        [
            "1st results on unsuspected set.",
            "We generate that with the first date with IID features.",
            "We control the number of relevant features, so here's the results we can see.",
            "Our model can perform as well as the sparse model memorize gap is sparse.",
            "So the second set is with correlated features, so again we can have the same conclusion."
        ],
        [
            "So the second part of your promises download the real CR data set so large and that is partly tenfold so.",
            "Random subsample subset to do 10 first cross validation, R model performed the best on different subsets and also.",
            "Yeah, performer."
        ],
        [
            "The models.",
            "So that's how it's about sensitivity of the different model to the regular content.",
            "So we want to model.",
            "We use, we choose turn different constants and she's afraid of performance, so so you can see myself with much sensitive to the regular content."
        ],
        [
            "So these are proof summary.",
            "Yeah, that's good.",
            "Our attitude to pass the first is general framework and we also started the special Laplace mechanism.",
            "Yeah OK thanks."
        ],
        [
            "So we use a Gaussian priors.",
            "Betrayed models the identical spectacular next Monday morning at work.",
            "Yeah, yeah yes.",
            "So so the question is if we use the normal prior is a problem?",
            "Identical terms.",
            "Is it right?",
            "Yeah, the channel yes, yes.",
            "Actually the absolutely just spare cash with the standard more if you use a different normal price, just changing the correct matrix then no different in the training.",
            "Hi.",
            "Yeah.",
            "What yes?",
            "So so basically.",
            "So, so this is our algorithm, so we we can return to solve 2 step.",
            "So basically the cost is is consumed in this step.",
            "So this step as I mentioned it's it can be closed.",
            "This solved so.",
            "So our Department so just to we just do three to five iterations so it's compatible to the standard M3 and.",
            "Yeah.",
            "So I have a question.",
            "So in logistic regression the usual motivation trick to disallow plus price to induce sparsity, right?",
            "Yeah, but in the case of Max margin Markov networks, the sparsity is already induced by the margin constraints.",
            "So what is the motivation for on top of that, having a lot plus prior?",
            "I.",
            "Yes, he in logistic regression people use the Laplace to do sparse Bayesian learning.",
            "Yeah, actually that's motivation above.",
            "Or we just studied the loveless prior.",
            "How effect affect the?",
            "The Max margin learning.",
            "Yeah, but if you're already doing Max margin learning then the solutions are already sparse.",
            "Yeah, you mean you just user unknown.",
            "My point is that if you use Max margin constraints as you are, then there's no need for the L1 prior.",
            "That's different concept because as we just passed what it means, you just use the limited number of samples.",
            "So here we we need sparsity is the number of features.",
            "That they're different concepts.",
            "Yes, it's it's structure classification, right?",
            "So that we are equivalent in some sense.",
            "I.",
            "Quiet.",
            "Department of Transportation.",
            "So I can take it to you at night.",
            "I I agree with your point.",
            "These parts radiator on the infrastructure involves having the new apartment will be come out from having market center perfect interactions.",
            "We never had this problem, yeah?",
            "Yeah.",
            "So in fact, if you are going to have more everything.",
            "Yes.",
            "Supposed to be expensive.",
            "Marketing yourself.",
            "Eating, so did you observe better by doing distribution?",
            "Everything working as sensitive shipping boxes?",
            "That is, yeah.",
            "What we show here.",
            "So you are, but we do the average right?",
            "We do will perform best in learning.",
            "We do the averages so you can see this.",
            "This model is the most stable to incentive to the regular content.",
            "So this is the blue eyed double curve, yeah?",
            "So far for SCR, because you choose the change, the parameter is.",
            "You get the dependence parset machine so much sensitive to the contents you use.",
            "Yeah.",
            "Any other questions?"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our next talk is on LA Plus maximum margin Markov networks.",
                    "label": 0
                },
                {
                    "sent": "The papers by Jun Zhu, Eric Xing, and Bosang and will be presented by June.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "So is it is this talk.",
                    "label": 0
                },
                {
                    "sent": "I will introduce less Max margin Mark Network so I'm doing and from Shanghai University.",
                    "label": 0
                },
                {
                    "sent": "This work was done when I visit visited University and its joint work with Arash.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And quotes on.",
                    "label": 0
                },
                {
                    "sent": "So here's a content server causes this talk again.",
                    "label": 0
                },
                {
                    "sent": "I will briefly introduce the problem of strike prediction and introduce some.",
                    "label": 0
                },
                {
                    "sent": "Existing model is actually the maximum Mark network.",
                    "label": 0
                },
                {
                    "sent": "The second part is Nova Framework, proposed to do baseline next magin learning, and we refer to present them better theorem and their average introduce the spare case of last smack smack smack network.",
                    "label": 0
                },
                {
                    "sent": "Before concluding this talk about issues and empirical.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Doubts.",
                    "label": 0
                },
                {
                    "sent": "OK, let's consider that should initialize the complication problem.",
                    "label": 0
                },
                {
                    "sent": "So here we take a set of inputs and so each sample is.",
                    "label": 1
                },
                {
                    "sent": "A dimension feature vector and.",
                    "label": 0
                },
                {
                    "sent": "The change order will have true level.",
                    "label": 0
                },
                {
                    "sent": "So out of target to learn a mapping function.",
                    "label": 0
                },
                {
                    "sent": "So we take a test input and assign a class label.",
                    "label": 0
                },
                {
                    "sent": "So traditionally we use this rule so we have a score function.",
                    "label": 0
                },
                {
                    "sent": "So you can you have different definition of this F function.",
                    "label": 0
                },
                {
                    "sent": "So a common choice is the linear model.",
                    "label": 0
                },
                {
                    "sent": "So your problem those is the logistic regression and support conversion, so they perform different edge method.",
                    "label": 1
                },
                {
                    "sent": "Logistic regression is based on maximum alert for defamation.",
                    "label": 0
                },
                {
                    "sent": "So you define likelihood model, sorry.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so we define a lot of like the model which is distribution and.",
                    "label": 0
                },
                {
                    "sent": "That's where I'm performing.",
                    "label": 0
                },
                {
                    "sent": "This might be learning it directly.",
                    "label": 0
                },
                {
                    "sent": "Optimize the margin and subject to a set of constraints which to enforce that the model can predict correctly on the training data.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So your Lucas is the problem more complicated, for example, employs typing.",
                    "label": 0
                },
                {
                    "sent": "We have our input sequence of words or tokens, so our target is to assign appears tag to each each word.",
                    "label": 0
                },
                {
                    "sent": "So this problem is.",
                    "label": 0
                },
                {
                    "sent": "The character of different words potentially correlated together so.",
                    "label": 0
                },
                {
                    "sent": "So General Mesa inefficient newscast by Chili's sample independent.",
                    "label": 0
                },
                {
                    "sent": "Another example is the 2 dimensional image segmentation.",
                    "label": 0
                },
                {
                    "sent": "So our input 2 dimensional and target target to assign a class level to each pixel.",
                    "label": 0
                },
                {
                    "sent": "So again in this case this levels are correlated together.",
                    "label": 0
                },
                {
                    "sent": "So for example, the memory pixels tend to have the same class level.",
                    "label": 0
                },
                {
                    "sent": "So strike prediction is a framework to jointly do the prediction, so again we consider the supervised learning case.",
                    "label": 0
                },
                {
                    "sent": "So our training samples have this form so excited can contain multiple XJ, so each extra is the D dimensional feature vector.",
                    "label": 0
                },
                {
                    "sent": "So the age of production is.",
                    "label": 0
                },
                {
                    "sent": "A set of single level so.",
                    "label": 1
                },
                {
                    "sent": "So here I'll target you also learn a mapping function and for for incoming test test example run to assign out good good prediction.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For context, the two popular models for straight production.",
                    "label": 0
                },
                {
                    "sent": "This one is the contingent fees proposed by John Locke and his colleagues in 2001 and otherwise makes money market network.",
                    "label": 0
                },
                {
                    "sent": "Proposed by a task so they they take two different learning learning methods.",
                    "label": 0
                },
                {
                    "sent": "Sad for you based on just like legit logistic regression and maximum language mention we define conditional likelihood model.",
                    "label": 1
                },
                {
                    "sent": "Which is also implemented family distribution.",
                    "label": 0
                },
                {
                    "sent": "At the mystery, and is a generalization of SM to structure prediction case.",
                    "label": 0
                },
                {
                    "sent": "So if you optimize the margin and also these constraint to enforce their model predicted correctly on the training data.",
                    "label": 0
                },
                {
                    "sent": "So in this case because each production can have multiple single level, so we need a error function to augment the margin constraint.",
                    "label": 0
                },
                {
                    "sent": "So from this model can be complicated because we could have a potential number of possible predictions.",
                    "label": 0
                },
                {
                    "sent": "So turn back this model computationally efficient.",
                    "label": 0
                },
                {
                    "sent": "We usually make some Markov assumptions, so for example in this chain graph, bottom property says that giving a random variables these two numbers are independent.",
                    "label": 0
                },
                {
                    "sent": "So so when we define redefine these functions?",
                    "label": 0
                },
                {
                    "sent": "We can only define the clickers in this graph.",
                    "label": 0
                },
                {
                    "sent": "In this case, it's the signals of the edge.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we have introduced some models for both classification and strike prediction.",
                    "label": 0
                },
                {
                    "sent": "That's better there.",
                    "label": 0
                },
                {
                    "sent": "They take two different learning from work.",
                    "label": 0
                },
                {
                    "sent": "Why is the maximum likelihood estimation and otherwise mixed market learning?",
                    "label": 0
                },
                {
                    "sent": "So let's compare these two framework.",
                    "label": 0
                },
                {
                    "sent": "That's good, like about that dimension is the probabilistic method.",
                    "label": 0
                },
                {
                    "sent": "It define a joint or conditional like the model.",
                    "label": 0
                },
                {
                    "sent": "So the advantage is that it can be easily adapted to perform Bayesian learning.",
                    "label": 1
                },
                {
                    "sent": "So we can consider each do some prior knowledge of consistent missing information so that the best final learning directly optimizing margin so it's.",
                    "label": 0
                },
                {
                    "sent": "Somehow, like implicit probability interpretation.",
                    "label": 1
                },
                {
                    "sent": "So if not have obvious how to how to do basic learning, how to ensure the priority concern missing data?",
                    "label": 0
                },
                {
                    "sent": "So but learning has its advantages, so Windows there are some theoretical arguments about the prediction error as well, and then through network.",
                    "label": 0
                },
                {
                    "sent": "So maybe you're wondering whether we can combine the merits of from two work.",
                    "label": 0
                },
                {
                    "sent": "Well, there's some work I've already done for the classification problem.",
                    "label": 1
                },
                {
                    "sent": "This is the framework proposed by the collapse in 1999, so basically perform basic learning so you learn a polity redistribution and for prediction you take the average.",
                    "label": 1
                },
                {
                    "sent": "So this is a binary classification problem.",
                    "label": 0
                },
                {
                    "sent": "So 2 pictures.",
                    "label": 0
                },
                {
                    "sent": "The best posterior distribution you you solve this optimization problem.",
                    "label": 0
                },
                {
                    "sent": "It's a character vergence to the this is the prior.",
                    "label": 0
                },
                {
                    "sent": "So these constraints define a subspace of possible distribution.",
                    "label": 0
                },
                {
                    "sent": "So better if this constraint.",
                    "label": 0
                },
                {
                    "sent": "Are related to the margin.",
                    "label": 0
                },
                {
                    "sent": "It is similar to set up as well.",
                    "label": 0
                },
                {
                    "sent": "So this framework has a lot of nice property, so when I would like to mention that and we did actually as well with the spare case of this framework.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So we are probably the first part of work to attend this virtuous strike code in case so we propose the.",
                    "label": 0
                },
                {
                    "sent": "That's the Internet.",
                    "label": 0
                },
                {
                    "sent": "It's it's a long title so.",
                    "label": 0
                },
                {
                    "sent": "So, but here we we perform testing, learning and learn a party redistribution.",
                    "label": 0
                },
                {
                    "sent": "So we solve this optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So better, this is regularised care divergences mu is.",
                    "label": 0
                },
                {
                    "sent": "Max function.",
                    "label": 0
                },
                {
                    "sent": "So its function is also called generalized maximum entropy.",
                    "label": 1
                },
                {
                    "sent": "So to consider margin we define this subspace.",
                    "label": 0
                },
                {
                    "sent": "So that if we ignore this average, these constraints are the same as those of the standard measurement mark network.",
                    "label": 0
                },
                {
                    "sent": "So to show the basic idea so so before the end that we will choose the prior distribution subjective.",
                    "label": 0
                },
                {
                    "sent": "Then we see some data, so we re add some constraint because we want to make a correct prediction on the same data so.",
                    "label": 0
                },
                {
                    "sent": "This constraint is a linear function of PW, so we define our plans, distribute space so this is inequality implies that we only one up space is feasible.",
                    "label": 0
                },
                {
                    "sent": "So when we see more and more data, we add more and more constraint.",
                    "label": 0
                },
                {
                    "sent": "So the intersection intersection of all the halfspaces defined this piece of subspace.",
                    "label": 0
                },
                {
                    "sent": "So Max entropy principle says that we should choose this one.",
                    "label": 0
                },
                {
                    "sent": "At the the small small care divergent to the prior.",
                    "label": 0
                },
                {
                    "sent": "So that's the best idea.",
                    "label": 0
                },
                {
                    "sent": "So to solve this problem, we will note that this is a convex function of PW and also we assume you is a convex function.",
                    "label": 0
                },
                {
                    "sent": "So there was this.",
                    "label": 0
                },
                {
                    "sent": "This constraint is a linear of PW, so is defined a convex set.",
                    "label": 0
                },
                {
                    "sent": "So this is a correct program, so we can define a library function and applies a calculus of variation and get.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is a test results.",
                    "label": 0
                },
                {
                    "sent": "So the general solution of this problem is of this form.",
                    "label": 0
                },
                {
                    "sent": "This is prior and this part can be seen as a language model because it related to the data.",
                    "label": 0
                },
                {
                    "sent": "So there is a part in functional normalization factor to make the whole thing is the probability distribution.",
                    "label": 0
                },
                {
                    "sent": "So these parameters are achieved by solving this problem.",
                    "label": 0
                },
                {
                    "sent": "So you size the convex conjugate of new function.",
                    "label": 1
                },
                {
                    "sent": "So here's some basic design of complex conjugate.",
                    "label": 0
                },
                {
                    "sent": "This is the definition so far is atomic functions, and by size conjugate.",
                    "label": 0
                },
                {
                    "sent": "So here's some here to spare case.",
                    "label": 0
                },
                {
                    "sent": "For the care divergent, we have the conjugate is lock party function, so it's not spread.",
                    "label": 0
                },
                {
                    "sent": "While we have this part here.",
                    "label": 0
                },
                {
                    "sent": "So also for L1 norm.",
                    "label": 0
                },
                {
                    "sent": "Function we have.",
                    "label": 0
                },
                {
                    "sent": "This is the controller is just a set of box constraint.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as I have mentioned to me, different works.",
                    "label": 0
                },
                {
                    "sent": "That's why we have the special case of MDD.",
                    "label": 0
                },
                {
                    "sent": "So here we have a similar results.",
                    "label": 0
                },
                {
                    "sent": "If we make some assumption up front, we can reduce to the standard M3, and so this is a theory.",
                    "label": 0
                },
                {
                    "sent": "Supposed this is a linear model and the prior stand normal.",
                    "label": 0
                },
                {
                    "sent": "So MU functions just right now so have.",
                    "label": 0
                },
                {
                    "sent": "The party real history is also a normal with the ship Jimmy, so he knows the variance.",
                    "label": 0
                },
                {
                    "sent": "The correct metric doesn't change is also identical metric.",
                    "label": 0
                },
                {
                    "sent": "So that you know that your problem is this one.",
                    "label": 0
                },
                {
                    "sent": "So this is the same as the standard M3.",
                    "label": 0
                },
                {
                    "sent": "So if you use the posterior to do prediction, you can get the same particular SM three and so are from country called the standard M streaming.",
                    "label": 0
                },
                {
                    "sent": "So this is good, but it would be better if we can do a different additional thing.",
                    "label": 0
                },
                {
                    "sent": "So actually makes sense that can have at least three advantages.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here they are.",
                    "label": 0
                },
                {
                    "sent": "So first.",
                    "label": 0
                },
                {
                    "sent": "An excellent Internet has some packages and bound by the prediction error.",
                    "label": 0
                },
                {
                    "sent": "Also it has a prior we can introduce some regularization fix, so just pass it.",
                    "label": 0
                },
                {
                    "sent": "Also, because you perform best in learning, we can consider missing that of some some hidden strong.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sure.",
                    "label": 0
                },
                {
                    "sent": "So in this talk I will cover the first 2, the third one is beyond the scope of this paper.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So OK, so first, why is that going to the theater without upper bound?",
                    "label": 0
                },
                {
                    "sent": "But because it makes sense that perform basic learning.",
                    "label": 0
                },
                {
                    "sent": "Also, it's subsume observing, so we also call it the best management network.",
                    "label": 1
                },
                {
                    "sent": "So this is basically results so.",
                    "label": 0
                },
                {
                    "sent": "So this is our model, we devices margin and if you assume the function is bounded then we can have this powerful choose with provision at least one minus Delta.",
                    "label": 0
                },
                {
                    "sent": "So basically says if you can find a distribution that have a small care divergent.",
                    "label": 0
                },
                {
                    "sent": "Also make most of the training data have a light bound like margin.",
                    "label": 0
                },
                {
                    "sent": "Then you can guarantee to more like to particular correctly on the.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Teacher test data.",
                    "label": 0
                },
                {
                    "sent": "So, so the second one is that supplier can be designed to induce sparsity effect.",
                    "label": 1
                },
                {
                    "sent": "So, scientists is desirable property in machine learning set so many data may be high dimensional space or have some involving Iran features.",
                    "label": 1
                },
                {
                    "sent": "Normal price is not so good because Windows at 1st and almost as the party is also normal.",
                    "label": 0
                },
                {
                    "sent": "So the correct material doesn't change so it means it cannot distinguish relevant or irrelevant feature.",
                    "label": 0
                },
                {
                    "sent": "So here we use the Laplace.",
                    "label": 1
                },
                {
                    "sent": "This user graph issues.",
                    "label": 0
                },
                {
                    "sent": "Compare these two different price this formula this mathematical formulas.",
                    "label": 0
                },
                {
                    "sent": "So when I say representation we used is also called sticky mixture interpretation of lot of Laplace prior.",
                    "label": 0
                },
                {
                    "sent": "So we first have a normal you have a high pressure on the variance so.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So let's first see how the Laplace priors affect the partial mean.",
                    "label": 0
                },
                {
                    "sent": "So if you do the exact computation, we could get the part in function as this one, so it came as a component of data.",
                    "label": 0
                },
                {
                    "sent": "Is a vector, so you take this directive you can get.",
                    "label": 0
                },
                {
                    "sent": "You can get this this formula and you can take another approach to use the definition of Z.",
                    "label": 0
                },
                {
                    "sent": "We also take the derivative.",
                    "label": 0
                },
                {
                    "sent": "Get this one if you compare these two you get the partial me is a function of eater.",
                    "label": 0
                },
                {
                    "sent": "So also in the M3 four M3 we can follow the same steps because it's a spare case with a standard prior then get this audio me so we know that there is a transformation businesses too.",
                    "label": 0
                },
                {
                    "sent": "2 results.",
                    "label": 0
                },
                {
                    "sent": "So if we put the key on this axis and possibly on this one, so you can enjoy this curve.",
                    "label": 0
                },
                {
                    "sent": "The blue is for the stand normal M3 and so the other two are for the Laplace prior with different parameters.",
                    "label": 0
                },
                {
                    "sent": "So you can see.",
                    "label": 0
                },
                {
                    "sent": "Around the Rivera party.",
                    "label": 0
                },
                {
                    "sent": "Reassuringly effect, that's the best idea, so let's see how to solve this.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Problem.",
                    "label": 0
                },
                {
                    "sent": "So remember we do these active computation.",
                    "label": 0
                },
                {
                    "sent": "We get this body function and we get this do this can be hard because this quarter, here is the logarithm.",
                    "label": 0
                },
                {
                    "sent": "Remember this this term depends on all the parameters.",
                    "label": 0
                },
                {
                    "sent": "So return to rational Bayesian learning.",
                    "label": 1
                },
                {
                    "sent": "So we use the hierarchical advantage and the River bound.",
                    "label": 1
                },
                {
                    "sent": "Our problem is this one, so we do our math memorization so this step can be nice down cause.",
                    "label": 0
                },
                {
                    "sent": "So we keep this part fixed.",
                    "label": 0
                },
                {
                    "sent": "OK, actually, surprise normal.",
                    "label": 0
                },
                {
                    "sent": "So this is standard M3 optimization problem so that we could PW fixed so we can have a closed form solution of you talk.",
                    "label": 1
                },
                {
                    "sent": "So this can nicely done.",
                    "label": 0
                },
                {
                    "sent": "So return to resolve these two step.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this is this is a procedure, so we first start with you me and identity covariance of software through and update occurrence return to sources to step.",
                    "label": 0
                },
                {
                    "sent": "So here's a detailed derivative.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, this is basically the model and a better theory, so let's shoot some impairments.",
                    "label": 0
                },
                {
                    "sent": "OK, so we can see if we compare with the standard EMS CRF will perform.",
                    "label": 0
                },
                {
                    "sent": "And also with different norm Elterlein remember ranges gives sparse estimation.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "1st results on unsuspected set.",
                    "label": 0
                },
                {
                    "sent": "We generate that with the first date with IID features.",
                    "label": 1
                },
                {
                    "sent": "We control the number of relevant features, so here's the results we can see.",
                    "label": 0
                },
                {
                    "sent": "Our model can perform as well as the sparse model memorize gap is sparse.",
                    "label": 0
                },
                {
                    "sent": "So the second set is with correlated features, so again we can have the same conclusion.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the second part of your promises download the real CR data set so large and that is partly tenfold so.",
                    "label": 0
                },
                {
                    "sent": "Random subsample subset to do 10 first cross validation, R model performed the best on different subsets and also.",
                    "label": 0
                },
                {
                    "sent": "Yeah, performer.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The models.",
                    "label": 0
                },
                {
                    "sent": "So that's how it's about sensitivity of the different model to the regular content.",
                    "label": 0
                },
                {
                    "sent": "So we want to model.",
                    "label": 0
                },
                {
                    "sent": "We use, we choose turn different constants and she's afraid of performance, so so you can see myself with much sensitive to the regular content.",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these are proof summary.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's good.",
                    "label": 0
                },
                {
                    "sent": "Our attitude to pass the first is general framework and we also started the special Laplace mechanism.",
                    "label": 0
                },
                {
                    "sent": "Yeah OK thanks.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we use a Gaussian priors.",
                    "label": 0
                },
                {
                    "sent": "Betrayed models the identical spectacular next Monday morning at work.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah yes.",
                    "label": 0
                },
                {
                    "sent": "So so the question is if we use the normal prior is a problem?",
                    "label": 0
                },
                {
                    "sent": "Identical terms.",
                    "label": 0
                },
                {
                    "sent": "Is it right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, the channel yes, yes.",
                    "label": 0
                },
                {
                    "sent": "Actually the absolutely just spare cash with the standard more if you use a different normal price, just changing the correct matrix then no different in the training.",
                    "label": 0
                },
                {
                    "sent": "Hi.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "What yes?",
                    "label": 0
                },
                {
                    "sent": "So so basically.",
                    "label": 0
                },
                {
                    "sent": "So, so this is our algorithm, so we we can return to solve 2 step.",
                    "label": 0
                },
                {
                    "sent": "So basically the cost is is consumed in this step.",
                    "label": 0
                },
                {
                    "sent": "So this step as I mentioned it's it can be closed.",
                    "label": 0
                },
                {
                    "sent": "This solved so.",
                    "label": 0
                },
                {
                    "sent": "So our Department so just to we just do three to five iterations so it's compatible to the standard M3 and.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So I have a question.",
                    "label": 0
                },
                {
                    "sent": "So in logistic regression the usual motivation trick to disallow plus price to induce sparsity, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, but in the case of Max margin Markov networks, the sparsity is already induced by the margin constraints.",
                    "label": 0
                },
                {
                    "sent": "So what is the motivation for on top of that, having a lot plus prior?",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "Yes, he in logistic regression people use the Laplace to do sparse Bayesian learning.",
                    "label": 0
                },
                {
                    "sent": "Yeah, actually that's motivation above.",
                    "label": 0
                },
                {
                    "sent": "Or we just studied the loveless prior.",
                    "label": 0
                },
                {
                    "sent": "How effect affect the?",
                    "label": 0
                },
                {
                    "sent": "The Max margin learning.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but if you're already doing Max margin learning then the solutions are already sparse.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you mean you just user unknown.",
                    "label": 0
                },
                {
                    "sent": "My point is that if you use Max margin constraints as you are, then there's no need for the L1 prior.",
                    "label": 0
                },
                {
                    "sent": "That's different concept because as we just passed what it means, you just use the limited number of samples.",
                    "label": 0
                },
                {
                    "sent": "So here we we need sparsity is the number of features.",
                    "label": 0
                },
                {
                    "sent": "That they're different concepts.",
                    "label": 0
                },
                {
                    "sent": "Yes, it's it's structure classification, right?",
                    "label": 0
                },
                {
                    "sent": "So that we are equivalent in some sense.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "Quiet.",
                    "label": 0
                },
                {
                    "sent": "Department of Transportation.",
                    "label": 0
                },
                {
                    "sent": "So I can take it to you at night.",
                    "label": 0
                },
                {
                    "sent": "I I agree with your point.",
                    "label": 0
                },
                {
                    "sent": "These parts radiator on the infrastructure involves having the new apartment will be come out from having market center perfect interactions.",
                    "label": 0
                },
                {
                    "sent": "We never had this problem, yeah?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So in fact, if you are going to have more everything.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Supposed to be expensive.",
                    "label": 0
                },
                {
                    "sent": "Marketing yourself.",
                    "label": 0
                },
                {
                    "sent": "Eating, so did you observe better by doing distribution?",
                    "label": 0
                },
                {
                    "sent": "Everything working as sensitive shipping boxes?",
                    "label": 0
                },
                {
                    "sent": "That is, yeah.",
                    "label": 0
                },
                {
                    "sent": "What we show here.",
                    "label": 0
                },
                {
                    "sent": "So you are, but we do the average right?",
                    "label": 0
                },
                {
                    "sent": "We do will perform best in learning.",
                    "label": 0
                },
                {
                    "sent": "We do the averages so you can see this.",
                    "label": 0
                },
                {
                    "sent": "This model is the most stable to incentive to the regular content.",
                    "label": 0
                },
                {
                    "sent": "So this is the blue eyed double curve, yeah?",
                    "label": 0
                },
                {
                    "sent": "So far for SCR, because you choose the change, the parameter is.",
                    "label": 0
                },
                {
                    "sent": "You get the dependence parset machine so much sensitive to the contents you use.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                }
            ]
        }
    }
}