{
    "id": "h6624oh7aeqvdrieervxwmu5lddpsk7q",
    "title": "Primal-Dual Subgradient Methods for Huge-Scale Problems",
    "info": {
        "author": [
            "Yurii Nesterov, Universit\u00e9 catholique de Louvain"
        ],
        "published": "Aug. 26, 2013",
        "recorded": "July 2013",
        "category": [
            "Top->Computer Science->Optimization Methods",
            "Top->Computer Science->Machine Learning->Kernel Methods->Support Vector Machines",
            "Top->Computer Science->Compressed Sensing",
            "Top->Computer Science->Machine Learning->Regularization"
        ]
    },
    "url": "http://videolectures.net/roks2013_nesterov_methods/",
    "segmentation": [
        [
            "I will speak about the special methods for solving the problem of huge scale optimization problems, so we."
        ],
        [
            "Start just from describing the structure of such problems and after that we look at some special subgradient schemes which are able to solve the problems of this size.",
            "So we start with some usual subgradient methods and in the end we will speak about methods for solving optimization problems.",
            "The problems which were discussed by Stevie Assist you in Boot yesterday yesterday so."
        ],
        [
            "So before we start, just let us speak a little bit about the sizes of the problems, the classification of the sizes which we have in optimization.",
            "So in this field is when people started from problems of small and medium size.",
            "Mainly so small problems they have dimension of several hundreds maybe.",
            "And then actually so we are absolutely free in our actions.",
            "So all operations, all of the real problems which we would.",
            "I would imagine so all of them so are allowed and we do not need too much memory for these problems, so after so we have medium size problems where so we can do origin not all operations, but some complicated operations like matrix inversion so and they mention of this problem, so which we can solve is of the order of several thousands and there of course the complexity which is allowed per iteration.",
            "So in cube so.",
            "It is possible, but we would like to have actually methods which the smaller cause like an square.",
            "So the amount of memory which we need for medium size problems or is measured in megabytes, usually in the last decades.",
            "People who are working with large scale problems you have already seen this terminology, so in this for these problems actually, so we're able to do matrix vector multiplication, so the complexity.",
            "Over the order of N Square, Yves still possible pure iterations, but it is already difficult and therefore people would like to work with these problems.",
            "So with the methods which have complexity of the order or dimension, and for these problems we can have several millions of variables and therefore the memory which is necessary to solve these problems is of the order of gigabytes.",
            "But again, so all of this is feasible.",
            "But today I would like to discuss the problems of the bigger size, much bigger size which.",
            "Should be called huge scale problems.",
            "Actually, for problems of this size when we have many millions or even billions of web.",
            "Also, even the usual operations with the vector like addition of the vector you already questionable, we should think twice before we decide to add 2 vectors.",
            "Becausw this is indeed takes a lot of computational time and therefore problems of this size, so the iteration costs.",
            "So of the order of dimension is already too much, so we should try to use here somehow.",
            "The methods with the logarithmic cost of iterations, and of course the memory which we need for this method is already quite bit of the order of terabytes.",
            "The applications where we have this huge scale problems are well known.",
            "So basically there are two applications which are U, so they're related to problems.",
            "So Internet or in digital communications, but we always have the problems which have the let me say maximum possible size size which we can solve in the computer and they related to the finite element schemes or partial differential equation and so on.",
            "So in some sense this is the problems which arise from discretization of infinite dimensional problems there of course we will choose since the discretization gives us security of these problems.",
            "There were always shows the maximum possible.",
            "So the size of the problem, which is still still so so in, and of course in order to solve the problems huge scale problems.",
            "So we need to use somehow the sparsity of the problem.",
            "So without sparsity of course this is impossible.",
            "No."
        ],
        [
            "Just look at the structure of the sparse problems.",
            "So very often they're not very complicated.",
            "I mean that the objective function and constraints are quite simple, so simple convex problems where the.",
            "The objective function usually have a special structure, so this is a simple convex function PSI inside of which we have linear.",
            "Array.",
            "X side should be convex, so not necessarily differentiable.",
            "It may be sub differential and the main difficulty here comes from the linear.",
            "Which is a big matrix from RN to RN and this is a sparse matrix.",
            "No, in order to use sparsity, it is convenient to you to use the so-called sparsity coefficients, which can be defined as the number of non zero elements in some vector or matrix in some finite dimensional object X divided by the maximum number of non zero elements in this objects.",
            "For example, for the M by N matrix, for this a sparsity coefficient should be defined like that.",
            "The number of non zero elements divided by M * N. So the maximum number of non zero in this vector.",
            "While this convenient so just because using this sparsity coefficient it is easy to see the acceleration in computational time which we have in view of sparsity.",
            "For example if we speak about matrix vector multiplication.",
            "So the computation of the matrix vector products X needs P of.",
            "AAP reaction so we just multiply only non zero elements when we compute this product and therefore is as compared with the full matrix vector multiplication which is M. So we use the computational time in gamma of a types in this positive coefficients arises always when we speak about the numerical methods for."
        ],
        [
            "Solving sparse optimization problems.",
            "For example, if you look at the simplest gradient methods like this, so we move along into gradient and project.",
            "And if you look at the complexity of this scheme so the projection is they said is simple, needs of the order of dimension of operations.",
            "Now that displays when also is is cheap.",
            "The main complexity.",
            "Again in this simple schemes come from the necessity to compute.",
            "The gradient was subgradient of this function which can be computed in accordance to this formula.",
            "And now if you count how many operations we need in order to compute that, you can easily see that this is P of a operations and this means that as compared with full matrices.",
            "So for sparse matrices the sub gradient gradient methods need gamma of eight times smaller.",
            "I don't know amount of computational time type.",
            "Now for large and huge scale problems, of course, this coefficient gamma is very small, so it is of the order maybe 10 to the minus six, so the acceleration is indeed very very big.",
            "But the question is can we get more can we achieve?",
            "Higher level of acceleration."
        ],
        [
            "Well, let us look actually so in fact, the main complexity in all these huge scale optimization problems comes from the necessity to compute the matrix vector product and usually in the optimization scheme.",
            "So do we apply a kind of?",
            "Iterative procedure, so we have a vector X and after that we generate the new point with some displacement D and we need to compute again the new matrix vector product.",
            "But know that here so we can represent this new matrix vector.",
            "Product is an old vector matrix product plus the operator E multiplied by the displacement.",
            "So the question is, what happens if D is sparse?",
            "Clearly?",
            "So if this part so this, the complexity of this update is much smaller than, But the question is how much?",
            "Let us just compute?",
            "Let's try to estimate.",
            "So what is the complexity of this vector matrix product?",
            "So for that we denote by Sigma of deal the set of non zero elements of the vector deal.",
            "Then of course in this product we multiply only.",
            "Non zero elements deep by the columns of matrix A and then of course the number of operations which you need to compute that is equal to this.",
            "So the summation of all non zero of number of all non zero elements in the column of increase point column of matrix A and it appears that this this quantity can be indeed very small.",
            "Let us understand why.",
            "So let us write it in this form so we multiply it by M and divided by M. So then we have this.",
            "Number of operation equal to M multiplied by the sum of sparsity coefficients of the columns of my active columns of metrics.",
            "Now we divided by divided by Gamma Odeon multiplied by gamma audio gamevideos P of G / M so we don't change anything.",
            "But now let us look at this formula.",
            "So here P of G number of elements.",
            "This is the number of non zero elements in the vector D. OK, so POD is just the number of terms in this sum, so here we have a kind of average coefficient already, so which is every sparsity confusion which is smaller than the maximum sparsity coefficients.",
            "OK, and you see here we already have the product, the product of the sparsity coefficient multiplied by the.",
            "Full size of the matrix M. Therefore, if we have a good situation which means that vector G and the columns of matrix E have uniform sparsity, so their sparsity coefficient is compareable with the sparsity coefficient of the whole matrix, then the amount of computation which we need so is already can be estimated by the squared sparsity confusions.",
            "And this is of course is much better than just the sparsity confusions becausw.",
            "The order for sparsity coefficients which can we can expect, is that enter the minus six.",
            "We square it and we get 10 to the minus 12.",
            "So acceleration is absolutely astromical, so one second in this acceleration correspond to 32,000 years.",
            "So so with this strategy we are able to solve the problem of completely different level of complexity is something which would be just impossible to solve with the usual ticket.",
            "But of course the question is can we use this?",
            "Observe."
        ],
        [
            "It should or not, because here's the full.",
            "So we need special methods methods where we do not have the full vector operations are not allowed at all.",
            "This is this is too much so and it is not clear if it is possible.",
            "And also we need simple problems problems where we can have sparse sparse gradients or sparse subtree.",
            "So is it possible or not?",
            "Let us look.",
            "Let us look first thing to look is of course the quadratic function where we have convex quadratic.",
            "So and we say that the matrix is sparse, so very natural situation, we look at the formula for the gradient and we see that actually, no, there's no hope even if the metrics A is very sparse.",
            "Unfortunately, the gradient of this quadratic function is never sparse.",
            "Have all usually an unknown zero nonzero components, so.",
            "For smooth function this doesn't work, but what happens if we look at non smooth functions, piecewise linear functions?",
            "So maybe the most important function in optimization application optimization, so the maximum of the linear form let us look at its subgradient.",
            "So this is so it is well known just the gradient of the active piece or active linear piece.",
            "Yeah, and then of course this subgradient if sparse is sparse.",
            "If our vectors here are sparse, so the vectors which form the maximum of our function, so here it seems it works.",
            "But know that now we have one more complication because we need to save what we can do with this Max type functions again.",
            "So in the usual computations we always say so that computing maximum is easier.",
            "So which means that we compute the maximum of.",
            "Tim Williams using just MM comparisons, but now again it is not allowed because M is so big that complexity of M for each operation is not possible.",
            "So the question is.",
            "Can we compute or area compute Max type operations in efficient way so faster than the number of terms in this Max functions and."
        ],
        [
            "The answer is positive and actually this is can be done by the special technique which is related to the short computational trees.",
            "So short computational theory is so, so the function our function will say that function F is short three representable, if its computational 3 have binary computation 3, have hate proportion to the logarithm of the damage.",
            "OK, just to see what is it.",
            "So it is better to look at the picture.",
            "So look at this table.",
            "Yeah, so in the low level.",
            "This is the computacional table, so at the lower level in these cells we put just our initial variables after that.",
            "So of the variables of the next level as compared using some functions of as applied to the sales of the previous level.",
            "So we compute this guy after that.",
            "This doesn't matter.",
            "What are the operations, some simple operation and after do continue from this together compute debt, so we go up and up and finally we compute the value of our function.",
            "This is the example of the short computer binary team and so this computational scheme.",
            "Actually it can be applied to some important examples."
        ],
        [
            "Like a symmetric functions of several variables.",
            "If you look for example at the LP norm, just LP norm or log of the sum of exponents.",
            "So with this function of two variables or for us, which is the most important example, is the maximum of two entries, so all of them.",
            "So the computation of this value can be implemented using this short computational tape, so.",
            "This is, uh, this schema is very compact.",
            "I mean the number of auxiliary cells.",
            "We compute how many auxiliary cells do we need so it is proportional to the dimension of our variables, so we don't need too much additional memory.",
            "Now.",
            "Of course, complexity of the computation of the function is not increased the tool, but what is more important, if our new vector X plus differs from X only in one?",
            "And then we can compute.",
            "Then you value of our function only in the logarithmic cost.",
            "So it is absolutely evident from the picture."
        ],
        [
            "For example, so if we already computed once this the value of our function and we change, for example this guy, then the number of operations which we need to update just this, this that and that's it.",
            "So all guys which are directly up.",
            "With respect to our guys, so must be modified and the number of these guys is equal to the number of level, which is just the logarithm of the damage."
        ],
        [
            "So therefore the updating procedure is very cheap and this means that if at each iteration we update just a small number of entries of vector X, so we can have the logarithmic cost for each iteration.",
            "But so this means, but that actually we could have the subgradient minimization schemes with sub linear iterations cost.",
            "But of course so we need."
        ],
        [
            "Do you find this method and actually nonsmooth optimization is quite old field over idea, I don't know.",
            "It started in the beginning of the 60s, and there are many methods which were proposed there, and actually if you look at all of them, so not too many methods indeed can be applied in this technique, becausw.",
            "So before the operations with vectors considered as absolutely trivial one, nobody thought.",
            "To about the complexity of addition of vectors and all things like that.",
            "Therefore in the usual primal dual methods for smooth optimization, we can have some averaging or I don't know.",
            "I Additionally vector operations and all things like that and only the oldest.",
            "All this methods, so which were developed in the beginning of 60s, so they indeed can be used in this framework.",
            "These methods actually wear out of use for 50 years more or less just because they are very.",
            "Inefficient actually.",
            "The new methods have much more efficient.",
            "Much better efficiency, but now so we have completely another situation.",
            "So for our goal now is just to reduce the supply methods which have the reduced cost per iteration.",
            "So let us look at this simple subgradient methods.",
            "So one of them was first of them, both proposed by Buddies Project in 1967 and this is more or less the method where this method where we minimize some nonsmooth functions for which we know the optimal value of.",
            "Object, so just thinks that this is the method for solving the system of.",
            "Non linear inequalities where we know that the optimal value is zero.",
            "OK, So what is it?",
            "So we have the projection of the point XK minus this subgradient and you see clearly if Q is a simple structure is separable.",
            "So at the end subgradient subgradient is simple, subgradient has a small number of nonzero elements, so this is exactly the scheme, the scheme which we need so for.",
            "The complexity of this method is described by this result, which tells us that in order to be at epsilon solution of our problems, we need one or epsilon square iteration.",
            "This is the usual complexity bounds for nonsmooth optimization methods, but here, so the advantage is that we can use this scheme with the sparse updates of the subgradient.",
            "So this is 1 method, the sex."
        ],
        [
            "The method also so is very old, so it is already for general optimization problem, nonsmooth optimization problems.",
            "So we have objective function, and we have convex inequality constraints.",
            "Since we are working in with nonsmooth functions, so we can pack in one nonsmooth inequality constraints, many different inequality constraints, so this is already a serious problem, and then so we can solve it.",
            "Like that, so this method has only one parameter, the step size each and it works.",
            "In the natural way, so at each iteration we check if our point XC is feasible.",
            "So which means that if we check if the value of the constraint at the point XK is small enough, this is exactly the criterion which we need to check.",
            "So if it is not feasible, which means that the value of inequality constraint is big enough, then we update our point using the subgradient of the constraints.",
            "So using some.",
            "Step size effect if it is feasible, this means that this is smaller than that.",
            "Age is again the stepsize parameter which is considered to be small.",
            "Then we update the point using the subgradient of the objective functions.",
            "Very simple scheme and again, so the main interest for us here is that we have here that in both cases the pure subgradient step.",
            "Therefore, if these guys are sparse so then we can use this.",
            "Xbox updating technique and for this method so we can prove the following results.",
            "So we'll look at the value of objective function computed the good iterations feasible iterations, which means FK iteration where we have feasible point.",
            "OK, so we take the best value of our objective function among this point and then we can say so that if the number of iterations is big enough so bigger than that.",
            "Then this set is not empty because first of all we should find somehow the feasible set of points.",
            "So the point which satisfies this inequality, GFX case smaller H multiplied by the norm of the gradient.",
            "So FK is not small and then four.",
            "Then we can guarantee this inequality.",
            "So the residual in the function value is smaller than age multiplied by the Lipschitz constant of the function and the value of inequality constraints also is more than enough from this result.",
            "So it is clear what should be the value of our parameter H, so it should be probably chosen.",
            "Is epsilon the final accuracy of the problem divided by the maximum of these Lipschitz constant?",
            "And actually, in many situations, especially when we speak about huge scale problems where we all will see the structure and can check the structure.",
            "So these constants are available.",
            "For example, if we're speaking huge linear programming problem is just the maximum Euclidean norm of the column or the roles of the metrics A.",
            "So we can compute that in advance, we can choose properly the stepsize parameter and after that we can just run this methods again, so the complexity.",
            "Will be.",
            "In terms of a curious, you're not very good, so it will be proportional to one over epsilon Square, but now so that our main witness advantage is that each iterations iteration is extremely cheap, so we can have millions and billions of these operations in reasonable time, and we can solve the problems of very big size.",
            "So all of that was about.",
            "Under optimization problem, so we have the objective function inequality constraints.",
            "But the question is what we can do with chronic problems.",
            "So you have seen yesterday that actually the conic problems.",
            "They are now very widespread and in this problem.",
            "So what is important?",
            "So we have primal formulation.",
            "We have dual formulation, so and we're able to solve both of them by interior point methods.",
            "But the question is can we do this for the?",
            "Huge scale formulations and can we use their simple methods because there are problems of big size.",
            "Actually interior point methods.",
            "They Fortunately do not work.",
            "So let us."
        ],
        [
            "Look at our possibility and for that, so let's look at the special classes of special classes of chronic problems.",
            "So problems were there, our space of variable E is partitioned on Earth.",
            "Small spaces EG.",
            "OK, in this way, just if we speak about, for example linear programming, then EJ is just one dimension, but there are important examples when this EGR.",
            "Have they have small dimension like for example in problems with the shape design or trans apology design they have dimension of.",
            "I don't know time 3 or 9.",
            "Something like that.",
            "Something that dimension is small so it doesn't depend anyway.",
            "So the number of pieces.",
            "So we have this partition now.",
            "Therefore, so we define the scalar product and actually we partition our linear.",
            "Accordingly to the partition of the variables.",
            "So the product of a by X so can be written in this form.",
            "So we just.",
            "Compute the sum of multiplication of corresponding matrices AJ.",
            "So these guys are matrices.",
            "Multiplied by corresponding vectors XJ which belongs to this space, no accordingly to this partition.",
            "So we have the partition of the comb in the.",
            "In the corner problem, then we can only linear constraint is the feasible cone, and we assume that our feasable cool can be formed as a direct product of the small quotes.",
            "OK, which are closed, convex, pointed.",
            "Think about that like you have nontrivial examples, so in non trivial example, this KJ are the small matrices.",
            "Small symmetric matrices like 3 by three which are positive semi definite.",
            "So this will be important application example.",
            "No, we write down the Yukon for this guy.",
            "So which is again the direct product of the dual cones.",
            "This small dimensional and we can write down the optimization problems, so this kind of communication problems so it has the primal form.",
            "So which we have seen yesterday.",
            "But it also what is important for us.",
            "It has the dual form.",
            "So in the old form we maximize BY OK and we have two types of variables.",
            "The variables, why?",
            "So, which are included in the objective function in the slack variables S?",
            "So which belongs to the dual cone?",
            "So under some.",
            "Usual assumption, so we can guarantee the zero dualogic grab that both problems are solvable.",
            "So everything is OK, but the question is of course how we can solve it if we have the.",
            "Really big dimension.",
            "I should say that actually.",
            "So this primal dual pair of optimal optimization problem.",
            "It is treated usually the interior point methods.",
            "Bartholin schemes and talking like that, but unfortunately now Speaking of about the problems of very big dimensions, so where all these metrics operations is simply impossible with hundreds of millions variables.",
            "Clearly we can address that.",
            "The question is, can we solve this pair of primordial problems using the subgradient technique?",
            "And the the answer is yes, and for that so we need to do something strange at the first glance, because you see.",
            "20 years ago many people will have had a lot of fun in trying to write in your usual optimization problems in the conic form, and now we should do some something."
        ],
        [
            "So we have the chronic formulation and we would like to rewrite the our dual optimization problems in the functional form because it is exactly the functional form for which we can apply this simple subgradient scheme which we have seen at the previous slides.",
            "But now our situation.",
            "Is more delicate because we need to solve 2 problems, not only the primal problems about which we already knows all the dual problem about which we already know something, but also the primal problem.",
            "Primal problem where we have this X and the equality constraints X equal to be so.",
            "Therefore let us look at the dual problem and let us write down the.",
            "Constraints which we have individual problem, they can be written in this form.",
            "Let us write down them in the functional form.",
            "So this is the functional form which we need.",
            "So how we can do that?",
            "So of course.",
            "So the main advantage of this formulation is that these constraints are separable, so OK, so we have inclusions.",
            "So that the linear value of linear.",
            "Belongs to some convex.",
            "Let us write this in the functional form.",
            "So how we can do that?",
            "So in order to do that, so first of all we take in our dual cone some scaling element.",
            "OK, so the good example of for all this construction is the positive ordered.",
            "So let's think about this cage star.",
            "OK, so this complicated notation but not us.",
            "I think that this is just a positive autumn, so in the positive Wharton would take scaling element so and usually it is convenient to take it as a vector of all ones.",
            "So DJ will be the actor of all ones and after that we define this scaling function.",
            "CJ so it is for our code of UJ which at minimum town such that Audi Jim and you J in the cold again.",
            "So if this is if this is the positive autumn this means that our J. Cole DJ minus UJ great or equal to 0 so Tao then.",
            "So this is a simple computation is just the maximum element of the vector UG.",
            "So this is therefore it is called the scaling function.",
            "So if this guy.",
            "Is 0.",
            "This means that the vector UJ is negative, so more or less what we need.",
            "So, but this construction is already general so we can do the same with inequalities and for example for the with the columns of small positive semidefinite matrices then our scaling element DJ should be chosen as the unit metrics and then so this minimal towel will be just the maximal agent value of corresponding matrix UG and so on.",
            "So for other columns with.",
            "Do something different, but the structure is always the same.",
            "While this function is convenient for us.",
            "Actually this convenient becausw so it is easy to do.",
            "I will limit the derivation, but people which you are familiar with this query formulation can easily derive the primal form or of primal representation of this scaling function.",
            "Yeah, which is just so this is a function of Uggs.",
            "The maximum UGA subject to the following constraints XJ belongs to the cool and the scalar product of J with the this scaling element is equal to 1 so exactly it is exactly the same as this again.",
            "So if we speak about the positive autumn, then the constraint in this problem will be the simplex and then of course so the maximization of the linear form over the simplest.",
            "Again, gives us the maximum element of the vector UJ, so this is absolutely equivalent things, but for us, so this representation is important just because because of.",
            "Explicit form from which we can see what are subgradients of this function in the UG.",
            "So we have these Max operations so of linear functions of depends on you.",
            "Therefore the subgradient of this guy are exactly the elements of our feasible sets which which gives us the maximum value of our objective function.",
            "So this is the usual formula and in our setting so we assume so that all of that.",
            "This will dimension and we're able to solve this problem and to pick up any element from the subgradient, so will need this IG G. Of Yugi, the guy which implements this maximum.",
            "So this will be early key element for construction.",
            "The primal feasible solution in our in our setting.",
            "OK and again so for normally near cones, so this have different meanings.",
            "So for example, this is if this is the column of positive semidefinite matrices then.",
            "Exjade ECG of who you will be rank one matrix which is generated by the maximal eigenvector of our metrics.",
            "You and so we assume that this is a simple operation.",
            "So we have small dimensional cone and we're able easily to compute the maximum eigenvector for example of the three by three matrix.",
            "Something like this.",
            "OK, so this computation is easy and this computation is feasible.",
            "So then so we already are closed through this functional form, so the inclusion these, the inclusion, the inequality constraints for our dual problems can be written in this functional form already.",
            "So this is the function of Y.",
            "So we say FJ of Y is less or a code and zero and there is one small detail.",
            "So is the subgradients.",
            "So we source addition.",
            "She ate this guy in.",
            "We differentiate this guy in.",
            "Why so?",
            "This is the gradient of these guys.",
            "So the guy which implements this maximum computed at this point of course and multiply by AJ transport the usual formula, which gives us a subgradient.",
            "Gradient of our functions so and for the gradient method again.",
            "So all these small details are very important.",
            "For for implementation we need bounds.",
            "For the norm of discover gradients, of course.",
            "Here we get a function with bounded subgradients becausw.",
            "XJ belong to the bounded sets, but for us it is important to see what is this bound and there is some.",
            "I don't know formula which can be written in terms of self concordant barrier for this kokuin, but it doesn't matter.",
            "So there is a formula which for particular applications it can be easily computed."
        ],
        [
            "Example Eve Kijiji is just the rage, the positive race so we're in framework of linear programming then.",
            "So this Sigma J is just the Euclidean norm of the column of matrix AJ and if it is KJ is a set of positive definite matrix then again so by some reason and so we get a formula for Sigma J which is this.",
            "And again this is.",
            "Small dimensional simple problem because this is quadratic function in small dimension in small dimension.",
            "So which we maximize subject to the?",
            "On the Euclidean ball, basically so this is the Frobenius norm is equal to 1.",
            "So this is just the maximal eigenvalue of small dimensional metrics, and we assume that this computation is feasible.",
            "So this sigmas are available to the numerical scheme."
        ],
        [
            "Now, so we already can say what is our problem.",
            "So we rewrite our dual problem in the following form.",
            "We do not change the objective.",
            "Our Y is the unconstrained.",
            "Sorry so we should delete here is there is no S anymore.",
            "OK, and we have functional constraints function constraints which are able to compute and to work the to subdivision shade so.",
            "We can compute the gradient of this guy, so again, so this is gradient, so it is computed in the following way.",
            "We take the active index of this Max.",
            "OK, so we this active index of these marks gives us this XJ of while the guy which maximizes executive multiply by this with you on the message.",
            "Simplex primal simplex.",
            "So we multiply it, but the corresponding column and this.",
            "This is our subgradient.",
            "So again, everything here is computable.",
            "And from this is scaling condition.",
            "We know that the norm of this gradient is smaller than one, therefore, so we just apply this scheme which we have seen for solving the problems with inequality constraints.",
            "But now we applied to chronic problem.",
            "What is it?",
            "So if our constraints is feasible, so we are in this situation F. So we apply the subgradients of equally of the objective function.",
            "So in our situation this is just the.",
            "Victor if not so, we update our variables in accordance to the subgradient of the constraints.",
            "So this method is already adjusted.",
            "The trivial specification of the scheme which we have seen for general problems, but now look so we have here very special objective function and also what we need.",
            "So we have a process in the dual variables in the variables Y.",
            "But we need to reconstruct somehow the primal variables.",
            "The problems from which we started.",
            "So where we have X but for this primal variables we already have some objects which will be very useful.",
            "So we have computer able to compute this XGY.",
            "So the guys which belong to the primal cones and this is exactly this points which will be used for reconstruction the primal variables."
        ],
        [
            "How this can be done now in our process so we have the operations of two types, so after big N iterations, so we have some feasible iterations.",
            "OK, so the iterations where we.",
            "The constraints are satisfied and we move in accordance to the gradient of the objective function.",
            "So the vector B&GN.",
            "So the gradients which are infeasible.",
            "Now so for F again, so our constraints are satisfied.",
            "And so in order to understand what we're doing for in Physio for infeasible constraints, let us introduce the following notation e.g of XJ.",
            "So what is it actually?",
            "This is the vector which has all.",
            "Elements equal to 0.",
            "Except a single entry, so this entry is maybe of course.",
            "Small dimensions, but they mention great or equal to 1, but small, so it is important that it has no nothing in common with the big dimension in our space.",
            "So this position we just put this small dimensional vector XG this guy so we put here for example is XJ is 1 dimensional, so only one position of this vector is different from one, but at this point so we put the guy which belongs to infeasible to our opponent.",
            "Cagey.",
            "And then so this horrible formula.",
            "But we will see in a minute what does it mean so we can form the approximate primal dual solution in accordance to this formula.",
            "OK, with that let us look at the second line.",
            "Second line is simple, so we just compute the average of the dual iterates and this is a more or less clear.",
            "What is it?",
            "What is this guy?",
            "This terrible guide?",
            "So let us look at the structure.",
            "There is this color.",
            "So we sum up over infeasible iterations.",
            "The vectors where only one coordinate is different from zero with some factors which we are able to compute and this.",
            "The non zero coding that we put our guide the solution of our primal problem.",
            "With this objective function, when we compute the value value of the constraints, therefore, this construction is feasible for our.",
            "Now why it is interesting to look exactly at this Victor, it is clear from.",
            "So yeah, so this is just.",
            "It tells us that the dual constraints is feasible, but this thing is evident just becausw.",
            "So there form the we have an average of points.",
            "So among the feasible to rayshon.",
            "So let us look at this representation.",
            "So what is the formula that let's look at the formula for our iterate after N iterations?",
            "We had two types of steps when we had feasible steps.",
            "We applied the gradient for the of the objective function, which is just B.",
            "So there is this term in YN plus one which corresponds to defeasible step and when we applied in fish with the set the steps were infeasible.",
            "So we applied the gradients of the violated constraint.",
            "So this is this.",
            "We had the formula and look what what is it actually?",
            "This is a our operator multiplied exactly by this vector.",
            "Which means that if we divide this expression by this guy, we will have here the residual B -- 8 XM.",
            "OK, and now what is why?",
            "Why are our you wearables?",
            "So M for solvable problems.",
            "This dual variables must be bounded.",
            "Normally they are bounded.",
            "So if we divide so this representation by something which potentially can go to Infinity, we get the residual B -- A X.",
            "Which goes to 0.",
            "So the point becomes feasible.",
            "So this is exactly the explanation of this of this terrible formula."
        ],
        [
            "Now we can actually say what did the convergence result.",
            "The convergence isn't exactly what we should expect, so if so, the.",
            "OK, first there is always a relation between the number of feasible steps.",
            "Yeah, so here we."
        ],
        [
            "Need to have that the number of visible steps be increasing and can go to Infinity."
        ],
        [
            "So the number of visible steps such as this inequality, when the number of total number of steps is big.",
            "So this goal to Infinity.",
            "So if N is bigger than that, then so the number of visible step is positive, so it is at least one.",
            "And if it is 1, so we have an upper bound for the duality gap, which is related to our step size each now and again.",
            "So if it is bigger than that then.",
            "We have again so this inequality for the duality gap at the point X bar's bar, and Moreover what is important.",
            "So we see that during this process there is video of power linear system in the primal space for.",
            "Victor XNX Bar N, which is feasible for our call.",
            "It goes to zero and goes to 0.",
            "Proportion into that OK, and know that even if so in the in this method, so our accuracy final accuracy depends on the step size parameter, so this should be small and fixed.",
            "But independently to this fact that the age is fixed, we still get feasible primal solution in the limit.",
            "OK, so our strategy for forming this primal solution is perfect."
        ],
        [
            "No, So what we can have for huge scale problems.",
            "So when we have quick problems of this type, let us look just what happens with linear program.",
            "In the simplest case.",
            "So for linear programming all confusion they can be necessary.",
            "Conditions can be easily computed and let us assume that our data is sparse.",
            "This means that the number of zeros elements in the objective in columns and rows.",
            "And in the right hand sides is small.",
            "OK, so we have this bounds R&Q which are much smaller than the dimension of the space.",
            "Then in our scheme, so we need to do some preliminary work which is compara bulto the number of non zero elements in the matrix and after that at each iteration.",
            "So if we look how many operations we need.",
            "So the most important operations is the operation which is related to updating this Lex.",
            "And it is proportional to that RQ logarithm of 2 / N. OK, so this means that the complexity of these iterations is is dead and the dependence of North is extremely extremely slow, so more or less this complexity of an iteration is independent on."
        ],
        [
            "The damage.",
            "Now let us look her how it works with the computational experiments.",
            "So this is just well, it is a little bit strange.",
            "Computational experiments becausw we look at the same method, so namely the pollex gradient methods.",
            "So in two variants in two.",
            "Oh, implementation of the iteration, the first one is the sparse updates and 2nd is the standard one.",
            "OK, for that we choose the maximum function which is maximum.",
            "In your functions or which hasbeen on zero elements in each function.",
            "So this couple failed.",
            "The number of iterations we need to perform operations to perform this part that they update is proportional to the square.",
            "Then actually the cost of 1 iterations of this gradient method with the sparse update is more or less people less than P ^2 multiplied by the logarithm of the damage.",
            "If we apply this method directly, so we of course do not multiply zeros, but we cannot do.",
            "Better than PPP tightness in, because this is just the complexity of multiplying the sparse matrix by by Victor.",
            "OK now so, but of course if you compare this to guys, do you see that the usual gradient method so have no chances, just cause.",
            "The logo is grows very slowly, so for problems of moderate size it is 10.",
            "For problems of the large scale problems it is 24 huge scale problem.",
            "This is 30 so it is not changing so this term is not changing at all, more or less, so the difference is only three times and the dimension is changing in 1 million.",
            "And of course if you look what happens with time per iteration.",
            "So here we compare the 10,000 durations.",
            "Of the sparse method in the gradient method, you see that what happens with the computational time.",
            "This guy still have a couple of seconds for all of that, and here we grow from 3 seconds to 400.",
            "So if you continue that up to dimension of millions, or we end up with that.",
            "OK, so the number of iterations 1000 duration for another problem, so the sparse update needs less than one second in here this is already.",
            "OK for 40 minutes, so one second of this computation corresponds to 100 million.",
            "So this is about the cost per iterations and about the."
        ],
        [
            "Urgency actually, so everything remains more or less as it should be.",
            "So here we see the table about the solution of piecewise linear.",
            "Problem by this method.",
            "So this is the progress and the objective function.",
            "This is the number of iterations and this is the computational time.",
            "Of course.",
            "Here we have the number of iteration is extremely big, so hundreds of million iterations.",
            "But know that now so that each iterations is very cheap, so we had millions.",
            "You can have millions and billions of iterations here, and we don't care at all, just cause in the total will get very reasonable computational time.",
            "The method is.",
            "If we look at the method, so the method is not good, the method is extremely slow the method.",
            "So what is bad with this method?",
            "Actually it it works completely in accordance to the theoretical worst case theoretical prediction.",
            "So if we compute the parameters of this problem and put how many iterations we need in order to achieve this security, we get 5.3 * 10.",
            "To the 7:00 instead that 1.5 so we accelerate only in three times so, but.",
            "OK, so this means that method is extremely slow and extremely.",
            "Weird, but since each iterations is very cheap so we gain.",
            "And this is the explanation actually why the the method of this type were never used in computational practice, because if we would do that using the usual arithmetic, so the same computation will give us one year of computational time, so this is too slow.",
            "But now the situation is changed and we can use this method quite efficient."
        ],
        [
            "So this is thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I will speak about the special methods for solving the problem of huge scale optimization problems, so we.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Start just from describing the structure of such problems and after that we look at some special subgradient schemes which are able to solve the problems of this size.",
                    "label": 0
                },
                {
                    "sent": "So we start with some usual subgradient methods and in the end we will speak about methods for solving optimization problems.",
                    "label": 1
                },
                {
                    "sent": "The problems which were discussed by Stevie Assist you in Boot yesterday yesterday so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So before we start, just let us speak a little bit about the sizes of the problems, the classification of the sizes which we have in optimization.",
                    "label": 0
                },
                {
                    "sent": "So in this field is when people started from problems of small and medium size.",
                    "label": 0
                },
                {
                    "sent": "Mainly so small problems they have dimension of several hundreds maybe.",
                    "label": 0
                },
                {
                    "sent": "And then actually so we are absolutely free in our actions.",
                    "label": 0
                },
                {
                    "sent": "So all operations, all of the real problems which we would.",
                    "label": 0
                },
                {
                    "sent": "I would imagine so all of them so are allowed and we do not need too much memory for these problems, so after so we have medium size problems where so we can do origin not all operations, but some complicated operations like matrix inversion so and they mention of this problem, so which we can solve is of the order of several thousands and there of course the complexity which is allowed per iteration.",
                    "label": 0
                },
                {
                    "sent": "So in cube so.",
                    "label": 0
                },
                {
                    "sent": "It is possible, but we would like to have actually methods which the smaller cause like an square.",
                    "label": 0
                },
                {
                    "sent": "So the amount of memory which we need for medium size problems or is measured in megabytes, usually in the last decades.",
                    "label": 0
                },
                {
                    "sent": "People who are working with large scale problems you have already seen this terminology, so in this for these problems actually, so we're able to do matrix vector multiplication, so the complexity.",
                    "label": 0
                },
                {
                    "sent": "Over the order of N Square, Yves still possible pure iterations, but it is already difficult and therefore people would like to work with these problems.",
                    "label": 0
                },
                {
                    "sent": "So with the methods which have complexity of the order or dimension, and for these problems we can have several millions of variables and therefore the memory which is necessary to solve these problems is of the order of gigabytes.",
                    "label": 0
                },
                {
                    "sent": "But again, so all of this is feasible.",
                    "label": 0
                },
                {
                    "sent": "But today I would like to discuss the problems of the bigger size, much bigger size which.",
                    "label": 0
                },
                {
                    "sent": "Should be called huge scale problems.",
                    "label": 0
                },
                {
                    "sent": "Actually, for problems of this size when we have many millions or even billions of web.",
                    "label": 0
                },
                {
                    "sent": "Also, even the usual operations with the vector like addition of the vector you already questionable, we should think twice before we decide to add 2 vectors.",
                    "label": 0
                },
                {
                    "sent": "Becausw this is indeed takes a lot of computational time and therefore problems of this size, so the iteration costs.",
                    "label": 0
                },
                {
                    "sent": "So of the order of dimension is already too much, so we should try to use here somehow.",
                    "label": 0
                },
                {
                    "sent": "The methods with the logarithmic cost of iterations, and of course the memory which we need for this method is already quite bit of the order of terabytes.",
                    "label": 0
                },
                {
                    "sent": "The applications where we have this huge scale problems are well known.",
                    "label": 0
                },
                {
                    "sent": "So basically there are two applications which are U, so they're related to problems.",
                    "label": 0
                },
                {
                    "sent": "So Internet or in digital communications, but we always have the problems which have the let me say maximum possible size size which we can solve in the computer and they related to the finite element schemes or partial differential equation and so on.",
                    "label": 0
                },
                {
                    "sent": "So in some sense this is the problems which arise from discretization of infinite dimensional problems there of course we will choose since the discretization gives us security of these problems.",
                    "label": 0
                },
                {
                    "sent": "There were always shows the maximum possible.",
                    "label": 0
                },
                {
                    "sent": "So the size of the problem, which is still still so so in, and of course in order to solve the problems huge scale problems.",
                    "label": 0
                },
                {
                    "sent": "So we need to use somehow the sparsity of the problem.",
                    "label": 0
                },
                {
                    "sent": "So without sparsity of course this is impossible.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just look at the structure of the sparse problems.",
                    "label": 1
                },
                {
                    "sent": "So very often they're not very complicated.",
                    "label": 0
                },
                {
                    "sent": "I mean that the objective function and constraints are quite simple, so simple convex problems where the.",
                    "label": 0
                },
                {
                    "sent": "The objective function usually have a special structure, so this is a simple convex function PSI inside of which we have linear.",
                    "label": 1
                },
                {
                    "sent": "Array.",
                    "label": 0
                },
                {
                    "sent": "X side should be convex, so not necessarily differentiable.",
                    "label": 0
                },
                {
                    "sent": "It may be sub differential and the main difficulty here comes from the linear.",
                    "label": 0
                },
                {
                    "sent": "Which is a big matrix from RN to RN and this is a sparse matrix.",
                    "label": 1
                },
                {
                    "sent": "No, in order to use sparsity, it is convenient to you to use the so-called sparsity coefficients, which can be defined as the number of non zero elements in some vector or matrix in some finite dimensional object X divided by the maximum number of non zero elements in this objects.",
                    "label": 0
                },
                {
                    "sent": "For example, for the M by N matrix, for this a sparsity coefficient should be defined like that.",
                    "label": 0
                },
                {
                    "sent": "The number of non zero elements divided by M * N. So the maximum number of non zero in this vector.",
                    "label": 0
                },
                {
                    "sent": "While this convenient so just because using this sparsity coefficient it is easy to see the acceleration in computational time which we have in view of sparsity.",
                    "label": 0
                },
                {
                    "sent": "For example if we speak about matrix vector multiplication.",
                    "label": 0
                },
                {
                    "sent": "So the computation of the matrix vector products X needs P of.",
                    "label": 0
                },
                {
                    "sent": "AAP reaction so we just multiply only non zero elements when we compute this product and therefore is as compared with the full matrix vector multiplication which is M. So we use the computational time in gamma of a types in this positive coefficients arises always when we speak about the numerical methods for.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Solving sparse optimization problems.",
                    "label": 0
                },
                {
                    "sent": "For example, if you look at the simplest gradient methods like this, so we move along into gradient and project.",
                    "label": 0
                },
                {
                    "sent": "And if you look at the complexity of this scheme so the projection is they said is simple, needs of the order of dimension of operations.",
                    "label": 0
                },
                {
                    "sent": "Now that displays when also is is cheap.",
                    "label": 0
                },
                {
                    "sent": "The main complexity.",
                    "label": 0
                },
                {
                    "sent": "Again in this simple schemes come from the necessity to compute.",
                    "label": 0
                },
                {
                    "sent": "The gradient was subgradient of this function which can be computed in accordance to this formula.",
                    "label": 0
                },
                {
                    "sent": "And now if you count how many operations we need in order to compute that, you can easily see that this is P of a operations and this means that as compared with full matrices.",
                    "label": 1
                },
                {
                    "sent": "So for sparse matrices the sub gradient gradient methods need gamma of eight times smaller.",
                    "label": 0
                },
                {
                    "sent": "I don't know amount of computational time type.",
                    "label": 1
                },
                {
                    "sent": "Now for large and huge scale problems, of course, this coefficient gamma is very small, so it is of the order maybe 10 to the minus six, so the acceleration is indeed very very big.",
                    "label": 1
                },
                {
                    "sent": "But the question is can we get more can we achieve?",
                    "label": 0
                },
                {
                    "sent": "Higher level of acceleration.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, let us look actually so in fact, the main complexity in all these huge scale optimization problems comes from the necessity to compute the matrix vector product and usually in the optimization scheme.",
                    "label": 0
                },
                {
                    "sent": "So do we apply a kind of?",
                    "label": 0
                },
                {
                    "sent": "Iterative procedure, so we have a vector X and after that we generate the new point with some displacement D and we need to compute again the new matrix vector product.",
                    "label": 0
                },
                {
                    "sent": "But know that here so we can represent this new matrix vector.",
                    "label": 0
                },
                {
                    "sent": "Product is an old vector matrix product plus the operator E multiplied by the displacement.",
                    "label": 0
                },
                {
                    "sent": "So the question is, what happens if D is sparse?",
                    "label": 1
                },
                {
                    "sent": "Clearly?",
                    "label": 0
                },
                {
                    "sent": "So if this part so this, the complexity of this update is much smaller than, But the question is how much?",
                    "label": 0
                },
                {
                    "sent": "Let us just compute?",
                    "label": 0
                },
                {
                    "sent": "Let's try to estimate.",
                    "label": 0
                },
                {
                    "sent": "So what is the complexity of this vector matrix product?",
                    "label": 0
                },
                {
                    "sent": "So for that we denote by Sigma of deal the set of non zero elements of the vector deal.",
                    "label": 0
                },
                {
                    "sent": "Then of course in this product we multiply only.",
                    "label": 0
                },
                {
                    "sent": "Non zero elements deep by the columns of matrix A and then of course the number of operations which you need to compute that is equal to this.",
                    "label": 0
                },
                {
                    "sent": "So the summation of all non zero of number of all non zero elements in the column of increase point column of matrix A and it appears that this this quantity can be indeed very small.",
                    "label": 0
                },
                {
                    "sent": "Let us understand why.",
                    "label": 0
                },
                {
                    "sent": "So let us write it in this form so we multiply it by M and divided by M. So then we have this.",
                    "label": 0
                },
                {
                    "sent": "Number of operation equal to M multiplied by the sum of sparsity coefficients of the columns of my active columns of metrics.",
                    "label": 0
                },
                {
                    "sent": "Now we divided by divided by Gamma Odeon multiplied by gamma audio gamevideos P of G / M so we don't change anything.",
                    "label": 0
                },
                {
                    "sent": "But now let us look at this formula.",
                    "label": 0
                },
                {
                    "sent": "So here P of G number of elements.",
                    "label": 0
                },
                {
                    "sent": "This is the number of non zero elements in the vector D. OK, so POD is just the number of terms in this sum, so here we have a kind of average coefficient already, so which is every sparsity confusion which is smaller than the maximum sparsity coefficients.",
                    "label": 0
                },
                {
                    "sent": "OK, and you see here we already have the product, the product of the sparsity coefficient multiplied by the.",
                    "label": 0
                },
                {
                    "sent": "Full size of the matrix M. Therefore, if we have a good situation which means that vector G and the columns of matrix E have uniform sparsity, so their sparsity coefficient is compareable with the sparsity coefficient of the whole matrix, then the amount of computation which we need so is already can be estimated by the squared sparsity confusions.",
                    "label": 0
                },
                {
                    "sent": "And this is of course is much better than just the sparsity confusions becausw.",
                    "label": 0
                },
                {
                    "sent": "The order for sparsity coefficients which can we can expect, is that enter the minus six.",
                    "label": 0
                },
                {
                    "sent": "We square it and we get 10 to the minus 12.",
                    "label": 0
                },
                {
                    "sent": "So acceleration is absolutely astromical, so one second in this acceleration correspond to 32,000 years.",
                    "label": 0
                },
                {
                    "sent": "So so with this strategy we are able to solve the problem of completely different level of complexity is something which would be just impossible to solve with the usual ticket.",
                    "label": 0
                },
                {
                    "sent": "But of course the question is can we use this?",
                    "label": 0
                },
                {
                    "sent": "Observe.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It should or not, because here's the full.",
                    "label": 0
                },
                {
                    "sent": "So we need special methods methods where we do not have the full vector operations are not allowed at all.",
                    "label": 0
                },
                {
                    "sent": "This is this is too much so and it is not clear if it is possible.",
                    "label": 0
                },
                {
                    "sent": "And also we need simple problems problems where we can have sparse sparse gradients or sparse subtree.",
                    "label": 1
                },
                {
                    "sent": "So is it possible or not?",
                    "label": 1
                },
                {
                    "sent": "Let us look.",
                    "label": 0
                },
                {
                    "sent": "Let us look first thing to look is of course the quadratic function where we have convex quadratic.",
                    "label": 0
                },
                {
                    "sent": "So and we say that the matrix is sparse, so very natural situation, we look at the formula for the gradient and we see that actually, no, there's no hope even if the metrics A is very sparse.",
                    "label": 1
                },
                {
                    "sent": "Unfortunately, the gradient of this quadratic function is never sparse.",
                    "label": 0
                },
                {
                    "sent": "Have all usually an unknown zero nonzero components, so.",
                    "label": 0
                },
                {
                    "sent": "For smooth function this doesn't work, but what happens if we look at non smooth functions, piecewise linear functions?",
                    "label": 0
                },
                {
                    "sent": "So maybe the most important function in optimization application optimization, so the maximum of the linear form let us look at its subgradient.",
                    "label": 1
                },
                {
                    "sent": "So this is so it is well known just the gradient of the active piece or active linear piece.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and then of course this subgradient if sparse is sparse.",
                    "label": 0
                },
                {
                    "sent": "If our vectors here are sparse, so the vectors which form the maximum of our function, so here it seems it works.",
                    "label": 0
                },
                {
                    "sent": "But know that now we have one more complication because we need to save what we can do with this Max type functions again.",
                    "label": 0
                },
                {
                    "sent": "So in the usual computations we always say so that computing maximum is easier.",
                    "label": 0
                },
                {
                    "sent": "So which means that we compute the maximum of.",
                    "label": 0
                },
                {
                    "sent": "Tim Williams using just MM comparisons, but now again it is not allowed because M is so big that complexity of M for each operation is not possible.",
                    "label": 0
                },
                {
                    "sent": "So the question is.",
                    "label": 0
                },
                {
                    "sent": "Can we compute or area compute Max type operations in efficient way so faster than the number of terms in this Max functions and.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The answer is positive and actually this is can be done by the special technique which is related to the short computational trees.",
                    "label": 1
                },
                {
                    "sent": "So short computational theory is so, so the function our function will say that function F is short three representable, if its computational 3 have binary computation 3, have hate proportion to the logarithm of the damage.",
                    "label": 0
                },
                {
                    "sent": "OK, just to see what is it.",
                    "label": 0
                },
                {
                    "sent": "So it is better to look at the picture.",
                    "label": 0
                },
                {
                    "sent": "So look at this table.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so in the low level.",
                    "label": 0
                },
                {
                    "sent": "This is the computacional table, so at the lower level in these cells we put just our initial variables after that.",
                    "label": 0
                },
                {
                    "sent": "So of the variables of the next level as compared using some functions of as applied to the sales of the previous level.",
                    "label": 1
                },
                {
                    "sent": "So we compute this guy after that.",
                    "label": 0
                },
                {
                    "sent": "This doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "What are the operations, some simple operation and after do continue from this together compute debt, so we go up and up and finally we compute the value of our function.",
                    "label": 1
                },
                {
                    "sent": "This is the example of the short computer binary team and so this computational scheme.",
                    "label": 0
                },
                {
                    "sent": "Actually it can be applied to some important examples.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Like a symmetric functions of several variables.",
                    "label": 1
                },
                {
                    "sent": "If you look for example at the LP norm, just LP norm or log of the sum of exponents.",
                    "label": 0
                },
                {
                    "sent": "So with this function of two variables or for us, which is the most important example, is the maximum of two entries, so all of them.",
                    "label": 0
                },
                {
                    "sent": "So the computation of this value can be implemented using this short computational tape, so.",
                    "label": 0
                },
                {
                    "sent": "This is, uh, this schema is very compact.",
                    "label": 0
                },
                {
                    "sent": "I mean the number of auxiliary cells.",
                    "label": 0
                },
                {
                    "sent": "We compute how many auxiliary cells do we need so it is proportional to the dimension of our variables, so we don't need too much additional memory.",
                    "label": 1
                },
                {
                    "sent": "Now.",
                    "label": 1
                },
                {
                    "sent": "Of course, complexity of the computation of the function is not increased the tool, but what is more important, if our new vector X plus differs from X only in one?",
                    "label": 1
                },
                {
                    "sent": "And then we can compute.",
                    "label": 0
                },
                {
                    "sent": "Then you value of our function only in the logarithmic cost.",
                    "label": 0
                },
                {
                    "sent": "So it is absolutely evident from the picture.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For example, so if we already computed once this the value of our function and we change, for example this guy, then the number of operations which we need to update just this, this that and that's it.",
                    "label": 0
                },
                {
                    "sent": "So all guys which are directly up.",
                    "label": 0
                },
                {
                    "sent": "With respect to our guys, so must be modified and the number of these guys is equal to the number of level, which is just the logarithm of the damage.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So therefore the updating procedure is very cheap and this means that if at each iteration we update just a small number of entries of vector X, so we can have the logarithmic cost for each iteration.",
                    "label": 1
                },
                {
                    "sent": "But so this means, but that actually we could have the subgradient minimization schemes with sub linear iterations cost.",
                    "label": 1
                },
                {
                    "sent": "But of course so we need.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do you find this method and actually nonsmooth optimization is quite old field over idea, I don't know.",
                    "label": 0
                },
                {
                    "sent": "It started in the beginning of the 60s, and there are many methods which were proposed there, and actually if you look at all of them, so not too many methods indeed can be applied in this technique, becausw.",
                    "label": 0
                },
                {
                    "sent": "So before the operations with vectors considered as absolutely trivial one, nobody thought.",
                    "label": 0
                },
                {
                    "sent": "To about the complexity of addition of vectors and all things like that.",
                    "label": 0
                },
                {
                    "sent": "Therefore in the usual primal dual methods for smooth optimization, we can have some averaging or I don't know.",
                    "label": 1
                },
                {
                    "sent": "I Additionally vector operations and all things like that and only the oldest.",
                    "label": 0
                },
                {
                    "sent": "All this methods, so which were developed in the beginning of 60s, so they indeed can be used in this framework.",
                    "label": 0
                },
                {
                    "sent": "These methods actually wear out of use for 50 years more or less just because they are very.",
                    "label": 0
                },
                {
                    "sent": "Inefficient actually.",
                    "label": 0
                },
                {
                    "sent": "The new methods have much more efficient.",
                    "label": 0
                },
                {
                    "sent": "Much better efficiency, but now so we have completely another situation.",
                    "label": 0
                },
                {
                    "sent": "So for our goal now is just to reduce the supply methods which have the reduced cost per iteration.",
                    "label": 0
                },
                {
                    "sent": "So let us look at this simple subgradient methods.",
                    "label": 1
                },
                {
                    "sent": "So one of them was first of them, both proposed by Buddies Project in 1967 and this is more or less the method where this method where we minimize some nonsmooth functions for which we know the optimal value of.",
                    "label": 0
                },
                {
                    "sent": "Object, so just thinks that this is the method for solving the system of.",
                    "label": 1
                },
                {
                    "sent": "Non linear inequalities where we know that the optimal value is zero.",
                    "label": 0
                },
                {
                    "sent": "OK, So what is it?",
                    "label": 0
                },
                {
                    "sent": "So we have the projection of the point XK minus this subgradient and you see clearly if Q is a simple structure is separable.",
                    "label": 1
                },
                {
                    "sent": "So at the end subgradient subgradient is simple, subgradient has a small number of nonzero elements, so this is exactly the scheme, the scheme which we need so for.",
                    "label": 0
                },
                {
                    "sent": "The complexity of this method is described by this result, which tells us that in order to be at epsilon solution of our problems, we need one or epsilon square iteration.",
                    "label": 0
                },
                {
                    "sent": "This is the usual complexity bounds for nonsmooth optimization methods, but here, so the advantage is that we can use this scheme with the sparse updates of the subgradient.",
                    "label": 0
                },
                {
                    "sent": "So this is 1 method, the sex.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The method also so is very old, so it is already for general optimization problem, nonsmooth optimization problems.",
                    "label": 0
                },
                {
                    "sent": "So we have objective function, and we have convex inequality constraints.",
                    "label": 0
                },
                {
                    "sent": "Since we are working in with nonsmooth functions, so we can pack in one nonsmooth inequality constraints, many different inequality constraints, so this is already a serious problem, and then so we can solve it.",
                    "label": 0
                },
                {
                    "sent": "Like that, so this method has only one parameter, the step size each and it works.",
                    "label": 0
                },
                {
                    "sent": "In the natural way, so at each iteration we check if our point XC is feasible.",
                    "label": 0
                },
                {
                    "sent": "So which means that if we check if the value of the constraint at the point XK is small enough, this is exactly the criterion which we need to check.",
                    "label": 0
                },
                {
                    "sent": "So if it is not feasible, which means that the value of inequality constraint is big enough, then we update our point using the subgradient of the constraints.",
                    "label": 0
                },
                {
                    "sent": "So using some.",
                    "label": 0
                },
                {
                    "sent": "Step size effect if it is feasible, this means that this is smaller than that.",
                    "label": 0
                },
                {
                    "sent": "Age is again the stepsize parameter which is considered to be small.",
                    "label": 0
                },
                {
                    "sent": "Then we update the point using the subgradient of the objective functions.",
                    "label": 0
                },
                {
                    "sent": "Very simple scheme and again, so the main interest for us here is that we have here that in both cases the pure subgradient step.",
                    "label": 0
                },
                {
                    "sent": "Therefore, if these guys are sparse so then we can use this.",
                    "label": 0
                },
                {
                    "sent": "Xbox updating technique and for this method so we can prove the following results.",
                    "label": 0
                },
                {
                    "sent": "So we'll look at the value of objective function computed the good iterations feasible iterations, which means FK iteration where we have feasible point.",
                    "label": 0
                },
                {
                    "sent": "OK, so we take the best value of our objective function among this point and then we can say so that if the number of iterations is big enough so bigger than that.",
                    "label": 0
                },
                {
                    "sent": "Then this set is not empty because first of all we should find somehow the feasible set of points.",
                    "label": 0
                },
                {
                    "sent": "So the point which satisfies this inequality, GFX case smaller H multiplied by the norm of the gradient.",
                    "label": 0
                },
                {
                    "sent": "So FK is not small and then four.",
                    "label": 0
                },
                {
                    "sent": "Then we can guarantee this inequality.",
                    "label": 0
                },
                {
                    "sent": "So the residual in the function value is smaller than age multiplied by the Lipschitz constant of the function and the value of inequality constraints also is more than enough from this result.",
                    "label": 0
                },
                {
                    "sent": "So it is clear what should be the value of our parameter H, so it should be probably chosen.",
                    "label": 0
                },
                {
                    "sent": "Is epsilon the final accuracy of the problem divided by the maximum of these Lipschitz constant?",
                    "label": 0
                },
                {
                    "sent": "And actually, in many situations, especially when we speak about huge scale problems where we all will see the structure and can check the structure.",
                    "label": 0
                },
                {
                    "sent": "So these constants are available.",
                    "label": 0
                },
                {
                    "sent": "For example, if we're speaking huge linear programming problem is just the maximum Euclidean norm of the column or the roles of the metrics A.",
                    "label": 0
                },
                {
                    "sent": "So we can compute that in advance, we can choose properly the stepsize parameter and after that we can just run this methods again, so the complexity.",
                    "label": 0
                },
                {
                    "sent": "Will be.",
                    "label": 0
                },
                {
                    "sent": "In terms of a curious, you're not very good, so it will be proportional to one over epsilon Square, but now so that our main witness advantage is that each iterations iteration is extremely cheap, so we can have millions and billions of these operations in reasonable time, and we can solve the problems of very big size.",
                    "label": 0
                },
                {
                    "sent": "So all of that was about.",
                    "label": 0
                },
                {
                    "sent": "Under optimization problem, so we have the objective function inequality constraints.",
                    "label": 0
                },
                {
                    "sent": "But the question is what we can do with chronic problems.",
                    "label": 0
                },
                {
                    "sent": "So you have seen yesterday that actually the conic problems.",
                    "label": 0
                },
                {
                    "sent": "They are now very widespread and in this problem.",
                    "label": 0
                },
                {
                    "sent": "So what is important?",
                    "label": 0
                },
                {
                    "sent": "So we have primal formulation.",
                    "label": 0
                },
                {
                    "sent": "We have dual formulation, so and we're able to solve both of them by interior point methods.",
                    "label": 0
                },
                {
                    "sent": "But the question is can we do this for the?",
                    "label": 0
                },
                {
                    "sent": "Huge scale formulations and can we use their simple methods because there are problems of big size.",
                    "label": 0
                },
                {
                    "sent": "Actually interior point methods.",
                    "label": 0
                },
                {
                    "sent": "They Fortunately do not work.",
                    "label": 0
                },
                {
                    "sent": "So let us.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Look at our possibility and for that, so let's look at the special classes of special classes of chronic problems.",
                    "label": 0
                },
                {
                    "sent": "So problems were there, our space of variable E is partitioned on Earth.",
                    "label": 1
                },
                {
                    "sent": "Small spaces EG.",
                    "label": 0
                },
                {
                    "sent": "OK, in this way, just if we speak about, for example linear programming, then EJ is just one dimension, but there are important examples when this EGR.",
                    "label": 0
                },
                {
                    "sent": "Have they have small dimension like for example in problems with the shape design or trans apology design they have dimension of.",
                    "label": 0
                },
                {
                    "sent": "I don't know time 3 or 9.",
                    "label": 0
                },
                {
                    "sent": "Something like that.",
                    "label": 0
                },
                {
                    "sent": "Something that dimension is small so it doesn't depend anyway.",
                    "label": 0
                },
                {
                    "sent": "So the number of pieces.",
                    "label": 0
                },
                {
                    "sent": "So we have this partition now.",
                    "label": 0
                },
                {
                    "sent": "Therefore, so we define the scalar product and actually we partition our linear.",
                    "label": 0
                },
                {
                    "sent": "Accordingly to the partition of the variables.",
                    "label": 0
                },
                {
                    "sent": "So the product of a by X so can be written in this form.",
                    "label": 0
                },
                {
                    "sent": "So we just.",
                    "label": 0
                },
                {
                    "sent": "Compute the sum of multiplication of corresponding matrices AJ.",
                    "label": 0
                },
                {
                    "sent": "So these guys are matrices.",
                    "label": 0
                },
                {
                    "sent": "Multiplied by corresponding vectors XJ which belongs to this space, no accordingly to this partition.",
                    "label": 0
                },
                {
                    "sent": "So we have the partition of the comb in the.",
                    "label": 0
                },
                {
                    "sent": "In the corner problem, then we can only linear constraint is the feasible cone, and we assume that our feasable cool can be formed as a direct product of the small quotes.",
                    "label": 0
                },
                {
                    "sent": "OK, which are closed, convex, pointed.",
                    "label": 1
                },
                {
                    "sent": "Think about that like you have nontrivial examples, so in non trivial example, this KJ are the small matrices.",
                    "label": 0
                },
                {
                    "sent": "Small symmetric matrices like 3 by three which are positive semi definite.",
                    "label": 0
                },
                {
                    "sent": "So this will be important application example.",
                    "label": 0
                },
                {
                    "sent": "No, we write down the Yukon for this guy.",
                    "label": 0
                },
                {
                    "sent": "So which is again the direct product of the dual cones.",
                    "label": 0
                },
                {
                    "sent": "This small dimensional and we can write down the optimization problems, so this kind of communication problems so it has the primal form.",
                    "label": 0
                },
                {
                    "sent": "So which we have seen yesterday.",
                    "label": 0
                },
                {
                    "sent": "But it also what is important for us.",
                    "label": 0
                },
                {
                    "sent": "It has the dual form.",
                    "label": 0
                },
                {
                    "sent": "So in the old form we maximize BY OK and we have two types of variables.",
                    "label": 0
                },
                {
                    "sent": "The variables, why?",
                    "label": 0
                },
                {
                    "sent": "So, which are included in the objective function in the slack variables S?",
                    "label": 0
                },
                {
                    "sent": "So which belongs to the dual cone?",
                    "label": 0
                },
                {
                    "sent": "So under some.",
                    "label": 0
                },
                {
                    "sent": "Usual assumption, so we can guarantee the zero dualogic grab that both problems are solvable.",
                    "label": 0
                },
                {
                    "sent": "So everything is OK, but the question is of course how we can solve it if we have the.",
                    "label": 0
                },
                {
                    "sent": "Really big dimension.",
                    "label": 0
                },
                {
                    "sent": "I should say that actually.",
                    "label": 0
                },
                {
                    "sent": "So this primal dual pair of optimal optimization problem.",
                    "label": 0
                },
                {
                    "sent": "It is treated usually the interior point methods.",
                    "label": 0
                },
                {
                    "sent": "Bartholin schemes and talking like that, but unfortunately now Speaking of about the problems of very big dimensions, so where all these metrics operations is simply impossible with hundreds of millions variables.",
                    "label": 0
                },
                {
                    "sent": "Clearly we can address that.",
                    "label": 0
                },
                {
                    "sent": "The question is, can we solve this pair of primordial problems using the subgradient technique?",
                    "label": 0
                },
                {
                    "sent": "And the the answer is yes, and for that so we need to do something strange at the first glance, because you see.",
                    "label": 0
                },
                {
                    "sent": "20 years ago many people will have had a lot of fun in trying to write in your usual optimization problems in the conic form, and now we should do some something.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we have the chronic formulation and we would like to rewrite the our dual optimization problems in the functional form because it is exactly the functional form for which we can apply this simple subgradient scheme which we have seen at the previous slides.",
                    "label": 0
                },
                {
                    "sent": "But now our situation.",
                    "label": 0
                },
                {
                    "sent": "Is more delicate because we need to solve 2 problems, not only the primal problems about which we already knows all the dual problem about which we already know something, but also the primal problem.",
                    "label": 1
                },
                {
                    "sent": "Primal problem where we have this X and the equality constraints X equal to be so.",
                    "label": 0
                },
                {
                    "sent": "Therefore let us look at the dual problem and let us write down the.",
                    "label": 0
                },
                {
                    "sent": "Constraints which we have individual problem, they can be written in this form.",
                    "label": 1
                },
                {
                    "sent": "Let us write down them in the functional form.",
                    "label": 0
                },
                {
                    "sent": "So this is the functional form which we need.",
                    "label": 0
                },
                {
                    "sent": "So how we can do that?",
                    "label": 1
                },
                {
                    "sent": "So of course.",
                    "label": 0
                },
                {
                    "sent": "So the main advantage of this formulation is that these constraints are separable, so OK, so we have inclusions.",
                    "label": 0
                },
                {
                    "sent": "So that the linear value of linear.",
                    "label": 0
                },
                {
                    "sent": "Belongs to some convex.",
                    "label": 1
                },
                {
                    "sent": "Let us write this in the functional form.",
                    "label": 0
                },
                {
                    "sent": "So how we can do that?",
                    "label": 0
                },
                {
                    "sent": "So in order to do that, so first of all we take in our dual cone some scaling element.",
                    "label": 0
                },
                {
                    "sent": "OK, so the good example of for all this construction is the positive ordered.",
                    "label": 0
                },
                {
                    "sent": "So let's think about this cage star.",
                    "label": 0
                },
                {
                    "sent": "OK, so this complicated notation but not us.",
                    "label": 0
                },
                {
                    "sent": "I think that this is just a positive autumn, so in the positive Wharton would take scaling element so and usually it is convenient to take it as a vector of all ones.",
                    "label": 0
                },
                {
                    "sent": "So DJ will be the actor of all ones and after that we define this scaling function.",
                    "label": 0
                },
                {
                    "sent": "CJ so it is for our code of UJ which at minimum town such that Audi Jim and you J in the cold again.",
                    "label": 0
                },
                {
                    "sent": "So if this is if this is the positive autumn this means that our J. Cole DJ minus UJ great or equal to 0 so Tao then.",
                    "label": 0
                },
                {
                    "sent": "So this is a simple computation is just the maximum element of the vector UG.",
                    "label": 0
                },
                {
                    "sent": "So this is therefore it is called the scaling function.",
                    "label": 0
                },
                {
                    "sent": "So if this guy.",
                    "label": 0
                },
                {
                    "sent": "Is 0.",
                    "label": 0
                },
                {
                    "sent": "This means that the vector UJ is negative, so more or less what we need.",
                    "label": 0
                },
                {
                    "sent": "So, but this construction is already general so we can do the same with inequalities and for example for the with the columns of small positive semidefinite matrices then our scaling element DJ should be chosen as the unit metrics and then so this minimal towel will be just the maximal agent value of corresponding matrix UG and so on.",
                    "label": 0
                },
                {
                    "sent": "So for other columns with.",
                    "label": 0
                },
                {
                    "sent": "Do something different, but the structure is always the same.",
                    "label": 0
                },
                {
                    "sent": "While this function is convenient for us.",
                    "label": 0
                },
                {
                    "sent": "Actually this convenient becausw so it is easy to do.",
                    "label": 0
                },
                {
                    "sent": "I will limit the derivation, but people which you are familiar with this query formulation can easily derive the primal form or of primal representation of this scaling function.",
                    "label": 0
                },
                {
                    "sent": "Yeah, which is just so this is a function of Uggs.",
                    "label": 0
                },
                {
                    "sent": "The maximum UGA subject to the following constraints XJ belongs to the cool and the scalar product of J with the this scaling element is equal to 1 so exactly it is exactly the same as this again.",
                    "label": 0
                },
                {
                    "sent": "So if we speak about the positive autumn, then the constraint in this problem will be the simplex and then of course so the maximization of the linear form over the simplest.",
                    "label": 0
                },
                {
                    "sent": "Again, gives us the maximum element of the vector UJ, so this is absolutely equivalent things, but for us, so this representation is important just because because of.",
                    "label": 0
                },
                {
                    "sent": "Explicit form from which we can see what are subgradients of this function in the UG.",
                    "label": 0
                },
                {
                    "sent": "So we have these Max operations so of linear functions of depends on you.",
                    "label": 0
                },
                {
                    "sent": "Therefore the subgradient of this guy are exactly the elements of our feasible sets which which gives us the maximum value of our objective function.",
                    "label": 0
                },
                {
                    "sent": "So this is the usual formula and in our setting so we assume so that all of that.",
                    "label": 0
                },
                {
                    "sent": "This will dimension and we're able to solve this problem and to pick up any element from the subgradient, so will need this IG G. Of Yugi, the guy which implements this maximum.",
                    "label": 0
                },
                {
                    "sent": "So this will be early key element for construction.",
                    "label": 0
                },
                {
                    "sent": "The primal feasible solution in our in our setting.",
                    "label": 0
                },
                {
                    "sent": "OK and again so for normally near cones, so this have different meanings.",
                    "label": 0
                },
                {
                    "sent": "So for example, this is if this is the column of positive semidefinite matrices then.",
                    "label": 0
                },
                {
                    "sent": "Exjade ECG of who you will be rank one matrix which is generated by the maximal eigenvector of our metrics.",
                    "label": 0
                },
                {
                    "sent": "You and so we assume that this is a simple operation.",
                    "label": 0
                },
                {
                    "sent": "So we have small dimensional cone and we're able easily to compute the maximum eigenvector for example of the three by three matrix.",
                    "label": 0
                },
                {
                    "sent": "Something like this.",
                    "label": 0
                },
                {
                    "sent": "OK, so this computation is easy and this computation is feasible.",
                    "label": 0
                },
                {
                    "sent": "So then so we already are closed through this functional form, so the inclusion these, the inclusion, the inequality constraints for our dual problems can be written in this functional form already.",
                    "label": 0
                },
                {
                    "sent": "So this is the function of Y.",
                    "label": 0
                },
                {
                    "sent": "So we say FJ of Y is less or a code and zero and there is one small detail.",
                    "label": 0
                },
                {
                    "sent": "So is the subgradients.",
                    "label": 0
                },
                {
                    "sent": "So we source addition.",
                    "label": 0
                },
                {
                    "sent": "She ate this guy in.",
                    "label": 0
                },
                {
                    "sent": "We differentiate this guy in.",
                    "label": 0
                },
                {
                    "sent": "Why so?",
                    "label": 0
                },
                {
                    "sent": "This is the gradient of these guys.",
                    "label": 0
                },
                {
                    "sent": "So the guy which implements this maximum computed at this point of course and multiply by AJ transport the usual formula, which gives us a subgradient.",
                    "label": 0
                },
                {
                    "sent": "Gradient of our functions so and for the gradient method again.",
                    "label": 0
                },
                {
                    "sent": "So all these small details are very important.",
                    "label": 0
                },
                {
                    "sent": "For for implementation we need bounds.",
                    "label": 0
                },
                {
                    "sent": "For the norm of discover gradients, of course.",
                    "label": 0
                },
                {
                    "sent": "Here we get a function with bounded subgradients becausw.",
                    "label": 0
                },
                {
                    "sent": "XJ belong to the bounded sets, but for us it is important to see what is this bound and there is some.",
                    "label": 0
                },
                {
                    "sent": "I don't know formula which can be written in terms of self concordant barrier for this kokuin, but it doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "So there is a formula which for particular applications it can be easily computed.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Example Eve Kijiji is just the rage, the positive race so we're in framework of linear programming then.",
                    "label": 0
                },
                {
                    "sent": "So this Sigma J is just the Euclidean norm of the column of matrix AJ and if it is KJ is a set of positive definite matrix then again so by some reason and so we get a formula for Sigma J which is this.",
                    "label": 0
                },
                {
                    "sent": "And again this is.",
                    "label": 0
                },
                {
                    "sent": "Small dimensional simple problem because this is quadratic function in small dimension in small dimension.",
                    "label": 0
                },
                {
                    "sent": "So which we maximize subject to the?",
                    "label": 0
                },
                {
                    "sent": "On the Euclidean ball, basically so this is the Frobenius norm is equal to 1.",
                    "label": 0
                },
                {
                    "sent": "So this is just the maximal eigenvalue of small dimensional metrics, and we assume that this computation is feasible.",
                    "label": 0
                },
                {
                    "sent": "So this sigmas are available to the numerical scheme.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, so we already can say what is our problem.",
                    "label": 0
                },
                {
                    "sent": "So we rewrite our dual problem in the following form.",
                    "label": 1
                },
                {
                    "sent": "We do not change the objective.",
                    "label": 0
                },
                {
                    "sent": "Our Y is the unconstrained.",
                    "label": 0
                },
                {
                    "sent": "Sorry so we should delete here is there is no S anymore.",
                    "label": 0
                },
                {
                    "sent": "OK, and we have functional constraints function constraints which are able to compute and to work the to subdivision shade so.",
                    "label": 0
                },
                {
                    "sent": "We can compute the gradient of this guy, so again, so this is gradient, so it is computed in the following way.",
                    "label": 0
                },
                {
                    "sent": "We take the active index of this Max.",
                    "label": 1
                },
                {
                    "sent": "OK, so we this active index of these marks gives us this XJ of while the guy which maximizes executive multiply by this with you on the message.",
                    "label": 0
                },
                {
                    "sent": "Simplex primal simplex.",
                    "label": 0
                },
                {
                    "sent": "So we multiply it, but the corresponding column and this.",
                    "label": 0
                },
                {
                    "sent": "This is our subgradient.",
                    "label": 0
                },
                {
                    "sent": "So again, everything here is computable.",
                    "label": 0
                },
                {
                    "sent": "And from this is scaling condition.",
                    "label": 0
                },
                {
                    "sent": "We know that the norm of this gradient is smaller than one, therefore, so we just apply this scheme which we have seen for solving the problems with inequality constraints.",
                    "label": 0
                },
                {
                    "sent": "But now we applied to chronic problem.",
                    "label": 0
                },
                {
                    "sent": "What is it?",
                    "label": 0
                },
                {
                    "sent": "So if our constraints is feasible, so we are in this situation F. So we apply the subgradients of equally of the objective function.",
                    "label": 0
                },
                {
                    "sent": "So in our situation this is just the.",
                    "label": 0
                },
                {
                    "sent": "Victor if not so, we update our variables in accordance to the subgradient of the constraints.",
                    "label": 0
                },
                {
                    "sent": "So this method is already adjusted.",
                    "label": 0
                },
                {
                    "sent": "The trivial specification of the scheme which we have seen for general problems, but now look so we have here very special objective function and also what we need.",
                    "label": 0
                },
                {
                    "sent": "So we have a process in the dual variables in the variables Y.",
                    "label": 0
                },
                {
                    "sent": "But we need to reconstruct somehow the primal variables.",
                    "label": 0
                },
                {
                    "sent": "The problems from which we started.",
                    "label": 0
                },
                {
                    "sent": "So where we have X but for this primal variables we already have some objects which will be very useful.",
                    "label": 0
                },
                {
                    "sent": "So we have computer able to compute this XGY.",
                    "label": 0
                },
                {
                    "sent": "So the guys which belong to the primal cones and this is exactly this points which will be used for reconstruction the primal variables.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How this can be done now in our process so we have the operations of two types, so after big N iterations, so we have some feasible iterations.",
                    "label": 0
                },
                {
                    "sent": "OK, so the iterations where we.",
                    "label": 0
                },
                {
                    "sent": "The constraints are satisfied and we move in accordance to the gradient of the objective function.",
                    "label": 0
                },
                {
                    "sent": "So the vector B&GN.",
                    "label": 0
                },
                {
                    "sent": "So the gradients which are infeasible.",
                    "label": 0
                },
                {
                    "sent": "Now so for F again, so our constraints are satisfied.",
                    "label": 0
                },
                {
                    "sent": "And so in order to understand what we're doing for in Physio for infeasible constraints, let us introduce the following notation e.g of XJ.",
                    "label": 0
                },
                {
                    "sent": "So what is it actually?",
                    "label": 0
                },
                {
                    "sent": "This is the vector which has all.",
                    "label": 0
                },
                {
                    "sent": "Elements equal to 0.",
                    "label": 0
                },
                {
                    "sent": "Except a single entry, so this entry is maybe of course.",
                    "label": 0
                },
                {
                    "sent": "Small dimensions, but they mention great or equal to 1, but small, so it is important that it has no nothing in common with the big dimension in our space.",
                    "label": 0
                },
                {
                    "sent": "So this position we just put this small dimensional vector XG this guy so we put here for example is XJ is 1 dimensional, so only one position of this vector is different from one, but at this point so we put the guy which belongs to infeasible to our opponent.",
                    "label": 0
                },
                {
                    "sent": "Cagey.",
                    "label": 0
                },
                {
                    "sent": "And then so this horrible formula.",
                    "label": 0
                },
                {
                    "sent": "But we will see in a minute what does it mean so we can form the approximate primal dual solution in accordance to this formula.",
                    "label": 0
                },
                {
                    "sent": "OK, with that let us look at the second line.",
                    "label": 0
                },
                {
                    "sent": "Second line is simple, so we just compute the average of the dual iterates and this is a more or less clear.",
                    "label": 0
                },
                {
                    "sent": "What is it?",
                    "label": 0
                },
                {
                    "sent": "What is this guy?",
                    "label": 0
                },
                {
                    "sent": "This terrible guide?",
                    "label": 0
                },
                {
                    "sent": "So let us look at the structure.",
                    "label": 0
                },
                {
                    "sent": "There is this color.",
                    "label": 0
                },
                {
                    "sent": "So we sum up over infeasible iterations.",
                    "label": 0
                },
                {
                    "sent": "The vectors where only one coordinate is different from zero with some factors which we are able to compute and this.",
                    "label": 0
                },
                {
                    "sent": "The non zero coding that we put our guide the solution of our primal problem.",
                    "label": 0
                },
                {
                    "sent": "With this objective function, when we compute the value value of the constraints, therefore, this construction is feasible for our.",
                    "label": 0
                },
                {
                    "sent": "Now why it is interesting to look exactly at this Victor, it is clear from.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so this is just.",
                    "label": 0
                },
                {
                    "sent": "It tells us that the dual constraints is feasible, but this thing is evident just becausw.",
                    "label": 0
                },
                {
                    "sent": "So there form the we have an average of points.",
                    "label": 0
                },
                {
                    "sent": "So among the feasible to rayshon.",
                    "label": 0
                },
                {
                    "sent": "So let us look at this representation.",
                    "label": 0
                },
                {
                    "sent": "So what is the formula that let's look at the formula for our iterate after N iterations?",
                    "label": 0
                },
                {
                    "sent": "We had two types of steps when we had feasible steps.",
                    "label": 0
                },
                {
                    "sent": "We applied the gradient for the of the objective function, which is just B.",
                    "label": 0
                },
                {
                    "sent": "So there is this term in YN plus one which corresponds to defeasible step and when we applied in fish with the set the steps were infeasible.",
                    "label": 0
                },
                {
                    "sent": "So we applied the gradients of the violated constraint.",
                    "label": 0
                },
                {
                    "sent": "So this is this.",
                    "label": 0
                },
                {
                    "sent": "We had the formula and look what what is it actually?",
                    "label": 0
                },
                {
                    "sent": "This is a our operator multiplied exactly by this vector.",
                    "label": 0
                },
                {
                    "sent": "Which means that if we divide this expression by this guy, we will have here the residual B -- 8 XM.",
                    "label": 0
                },
                {
                    "sent": "OK, and now what is why?",
                    "label": 0
                },
                {
                    "sent": "Why are our you wearables?",
                    "label": 0
                },
                {
                    "sent": "So M for solvable problems.",
                    "label": 0
                },
                {
                    "sent": "This dual variables must be bounded.",
                    "label": 0
                },
                {
                    "sent": "Normally they are bounded.",
                    "label": 0
                },
                {
                    "sent": "So if we divide so this representation by something which potentially can go to Infinity, we get the residual B -- A X.",
                    "label": 0
                },
                {
                    "sent": "Which goes to 0.",
                    "label": 0
                },
                {
                    "sent": "So the point becomes feasible.",
                    "label": 0
                },
                {
                    "sent": "So this is exactly the explanation of this of this terrible formula.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we can actually say what did the convergence result.",
                    "label": 0
                },
                {
                    "sent": "The convergence isn't exactly what we should expect, so if so, the.",
                    "label": 0
                },
                {
                    "sent": "OK, first there is always a relation between the number of feasible steps.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so here we.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Need to have that the number of visible steps be increasing and can go to Infinity.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the number of visible steps such as this inequality, when the number of total number of steps is big.",
                    "label": 0
                },
                {
                    "sent": "So this goal to Infinity.",
                    "label": 0
                },
                {
                    "sent": "So if N is bigger than that, then so the number of visible step is positive, so it is at least one.",
                    "label": 0
                },
                {
                    "sent": "And if it is 1, so we have an upper bound for the duality gap, which is related to our step size each now and again.",
                    "label": 0
                },
                {
                    "sent": "So if it is bigger than that then.",
                    "label": 0
                },
                {
                    "sent": "We have again so this inequality for the duality gap at the point X bar's bar, and Moreover what is important.",
                    "label": 0
                },
                {
                    "sent": "So we see that during this process there is video of power linear system in the primal space for.",
                    "label": 0
                },
                {
                    "sent": "Victor XNX Bar N, which is feasible for our call.",
                    "label": 0
                },
                {
                    "sent": "It goes to zero and goes to 0.",
                    "label": 0
                },
                {
                    "sent": "Proportion into that OK, and know that even if so in the in this method, so our accuracy final accuracy depends on the step size parameter, so this should be small and fixed.",
                    "label": 0
                },
                {
                    "sent": "But independently to this fact that the age is fixed, we still get feasible primal solution in the limit.",
                    "label": 0
                },
                {
                    "sent": "OK, so our strategy for forming this primal solution is perfect.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No, So what we can have for huge scale problems.",
                    "label": 0
                },
                {
                    "sent": "So when we have quick problems of this type, let us look just what happens with linear program.",
                    "label": 0
                },
                {
                    "sent": "In the simplest case.",
                    "label": 0
                },
                {
                    "sent": "So for linear programming all confusion they can be necessary.",
                    "label": 0
                },
                {
                    "sent": "Conditions can be easily computed and let us assume that our data is sparse.",
                    "label": 0
                },
                {
                    "sent": "This means that the number of zeros elements in the objective in columns and rows.",
                    "label": 0
                },
                {
                    "sent": "And in the right hand sides is small.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have this bounds R&Q which are much smaller than the dimension of the space.",
                    "label": 0
                },
                {
                    "sent": "Then in our scheme, so we need to do some preliminary work which is compara bulto the number of non zero elements in the matrix and after that at each iteration.",
                    "label": 0
                },
                {
                    "sent": "So if we look how many operations we need.",
                    "label": 0
                },
                {
                    "sent": "So the most important operations is the operation which is related to updating this Lex.",
                    "label": 0
                },
                {
                    "sent": "And it is proportional to that RQ logarithm of 2 / N. OK, so this means that the complexity of these iterations is is dead and the dependence of North is extremely extremely slow, so more or less this complexity of an iteration is independent on.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The damage.",
                    "label": 0
                },
                {
                    "sent": "Now let us look her how it works with the computational experiments.",
                    "label": 1
                },
                {
                    "sent": "So this is just well, it is a little bit strange.",
                    "label": 0
                },
                {
                    "sent": "Computational experiments becausw we look at the same method, so namely the pollex gradient methods.",
                    "label": 0
                },
                {
                    "sent": "So in two variants in two.",
                    "label": 1
                },
                {
                    "sent": "Oh, implementation of the iteration, the first one is the sparse updates and 2nd is the standard one.",
                    "label": 0
                },
                {
                    "sent": "OK, for that we choose the maximum function which is maximum.",
                    "label": 0
                },
                {
                    "sent": "In your functions or which hasbeen on zero elements in each function.",
                    "label": 0
                },
                {
                    "sent": "So this couple failed.",
                    "label": 0
                },
                {
                    "sent": "The number of iterations we need to perform operations to perform this part that they update is proportional to the square.",
                    "label": 0
                },
                {
                    "sent": "Then actually the cost of 1 iterations of this gradient method with the sparse update is more or less people less than P ^2 multiplied by the logarithm of the damage.",
                    "label": 0
                },
                {
                    "sent": "If we apply this method directly, so we of course do not multiply zeros, but we cannot do.",
                    "label": 0
                },
                {
                    "sent": "Better than PPP tightness in, because this is just the complexity of multiplying the sparse matrix by by Victor.",
                    "label": 0
                },
                {
                    "sent": "OK now so, but of course if you compare this to guys, do you see that the usual gradient method so have no chances, just cause.",
                    "label": 0
                },
                {
                    "sent": "The logo is grows very slowly, so for problems of moderate size it is 10.",
                    "label": 0
                },
                {
                    "sent": "For problems of the large scale problems it is 24 huge scale problem.",
                    "label": 0
                },
                {
                    "sent": "This is 30 so it is not changing so this term is not changing at all, more or less, so the difference is only three times and the dimension is changing in 1 million.",
                    "label": 1
                },
                {
                    "sent": "And of course if you look what happens with time per iteration.",
                    "label": 0
                },
                {
                    "sent": "So here we compare the 10,000 durations.",
                    "label": 0
                },
                {
                    "sent": "Of the sparse method in the gradient method, you see that what happens with the computational time.",
                    "label": 1
                },
                {
                    "sent": "This guy still have a couple of seconds for all of that, and here we grow from 3 seconds to 400.",
                    "label": 0
                },
                {
                    "sent": "So if you continue that up to dimension of millions, or we end up with that.",
                    "label": 0
                },
                {
                    "sent": "OK, so the number of iterations 1000 duration for another problem, so the sparse update needs less than one second in here this is already.",
                    "label": 0
                },
                {
                    "sent": "OK for 40 minutes, so one second of this computation corresponds to 100 million.",
                    "label": 0
                },
                {
                    "sent": "So this is about the cost per iterations and about the.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Urgency actually, so everything remains more or less as it should be.",
                    "label": 0
                },
                {
                    "sent": "So here we see the table about the solution of piecewise linear.",
                    "label": 0
                },
                {
                    "sent": "Problem by this method.",
                    "label": 0
                },
                {
                    "sent": "So this is the progress and the objective function.",
                    "label": 0
                },
                {
                    "sent": "This is the number of iterations and this is the computational time.",
                    "label": 0
                },
                {
                    "sent": "Of course.",
                    "label": 0
                },
                {
                    "sent": "Here we have the number of iteration is extremely big, so hundreds of million iterations.",
                    "label": 0
                },
                {
                    "sent": "But know that now so that each iterations is very cheap, so we had millions.",
                    "label": 0
                },
                {
                    "sent": "You can have millions and billions of iterations here, and we don't care at all, just cause in the total will get very reasonable computational time.",
                    "label": 0
                },
                {
                    "sent": "The method is.",
                    "label": 0
                },
                {
                    "sent": "If we look at the method, so the method is not good, the method is extremely slow the method.",
                    "label": 0
                },
                {
                    "sent": "So what is bad with this method?",
                    "label": 0
                },
                {
                    "sent": "Actually it it works completely in accordance to the theoretical worst case theoretical prediction.",
                    "label": 0
                },
                {
                    "sent": "So if we compute the parameters of this problem and put how many iterations we need in order to achieve this security, we get 5.3 * 10.",
                    "label": 0
                },
                {
                    "sent": "To the 7:00 instead that 1.5 so we accelerate only in three times so, but.",
                    "label": 0
                },
                {
                    "sent": "OK, so this means that method is extremely slow and extremely.",
                    "label": 0
                },
                {
                    "sent": "Weird, but since each iterations is very cheap so we gain.",
                    "label": 0
                },
                {
                    "sent": "And this is the explanation actually why the the method of this type were never used in computational practice, because if we would do that using the usual arithmetic, so the same computation will give us one year of computational time, so this is too slow.",
                    "label": 0
                },
                {
                    "sent": "But now the situation is changed and we can use this method quite efficient.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is thank you.",
                    "label": 0
                }
            ]
        }
    }
}