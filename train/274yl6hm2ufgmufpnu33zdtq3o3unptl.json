{
    "id": "274yl6hm2ufgmufpnu33zdtq3o3unptl",
    "title": "Sempala: Interactive SPARQL Query Processing on Hadoop",
    "info": {
        "author": [
            "Alexander Sch\u00e4tzle, University of Freiburg"
        ],
        "published": "Dec. 19, 2014",
        "recorded": "October 2014",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2014_schaetzle_sempala/",
    "segmentation": [
        [
            "So welcome everybody to my talk about some polo with the joint work of me and my Co so smart in zip lock into the NOI Gloucester from the University of Freiburg."
        ],
        [
            "And at the beginning I will spend a little bit more time than usual and motivation, because in from my perspective I think this is one of the most important things from these talk that you should remember becausw the technical stuff and later on you can always read up in the paper, asked me afterwards and in offline discussions.",
            "For me the most important thing would be this.",
            "If you come up out of this talk that you remember why we do what we do in this in our work here.",
            "So the first thing we haven't discussed so much because, OK, so many people arrived in real world applications and we see.",
            "That data set so continues growing.",
            "This is why we're sitting here in the in this session about large scale at F processing.",
            "And of course if you think of web scale data size sets, single machine solutions are not really feasible to store the data in curd in so of course one solution is to design A specialized cluster infrastructure.",
            "This is something like virtuoso for example, therefore stored us to build up cluster systems to handle these large data sizes.",
            "Our idea is a little different from that our approach.",
            "Our approach is if we look at current big data applications besides ascendente web domain, we see that the Hadoop ecosystem has become more or less defecto standard for big data applications and our idea was also from previous work that we have done.",
            "Why can't we use Big Hadoop for semantic web purposes also and then built implementations on top of Hadoop and we do this for two main reasons or we?",
            "Promoted for two main reasons.",
            "The first is if you think of web scale data sizes you always need solutions that scale out means you don't want to have even more powerful powerful machines.",
            "You just want to add more machines to your cluster seamlessly.",
            "If you want to have more data to be processed, just simply add some more notes to your cluster and it should more or less scale linearly and the 2nd is if we look at industry then we see that there's a lot of existing infrastructure.",
            "Build something around that you can also reduce this from the.",
            "Ecosystem will all of projects and nowadays build when I do so we believe in some situations that we can have superior cost benefit ratio compared to setting up specialized infrastructures merely for processing RDF data.",
            "So if we currently look at situation what is on on spark new pairs of existing approaches for that cause about Hadoop initially came from an implementation basically from map reduce."
        ],
        [
            "Uh of Google.",
            "So many existing solutions, including our previous work and more less central derau.",
            "More less.",
            "Central map reduce will see it in the previous talk.",
            "These solutions normally scale well.",
            "It only depends on class size.",
            "Norman notes here if you can rent your clusters on Amazon, scale him to arbitrary sizes.",
            "But the problem is map reduce is a batch processing framework and as it is inherently slow, this is a good system for it's it's good for unselective tasks.",
            "If you have a big input size you generate large output size.",
            "These things worked relatively fine, but we were also in our previous work, always unhappy somehow with the situation that if we think of sparkle queries, this is not the typical situation for sparkle, right?",
            "Normally sparkle queries we will look at them.",
            "They have some kind of an exploitive nature, talk stars or means, even if you have a large input data size.",
            "The output size is normally rather small.",
            "You only requesting give me all the information you have about a specific entity.",
            "Combine it with some specific other entity and you're normally not really satisfied with having minutes like we've seen improving.",
            "Or hours to run these kind of queries on MapReduce clusters for this kind of curious, we want to have response at least in the order of seconds, so milliseconds is hard to achieve or learn Hadoop platform because it's not meant to be for low latency applications, but at least being kind of seconds."
        ],
        [
            "And so.",
            "Then if we currently look at what's Impalas or what we've done in our work here, some from to be one phrase, it's in Sparkling Hadoop query engine.",
            "So is it specially designed for these kind of our talks?",
            "All selective queries.",
            "So it means not simple queries, but queries that retrieve only a limited number of resource compared to the input where you can violate it fast.",
            "Prune your large input data set it's built on top of Impala which is an MPP SQL query engine.",
            "Currently it's a relatively new project on Hadoop and.",
            "The idea of Hadoop is to store all the data in a central data store.",
            "Every data should be stored in HDF, so that we don't have to move data between application A&B.",
            "But we still don't store them as plain RDF data.",
            "Work for comparable performance reasons, we use a column storage for pack.",
            "Here I will come to this next slide.",
            "So how do we represent our F data in and on each DFS?"
        ],
        [
            "We use what we call the Unified property Table.",
            "If you use no existing work about property tables normally you have some kind of clustering algorithm or sometimes to identify which property should be stored together in a in a table, because there's some tradeoff between sparsity of the devil and query complexity, sing it with back here as it is a column oriented format and it's especially optimized for white tables where you only select a few columns on a request.",
            "We decided to put all the properties in one single.",
            "Table to represent them, which may."
        ],
        [
            "It's much easier to formulate the queries and we don't have.",
            "We don't need any kind of clustering algorithm for that and also the advantages we can more or less answer every kind of star pattern shape queries without the need for joints, because normally joins are the most expensive operations and the less number of joins you have, the more best performance you normally get.",
            "The problem, of course is you get sparse table with a distinct, But the good news for Pocky is Bucky is an implementation of our.",
            "Inspired by Google Protocol buffers.",
            "So if you want to read about their so null values in Paki nearly cost no storage overhead, they are represented by definition levels, so storing large amount of null values and table is not a big problem for Pookie.",
            "The next problem is that if we look at this multivalued attributes, for example in this graph articled one having two offers, we delivered the problem.",
            "Arcade does support nested data, so we could store this for example as lists or Maps in our corresponding.",
            "Column, but unfortunately empowering it.",
            "Current version does not support nested structures.",
            "It needs only support flat tractors.",
            "What we do is we implement and application strategy where moralist applicate the entire row using just changing the value of the multivalued predicate or keeping all the other values fixed.",
            "And this is a rather simple static, but it's a powerful strategy.",
            "Becausw at first glance it seems to have an large storage overhead if we think of many multivalued attributes, but we can run in some."
        ],
        [
            "In support for Pookie for run length encoding, and this is the perfect scenario in this case, because all these supplicate entries that we generate will be in consecutive rows.",
            "So directly one after another.",
            "So what happens in in fact is that all these values are not stored several times, but instead only once, together with the number of occurrences, and we combine this with compression in the data and also with dictionary encoding, so that we have seen in our elevations data size that we use order that we generate.",
            "Using pack is actually smaller than the idea of input.",
            "Data size we don't generate a storage problem in this case."
        ],
        [
            "If we come to the query compiler components Impala, the overall procedure is more less security cost compilations or we start with in Sparkle 1.0 query.",
            "The first thing I will do is generate the corresponding algorithms question for these sparkly Curie and we also perform some optimization basic.",
            "I'll be back.",
            "Optimizations on this level.",
            "We could do much more, but algebraic optimization of Spark freezes own research topic on its own, but we have seen that this is a worse full thing to do even for cluster.",
            "Environments like Hadoop you profit quite a lot from these kind of algebraic optimizations.",
            "And finally we more or less produce Impala asset is in SQL query engine based on Hadoop so we more or less generate SQL code from that I would go a little bit into detail of how we do this exactly."
        ],
        [
            "Equal basic ref patterns.",
            "I don't have much time for these things, but the basic reference.",
            "The most important part because this is where we really access the data.",
            "So the idea is we did decompose a basic ref pattern in what we called Oakland disjoint triple groups at triple groups in our respective is just every triple Python.",
            "It has the same subject.",
            "It doesn't depend of matter where there is a constant value or variable.",
            "In this example here we do have two triple groups, one for the variable S."
        ],
        [
            "14 variable see the idea behind this is that we can answer these triple groups without our table without having the need for joints.",
            "So we generate the subqueries for every of these kind of people groups and we don't need joins for that.",
            "And of course we then have.",
            "Finally we have to join the results of."
        ],
        [
            "Individual groups to get the overall result, we call these adjoining group.",
            "So if we look at how this really looks like in."
        ],
        [
            "Impala SQL we have two queries here.",
            "One query for the first triple group than other query for the second one we have no joints here because all the properties are stored together in in the same role an we can more or less select like.",
            "For example if we want to have only the author which is politics we can say OK we only want the author power loaders and we always have to check that the additional properties we want like pages is not null.",
            "This comes from these sparsity of the TBF 'cause if there would be another unit table then it would mean that this property is not defined for this subject so this wouldn't be a valid.",
            "Result."
        ],
        [
            "And finally, then we join this the subqueries to generate the overall result."
        ],
        [
            "So the most interesting part or the interesting part is limited.",
            "Give some insights about evaluation.",
            "We have also used to roll out small as a previous speaker.",
            "Always said foreign to produce in our small cluster, and O'Hara low end configuration we have 10 machines with 32 gig RAM each and two discrete.",
            "The Ram is an important thing in this case because of course Impala just uses highly moreles tries to keep every data in memory during query execution.",
            "And it currently in its current version it doesn't have a phobic strategies, or if it doesn't fit into memory and more or less crashes, but there is an announcement for of this joins in the next version end of the year.",
            "But if you look at typical Hadoop cluster production clusters currently we normally Tbilisi machines having 256 gigabyte of RAM and 12 E person machine each or even more.",
            "We have used any women BS PM benchmarks and we compared our system mainly with other Hadoop based scenarios because this is what we wanted to achieve.",
            "We wanted to have.",
            "Better performance for Hadoop based systems for especially for selective queries where you don't.",
            "I'm not satisfied.",
            "Waiting too long for their query results.",
            "So you want your results in order of seconds for systems is Hive's Impala has the same query languages- us we more or less can say we can use HIVE as an alternative execution engine instead of Impala.",
            "Then of course the query runs as MapReduce because life isn't MapReduce based system in its version that we've used up to now and we compared it with the system pick sparkles similar to the hour.",
            "Previous Speaker introduced where we more or less translates Parker keywords in the pig Latin expression.",
            "An pick is also an undeveloped language on top of Hadoop.",
            "Then we compared also with map Merge which is an merge join implementation of Basic Weapon which size to optimize some stuff?",
            "Who's running the shuffle phase is also my previous speaker that there most of the time is spent on shuffling data between the nodes and Maps.",
            "In is a system which is based on Apache a Spade noise square store where.",
            "And if data is actually stored in the North that knows their scale database so."
        ],
        [
            "I only discuss some real about Lube because Molly the results are phobias, bam, or the same number for some time limitations.",
            "If we look at the overall query performance from the Impala in black bars, we can see that on average we have an order of magnitude performance improvement compared to Hadoop based solutions.",
            "But if more interesting is if we look a bit more into details."
        ],
        [
            "Specially for the highly selective queries, which are always the star shape queries.",
            "Also in LBM benchmark which we were the most interested in because you don't you want don't want these queries to run for solo then we can see that the geometric Impala for this query size of 3000 universities, including inferencing, was something around 700 million.",
            "Triples was around eight 6 seconds which is of course much faster than the competitors where you normally have to wait at least in a minute.",
            "For the query result."
        ],
        [
            "And also interesting thing is.",
            "But even if we look at queries which are have more complicated structures like these queries made, more of them may know them.",
            "They also rather selective tree, so they retrieve only relatively small number of results.",
            "So this is also an in setting which is good for our case, but the query itself is a little bit more complex than the other star shape queries, and also for this kind of queries we haven't visited the geometric mean of simple about something around 20 seconds, whereas the other systems scale much worse than we expect.",
            "Somehow we couldn't really prove that, but we expect that these differences to get.",
            "Even bigger for bigger cluster configurations, if you have bigger clusters with more nodes and having regular dates that the differences is much higher, because we've seen from our experiments that the runtime increases in Simmens Impala is much lower than runtime.",
            "Increasement in these MapReduce based systems.",
            "So because the runtimes always start more or less at 10s at 10 seconds and increased slightly to 20 seconds and sometimes the query runtime is nearly stable from one data set to do next."
        ],
        [
            "So to sum everything up, what have demonstrated here is some Sparkle query engine for Hadoop and we've decided to not implement the directly on Hadoop because one of the advantages that we have also learned from previous work is that if you use something like an underlying platform, something like Impala which is continuously updated even if the the new versions to how do I get upgraded then we always stay compatible because more less.",
            "Our interfaces to SQL combine engine of of Impala so we don't have any kind of problems.",
            "When if change of changes to the Hadoop platform directly means that our system is not working anymore and the system is optimized for these kind of workload that we are interested in an we use a state of the art, storage format.",
            "With Sparky this is not an Impala specific format.",
            "You can also use this for any other kind of project in Hadoop and you can also access this kind of data.",
            "So if interested the data is actually stored in.",
            "In half so."
        ],
        [
            "Sum it up.",
            "Already would be to refine it if data layout at the end.",
            "Because if if we have nested data structures and power for next year and we're currently also looking in features of Sparkle 11 which we are currently missing, like subqueries in aggregations.",
            "So that's it.",
            "Thank you for attention.",
            "May leave some questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So welcome everybody to my talk about some polo with the joint work of me and my Co so smart in zip lock into the NOI Gloucester from the University of Freiburg.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And at the beginning I will spend a little bit more time than usual and motivation, because in from my perspective I think this is one of the most important things from these talk that you should remember becausw the technical stuff and later on you can always read up in the paper, asked me afterwards and in offline discussions.",
                    "label": 0
                },
                {
                    "sent": "For me the most important thing would be this.",
                    "label": 0
                },
                {
                    "sent": "If you come up out of this talk that you remember why we do what we do in this in our work here.",
                    "label": 0
                },
                {
                    "sent": "So the first thing we haven't discussed so much because, OK, so many people arrived in real world applications and we see.",
                    "label": 0
                },
                {
                    "sent": "That data set so continues growing.",
                    "label": 0
                },
                {
                    "sent": "This is why we're sitting here in the in this session about large scale at F processing.",
                    "label": 0
                },
                {
                    "sent": "And of course if you think of web scale data size sets, single machine solutions are not really feasible to store the data in curd in so of course one solution is to design A specialized cluster infrastructure.",
                    "label": 1
                },
                {
                    "sent": "This is something like virtuoso for example, therefore stored us to build up cluster systems to handle these large data sizes.",
                    "label": 0
                },
                {
                    "sent": "Our idea is a little different from that our approach.",
                    "label": 0
                },
                {
                    "sent": "Our approach is if we look at current big data applications besides ascendente web domain, we see that the Hadoop ecosystem has become more or less defecto standard for big data applications and our idea was also from previous work that we have done.",
                    "label": 1
                },
                {
                    "sent": "Why can't we use Big Hadoop for semantic web purposes also and then built implementations on top of Hadoop and we do this for two main reasons or we?",
                    "label": 0
                },
                {
                    "sent": "Promoted for two main reasons.",
                    "label": 0
                },
                {
                    "sent": "The first is if you think of web scale data sizes you always need solutions that scale out means you don't want to have even more powerful powerful machines.",
                    "label": 0
                },
                {
                    "sent": "You just want to add more machines to your cluster seamlessly.",
                    "label": 0
                },
                {
                    "sent": "If you want to have more data to be processed, just simply add some more notes to your cluster and it should more or less scale linearly and the 2nd is if we look at industry then we see that there's a lot of existing infrastructure.",
                    "label": 0
                },
                {
                    "sent": "Build something around that you can also reduce this from the.",
                    "label": 0
                },
                {
                    "sent": "Ecosystem will all of projects and nowadays build when I do so we believe in some situations that we can have superior cost benefit ratio compared to setting up specialized infrastructures merely for processing RDF data.",
                    "label": 0
                },
                {
                    "sent": "So if we currently look at situation what is on on spark new pairs of existing approaches for that cause about Hadoop initially came from an implementation basically from map reduce.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Uh of Google.",
                    "label": 0
                },
                {
                    "sent": "So many existing solutions, including our previous work and more less central derau.",
                    "label": 1
                },
                {
                    "sent": "More less.",
                    "label": 0
                },
                {
                    "sent": "Central map reduce will see it in the previous talk.",
                    "label": 0
                },
                {
                    "sent": "These solutions normally scale well.",
                    "label": 0
                },
                {
                    "sent": "It only depends on class size.",
                    "label": 0
                },
                {
                    "sent": "Norman notes here if you can rent your clusters on Amazon, scale him to arbitrary sizes.",
                    "label": 0
                },
                {
                    "sent": "But the problem is map reduce is a batch processing framework and as it is inherently slow, this is a good system for it's it's good for unselective tasks.",
                    "label": 1
                },
                {
                    "sent": "If you have a big input size you generate large output size.",
                    "label": 0
                },
                {
                    "sent": "These things worked relatively fine, but we were also in our previous work, always unhappy somehow with the situation that if we think of sparkle queries, this is not the typical situation for sparkle, right?",
                    "label": 0
                },
                {
                    "sent": "Normally sparkle queries we will look at them.",
                    "label": 0
                },
                {
                    "sent": "They have some kind of an exploitive nature, talk stars or means, even if you have a large input data size.",
                    "label": 0
                },
                {
                    "sent": "The output size is normally rather small.",
                    "label": 0
                },
                {
                    "sent": "You only requesting give me all the information you have about a specific entity.",
                    "label": 0
                },
                {
                    "sent": "Combine it with some specific other entity and you're normally not really satisfied with having minutes like we've seen improving.",
                    "label": 0
                },
                {
                    "sent": "Or hours to run these kind of queries on MapReduce clusters for this kind of curious, we want to have response at least in the order of seconds, so milliseconds is hard to achieve or learn Hadoop platform because it's not meant to be for low latency applications, but at least being kind of seconds.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "Then if we currently look at what's Impalas or what we've done in our work here, some from to be one phrase, it's in Sparkling Hadoop query engine.",
                    "label": 0
                },
                {
                    "sent": "So is it specially designed for these kind of our talks?",
                    "label": 0
                },
                {
                    "sent": "All selective queries.",
                    "label": 0
                },
                {
                    "sent": "So it means not simple queries, but queries that retrieve only a limited number of resource compared to the input where you can violate it fast.",
                    "label": 0
                },
                {
                    "sent": "Prune your large input data set it's built on top of Impala which is an MPP SQL query engine.",
                    "label": 1
                },
                {
                    "sent": "Currently it's a relatively new project on Hadoop and.",
                    "label": 0
                },
                {
                    "sent": "The idea of Hadoop is to store all the data in a central data store.",
                    "label": 0
                },
                {
                    "sent": "Every data should be stored in HDF, so that we don't have to move data between application A&B.",
                    "label": 0
                },
                {
                    "sent": "But we still don't store them as plain RDF data.",
                    "label": 0
                },
                {
                    "sent": "Work for comparable performance reasons, we use a column storage for pack.",
                    "label": 0
                },
                {
                    "sent": "Here I will come to this next slide.",
                    "label": 0
                },
                {
                    "sent": "So how do we represent our F data in and on each DFS?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We use what we call the Unified property Table.",
                    "label": 1
                },
                {
                    "sent": "If you use no existing work about property tables normally you have some kind of clustering algorithm or sometimes to identify which property should be stored together in a in a table, because there's some tradeoff between sparsity of the devil and query complexity, sing it with back here as it is a column oriented format and it's especially optimized for white tables where you only select a few columns on a request.",
                    "label": 1
                },
                {
                    "sent": "We decided to put all the properties in one single.",
                    "label": 0
                },
                {
                    "sent": "Table to represent them, which may.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's much easier to formulate the queries and we don't have.",
                    "label": 0
                },
                {
                    "sent": "We don't need any kind of clustering algorithm for that and also the advantages we can more or less answer every kind of star pattern shape queries without the need for joints, because normally joins are the most expensive operations and the less number of joins you have, the more best performance you normally get.",
                    "label": 0
                },
                {
                    "sent": "The problem, of course is you get sparse table with a distinct, But the good news for Pocky is Bucky is an implementation of our.",
                    "label": 0
                },
                {
                    "sent": "Inspired by Google Protocol buffers.",
                    "label": 0
                },
                {
                    "sent": "So if you want to read about their so null values in Paki nearly cost no storage overhead, they are represented by definition levels, so storing large amount of null values and table is not a big problem for Pookie.",
                    "label": 1
                },
                {
                    "sent": "The next problem is that if we look at this multivalued attributes, for example in this graph articled one having two offers, we delivered the problem.",
                    "label": 0
                },
                {
                    "sent": "Arcade does support nested data, so we could store this for example as lists or Maps in our corresponding.",
                    "label": 0
                },
                {
                    "sent": "Column, but unfortunately empowering it.",
                    "label": 0
                },
                {
                    "sent": "Current version does not support nested structures.",
                    "label": 0
                },
                {
                    "sent": "It needs only support flat tractors.",
                    "label": 0
                },
                {
                    "sent": "What we do is we implement and application strategy where moralist applicate the entire row using just changing the value of the multivalued predicate or keeping all the other values fixed.",
                    "label": 1
                },
                {
                    "sent": "And this is a rather simple static, but it's a powerful strategy.",
                    "label": 0
                },
                {
                    "sent": "Becausw at first glance it seems to have an large storage overhead if we think of many multivalued attributes, but we can run in some.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In support for Pookie for run length encoding, and this is the perfect scenario in this case, because all these supplicate entries that we generate will be in consecutive rows.",
                    "label": 0
                },
                {
                    "sent": "So directly one after another.",
                    "label": 0
                },
                {
                    "sent": "So what happens in in fact is that all these values are not stored several times, but instead only once, together with the number of occurrences, and we combine this with compression in the data and also with dictionary encoding, so that we have seen in our elevations data size that we use order that we generate.",
                    "label": 0
                },
                {
                    "sent": "Using pack is actually smaller than the idea of input.",
                    "label": 0
                },
                {
                    "sent": "Data size we don't generate a storage problem in this case.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we come to the query compiler components Impala, the overall procedure is more less security cost compilations or we start with in Sparkle 1.0 query.",
                    "label": 0
                },
                {
                    "sent": "The first thing I will do is generate the corresponding algorithms question for these sparkly Curie and we also perform some optimization basic.",
                    "label": 0
                },
                {
                    "sent": "I'll be back.",
                    "label": 0
                },
                {
                    "sent": "Optimizations on this level.",
                    "label": 0
                },
                {
                    "sent": "We could do much more, but algebraic optimization of Spark freezes own research topic on its own, but we have seen that this is a worse full thing to do even for cluster.",
                    "label": 0
                },
                {
                    "sent": "Environments like Hadoop you profit quite a lot from these kind of algebraic optimizations.",
                    "label": 0
                },
                {
                    "sent": "And finally we more or less produce Impala asset is in SQL query engine based on Hadoop so we more or less generate SQL code from that I would go a little bit into detail of how we do this exactly.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Equal basic ref patterns.",
                    "label": 0
                },
                {
                    "sent": "I don't have much time for these things, but the basic reference.",
                    "label": 0
                },
                {
                    "sent": "The most important part because this is where we really access the data.",
                    "label": 0
                },
                {
                    "sent": "So the idea is we did decompose a basic ref pattern in what we called Oakland disjoint triple groups at triple groups in our respective is just every triple Python.",
                    "label": 1
                },
                {
                    "sent": "It has the same subject.",
                    "label": 1
                },
                {
                    "sent": "It doesn't depend of matter where there is a constant value or variable.",
                    "label": 0
                },
                {
                    "sent": "In this example here we do have two triple groups, one for the variable S.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "14 variable see the idea behind this is that we can answer these triple groups without our table without having the need for joints.",
                    "label": 1
                },
                {
                    "sent": "So we generate the subqueries for every of these kind of people groups and we don't need joins for that.",
                    "label": 0
                },
                {
                    "sent": "And of course we then have.",
                    "label": 0
                },
                {
                    "sent": "Finally we have to join the results of.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Individual groups to get the overall result, we call these adjoining group.",
                    "label": 0
                },
                {
                    "sent": "So if we look at how this really looks like in.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Impala SQL we have two queries here.",
                    "label": 0
                },
                {
                    "sent": "One query for the first triple group than other query for the second one we have no joints here because all the properties are stored together in in the same role an we can more or less select like.",
                    "label": 0
                },
                {
                    "sent": "For example if we want to have only the author which is politics we can say OK we only want the author power loaders and we always have to check that the additional properties we want like pages is not null.",
                    "label": 0
                },
                {
                    "sent": "This comes from these sparsity of the TBF 'cause if there would be another unit table then it would mean that this property is not defined for this subject so this wouldn't be a valid.",
                    "label": 0
                },
                {
                    "sent": "Result.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finally, then we join this the subqueries to generate the overall result.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the most interesting part or the interesting part is limited.",
                    "label": 0
                },
                {
                    "sent": "Give some insights about evaluation.",
                    "label": 0
                },
                {
                    "sent": "We have also used to roll out small as a previous speaker.",
                    "label": 0
                },
                {
                    "sent": "Always said foreign to produce in our small cluster, and O'Hara low end configuration we have 10 machines with 32 gig RAM each and two discrete.",
                    "label": 0
                },
                {
                    "sent": "The Ram is an important thing in this case because of course Impala just uses highly moreles tries to keep every data in memory during query execution.",
                    "label": 0
                },
                {
                    "sent": "And it currently in its current version it doesn't have a phobic strategies, or if it doesn't fit into memory and more or less crashes, but there is an announcement for of this joins in the next version end of the year.",
                    "label": 0
                },
                {
                    "sent": "But if you look at typical Hadoop cluster production clusters currently we normally Tbilisi machines having 256 gigabyte of RAM and 12 E person machine each or even more.",
                    "label": 0
                },
                {
                    "sent": "We have used any women BS PM benchmarks and we compared our system mainly with other Hadoop based scenarios because this is what we wanted to achieve.",
                    "label": 0
                },
                {
                    "sent": "We wanted to have.",
                    "label": 0
                },
                {
                    "sent": "Better performance for Hadoop based systems for especially for selective queries where you don't.",
                    "label": 1
                },
                {
                    "sent": "I'm not satisfied.",
                    "label": 0
                },
                {
                    "sent": "Waiting too long for their query results.",
                    "label": 0
                },
                {
                    "sent": "So you want your results in order of seconds for systems is Hive's Impala has the same query languages- us we more or less can say we can use HIVE as an alternative execution engine instead of Impala.",
                    "label": 1
                },
                {
                    "sent": "Then of course the query runs as MapReduce because life isn't MapReduce based system in its version that we've used up to now and we compared it with the system pick sparkles similar to the hour.",
                    "label": 0
                },
                {
                    "sent": "Previous Speaker introduced where we more or less translates Parker keywords in the pig Latin expression.",
                    "label": 1
                },
                {
                    "sent": "An pick is also an undeveloped language on top of Hadoop.",
                    "label": 1
                },
                {
                    "sent": "Then we compared also with map Merge which is an merge join implementation of Basic Weapon which size to optimize some stuff?",
                    "label": 0
                },
                {
                    "sent": "Who's running the shuffle phase is also my previous speaker that there most of the time is spent on shuffling data between the nodes and Maps.",
                    "label": 0
                },
                {
                    "sent": "In is a system which is based on Apache a Spade noise square store where.",
                    "label": 0
                },
                {
                    "sent": "And if data is actually stored in the North that knows their scale database so.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I only discuss some real about Lube because Molly the results are phobias, bam, or the same number for some time limitations.",
                    "label": 0
                },
                {
                    "sent": "If we look at the overall query performance from the Impala in black bars, we can see that on average we have an order of magnitude performance improvement compared to Hadoop based solutions.",
                    "label": 0
                },
                {
                    "sent": "But if more interesting is if we look a bit more into details.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Specially for the highly selective queries, which are always the star shape queries.",
                    "label": 0
                },
                {
                    "sent": "Also in LBM benchmark which we were the most interested in because you don't you want don't want these queries to run for solo then we can see that the geometric Impala for this query size of 3000 universities, including inferencing, was something around 700 million.",
                    "label": 0
                },
                {
                    "sent": "Triples was around eight 6 seconds which is of course much faster than the competitors where you normally have to wait at least in a minute.",
                    "label": 0
                },
                {
                    "sent": "For the query result.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And also interesting thing is.",
                    "label": 0
                },
                {
                    "sent": "But even if we look at queries which are have more complicated structures like these queries made, more of them may know them.",
                    "label": 0
                },
                {
                    "sent": "They also rather selective tree, so they retrieve only relatively small number of results.",
                    "label": 0
                },
                {
                    "sent": "So this is also an in setting which is good for our case, but the query itself is a little bit more complex than the other star shape queries, and also for this kind of queries we haven't visited the geometric mean of simple about something around 20 seconds, whereas the other systems scale much worse than we expect.",
                    "label": 0
                },
                {
                    "sent": "Somehow we couldn't really prove that, but we expect that these differences to get.",
                    "label": 0
                },
                {
                    "sent": "Even bigger for bigger cluster configurations, if you have bigger clusters with more nodes and having regular dates that the differences is much higher, because we've seen from our experiments that the runtime increases in Simmens Impala is much lower than runtime.",
                    "label": 0
                },
                {
                    "sent": "Increasement in these MapReduce based systems.",
                    "label": 0
                },
                {
                    "sent": "So because the runtimes always start more or less at 10s at 10 seconds and increased slightly to 20 seconds and sometimes the query runtime is nearly stable from one data set to do next.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to sum everything up, what have demonstrated here is some Sparkle query engine for Hadoop and we've decided to not implement the directly on Hadoop because one of the advantages that we have also learned from previous work is that if you use something like an underlying platform, something like Impala which is continuously updated even if the the new versions to how do I get upgraded then we always stay compatible because more less.",
                    "label": 1
                },
                {
                    "sent": "Our interfaces to SQL combine engine of of Impala so we don't have any kind of problems.",
                    "label": 0
                },
                {
                    "sent": "When if change of changes to the Hadoop platform directly means that our system is not working anymore and the system is optimized for these kind of workload that we are interested in an we use a state of the art, storage format.",
                    "label": 0
                },
                {
                    "sent": "With Sparky this is not an Impala specific format.",
                    "label": 0
                },
                {
                    "sent": "You can also use this for any other kind of project in Hadoop and you can also access this kind of data.",
                    "label": 0
                },
                {
                    "sent": "So if interested the data is actually stored in.",
                    "label": 0
                },
                {
                    "sent": "In half so.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sum it up.",
                    "label": 0
                },
                {
                    "sent": "Already would be to refine it if data layout at the end.",
                    "label": 0
                },
                {
                    "sent": "Because if if we have nested data structures and power for next year and we're currently also looking in features of Sparkle 11 which we are currently missing, like subqueries in aggregations.",
                    "label": 0
                },
                {
                    "sent": "So that's it.",
                    "label": 0
                },
                {
                    "sent": "Thank you for attention.",
                    "label": 0
                },
                {
                    "sent": "May leave some questions.",
                    "label": 0
                }
            ]
        }
    }
}