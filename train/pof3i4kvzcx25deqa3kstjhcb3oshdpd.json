{
    "id": "pof3i4kvzcx25deqa3kstjhcb3oshdpd",
    "title": "Model-based Bayesian RL",
    "info": {
        "author": [
            "Pascal Poupart, School of Computer Science, University of Waterloo"
        ],
        "published": "June 22, 2007",
        "recorded": "June 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/icml07_poupart_mbbrl/",
    "segmentation": [
        [
            "OK, thanks Mohammed.",
            "So I'm going to continue with.",
            "The history of Asian reinforcement learning."
        ],
        [
            "And actually, the common belief is that while reinforcement learning as we know it in artificial intelligence really started in the 1980s, so with this pioneering work of Sutton Barto and others, and actually Bayesian techniques have not been really common.",
            "It's only really recently that they've surfaced in artificial intelligence, so they can't believe then is that while Bashan reinforcement learning is a new approach.",
            "But actually it turns out that this is not the case."
        ],
        [
            "So if you look back in history.",
            "Well, it is well.",
            "OK, first question is what is RL?",
            "And then Muhammad has defined it and so on.",
            "But another way of looking at it is that it's really the problem of controlling a Markov chain with unknown probabilities and then in the operations research literature then they've already been tackling this problem all the way since the 1950s and 1960s."
        ],
        [
            "OK. And in that literature it was known under the name of adaptive control processes, dual control, optimal learning and others, and some of the people that really para neared these approaches in operations research include Bellman himself.",
            "In fact, he wrote a book in 1961 on adaptive control processes, a guided tour.",
            "And he actually had some earlier papers in the 1950s where he formulated the problem and tackled it with some beige and approaches at about the same time felt bone in Europe.",
            "Also looked at dual control and then at MIT, Ronald Howard and some of these students also considered the problem.",
            "So really we could say that the use of beige and technique really dates back from the 1950s and 1960s.",
            "But in operations research."
        ],
        [
            "So now you may ask.",
            "Well, so if it's already been tackled in operations research, then what is it that we're doing here in AI?",
            "And how does our work related to the work that's been done in operations research?",
            "So it turns out that in operations research, while several decades ago, I mean the computers were not very powerful, algorithmic techniques were not as scalable, and so on, so they really focused on that Article Foundation, but they did also look obviously at algorithmic.",
            "Problems and they did find some solutions for some special cases, and perhaps one of the most well known special case is known as the bandit problem, where you essentially don't have states you just have like 1 state.",
            "That's always the same, and then you would pick actions and then you would like to pick the action that will give you the most reward, but you don't know which action gives you the most rewards, so it's kind of like going to a casino and you think that one slot machine will give you.",
            "More chances of winning and then you simply want to experiment so it turns out that there's a solution known as Gittins indices are named after Gittins, and this doesn't require substantial computation, at least for the very basic problem.",
            "So there are some variants that are being tackled today and actually in this conference or some papers about bandit problems.",
            "The more general version.",
            "But in any case, some of the special cases of beige and reinforcement lion.",
            "Learning like just basic bandit problems have been tackled and there were some solutions that were device OK, but in general yeah there are solutions.",
            "Then we scale so the work that's really been a contribution to reinforcement learning in artificial intelligence for Bayesian techniques has been to try to improve the scalability an also I guess, improve the modeling and so on."
        ],
        [
            "So now you may ask.",
            "OK, so what's the work that's been done in artificial intelligence?",
            "So here I've gathered on this slide the names of several people with the dates of their publications.",
            "This is by no means you know something exhaustive, OK, but just so that you get a sense, there's actually quite a bit of work in model based reinforcement learning as well as model free basean reinforcement learning."
        ],
        [
            "OK, so this was just a bit of history.",
            "Now let me go into the details of model based Bayesian reinforcement learning and then I'll talk more specifically about priors, how to optimize the policy will discuss some of the advantages and disadvantages of using Bayesian techniques, and then I'll serve a few approaches that use Bayesian techniques for some of the variants of reinforcement learning, like inverse reinforcement, learning, multiagent reinforcement learning, and so on."
        ],
        [
            "OK, so as Mohammed explained.",
            "Reinforcement learning is really formulated as a Markov decision process that has the following components, but here I've modified a little bit the definition.",
            "OK, because it will be useful for the next slide.",
            "So what I've done is I've pushed into the set of states the reward.",
            "OK, so I'm going to call states basically a top hole, then includes the physical state component as well as the reward component.",
            "OK, so abstractly I mean these are random variables, right?",
            "And then I could just put them together and it's not a problem.",
            "OK, so I'm just going from non denote by X this top wall and whenever I want to refer to the reward component I'll use a subscript R and otherwise ask for the state part.",
            "OK, we have actions as usual and then the main problem for reinforcement learning is that we don't know what are the transition and reward probabilities.",
            "Which is this distribution P here an with respect to what Muhammad explained before now in Pi also have the distribution over rewards.",
            "Q Because X now refers to both States and rewards OK.",
            "So it's going to simplify things, at least for the Model Bayes approach, because now the way I can see this is that I only have like 1 unknown OK and it's a distribution over states or rewards and the way to tackle this if we're basean is that whenever we've gotten unknown, we Simply put a distribution over it to model this uncertainty, and then we would compute a posterior.",
            "That takes into account the information that we receive.",
            "So here I'm going to use Theta to denote an unknown and so here Theta XAX prime will be this unknown probability of reaching X prime from X and A and so this is a random variable that has the range from zero to 1.",
            "And whenever I want to talk about several variables together, like all of those that they know, probabilities starting from XNA but reaching different possible ex prime, then I'm just going to note that by Theta XA.",
            "So, really together, these correspond to a multinomial distribution."
        ],
        [
            "OK, so.",
            "Now we've encoded with a random variable or unknown.",
            "So what we need to do next is come up with a prior and compute the posterior.",
            "So for the moment, let's just say that we've got some distribution.",
            "I'm going to call it be here, OK, B4 belief, because that will be my belief about what is the underlying model.",
            "Otherwise, the belief of the agent, the learner about the underlying model, and then so this agent wants to update its belief and compute a posterior.",
            "And this posterior will be conditioned on the current state, the action taken, and then the next state that it observes, and then if the agent goes on for several time steps then there would be several of those triples OK, and then we could simply update at each time step what happens now.",
            "Given this, then we apply Bayes theorem an it's as simple as this, right?",
            "So we take the prior multiplied by the likelihood.",
            "Which here, according to the definition I had on my previous slide, is simply Theta XA X prime.",
            "OK, so this is the beauty of vision techniques please.",
            "So far it's really simple conceptually.",
            "OK, now let's get into the more tricky part.",
            "What is a prior and wood?",
            "Does it matter what kind of prior we pick?",
            "Will it influence the computation down the road and it turns out that."
        ],
        [
            "It does.",
            "OK.",
            "So I'm not the part I'm going to talk about the prior."
        ],
        [
            "College.",
            "And OK, so we want to have a prior.",
            "I'm going to propose here to consider a prior that corresponds to a monomial in Theta.",
            "OK, so Theta is a variable and mono meal is simply some expression where we've got Theta with some exponents that are constants.",
            "OK, and here I've got another coefficient K which is a constant.",
            "So let's just say that we've got a prior that looks like this.",
            "I could make up any function.",
            "Of Theatre and declare that this is my prior as long as I can renormalize it when I take the integral by using K OK.",
            "So now the question is, well, what will the posterior look like?",
            "And it turns out that by picking a monomial I can show that the posterior is also a monomial, and it's easy to see because Bayes theorem which is simply to compute the posterior by multiplying the prior with the likelihood.",
            "Here this is my prior times my likelihood, so my likelihood simply increments one of the exponent.",
            "In my monomial, so it's very simple.",
            "And then the beauty of this is that now I have a distribution that will always be of the same family, always in a family of monomials.",
            "And it turns out that these distributions, whenever they remain in the same family.",
            "After abrasion update, they're going to be called conjugate priors, because if you pick the prior like this, Anna posters the same family forgiven likelihood, then their conjugates with."
        ],
        [
            "The likelihood.",
            "OK, so.",
            "Now what are monomial distributions?",
            "OK, it turns out there in the literature, if you look carefully, they are called the rich lands.",
            "And so there is that is really defined in this way and then K here is simply a renormalization factor, such that if we take the integral of this part, then K will be equal to 1 divided by the integral.",
            "Now again their shots are conjugate priors and their conjugate priors here for discrete likelihood distributions, and I've put on this slide a few examples of some very simple Dirichlet."
        ],
        [
            "OK, so now the question next is, well, what prior should we pick or what the rich?",
            "Let's should we pick?",
            "So if we don't have any information, we don't have any preference.",
            "Intuitively, we want we would like to pick a distribution that's uniform.",
            "It turns out that their sleds that has coefficients one and one for the exponents would give us a uniform distribution.",
            "So here P is my random variable.",
            "So let's say for now, let's simplify things a little bit.",
            "Let's see that we've got a coin.",
            "And we don't know it's probability OK of of head or tail, so it's a number between zero and one.",
            "I'm going to call that PK and then hear those numbers or the exponents in my monomial corresponding to.",
            "The probability of having the property of tail OK.",
            "So intuitively, the higher one exponent is, then the more weight it should give to that event.",
            "OK, so here we can see that there are several distributions.",
            "The uniform is here.",
            "The Green one is 1 where we would essentially see that.",
            "Well, I believe the probability of head is roughly .2, and I've got some confidence, but I'm not totally confident and then perhaps if I became more confident this it could look like this.",
            "And then the main difference between those distributions.",
            "Here is the magnitude of the coefficients, so the exponents and one and two.",
            "The larger they are, it'll mean the more confident I am.",
            "And one way of interpreting them would be to say that I've flipped a coin a certain number of times, and.",
            "An one would be the number of times that I got had an end to the number of times that I got there.",
            "And then I could build.",
            "Irish Lad distribution that would correspond to one of those if let's say that I saw had twice and till eight times or had 20 times and till 80 times and you can see that the more times you've seen the coin flip, perhaps the more confident you are in this property being .2 and that's why it will be peaked.",
            "So now that we have some understanding.",
            "The way we would naturally pick a prior that's ended Irish last family would be to think of having some virtual flips, like if you think that a coin is of a certain probability, you could ask yourself, well, had I seen the coin being flipped, how many times do I think it would have given me head versus tail and then that's how you would pick those numbers?",
            "OK, now that's something intuitive there are, I guess, numerical techniques.",
            "If we wanted to really have.",
            "Um?",
            "Something computational where there are techniques where you can fit the mean and variance.",
            "So let's say that you know that you've got a distribution that has a certain mean and variance, and then translate that into picking N1 and N2 so that we have this same mean and variance.",
            "Turns out that for their sled the mean is really at the peak and corresponds to the ratio of N1 and two, and then the variance will decrease with.",
            "The magnitude of N1 and N."
        ],
        [
            "OK, so that's enough for Irish lads.",
            "Beyond that we couldn't code in two priors.",
            "Different types of information and often it will be useful to encode structural information.",
            "So for instance, what if we know that two different powders are supposed to be the same and we would like to pull the evidence about those parallels and reduce essentially the number of partners?",
            "Or we can simply do this by declaring Thetic Santee Thanks Prime.",
            "A prime to be the same if we think that the resulting distribution will be the same.",
            "Another simple case would be, what if we know that the dynamics are factored as in having dynamic Bayesian network.",
            "OK, so there would normally be some conditional independence and then the powers of the dynamic vision network would be that for each variable there's a distribution given its parents and then so for each pair of that distribution we would create a theater like this and then the benefit is that we've got exponentially fewer partners.",
            "We can also encode things like sparsity, which I don't have on this slide, and there's a vast literature out there in statistics and probabilities about picking or fitting distributions given some constraints or other types of information."
        ],
        [
            "OK, so this was to give you an idea of prior knowledge.",
            "Now let's move on to."
        ],
        [
            "Policy optimization OK so.",
            "Just a quick recall.",
            "This is traditional RL.",
            "OK, we're here again.",
            "I'm assuming that X includes States and rewards.",
            "OK, so we've got our states rewards together here with actions and we're missing the transition probabilities.",
            "Now, if we're taking a Bayesian approach, it turns out that there's a natural formulation as a palm DP or partially observable Markov decision process.",
            "And the idea here is simply that we're going to extend our notion of state to now also include the unknowns Theta.",
            "And the reason why I'm calling it a partially observable Markov decision process is simply because Theta is not observable.",
            "So X here the underlying states of physical states in the rewards.",
            "So far we've assumed that their observable, but Theta is not.",
            "But still, these are the things that are really defining our model in the States.",
            "So I mean, we could simply bundle them together and have this now become our new state space.",
            "OK. Now you may ask, well, why do we want to do that?",
            "The neat thing is because now the transition probabilities."
        ],
        [
            "Are going to be known.",
            "So you may wonder, like how did we go from here simple model to a more complex one where we actually know everything.",
            "OK, so this is not magic.",
            "Let's have a look more carefully at the transition probabilities.",
            "So from a graphical model, let's see.",
            "We've got a base net we can represent transition properties as follows.",
            "So there's our current state.",
            "There's the next state, there's the action, right?",
            "So the current state and action influence the next state now.",
            "If we include feta into the state, but we keep this factor.",
            "So now here's X and feta, then experiment Theta prime X prime depends on XNA as before, but also on Theta because we've defined earlier Theta xax prime to be this unknown probability.",
            "But now if I conditioned on Theta, that simply means like if I knew what theater was, then I would know.",
            "With what probability X prime would be reached so?",
            "By using this trick, then we actually know.",
            "This conditional distribution.",
            "Now we also have to have a conditional distribution for Theta, so here's a simple one.",
            "We're just going to keep Theta constant, so essentially the model doesn't change.",
            "Obviously, we could have more complicated models that include change overtime, but for now it's."
        ],
        [
            "Stick with that.",
            "There's another formulation so because partial observable Markov decision processes may sound like complicated and give some people some headaches, and we really like just Markov decision processes.",
            "Well, if you really just like that, then we can actually think of Palm DP's as simply belief MDP's.",
            "So in the literature, sometimes people refer to these models as palm DP.",
            "Sometimes this belief and EPS there really just the same.",
            "The difference is that instead of using Theta to augment the States, let's segment the state with the actual distribution over Theta.",
            "And that distribution is known to the agent because the agent built it.",
            "FIFA is unknown, but B is.",
            "So now we've got something that's fully observable.",
            "And we still know completely the transition properties.",
            "So now we've gone to a model that's again just a Markov decision process, but with these reformulation.",
            "We know all of the parts.",
            "Yes yes OK so yeah here this should be experiment V prime good."
        ],
        [
            "OK, so how did how can we build this one so?",
            "Pompey looks like this.",
            "The belief MVP.",
            "We replace theater by B.",
            "And now again, X prime will depend on X and a.",
            "It will also depend on B.",
            "The difference is that now we're going to integrate over all the thetas.",
            "OK, so we've got an integral, so it's slightly more complicated, but it's perfectly valid.",
            "And then for the property distributions of B prime, while this one will be also more complicated.",
            "So instead of having theater remain constant as we had before, now be prime is going to be the posterior that we would normally get.",
            "By applying Bayes rule and then so the property of getting that belief will be one and 0 otherwise."
        ],
        [
            "OK, so to summarize, then classic Carel.",
            "Would normally tackle this optimization problem where we want to find a policy, let's say using Bellman's equation that has the highest value and this value would depend only on the current state X.",
            "Now if we take Abbasian view and let's say that we take the belief MDP formulation well, now my states are really X&B.",
            "But I can still formulate.",
            "Bellman equation that is in terms of X&B.",
            "So the algorithms that we're going to see next are really just going to try to work with this equation as opposed to that one, and the nice benefit is that now we know this transition probability, Whereas here this is unknown.",
            "The second line is benefit is that here?",
            "Well, if we only condition the value on X, there's a problem because we don't know what kind of uncertainty we've got about the model.",
            "So when it comes time to decide between exploring and exploiting.",
            "We don't know what to do, so traditionally people would talk with heuristics like epsilon, greedy, Boltzmann and others, whereas here B actually encodes this uncertainties.",
            "We've got a full distribution over all of the possible models, so therefore we know where we should perhaps explore, or at least we know in principle, so we'll see later how this will really help us."
        ],
        [
            "OK, so actually just to clarify, what is the exploration exploitation tradeoff?",
            "Is this following dilemma where when it's time to pick an action should we pick it such that it maximizes the immediate reward?",
            "That's exploitation?",
            "Or to maximize information gain that's exploration?",
            "Right?",
            "Again, that's because we don't know what are the transition properties.",
            "So if I if I try and action just to see what will happen, then it gives me some information to update.",
            "What is that transition property and therefore in the future I'll be able to perhaps devise a better policy based on that, but in the mean time for the immediate reward I may actually pay a high cost of that action was not a good one, right?",
            "So there's this natural tradeoff between those two.",
            "Now to use Sutton's way of talking.",
            "I would say that while this is the wrong question to ask, so when solving a problem we have to ask the right question and actually Mohammed when explaining earlier what is RL, basically told us what is that question or at least what we should be doing when we're trying to optimize a policy.",
            "Is we just to maximize expected total rewards?",
            "So the value function as Mohamed explained earlier, is really just the expected sum of all the rewards that we're going to get in the future.",
            "So at one level, if this is really all we care about, then I'll argue that if we can find a policy that maximizes this, then why do we care whether we're exploiting our exploit, what we're exploring or exploiting?",
            "Right it it doesn't really matter how we classify the actions as being, exploring, exploiting and we don't have to worry about what we're really doing as long as we just maximizing here the rewards.",
            "So I'll simply claim then that we can get an optimal exploration exploitation tradeoff as long as we find a policy that has highest reward."
        ],
        [
            "So.",
            "Now, looking at the formulation to belief MDP formulation here, I'll claim that as long as we can solve this, find an optimal policy, then we've taken care of the exploration exploitation tradeoff.",
            "And so that's one really nice property of beige and take approach is that in principle.",
            "That's taken care of.",
            "Now obviously there's going to be some approximation, so we're not really going to find an optimal balance, but in principle we know what to do.",
            "OK so I'm gonna what I'm going to do next is tell you a bit about several approaches that people have devices over the years and then we'll see some of the challenges about doing optimizing policies and invasion way and then at the end I'll talk about one of my algorithm as well."
        ],
        [
            "OK, so in 1999 Deardon Friedman and Android proposed a technique that was based on a myopic notion of value of information, and the idea is that again, if we think of exploring, exploiting if we could simply augment the traditional value function that's in terms of the current state, not be just the current state, augment it with some value of the information.",
            "Alright.",
            "So here, what if we could have meant the Q function with another function, let's call it value of.",
            "Actually, here they called myopic value of perfect information, and then we're simply going to pick the action that maximizes this somewhere.",
            "Here.",
            "This is obviously the reward would get by simply exploiting what we know so far.",
            "And here this would be the value of exploring.",
            "And OK, there's some reasons why they didn't get the value of information precisely is because computing this is actually as hard as solving the original problem.",
            "OK, So what they do is competent myopic version, which means we're only look at the value of the information for a short period of time instead of the infinite horizon.",
            "And then they also computed an upper bound by.",
            "Looking at the perfect information, so if by trying an action you would find out precisely what would be the Q value of the resulting state action pair, then you will get perfect information, so they use that as an upper."
        ],
        [
            "OK, another approach.",
            "This one by strands, is actually based on what is known as the Thomson sampling, and the idea is very simple, being beige and we have Theta Anna distribution over Theta.",
            "So when we want to pick an action, how about we simply?",
            "We simply pick a theater from that distribution and then pick the action that's best for that theater.",
            "And.",
            "We can see here that by sampling a Theta we're going to do some exploration, because we're going to consider different models and then by picking the best action for that model we're exploiting.",
            "So it does provide some kind of heuristic for."
        ],
        [
            "Balancing the two.",
            "And here I've included a quick slide showing some results that this is taken from strands paper, and I've highlighted in green the values obtained by the beige and techniques.",
            "So here are three of them, so this is trans technique.",
            "This is the ordinance technique, and this is another technique I believe, by Leslie cabling earlier.",
            "And then we've got Q learning with the same uniform exploration Q learning with Boltzmann exploration.",
            "And here is some eurasec technique that as trans had developed earlier and we can see that.",
            "The vision approaches tend to earn higher reward at least faster, so here are phases.",
            "1000 time steps and this is the next 1000 time steps.",
            "And then I've simply added up all the rewards so we can see that the actually do a better job at earning rewards faster, which is the whole point here of balancing the exploration exploitation tradeoff.",
            "OK, now those techniques are not optimal because they are.",
            "Approximate OK, and then we can see in fact that this heuristic one is actually doing quite well."
        ],
        [
            "OK, one more technique basean sparse sampling.",
            "So another way of tackling Markov decision processes and polypes would be to simply build some look ahead search tree where we alternate between maximizing rewards by picking the best action and taking an expectation with respect to the reachable states.",
            "And then we can simply expand history, evaluate at the bottom of the Leafs by taking let's see the mean model and then propagating all the rewards up.",
            "OK, so Kerns was the one who had several years ago, I believe.",
            "Pioneered such approaches based on expanding this tree in a stochastic form and then one lizard bowling in Sherman's in 2005 adopted adapted this technique to."
        ],
        [
            "Asian reinforcement learning.",
            "I'm very quickly another one, this one based on policy gradient.",
            "The idea is that well, if we can model version reinforcement learning as a palm DP, one of the common approaches is to represent the policy by fine state controller that would have the following parameters.",
            "These are continuous parameters, so for each node of the control, what action is being picked and what will be the next node reached, and then you can simply do gradient dissent too.",
            "Minimize costs, otherwise maximize rewards.",
            "K."
        ],
        [
            "Another one again.",
            "If we think of this as a palm DP, while the main problem is actually that Theta is a continuous variable, but we've got algorithms for Palm DP's that work with discrete variables.",
            "So what if we simply discretize Theta and then apply your favorite palm DP algorithm so it works except that the discretization will lead to a grid that exponentially large in the number of dimensions?",
            "OK, so to summarize, all of these techniques.",
            "OK, they try to work with this version of Bellman's equation and the main difficulties really are that the belief or otherwise the unknown model C to our continuous.",
            "So solving this is not obvious.",
            "And then you have to resort to approximations and then a lot of those techniques work with value functions an having B or C to continuous means that.",
            "We have here a value function that's going to be quite difficult to represent, and in fact there's an interesting question.",
            "What is the characterization of that funk?"
        ],
        [
            "So I'm going to tell you now, this is a paper that appeared last year in 2006 by myself and some coauthors.",
            "I'm going to tell you how we can represent the value function exactly in a power trick way, and then I'll tell you about one algorithm for exploiting."
        ],
        [
            "This formulation.",
            "So.",
            "OK, the main idea here is that we want to come up with a parrot risation and it turns out that.",
            "The value function, the optimal value function for state X and model Theta is really the Max of a bunch of polynomials in Theta.",
            "The proof is very simple, so prove by induction.",
            "So I've defined earlier balance equation in terms of X&B.",
            "But now let's actually consider another version of it in terms of Theta, where this value function relates to this one.",
            "According to this integration.",
            "So now we've got Bellman's equation with respect to Theta here and we want to show that if we start here with the value function being a Max of polynomials, then after one Bellman backup we're going to have the the value function that's going to be still the Max of polynomials.",
            "And then if we just keep on doing this over and over, it will always remain in the class of Max of polynomials.",
            "So if this is a Max of polynomials.",
            "Gamma is the discount factor, which is a constant.",
            "The reward is also a constant.",
            "We don't know where the transition probabilities are, however, we've modeled this with data, So what we're really doing is simply multiplying Theta by this, and then if I pull out the Max and bring it here with this Max and then I'm left with an expectation of Theta times K plus gamma times a polynomial which is itself still a polynomial.",
            "So we actually know then what is the penetration and then the next question is, well, how could we exploit that to devise an effective algorithm?"
        ],
        [
            "OK, actually before I go to an algorithm, you may wonder what happens if we have a partially observable model.",
            "In other words, what if X is not fully observable?",
            "It turns out that the theorem still holds, so we're still going to have a Max of polynomials and then the beliefs are going to be mixtures of Dirichlet instead of being one there."
        ],
        [
            "Flat.",
            "OK, on this slide I've got one algorithm called B tool which is really a point based value iteration approach that's been adapted for beige and RL, but that in general the main difficulty here to tackle is the fact that Theta is continuous.",
            "OK, so we do have techniques for point based on iteration that works with continuous spaces, and so we adapted one here.",
            "So this was work done.",
            "Before with Nichols glasses.",
            "And so we adopted this technique to beige and reinforcement learning, and it's very simple.",
            "So we sample a bunch of belief points and then we perform Bellman backups at each one of those points.",
            "Performing in Belmont back up is really just.",
            "Computing this equation, but I'm going to do it in 2 three steps.",
            "The first step will be to find what is the best polynomial at the next time step by solving this.",
            "Now this may look very complicated because OK, I've got an integral polynomial multiplied by my belief and so on, but the belief is is a monomial that's a polynomial, so that's itself a polynomial integral of polynomial is very easy to compute, so we can do all of this in close form.",
            "Then once I've got that, I can find what is the best action for my current belief.",
            "And.",
            "We have to solve this, which again may look difficult but same trick.",
            "All of this will boil down to a polynomial with an integral that I can solve exactly.",
            "And so I get my best action and then I can build my new polynomial by taking that best action with the best polynomials at the next step.",
            "And so it gives me my new polynomial OK?",
            "Yeah, so a star could be there could be several optimal actions, so I'll just pick one of them, or otherwise I could pick any convex combination of them."
        ],
        [
            "OK, so this is all nice, however, there's some computational issues so."
        ],
        [
            "In the last step here.",
            "What I didn't tell you is that the this polynomial is going to be much larger than the ones we."
        ],
        [
            "Started with.",
            "So if you look carefully.",
            "So if I think of a polynomial, I simply being some of monomials and I represent polynomials.",
            "By their components.",
            "So each monomial separately turns out that when I do the computation, those two sons are going to get together and actually this should be X prime, I'm sorry.",
            "Yeah, so these should be all X primes instead of S primes.",
            "Then I get a sum that's going to be I * S An.",
            "So the number of monomial's will grow by a factor of S or actually effective Exelon number of states.",
            "So overtime the number of monomials will grow exponentially in the number of states."
        ],
        [
            "So we have to deal with this, so we're going to do an approximation.",
            "The idea is very simple.",
            "We're going to project our polynomial onto a set of basis functions which are going to be monomial.",
            "So essentially we're going to compress our polynomial that has exponentially many monomials onto smaller set of monomials.",
            "Now to find the best linear combination of basis monomials.",
            "We can use an optimization criterion that could be simply minimizing some Ln norm.",
            "And again, this looks complicated, but if you take a Euclidean norm.",
            "Turns out that this is as simple as solving a linear system of equations.",
            "Ax equals B or a would be this integral.",
            "So each entry in A is the result of this integral.",
            "But these things are polynomials or monomials, so we can solve these integrals exactly and same thing for B.",
            "And then C corresponds to X, so X corresponds to C, yeah.",
            "So I can solve this readily without any."
        ],
        [
            "Problem.",
            "So this gives me a way to project down, but one important question is well.",
            "What would be my basis functions?",
            "So.",
            "One way to pick basis functions to look at.",
            "Equation we use to compute those polynomials as well as.",
            "The Bayes update for beliefs and the Bayes updates we take the problem multi by Theta.",
            "Annabelle man back up we take a polynomial an we multiply by Sita.",
            "So presumably the monomials that we get as beliefs are going to be the same monomials that are part of the polynomials for the value function.",
            "So we can actually do.",
            "Monitoring very easily so we can collect in this way abunch of monomials that we're going to use as our basis functions."
        ],
        [
            "OK, so some of the properties of this algorithm are that.",
            "We can do the optimization of policy at some sample belief points, and then we can do this offline, so without interacting with any environment because with the formulation we know all of the transition probabilities OK.",
            "The learning really happens when we do the belief monitoring.",
            "OK, so really we computer policy here, that's going to be good for the entire space of beliefs and then at runtime we do believe monitoring and then we check for our current belief.",
            "What would be the best action based on what we've already precomputed here?",
            "So the nice thing is that we can actually run this in real time because belief of this really takes us a fraction of a second.",
            "The other nice properties is that we can optimize the exploration exploitation tradeoff naturally, and then we can also encode prior knowledge into our prior disadvantages that while the policy is not necessarily going to be good at all, belief points because I've done an approximation here where I've sampled some belief points and I've also done a projection that approximates my exponentially large polynomials into small."
        ],
        [
            "Other ones Yep."
        ],
        [
            "No.",
            "So first step here.",
            "I'm sure there are many ways of picking belief points the way we do it is that we start with the initial belief and then we simply do forward lookahead, which is problem dependent.",
            "OK.",
            "So to do this for look ahead, we would use the transition probabilities formulated with the belief MDP formulation.",
            "And then.",
            "Simply collect the beliefs that could be reached in this way.",
            "OK."
        ],
        [
            "OK, so quick empirical evaluation.",
            "Yeah.",
            "OK, so we tested the beta algorithm or compared it against two you Ristic.",
            "One is the exploit touristic.",
            "So what if we don't do any exploration?",
            "We simply greedily select the best action of the mean model at each time step.",
            "OK, so this really involves just solving an MDP at each time step, which can actually take awhile, but on the other hand it's very simple to do.",
            "Or while we could use the same technique that was proposed earlier by Pino, which is to discretize Theta and whenever we can do the computation, that'll be great.",
            "But if the number of states gets too large, then we may not be able to provide."
        ],
        [
            "Some actions.",
            "OK, so beetle is here with its runtime.",
            "We've got the discrete Palm DP version and the exploit here is sick, and here's an upper bound, which is what if we just executed the optimal policy as if in Oracle, just gave it to us?",
            "OK, and you will see that beetle does well when the number of free powders is fairly small, but not so well when the number of free partners is large.",
            "And this is because of two reasons.",
            "First, we actually used.",
            "The prior that was uninformed, so our uniform priors.",
            "So if we've got lots of partners, that means we have to take a while to learn.",
            "And then also, this is where the approximations are going to be.",
            "The most crude in the projection.",
            "So Beetle doesn't do so well.",
            "And then same thing here."
        ],
        [
            "We also tested what will happen if we change the prior to be more informative.",
            "So the idea is that we increase our confidence.",
            "So here the parameter K is simply the magnitude of those exponents under Dirichlet.",
            "OK, so the larger that means, the more confident we are, and then it would be confidence in terms of the true underlying model.",
            "OK, so you can see that as the confidence increases then the policies that are obtained are better and these are the.",
            "Yeah, that all expected to reward for 1000 time steps."
        ],
        [
            "OK.",
            "I'm.",
            "So we've covered para knowledge policy optimization, so now let's discuss a little bit these techniques, and then we'll talk about Bayesian approaches for us."
        ],
        [
            "There are all variants.",
            "OK, so in my discussion here I'll I'll come back to priors.",
            "I'll talk again about online learning as well as."
        ],
        [
            "Active learning.",
            "So the first question that may come up to your mind is, well, would it be better to learn everything from scratch without having to specify any prior?",
            "OK, a lot of people really, you know are.",
            "Concern about priors and I'm gonna argue that no, we shouldn't be worried.",
            "And.",
            "My main argument is going to be that there's no such thing as reinforcement learning without any prior.",
            "If you think about it, every learning algorithm, this is a well known fact of machine learning has a learning bias.",
            "So what does basean reinforcement learning do?",
            "It puts the bias explicitly in the prior, and it forces you to select it.",
            "Whereas other oral techniques all have an implicit bias.",
            "But you don't get to pick it, so it feels OK.",
            "But it's there.",
            "And sometimes you actually have to pick it, so if you do policy search, sometimes you have to pick apart relation for the policy space and that's in itself bias for the hypothesis space.",
            "If you're doing value function approximation often you'll have to pick a function approximator, and that too is."
        ],
        [
            "Bias.",
            "So just to clarify this.",
            "Oh yes.",
            "Selecting the Bay."
        ],
        [
            "Is function.",
            "Alright.",
            "Right, so OK in my beetle algorithm I had to select basis functions.",
            "When I did the projection.",
            "So yes, you could argue that this is also another bias, but I guess that's a bias that comes.",
            "Yeah, in the same way from the fact that I'm doing an approximation.",
            "Yes, yeah."
        ],
        [
            "OK, so OK.",
            "If we look at function approximation.",
            "Either yeah, if we use basis functions that your bias or otherwise.",
            "If you use like a neural network, radial basis functions.",
            "Whatever day introduces, well, a bias, and we know that for reinforcement learning a lot of those function approximators if you're not careful, there could be some divergance.",
            "And one possible cause here could could be that.",
            "All of those.",
            "They have an implicit assumption about how the value is going to generalize across states, so that when you do an update for one state, then it will affect the neighboring states.",
            "But if those updates are not.",
            "Like if there's no reason why the neighboring state should be changed as."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, thanks Mohammed.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to continue with.",
                    "label": 0
                },
                {
                    "sent": "The history of Asian reinforcement learning.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And actually, the common belief is that while reinforcement learning as we know it in artificial intelligence really started in the 1980s, so with this pioneering work of Sutton Barto and others, and actually Bayesian techniques have not been really common.",
                    "label": 1
                },
                {
                    "sent": "It's only really recently that they've surfaced in artificial intelligence, so they can't believe then is that while Bashan reinforcement learning is a new approach.",
                    "label": 0
                },
                {
                    "sent": "But actually it turns out that this is not the case.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if you look back in history.",
                    "label": 0
                },
                {
                    "sent": "Well, it is well.",
                    "label": 0
                },
                {
                    "sent": "OK, first question is what is RL?",
                    "label": 0
                },
                {
                    "sent": "And then Muhammad has defined it and so on.",
                    "label": 0
                },
                {
                    "sent": "But another way of looking at it is that it's really the problem of controlling a Markov chain with unknown probabilities and then in the operations research literature then they've already been tackling this problem all the way since the 1950s and 1960s.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. And in that literature it was known under the name of adaptive control processes, dual control, optimal learning and others, and some of the people that really para neared these approaches in operations research include Bellman himself.",
                    "label": 1
                },
                {
                    "sent": "In fact, he wrote a book in 1961 on adaptive control processes, a guided tour.",
                    "label": 0
                },
                {
                    "sent": "And he actually had some earlier papers in the 1950s where he formulated the problem and tackled it with some beige and approaches at about the same time felt bone in Europe.",
                    "label": 0
                },
                {
                    "sent": "Also looked at dual control and then at MIT, Ronald Howard and some of these students also considered the problem.",
                    "label": 0
                },
                {
                    "sent": "So really we could say that the use of beige and technique really dates back from the 1950s and 1960s.",
                    "label": 0
                },
                {
                    "sent": "But in operations research.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now you may ask.",
                    "label": 0
                },
                {
                    "sent": "Well, so if it's already been tackled in operations research, then what is it that we're doing here in AI?",
                    "label": 0
                },
                {
                    "sent": "And how does our work related to the work that's been done in operations research?",
                    "label": 0
                },
                {
                    "sent": "So it turns out that in operations research, while several decades ago, I mean the computers were not very powerful, algorithmic techniques were not as scalable, and so on, so they really focused on that Article Foundation, but they did also look obviously at algorithmic.",
                    "label": 0
                },
                {
                    "sent": "Problems and they did find some solutions for some special cases, and perhaps one of the most well known special case is known as the bandit problem, where you essentially don't have states you just have like 1 state.",
                    "label": 1
                },
                {
                    "sent": "That's always the same, and then you would pick actions and then you would like to pick the action that will give you the most reward, but you don't know which action gives you the most rewards, so it's kind of like going to a casino and you think that one slot machine will give you.",
                    "label": 1
                },
                {
                    "sent": "More chances of winning and then you simply want to experiment so it turns out that there's a solution known as Gittins indices are named after Gittins, and this doesn't require substantial computation, at least for the very basic problem.",
                    "label": 0
                },
                {
                    "sent": "So there are some variants that are being tackled today and actually in this conference or some papers about bandit problems.",
                    "label": 0
                },
                {
                    "sent": "The more general version.",
                    "label": 0
                },
                {
                    "sent": "But in any case, some of the special cases of beige and reinforcement lion.",
                    "label": 0
                },
                {
                    "sent": "Learning like just basic bandit problems have been tackled and there were some solutions that were device OK, but in general yeah there are solutions.",
                    "label": 0
                },
                {
                    "sent": "Then we scale so the work that's really been a contribution to reinforcement learning in artificial intelligence for Bayesian techniques has been to try to improve the scalability an also I guess, improve the modeling and so on.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now you may ask.",
                    "label": 0
                },
                {
                    "sent": "OK, so what's the work that's been done in artificial intelligence?",
                    "label": 0
                },
                {
                    "sent": "So here I've gathered on this slide the names of several people with the dates of their publications.",
                    "label": 0
                },
                {
                    "sent": "This is by no means you know something exhaustive, OK, but just so that you get a sense, there's actually quite a bit of work in model based reinforcement learning as well as model free basean reinforcement learning.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this was just a bit of history.",
                    "label": 0
                },
                {
                    "sent": "Now let me go into the details of model based Bayesian reinforcement learning and then I'll talk more specifically about priors, how to optimize the policy will discuss some of the advantages and disadvantages of using Bayesian techniques, and then I'll serve a few approaches that use Bayesian techniques for some of the variants of reinforcement learning, like inverse reinforcement, learning, multiagent reinforcement learning, and so on.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so as Mohammed explained.",
                    "label": 0
                },
                {
                    "sent": "Reinforcement learning is really formulated as a Markov decision process that has the following components, but here I've modified a little bit the definition.",
                    "label": 1
                },
                {
                    "sent": "OK, because it will be useful for the next slide.",
                    "label": 0
                },
                {
                    "sent": "So what I've done is I've pushed into the set of states the reward.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm going to call states basically a top hole, then includes the physical state component as well as the reward component.",
                    "label": 1
                },
                {
                    "sent": "OK, so abstractly I mean these are random variables, right?",
                    "label": 0
                },
                {
                    "sent": "And then I could just put them together and it's not a problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm just going from non denote by X this top wall and whenever I want to refer to the reward component I'll use a subscript R and otherwise ask for the state part.",
                    "label": 0
                },
                {
                    "sent": "OK, we have actions as usual and then the main problem for reinforcement learning is that we don't know what are the transition and reward probabilities.",
                    "label": 1
                },
                {
                    "sent": "Which is this distribution P here an with respect to what Muhammad explained before now in Pi also have the distribution over rewards.",
                    "label": 0
                },
                {
                    "sent": "Q Because X now refers to both States and rewards OK.",
                    "label": 0
                },
                {
                    "sent": "So it's going to simplify things, at least for the Model Bayes approach, because now the way I can see this is that I only have like 1 unknown OK and it's a distribution over states or rewards and the way to tackle this if we're basean is that whenever we've gotten unknown, we Simply put a distribution over it to model this uncertainty, and then we would compute a posterior.",
                    "label": 0
                },
                {
                    "sent": "That takes into account the information that we receive.",
                    "label": 0
                },
                {
                    "sent": "So here I'm going to use Theta to denote an unknown and so here Theta XAX prime will be this unknown probability of reaching X prime from X and A and so this is a random variable that has the range from zero to 1.",
                    "label": 0
                },
                {
                    "sent": "And whenever I want to talk about several variables together, like all of those that they know, probabilities starting from XNA but reaching different possible ex prime, then I'm just going to note that by Theta XA.",
                    "label": 0
                },
                {
                    "sent": "So, really together, these correspond to a multinomial distribution.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Now we've encoded with a random variable or unknown.",
                    "label": 0
                },
                {
                    "sent": "So what we need to do next is come up with a prior and compute the posterior.",
                    "label": 0
                },
                {
                    "sent": "So for the moment, let's just say that we've got some distribution.",
                    "label": 0
                },
                {
                    "sent": "I'm going to call it be here, OK, B4 belief, because that will be my belief about what is the underlying model.",
                    "label": 1
                },
                {
                    "sent": "Otherwise, the belief of the agent, the learner about the underlying model, and then so this agent wants to update its belief and compute a posterior.",
                    "label": 0
                },
                {
                    "sent": "And this posterior will be conditioned on the current state, the action taken, and then the next state that it observes, and then if the agent goes on for several time steps then there would be several of those triples OK, and then we could simply update at each time step what happens now.",
                    "label": 1
                },
                {
                    "sent": "Given this, then we apply Bayes theorem an it's as simple as this, right?",
                    "label": 1
                },
                {
                    "sent": "So we take the prior multiplied by the likelihood.",
                    "label": 0
                },
                {
                    "sent": "Which here, according to the definition I had on my previous slide, is simply Theta XA X prime.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the beauty of vision techniques please.",
                    "label": 0
                },
                {
                    "sent": "So far it's really simple conceptually.",
                    "label": 0
                },
                {
                    "sent": "OK, now let's get into the more tricky part.",
                    "label": 0
                },
                {
                    "sent": "What is a prior and wood?",
                    "label": 0
                },
                {
                    "sent": "Does it matter what kind of prior we pick?",
                    "label": 0
                },
                {
                    "sent": "Will it influence the computation down the road and it turns out that.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It does.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So I'm not the part I'm going to talk about the prior.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "College.",
                    "label": 0
                },
                {
                    "sent": "And OK, so we want to have a prior.",
                    "label": 0
                },
                {
                    "sent": "I'm going to propose here to consider a prior that corresponds to a monomial in Theta.",
                    "label": 1
                },
                {
                    "sent": "OK, so Theta is a variable and mono meal is simply some expression where we've got Theta with some exponents that are constants.",
                    "label": 1
                },
                {
                    "sent": "OK, and here I've got another coefficient K which is a constant.",
                    "label": 0
                },
                {
                    "sent": "So let's just say that we've got a prior that looks like this.",
                    "label": 0
                },
                {
                    "sent": "I could make up any function.",
                    "label": 0
                },
                {
                    "sent": "Of Theatre and declare that this is my prior as long as I can renormalize it when I take the integral by using K OK.",
                    "label": 0
                },
                {
                    "sent": "So now the question is, well, what will the posterior look like?",
                    "label": 0
                },
                {
                    "sent": "And it turns out that by picking a monomial I can show that the posterior is also a monomial, and it's easy to see because Bayes theorem which is simply to compute the posterior by multiplying the prior with the likelihood.",
                    "label": 1
                },
                {
                    "sent": "Here this is my prior times my likelihood, so my likelihood simply increments one of the exponent.",
                    "label": 0
                },
                {
                    "sent": "In my monomial, so it's very simple.",
                    "label": 0
                },
                {
                    "sent": "And then the beauty of this is that now I have a distribution that will always be of the same family, always in a family of monomials.",
                    "label": 1
                },
                {
                    "sent": "And it turns out that these distributions, whenever they remain in the same family.",
                    "label": 0
                },
                {
                    "sent": "After abrasion update, they're going to be called conjugate priors, because if you pick the prior like this, Anna posters the same family forgiven likelihood, then their conjugates with.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The likelihood.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Now what are monomial distributions?",
                    "label": 0
                },
                {
                    "sent": "OK, it turns out there in the literature, if you look carefully, they are called the rich lands.",
                    "label": 0
                },
                {
                    "sent": "And so there is that is really defined in this way and then K here is simply a renormalization factor, such that if we take the integral of this part, then K will be equal to 1 divided by the integral.",
                    "label": 0
                },
                {
                    "sent": "Now again their shots are conjugate priors and their conjugate priors here for discrete likelihood distributions, and I've put on this slide a few examples of some very simple Dirichlet.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now the question next is, well, what prior should we pick or what the rich?",
                    "label": 0
                },
                {
                    "sent": "Let's should we pick?",
                    "label": 0
                },
                {
                    "sent": "So if we don't have any information, we don't have any preference.",
                    "label": 0
                },
                {
                    "sent": "Intuitively, we want we would like to pick a distribution that's uniform.",
                    "label": 0
                },
                {
                    "sent": "It turns out that their sleds that has coefficients one and one for the exponents would give us a uniform distribution.",
                    "label": 0
                },
                {
                    "sent": "So here P is my random variable.",
                    "label": 0
                },
                {
                    "sent": "So let's say for now, let's simplify things a little bit.",
                    "label": 0
                },
                {
                    "sent": "Let's see that we've got a coin.",
                    "label": 0
                },
                {
                    "sent": "And we don't know it's probability OK of of head or tail, so it's a number between zero and one.",
                    "label": 0
                },
                {
                    "sent": "I'm going to call that PK and then hear those numbers or the exponents in my monomial corresponding to.",
                    "label": 0
                },
                {
                    "sent": "The probability of having the property of tail OK.",
                    "label": 0
                },
                {
                    "sent": "So intuitively, the higher one exponent is, then the more weight it should give to that event.",
                    "label": 0
                },
                {
                    "sent": "OK, so here we can see that there are several distributions.",
                    "label": 0
                },
                {
                    "sent": "The uniform is here.",
                    "label": 0
                },
                {
                    "sent": "The Green one is 1 where we would essentially see that.",
                    "label": 0
                },
                {
                    "sent": "Well, I believe the probability of head is roughly .2, and I've got some confidence, but I'm not totally confident and then perhaps if I became more confident this it could look like this.",
                    "label": 0
                },
                {
                    "sent": "And then the main difference between those distributions.",
                    "label": 0
                },
                {
                    "sent": "Here is the magnitude of the coefficients, so the exponents and one and two.",
                    "label": 0
                },
                {
                    "sent": "The larger they are, it'll mean the more confident I am.",
                    "label": 0
                },
                {
                    "sent": "And one way of interpreting them would be to say that I've flipped a coin a certain number of times, and.",
                    "label": 0
                },
                {
                    "sent": "An one would be the number of times that I got had an end to the number of times that I got there.",
                    "label": 0
                },
                {
                    "sent": "And then I could build.",
                    "label": 0
                },
                {
                    "sent": "Irish Lad distribution that would correspond to one of those if let's say that I saw had twice and till eight times or had 20 times and till 80 times and you can see that the more times you've seen the coin flip, perhaps the more confident you are in this property being .2 and that's why it will be peaked.",
                    "label": 0
                },
                {
                    "sent": "So now that we have some understanding.",
                    "label": 0
                },
                {
                    "sent": "The way we would naturally pick a prior that's ended Irish last family would be to think of having some virtual flips, like if you think that a coin is of a certain probability, you could ask yourself, well, had I seen the coin being flipped, how many times do I think it would have given me head versus tail and then that's how you would pick those numbers?",
                    "label": 0
                },
                {
                    "sent": "OK, now that's something intuitive there are, I guess, numerical techniques.",
                    "label": 0
                },
                {
                    "sent": "If we wanted to really have.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Something computational where there are techniques where you can fit the mean and variance.",
                    "label": 0
                },
                {
                    "sent": "So let's say that you know that you've got a distribution that has a certain mean and variance, and then translate that into picking N1 and N2 so that we have this same mean and variance.",
                    "label": 0
                },
                {
                    "sent": "Turns out that for their sled the mean is really at the peak and corresponds to the ratio of N1 and two, and then the variance will decrease with.",
                    "label": 0
                },
                {
                    "sent": "The magnitude of N1 and N.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so that's enough for Irish lads.",
                    "label": 0
                },
                {
                    "sent": "Beyond that we couldn't code in two priors.",
                    "label": 0
                },
                {
                    "sent": "Different types of information and often it will be useful to encode structural information.",
                    "label": 0
                },
                {
                    "sent": "So for instance, what if we know that two different powders are supposed to be the same and we would like to pull the evidence about those parallels and reduce essentially the number of partners?",
                    "label": 0
                },
                {
                    "sent": "Or we can simply do this by declaring Thetic Santee Thanks Prime.",
                    "label": 0
                },
                {
                    "sent": "A prime to be the same if we think that the resulting distribution will be the same.",
                    "label": 0
                },
                {
                    "sent": "Another simple case would be, what if we know that the dynamics are factored as in having dynamic Bayesian network.",
                    "label": 1
                },
                {
                    "sent": "OK, so there would normally be some conditional independence and then the powers of the dynamic vision network would be that for each variable there's a distribution given its parents and then so for each pair of that distribution we would create a theater like this and then the benefit is that we've got exponentially fewer partners.",
                    "label": 0
                },
                {
                    "sent": "We can also encode things like sparsity, which I don't have on this slide, and there's a vast literature out there in statistics and probabilities about picking or fitting distributions given some constraints or other types of information.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this was to give you an idea of prior knowledge.",
                    "label": 0
                },
                {
                    "sent": "Now let's move on to.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Policy optimization OK so.",
                    "label": 0
                },
                {
                    "sent": "Just a quick recall.",
                    "label": 0
                },
                {
                    "sent": "This is traditional RL.",
                    "label": 0
                },
                {
                    "sent": "OK, we're here again.",
                    "label": 0
                },
                {
                    "sent": "I'm assuming that X includes States and rewards.",
                    "label": 0
                },
                {
                    "sent": "OK, so we've got our states rewards together here with actions and we're missing the transition probabilities.",
                    "label": 1
                },
                {
                    "sent": "Now, if we're taking a Bayesian approach, it turns out that there's a natural formulation as a palm DP or partially observable Markov decision process.",
                    "label": 0
                },
                {
                    "sent": "And the idea here is simply that we're going to extend our notion of state to now also include the unknowns Theta.",
                    "label": 0
                },
                {
                    "sent": "And the reason why I'm calling it a partially observable Markov decision process is simply because Theta is not observable.",
                    "label": 0
                },
                {
                    "sent": "So X here the underlying states of physical states in the rewards.",
                    "label": 0
                },
                {
                    "sent": "So far we've assumed that their observable, but Theta is not.",
                    "label": 0
                },
                {
                    "sent": "But still, these are the things that are really defining our model in the States.",
                    "label": 0
                },
                {
                    "sent": "So I mean, we could simply bundle them together and have this now become our new state space.",
                    "label": 0
                },
                {
                    "sent": "OK. Now you may ask, well, why do we want to do that?",
                    "label": 0
                },
                {
                    "sent": "The neat thing is because now the transition probabilities.",
                    "label": 1
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Are going to be known.",
                    "label": 0
                },
                {
                    "sent": "So you may wonder, like how did we go from here simple model to a more complex one where we actually know everything.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is not magic.",
                    "label": 0
                },
                {
                    "sent": "Let's have a look more carefully at the transition probabilities.",
                    "label": 1
                },
                {
                    "sent": "So from a graphical model, let's see.",
                    "label": 0
                },
                {
                    "sent": "We've got a base net we can represent transition properties as follows.",
                    "label": 0
                },
                {
                    "sent": "So there's our current state.",
                    "label": 0
                },
                {
                    "sent": "There's the next state, there's the action, right?",
                    "label": 0
                },
                {
                    "sent": "So the current state and action influence the next state now.",
                    "label": 0
                },
                {
                    "sent": "If we include feta into the state, but we keep this factor.",
                    "label": 0
                },
                {
                    "sent": "So now here's X and feta, then experiment Theta prime X prime depends on XNA as before, but also on Theta because we've defined earlier Theta xax prime to be this unknown probability.",
                    "label": 0
                },
                {
                    "sent": "But now if I conditioned on Theta, that simply means like if I knew what theater was, then I would know.",
                    "label": 0
                },
                {
                    "sent": "With what probability X prime would be reached so?",
                    "label": 0
                },
                {
                    "sent": "By using this trick, then we actually know.",
                    "label": 0
                },
                {
                    "sent": "This conditional distribution.",
                    "label": 0
                },
                {
                    "sent": "Now we also have to have a conditional distribution for Theta, so here's a simple one.",
                    "label": 0
                },
                {
                    "sent": "We're just going to keep Theta constant, so essentially the model doesn't change.",
                    "label": 0
                },
                {
                    "sent": "Obviously, we could have more complicated models that include change overtime, but for now it's.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stick with that.",
                    "label": 0
                },
                {
                    "sent": "There's another formulation so because partial observable Markov decision processes may sound like complicated and give some people some headaches, and we really like just Markov decision processes.",
                    "label": 0
                },
                {
                    "sent": "Well, if you really just like that, then we can actually think of Palm DP's as simply belief MDP's.",
                    "label": 0
                },
                {
                    "sent": "So in the literature, sometimes people refer to these models as palm DP.",
                    "label": 0
                },
                {
                    "sent": "Sometimes this belief and EPS there really just the same.",
                    "label": 0
                },
                {
                    "sent": "The difference is that instead of using Theta to augment the States, let's segment the state with the actual distribution over Theta.",
                    "label": 0
                },
                {
                    "sent": "And that distribution is known to the agent because the agent built it.",
                    "label": 0
                },
                {
                    "sent": "FIFA is unknown, but B is.",
                    "label": 0
                },
                {
                    "sent": "So now we've got something that's fully observable.",
                    "label": 0
                },
                {
                    "sent": "And we still know completely the transition properties.",
                    "label": 0
                },
                {
                    "sent": "So now we've gone to a model that's again just a Markov decision process, but with these reformulation.",
                    "label": 0
                },
                {
                    "sent": "We know all of the parts.",
                    "label": 0
                },
                {
                    "sent": "Yes yes OK so yeah here this should be experiment V prime good.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so how did how can we build this one so?",
                    "label": 0
                },
                {
                    "sent": "Pompey looks like this.",
                    "label": 0
                },
                {
                    "sent": "The belief MVP.",
                    "label": 0
                },
                {
                    "sent": "We replace theater by B.",
                    "label": 0
                },
                {
                    "sent": "And now again, X prime will depend on X and a.",
                    "label": 0
                },
                {
                    "sent": "It will also depend on B.",
                    "label": 0
                },
                {
                    "sent": "The difference is that now we're going to integrate over all the thetas.",
                    "label": 0
                },
                {
                    "sent": "OK, so we've got an integral, so it's slightly more complicated, but it's perfectly valid.",
                    "label": 0
                },
                {
                    "sent": "And then for the property distributions of B prime, while this one will be also more complicated.",
                    "label": 0
                },
                {
                    "sent": "So instead of having theater remain constant as we had before, now be prime is going to be the posterior that we would normally get.",
                    "label": 0
                },
                {
                    "sent": "By applying Bayes rule and then so the property of getting that belief will be one and 0 otherwise.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so to summarize, then classic Carel.",
                    "label": 0
                },
                {
                    "sent": "Would normally tackle this optimization problem where we want to find a policy, let's say using Bellman's equation that has the highest value and this value would depend only on the current state X.",
                    "label": 0
                },
                {
                    "sent": "Now if we take Abbasian view and let's say that we take the belief MDP formulation well, now my states are really X&B.",
                    "label": 0
                },
                {
                    "sent": "But I can still formulate.",
                    "label": 0
                },
                {
                    "sent": "Bellman equation that is in terms of X&B.",
                    "label": 0
                },
                {
                    "sent": "So the algorithms that we're going to see next are really just going to try to work with this equation as opposed to that one, and the nice benefit is that now we know this transition probability, Whereas here this is unknown.",
                    "label": 0
                },
                {
                    "sent": "The second line is benefit is that here?",
                    "label": 0
                },
                {
                    "sent": "Well, if we only condition the value on X, there's a problem because we don't know what kind of uncertainty we've got about the model.",
                    "label": 0
                },
                {
                    "sent": "So when it comes time to decide between exploring and exploiting.",
                    "label": 0
                },
                {
                    "sent": "We don't know what to do, so traditionally people would talk with heuristics like epsilon, greedy, Boltzmann and others, whereas here B actually encodes this uncertainties.",
                    "label": 0
                },
                {
                    "sent": "We've got a full distribution over all of the possible models, so therefore we know where we should perhaps explore, or at least we know in principle, so we'll see later how this will really help us.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so actually just to clarify, what is the exploration exploitation tradeoff?",
                    "label": 0
                },
                {
                    "sent": "Is this following dilemma where when it's time to pick an action should we pick it such that it maximizes the immediate reward?",
                    "label": 0
                },
                {
                    "sent": "That's exploitation?",
                    "label": 0
                },
                {
                    "sent": "Or to maximize information gain that's exploration?",
                    "label": 1
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Again, that's because we don't know what are the transition properties.",
                    "label": 0
                },
                {
                    "sent": "So if I if I try and action just to see what will happen, then it gives me some information to update.",
                    "label": 0
                },
                {
                    "sent": "What is that transition property and therefore in the future I'll be able to perhaps devise a better policy based on that, but in the mean time for the immediate reward I may actually pay a high cost of that action was not a good one, right?",
                    "label": 0
                },
                {
                    "sent": "So there's this natural tradeoff between those two.",
                    "label": 0
                },
                {
                    "sent": "Now to use Sutton's way of talking.",
                    "label": 0
                },
                {
                    "sent": "I would say that while this is the wrong question to ask, so when solving a problem we have to ask the right question and actually Mohammed when explaining earlier what is RL, basically told us what is that question or at least what we should be doing when we're trying to optimize a policy.",
                    "label": 1
                },
                {
                    "sent": "Is we just to maximize expected total rewards?",
                    "label": 0
                },
                {
                    "sent": "So the value function as Mohamed explained earlier, is really just the expected sum of all the rewards that we're going to get in the future.",
                    "label": 0
                },
                {
                    "sent": "So at one level, if this is really all we care about, then I'll argue that if we can find a policy that maximizes this, then why do we care whether we're exploiting our exploit, what we're exploring or exploiting?",
                    "label": 0
                },
                {
                    "sent": "Right it it doesn't really matter how we classify the actions as being, exploring, exploiting and we don't have to worry about what we're really doing as long as we just maximizing here the rewards.",
                    "label": 0
                },
                {
                    "sent": "So I'll simply claim then that we can get an optimal exploration exploitation tradeoff as long as we find a policy that has highest reward.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Now, looking at the formulation to belief MDP formulation here, I'll claim that as long as we can solve this, find an optimal policy, then we've taken care of the exploration exploitation tradeoff.",
                    "label": 0
                },
                {
                    "sent": "And so that's one really nice property of beige and take approach is that in principle.",
                    "label": 0
                },
                {
                    "sent": "That's taken care of.",
                    "label": 0
                },
                {
                    "sent": "Now obviously there's going to be some approximation, so we're not really going to find an optimal balance, but in principle we know what to do.",
                    "label": 0
                },
                {
                    "sent": "OK so I'm gonna what I'm going to do next is tell you a bit about several approaches that people have devices over the years and then we'll see some of the challenges about doing optimizing policies and invasion way and then at the end I'll talk about one of my algorithm as well.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so in 1999 Deardon Friedman and Android proposed a technique that was based on a myopic notion of value of information, and the idea is that again, if we think of exploring, exploiting if we could simply augment the traditional value function that's in terms of the current state, not be just the current state, augment it with some value of the information.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So here, what if we could have meant the Q function with another function, let's call it value of.",
                    "label": 0
                },
                {
                    "sent": "Actually, here they called myopic value of perfect information, and then we're simply going to pick the action that maximizes this somewhere.",
                    "label": 1
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "This is obviously the reward would get by simply exploiting what we know so far.",
                    "label": 1
                },
                {
                    "sent": "And here this would be the value of exploring.",
                    "label": 0
                },
                {
                    "sent": "And OK, there's some reasons why they didn't get the value of information precisely is because computing this is actually as hard as solving the original problem.",
                    "label": 1
                },
                {
                    "sent": "OK, So what they do is competent myopic version, which means we're only look at the value of the information for a short period of time instead of the infinite horizon.",
                    "label": 0
                },
                {
                    "sent": "And then they also computed an upper bound by.",
                    "label": 0
                },
                {
                    "sent": "Looking at the perfect information, so if by trying an action you would find out precisely what would be the Q value of the resulting state action pair, then you will get perfect information, so they use that as an upper.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, another approach.",
                    "label": 0
                },
                {
                    "sent": "This one by strands, is actually based on what is known as the Thomson sampling, and the idea is very simple, being beige and we have Theta Anna distribution over Theta.",
                    "label": 0
                },
                {
                    "sent": "So when we want to pick an action, how about we simply?",
                    "label": 0
                },
                {
                    "sent": "We simply pick a theater from that distribution and then pick the action that's best for that theater.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We can see here that by sampling a Theta we're going to do some exploration, because we're going to consider different models and then by picking the best action for that model we're exploiting.",
                    "label": 1
                },
                {
                    "sent": "So it does provide some kind of heuristic for.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Balancing the two.",
                    "label": 0
                },
                {
                    "sent": "And here I've included a quick slide showing some results that this is taken from strands paper, and I've highlighted in green the values obtained by the beige and techniques.",
                    "label": 0
                },
                {
                    "sent": "So here are three of them, so this is trans technique.",
                    "label": 0
                },
                {
                    "sent": "This is the ordinance technique, and this is another technique I believe, by Leslie cabling earlier.",
                    "label": 0
                },
                {
                    "sent": "And then we've got Q learning with the same uniform exploration Q learning with Boltzmann exploration.",
                    "label": 0
                },
                {
                    "sent": "And here is some eurasec technique that as trans had developed earlier and we can see that.",
                    "label": 0
                },
                {
                    "sent": "The vision approaches tend to earn higher reward at least faster, so here are phases.",
                    "label": 0
                },
                {
                    "sent": "1000 time steps and this is the next 1000 time steps.",
                    "label": 0
                },
                {
                    "sent": "And then I've simply added up all the rewards so we can see that the actually do a better job at earning rewards faster, which is the whole point here of balancing the exploration exploitation tradeoff.",
                    "label": 0
                },
                {
                    "sent": "OK, now those techniques are not optimal because they are.",
                    "label": 0
                },
                {
                    "sent": "Approximate OK, and then we can see in fact that this heuristic one is actually doing quite well.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, one more technique basean sparse sampling.",
                    "label": 1
                },
                {
                    "sent": "So another way of tackling Markov decision processes and polypes would be to simply build some look ahead search tree where we alternate between maximizing rewards by picking the best action and taking an expectation with respect to the reachable states.",
                    "label": 0
                },
                {
                    "sent": "And then we can simply expand history, evaluate at the bottom of the Leafs by taking let's see the mean model and then propagating all the rewards up.",
                    "label": 1
                },
                {
                    "sent": "OK, so Kerns was the one who had several years ago, I believe.",
                    "label": 0
                },
                {
                    "sent": "Pioneered such approaches based on expanding this tree in a stochastic form and then one lizard bowling in Sherman's in 2005 adopted adapted this technique to.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Asian reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "I'm very quickly another one, this one based on policy gradient.",
                    "label": 0
                },
                {
                    "sent": "The idea is that well, if we can model version reinforcement learning as a palm DP, one of the common approaches is to represent the policy by fine state controller that would have the following parameters.",
                    "label": 0
                },
                {
                    "sent": "These are continuous parameters, so for each node of the control, what action is being picked and what will be the next node reached, and then you can simply do gradient dissent too.",
                    "label": 0
                },
                {
                    "sent": "Minimize costs, otherwise maximize rewards.",
                    "label": 0
                },
                {
                    "sent": "K.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another one again.",
                    "label": 0
                },
                {
                    "sent": "If we think of this as a palm DP, while the main problem is actually that Theta is a continuous variable, but we've got algorithms for Palm DP's that work with discrete variables.",
                    "label": 0
                },
                {
                    "sent": "So what if we simply discretize Theta and then apply your favorite palm DP algorithm so it works except that the discretization will lead to a grid that exponentially large in the number of dimensions?",
                    "label": 1
                },
                {
                    "sent": "OK, so to summarize, all of these techniques.",
                    "label": 0
                },
                {
                    "sent": "OK, they try to work with this version of Bellman's equation and the main difficulties really are that the belief or otherwise the unknown model C to our continuous.",
                    "label": 0
                },
                {
                    "sent": "So solving this is not obvious.",
                    "label": 0
                },
                {
                    "sent": "And then you have to resort to approximations and then a lot of those techniques work with value functions an having B or C to continuous means that.",
                    "label": 0
                },
                {
                    "sent": "We have here a value function that's going to be quite difficult to represent, and in fact there's an interesting question.",
                    "label": 0
                },
                {
                    "sent": "What is the characterization of that funk?",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to tell you now, this is a paper that appeared last year in 2006 by myself and some coauthors.",
                    "label": 0
                },
                {
                    "sent": "I'm going to tell you how we can represent the value function exactly in a power trick way, and then I'll tell you about one algorithm for exploiting.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This formulation.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK, the main idea here is that we want to come up with a parrot risation and it turns out that.",
                    "label": 0
                },
                {
                    "sent": "The value function, the optimal value function for state X and model Theta is really the Max of a bunch of polynomials in Theta.",
                    "label": 1
                },
                {
                    "sent": "The proof is very simple, so prove by induction.",
                    "label": 0
                },
                {
                    "sent": "So I've defined earlier balance equation in terms of X&B.",
                    "label": 0
                },
                {
                    "sent": "But now let's actually consider another version of it in terms of Theta, where this value function relates to this one.",
                    "label": 1
                },
                {
                    "sent": "According to this integration.",
                    "label": 0
                },
                {
                    "sent": "So now we've got Bellman's equation with respect to Theta here and we want to show that if we start here with the value function being a Max of polynomials, then after one Bellman backup we're going to have the the value function that's going to be still the Max of polynomials.",
                    "label": 1
                },
                {
                    "sent": "And then if we just keep on doing this over and over, it will always remain in the class of Max of polynomials.",
                    "label": 0
                },
                {
                    "sent": "So if this is a Max of polynomials.",
                    "label": 0
                },
                {
                    "sent": "Gamma is the discount factor, which is a constant.",
                    "label": 0
                },
                {
                    "sent": "The reward is also a constant.",
                    "label": 0
                },
                {
                    "sent": "We don't know where the transition probabilities are, however, we've modeled this with data, So what we're really doing is simply multiplying Theta by this, and then if I pull out the Max and bring it here with this Max and then I'm left with an expectation of Theta times K plus gamma times a polynomial which is itself still a polynomial.",
                    "label": 0
                },
                {
                    "sent": "So we actually know then what is the penetration and then the next question is, well, how could we exploit that to devise an effective algorithm?",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, actually before I go to an algorithm, you may wonder what happens if we have a partially observable model.",
                    "label": 1
                },
                {
                    "sent": "In other words, what if X is not fully observable?",
                    "label": 1
                },
                {
                    "sent": "It turns out that the theorem still holds, so we're still going to have a Max of polynomials and then the beliefs are going to be mixtures of Dirichlet instead of being one there.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Flat.",
                    "label": 0
                },
                {
                    "sent": "OK, on this slide I've got one algorithm called B tool which is really a point based value iteration approach that's been adapted for beige and RL, but that in general the main difficulty here to tackle is the fact that Theta is continuous.",
                    "label": 0
                },
                {
                    "sent": "OK, so we do have techniques for point based on iteration that works with continuous spaces, and so we adapted one here.",
                    "label": 0
                },
                {
                    "sent": "So this was work done.",
                    "label": 0
                },
                {
                    "sent": "Before with Nichols glasses.",
                    "label": 0
                },
                {
                    "sent": "And so we adopted this technique to beige and reinforcement learning, and it's very simple.",
                    "label": 0
                },
                {
                    "sent": "So we sample a bunch of belief points and then we perform Bellman backups at each one of those points.",
                    "label": 1
                },
                {
                    "sent": "Performing in Belmont back up is really just.",
                    "label": 0
                },
                {
                    "sent": "Computing this equation, but I'm going to do it in 2 three steps.",
                    "label": 0
                },
                {
                    "sent": "The first step will be to find what is the best polynomial at the next time step by solving this.",
                    "label": 0
                },
                {
                    "sent": "Now this may look very complicated because OK, I've got an integral polynomial multiplied by my belief and so on, but the belief is is a monomial that's a polynomial, so that's itself a polynomial integral of polynomial is very easy to compute, so we can do all of this in close form.",
                    "label": 0
                },
                {
                    "sent": "Then once I've got that, I can find what is the best action for my current belief.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We have to solve this, which again may look difficult but same trick.",
                    "label": 0
                },
                {
                    "sent": "All of this will boil down to a polynomial with an integral that I can solve exactly.",
                    "label": 0
                },
                {
                    "sent": "And so I get my best action and then I can build my new polynomial by taking that best action with the best polynomials at the next step.",
                    "label": 0
                },
                {
                    "sent": "And so it gives me my new polynomial OK?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so a star could be there could be several optimal actions, so I'll just pick one of them, or otherwise I could pick any convex combination of them.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is all nice, however, there's some computational issues so.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the last step here.",
                    "label": 0
                },
                {
                    "sent": "What I didn't tell you is that the this polynomial is going to be much larger than the ones we.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Started with.",
                    "label": 0
                },
                {
                    "sent": "So if you look carefully.",
                    "label": 0
                },
                {
                    "sent": "So if I think of a polynomial, I simply being some of monomials and I represent polynomials.",
                    "label": 0
                },
                {
                    "sent": "By their components.",
                    "label": 0
                },
                {
                    "sent": "So each monomial separately turns out that when I do the computation, those two sons are going to get together and actually this should be X prime, I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so these should be all X primes instead of S primes.",
                    "label": 0
                },
                {
                    "sent": "Then I get a sum that's going to be I * S An.",
                    "label": 0
                },
                {
                    "sent": "So the number of monomial's will grow by a factor of S or actually effective Exelon number of states.",
                    "label": 0
                },
                {
                    "sent": "So overtime the number of monomials will grow exponentially in the number of states.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we have to deal with this, so we're going to do an approximation.",
                    "label": 0
                },
                {
                    "sent": "The idea is very simple.",
                    "label": 0
                },
                {
                    "sent": "We're going to project our polynomial onto a set of basis functions which are going to be monomial.",
                    "label": 1
                },
                {
                    "sent": "So essentially we're going to compress our polynomial that has exponentially many monomials onto smaller set of monomials.",
                    "label": 0
                },
                {
                    "sent": "Now to find the best linear combination of basis monomials.",
                    "label": 1
                },
                {
                    "sent": "We can use an optimization criterion that could be simply minimizing some Ln norm.",
                    "label": 0
                },
                {
                    "sent": "And again, this looks complicated, but if you take a Euclidean norm.",
                    "label": 0
                },
                {
                    "sent": "Turns out that this is as simple as solving a linear system of equations.",
                    "label": 1
                },
                {
                    "sent": "Ax equals B or a would be this integral.",
                    "label": 0
                },
                {
                    "sent": "So each entry in A is the result of this integral.",
                    "label": 0
                },
                {
                    "sent": "But these things are polynomials or monomials, so we can solve these integrals exactly and same thing for B.",
                    "label": 0
                },
                {
                    "sent": "And then C corresponds to X, so X corresponds to C, yeah.",
                    "label": 0
                },
                {
                    "sent": "So I can solve this readily without any.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Problem.",
                    "label": 0
                },
                {
                    "sent": "So this gives me a way to project down, but one important question is well.",
                    "label": 0
                },
                {
                    "sent": "What would be my basis functions?",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 1
                },
                {
                    "sent": "One way to pick basis functions to look at.",
                    "label": 0
                },
                {
                    "sent": "Equation we use to compute those polynomials as well as.",
                    "label": 0
                },
                {
                    "sent": "The Bayes update for beliefs and the Bayes updates we take the problem multi by Theta.",
                    "label": 0
                },
                {
                    "sent": "Annabelle man back up we take a polynomial an we multiply by Sita.",
                    "label": 0
                },
                {
                    "sent": "So presumably the monomials that we get as beliefs are going to be the same monomials that are part of the polynomials for the value function.",
                    "label": 0
                },
                {
                    "sent": "So we can actually do.",
                    "label": 0
                },
                {
                    "sent": "Monitoring very easily so we can collect in this way abunch of monomials that we're going to use as our basis functions.",
                    "label": 1
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so some of the properties of this algorithm are that.",
                    "label": 0
                },
                {
                    "sent": "We can do the optimization of policy at some sample belief points, and then we can do this offline, so without interacting with any environment because with the formulation we know all of the transition probabilities OK.",
                    "label": 0
                },
                {
                    "sent": "The learning really happens when we do the belief monitoring.",
                    "label": 1
                },
                {
                    "sent": "OK, so really we computer policy here, that's going to be good for the entire space of beliefs and then at runtime we do believe monitoring and then we check for our current belief.",
                    "label": 0
                },
                {
                    "sent": "What would be the best action based on what we've already precomputed here?",
                    "label": 0
                },
                {
                    "sent": "So the nice thing is that we can actually run this in real time because belief of this really takes us a fraction of a second.",
                    "label": 1
                },
                {
                    "sent": "The other nice properties is that we can optimize the exploration exploitation tradeoff naturally, and then we can also encode prior knowledge into our prior disadvantages that while the policy is not necessarily going to be good at all, belief points because I've done an approximation here where I've sampled some belief points and I've also done a projection that approximates my exponentially large polynomials into small.",
                    "label": 1
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other ones Yep.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "So first step here.",
                    "label": 0
                },
                {
                    "sent": "I'm sure there are many ways of picking belief points the way we do it is that we start with the initial belief and then we simply do forward lookahead, which is problem dependent.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So to do this for look ahead, we would use the transition probabilities formulated with the belief MDP formulation.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "Simply collect the beliefs that could be reached in this way.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so quick empirical evaluation.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, so we tested the beta algorithm or compared it against two you Ristic.",
                    "label": 0
                },
                {
                    "sent": "One is the exploit touristic.",
                    "label": 0
                },
                {
                    "sent": "So what if we don't do any exploration?",
                    "label": 0
                },
                {
                    "sent": "We simply greedily select the best action of the mean model at each time step.",
                    "label": 1
                },
                {
                    "sent": "OK, so this really involves just solving an MDP at each time step, which can actually take awhile, but on the other hand it's very simple to do.",
                    "label": 0
                },
                {
                    "sent": "Or while we could use the same technique that was proposed earlier by Pino, which is to discretize Theta and whenever we can do the computation, that'll be great.",
                    "label": 0
                },
                {
                    "sent": "But if the number of states gets too large, then we may not be able to provide.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some actions.",
                    "label": 0
                },
                {
                    "sent": "OK, so beetle is here with its runtime.",
                    "label": 0
                },
                {
                    "sent": "We've got the discrete Palm DP version and the exploit here is sick, and here's an upper bound, which is what if we just executed the optimal policy as if in Oracle, just gave it to us?",
                    "label": 0
                },
                {
                    "sent": "OK, and you will see that beetle does well when the number of free powders is fairly small, but not so well when the number of free partners is large.",
                    "label": 0
                },
                {
                    "sent": "And this is because of two reasons.",
                    "label": 0
                },
                {
                    "sent": "First, we actually used.",
                    "label": 0
                },
                {
                    "sent": "The prior that was uninformed, so our uniform priors.",
                    "label": 0
                },
                {
                    "sent": "So if we've got lots of partners, that means we have to take a while to learn.",
                    "label": 0
                },
                {
                    "sent": "And then also, this is where the approximations are going to be.",
                    "label": 0
                },
                {
                    "sent": "The most crude in the projection.",
                    "label": 0
                },
                {
                    "sent": "So Beetle doesn't do so well.",
                    "label": 0
                },
                {
                    "sent": "And then same thing here.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also tested what will happen if we change the prior to be more informative.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that we increase our confidence.",
                    "label": 0
                },
                {
                    "sent": "So here the parameter K is simply the magnitude of those exponents under Dirichlet.",
                    "label": 0
                },
                {
                    "sent": "OK, so the larger that means, the more confident we are, and then it would be confidence in terms of the true underlying model.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can see that as the confidence increases then the policies that are obtained are better and these are the.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that all expected to reward for 1000 time steps.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "So we've covered para knowledge policy optimization, so now let's discuss a little bit these techniques, and then we'll talk about Bayesian approaches for us.",
                    "label": 1
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There are all variants.",
                    "label": 0
                },
                {
                    "sent": "OK, so in my discussion here I'll I'll come back to priors.",
                    "label": 0
                },
                {
                    "sent": "I'll talk again about online learning as well as.",
                    "label": 1
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Active learning.",
                    "label": 0
                },
                {
                    "sent": "So the first question that may come up to your mind is, well, would it be better to learn everything from scratch without having to specify any prior?",
                    "label": 1
                },
                {
                    "sent": "OK, a lot of people really, you know are.",
                    "label": 0
                },
                {
                    "sent": "Concern about priors and I'm gonna argue that no, we shouldn't be worried.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 1
                },
                {
                    "sent": "My main argument is going to be that there's no such thing as reinforcement learning without any prior.",
                    "label": 0
                },
                {
                    "sent": "If you think about it, every learning algorithm, this is a well known fact of machine learning has a learning bias.",
                    "label": 0
                },
                {
                    "sent": "So what does basean reinforcement learning do?",
                    "label": 0
                },
                {
                    "sent": "It puts the bias explicitly in the prior, and it forces you to select it.",
                    "label": 0
                },
                {
                    "sent": "Whereas other oral techniques all have an implicit bias.",
                    "label": 0
                },
                {
                    "sent": "But you don't get to pick it, so it feels OK.",
                    "label": 0
                },
                {
                    "sent": "But it's there.",
                    "label": 0
                },
                {
                    "sent": "And sometimes you actually have to pick it, so if you do policy search, sometimes you have to pick apart relation for the policy space and that's in itself bias for the hypothesis space.",
                    "label": 0
                },
                {
                    "sent": "If you're doing value function approximation often you'll have to pick a function approximator, and that too is.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bias.",
                    "label": 0
                },
                {
                    "sent": "So just to clarify this.",
                    "label": 0
                },
                {
                    "sent": "Oh yes.",
                    "label": 0
                },
                {
                    "sent": "Selecting the Bay.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is function.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "Right, so OK in my beetle algorithm I had to select basis functions.",
                    "label": 0
                },
                {
                    "sent": "When I did the projection.",
                    "label": 0
                },
                {
                    "sent": "So yes, you could argue that this is also another bias, but I guess that's a bias that comes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, in the same way from the fact that I'm doing an approximation.",
                    "label": 0
                },
                {
                    "sent": "Yes, yeah.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so OK.",
                    "label": 0
                },
                {
                    "sent": "If we look at function approximation.",
                    "label": 0
                },
                {
                    "sent": "Either yeah, if we use basis functions that your bias or otherwise.",
                    "label": 0
                },
                {
                    "sent": "If you use like a neural network, radial basis functions.",
                    "label": 1
                },
                {
                    "sent": "Whatever day introduces, well, a bias, and we know that for reinforcement learning a lot of those function approximators if you're not careful, there could be some divergance.",
                    "label": 0
                },
                {
                    "sent": "And one possible cause here could could be that.",
                    "label": 1
                },
                {
                    "sent": "All of those.",
                    "label": 0
                },
                {
                    "sent": "They have an implicit assumption about how the value is going to generalize across states, so that when you do an update for one state, then it will affect the neighboring states.",
                    "label": 0
                },
                {
                    "sent": "But if those updates are not.",
                    "label": 0
                },
                {
                    "sent": "Like if there's no reason why the neighboring state should be changed as.",
                    "label": 0
                }
            ]
        }
    }
}