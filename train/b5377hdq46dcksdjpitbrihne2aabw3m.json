{
    "id": "b5377hdq46dcksdjpitbrihne2aabw3m",
    "title": "Continuous Relaxations for Discrete Hamiltonian Monte Carlo",
    "info": {
        "author": [
            "Yichuan Zhang, School of Informatics, University of Edinburgh"
        ],
        "published": "Jan. 14, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Graphical Models"
        ]
    },
    "url": "http://videolectures.net/machine_zhang_continuous_relaxations/",
    "segmentation": [
        [
            "Hello, I run my introduced through our work so it is well known discrete influence can be difficult.",
            "So in this work we try to take advantage of gradient information to do more efficient inference on discrete work.",
            "It is inspired by the continuous relaxation in the optimization algorithm.",
            "But now here we apply this idea for.",
            "For approximately probabilistic inference algorithm.",
            "So first we transform the discrete undirected graphical model into the fully continuous one.",
            "Then we use gradient based Hamiltonian Monte Carlo method.",
            "So you can here we use a great information to generate the sample and do inference based on the content examples."
        ],
        [
            "So a little bit more background about the Gaussian integral trick.",
            "This is why we use to transform the discrete one into the continuous model.",
            "So first consider as is a vector of binary random variables, it is defined as a standard bottom machine and X is continuous oxygen variable condition.",
            "On the discrete one, as is a Gaussian distributed.",
            "You can see it's mean depends on discrete variable and hear the.",
            "Matrix D is diagonal constant, so W + D it is a positive definite matrix by a little bit algebra we can workout the joint distribution of Accent South.",
            "We can see here.",
            "Actually the quadratic term events or it should be as quadratic form.",
            "Here is cancel out.",
            "So as it turns out to be the discrete variable conditionally independent given X.",
            "So we can easily sorry.",
            "So we can easily marginalized the discrete variable.",
            "It will be end up with this scary marginal continuous model.",
            "Here, this setup corresponds to last graphical representation more general.",
            "Form of this trick can give more complicated form, but we focus on this one district, also considered by some other previous works."
        ],
        [
            "Here, because we're trying to sample the continuous marginal, actually it can be challenging because essentially it is a mixture of 2, two and Gaussian distributions, and each of them have different means, so it's supposed to have multimodal landscape, but actually look at here we can interpret as poster distribution.",
            "Here is like a likelihood function.",
            "This is the Gaussian prior if we use.",
            "This multi it corresponds to the strong Gaussian prior, so it may more likely end up with single mode continuous marginal.",
            "In this paper we showed up this theorem.",
            "So Lock PX is log concave if only if the eigen spectrum of double plastic matrix is narrow.",
            "Here is some demo we show by change the Eigen gap.",
            "We can end up with a Gaussian shaped marginal.",
            "Here's some."
        ],
        [
            "Result on synthetic data that blowers better.",
            "We sample the synthetic model from two different scheme.",
            "Why is normal 1?",
            "One is the first one.",
            "The first one supposed to be more difficult.",
            "And then we can see DHMC at massive definitely outperformed the deep sampling, but for the frustrated one, it may perform worse specific setup.",
            "So it matched our anticipation.",
            "It is like the frustrated MRF is more likely to give non concave log PX for more details and results please come to our poster, thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hello, I run my introduced through our work so it is well known discrete influence can be difficult.",
                    "label": 0
                },
                {
                    "sent": "So in this work we try to take advantage of gradient information to do more efficient inference on discrete work.",
                    "label": 0
                },
                {
                    "sent": "It is inspired by the continuous relaxation in the optimization algorithm.",
                    "label": 0
                },
                {
                    "sent": "But now here we apply this idea for.",
                    "label": 0
                },
                {
                    "sent": "For approximately probabilistic inference algorithm.",
                    "label": 0
                },
                {
                    "sent": "So first we transform the discrete undirected graphical model into the fully continuous one.",
                    "label": 0
                },
                {
                    "sent": "Then we use gradient based Hamiltonian Monte Carlo method.",
                    "label": 1
                },
                {
                    "sent": "So you can here we use a great information to generate the sample and do inference based on the content examples.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So a little bit more background about the Gaussian integral trick.",
                    "label": 0
                },
                {
                    "sent": "This is why we use to transform the discrete one into the continuous model.",
                    "label": 0
                },
                {
                    "sent": "So first consider as is a vector of binary random variables, it is defined as a standard bottom machine and X is continuous oxygen variable condition.",
                    "label": 0
                },
                {
                    "sent": "On the discrete one, as is a Gaussian distributed.",
                    "label": 0
                },
                {
                    "sent": "You can see it's mean depends on discrete variable and hear the.",
                    "label": 0
                },
                {
                    "sent": "Matrix D is diagonal constant, so W + D it is a positive definite matrix by a little bit algebra we can workout the joint distribution of Accent South.",
                    "label": 0
                },
                {
                    "sent": "We can see here.",
                    "label": 0
                },
                {
                    "sent": "Actually the quadratic term events or it should be as quadratic form.",
                    "label": 0
                },
                {
                    "sent": "Here is cancel out.",
                    "label": 0
                },
                {
                    "sent": "So as it turns out to be the discrete variable conditionally independent given X.",
                    "label": 0
                },
                {
                    "sent": "So we can easily sorry.",
                    "label": 0
                },
                {
                    "sent": "So we can easily marginalized the discrete variable.",
                    "label": 0
                },
                {
                    "sent": "It will be end up with this scary marginal continuous model.",
                    "label": 0
                },
                {
                    "sent": "Here, this setup corresponds to last graphical representation more general.",
                    "label": 0
                },
                {
                    "sent": "Form of this trick can give more complicated form, but we focus on this one district, also considered by some other previous works.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here, because we're trying to sample the continuous marginal, actually it can be challenging because essentially it is a mixture of 2, two and Gaussian distributions, and each of them have different means, so it's supposed to have multimodal landscape, but actually look at here we can interpret as poster distribution.",
                    "label": 0
                },
                {
                    "sent": "Here is like a likelihood function.",
                    "label": 0
                },
                {
                    "sent": "This is the Gaussian prior if we use.",
                    "label": 1
                },
                {
                    "sent": "This multi it corresponds to the strong Gaussian prior, so it may more likely end up with single mode continuous marginal.",
                    "label": 0
                },
                {
                    "sent": "In this paper we showed up this theorem.",
                    "label": 0
                },
                {
                    "sent": "So Lock PX is log concave if only if the eigen spectrum of double plastic matrix is narrow.",
                    "label": 1
                },
                {
                    "sent": "Here is some demo we show by change the Eigen gap.",
                    "label": 0
                },
                {
                    "sent": "We can end up with a Gaussian shaped marginal.",
                    "label": 0
                },
                {
                    "sent": "Here's some.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Result on synthetic data that blowers better.",
                    "label": 0
                },
                {
                    "sent": "We sample the synthetic model from two different scheme.",
                    "label": 0
                },
                {
                    "sent": "Why is normal 1?",
                    "label": 0
                },
                {
                    "sent": "One is the first one.",
                    "label": 0
                },
                {
                    "sent": "The first one supposed to be more difficult.",
                    "label": 0
                },
                {
                    "sent": "And then we can see DHMC at massive definitely outperformed the deep sampling, but for the frustrated one, it may perform worse specific setup.",
                    "label": 0
                },
                {
                    "sent": "So it matched our anticipation.",
                    "label": 0
                },
                {
                    "sent": "It is like the frustrated MRF is more likely to give non concave log PX for more details and results please come to our poster, thanks.",
                    "label": 1
                }
            ]
        }
    }
}