{
    "id": "3lmt5ax4hu6x572kduf3au7rcnssjcx6",
    "title": "Sparsity in Grammar Induction",
    "info": {
        "author": [
            "Jennifer A. Gillenwater, University of Pennsylvania"
        ],
        "published": "Jan. 19, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Programming Languages"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops09_gillenwater_sgi/",
    "segmentation": [
        [
            "Can you guys only hear me?",
            "Yeah, OK, great.",
            "So I'm Jenny and this is joint work with Kuzman Ganchev, Jewel Grassa, Ben Tasker and Fernando Parra.",
            "So I'm gonna talk about sparsity, independency, grammar induction.",
            "And."
        ],
        [
            "To begin with, before I talk about sparsity, I'm just going to introduce the model we're using.",
            "This is the dependency model with balance, which was introduced by a client in Manning in 2004, and.",
            "The way it works is that it assumes that all sentences in the corpus are tagged with part of speech tags.",
            "So for this sentence here we have noun, verb, adjective, noun and the model ignores the actual words in the sentence and just uses the part of speech tags.",
            "In this case, so the model is generative model and for this sentence it would start by generating the root with this data root V probability.",
            "And then it generates children to the right hand side first."
        ],
        [
            "And then children to the."
        ],
        [
            "Left hand side."
        ],
        [
            "And so on.",
            "Recurring on the children of the children."
        ],
        [
            "Here's an outline of what the rest of the talk is going to be, and talk about a problem with this model faces.",
            "And then a measure of parent child pair sparsity that characterizes this problem.",
            "A modification we can make to the model objective that can help to alleviate this problem and how the modification can improve parsing accuracy."
        ],
        [
            "This is just the traditional marginal log likelihood objective for the model, so its sum over all possible parses.",
            "Why joint probability of XR sentence?",
            "Why are parse and Y sub axes expectation over the whole corpus capital X?",
            "The standard optimization method here is expectation maximization.",
            "But the problem with just using EM is that this grammar is very permissive, so we have it's very permissive.",
            "So what I mean by that is that we have a parameter for every single parent child part of speech tag pair, and that leaves us with a lot of ambiguity in the model.",
            "So yeah, may learn grammar that's not very concise.",
            "It might learn a grammar that assigns zero probability to only a few of these parent child or speech tag pairs, whereas for most gold parses many of these pairs don't occur in any bold parts.",
            "So in the next slide I'm going to more precisely define what I mean by concise so that we can see how we got incorporated definition of concise into the objective to try and constrain the and prevent it from learning.",
            "These are these grammars that are not as concise as we'd like."
        ],
        [
            "If we have these gold trees here, the 1st two are actually the same in the second or the same.",
            "I'm going to consider things one edge at a time.",
            "An ignore route for right now.",
            "If we look at the first edge here for the top row, this is a verb with a noun as its child.",
            "I'm gonna mark A1 in the verb goes to noun.",
            "Column here, so across the top.",
            "Here I have all possible parent child pairs.",
            "For now this children.",
            "An adjective says children.",
            "One in the call, one in the second column for this edge.",
            "If we consider the other edge in the parse, we get a one in the same column because that there is also a verb noun pair.",
            "If we look at another sentence such as the one tagged verb adjective noun, we again end up with one more mark in the same column, and we end up with another mark in the noun adjective column.",
            "So you can see a lot of columns don't end up with Mark, so although some of these pairs may actually occur in ago grammar, alot of parent child tag pairs won't occur.",
            "For example, you can imagine that if you have a determiner tag, the terminal tag is unlikely to be the parent of anything else, so all pairs where you have determine areas, parent or something else are unlikely to ever occur in a gold parse."
        ],
        [
            "So the way that we can use this to come up with a measure of sparsity is if we take the Max over each column and then we sum the Max is.",
            "This is like a count of the number of unique parent child tag pairs that occur.",
            "In this case, 2."
        ],
        [
            "If we don't have label data, which is the situation we're working with here, we're talking about unsupervised grammar induction.",
            "But we have some.",
            "Posterior distribution over our parse trees P, Theta Y given X.",
            "We can approximate this person you measure by using the posterior trees.",
            "So these are the same sentences here the same children in each case.",
            "Anne.",
            "This is an illustration of what the posteriors might be so.",
            "In the first one, we have probability that the parent of the noun is another noun being .4 probability that the parent of the noun is the verb being .6.",
            "And the same for the other noun in the sentence and then in the 3rd row.",
            "We have that the parent of this noun might be verb, or it might be adjective an if our model assigns some probability to the adjective noun relation, then we suddenly have a number in this column.",
            "So in the previous slide we didn't have numbers in this column or in this column.",
            "And for the third, for the last row here we have possible parents for the adjective being verb and noun.",
            "Maybe your model is confused and it thinks that verb is a very likely parent, more likely parent of adjectives and noun, so we would end up with a .6 in this column.",
            ".4 in this column and we can do the same thing as before."
        ],
        [
            "Take the Max over each column."
        ],
        [
            "Sum it up and the number we get here is 2.4 on the previous slide we had two so you can see how if we start spreading probability around too many different pairs, we start to have a larger sum, so this sparsity measure starts to increase."
        ],
        [
            "I'm going to show some tables now for the actual data for actual data.",
            "And this is just a small example of what the components of the big tables are going to be.",
            "So across the top here.",
            "We have parent part of speech and down the road we have child power speech.",
            "Blue indicates a high, posterior and white indicates low posterior.",
            "And these are actually massive tables, so the."
        ],
        [
            "Blue right here.",
            "Would indicate that somewhere in our corpus, if foreign word has a high posterior probability of being the parent of the determiner.",
            "It doesn't matter if this is only one sentence or two, sentence is, if anywhere there's a high posterior, it's going to become dark because we'll be taking the Max overall all instances of this type of link.",
            "Another thing that this table says is if you have a white square such as the one there, then this is something like WH.",
            "Determiners never dominate terminer's.",
            "We've never seen an instance of that type of edge."
        ],
        [
            "So here these kind of edge type tables for a couple of languages.",
            "This is supervised model initialization.",
            "I.",
            "So this Portuguese and English, and as you can see, there's a lot of white squares here, indicating that the substantial number of grandchild pairs that are never seen in the corpus.",
            "There shouldn't be seen given this supervised initialization."
        ],
        [
            "However, if we compare to tables for EM, so in this case the comparison is red.",
            "Here means the.",
            "The posterior is less than that from the supervised case, and black here means that it's greater than from the supervised case.",
            "So you can see that there is a lot of black in these tables, and that's an indication that the model is assigning probability to too many.",
            "Types of pairs.",
            "So too many parent child pairs are getting some probability."
        ],
        [
            "So there's been some other attempts to improve this model.",
            "I'm just going to go through a few of those before I talk about the constraint that we used to try and enforce sparsity.",
            "So first of all, Smith and Eisner in 2006.",
            "Structural in the line to try and constrain dependency links so they start out with.",
            "Forcing the model to have very short dependencies and then allowing it more freedom as me in later am iterations.",
            "He did it all in 2009, fourth on some model extensions, so they added some more balance to the model and they added conditioning the child probabilities on balance.",
            "Another approach, this one taken by a lot of people, is to use parameter regularization.",
            "So put some prior P, Theta on the parameters of the model.",
            "Here's an example of some of the priors that have been used.",
            "I'm not going to go through them all, but there's this discount individually prior or logistic.",
            "Normal prior hierarchical hierarchical drizly processes.",
            "All of these have been used to try an.",
            "Help the model to learn better and to some extent to enforce sparsity.",
            "So all of these can cut down on the number of children that a particular parent has, but from our sourcing measure, we've seen that we really want to cut down on number of parent child pairs, not necessarily just the number of children that a particular tag has.",
            "So the problem is that the prior on the parameters.",
            "Can't do the same thing as a condition on the posteriors, so the condition that we've been talking about the sparsity measure we define talks about a Max over posteriors."
        ],
        [
            "A parent child probabilities which isn't the same as the model parameters, which is state of child given parent so.",
            "Will you propose?"
        ],
        [
            "Is to do this kind of regularization more directly by regularising the posteriors instead of regularising the parameters?",
            "Um?",
            "So what we want to do is to minimize this number of unique parent child pairs directly through an eastep penalty term on the posteriors."
        ],
        [
            "The M step is exactly the same as in regular M. And."
        ],
        [
            "This step will be slightly different from yeah, so this is the step for EM.",
            "Um?",
            "And the posterior distribution over parses.",
            "It might come up with, could look something like this.",
            "So larger dots here indicate higher probability.",
            "Um, so, for example, to interpret this, the posterior Q decomposes into these two parts.",
            "Probability of the root for each tag.",
            "And probability that a particular tag is apparent of another one.",
            "So for the sentence illustrated here, we have determiner, noun verb.",
            "Noun verb has highest probability of being root.",
            "And in terms of parent child pairs, we might see something like that.",
            "The model is unsure whether determiner should be parents of nouns or verbs should be parents of nouns, so we have relatively large probabilities here in here."
        ],
        [
            "If we add.",
            "A regularization term for the posteriors.",
            "What I'm calling L1L Infinity of Q here and we multiply by some strength saying mother just indicates.",
            "It's it's weight relative to the other component of the objective here.",
            "We see that the posterior distribution might change in a way similar to this, so before we had relatively equal size dots here and here, but with this extra term.",
            "We want the sum.",
            "Of this, to be small, some of the Max is to be small, so we might choose to make these much smaller and put some of the mass on here.",
            "Since then we only pay the cost once instead of several times.",
            "So."
        ],
        [
            "Somebody described some experiments that we did to see how our posterior regularization performs.",
            "Um?",
            "To be really thorough, we tried this on 12 languages and 11 of them are from the condo lecture task and English one is from Penn Treebank data.",
            "Processing for the train and test sets following other working literature, we strip punctuation and for now we only consider sentences of length less than equal to 10.",
            "For the training set.",
            "During experimentation, we found that it tends to make a much more stable model if we also eliminate sentence is of length less than three from the training sets.",
            "So we took those out of the training, but they're still in the tests for better comparison to other methods.",
            "Just a reminder that we assume part of speech tags are given.",
            "Everything here is done with our speech tags.",
            "We don't have any parse trees because it's unsupervised learning.",
            "The way we initialize our models is according to.",
            "Initialization given in kind of Manning paper from 2004.",
            "Here's some baseline number."
        ],
        [
            "It will compare against.",
            "These are the best of link left and link right for each language by Link left I mean that we set the route.",
            "We guess that the root is the leftmost item in the sentence.",
            "We guess that word one next to it is it's child.",
            "We guess that were two is the child of word one, and so on through the rest of the sentence week, right means that we guess the route is the item furthest to the right in the sentence.",
            "And then we set the links all pointing to the left.",
            "So most of the baselines are fairly low, but you can notice that Turkish and Japanese are much higher than the others.",
            "This can be explained by the fact that these are the two languages here that are verb final, so it's understandable that the link right baseline would do much better for them."
        ],
        [
            "Comparing to EM.",
            "The baseline wins substantially on the verb final languages.",
            "It also wins on Bulgarian and is pretty close on some of the others."
        ],
        [
            "If we look at supervised model accuracy, we can see that we can do a lot better, so we indeed have room for improvement."
        ],
        [
            "And here's how much adding our sparsity constraints.",
            "Improve things.",
            "On 11 out of 12 languages it improves Overeem, the only one that it loses on is Swedish.",
            "And on Portuguese it improves by the most 30%.",
            "There's an average of 10% accuracy increase across all languages."
        ],
        [
            "And.",
            "Just take a look at Farsetti to see if it in fact.",
            "If our constraint in fact increases sparsity and results in a more concise grammar, here is the supervised socially measure compared to EM.",
            "So supervised is indeed more sparse than M. Anne."
        ],
        [
            "And.",
            "If we look at how our model performs wear this yellow bar here, so we have very much reduced that measure.",
            "So we're."
        ],
        [
            "In fact, overshooting with sparsity, these are again the edge type tables comparing to supervised so.",
            "Almost all red indicates that we were.",
            "We have over sparsification and most of the extra probability mass goes to a single very dark square in each row."
        ],
        [
            "Comparing to some other methods, PR outperforms the discounting Richard Pryor in 10 out of 12 cases.",
            "And then literally prior also has a higher number of these unique parent child pairs and expectations, so its sparsity measure indicates that literally prior does not make things as sparse as.",
            "The supervised model would prefer.",
            "So we're certainly doing something different there.",
            "Pure performance for the shared logistic normal prior.",
            "We haven't implemented this, but we can compare on English to Conan Smith.",
            "They got 61.3% accuracy for English and we get 62%, so the method is comparable to Earth."
        ],
        [
            "And I just want to.",
            "Give an illustration of why.",
            "PR has higher accuracy for Spanish site.",
            "Right here is an example of posteriors Feraheme parse.",
            "So for example, if we look at the last word here, the.",
            "Possible parents of it are this noun and the main verb and the main verb has a slightly higher probability.",
            ".5 seven of being this parent than the Mount, which has probably .43.",
            "If we look so red edges indicate edges that are incorrect according to the gold parse and green edges or edges that are correct according to gold parse."
        ],
        [
            "If we look at the PR parts we see a lot more green edges and the main reason for this is that.",
            "C. Regularization is able to figure out that the.",
            "Determiner noun relation should have determiners as children of nouns instead of the other way around.",
            "So why is it able to figure this out?"
        ],
        [
            "Gonna look at some slightly shorter examples to explain this, so we just take the beginning of that phrase that we had before, and the correct parts for it, and we count the number of unique parent child pairs in there.",
            "We get these two.",
            "And then if we count.",
            "If we take the."
        ],
        [
            "Park City, and posits and count the number of unique parent child pairs in there.",
            "We get these two."
        ],
        [
            "And then suppose we look at some other sentence from our corpus which has a noun without a determiner before it.",
            "Then we end up with these two pairs.",
            "So if PR chose them."
        ],
        [
            "First things.",
            "Like one and three.",
            "Then we'd end up with a total of three unique pairs.",
            "If instead it chose this incorrect parse.",
            "Paired with this one like Ian would then it would end up with four unique pairs total.",
            "This would be larger and we pay greater penalty in terms of are in terms of our sparsity measure, so this is 1 explanation for why PR is able to learn the correct thing in this case."
        ],
        [
            "To summarize.",
            "We notice that supervised models for part of speech tagging or propensity grammar induction tend to show.",
            "A small number of unique parent child pairs and EM doesn't reflect this.",
            "Our solution was to use posterior regularization to decrease the expected number of these pairs, and in the end we are able to improve Overeem accuracy and 11 out of 12 languages we tested on."
        ],
        [
            "Some future directions.",
            "We have this tendency to over over specify, but if we reduce the Sigma which was our regularization strength too much, we notice that has a negative impact on accuracy.",
            "So we tried a couple of values but.",
            "If it's too low.",
            "Even if it corresponds closer to supervise sparsity, it has lower accuracy, so we need a unsupervised way of choosing a good Sigma or one other possibility is to investigate sparsity and some different aspect of the grammar.",
            "Maybe parent child part of speech tag pairs is not the most sparse aspect of the grammar.",
            "Another thing is that this kind of sparsity constraint might provide enough guidance to use a more complicated model.",
            "Um?",
            "And that could result in improved performance also.",
            "Another thing we want to try is joint induction of part of speech and parse trees.",
            "So that's it, thanks.",
            "I have a question about the regularizations.",
            "So it looks like in the experiments we reported you can fix that.",
            "I value and it works pretty well.",
            "How sensitive was performance to that and did you?",
            "Did you try like limiting conditions when you crank it up really high to see what would happen to the grammar when you demented incredibly high sparsity?",
            "So we tried very low value and we try to solve the strength here is 100.",
            "We also tried say 200.",
            "I don't think we went beyond that.",
            "The answer is that it's there's no one value that's best for all languages.",
            "And.",
            "I think the one thing we have to do though is that we need to.",
            "We need to take this value and normalize it based on corpus size and number for speech tags and such, because right now.",
            "Although it's strength 100 for all languages, it really isn't the same strength for all languages because of different corpus sizes.",
            "You said it wrong.",
            "Does it hurt really bad?",
            "It definitely hurts really badly if you set it too low.",
            "I'm not sure if it hurts really badly if you set it way too high.",
            "So what happens in a bit too much pressure on?",
            "You know?",
            "What does it learn is intended for some kind of generate base?",
            "Yeah, that's an interesting question."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Can you guys only hear me?",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, great.",
                    "label": 0
                },
                {
                    "sent": "So I'm Jenny and this is joint work with Kuzman Ganchev, Jewel Grassa, Ben Tasker and Fernando Parra.",
                    "label": 0
                },
                {
                    "sent": "So I'm gonna talk about sparsity, independency, grammar induction.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To begin with, before I talk about sparsity, I'm just going to introduce the model we're using.",
                    "label": 0
                },
                {
                    "sent": "This is the dependency model with balance, which was introduced by a client in Manning in 2004, and.",
                    "label": 1
                },
                {
                    "sent": "The way it works is that it assumes that all sentences in the corpus are tagged with part of speech tags.",
                    "label": 0
                },
                {
                    "sent": "So for this sentence here we have noun, verb, adjective, noun and the model ignores the actual words in the sentence and just uses the part of speech tags.",
                    "label": 0
                },
                {
                    "sent": "In this case, so the model is generative model and for this sentence it would start by generating the root with this data root V probability.",
                    "label": 0
                },
                {
                    "sent": "And then it generates children to the right hand side first.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then children to the.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Left hand side.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "Recurring on the children of the children.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's an outline of what the rest of the talk is going to be, and talk about a problem with this model faces.",
                    "label": 1
                },
                {
                    "sent": "And then a measure of parent child pair sparsity that characterizes this problem.",
                    "label": 1
                },
                {
                    "sent": "A modification we can make to the model objective that can help to alleviate this problem and how the modification can improve parsing accuracy.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is just the traditional marginal log likelihood objective for the model, so its sum over all possible parses.",
                    "label": 1
                },
                {
                    "sent": "Why joint probability of XR sentence?",
                    "label": 0
                },
                {
                    "sent": "Why are parse and Y sub axes expectation over the whole corpus capital X?",
                    "label": 0
                },
                {
                    "sent": "The standard optimization method here is expectation maximization.",
                    "label": 1
                },
                {
                    "sent": "But the problem with just using EM is that this grammar is very permissive, so we have it's very permissive.",
                    "label": 1
                },
                {
                    "sent": "So what I mean by that is that we have a parameter for every single parent child part of speech tag pair, and that leaves us with a lot of ambiguity in the model.",
                    "label": 0
                },
                {
                    "sent": "So yeah, may learn grammar that's not very concise.",
                    "label": 0
                },
                {
                    "sent": "It might learn a grammar that assigns zero probability to only a few of these parent child or speech tag pairs, whereas for most gold parses many of these pairs don't occur in any bold parts.",
                    "label": 0
                },
                {
                    "sent": "So in the next slide I'm going to more precisely define what I mean by concise so that we can see how we got incorporated definition of concise into the objective to try and constrain the and prevent it from learning.",
                    "label": 1
                },
                {
                    "sent": "These are these grammars that are not as concise as we'd like.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we have these gold trees here, the 1st two are actually the same in the second or the same.",
                    "label": 0
                },
                {
                    "sent": "I'm going to consider things one edge at a time.",
                    "label": 0
                },
                {
                    "sent": "An ignore route for right now.",
                    "label": 0
                },
                {
                    "sent": "If we look at the first edge here for the top row, this is a verb with a noun as its child.",
                    "label": 0
                },
                {
                    "sent": "I'm gonna mark A1 in the verb goes to noun.",
                    "label": 0
                },
                {
                    "sent": "Column here, so across the top.",
                    "label": 0
                },
                {
                    "sent": "Here I have all possible parent child pairs.",
                    "label": 0
                },
                {
                    "sent": "For now this children.",
                    "label": 0
                },
                {
                    "sent": "An adjective says children.",
                    "label": 0
                },
                {
                    "sent": "One in the call, one in the second column for this edge.",
                    "label": 0
                },
                {
                    "sent": "If we consider the other edge in the parse, we get a one in the same column because that there is also a verb noun pair.",
                    "label": 0
                },
                {
                    "sent": "If we look at another sentence such as the one tagged verb adjective noun, we again end up with one more mark in the same column, and we end up with another mark in the noun adjective column.",
                    "label": 0
                },
                {
                    "sent": "So you can see a lot of columns don't end up with Mark, so although some of these pairs may actually occur in ago grammar, alot of parent child tag pairs won't occur.",
                    "label": 0
                },
                {
                    "sent": "For example, you can imagine that if you have a determiner tag, the terminal tag is unlikely to be the parent of anything else, so all pairs where you have determine areas, parent or something else are unlikely to ever occur in a gold parse.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the way that we can use this to come up with a measure of sparsity is if we take the Max over each column and then we sum the Max is.",
                    "label": 1
                },
                {
                    "sent": "This is like a count of the number of unique parent child tag pairs that occur.",
                    "label": 1
                },
                {
                    "sent": "In this case, 2.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we don't have label data, which is the situation we're working with here, we're talking about unsupervised grammar induction.",
                    "label": 0
                },
                {
                    "sent": "But we have some.",
                    "label": 0
                },
                {
                    "sent": "Posterior distribution over our parse trees P, Theta Y given X.",
                    "label": 0
                },
                {
                    "sent": "We can approximate this person you measure by using the posterior trees.",
                    "label": 0
                },
                {
                    "sent": "So these are the same sentences here the same children in each case.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "This is an illustration of what the posteriors might be so.",
                    "label": 0
                },
                {
                    "sent": "In the first one, we have probability that the parent of the noun is another noun being .4 probability that the parent of the noun is the verb being .6.",
                    "label": 0
                },
                {
                    "sent": "And the same for the other noun in the sentence and then in the 3rd row.",
                    "label": 0
                },
                {
                    "sent": "We have that the parent of this noun might be verb, or it might be adjective an if our model assigns some probability to the adjective noun relation, then we suddenly have a number in this column.",
                    "label": 0
                },
                {
                    "sent": "So in the previous slide we didn't have numbers in this column or in this column.",
                    "label": 0
                },
                {
                    "sent": "And for the third, for the last row here we have possible parents for the adjective being verb and noun.",
                    "label": 0
                },
                {
                    "sent": "Maybe your model is confused and it thinks that verb is a very likely parent, more likely parent of adjectives and noun, so we would end up with a .6 in this column.",
                    "label": 0
                },
                {
                    "sent": ".4 in this column and we can do the same thing as before.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Take the Max over each column.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sum it up and the number we get here is 2.4 on the previous slide we had two so you can see how if we start spreading probability around too many different pairs, we start to have a larger sum, so this sparsity measure starts to increase.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to show some tables now for the actual data for actual data.",
                    "label": 0
                },
                {
                    "sent": "And this is just a small example of what the components of the big tables are going to be.",
                    "label": 0
                },
                {
                    "sent": "So across the top here.",
                    "label": 0
                },
                {
                    "sent": "We have parent part of speech and down the road we have child power speech.",
                    "label": 0
                },
                {
                    "sent": "Blue indicates a high, posterior and white indicates low posterior.",
                    "label": 0
                },
                {
                    "sent": "And these are actually massive tables, so the.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Blue right here.",
                    "label": 0
                },
                {
                    "sent": "Would indicate that somewhere in our corpus, if foreign word has a high posterior probability of being the parent of the determiner.",
                    "label": 1
                },
                {
                    "sent": "It doesn't matter if this is only one sentence or two, sentence is, if anywhere there's a high posterior, it's going to become dark because we'll be taking the Max overall all instances of this type of link.",
                    "label": 0
                },
                {
                    "sent": "Another thing that this table says is if you have a white square such as the one there, then this is something like WH.",
                    "label": 0
                },
                {
                    "sent": "Determiners never dominate terminer's.",
                    "label": 0
                },
                {
                    "sent": "We've never seen an instance of that type of edge.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here these kind of edge type tables for a couple of languages.",
                    "label": 0
                },
                {
                    "sent": "This is supervised model initialization.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "So this Portuguese and English, and as you can see, there's a lot of white squares here, indicating that the substantial number of grandchild pairs that are never seen in the corpus.",
                    "label": 0
                },
                {
                    "sent": "There shouldn't be seen given this supervised initialization.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, if we compare to tables for EM, so in this case the comparison is red.",
                    "label": 0
                },
                {
                    "sent": "Here means the.",
                    "label": 0
                },
                {
                    "sent": "The posterior is less than that from the supervised case, and black here means that it's greater than from the supervised case.",
                    "label": 0
                },
                {
                    "sent": "So you can see that there is a lot of black in these tables, and that's an indication that the model is assigning probability to too many.",
                    "label": 0
                },
                {
                    "sent": "Types of pairs.",
                    "label": 0
                },
                {
                    "sent": "So too many parent child pairs are getting some probability.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's been some other attempts to improve this model.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to go through a few of those before I talk about the constraint that we used to try and enforce sparsity.",
                    "label": 0
                },
                {
                    "sent": "So first of all, Smith and Eisner in 2006.",
                    "label": 1
                },
                {
                    "sent": "Structural in the line to try and constrain dependency links so they start out with.",
                    "label": 0
                },
                {
                    "sent": "Forcing the model to have very short dependencies and then allowing it more freedom as me in later am iterations.",
                    "label": 0
                },
                {
                    "sent": "He did it all in 2009, fourth on some model extensions, so they added some more balance to the model and they added conditioning the child probabilities on balance.",
                    "label": 0
                },
                {
                    "sent": "Another approach, this one taken by a lot of people, is to use parameter regularization.",
                    "label": 0
                },
                {
                    "sent": "So put some prior P, Theta on the parameters of the model.",
                    "label": 0
                },
                {
                    "sent": "Here's an example of some of the priors that have been used.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go through them all, but there's this discount individually prior or logistic.",
                    "label": 0
                },
                {
                    "sent": "Normal prior hierarchical hierarchical drizly processes.",
                    "label": 0
                },
                {
                    "sent": "All of these have been used to try an.",
                    "label": 0
                },
                {
                    "sent": "Help the model to learn better and to some extent to enforce sparsity.",
                    "label": 0
                },
                {
                    "sent": "So all of these can cut down on the number of children that a particular parent has, but from our sourcing measure, we've seen that we really want to cut down on number of parent child pairs, not necessarily just the number of children that a particular tag has.",
                    "label": 0
                },
                {
                    "sent": "So the problem is that the prior on the parameters.",
                    "label": 0
                },
                {
                    "sent": "Can't do the same thing as a condition on the posteriors, so the condition that we've been talking about the sparsity measure we define talks about a Max over posteriors.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A parent child probabilities which isn't the same as the model parameters, which is state of child given parent so.",
                    "label": 0
                },
                {
                    "sent": "Will you propose?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is to do this kind of regularization more directly by regularising the posteriors instead of regularising the parameters?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So what we want to do is to minimize this number of unique parent child pairs directly through an eastep penalty term on the posteriors.",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The M step is exactly the same as in regular M. And.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This step will be slightly different from yeah, so this is the step for EM.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And the posterior distribution over parses.",
                    "label": 0
                },
                {
                    "sent": "It might come up with, could look something like this.",
                    "label": 0
                },
                {
                    "sent": "So larger dots here indicate higher probability.",
                    "label": 0
                },
                {
                    "sent": "Um, so, for example, to interpret this, the posterior Q decomposes into these two parts.",
                    "label": 0
                },
                {
                    "sent": "Probability of the root for each tag.",
                    "label": 0
                },
                {
                    "sent": "And probability that a particular tag is apparent of another one.",
                    "label": 0
                },
                {
                    "sent": "So for the sentence illustrated here, we have determiner, noun verb.",
                    "label": 0
                },
                {
                    "sent": "Noun verb has highest probability of being root.",
                    "label": 0
                },
                {
                    "sent": "And in terms of parent child pairs, we might see something like that.",
                    "label": 0
                },
                {
                    "sent": "The model is unsure whether determiner should be parents of nouns or verbs should be parents of nouns, so we have relatively large probabilities here in here.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we add.",
                    "label": 0
                },
                {
                    "sent": "A regularization term for the posteriors.",
                    "label": 0
                },
                {
                    "sent": "What I'm calling L1L Infinity of Q here and we multiply by some strength saying mother just indicates.",
                    "label": 0
                },
                {
                    "sent": "It's it's weight relative to the other component of the objective here.",
                    "label": 0
                },
                {
                    "sent": "We see that the posterior distribution might change in a way similar to this, so before we had relatively equal size dots here and here, but with this extra term.",
                    "label": 0
                },
                {
                    "sent": "We want the sum.",
                    "label": 0
                },
                {
                    "sent": "Of this, to be small, some of the Max is to be small, so we might choose to make these much smaller and put some of the mass on here.",
                    "label": 0
                },
                {
                    "sent": "Since then we only pay the cost once instead of several times.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Somebody described some experiments that we did to see how our posterior regularization performs.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "To be really thorough, we tried this on 12 languages and 11 of them are from the condo lecture task and English one is from Penn Treebank data.",
                    "label": 1
                },
                {
                    "sent": "Processing for the train and test sets following other working literature, we strip punctuation and for now we only consider sentences of length less than equal to 10.",
                    "label": 1
                },
                {
                    "sent": "For the training set.",
                    "label": 0
                },
                {
                    "sent": "During experimentation, we found that it tends to make a much more stable model if we also eliminate sentence is of length less than three from the training sets.",
                    "label": 0
                },
                {
                    "sent": "So we took those out of the training, but they're still in the tests for better comparison to other methods.",
                    "label": 0
                },
                {
                    "sent": "Just a reminder that we assume part of speech tags are given.",
                    "label": 0
                },
                {
                    "sent": "Everything here is done with our speech tags.",
                    "label": 0
                },
                {
                    "sent": "We don't have any parse trees because it's unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "The way we initialize our models is according to.",
                    "label": 0
                },
                {
                    "sent": "Initialization given in kind of Manning paper from 2004.",
                    "label": 0
                },
                {
                    "sent": "Here's some baseline number.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It will compare against.",
                    "label": 0
                },
                {
                    "sent": "These are the best of link left and link right for each language by Link left I mean that we set the route.",
                    "label": 0
                },
                {
                    "sent": "We guess that the root is the leftmost item in the sentence.",
                    "label": 0
                },
                {
                    "sent": "We guess that word one next to it is it's child.",
                    "label": 0
                },
                {
                    "sent": "We guess that were two is the child of word one, and so on through the rest of the sentence week, right means that we guess the route is the item furthest to the right in the sentence.",
                    "label": 0
                },
                {
                    "sent": "And then we set the links all pointing to the left.",
                    "label": 0
                },
                {
                    "sent": "So most of the baselines are fairly low, but you can notice that Turkish and Japanese are much higher than the others.",
                    "label": 0
                },
                {
                    "sent": "This can be explained by the fact that these are the two languages here that are verb final, so it's understandable that the link right baseline would do much better for them.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Comparing to EM.",
                    "label": 0
                },
                {
                    "sent": "The baseline wins substantially on the verb final languages.",
                    "label": 1
                },
                {
                    "sent": "It also wins on Bulgarian and is pretty close on some of the others.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we look at supervised model accuracy, we can see that we can do a lot better, so we indeed have room for improvement.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here's how much adding our sparsity constraints.",
                    "label": 0
                },
                {
                    "sent": "Improve things.",
                    "label": 0
                },
                {
                    "sent": "On 11 out of 12 languages it improves Overeem, the only one that it loses on is Swedish.",
                    "label": 0
                },
                {
                    "sent": "And on Portuguese it improves by the most 30%.",
                    "label": 0
                },
                {
                    "sent": "There's an average of 10% accuracy increase across all languages.",
                    "label": 1
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Just take a look at Farsetti to see if it in fact.",
                    "label": 0
                },
                {
                    "sent": "If our constraint in fact increases sparsity and results in a more concise grammar, here is the supervised socially measure compared to EM.",
                    "label": 0
                },
                {
                    "sent": "So supervised is indeed more sparse than M. Anne.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "If we look at how our model performs wear this yellow bar here, so we have very much reduced that measure.",
                    "label": 0
                },
                {
                    "sent": "So we're.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In fact, overshooting with sparsity, these are again the edge type tables comparing to supervised so.",
                    "label": 0
                },
                {
                    "sent": "Almost all red indicates that we were.",
                    "label": 0
                },
                {
                    "sent": "We have over sparsification and most of the extra probability mass goes to a single very dark square in each row.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Comparing to some other methods, PR outperforms the discounting Richard Pryor in 10 out of 12 cases.",
                    "label": 0
                },
                {
                    "sent": "And then literally prior also has a higher number of these unique parent child pairs and expectations, so its sparsity measure indicates that literally prior does not make things as sparse as.",
                    "label": 1
                },
                {
                    "sent": "The supervised model would prefer.",
                    "label": 0
                },
                {
                    "sent": "So we're certainly doing something different there.",
                    "label": 1
                },
                {
                    "sent": "Pure performance for the shared logistic normal prior.",
                    "label": 1
                },
                {
                    "sent": "We haven't implemented this, but we can compare on English to Conan Smith.",
                    "label": 0
                },
                {
                    "sent": "They got 61.3% accuracy for English and we get 62%, so the method is comparable to Earth.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I just want to.",
                    "label": 0
                },
                {
                    "sent": "Give an illustration of why.",
                    "label": 0
                },
                {
                    "sent": "PR has higher accuracy for Spanish site.",
                    "label": 0
                },
                {
                    "sent": "Right here is an example of posteriors Feraheme parse.",
                    "label": 0
                },
                {
                    "sent": "So for example, if we look at the last word here, the.",
                    "label": 0
                },
                {
                    "sent": "Possible parents of it are this noun and the main verb and the main verb has a slightly higher probability.",
                    "label": 0
                },
                {
                    "sent": ".5 seven of being this parent than the Mount, which has probably .43.",
                    "label": 0
                },
                {
                    "sent": "If we look so red edges indicate edges that are incorrect according to the gold parse and green edges or edges that are correct according to gold parse.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we look at the PR parts we see a lot more green edges and the main reason for this is that.",
                    "label": 0
                },
                {
                    "sent": "C. Regularization is able to figure out that the.",
                    "label": 0
                },
                {
                    "sent": "Determiner noun relation should have determiners as children of nouns instead of the other way around.",
                    "label": 0
                },
                {
                    "sent": "So why is it able to figure this out?",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Gonna look at some slightly shorter examples to explain this, so we just take the beginning of that phrase that we had before, and the correct parts for it, and we count the number of unique parent child pairs in there.",
                    "label": 0
                },
                {
                    "sent": "We get these two.",
                    "label": 0
                },
                {
                    "sent": "And then if we count.",
                    "label": 0
                },
                {
                    "sent": "If we take the.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Park City, and posits and count the number of unique parent child pairs in there.",
                    "label": 0
                },
                {
                    "sent": "We get these two.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then suppose we look at some other sentence from our corpus which has a noun without a determiner before it.",
                    "label": 0
                },
                {
                    "sent": "Then we end up with these two pairs.",
                    "label": 0
                },
                {
                    "sent": "So if PR chose them.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First things.",
                    "label": 0
                },
                {
                    "sent": "Like one and three.",
                    "label": 0
                },
                {
                    "sent": "Then we'd end up with a total of three unique pairs.",
                    "label": 1
                },
                {
                    "sent": "If instead it chose this incorrect parse.",
                    "label": 0
                },
                {
                    "sent": "Paired with this one like Ian would then it would end up with four unique pairs total.",
                    "label": 1
                },
                {
                    "sent": "This would be larger and we pay greater penalty in terms of are in terms of our sparsity measure, so this is 1 explanation for why PR is able to learn the correct thing in this case.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To summarize.",
                    "label": 0
                },
                {
                    "sent": "We notice that supervised models for part of speech tagging or propensity grammar induction tend to show.",
                    "label": 0
                },
                {
                    "sent": "A small number of unique parent child pairs and EM doesn't reflect this.",
                    "label": 0
                },
                {
                    "sent": "Our solution was to use posterior regularization to decrease the expected number of these pairs, and in the end we are able to improve Overeem accuracy and 11 out of 12 languages we tested on.",
                    "label": 1
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some future directions.",
                    "label": 0
                },
                {
                    "sent": "We have this tendency to over over specify, but if we reduce the Sigma which was our regularization strength too much, we notice that has a negative impact on accuracy.",
                    "label": 1
                },
                {
                    "sent": "So we tried a couple of values but.",
                    "label": 0
                },
                {
                    "sent": "If it's too low.",
                    "label": 0
                },
                {
                    "sent": "Even if it corresponds closer to supervise sparsity, it has lower accuracy, so we need a unsupervised way of choosing a good Sigma or one other possibility is to investigate sparsity and some different aspect of the grammar.",
                    "label": 1
                },
                {
                    "sent": "Maybe parent child part of speech tag pairs is not the most sparse aspect of the grammar.",
                    "label": 0
                },
                {
                    "sent": "Another thing is that this kind of sparsity constraint might provide enough guidance to use a more complicated model.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And that could result in improved performance also.",
                    "label": 1
                },
                {
                    "sent": "Another thing we want to try is joint induction of part of speech and parse trees.",
                    "label": 0
                },
                {
                    "sent": "So that's it, thanks.",
                    "label": 0
                },
                {
                    "sent": "I have a question about the regularizations.",
                    "label": 0
                },
                {
                    "sent": "So it looks like in the experiments we reported you can fix that.",
                    "label": 0
                },
                {
                    "sent": "I value and it works pretty well.",
                    "label": 0
                },
                {
                    "sent": "How sensitive was performance to that and did you?",
                    "label": 0
                },
                {
                    "sent": "Did you try like limiting conditions when you crank it up really high to see what would happen to the grammar when you demented incredibly high sparsity?",
                    "label": 0
                },
                {
                    "sent": "So we tried very low value and we try to solve the strength here is 100.",
                    "label": 0
                },
                {
                    "sent": "We also tried say 200.",
                    "label": 0
                },
                {
                    "sent": "I don't think we went beyond that.",
                    "label": 0
                },
                {
                    "sent": "The answer is that it's there's no one value that's best for all languages.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "I think the one thing we have to do though is that we need to.",
                    "label": 0
                },
                {
                    "sent": "We need to take this value and normalize it based on corpus size and number for speech tags and such, because right now.",
                    "label": 0
                },
                {
                    "sent": "Although it's strength 100 for all languages, it really isn't the same strength for all languages because of different corpus sizes.",
                    "label": 0
                },
                {
                    "sent": "You said it wrong.",
                    "label": 0
                },
                {
                    "sent": "Does it hurt really bad?",
                    "label": 0
                },
                {
                    "sent": "It definitely hurts really badly if you set it too low.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure if it hurts really badly if you set it way too high.",
                    "label": 0
                },
                {
                    "sent": "So what happens in a bit too much pressure on?",
                    "label": 0
                },
                {
                    "sent": "You know?",
                    "label": 0
                },
                {
                    "sent": "What does it learn is intended for some kind of generate base?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's an interesting question.",
                    "label": 0
                }
            ]
        }
    }
}