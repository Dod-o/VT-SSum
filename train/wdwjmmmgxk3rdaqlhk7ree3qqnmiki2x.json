{
    "id": "wdwjmmmgxk3rdaqlhk7ree3qqnmiki2x",
    "title": "Dirichlet process mixtures of generalised linear models",
    "info": {
        "author": [
            "Lauren A. Hannah, Department of Operations Research and Financial Engineering, Princeton University"
        ],
        "published": "May 20, 2010",
        "recorded": "May 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning->Dirichlet Processes"
        ]
    },
    "url": "http://videolectures.net/aistats2010_hannah_dpmog/",
    "segmentation": [
        [
            "So during the process, mixtures of generalized linear models, I'm Lauren, my coauthors were Dave Bly and my advisor, Warren Pal.",
            "Alright, so this talk is about now."
        ],
        [
            "Parametric regression and I want to do nonparametric regression in very general setting.",
            "So we might have continuous covariates and continuous response like we have in the left.",
            "Or we might have something a little bit uglier like categorical covariates and account response like we do in the."
        ],
        [
            "I want a model that can take care of all of us.",
            "So we're going to have covert Saxon response YX&Y might have different forms.",
            "You continuous count categorical, circular or worse.",
            "The goal of all this is prediction.",
            "We want to compute the expected value of Y given covariates X.",
            "So parametric regression restricts shape.",
            "Usually the straight line or polynomial or whatever else you want.",
            "But nonparametric regression tries to fit a function rather than rather than as."
        ],
        [
            "Pacific one.",
            "OK, so my goals for all those are flexible model would be able to accommodate very general input and output types.",
            "I want this to be successfully applied to data with different characteristics such as heteroskedasticity or overdispersion.",
            "I'd like some sort of theoretical assurances, such as asymptotical biasness and obviously computational tracked."
        ],
        [
            "Gillettii so.",
            "The whole idea on this is that locali a complex model can be represented by a simpler model, such as a continuous curve.",
            "Locally can probably be represented by a straight line.",
            "So to do this, we're going to use the richly processed mixture models.",
            "These will cluster the observations probabilistically, and they can accommodate many data types, but in this cluster who want to fit, you have a GLM generalized linear model that fits well.",
            "So the clusters and the local GLM coefficients are going to be latent variables, and then we're going to predict a mean response by averaging."
        ],
        [
            "Steer draws.",
            "So when we talk about when we talk about the DP GLM dearsley process, mixtures are generalized linear models.",
            "It's a general regression method that can accommodate all types of input, dearly, process in all types of output accommodate accommodate by generalized linear model, so this would be continuous count categorical circular covariance in response.",
            "So this is a generalization of existing special case methods such as Chicago Neil and we're going to give conditions for asymptotic unbiasedness."
        ],
        [
            "Alright, so this is what we do.",
            "We're going to start with some training data."
        ],
        [
            "Pick clusters and generalized linear models within those clusters.",
            "In this case, it's just."
        ],
        [
            "Via linear regression.",
            "Then we're going to get some testing data.",
            "And."
        ],
        [
            "Then what we're going to do?",
            "Try to predict a mean function, so we're going to take those testing covariates.",
            "Fit them in the model up top.",
            "We're going to have a lot of these different clusterings and then average over them to get the black line there."
        ],
        [
            "In function.",
            "OK, So what is a deer sleep process?",
            "It's a distribution over distributions.",
            "So draw from the DP is a random measure and thing about these random measures is that if the drone from judicial process are almost surely discreet.",
            "Which is great if you want to have them as a distribution for parameters of a mixture model, which will produce a clustering effect.",
            "So it's parameterized by a base measure G, not in a scale parameter Alpha, and it has this kind of neat property that you have Theta one friend drawn from P distribution and peas drawn from a Dirichlet process.",
            "Then you can integrate out P to get the predicted distribution of Theta N + 1, so it's one over Alpha plus N times point measures at your.",
            "Theta one through N plus Alpha over Alpha plus N * G, Not the base measure.",
            "So basically Jeannot tells you where your data is there going to be an Alpha.",
            "Kind of tells you how likely they are to stick to each other, so we're going to use this as a prior on distribution for hidden parameters of a mixture model."
        ],
        [
            "Did I?",
            "So the representation of the DP GLM has this form.",
            "We're going to drop key from a garishly process.",
            "Then we're going to draw hidden parameters from P and then these in parameters.",
            "Parameterized two parametric functions F of X&FYF of X is a distribution for the covariates.",
            "FY is a distribution for the response, which is conditioned on the covariates and parameters that have to deal with the response.",
            "So for example, let's say we have continuous covariates and a continuous response.",
            "We're going to use Gaussian model.",
            "So we're going to draw Theta from P. Data is going to be mean and variance for a normal distribution for covariates and slope intercept parameters an variance for normal distribution for why that has a mean beta not plus beta one times."
        ],
        [
            "Barrett this is an example.",
            "So there's a Gaussian clustering for the covariates, and then a linear model for the response.",
            "And notice that the variance can change with the."
        ],
        [
            "Coverts so how do we compute all this?",
            "Will start with data and data points, and we want to compute the expectation of Y at a query point X.",
            "So what were forced going to do is choose the GLM that we want.",
            "There are lots of different models.",
            "Then we're going to choose a dear sleep process based measure G, not.",
            "And then we're going to estimate the posterior distribution of our hidden parameters.",
            "So I use Gibbs sampling.",
            "You can also use variational inference.",
            "I usually use Neil Algorithms 3, six or eight depending on what I need, so we're going to obtain mids samples of these parameters, and then we're going to use these to compute the predicted value.",
            "The response.",
            "This is done with the tower property of expect."
        ],
        [
            "Tations so there is kind of an ugly integral.",
            "It's really not that bad.",
            "So given the parameters expectation of Y is really easy to compute per cluster, and then we wait this by the.",
            "Covariate density of X evaluated that cluster.",
            "And then we have an A term for the expectation under G, not.",
            "It's all normalized by B.",
            "So we're going to get EM observations of Theta, yeah, but that is unknown, so we need to average over these samples.",
            "So basically we do this for our M samples and we just take a simple average."
        ],
        [
            "So as some token biasness.",
            "The idea behind this is that we want the estimates or mean function to converge to the trimming function as we get more observations.",
            "This basically a frequentist justification of Bayesian methods.",
            "This is not necessarily a given.",
            "When you used a richly process, priors, Diaconis and Friedman had a very simple example.",
            "With a student T distribution that did not converge, so asymptotically biasness depends on the true distribution of FX&Y, which are going to note my F not.",
            "The model AK you know F of X&FY and her base."
        ],
        [
            "Measure G not.",
            "So the DP GLM is lost some topical in biasness over compact set of covariates.",
            "If it puts positive power density on LCL neighborhoods and all Cal variance neighborhoods, there is a finite variance for every under the true distribution for every covariant.",
            "And.",
            "Basically.",
            "If the expectation under Jeannot is a little bit more than integral, three usual."
        ],
        [
            "Follows from 1.",
            "OK, so the first condition that Cal conditions rather hard to show.",
            "Get satisfied under a Gaussian model with conjugate base measures, which is a normal inverse gamma or continuous and categorical covariates response again with conjugate base measures with categorical covariates.",
            "It's a deer ishly, so anything else is an open question, probably going."
        ],
        [
            "Be hard to prove.",
            "Alright, so here's the fun part.",
            "Empirical analysis."
        ],
        [
            "So why would I want to use this over just a standard dearsley processed mixture model for regression?",
            "So standard DP mixture models on the left?",
            "the DP GLM is on the right and what I did was I took some clustered data.",
            "I used only one dimension as a projective dimension and then I added on a bunch of spurious dimensions.",
            "Those are shown vertically, so without a lot.",
            "You know, as far as dimensions, the dearsley process mixture does OK.",
            "But as you add more and more sparse dimensions, it does worse and worse.",
            "Basically it starts trying to fit the mixtures too.",
            "The covariance rather than response, so the covariant posterior swamps response but exterior but with PDP LM you have some more parameters and their associated with the response so.",
            "It's more resistant to dimensionality.",
            "The downside, which you can see on #20 down there, is that all these parameters do add in some noise, but not that much."
        ],
        [
            "OK, so one of the other features of this is that you can accommodate a lot of different data properties such as headers capacity or overdispersion in natural way.",
            "So I compared it to Gaussian processes, top right, a treed Gaussian process, bottom left, and tree linear model, bottom right.",
            "Gaussian processes have constant variance that's assumed in the model.",
            "So basically your main function can get.",
            "A little disport distorted in the areas with high variance that read models are designed to deal with.",
            "Changing variance, but again these can get a little distorted in the areas with high variance and don't necessarily produce a smooth function."
        ],
        [
            "So I compare compare the PMM two other methods on three datasets, cosmic microwave background, which is chosen because it is heteroskedastic.",
            "Concrete compressive strength, which has fairly low noise but eight covariates so has moderate dimensionality and solar flare data which has 11 categorical covariance and account response.",
            "So it's again has moderate dimensionality but.",
            "Kind of odd."
        ],
        [
            "Very response types.",
            "So I compared with linear least squares, tree regression, tree linear models.",
            "Gaussian process regression tree.",
            "Gaussian processes directly process without the GLM and puts on regression for the."
        ],
        [
            "Dataset.",
            "So on the HETEROSKEDASTIC data set, the DP GLM was continually one of the best methods."
        ],
        [
            "On the concrete compressive strength data set.",
            "It wasn't necessarily the best method, but it was competitive.",
            "And some of the other kind of high tech Bayesian methods.",
            "Weren't stable on this data set, so they weren't plotted on there.",
            "Basically they do very well or blow up and have errors of.",
            "Up to 12,000.",
            "When everything else?"
        ],
        [
            "To follow one.",
            "On the solar data set, I couldn't compare compare against much due to the categorical covariates and account response.",
            "But again, the PGM was one of the best methods."
        ],
        [
            "So issues with this.",
            "I'd like to automate choice of base measured in hyperparameters, and I'd like to investigate the balance between modeling covariance in response.",
            "So the benefits of this method is is very flexible.",
            "I can use it in many settings, so generally competitive the outputs are generally stable an you can comment headers capacity and overdispersion in a natural manner."
        ],
        [
            "Questions.",
            "So first.",
            "1st.",
            "The model, it seems that you're modeling the covariance as well.",
            "The other hand in regression setting coverage cutoff nuisance parameters.",
            "I aren't you going to be heard because you're trying to model more than strictly speaking what you need.",
            "It depends on the setting.",
            "It's kind of a tradeoff between the flexibility you know and as you said, the extra noise in there.",
            "So I've had pretty good results with this, but on the other hand, if I get some data set with.",
            "Like 200 covariates.",
            "Yeah, I would just be adding noise.",
            "Second question is regarding the period.",
            "Property.",
            "Taking spectation over the data as well?",
            "Or is it?",
            "Like probably one conversion probability, one convergence, yeah?",
            "On the competitive.",
            "Solution times to the other method as well.",
            "Program cards.",
            "So traditional cart, yeah, it's a lot slower.",
            "It's actually faster than the Bayesian cart implementation that I had, which is a TGP package in R. Gibbs sampling can be fairly slow, but it's not that bad if you have conjugate priors and use something like.",
            "I use Java or C++.",
            "It's on larger datasets.",
            "It's actually much faster than like Gaussian processes, 'cause you don't have to do a large matrix inversion.",
            "So like 5000 data points or more.",
            "Usually, so if I'm I have like an hourly wind data set for a year.",
            "Usually if I want to run that to convergence is about 10 minutes.",
            "And.",
            "Using the joint.",
            "Of blasters.",
            "I would like to have different number of questions correct and for why even ex no no.",
            "So the way that I write.",
            "Unless you have access three months, then the progression factors anymore.",
            "Right, it's it's not that much of a problem.",
            "So basically I kind of want to get away from that by doing a joint density, so that would probably have you know, 3 * 2 modes.",
            "The Giants.",
            "Ensures that your mother is very consistent for the joint right.",
            "Need this for the.",
            "Yeah I do.",
            "Because as you said, right?",
            "So there are a lot of papers.",
            "Yeah, I know.",
            "Yeah, so there are a lot of papers that will do some sort of other process over the covariates and then use like a deer sleep process to cluster the responses and these tend to work pretty well, but they're just not as flexible as I need so.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So during the process, mixtures of generalized linear models, I'm Lauren, my coauthors were Dave Bly and my advisor, Warren Pal.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this talk is about now.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Parametric regression and I want to do nonparametric regression in very general setting.",
                    "label": 0
                },
                {
                    "sent": "So we might have continuous covariates and continuous response like we have in the left.",
                    "label": 0
                },
                {
                    "sent": "Or we might have something a little bit uglier like categorical covariates and account response like we do in the.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I want a model that can take care of all of us.",
                    "label": 0
                },
                {
                    "sent": "So we're going to have covert Saxon response YX&Y might have different forms.",
                    "label": 0
                },
                {
                    "sent": "You continuous count categorical, circular or worse.",
                    "label": 1
                },
                {
                    "sent": "The goal of all this is prediction.",
                    "label": 0
                },
                {
                    "sent": "We want to compute the expected value of Y given covariates X.",
                    "label": 0
                },
                {
                    "sent": "So parametric regression restricts shape.",
                    "label": 1
                },
                {
                    "sent": "Usually the straight line or polynomial or whatever else you want.",
                    "label": 0
                },
                {
                    "sent": "But nonparametric regression tries to fit a function rather than rather than as.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Pacific one.",
                    "label": 0
                },
                {
                    "sent": "OK, so my goals for all those are flexible model would be able to accommodate very general input and output types.",
                    "label": 0
                },
                {
                    "sent": "I want this to be successfully applied to data with different characteristics such as heteroskedasticity or overdispersion.",
                    "label": 1
                },
                {
                    "sent": "I'd like some sort of theoretical assurances, such as asymptotical biasness and obviously computational tracked.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Gillettii so.",
                    "label": 0
                },
                {
                    "sent": "The whole idea on this is that locali a complex model can be represented by a simpler model, such as a continuous curve.",
                    "label": 1
                },
                {
                    "sent": "Locally can probably be represented by a straight line.",
                    "label": 0
                },
                {
                    "sent": "So to do this, we're going to use the richly processed mixture models.",
                    "label": 1
                },
                {
                    "sent": "These will cluster the observations probabilistically, and they can accommodate many data types, but in this cluster who want to fit, you have a GLM generalized linear model that fits well.",
                    "label": 0
                },
                {
                    "sent": "So the clusters and the local GLM coefficients are going to be latent variables, and then we're going to predict a mean response by averaging.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Steer draws.",
                    "label": 0
                },
                {
                    "sent": "So when we talk about when we talk about the DP GLM dearsley process, mixtures are generalized linear models.",
                    "label": 1
                },
                {
                    "sent": "It's a general regression method that can accommodate all types of input, dearly, process in all types of output accommodate accommodate by generalized linear model, so this would be continuous count categorical circular covariance in response.",
                    "label": 0
                },
                {
                    "sent": "So this is a generalization of existing special case methods such as Chicago Neil and we're going to give conditions for asymptotic unbiasedness.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so this is what we do.",
                    "label": 0
                },
                {
                    "sent": "We're going to start with some training data.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pick clusters and generalized linear models within those clusters.",
                    "label": 0
                },
                {
                    "sent": "In this case, it's just.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Via linear regression.",
                    "label": 0
                },
                {
                    "sent": "Then we're going to get some testing data.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then what we're going to do?",
                    "label": 0
                },
                {
                    "sent": "Try to predict a mean function, so we're going to take those testing covariates.",
                    "label": 1
                },
                {
                    "sent": "Fit them in the model up top.",
                    "label": 1
                },
                {
                    "sent": "We're going to have a lot of these different clusterings and then average over them to get the black line there.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In function.",
                    "label": 0
                },
                {
                    "sent": "OK, So what is a deer sleep process?",
                    "label": 0
                },
                {
                    "sent": "It's a distribution over distributions.",
                    "label": 1
                },
                {
                    "sent": "So draw from the DP is a random measure and thing about these random measures is that if the drone from judicial process are almost surely discreet.",
                    "label": 1
                },
                {
                    "sent": "Which is great if you want to have them as a distribution for parameters of a mixture model, which will produce a clustering effect.",
                    "label": 0
                },
                {
                    "sent": "So it's parameterized by a base measure G, not in a scale parameter Alpha, and it has this kind of neat property that you have Theta one friend drawn from P distribution and peas drawn from a Dirichlet process.",
                    "label": 0
                },
                {
                    "sent": "Then you can integrate out P to get the predicted distribution of Theta N + 1, so it's one over Alpha plus N times point measures at your.",
                    "label": 0
                },
                {
                    "sent": "Theta one through N plus Alpha over Alpha plus N * G, Not the base measure.",
                    "label": 0
                },
                {
                    "sent": "So basically Jeannot tells you where your data is there going to be an Alpha.",
                    "label": 1
                },
                {
                    "sent": "Kind of tells you how likely they are to stick to each other, so we're going to use this as a prior on distribution for hidden parameters of a mixture model.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Did I?",
                    "label": 0
                },
                {
                    "sent": "So the representation of the DP GLM has this form.",
                    "label": 0
                },
                {
                    "sent": "We're going to drop key from a garishly process.",
                    "label": 0
                },
                {
                    "sent": "Then we're going to draw hidden parameters from P and then these in parameters.",
                    "label": 0
                },
                {
                    "sent": "Parameterized two parametric functions F of X&FYF of X is a distribution for the covariates.",
                    "label": 0
                },
                {
                    "sent": "FY is a distribution for the response, which is conditioned on the covariates and parameters that have to deal with the response.",
                    "label": 0
                },
                {
                    "sent": "So for example, let's say we have continuous covariates and a continuous response.",
                    "label": 0
                },
                {
                    "sent": "We're going to use Gaussian model.",
                    "label": 0
                },
                {
                    "sent": "So we're going to draw Theta from P. Data is going to be mean and variance for a normal distribution for covariates and slope intercept parameters an variance for normal distribution for why that has a mean beta not plus beta one times.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Barrett this is an example.",
                    "label": 0
                },
                {
                    "sent": "So there's a Gaussian clustering for the covariates, and then a linear model for the response.",
                    "label": 0
                },
                {
                    "sent": "And notice that the variance can change with the.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Coverts so how do we compute all this?",
                    "label": 0
                },
                {
                    "sent": "Will start with data and data points, and we want to compute the expectation of Y at a query point X.",
                    "label": 0
                },
                {
                    "sent": "So what were forced going to do is choose the GLM that we want.",
                    "label": 1
                },
                {
                    "sent": "There are lots of different models.",
                    "label": 0
                },
                {
                    "sent": "Then we're going to choose a dear sleep process based measure G, not.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to estimate the posterior distribution of our hidden parameters.",
                    "label": 1
                },
                {
                    "sent": "So I use Gibbs sampling.",
                    "label": 0
                },
                {
                    "sent": "You can also use variational inference.",
                    "label": 0
                },
                {
                    "sent": "I usually use Neil Algorithms 3, six or eight depending on what I need, so we're going to obtain mids samples of these parameters, and then we're going to use these to compute the predicted value.",
                    "label": 1
                },
                {
                    "sent": "The response.",
                    "label": 0
                },
                {
                    "sent": "This is done with the tower property of expect.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tations so there is kind of an ugly integral.",
                    "label": 0
                },
                {
                    "sent": "It's really not that bad.",
                    "label": 0
                },
                {
                    "sent": "So given the parameters expectation of Y is really easy to compute per cluster, and then we wait this by the.",
                    "label": 0
                },
                {
                    "sent": "Covariate density of X evaluated that cluster.",
                    "label": 0
                },
                {
                    "sent": "And then we have an A term for the expectation under G, not.",
                    "label": 0
                },
                {
                    "sent": "It's all normalized by B.",
                    "label": 0
                },
                {
                    "sent": "So we're going to get EM observations of Theta, yeah, but that is unknown, so we need to average over these samples.",
                    "label": 1
                },
                {
                    "sent": "So basically we do this for our M samples and we just take a simple average.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as some token biasness.",
                    "label": 0
                },
                {
                    "sent": "The idea behind this is that we want the estimates or mean function to converge to the trimming function as we get more observations.",
                    "label": 1
                },
                {
                    "sent": "This basically a frequentist justification of Bayesian methods.",
                    "label": 1
                },
                {
                    "sent": "This is not necessarily a given.",
                    "label": 1
                },
                {
                    "sent": "When you used a richly process, priors, Diaconis and Friedman had a very simple example.",
                    "label": 0
                },
                {
                    "sent": "With a student T distribution that did not converge, so asymptotically biasness depends on the true distribution of FX&Y, which are going to note my F not.",
                    "label": 0
                },
                {
                    "sent": "The model AK you know F of X&FY and her base.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Measure G not.",
                    "label": 0
                },
                {
                    "sent": "So the DP GLM is lost some topical in biasness over compact set of covariates.",
                    "label": 1
                },
                {
                    "sent": "If it puts positive power density on LCL neighborhoods and all Cal variance neighborhoods, there is a finite variance for every under the true distribution for every covariant.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Basically.",
                    "label": 0
                },
                {
                    "sent": "If the expectation under Jeannot is a little bit more than integral, three usual.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Follows from 1.",
                    "label": 0
                },
                {
                    "sent": "OK, so the first condition that Cal conditions rather hard to show.",
                    "label": 1
                },
                {
                    "sent": "Get satisfied under a Gaussian model with conjugate base measures, which is a normal inverse gamma or continuous and categorical covariates response again with conjugate base measures with categorical covariates.",
                    "label": 1
                },
                {
                    "sent": "It's a deer ishly, so anything else is an open question, probably going.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Be hard to prove.",
                    "label": 0
                },
                {
                    "sent": "Alright, so here's the fun part.",
                    "label": 0
                },
                {
                    "sent": "Empirical analysis.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So why would I want to use this over just a standard dearsley processed mixture model for regression?",
                    "label": 0
                },
                {
                    "sent": "So standard DP mixture models on the left?",
                    "label": 0
                },
                {
                    "sent": "the DP GLM is on the right and what I did was I took some clustered data.",
                    "label": 0
                },
                {
                    "sent": "I used only one dimension as a projective dimension and then I added on a bunch of spurious dimensions.",
                    "label": 0
                },
                {
                    "sent": "Those are shown vertically, so without a lot.",
                    "label": 0
                },
                {
                    "sent": "You know, as far as dimensions, the dearsley process mixture does OK.",
                    "label": 0
                },
                {
                    "sent": "But as you add more and more sparse dimensions, it does worse and worse.",
                    "label": 0
                },
                {
                    "sent": "Basically it starts trying to fit the mixtures too.",
                    "label": 0
                },
                {
                    "sent": "The covariance rather than response, so the covariant posterior swamps response but exterior but with PDP LM you have some more parameters and their associated with the response so.",
                    "label": 0
                },
                {
                    "sent": "It's more resistant to dimensionality.",
                    "label": 0
                },
                {
                    "sent": "The downside, which you can see on #20 down there, is that all these parameters do add in some noise, but not that much.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so one of the other features of this is that you can accommodate a lot of different data properties such as headers capacity or overdispersion in natural way.",
                    "label": 0
                },
                {
                    "sent": "So I compared it to Gaussian processes, top right, a treed Gaussian process, bottom left, and tree linear model, bottom right.",
                    "label": 1
                },
                {
                    "sent": "Gaussian processes have constant variance that's assumed in the model.",
                    "label": 0
                },
                {
                    "sent": "So basically your main function can get.",
                    "label": 0
                },
                {
                    "sent": "A little disport distorted in the areas with high variance that read models are designed to deal with.",
                    "label": 0
                },
                {
                    "sent": "Changing variance, but again these can get a little distorted in the areas with high variance and don't necessarily produce a smooth function.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I compare compare the PMM two other methods on three datasets, cosmic microwave background, which is chosen because it is heteroskedastic.",
                    "label": 0
                },
                {
                    "sent": "Concrete compressive strength, which has fairly low noise but eight covariates so has moderate dimensionality and solar flare data which has 11 categorical covariance and account response.",
                    "label": 1
                },
                {
                    "sent": "So it's again has moderate dimensionality but.",
                    "label": 0
                },
                {
                    "sent": "Kind of odd.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very response types.",
                    "label": 0
                },
                {
                    "sent": "So I compared with linear least squares, tree regression, tree linear models.",
                    "label": 1
                },
                {
                    "sent": "Gaussian process regression tree.",
                    "label": 1
                },
                {
                    "sent": "Gaussian processes directly process without the GLM and puts on regression for the.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dataset.",
                    "label": 0
                },
                {
                    "sent": "So on the HETEROSKEDASTIC data set, the DP GLM was continually one of the best methods.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On the concrete compressive strength data set.",
                    "label": 1
                },
                {
                    "sent": "It wasn't necessarily the best method, but it was competitive.",
                    "label": 0
                },
                {
                    "sent": "And some of the other kind of high tech Bayesian methods.",
                    "label": 0
                },
                {
                    "sent": "Weren't stable on this data set, so they weren't plotted on there.",
                    "label": 0
                },
                {
                    "sent": "Basically they do very well or blow up and have errors of.",
                    "label": 0
                },
                {
                    "sent": "Up to 12,000.",
                    "label": 0
                },
                {
                    "sent": "When everything else?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To follow one.",
                    "label": 0
                },
                {
                    "sent": "On the solar data set, I couldn't compare compare against much due to the categorical covariates and account response.",
                    "label": 0
                },
                {
                    "sent": "But again, the PGM was one of the best methods.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So issues with this.",
                    "label": 0
                },
                {
                    "sent": "I'd like to automate choice of base measured in hyperparameters, and I'd like to investigate the balance between modeling covariance in response.",
                    "label": 1
                },
                {
                    "sent": "So the benefits of this method is is very flexible.",
                    "label": 0
                },
                {
                    "sent": "I can use it in many settings, so generally competitive the outputs are generally stable an you can comment headers capacity and overdispersion in a natural manner.",
                    "label": 1
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "So first.",
                    "label": 0
                },
                {
                    "sent": "1st.",
                    "label": 0
                },
                {
                    "sent": "The model, it seems that you're modeling the covariance as well.",
                    "label": 0
                },
                {
                    "sent": "The other hand in regression setting coverage cutoff nuisance parameters.",
                    "label": 0
                },
                {
                    "sent": "I aren't you going to be heard because you're trying to model more than strictly speaking what you need.",
                    "label": 0
                },
                {
                    "sent": "It depends on the setting.",
                    "label": 0
                },
                {
                    "sent": "It's kind of a tradeoff between the flexibility you know and as you said, the extra noise in there.",
                    "label": 0
                },
                {
                    "sent": "So I've had pretty good results with this, but on the other hand, if I get some data set with.",
                    "label": 0
                },
                {
                    "sent": "Like 200 covariates.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I would just be adding noise.",
                    "label": 0
                },
                {
                    "sent": "Second question is regarding the period.",
                    "label": 0
                },
                {
                    "sent": "Property.",
                    "label": 0
                },
                {
                    "sent": "Taking spectation over the data as well?",
                    "label": 0
                },
                {
                    "sent": "Or is it?",
                    "label": 0
                },
                {
                    "sent": "Like probably one conversion probability, one convergence, yeah?",
                    "label": 0
                },
                {
                    "sent": "On the competitive.",
                    "label": 0
                },
                {
                    "sent": "Solution times to the other method as well.",
                    "label": 0
                },
                {
                    "sent": "Program cards.",
                    "label": 0
                },
                {
                    "sent": "So traditional cart, yeah, it's a lot slower.",
                    "label": 0
                },
                {
                    "sent": "It's actually faster than the Bayesian cart implementation that I had, which is a TGP package in R. Gibbs sampling can be fairly slow, but it's not that bad if you have conjugate priors and use something like.",
                    "label": 0
                },
                {
                    "sent": "I use Java or C++.",
                    "label": 0
                },
                {
                    "sent": "It's on larger datasets.",
                    "label": 0
                },
                {
                    "sent": "It's actually much faster than like Gaussian processes, 'cause you don't have to do a large matrix inversion.",
                    "label": 0
                },
                {
                    "sent": "So like 5000 data points or more.",
                    "label": 0
                },
                {
                    "sent": "Usually, so if I'm I have like an hourly wind data set for a year.",
                    "label": 0
                },
                {
                    "sent": "Usually if I want to run that to convergence is about 10 minutes.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Using the joint.",
                    "label": 0
                },
                {
                    "sent": "Of blasters.",
                    "label": 0
                },
                {
                    "sent": "I would like to have different number of questions correct and for why even ex no no.",
                    "label": 0
                },
                {
                    "sent": "So the way that I write.",
                    "label": 0
                },
                {
                    "sent": "Unless you have access three months, then the progression factors anymore.",
                    "label": 0
                },
                {
                    "sent": "Right, it's it's not that much of a problem.",
                    "label": 0
                },
                {
                    "sent": "So basically I kind of want to get away from that by doing a joint density, so that would probably have you know, 3 * 2 modes.",
                    "label": 0
                },
                {
                    "sent": "The Giants.",
                    "label": 0
                },
                {
                    "sent": "Ensures that your mother is very consistent for the joint right.",
                    "label": 0
                },
                {
                    "sent": "Need this for the.",
                    "label": 0
                },
                {
                    "sent": "Yeah I do.",
                    "label": 0
                },
                {
                    "sent": "Because as you said, right?",
                    "label": 0
                },
                {
                    "sent": "So there are a lot of papers.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I know.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so there are a lot of papers that will do some sort of other process over the covariates and then use like a deer sleep process to cluster the responses and these tend to work pretty well, but they're just not as flexible as I need so.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}